[{"id": "1809.00052", "submitter": "Niki Gitinabard", "authors": "Niki Gitinabard, Farzaneh Khoshnevisan, Collin F. Lynch, and Elle Yuan\n  Wang", "title": "Your Actions or Your Associates? Predicting Certification and Dropout in\n  MOOCs with Behavioral and Social Features", "comments": "Published at the 11th International Conference on Educational Data\n  Mining (EDM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high level of attrition and low rate of certification in Massive Open\nOnline Courses (MOOCs) has prompted a great deal of research. Prior researchers\nhave focused on predicting dropout based upon behavioral features such as\nstudent confusion, click-stream patterns, and social interactions. However, few\nstudies have focused on combining student logs with forum data.\n  In this work, we use data from two different offerings of the same MOOC. We\nconduct a survival analysis to identify likely dropouts. We then examine two\nclasses of features, social and behavioral, and apply a combination of modeling\nand feature-selection methods to identify the most relevant features to predict\nboth dropout and certification. We examine the utility of three different model\ntypes and we consider the impact of different definitions of dropout on the\npredictors. Finally, we assess the reliability of the models over time by\nevaluating whether or not models from week 1 can predict dropout in week 2, and\nso on. The outcomes of this study will help instructors identify students\nlikely to fail or dropout as soon as the first two weeks and provide them with\nmore support.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 20:39:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gitinabard", "Niki", ""], ["Khoshnevisan", "Farzaneh", ""], ["Lynch", "Collin F.", ""], ["Wang", "Elle Yuan", ""]]}, {"id": "1809.00065", "submitter": "Siwakorn Srisakaokul", "authors": "Siwakorn Srisakaokul, Yuhao Zhang, Zexuan Zhong, Wei Yang, Tao Xie, Bo\n  Li", "title": "MULDEF: Multi-model-based Defense Against Adversarial Examples for\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being popularly used in many applications, neural network models have\nbeen found to be vulnerable to adversarial examples, i.e., carefully crafted\nexamples aiming to mislead machine learning models. Adversarial examples can\npose potential risks on safety and security critical applications. However,\nexisting defense approaches are still vulnerable to attacks, especially in a\nwhite-box attack scenario. To address this issue, we propose a new defense\napproach, named MulDef, based on robustness diversity. Our approach consists of\n(1) a general defense framework based on multiple models and (2) a technique\nfor generating these multiple models to achieve high defense capability. In\nparticular, given a target model, our framework includes multiple models\n(constructed from the target model) to form a model family. The model family is\ndesigned to achieve robustness diversity (i.e., an adversarial example\nsuccessfully attacking one model cannot succeed in attacking other models in\nthe family). At runtime, a model is randomly selected from the family to be\napplied on each input example. Our general framework can inspire rich future\nresearch to construct a desirable model family achieving higher robustness\ndiversity. Our evaluation results show that MulDef (with only up to 5 models in\nthe family) can substantially improve the target model's accuracy on\nadversarial examples by 22-74% in a white-box attack scenario, while\nmaintaining similar accuracy on legitimate examples.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 21:22:52 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 01:37:19 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 03:53:19 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Srisakaokul", "Siwakorn", ""], ["Zhang", "Yuhao", ""], ["Zhong", "Zexuan", ""], ["Yang", "Wei", ""], ["Xie", "Tao", ""], ["Li", "Bo", ""]]}, {"id": "1809.00068", "submitter": "Wei Wang", "authors": "Wei Wang and Taro Watanabe and Macduff Hughes and Tetsuji Nakagawa and\n  Ciprian Chelba", "title": "Denoising Neural Machine Translation Training with Trusted Data and\n  Online Data Selection", "comments": "11 pages, 2018 Third Conference on Machine Translation (WMT18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring domain relevance of data and identifying or selecting well-fit\ndomain data for machine translation (MT) is a well-studied topic, but denoising\nis not yet. Denoising is concerned with a different type of data quality and\ntries to reduce the negative impact of data noise on MT training, in\nparticular, neural MT (NMT) training. This paper generalizes methods for\nmeasuring and selecting data for domain MT and applies them to denoising NMT\ntraining. The proposed approach uses trusted data and a denoising curriculum\nrealized by online data selection. Intrinsic and extrinsic evaluations of the\napproach show its significant effectiveness for NMT to train on data with\nsevere noise.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 22:01:45 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Wei", ""], ["Watanabe", "Taro", ""], ["Hughes", "Macduff", ""], ["Nakagawa", "Tetsuji", ""], ["Chelba", "Ciprian", ""]]}, {"id": "1809.00082", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios and Cody Hyndman", "title": "NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation", "comments": "28 pages: main body, 24 pages: appendix, 8 Figures, 11 Tables", "journal-ref": "Journal of Machine Learning Research (JMLR), Volume: 22; 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 23:38:00 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:43:31 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 12:23:47 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 09:50:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kratsios", "Anastasis", ""], ["Hyndman", "Cody", ""]]}, {"id": "1809.00101", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Ruimao Zhang, Jiefeng Peng, Guanbin Li, Bowen Du, and\n  Liang Lin", "title": "Attentive Crowd Flow Machines", "comments": "ACM MM, full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 02:22:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Liu", "Lingbo", ""], ["Zhang", "Ruimao", ""], ["Peng", "Jiefeng", ""], ["Li", "Guanbin", ""], ["Du", "Bowen", ""], ["Lin", "Liang", ""]]}, {"id": "1809.00175", "submitter": "Kelvin Hsu", "authors": "Kelvin Hsu, Richard Nock, Fabio Ramos", "title": "Hyperparameter Learning for Conditional Kernel Mean Embeddings with\n  Rademacher Complexity Bounds", "comments": "Best Student Machine Learning Paper Award Winner at ECML-PKDD 2018\n  (European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional kernel mean embeddings are nonparametric models that encode\nconditional expectations in a reproducing kernel Hilbert space. While they\nprovide a flexible and powerful framework for probabilistic inference, their\nperformance is highly dependent on the choice of kernel and regularization\nhyperparameters. Nevertheless, current hyperparameter tuning methods\npredominantly rely on expensive cross validation or heuristics that is not\noptimized for the inference task. For conditional kernel mean embeddings with\ncategorical targets and arbitrary inputs, we propose a hyperparameter learning\nframework based on Rademacher complexity bounds to prevent overfitting by\nbalancing data fit against model complexity. Our approach only requires batch\nupdates, allowing scalable kernel hyperparameter tuning without invoking kernel\napproximations. Experiments demonstrate that our learning framework outperforms\ncompeting methods, and can be further extended to incorporate and learn deep\nneural network weights to improve generalization.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:33:31 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 21:08:09 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 04:29:37 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Hsu", "Kelvin", ""], ["Nock", "Richard", ""], ["Ramos", "Fabio", ""]]}, {"id": "1809.00238", "submitter": "Sabri Pllana", "authors": "Yasser Alsouda, Sabri Pllana, Arianit Kurti", "title": "A Machine Learning Driven IoT Solution for Noise Classification in Smart\n  Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning based method for noise classification using a\nlow-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients\nfor audio feature extraction and supervised classification algorithms (that is,\nsupport vector machine and k-nearest neighbors) for noise classification. We\nevaluate our approach experimentally with a dataset of about 3000 sound samples\ngrouped in eight sound classes (such as, car horn, jackhammer, or street\nmusic). We explore the parameter space of support vector machine and k-nearest\nneighbors algorithms to estimate the optimal parameter values for\nclassification of sound samples in the dataset under study. We achieve a noise\nclassification accuracy in the range 85% -- 100%. Training and testing of our\nk-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than\na second for a dataset with features of more than 3000 sound samples.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 19:11:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Alsouda", "Yasser", ""], ["Pllana", "Sabri", ""], ["Kurti", "Arianit", ""]]}, {"id": "1809.00306", "submitter": "Xi Zhang", "authors": "Xi Zhang and Yixuan Li and Senzhang Wang and Binxing Fang and Philip\n  S. Yu", "title": "Enhancing Stock Market Prediction with Extended Coupled Hidden Markov\n  Model over Multi-Sourced Data", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional stock market prediction methods commonly only utilize the\nhistorical trading data, ignoring the fact that stock market fluctuations can\nbe impacted by various other information sources such as stock related events.\nAlthough some recent works propose event-driven prediction approaches by\nconsidering the event data, how to leverage the joint impacts of multiple data\nsources still remains an open research problem. In this work, we study how to\nexplore multiple data sources to improve the performance of the stock\nprediction. We introduce an Extended Coupled Hidden Markov Model incorporating\nthe news events with the historical trading data. To address the data sparsity\nissue of news events for each single stock, we further study the fluctuation\ncorrelations between the stocks and incorporate the correlations into the model\nto facilitate the prediction task. Evaluations on China A-share market data in\n2016 show the superior performance of our model against previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 07:48:24 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhang", "Xi", ""], ["Li", "Yixuan", ""], ["Wang", "Senzhang", ""], ["Fang", "Binxing", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.00338", "submitter": "Jian Zhao", "authors": "Jian Zhao, Yu Cheng, Yi Cheng, Yang Yang, Haochong Lan, Fang Zhao, Lin\n  Xiong, Yan Xu, Jianshu Li, Sugiri Pranata, Shengmei Shen, Junliang Xing,\n  Hengzhu Liu, Shuicheng Yan, Jiashi Feng", "title": "Look Across Elapse: Disentangled Representation Learning and\n  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations. As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 13:58:37 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 01:53:17 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Zhao", "Jian", ""], ["Cheng", "Yu", ""], ["Cheng", "Yi", ""], ["Yang", "Yang", ""], ["Lan", "Haochong", ""], ["Zhao", "Fang", ""], ["Xiong", "Lin", ""], ["Xu", "Yan", ""], ["Li", "Jianshu", ""], ["Pranata", "Sugiri", ""], ["Shen", "Shengmei", ""], ["Xing", "Junliang", ""], ["Liu", "Hengzhu", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1809.00366", "submitter": "David Cortes", "authors": "David Cortes", "title": "Cold-start recommendations in Collective Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the ability of collective matrix factorization models in\nrecommender systems to make predictions about users and items for which there\nis side information available but no feedback or interactions data, and\nproposes a new formulation with a faster cold-start prediction formula that can\nbe used in real-time systems. While these cold-start recommendations are not as\ngood as warm-start ones, they were found to be of better quality than\nnon-personalized recommendations, and predictions about new users were found to\nbe more reliable than those about new items. The formulation proposed here\nresulted in improved cold-start recommendations in many scenarios, at the\nexpense of worse warm-start ones.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 16:24:38 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 15:07:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Cortes", "David", ""]]}, {"id": "1809.00381", "submitter": "Rachel Bittner", "authors": "Rachel M. Bittner and Brian McFee and Juan P. Bello", "title": "Multitask Learning for Fundamental Frequency Estimation in Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fundamental frequency (f0) estimation from polyphonic music includes the\ntasks of multiple-f0, melody, vocal, and bass line estimation. Historically\nthese problems have been approached separately, and only recently, using\nlearning-based approaches. We present a multitask deep learning architecture\nthat jointly estimates outputs for various tasks including multiple-f0, melody,\nvocal and bass line estimation, and is trained using a large,\nsemi-automatically annotated dataset. We show that the multitask model\noutperforms its single-task counterparts, and explore the effect of various\ndesign decisions in our approach, and show that it performs better or at least\ncompetitively when compared against strong baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 20:03:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Bittner", "Rachel M.", ""], ["McFee", "Brian", ""], ["Bello", "Juan P.", ""]]}, {"id": "1809.00403", "submitter": "Gang Chen", "authors": "Gang Chen and Yiming Peng and Mengjie Zhang", "title": "Effective Exploration for Deep Reinforcement Learning via Bootstrapped\n  Q-Ensembles under Tsallis Entropy Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep reinforcement learning (DRL) has achieved outstanding success\non solving many difficult and large-scale RL problems. However the high sample\ncost required for effective learning often makes DRL unaffordable in\nresource-limited applications. With the aim of improving sample efficiency and\nlearning performance, we will develop a new DRL algorithm in this paper that\nseamless integrates entropy-induced and bootstrap-induced techniques for\nefficient and deep exploration of the learning environment. Specifically, a\ngeneral form of Tsallis entropy regularizer will be utilized to drive\nentropy-induced exploration based on efficient approximation of optimal\naction-selection policies. Different from many existing works that rely on\naction dithering strategies for exploration, our algorithm is efficient in\nexploring actions with clear exploration value. Meanwhile, by employing an\nensemble of Q-networks under varied Tsallis entropy regularization, the\ndiversity of the ensemble can be further enhanced to enable effective\nbootstrap-induced exploration. Experiments on Atari game playing tasks clearly\ndemonstrate that our new algorithm can achieve more efficient and effective\nexploration for DRL, in comparison to recently proposed exploration methods\nincluding Bootstrapped Deep Q-Network and UCB Q-Ensemble.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 22:17:52 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 01:45:01 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Chen", "Gang", ""], ["Peng", "Yiming", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1809.00510", "submitter": "Hugo Caselles-Dupr\\'e", "authors": "Hugo Caselles-Dupr\\'e, Louis Annabi, Oksana Hagen, Michael\n  Garcia-Ortiz, David Filliat", "title": "Flatland: a Lightweight First-Person 2-D Environment for Reinforcement\n  Learning", "comments": "Accepted to the Workshop on Continual Unsupervised Sensorimotor\n  Learning (ICDL-EpiRob 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flatland is a simple, lightweight environment for fast prototyping and\ntesting of reinforcement learning agents. It is of lower complexity compared to\nsimilar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical\nproperties of the real world, such as continuity, multi-modal\npartially-observable states with first-person view and coherent physics. We\npropose to use it as an intermediary benchmark for problems related to Lifelong\nLearning. Flatland is highly customizable and offers a wide range of task\ndifficulty to extensively evaluate the properties of artificial agents. We\nexperiment with three reinforcement learning baseline agents and show that they\ncan rapidly solve a navigation task in Flatland. A video of an agent acting in\nFlatland is available here: https://youtu.be/I5y6Y2ZypdA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 09:07:30 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 08:29:30 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Caselles-Dupr\u00e9", "Hugo", ""], ["Annabi", "Louis", ""], ["Hagen", "Oksana", ""], ["Garcia-Ortiz", "Michael", ""], ["Filliat", "David", ""]]}, {"id": "1809.00542", "submitter": "Nikola Simidjievski", "authors": "Matej Petkovi\\'c, Redouane Boumghar, Martin Breskvar, Sa\\v{s}o\n  D\\v{z}eroski, Dragi Kocev, Jurica Levati\\'c, Luke Lucas, Alja\\v{z} Osojnik,\n  Bernard \\v{Z}enko and Nikola Simidjievski", "title": "Machine learning for predicting thermal power consumption of the Mars\n  Express Spacecraft", "comments": null, "journal-ref": "IEEE Aerospace and Electronic Systems Magazine. 34(7), 46-60.\n  (2019)", "doi": "10.1109/MAES.2019.2915456", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-board\nequipment within its pre-defined operating temperatures range. To plan and\noptimize the scientific operations of MEX, its operators need to estimate in\nadvance, as accurately as possible, the power consumption of the thermal\nsubsystem. The remaining power can then be allocated for scientific purposes.\nWe present a machine learning pipeline for efficiently constructing accurate\npredictive models for predicting the power of the thermal subsystem on board\nMEX. In particular, we employ state-of-the-art feature engineering approaches\nfor transforming raw telemetry data, in turn used for constructing accurate\nmodels with different state-of-the-art machine learning methods. We show that\nthe proposed pipeline considerably improve our previous (competition-winning)\nwork in terms of time efficiency and predictive performance. Moreover, while\nachieving superior predictive performance, the constructed models also provide\nimportant insight into the spacecraft's behavior, allowing for further analyses\nand optimal planning of MEX's operation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 10:43:02 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 13:43:33 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Petkovi\u0107", "Matej", ""], ["Boumghar", "Redouane", ""], ["Breskvar", "Martin", ""], ["D\u017eeroski", "Sa\u0161o", ""], ["Kocev", "Dragi", ""], ["Levati\u0107", "Jurica", ""], ["Lucas", "Luke", ""], ["Osojnik", "Alja\u017e", ""], ["\u017denko", "Bernard", ""], ["Simidjievski", "Nikola", ""]]}, {"id": "1809.00593", "submitter": "Tanguy Kerdoncuff", "authors": "Tanguy Kerdoncuff, R\\'emi Emonet", "title": "IoU is not submodular", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short article aims at demonstrate that the Intersection over Union (or\nJaccard index) is not a submodular function. This mistake has been made in an\narticle which is cited and used as a foundation in another article. The\nIntersection of Union is widely used in machine learning as a cost function\nespecially for imbalance data and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 13:21:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kerdoncuff", "Tanguy", ""], ["Emonet", "R\u00e9mi", ""]]}, {"id": "1809.00594", "submitter": "Sanli Tang", "authors": "Sanli Tang, Xiaolin Huang, Mingjian Chen, Chengjin Sun, and Jie Yang", "title": "Adversarial Attack Type I: Cheat Classifiers by Significant Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of deep neural networks, the adversarial attack can\ncheat some well-trained classifiers by small permutations. In this paper, we\npropose another type of adversarial attack that can cheat classifiers by\nsignificant changes. For example, we can significantly change a face but\nwell-trained neural networks still recognize the adversarial and the original\nexample as the same person. Statistically, the existing adversarial attack\nincreases Type II error and the proposed one aims at Type I error, which are\nhence named as Type II and Type I adversarial attack, respectively. The two\ntypes of attack are equally important but are essentially different, which are\nintuitively explained and numerically evaluated. To implement the proposed\nattack, a supervised variation autoencoder is designed and then the classifier\nis attacked by updating the latent variables using gradient information.\n{Besides, with pre-trained generative models, Type I attack on latent spaces is\ninvestigated as well.} Experimental results show that our method is practical\nand effective to generate Type I adversarial examples on large-scale image\ndatasets. Most of these generated examples can pass detectors designed for\ndefending Type II attack and the strengthening strategy is only efficient with\na specific type attack, both implying that the underlying reasons for Type I\nand Type II attack are different.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 13:25:06 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:00:58 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tang", "Sanli", ""], ["Huang", "Xiaolin", ""], ["Chen", "Mingjian", ""], ["Sun", "Chengjin", ""], ["Yang", "Jie", ""]]}, {"id": "1809.00653", "submitter": "Vlad Niculae", "authors": "Vlad Niculae, Andr\\'e F. T. Martins, Claire Cardie", "title": "Towards Dynamic Computation Graphs via Sparse Latent Structure", "comments": "EMNLP 2018; 9 pages (incl. appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep NLP models benefit from underlying structures in the data---e.g., parse\ntrees---typically extracted using off-the-shelf parsers. Recent attempts to\njointly learn the latent structure encounter a tradeoff: either make\nfactorization assumptions that limit expressiveness, or sacrifice end-to-end\ndifferentiability. Using the recently proposed SparseMAP inference, which\nretrieves a sparse distribution over latent structures, we propose a novel\napproach for end-to-end learning of latent structure predictors jointly with a\ndownstream predictor. To the best of our knowledge, our method is the first to\nenable unrestricted dynamic computation graph construction from the global\nlatent structure, while maintaining differentiability.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:52:19 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Niculae", "Vlad", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Cardie", "Claire", ""]]}, {"id": "1809.00710", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Soomin Lee and Alexander Gasnikov and Angelia\n  Nedi\\'c", "title": "A Dual Approach for Optimal Algorithms in Distributed Optimization over\n  Networks", "comments": "This work is an extended version of the manuscript: Optimal\n  Algorithms for Distributed Optimization arXiv:1712.00232", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dual-based algorithms for distributed convex optimization problems\nover networks, where the objective is to minimize a sum $\\sum_{i=1}^{m}f_i(z)$\nof functions over in a network. We provide complexity bounds for four different\ncases, namely: each function $f_i$ is strongly convex and smooth, each function\nis either strongly convex or smooth, and when it is convex but neither strongly\nconvex nor smooth. Our approach is based on the dual of an appropriately\nformulated primal problem, which includes a graph that models the communication\nrestrictions. We propose distributed algorithms that achieve the same optimal\nrates as their centralized counterparts (up to constant and logarithmic\nfactors), with an additional optimal cost related to the spectral properties of\nthe network. Initially, we focus on functions for which we can explicitly\nminimize its Legendre-Fenchel conjugate, i.e., admissible or dual friendly\nfunctions. Then, we study distributed optimization algorithms for non-dual\nfriendly functions, as well as a method to improve the dependency on the\nparameters of the functions involved. Numerical analysis of the proposed\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 20:13:25 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 21:27:17 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 13:55:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Lee", "Soomin", ""], ["Gasnikov", "Alexander", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1809.00734", "submitter": "Ivana Malenica", "authors": "Mark J. van der Laan and Ivana Malenica", "title": "Robust Estimation of Data-Dependent Causal Effects based on Observing a\n  Single Time-Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the case that one observes a single time-series, where at each time\nt one observes a data record O(t) involving treatment nodes A(t), possible\ncovariates L(t) and an outcome node Y(t). The data record at time t carries\ninformation for an (potentially causal) effect of the treatment A(t) on the\noutcome Y(t), in the context defined by a fixed dimensional summary measure\nCo(t). We are concerned with defining causal effects that can be consistently\nestimated, with valid inference, for sequentially randomized experiments\nwithout further assumptions. More generally, we consider the case when the\n(possibly causal) effects can be estimated in a double robust manner, analogue\nto double robust estimation of effects in the i.i.d. causal inference\nliterature. We propose a general class of averages of conditional\n(context-specific) causal parameters that can be estimated in a double robust\nmanner, therefore fully utilizing the sequential randomization. We propose a\ntargeted maximum likelihood estimator (TMLE) of these causal parameters, and\npresent a general theorem establishing the asymptotic consistency and normality\nof the TMLE. We extend our general framework to a number of typically studied\ncausal target parameters, including a sequentially adaptive design within a\nsingle unit that learns the optimal treatment rule for the unit over time. Our\nwork opens up robust statistical inference for causal questions based on\nobserving a single time-series on a particular unit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 22:02:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Malenica", "Ivana", ""]]}, {"id": "1809.00758", "submitter": "Myungsu Chae", "authors": "Myungsu Chae, Tae-Ho Kim, Young Hoon Shin, June-Woo Kim, and Soo-Young\n  Lee", "title": "End-to-end Multimodal Emotion and Gender Recognition with Dynamic Joint\n  Loss Weights", "comments": "IROS 2018 Workshop on Crossmodal Learning for Intelligent Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a method for improving the generalizability of\nmultiple tasks. In order to perform multiple classification tasks with one\nneural network model, the losses of each task should be combined. Previous\nstudies have mostly focused on multiple prediction tasks using joint loss with\nstatic weights for training models, choosing the weights between tasks without\nmaking sufficient considerations by setting them uniformly or empirically. In\nthis study, we propose a method to calculate joint loss using dynamic weights\nto improve the total performance, instead of the individual performance, of\ntasks. We apply this method to design an end-to-end multimodal emotion and\ngender recognition model using audio and video data. This approach provides\nproper weights for the loss of each task when the training process ends. In our\nexperiments, emotion and gender recognition with the proposed method yielded a\nlower joint loss, which is computed as the negative log-likelihood, than using\nstatic weights for joint loss. Moreover, our proposed model has better\ngeneralizability than other models. To the best of our knowledge, this research\nis the first to demonstrate the strength of using dynamic weights for joint\nloss for maximizing overall performance in emotion and gender recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 00:52:25 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 06:55:13 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 04:16:54 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Chae", "Myungsu", ""], ["Kim", "Tae-Ho", ""], ["Shin", "Young Hoon", ""], ["Kim", "June-Woo", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1809.00770", "submitter": "I-Chao Shen", "authors": "Shu-Hsuan Hsu, I-Chao Shen, Bing-Yu Chen", "title": "Transferring Deep Reinforcement Learning with Adversarial Objective and\n  Augmentation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, deep reinforcement learning has been proven to solve\nproblems which have complex states like video games or board games. The next\nstep of intelligent agents would be able to generalize between tasks, and using\nprior experience to pick up new skills more quickly. However, most\nreinforcement learning algorithms for now are often suffering from catastrophic\nforgetting even when facing a very similar target task. Our approach enables\nthe agents to generalize knowledge from a single source task, and boost the\nlearning progress with a semisupervised learning method when facing a new task.\nWe evaluate this approach on Atari games, which is a popular reinforcement\nlearning benchmark, and show that it outperforms common baselines based on\npre-training and fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:13:37 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hsu", "Shu-Hsuan", ""], ["Shen", "I-Chao", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1809.00800", "submitter": "Sho Yokoi", "authors": "Sho Yokoi, Sosuke Kobayashi, Kenji Fukumizu, Jun Suzuki, Kentaro Inui", "title": "Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse\n  Linguistic Expressions", "comments": "Accepted by EMNLP 2018", "journal-ref": "EMNLP 2018", "doi": "10.18653/v1/D18-1203", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new kernel-based co-occurrence measure that can\nbe applied to sparse linguistic expressions (e.g., sentences) with a very short\nlearning time, as an alternative to pointwise mutual information (PMI). As well\nas deriving PMI from mutual information, we derive this new measure from the\nHilbert--Schmidt independence criterion (HSIC); thus, we call the new measure\nthe pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of\nPMI that allows various similarity metrics (e.g., sentence embeddings) to be\nplugged in as kernels. Moreover, PHSIC can be estimated by simple and fast\n(linear in the size of the data) matrix calculations regardless of whether we\nuse linear or nonlinear kernels. Empirically, in a dialogue response selection\ntask, PHSIC is learned thousands of times faster than an RNN-based PMI while\noutperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is\nbeneficial as a criterion of a data selection task for machine translation\nowing to its ability to give high (low) scores to a consistent (inconsistent)\npair with other pairs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 05:33:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yokoi", "Sho", ""], ["Kobayashi", "Sosuke", ""], ["Fukumizu", "Kenji", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "1809.00811", "submitter": "Ahmed BenSaid", "authors": "Ahmed Ben Said and Abdelkarim Erradi and Azadeh Ghari Neiat and Athman\n  Bouguettaya", "title": "A Deep Learning Spatiotemporal Prediction Framework for Mobile\n  Crowdsourced Services", "comments": null, "journal-ref": null, "doi": "10.1007/s11036-018-1105-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers presents a deep learning-based framework to predict crowdsourced\nservice availability spatially and temporally. A novel two-stage prediction\nmodel is introduced based on historical spatio-temporal traces of mobile\ncrowdsourced services. The prediction model first clusters mobile crowdsourced\nservices into regions. The availability prediction of a mobile crowdsourced\nservice at a certain location and time is then formulated as a classification\nproblem. To determine the availability duration of predicted mobile\ncrowdsourced services, we formulate a forecasting task of time series using the\nGramian Angular Field. We validated the effectiveness of the proposed framework\nthrough multiple experiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 07:03:58 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Said", "Ahmed Ben", ""], ["Erradi", "Abdelkarim", ""], ["Neiat", "Azadeh Ghari", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "1809.00832", "submitter": "Eunji Jeong", "authors": "Eunji Jeong, Joo Seong Jeong, Soojeong Kim, Gyeong-In Yu, Byung-Gon\n  Chun", "title": "Improving the Expressiveness of Deep Learning Frameworks with Recursion", "comments": "Appeared in EuroSys 2018. 13 pages, 11 figures", "journal-ref": "EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,\n  Porto, Portugal", "doi": "10.1145/3190508.3190530", "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recursive neural networks have widely been used by researchers to handle\napplications with recursively or hierarchically structured data. However,\nembedded control flow deep learning frameworks such as TensorFlow, Theano,\nCaffe2, and MXNet fail to efficiently represent and execute such neural\nnetworks, due to lack of support for recursion. In this paper, we add recursion\nto the programming model of existing frameworks by complementing their design\nwith recursive execution of dataflow graphs as well as additional APIs for\nrecursive definitions. Unlike iterative implementations, which can only\nunderstand the topological index of each node in recursive data structures, our\nrecursive implementation is able to exploit the recursive relationships between\nnodes for efficient execution based on parallel computation. We present an\nimplementation on TensorFlow and evaluation results with various recursive\nneural network models, showing that our recursive implementation not only\nconveys the recursive nature of recursive neural networks better than other\nimplementations, but also uses given resources more effectively to reduce\ntraining and inference time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:31:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jeong", "Eunji", ""], ["Jeong", "Joo Seong", ""], ["Kim", "Soojeong", ""], ["Yu", "Gyeong-In", ""], ["Chun", "Byung-Gon", ""]]}, {"id": "1809.00836", "submitter": "Andrea Esuli", "authors": "Andrea Esuli, Alejandro Moreo Fern\\'andez, Fabrizio Sebastiani", "title": "A Recurrent Neural Network for Sentiment Quantification", "comments": "Accepted for publication at CIKM 2018", "journal-ref": null, "doi": "10.1145/3269206.3269287", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification is a supervised learning task that consists in predicting,\ngiven a set of classes C and a set D of unlabelled items, the prevalence (or\nrelative frequency) p(c|D) of each class c in C. Quantification can in\nprinciple be solved by classifying all the unlabelled items and counting how\nmany of them have been attributed to each class. However, this \"classify and\ncount\" approach has been shown to yield suboptimal quantification accuracy;\nthis has established quantification as a task of its own, and given rise to a\nnumber of methods specifically devised for it. We propose a recurrent neural\nnetwork architecture for quantification (that we call QuaNet) that observes the\nclassification predictions to learn higher-order \"quantification embeddings\",\nwhich are then refined by incorporating quantification predictions of simple\nclassify-and-count-like methods. We test {QuaNet on sentiment quantification on\ntext, showing that it substantially outperforms several state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:41:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Esuli", "Andrea", ""], ["Fern\u00e1ndez", "Alejandro Moreo", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1809.00846", "submitter": "Ping Luo", "authors": "Ping Luo and Xinjiang Wang and Wenqi Shao and Zhanglin Peng", "title": "Towards Understanding Regularization in Batch Normalization", "comments": "International Conference on Learning Representations (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) improves both convergence and generalization in\ntraining neural networks. This work understands these phenomena theoretically.\nWe analyze BN by using a basic block of neural networks, consisting of a kernel\nlayer, a BN layer, and a nonlinear activation function. This basic network\nhelps us understand the impacts of BN in three aspects. First, by viewing BN as\nan implicit regularizer, BN can be decomposed into population normalization\n(PN) and gamma decay as an explicit regularization. Second, learning dynamics\nof BN and the regularization show that training converged with large maximum\nand effective learning rate. Third, generalization of BN is explored by using\nstatistical mechanics. Experiments demonstrate that BN in convolutional neural\nnetworks share the same traits of regularization as the above analyses.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:01:10 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 17:56:50 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 06:11:53 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 05:23:45 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Luo", "Ping", ""], ["Wang", "Xinjiang", ""], ["Shao", "Wenqi", ""], ["Peng", "Zhanglin", ""]]}, {"id": "1809.00852", "submitter": "Huanhuan Yu", "authors": "Huanhuan Yu, Menglei Hu and Songcan Chen", "title": "Multi-target Unsupervised Domain Adaptation without Exactly Shared\n  Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to learn the unlabeled target\ndomain by transferring the knowledge of the labeled source domain. To date,\nmost of the existing works focus on the scenario of one source domain and one\ntarget domain (1S1T), and just a few works concern the scenario of multiple\nsource domains and one target domain (mS1T). While, to the best of our\nknowledge, almost no work concerns the scenario of one source domain and\nmultiple target domains (1SmT), in which these unlabeled target domains may not\nnecessarily share the same categories, therefore, contrasting to mS1T, 1SmT is\nmore challenging. Accordingly, for such a new UDA scenario, we propose a UDA\nframework through the model parameter adaptation (PA-1SmT). A key ingredient of\nPA-1SmT is to transfer knowledge through adaptive learning of a common model\nparameter dictionary, which is completely different from existing popular\nmethods for UDA, such as subspace alignment, distribution matching etc., and\ncan also be directly used for DA of privacy protection due to the fact that the\nknowledge is transferred just via the model parameters rather than data itself.\nFinally, our experimental results on three domain adaptation benchmark datasets\ndemonstrate the superiority of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:18:19 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 07:13:46 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yu", "Huanhuan", ""], ["Hu", "Menglei", ""], ["Chen", "Songcan", ""]]}, {"id": "1809.00862", "submitter": "Omar Mohammed", "authors": "Omar Mohammed, Gerard Bailly, Damien Pellier", "title": "Handwriting styles: benchmarks and evaluation metrics", "comments": "Submitted to IEEE International Workshop on Deep and Transfer\n  Learning (DTL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:54:25 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mohammed", "Omar", ""], ["Bailly", "Gerard", ""], ["Pellier", "Damien", ""]]}, {"id": "1809.00934", "submitter": "Xingyi Song", "authors": "Xingyi Song, Johann Petrak, Angus Roberts", "title": "A Deep Neural Network Sentence Level Classification Method with Context\n  Information", "comments": "Accepted at EMNLP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sentence classification task, context formed from sentences adjacent\nto the sentence being classified can provide important information for\nclassification. This context is, however, often ignored. Where methods do make\nuse of context, only small amounts are considered, making it difficult to\nscale. We present a new method for sentence classification, Context-LSTM-CNN,\nthat makes use of potentially large contexts. The method also utilizes\nlong-range dependencies within the sentence being classified, using an LSTM,\nand short-span features, using a stacked CNN. Our experiments demonstrate that\nthis approach consistently improves over previous methods on two different\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:45:33 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Song", "Xingyi", ""], ["Petrak", "Johann", ""], ["Roberts", "Angus", ""]]}, {"id": "1809.00946", "submitter": "Jerry Li", "authors": "Jerry Li", "title": "Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing\n  GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for translating unlabeled images from one domain into\nanalog images in another domain. We employ a progressively growing\nskip-connected encoder-generator structure and train it with a GAN loss for\nrealistic output, a cycle consistency loss for maintaining same-domain\ntranslation identity, and a semantic consistency loss that encourages the\nnetwork to keep the input semantic features in the output. We apply our\nframework on the task of translating face images, and show that it is capable\nof learning semantic mappings for face images with no supervised one-to-one\nimage mapping.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 23:09:03 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Jerry", ""]]}, {"id": "1809.00947", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Katrin H\\\"ansel, Richard Clegg, Ilias Leontiadis,\n  Hamed Haddadi, Laurissa Tokarchuk", "title": "Finding Dory in the Crowd: Detecting Social Interactions using\n  Multi-Modal Mobile Sensing", "comments": "21 pages, 6 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remembering our day-to-day social interactions is challenging even if you\naren't a blue memory challenged fish. The ability to automatically detect and\nremember these types of interactions is not only beneficial for individuals\ninterested in their behavior in crowded situations, but also of interest to\nthose who analyze crowd behavior. Currently, detecting social interactions is\noften performed using a variety of methods including ethnographic studies,\ncomputer vision techniques and manual annotation-based data analysis. However,\nmobile phones offer easier means for data collection that is easy to analyze\nand can preserve the user's privacy. In this work, we present a system for\ndetecting stationary social interactions inside crowds, leveraging multi-modal\nmobile sensing data such as Bluetooth Smart (BLE), accelerometer and gyroscope.\nTo inform the development of such system, we conducted a study with 24\nparticipants, where we asked them to socialize with each other for 45 minutes.\nWe built a machine learning system based on gradient-boosted trees that\npredicts both 1:1 and group interactions with 77.8% precision and 86.5% recall,\na 30.2% performance increase compared to a proximity-based approach. By\nutilizing a community detection-based method, we further detected the various\ngroup formation that exist within the crowd. Using mobile phone sensors already\ncarried by the majority of people in a crowd makes our approach particularly\nwell suited to real-life analysis of crowd behavior and influence strategies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 11:30:19 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:11:52 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Katevas", "Kleomenis", ""], ["H\u00e4nsel", "Katrin", ""], ["Clegg", "Richard", ""], ["Leontiadis", "Ilias", ""], ["Haddadi", "Hamed", ""], ["Tokarchuk", "Laurissa", ""]]}, {"id": "1809.00953", "submitter": "Burak Satar", "authors": "Burak Satar, Ahmet Emir Dirik", "title": "Deep Learning Based Vehicle Make-Model Classification", "comments": "10 pages, ICANN 2018: Artificial Neural Networks and Machine Learning", "journal-ref": "Lecture Notes in Computer Science book series 2018 (LNCS, volume\n  11141). Springer, Cham", "doi": "10.1007/978-3-030-01424-7_53", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper studies the problems of vehicle make & model classification. Some\nof the main challenges are reaching high classification accuracy and reducing\nthe annotation time of the images. To address these problems, we have created a\nfine-grained database using online vehicle marketplaces of Turkey. A pipeline\nis proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN\n(Convolutional Neural Network) model to train on the database. In the pipeline,\nwe first detect the vehicles by following an algorithm which reduces the time\nfor annotation. Then, we feed them into the CNN model. It is reached\napproximately 4% better classification accuracy result than using a\nconventional CNN model. Next, we propose to use the detected vehicles as ground\ntruth bounding box (GTBB) of the images and feed them into an SSD model in\nanother pipeline. At this stage, it is reached reasonable classification\naccuracy result without using perfectly shaped GTBB. Lastly, an application is\nimplemented in a use case by using our proposed pipelines. It detects the\nunauthorized vehicles by comparing their license plate numbers and make &\nmodels. It is assumed that license plates are readable.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:05:31 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 20:46:17 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Satar", "Burak", ""], ["Dirik", "Ahmet Emir", ""]]}, {"id": "1809.00957", "submitter": "Pankaj Roy", "authors": "Pankaj Raj Roy and Guillaume-Alexandre Bilodeau", "title": "Road User Abnormal Trajectory Detection using a Deep Autoencoder", "comments": "This paper has been accepted for oral presentation at ISVC'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the development of a method that detects abnormal\ntrajectories of road users at traffic intersections. The main difficulty with\nthis is the fact that there are very few abnormal data and the normal ones are\ninsufficient for the training of any kinds of machine learning model. To tackle\nthese problems, we proposed the solution of using a deep autoencoder network\ntrained solely through augmented data considered as normal. By generating\nartificial abnormal trajectories, our method is tested on four different\noutdoor urban users scenes and performs better compared to some classical\noutlier detection methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 23:18:52 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Roy", "Pankaj Raj", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1809.00961", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, Nabagata Saha, Samarjit Karmakar, A G Ramakrishnan", "title": "MSCE: An edge preserving robust loss function for improving\n  super-resolution algorithms", "comments": "Accepted in ICONIP-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancement in the deep learning technologies such as CNNs\nand GANs, there is significant improvement in the quality of the images\nreconstructed by deep learning based super-resolution (SR) techniques. In this\nwork, we propose a robust loss function based on the preservation of edges\nobtained by the Canny operator. This loss function, when combined with the\nexisting loss function such as mean square error (MSE), gives better SR\nreconstruction measured in terms of PSNR and SSIM. Our proposed loss function\nguarantees improved performance on any existing algorithm using MSE loss\nfunction, without any increase in the computational complexity during testing.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 22:00:10 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Saha", "Nabagata", ""], ["Karmakar", "Samarjit", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1809.00977", "submitter": "Jacob Nogas", "authors": "Jacob Nogas, Shehroz S. Khan, Alex Mihailidis", "title": "DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal\n  Convolutional Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human falls rarely occur; however, detecting falls is very important from the\nhealth and safety perspective. Due to the rarity of falls, it is difficult to\nemploy supervised classification techniques to detect them. Moreover, in these\nhighly skewed situations it is also difficult to extract domain specific\nfeatures to identify falls. In this paper, we present a novel framework,\n\\textit{DeepFall}, which formulates the fall detection problem as an anomaly\ndetection problem. The \\textit{DeepFall} framework presents the novel use of\ndeep spatio-temporal convolutional autoencoders to learn spatial and temporal\nfeatures from normal activities using non-invasive sensing modalities. We also\npresent a new anomaly scoring method that combines the reconstruction score of\nframes across a temporal window to detect unseen falls. We tested the\n\\textit{DeepFall} framework on three publicly available datasets collected\nthrough non-invasive sensing modalities, thermal camera and depth cameras and\nshow superior results in comparison to traditional autoencoder methods to\nidentify unseen falls.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:41:58 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:06:38 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 18:12:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Nogas", "Jacob", ""], ["Khan", "Shehroz S.", ""], ["Mihailidis", "Alex", ""]]}, {"id": "1809.00999", "submitter": "Abdallah Moussawi", "authors": "Abdallah Moussawi", "title": "Towards Large Scale Training Of Autoencoders For Collaborative Filtering", "comments": "2 pages, ACM RecSys 2018 Late-breaking Results Track (Posters)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply a mini-batch based negative sampling method to\nefficiently train a latent factor autoencoder model on large scale and sparse\ndata for implicit feedback collaborative filtering. We compare our work against\na state-of-the-art baseline model on different experimental datasets and show\nthat this method can lead to a good and fast approximation of the baseline\nmodel performance. The source code is available in\nhttps://github.com/amoussawi/recoder .\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 22:34:29 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 00:13:30 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 07:44:07 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Moussawi", "Abdallah", ""]]}, {"id": "1809.01000", "submitter": "Fei Jiang", "authors": "Fei Jiang, Guosheng Yin", "title": "Bayesian Outdoor Defect Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian defect detector to facilitate the defect detection on\nthe motion blurred images on rough texture surfaces. To enhance the accuracy of\nBayesian detection on removing non-defect pixels, we develop a class of\nreflected non-local prior distributions, which is constructed by using the mode\nof a distribution to subtract its density. The reflected non-local priors\nforces the Bayesian detector to approach 0 at the non-defect locations. We\nconduct experiments studies to demonstrate the superior performance of the\nBayesian detector in eliminating the non-defect points. We implement the\nBayesian detector in the motion blurred drone images, in which the detector\nsuccessfully identifies the hail damages on the rough surface and substantially\nenhances the accuracy of the entire defect detection pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 12:36:36 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jiang", "Fei", ""], ["Yin", "Guosheng", ""]]}, {"id": "1809.01015", "submitter": "Nicolo' Savioli", "authors": "Nicol\\'o Savioli, Miguel Silva Vieira, Pablo Lamata, Giovanni Montana", "title": "Automated segmentation on the entire cardiac cycle using a deep learning\n  work-flow", "comments": "6 pages, 2 figures, published on IEEE Xplore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The segmentation of the left ventricle (LV) from CINE MRI images is essential\nto infer important clinical parameters. Typically, machine learning algorithms\nfor automated LV segmentation use annotated contours from only two cardiac\nphases, diastole, and systole. In this work, we present an analysis work-flow\nfor fully-automated LV segmentation that learns from images acquired through\nthe cardiac cycle. The workflow consists of three components: first, for each\nimage in the sequence, we perform an automated localization and subsequent\ncropping of the bounding box containing the cardiac silhouette. Second, we\nidentify the LV contours using a Temporal Fully Convolutional Neural Network\n(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a\nrecurrent mechanism enforcing temporal coherence across consecutive frames.\nFinally, we further defined the boundaries using either one of two components:\nfully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials\nand Semantic Flow. Our initial experiments suggest that significant improvement\nin performance can potentially be achieved by using a recurrent neural network\ncomponent that explicitly learns cardiac motion patterns whilst performing LV\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:07:31 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Savioli", "Nicol\u00f3", ""], ["Vieira", "Miguel Silva", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1809.01018", "submitter": "Chen Chao", "authors": "Chao Chen and Boyuan Jiang and Xinyu Jin", "title": "Parameter Transfer Extreme Learning Machine based on Projective Model", "comments": "This paper was accepted as an oral paper by IJCNN 2018", "journal-ref": "2018 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN.2018.8489244", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, transfer learning has attracted much attention in the community\nof machine learning. In this paper, we mainly focus on the tasks of parameter\ntransfer under the framework of extreme learning machine (ELM). Unlike the\nexisting parameter transfer approaches, which incorporate the source model\ninformation into the target by regularizing the di erence between the source\nand target domain parameters, an intuitively appealing projective-model is\nproposed to bridge the source and target model parameters. Specifically, we\nformulate the parameter transfer in the ELM networks by the means of parameter\nprojection, and train the model by optimizing the projection matrix and\nclassifier parameters jointly. Further more, the `L2,1-norm structured sparsity\npenalty is imposed on the source domain parameters, which encourages the joint\nfeature selection and parameter transfer. To evaluate the e ectiveness of the\nproposed method, comprehensive experiments on several commonly used domain\nadaptation datasets are presented. The results show that the proposed method\nsignificantly outperforms the non-transfer ELM networks and other classical\ntransfer learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:24:19 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 14:40:22 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Chen", "Chao", ""], ["Jiang", "Boyuan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1809.01022", "submitter": "Yuan He", "authors": "Yuan He and Ming Jiang and Chunming Zhao", "title": "A Neural Network Aided Approach for LDPC Coded DCO-OFDM with Clipping\n  Distortion", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural network-aided bit-interleaved coded modulation\n(NN-BICM) receiver is designed to mitigate the nonlinear clipping distortion in\nthe LDPC coded direct currentbiased optical orthogonal frequency division\nmultiplexing (DCOOFDM) systems. Taking the cross-entropy as loss function, a\nfeed forward network is trained by backpropagation algorithm to output the\ncondition probability through the softmax activation function, thereby\nassisting in a modified log-likelihood ratio (LLR) improvement. To reduce the\ncomplexity, this feed-forward network simplifies the input layer with a single\nsymbol and the corresponding Gaussian variance instead of focusing on the\ninter-carrier interference between multiple subcarriers. On the basis of the\nneural network-aided BICM with Gray labelling, we propose a novel stacked\nnetwork architecture of the bitinterleaved coded modulation with iterative\ndecoding (NN-BICMID). Its performance has been improved further by calculating\nthe condition probability with the aid of a priori probability that derived\nfrom the extrinsic LLRs in the LDPC decoder at the last iteration, at the\nexpense of customizing neural network detectors at each iteration time\nseparately. Utilizing the optimal DC bias as the midpoint of the dynamic\nregion, the simulation results demonstrate that both the NN-BICM and NN-BICM-ID\nschemes achieve noticeable performance gains than other counterparts, in which\nthe NN-BICM-ID clearly outperforms NN-BICM with various modulation and coding\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:31:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["He", "Yuan", ""], ["Jiang", "Ming", ""], ["Zhao", "Chunming", ""]]}, {"id": "1809.01046", "submitter": "Bingjing Tang", "authors": "Aditi Iyer, Bingjing Tang, Vinayak Rao, Nan Kong", "title": "Group-Representative Functional Network Estimation from Multi-Subject\n  fMRI Data via MRF-based Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel two-phase approach to functional network estimation of\nmulti-subject functional Magnetic Resonance Imaging (fMRI) data, which applies\nmodel-based image segmentation to determine a group-representative connectivity\nmap. In our approach, we first improve clustering-based Independent Component\nAnalysis (ICA) to generate maps of components occurring consistently across\nsubjects, and then estimate the group-representative map through MAP-MRF\n(Maximum a priori - Markov random field) labeling. For the latter, we provide a\nnovel and efficient variational Bayes algorithm. We study the performance of\nthe proposed method using synthesized data following a theoretical model, and\ndemonstrate its viability in blind extraction of group-representative\nfunctional networks using simulated fMRI data. We anticipate the proposed\nmethod will be applied in identifying common neuronal characteristics in a\npopulation, and could be further extended to real-world clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:32:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Iyer", "Aditi", ""], ["Tang", "Bingjing", ""], ["Rao", "Vinayak", ""], ["Kong", "Nan", ""]]}, {"id": "1809.01079", "submitter": "Yuan Wu", "authors": "Yuan Wu, Lingling Li and Lian Li", "title": "Chi-Square Test Neural Network: A New Binary Classifier based on\n  Backpropagation Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the chi-square test neural network: a single hidden layer\nbackpropagation neural network using chi-square test theorem to redefine the\ncost function and the error function. The weights and thresholds are modified\nusing standard backpropagation algorithm. The proposed approach has the\nadvantage of making consistent data distribution over training and testing\nsets. It can be used for binary classification. The experimental results on\nreal world data sets indicate that the proposed algorithm can significantly\nimprove the classification accuracy comparing to related approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 16:38:01 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wu", "Yuan", ""], ["Li", "Lingling", ""], ["Li", "Lian", ""]]}, {"id": "1809.01090", "submitter": "Lu Bai", "authors": "Lu Bai, Yuhang Jiao, Luca Rossi, Lixin Cui, Jian Cheng, Edwin R.\n  Hancock", "title": "Graph Convolutional Neural Networks based on Quantum Vertex Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Quantum Spatial Graph Convolutional Neural Network\n(QSGCNN) model that can directly learn a classification function for graphs of\narbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural Network\n(GCNN) models, the proposed QSGCNN model incorporates the process of\nidentifying transitive aligned vertices between graphs, and transforms\narbitrary sized graphs into fixed-sized aligned vertex grid structures. In\norder to learn representative graph characteristics, a new quantum spatial\ngraph convolution is proposed and employed to extract multi-scale vertex\nfeatures, in terms of quantum information propagation between grid vertices of\neach graph. Since the quantum spatial convolution preserves the grid structures\nof the input vertices (i.e., the convolution layer does not change the original\nspatial sequence of vertices), the proposed QSGCNN model allows to directly\nemploy the traditional convolutional neural network architecture to further\nlearn from the global graph topology, providing an end-to-end deep learning\narchitecture that integrates the graph representation and learning in the\nquantum spatial graph convolution layer and the traditional convolutional layer\nfor graph classifications. We demonstrate the effectiveness of the proposed\nQSGCNN model in relation to existing state-of-the-art methods. The proposed\nQSGCNN model addresses the shortcomings of information loss and imprecise\ninformation representation arising in existing GCN models associated with the\nuse of SortPooling or SumPooling layers. Experiments on benchmark graph\nclassification datasets demonstrate the effectiveness of the proposed QSGCNN\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 16:53:04 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:52:53 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Bai", "Lu", ""], ["Jiao", "Yuhang", ""], ["Rossi", "Luca", ""], ["Cui", "Lixin", ""], ["Cheng", "Jian", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1809.01093", "submitter": "Aleksandar Bojchevski", "authors": "Aleksandar Bojchevski, Stephan G\\\"unnemann", "title": "Adversarial Attacks on Node Embeddings via Graph Poisoning", "comments": "ICML 2019, PMLR 97:695-704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of network representation learning is to learn low-dimensional node\nembeddings that capture the graph structure and are useful for solving\ndownstream tasks. However, despite the proliferation of such methods, there is\ncurrently no study of their robustness to adversarial attacks. We provide the\nfirst adversarial vulnerability analysis on the widely used family of methods\nbased on random walks. We derive efficient adversarial perturbations that\npoison the network structure and have a negative effect on both the quality of\nthe embeddings and the downstream tasks. We further show that our attacks are\ntransferable since they generalize to many models and are successful even when\nthe attacker is restricted.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 16:59:53 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 01:51:36 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 16:16:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Bojchevski", "Aleksandar", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1809.01129", "submitter": "Zac Cranko", "authors": "Zac Cranko, Simon Kornblith, Zhan Shi, Richard Nock", "title": "Lipschitz Networks and Distributional Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust risk minimisation has several advantages: it has been studied with\nregards to improving the generalisation properties of models and robustness to\nadversarial perturbation. We bound the distributionally robust risk for a model\nclass rich enough to include deep neural networks by a regularised empirical\nrisk involving the Lipschitz constant of the model. This allows us to\ninterpretand quantify the robustness properties of a deep neural network. As an\napplication we show the distributionally robust risk upperbounds the\nadversarial training risk.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 03:12:40 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Cranko", "Zac", ""], ["Kornblith", "Simon", ""], ["Shi", "Zhan", ""], ["Nock", "Richard", ""]]}, {"id": "1809.01133", "submitter": "Timos Papadopoulos", "authors": "Timos Papadopoulos, Stephen J. Roberts and Katherine J. Willis", "title": "Automated bird sound recognition in realistic settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CY cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the effectiveness of an automated bird sound identification\nsystem in a situation that emulates a realistic, typical application. We\ntrained classification algorithms on a crowd-sourced collection of bird audio\nrecording data and restricted our training methods to be completely free of\nmanual intervention. The approach is hence directly applicable to the analysis\nof multiple species collections, with labelling provided by crowd-sourced\ncollection. We evaluated the performance of the bird sound recognition system\non a realistic number of candidate classes, corresponding to real conditions.\nWe investigated the use of two canonical classification methods, chosen due to\ntheir widespread use and ease of interpretation, namely a k Nearest Neighbour\n(kNN) classifier with histogram-based features and a Support Vector Machine\n(SVM) with time-summarisation features. We further investigated the use of a\ncertainty measure, derived from the output probabilities of the classifiers, to\nenhance the interpretability and reliability of the class decisions. Our\nresults demonstrate that both identification methods achieved similar\nperformance, but we argue that the use of the kNN classifier offers somewhat\nmore flexibility. Furthermore, we show that employing an outcome certainty\nmeasure provides a valuable and consistent indicator of the reliability of\nclassification results. Our use of generic training data and our investigation\nof probabilistic classification methodologies that can flexibly address the\nvariable number of candidate species/classes that are expected to be\nencountered in the field, directly contribute to the development of a practical\nbird sound identification system with potentially global application. Further,\nwe show that certainty measures associated with identification outcomes can\nsignificantly contribute to the practical usability of the overall system.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 10:26:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Papadopoulos", "Timos", ""], ["Roberts", "Stephen J.", ""], ["Willis", "Katherine J.", ""]]}, {"id": "1809.01185", "submitter": "Yang Lu", "authors": "Yang Young Lu, Yingying Fan, Jinchi Lv, William Stafford Noble", "title": "DeepPINK: reproducible feature selection in deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become increasingly popular in both supervised and\nunsupervised machine learning thanks to its outstanding empirical performance.\nHowever, because of their intrinsic complexity, most deep learning methods are\nlargely treated as black box tools with little interpretability. Even though\nrecent attempts have been made to facilitate the interpretability of deep\nneural networks (DNNs), existing methods are susceptible to noise and lack of\nrobustness.\n  Therefore, scientists are justifiably cautious about the reproducibility of\nthe discoveries, which is often related to the interpretability of the\nunderlying statistical models. In this paper, we describe a method to increase\nthe interpretability and reproducibility of DNNs by incorporating the idea of\nfeature selection with controlled error rate. By designing a new DNN\narchitecture and integrating it with the recently proposed knockoffs framework,\nwe perform feature selection with a controlled error rate, while maintaining\nhigh power. This new method, DeepPINK (Deep feature selection using\nPaired-Input Nonlinear Knockoffs), is applied to both simulated and real data\nsets to demonstrate its empirical utility.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 18:24:37 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 04:17:27 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Lu", "Yang Young", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""], ["Noble", "William Stafford", ""]]}, {"id": "1809.01225", "submitter": "Tsung-Yu Hsieh", "authors": "Tsung-Yu Hsieh, Yasser EL-Manzalawy, Yiwei Sun, Vasant Honavar", "title": "Compositional Stochastic Average Gradient for Machine Learning and\n  Related Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning, statistical inference, and portfolio optimization\nproblems require minimization of a composition of expected value functions\n(CEVF). Of particular interest is the finite-sum versions of such compositional\noptimization problems (FS-CEVF). Compositional stochastic variance reduced\ngradient (C-SVRG) methods that combine stochastic compositional gradient\ndescent (SCGD) and stochastic variance reduced gradient descent (SVRG) methods\nare the state-of-the-art methods for FS-CEVF problems. We introduce\ncompositional stochastic average gradient descent (C-SAG) a novel extension of\nthe stochastic average gradient method (SAG) to minimize composition of\nfinite-sum functions. C-SAG, like SAG, estimates gradient by incorporating\nmemory of previous gradient information. We present theoretical analyses of\nC-SAG which show that C-SAG, like SAG, and C-SVRG, achieves a linear\nconvergence rate when the objective function is strongly convex; However, C-CAG\nachieves lower oracle query complexity per iteration than C-SVRG. Finally, we\npresent results of experiments showing that C-SAG converges substantially\nfaster than full gradient (FG), as well as C-SVRG.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 19:58:06 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:57:46 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Hsieh", "Tsung-Yu", ""], ["EL-Manzalawy", "Yasser", ""], ["Sun", "Yiwei", ""], ["Honavar", "Vasant", ""]]}, {"id": "1809.01229", "submitter": "Sotirios Chatzis", "authors": "Kyriakos Tolias and Sotirios Chatzis", "title": "t-Exponential Memory Networks for Question-Answering Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in deep learning have brought to the fore models that can\nmake multiple computational steps in the service of completing a task; these\nare capable of describ- ing long-term dependencies in sequential data. Novel\nrecurrent attention models over possibly large external memory modules\nconstitute the core mechanisms that enable these capabilities. Our work\naddresses learning subtler and more complex underlying temporal dynamics in\nlanguage modeling tasks that deal with sparse sequential data. To this end, we\nimprove upon these recent advances, by adopting concepts from the field of\nBayesian statistics, namely variational inference. Our proposed approach\nconsists in treating the network parameters as latent variables with a prior\ndistribution imposed over them. Our statistical assumptions go beyond the\nstandard practice of postulating Gaussian priors. Indeed, to allow for handling\noutliers, which are prevalent in long observed sequences of multivariate data,\nmultivariate t-exponential distributions are imposed. On this basis, we proceed\nto infer corresponding posteriors; these can be used for inference and\nprediction at test time, in a way that accounts for the uncertainty in the\navailable sparse training data. Specifically, to allow for our approach to best\nexploit the merits of the t-exponential family, our method considers a new\nt-divergence measure, which generalizes the concept of the Kullback-Leibler\ndivergence. We perform an extensive experimental evaluation of our approach,\nusing challenging language modeling benchmarks, and illustrate its superiority\nover existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 20:09:01 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Tolias", "Kyriakos", ""], ["Chatzis", "Sotirios", ""]]}, {"id": "1809.01293", "submitter": "Changyou Chen", "authors": "Jianyi Zhang and Ruiyi Zhang and Lawrence Carin and Changyou Chen", "title": "Stochastic Particle-Optimization Sampling and the Non-Asymptotic\n  Convergence Theory", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle-optimization-based sampling (POS) is a recently developed effective\nsampling technique that interactively updates a set of particles. A\nrepresentative algorithm is the Stein variational gradient descent (SVGD). We\nprove, under certain conditions, SVGD experiences a theoretical pitfall, {\\it\ni.e.}, particles tend to collapse. As a remedy, we generalize POS to a\nstochastic setting by injecting random noise into particle updates, thus\nyielding particle-optimization sampling (SPOS). Notably, for the first time, we\ndevelop {\\em non-asymptotic convergence theory} for the SPOS framework (related\nto SVGD), characterizing algorithm convergence in terms of the 1-Wasserstein\ndistance w.r.t.\\! the numbers of particles and iterations. Somewhat\nsurprisingly, with the same number of updates (not too large) for each\nparticle, our theory suggests adopting more particles does not necessarily lead\nto a better approximation of a target distribution, due to limited\ncomputational budget and numerical errors. This phenomenon is also observed in\nSVGD and verified via an experiment on synthetic data. Extensive experimental\nresults verify our theory and demonstrate the effectiveness of our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 01:55:28 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:53:58 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 19:45:25 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 02:45:30 GMT"}, {"version": "v5", "created": "Sun, 29 Mar 2020 07:44:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhang", "Jianyi", ""], ["Zhang", "Ruiyi", ""], ["Carin", "Lawrence", ""], ["Chen", "Changyou", ""]]}, {"id": "1809.01316", "submitter": "Donghyeon Kim", "authors": "Donghyeon Kim, Jinhyuk Lee, Donghee Choi, Jaehoon Choi, Jaewoo Kang", "title": "Learning User Preferences and Understanding Calendar Contexts for Event\n  Scheduling", "comments": "CIKM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With online calendar services gaining popularity worldwide, calendar data has\nbecome one of the richest context sources for understanding human behavior.\nHowever, event scheduling is still time-consuming even with the development of\nonline calendars. Although machine learning based event scheduling models have\nautomated scheduling processes to some extent, they often fail to understand\nsubtle user preferences and complex calendar contexts with event titles written\nin natural language. In this paper, we propose Neural Event Scheduling\nAssistant (NESA) which learns user preferences and understands calendar\ncontexts, directly from raw online calendars for fully automated and highly\neffective event scheduling. We leverage over 593K calendar events for NESA to\nlearn scheduling personal events, and we further utilize NESA for\nmulti-attendee event scheduling. NESA successfully incorporates deep neural\nnetworks such as Bidirectional Long Short-Term Memory, Convolutional Neural\nNetwork, and Highway Network for learning the preferences of each user and\nunderstanding calendar context based on natural languages. The experimental\nresults show that NESA significantly outperforms previous baseline models in\nterms of various evaluation metrics on both personal and multi-attendee event\nscheduling tasks. Our qualitative analysis demonstrates the effectiveness of\neach layer in NESA and learned user preferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:15:13 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 15:52:06 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 10:58:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Donghyeon", ""], ["Lee", "Jinhyuk", ""], ["Choi", "Donghee", ""], ["Choi", "Jaehoon", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1809.01319", "submitter": "Ingrid Baade", "authors": "Ingrid Annette Baade", "title": "Cross validation residuals for generalised least squares and other\n  correlated data models", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation residuals are well known for the ordinary least squares\nmodel. Here leave-M-out cross validation is extended to generalised least\nsquares. The relationship between cross validation residuals and Cook's\ndistance is demonstrated, in terms of an approximation to the difference in the\ngeneralised residual sum of squares for a model fit to all the data (training\nand test) and a model fit to a reduced dataset (training data only). For\ngeneralised least squares, as for ordinary least squares, there is no need to\nrefit the model to reduced size datasets as all the values for K fold cross\nvalidation are available after fitting the model to all the data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:44:55 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Baade", "Ingrid Annette", ""]]}, {"id": "1809.01341", "submitter": "Pouya Pezeshkpour", "authors": "Pouya Pezeshkpour, Liyan Chen and Sameer Singh", "title": "Embedding Multimodal Relational Data for Knowledge Base Completion", "comments": "Published at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing entities and relations in an embedding space is a well-studied\napproach for machine learning on relational data. Existing approaches, however,\nprimarily focus on simple link structure between a finite set of entities,\nignoring the variety of data types that are often used in knowledge bases, such\nas text, images, and numerical values. In this paper, we propose multimodal\nknowledge base embeddings (MKBE) that use different neural encoders for this\nvariety of observed data, and combine them with existing relational models to\nlearn embeddings of the entities and multimodal data. Further, using these\nlearned embedings and different neural decoders, we introduce a novel\nmultimodal imputation model to generate missing multimodal values, like text\nand images, from information in the knowledge base. We enrich existing\nrelational datasets to create two novel benchmarks that contain additional\ninformation such as textual descriptions and images of the original entities.\nWe demonstrate that our models utilize this additional information effectively\nto provide more accurate link prediction, achieving state-of-the-art results\nwith a considerable gap of 5-7% over existing methods. Further, we evaluate the\nquality of our generated multimodal values via a user study. We have release\nthe datasets and the open-source implementation of our models at\nhttps://github.com/pouyapez/mkbe\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:07:31 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 18:13:10 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Pezeshkpour", "Pouya", ""], ["Chen", "Liyan", ""], ["Singh", "Sameer", ""]]}, {"id": "1809.01353", "submitter": "Matteo Ronchetti", "authors": "Matteo Ronchetti", "title": "IKA: Independent Kernel Approximator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new method for low rank kernel approximation called\nIKA. The main advantage of IKA is that it produces a function $\\psi(x)$ defined\nas a linear combination of arbitrarily chosen functions. In contrast the\napproximation produced by Nystr\\\"om method is a linear combination of kernel\nevaluations. The proposed method consistently outperformed Nystr\\\"om method in\na comparison on the STL-10 dataset. Numerical results are reproducible using\nthe source code available at https://gitlab.com/matteo-ronchetti/IKA\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:49:12 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Ronchetti", "Matteo", ""]]}, {"id": "1809.01354", "submitter": "Quan Chen", "authors": "Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai", "title": "Semantic Human Matting", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human matting, high quality extraction of humans from natural images, is\ncrucial for a wide variety of applications. Since the matting problem is\nseverely under-constrained, most previous methods require user interactions to\ntake user designated trimaps or scribbles as constraints. This user-in-the-loop\nnature makes them difficult to be applied to large scale data or time-sensitive\nscenarios. In this paper, instead of using explicit user input constraints, we\nemploy implicit semantic constraints learned from data and propose an automatic\nhuman matting algorithm (SHM). SHM is the first algorithm that learns to\njointly fit both semantic information and high quality details with deep\nnetworks. In practice, simultaneously learning both coarse semantics and fine\ndetails is challenging. We propose a novel fusion strategy which naturally\ngives a probabilistic estimation of the alpha matte. We also construct a very\nlarge dataset with high quality annotations consisting of 35,513 unique\nforegrounds to facilitate the learning and evaluation of human matting.\nExtensive experiments on this dataset and plenty of real images show that SHM\nachieves comparable results with state-of-the-art interactive matting methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:50:24 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:36:31 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Quan", ""], ["Ge", "Tiezheng", ""], ["Xu", "Yanyu", ""], ["Zhang", "Zhiqiang", ""], ["Yang", "Xinxin", ""], ["Gai", "Kun", ""]]}, {"id": "1809.01357", "submitter": "Mike Wu", "authors": "Mike Wu, Milan Mosse, Noah Goodman, Chris Piech", "title": "Zero Shot Learning for Code Education: Rubric Sampling with Deep\n  Learning Inference", "comments": "To appear at AAAI 2019; 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern computer science education, massive open online courses (MOOCs) log\nthousands of hours of data about how students solve coding challenges. Being so\nrich in data, these platforms have garnered the interest of the machine\nlearning community, with many new algorithms attempting to autonomously provide\nfeedback to help future students learn. But what about those first hundred\nthousand students? In most educational contexts (i.e. classrooms), assignments\ndo not have enough historical data for supervised learning. In this paper, we\nintroduce a human-in-the-loop \"rubric sampling\" approach to tackle the \"zero\nshot\" feedback challenge. We are able to provide autonomous feedback for the\nfirst students working on an introductory programming assignment with accuracy\nthat substantially outperforms data-hungry algorithms and approaches human\nlevel fidelity. Rubric sampling requires minimal teacher effort, can associate\nfeedback with specific parts of a student's solution and can articulate a\nstudent's misconceptions in the language of the instructor. Deep learning\ninference enables rubric sampling to further improve as more assignment\nspecific student data is acquired. We demonstrate our results on a novel\ndataset from Code.org, the world's largest programming education platform.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:13:30 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 04:23:42 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Wu", "Mike", ""], ["Mosse", "Milan", ""], ["Goodman", "Noah", ""], ["Piech", "Chris", ""]]}, {"id": "1809.01369", "submitter": "Vahid Mostofi", "authors": "Vahid Mostofi, Sadegh Aliakbary", "title": "Towards quantitative methods to assess network generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing generative models is not an easy task. Generative models should\nsynthesize graphs which are not replicates of real networks but show\ntopological features similar to real graphs. We introduce an approach for\nassessing graph generative models using graph classifiers. The inability of an\nestablished graph classifier for distinguishing real and synthesized graphs\ncould be considered as a performance measurement for graph generators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:56:11 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Mostofi", "Vahid", ""], ["Aliakbary", "Sadegh", ""]]}, {"id": "1809.01382", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Jaouad Mourtada, St\\'ephane Ga\\\"iffas", "title": "On the optimality of the Hedge algorithm in the stochastic regime", "comments": null, "journal-ref": "Journal of Machine Learning Research, 20(83), 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the behavior of the Hedge algorithm in the online\nstochastic setting. We prove that anytime Hedge with decreasing learning rate,\nwhich is one of the simplest algorithm for the problem of prediction with\nexpert advice, is surprisingly both worst-case optimal and adaptive to the\neasier stochastic and adversarial with a gap problems. This shows that, in\nspite of its small, non-adaptive learning rate, Hedge possesses the same\noptimal regret guarantee in the stochastic case as recently introduced adaptive\nalgorithms. Moreover, our analysis exhibits qualitative differences with other\nvariants of the Hedge algorithm, such as the fixed-horizon version (with\nconstant learning rate) and the one based on the so-called \"doubling trick\",\nboth of which fail to adapt to the easier stochastic setting. Finally, we\ndiscuss the limitations of anytime Hedge and the improvements provided by\nsecond-order regret bounds in the stochastic case.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 08:32:01 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 09:34:03 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 21:38:24 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""]]}, {"id": "1809.01434", "submitter": "Arnab Karmakar", "authors": "Arnab Karmakar, Deepak Mishra, Anandmayee Tej", "title": "Stellar Cluster Detection using GMM with Deep Variational Autoencoder", "comments": "5 pages, 7 figures, under review in IEEE RAICS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.GA astro-ph.SR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting stellar clusters have always been an important research problem in\nAstronomy. Although images do not convey very detailed information in detecting\nstellar density enhancements, we attempt to understand if new machine learning\ntechniques can reveal patterns that would assist in drawing better inferences\nfrom the available image data. This paper describes an unsupervised approach in\ndetecting star clusters using Deep Variational Autoencoder combined with a\nGaussian Mixture Model. We show that our method works significantly well in\ncomparison with state-of-the-art detection algorithm in recognizing a variety\nof star clusters even in the presence of noise and distortion.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:06:06 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Karmakar", "Arnab", ""], ["Mishra", "Deepak", ""], ["Tej", "Anandmayee", ""]]}, {"id": "1809.01465", "submitter": "Simon Jenni", "authors": "Simon Jenni, Paolo Favaro", "title": "Deep Bilevel Learning", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel regularization approach to train neural networks that\nenjoys better generalization and test error than standard stochastic gradient\ndescent. Our approach is based on the principles of cross-validation, where a\nvalidation set is used to limit the model overfitting. We formulate such\nprinciples as a bilevel optimization problem. This formulation allows us to\ndefine the optimization of a cost on the validation set subject to another\noptimization on the training set. The overfitting is controlled by introducing\nweights on each mini-batch in the training set and by choosing their values so\nthat they minimize the error on the validation set. In practice, these weights\ndefine mini-batch learning rates in a gradient descent update equation that\nfavor gradients with better generalization capabilities. Because of its\nsimplicity, this approach can be integrated with other regularization methods\nand training schemes. We evaluate extensively our proposed algorithm on several\nneural network architectures and datasets, and find that it consistently\nimproves the generalization of the model, especially when labels are noisy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 12:50:24 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "1809.01471", "submitter": "Ecem Sogancioglu", "authors": "Ecem Sogancioglu, Shi Hu, Davide Belli, Bram van Ginneken", "title": "Chest X-ray Inpainting with Deep Generative Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks have been successfully applied to inpainting\nin natural images. However, the current state-of-the-art models have not yet\nbeen widely adopted in the medical imaging domain. In this paper, we\ninvestigate the performance of three recently published deep learning based\ninpainting models: context encoders, semantic image inpainting, and the\ncontextual attention model, applied to chest x-rays, as the chest exam is the\nmost commonly performed radiological procedure. We train these generative\nmodels on 1.2M 128 $\\times$ 128 patches from 60K healthy x-rays, and learn to\npredict the center 64 $\\times$ 64 region in each patch. We test the models on\nboth the healthy and abnormal radiographs. We evaluate the results by visual\ninspection and comparing the PSNR scores. The outputs of the models are in most\ncases highly realistic. We show that the methods have potential to enhance and\ndetect abnormalities. In addition, we perform a 2AFC observer study and show\nthat an experienced human observer performs poorly in detecting inpainted\nregions, particularly those generated by the contextual attention model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 09:21:22 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Sogancioglu", "Ecem", ""], ["Hu", "Shi", ""], ["Belli", "Davide", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1809.01477", "submitter": "Sahib Singh Budhiraja", "authors": "Sahib Singh Budhiraja, Vijay Mago", "title": "A Supervised Learning Approach For Heading Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As the Portable Document Format (PDF) file format increases in popularity,\nresearch in analysing its structure for text extraction and analysis is\nnecessary. Detecting headings can be a crucial component of classifying and\nextracting meaningful data. This research involves training a supervised\nlearning model to detect headings with features carefully selected through\nrecursive feature elimination. The best performing classifier had an accuracy\nof 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into\nheading detection contributes to the field of PDF based text extraction and can\nbe applied to the automation of large scale PDF text analysis in a variety of\nprofessional and policy based contexts.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 19:31:05 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Budhiraja", "Sahib Singh", ""], ["Mago", "Vijay", ""]]}, {"id": "1809.01478", "submitter": "Yu Meng", "authors": "Yu Meng, Jiaming Shen, Chao Zhang, Jiawei Han", "title": "Weakly-Supervised Neural Text Classification", "comments": "CIKM 2018 Full Paper", "journal-ref": null, "doi": "10.1145/3269206.3271737", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are gaining increasing popularity for the classic text\nclassification task, due to their strong expressive power and less requirement\nfor feature engineering. Despite such attractiveness, neural text\nclassification models suffer from the lack of training data in many real-world\napplications. Although many semi-supervised and weakly-supervised text\nclassification models exist, they cannot be easily applied to deep neural\nmodels and meanwhile support limited supervision types. In this paper, we\npropose a weakly-supervised method that addresses the lack of training data in\nneural text classification. Our method consists of two modules: (1) a\npseudo-document generator that leverages seed information to generate\npseudo-labeled documents for model pre-training, and (2) a self-training module\nthat bootstraps on real unlabeled data for model refinement. Our method has the\nflexibility to handle different types of weak supervision and can be easily\nintegrated into existing deep neural models for text classification. We have\nperformed extensive experiments on three real-world datasets from different\ndomains. The results demonstrate that our proposed method achieves inspiring\nperformance without requiring excessive training data and outperforms baseline\nmethods significantly.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 02:56:25 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 04:34:59 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Meng", "Yu", ""], ["Shen", "Jiaming", ""], ["Zhang", "Chao", ""], ["Han", "Jiawei", ""]]}, {"id": "1809.01485", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai and Santiago Segarra and Asuman E. Ozdaglar and Anna\n  Scaglione and Ali Jadbabaie", "title": "Blind Community Detection from Low-rank Excitations of a Graph Filter", "comments": "Single column format, 32 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers a new framework to detect communities in a graph from\nthe observation of signals at its nodes. We model the observed signals as noisy\noutputs of an unknown network process, represented as a graph filter that is\nexcited by a set of unknown low-rank inputs/excitations. Application scenarios\nof this model include diffusion dynamics, pricing experiments, and opinion\ndynamics. Rather than learning the precise parameters of the graph itself, we\naim at retrieving the community structure directly. The paper shows that\ncommunities can be detected by applying a spectral method to the covariance\nmatrix of graph signals. Our analysis indicates that the community detection\nperformance depends on a `low-pass' property of the graph filter. We also show\nthat the performance can be improved via a low-rank matrix plus sparse\ndecomposition method when the latent parameter vectors are known. Numerical\nexperiments demonstrate that our approach is effective.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 13:24:35 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 03:11:35 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wai", "Hoi-To", ""], ["Segarra", "Santiago", ""], ["Ozdaglar", "Asuman E.", ""], ["Scaglione", "Anna", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1809.01494", "submitter": "Marzieh Saeidi", "authors": "Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim\n  Rockt\\\"aschel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel", "title": "Interpretation of Natural Language Rules in Conversational Machine\n  Reading", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work in machine reading focuses on question answering problems where the\nanswer is directly expressed in the text to read. However, many real-world\nquestion answering problems require the reading of text not because it contains\nthe literal answer, but because it contains a recipe to derive an answer\ntogether with the reader's background knowledge. One example is the task of\ninterpreting regulations to answer \"Can I...?\" or \"Do I have to...?\" questions\nsuch as \"I am working in Canada. Do I have to carry on paying UK National\nInsurance?\" after reading a UK government website about this topic. This task\nrequires both the interpretation of rules and the application of background\nknowledge. It is further complicated due to the fact that, in practice, most\nquestions are underspecified, and a human assistant will regularly have to ask\nclarification questions such as \"How long have you been working abroad?\" when\nthe answer cannot be directly derived from the question and text. In this\npaper, we formalise this task and develop a crowd-sourcing strategy to collect\n32k task instances based on real-world rules and crowd-generated questions and\nscenarios. We analyse the challenges of this task and assess its difficulty by\nevaluating the performance of rule-based and machine-learning baselines. We\nobserve promising results when no background knowledge is necessary, and\nsubstantial room for improvement whenever background knowledge is needed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 19:44:51 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Saeidi", "Marzieh", ""], ["Bartolo", "Max", ""], ["Lewis", "Patrick", ""], ["Singh", "Sameer", ""], ["Rockt\u00e4schel", "Tim", ""], ["Sheldon", "Mike", ""], ["Bouchard", "Guillaume", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1809.01495", "submitter": "Laure Soulier", "authors": "Wafa Aissa, Laure Soulier, Ludovic Denoyer", "title": "A Reinforcement Learning-driven Translation Model for Search-Oriented\n  Conversational Systems", "comments": "This is the author's pre-print version of the work. It is posted here\n  for your personal use, not for redistribution. Please cite the definitive\n  version which will be published in Proceedings of the 2018 EMNLP Workshop\n  SCAI: The 2nd International Workshop on Search-Oriented Conversational AI -\n  ISBN: 978-1-948087-75-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search-oriented conversational systems rely on information needs expressed in\nnatural language (NL). We focus here on the understanding of NL expressions for\nbuilding keyword-based queries. We propose a reinforcement-learning-driven\ntranslation model framework able to 1) learn the translation from NL\nexpressions to queries in a supervised way, and, 2) to overcome the lack of\nlarge-scale dataset by framing the translation model as a word selection\napproach and injecting relevance feedback in the learning process. Experiments\nare carried out on two TREC datasets and outline the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:11:49 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Aissa", "Wafa", ""], ["Soulier", "Laure", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1809.01496", "submitter": "Jieyu Zhao", "authors": "Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang and Kai-Wei Chang", "title": "Learning Gender-Neutral Word Embeddings", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models have become a fundamental component in a wide range of\nNatural Language Processing (NLP) applications. However, embeddings trained on\nhuman-generated corpora have been demonstrated to inherit strong gender\nstereotypes that reflect social constructs. To address this concern, in this\npaper, we propose a novel training procedure for learning gender-neutral word\nembeddings. Our approach aims to preserve gender information in certain\ndimensions of word vectors while compelling other dimensions to be free of\ngender influence. Based on the proposed method, we generate a Gender-Neutral\nvariant of GloVe (GN-GloVe). Quantitative and qualitative experiments\ndemonstrate that GN-GloVe successfully isolates gender information without\nsacrificing the functionality of the embedding model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 21:11:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Zhao", "Jieyu", ""], ["Zhou", "Yichao", ""], ["Li", "Zeyu", ""], ["Wang", "Wei", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1809.01498", "submitter": "Benjamin Wilson", "authors": "Matthias Leimeister, Benjamin J. Wilson", "title": "Skip-gram word embeddings in hyperbolic space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that embeddings of tree-like graphs in\nhyperbolic space surpass their Euclidean counterparts in performance by a large\nmargin. Inspired by these results and scale-free structure in the word\nco-occurrence graph, we present an algorithm for learning word embeddings in\nhyperbolic space from free text. An objective function based on the hyperbolic\ndistance is derived and included in the skip-gram negative-sampling\narchitecture of word2vec. The hyperbolic word embeddings are then evaluated on\nword similarity and analogy benchmarks. The results demonstrate the potential\nof hyperbolic word embeddings, particularly in low dimensions, though without\nclear superiority over their Euclidean counterparts. We further discuss\nsubtleties in the formulation of the analogy task in curved spaces.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 13:54:45 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 12:36:58 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Leimeister", "Matthias", ""], ["Wilson", "Benjamin J.", ""]]}, {"id": "1809.01499", "submitter": "Samuel Carton", "authors": "Samuel Carton, Qiaozhu Mei, Paul Resnick", "title": "Extractive Adversarial Networks: High-Recall Explanations for\n  Identifying Personal Attacks in Social Media Posts", "comments": "Accepted to EMNLP 2018 Code and data available at\n  https://github.com/shcarton/rcnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adversarial method for producing high-recall explanations of\nneural text classifier decisions. Building on an existing architecture for\nextractive explanations via hard attention, we add an adversarial layer which\nscans the residual of the attention for remaining predictive signal. Motivated\nby the important domain of detecting personal attacks in social media comments,\nwe additionally demonstrate the importance of manually setting a semantically\nappropriate `default' behavior for the model by explicitly manipulating its\nbias term. We develop a validation set of human-annotated personal attacks to\nevaluate the impact of these changes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 00:15:30 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 20:59:09 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Carton", "Samuel", ""], ["Mei", "Qiaozhu", ""], ["Resnick", "Paul", ""]]}, {"id": "1809.01506", "submitter": "Prakhar Ganesh", "authors": "Prakhar Ganesh, Puneet Rakheja", "title": "VLSTM: Very Long Short-Term Memory Networks for High-Frequency Trading", "comments": "4 pages + 1 page references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.TR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Financial trading is at the forefront of time-series analysis, and has grown\nhand-in-hand with it. The advent of electronic trading has allowed complex\nmachine learning solutions to enter the field of financial trading. Financial\nmarkets have both long term and short term signals and thus a good predictive\nmodel in financial trading should be able to incorporate them together. One of\nthe most sought after forms of electronic trading is high-frequency trading\n(HFT), typically known for microsecond sensitive changes, which results in a\ntremendous amount of data. LSTMs are one of the most capable variants of the\nRNN family that can handle long-term dependencies, but even they are not\nequipped to handle such long sequences of the order of thousands of data points\nlike in HFT. We propose very-long short term memory networks, or VLSTMs, to\ndeal with such extreme length sequences. We explore the importance of VLSTMs in\nthe context of HFT. We compare our model on publicly available dataset and got\na 3.14\\% increase in F1-score over the existing state-of-the-art time-series\nforecasting models. We also show that our model has great parallelization\npotential, which is essential for practical purposes when trading on such\nmarkets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 13:44:12 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 09:02:39 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:15:45 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ganesh", "Prakhar", ""], ["Rakheja", "Puneet", ""]]}, {"id": "1809.01534", "submitter": "Daniel Watson", "authors": "Daniel Watson, Nasser Zalmout, Nizar Habash", "title": "Utilizing Character and Word Embeddings for Text Normalization with\n  Sequence-to-Sequence Models", "comments": "Accepted in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text normalization is an important enabling technology for several NLP tasks.\nRecently, neural-network-based approaches have outperformed well-established\nmodels in this task. However, in languages other than English, there has been\nlittle exploration in this direction. Both the scarcity of annotated data and\nthe complexity of the language increase the difficulty of the problem. To\naddress these challenges, we use a sequence-to-sequence model with\ncharacter-based attention, which in addition to its self-learned character\nembeddings, uses word embeddings pre-trained with an approach that also models\nsubword information. This provides the neural model with access to more\nlinguistic information especially suitable for text normalization, without\nlarge parallel corpora. We show that providing the model with word-level\nfeatures bridges the gap for the neural network approach to achieve a\nstate-of-the-art F1 score on a standard Arabic language correction shared task\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 16:44:04 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Watson", "Daniel", ""], ["Zalmout", "Nasser", ""], ["Habash", "Nizar", ""]]}, {"id": "1809.01560", "submitter": "Victor Gallego", "authors": "Victor Gallego, Roi Naveiro, David Rios Insua", "title": "Reinforcement Learning under Threats", "comments": "Extends the verson published at the Proceedings of the AAAI\n  Conference on Artificial Intelligence 33,\n  https://www.aaai.org/ojs/index.php/AAAI/article/view/5106", "journal-ref": null, "doi": "10.1609/aaai.v33i01.33019939", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several reinforcement learning (RL) scenarios, mainly in security\nsettings, there may be adversaries trying to interfere with the reward\ngenerating process. In this paper, we introduce Threatened Markov Decision\nProcesses (TMDPs), which provide a framework to support a decision maker\nagainst a potential adversary in RL. Furthermore, we propose a level-$k$\nthinking scheme resulting in a new learning framework to deal with TMDPs. After\nintroducing our framework and deriving theoretical results, relevant empirical\nevidence is given via extensive experiments, showing the benefits of accounting\nfor adversaries while the agent learns.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 14:56:09 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 12:15:05 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Gallego", "Victor", ""], ["Naveiro", "Roi", ""], ["Insua", "David Rios", ""]]}, {"id": "1809.01564", "submitter": "Julian Nubert", "authors": "Julian Nubert, Nicholas Giai Truong, Abel Lim, Herbert Ilhan Tanujaya,\n  Leah Lim, Mai Anh Vu", "title": "Traffic Density Estimation using a Convolutional Neural Network", "comments": "Machine Learning Project National University of Singapore. 6 pages, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to introduce and present a machine learning\napplication that aims to improve the quality of life of people in Singapore. In\nparticular, we investigate the use of machine learning solutions to tackle the\nproblem of traffic congestion in Singapore. In layman's terms, we seek to make\nSingapore (or any other city) a smoother place. To accomplish this aim, we\npresent an end-to-end system comprising of 1. A traffic density estimation\nalgorithm at traffic lights/junctions and 2. a suitable traffic signal control\nalgorithms that make use of the density information for better traffic control.\nTraffic density estimation can be obtained from traffic junction images using\nvarious machine learning techniques (combined with CV tools). After research\ninto various advanced machine learning methods, we decided on convolutional\nneural networks (CNNs). We conducted experiments on our algorithms, using the\npublicly available traffic camera dataset published by the Land Transport\nAuthority (LTA) to demonstrate the feasibility of this approach. With these\ntraffic density estimates, different traffic algorithms can be applied to\nminimize congestion at traffic junctions in general.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:03:23 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Nubert", "Julian", ""], ["Truong", "Nicholas Giai", ""], ["Lim", "Abel", ""], ["Tanujaya", "Herbert Ilhan", ""], ["Lim", "Leah", ""], ["Vu", "Mai Anh", ""]]}, {"id": "1809.01571", "submitter": "Shaohan Chen", "authors": "Shaohan Chen, Chuanhou Gao", "title": "Knowledge Integrated Classifier Design Based on Utility Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a systematic framework to design a classification model\nthat yields a classifier which optimizes a utility function based on prior\nknowledge. Specifically, as the data size grows, we prove that the produced\nclassifier asymptotically converges to the optimal classifier, an extended\nversion of the Bayes rule, which maximizes the utility function. Therefore, we\nprovide a meaningful theoretical interpretation for modeling with the knowledge\nincorporated. Our knowledge incorporation method allows domain experts to guide\nthe classifier towards correctly classifying data that they think to be more\nsignificant.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:14:08 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Chen", "Shaohan", ""], ["Gao", "Chuanhou", ""]]}, {"id": "1809.01575", "submitter": "Heinke Hihn", "authors": "Heinke Hihn, Sebastian Gottwald, and Daniel A. Braun", "title": "Bounded Rational Decision-Making with Adaptive Neural Network Priors", "comments": "Published in ANNPR 2018: Artificial Neural Networks in Pattern\n  Recognition", "journal-ref": "Pancioni L., Schwenker F., Trentin E. (eds) Artificial Neural\n  Networks in Pattern Recognition. ANNPR 2018. Lecture Notes in Computer\n  Science, vol 11081. Springer, Cham", "doi": "10.1007/978-3-319-99978-4_17", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bounded rationality investigates utility-optimizing decision-makers with\nlimited information-processing power. In particular, information theoretic\nbounded rationality models formalize resource constraints abstractly in terms\nof relative Shannon information, namely the Kullback-Leibler Divergence between\nthe agents' prior and posterior policy. Between prior and posterior lies an\nanytime deliberation process that can be instantiated by sample-based\nevaluations of the utility function through Markov Chain Monte Carlo (MCMC)\noptimization. The most simple model assumes a fixed prior and can relate\nabstract information-theoretic processing costs to the number of sample\nevaluations. However, more advanced models would also address the question of\nlearning, that is how the prior is adapted over time such that generated prior\nproposals become more efficient. In this work we investigate generative neural\nnetworks as priors that are optimized concurrently with anytime sample-based\ndecision-making processes such as MCMC. We evaluate this approach on toy\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:36:09 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Hihn", "Heinke", ""], ["Gottwald", "Sebastian", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1809.01587", "submitter": "Minsuk Kahng", "authors": "Minsuk Kahng, Nikhil Thorat, Duen Horng Chau, Fernanda Vi\\'egas,\n  Martin Wattenberg", "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive\n  Visual Experimentation", "comments": "This paper will be published in the IEEE Transactions on\n  Visualization and Computer Graphics, 25(1), January 2019, and presented at\n  IEEE VAST 2018", "journal-ref": null, "doi": "10.1109/TVCG.2018.2864500", "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success in deep learning has generated immense interest among\npractitioners and students, inspiring many to learn about this new technology.\nWhile visual and interactive approaches have been successfully developed to\nhelp people more easily learn deep learning, most existing tools focus on\nsimpler models. In this work, we present GAN Lab, the first interactive\nvisualization tool designed for non-experts to learn and experiment with\nGenerative Adversarial Networks (GANs), a popular class of complex deep\nlearning models. With GAN Lab, users can interactively train generative models\nand visualize the dynamic training process's intermediate results. GAN Lab\ntightly integrates an model overview graph that summarizes GAN's structure, and\na layered distributions view that helps users interpret the interplay between\nsubmodels. GAN Lab introduces new interactive experimentation features for\nlearning complex deep learning models, such as step-by-step training at\nmultiple levels of abstraction for understanding intricate training dynamics.\nImplemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web\nbrowsers, without the need for installation or specialized hardware, overcoming\na major practical challenge in deploying interactive tools for deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:51:50 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Kahng", "Minsuk", ""], ["Thorat", "Nikhil", ""], ["Chau", "Duen Horng", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1809.01604", "submitter": "Julian Dolby", "authors": "Kavitha Srinivas, Abraham Gale, Julian Dolby", "title": "Merging datasets through deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merging datasets is a key operation for data analytics. A frequent\nrequirement for merging is joining across columns that have different surface\nforms for the same entity (e.g., the name of a person might be represented as\n\"Douglas Adams\" or \"Adams, Douglas\"). Similarly, ontology alignment can require\nrecognizing distinct surface forms of the same entity, especially when\nontologies are independently developed. However, data management systems are\ncurrently limited to performing merges based on string equality, or at best\nusing string similarity. We propose an approach to performing merges based on\ndeep learning models. Our approach depends on (a) creating a deep learning\nmodel that maps surface forms of an entity into a set of vectors such that\nalternate forms for the same entity are closest in vector space, (b) indexing\nthese vectors using a nearest neighbors algorithm to find the forms that can be\npotentially joined together. To build these models, we had to adapt techniques\nfrom metric learning due to the characteristics of the data; specifically we\ndescribe novel sample selection techniques and loss functions that work for\nthis problem. To evaluate our approach, we used Wikidata as ground truth and\nbuilt models from datasets with approximately 1.1M people's names (200K\nidentities) and 130K company names (70K identities). We developed models that\nallow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the\nmodels available for aligning people or companies across multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 16:19:26 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Srinivas", "Kavitha", ""], ["Gale", "Abraham", ""], ["Dolby", "Julian", ""]]}, {"id": "1809.01605", "submitter": "Tadesse Zemicheal", "authors": "Thomas G. Dietterich, Tadesse Zemicheal", "title": "Anomaly Detection in the Presence of Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard methods for anomaly detection assume that all features are observed\nat both learning time and prediction time. Such methods cannot process data\ncontaining missing values. This paper studies five strategies for handling\nmissing values in test queries: (a) mean imputation, (b) MAP imputation, (c)\nreduction (reduced-dimension anomaly detectors via feature bagging), (d)\nmarginalization (for density estimators only), and (e) proportional\ndistribution (for tree-based methods only). Our analysis suggests that MAP\nimputation and proportional distribution should give better results than mean\nimputation, reduction, and marginalization. These hypotheses are largely\nconfirmed by experimental studies on synthetic data and on anomaly detection\nbenchmark data sets using the Isolation Forest (IF), LODA, and EGMM anomaly\ndetection algorithms. However, marginalization worked surprisingly well for\nEGMM, and there are exceptions where reduction works well on some benchmark\nproblems. We recommend proportional distribution for IF, MAP imputation for\nLODA, and marginalization for EGMM.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 16:21:05 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Dietterich", "Thomas G.", ""], ["Zemicheal", "Tadesse", ""]]}, {"id": "1809.01625", "submitter": "Md Ashad Alam PhD", "authors": "Md. Ashad Alam and Mohammad Shahjama and Md. Ferdush Rahman", "title": "Gene Shaving using influence function of a kernel method", "comments": "14 pages, 6 figures, submitted to ICCIT2018, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying significant subsets of the genes, gene shaving is an essential\nand challenging issue for biomedical research for a huge number of genes and\nthe complex nature of biological networks,. Since positive definite kernel\nbased methods on genomic information can improve the prediction of diseases, in\nthis paper we proposed a new method, \"kernel gene shaving (kernel canonical\ncorrelation analysis (kernel CCA) based gene shaving). This problem is\naddressed using the influence function of the kernel CCA. To investigate the\nperformance of the proposed method in a comparison of three popular gene\nselection methods (T-test, SAM and LIMMA), we were used extensive simulated and\nreal microarray gene expression datasets. The performance measures AUC was\ncomputed for each of the methods. The achievement of the proposed method has\nimproved than the three well-known gene selection methods. In real data\nanalysis, the proposed method identified a subsets of $210$ genes out of $2000$\ngenes. The network of these genes has significantly more interactions than\nexpected, which indicates that they may function in a concerted effort on colon\ncancer.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:09:00 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Alam", "Md. Ashad", ""], ["Shahjama", "Mohammad", ""], ["Rahman", "Md. Ferdush", ""]]}, {"id": "1809.01628", "submitter": "Mariana Souza", "authors": "Mariana A. Souza, George D. C. Cavalcanti, Rafael M. O. Cruz, Robert\n  Sabourin", "title": "Online local pool generation for dynamic classifier selection: an\n  extended version", "comments": "Extended version of the paper: M. A. Souza, G. D. Cavalcanti, R. M.\n  Cruz, R. Sabourin, Online local pool generation for dynamic classifier\n  selection, Pattern Recognition 85 (2019) 132 - 148", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Classifier Selection (DCS) techniques have difficulty in selecting\nthe most competent classifier in a pool, even when its presence is assured.\nSince the DCS techniques rely only on local data to estimate a classifier's\ncompetence, the manner in which the pool is generated could affect the choice\nof the best classifier for a given sample. That is, the global perspective in\nwhich pools are generated may not help the DCS techniques in selecting a\ncompetent classifier for samples that are likely to be mislabelled. Thus, we\npropose in this work an online pool generation method that produces a locally\naccurate pool for test samples in difficult regions of the feature space. The\ndifficulty of a given area is determined by the classification difficulty of\nthe samples in it. That way, by using classifiers that were generated in a\nlocal scope, it could be easier for the DCS techniques to select the best one\nfor the difficult samples. For the query samples in easy regions, a simple\nnearest neighbors rule is used. In the extended version of this work, a deep\nanalysis on the correlation between instance hardness and the performance of\nDCS techniques is presented. An instance hardness measure that conveys the\ndegree of local class overlap is then used to decide when the local pool is\nused in the proposed scheme. The proposed method yielded significantly greater\nrecognition rates in comparison to a Bagging-generated pool and two other\nglobal pool generation schemes for all DCS techniques evaluated. The proposed\nscheme's performance was also significantly superior to three state-of-the-art\nclassification models and statistically equivalent to five of them. Moreover,\nan extended analysis on the computational complexity of the proposed method and\nof several DS techniques is presented in this version. We also provide the\nimplementation of the proposed technique using the DESLib library on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:13:02 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Souza", "Mariana A.", ""], ["Cavalcanti", "George D. C.", ""], ["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""]]}, {"id": "1809.01635", "submitter": "Andrew Bray", "authors": "Simon Couch, Zeki Kazan, Kaiyan Shi, Andrew Bray, and Adam Groce", "title": "A Differentially Private Wilcoxon Signed-Rank Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hypothesis tests are a crucial statistical tool for data mining and are the\nworkhorse of scientific research in many fields. Here we present a\ndifferentially private analogue of the classic Wilcoxon signed-rank hypothesis\ntest, which is used when comparing sets of paired (e.g., before-and-after) data\nvalues. We present not only a private estimate of the test statistic, but a\nmethod to accurately compute a p-value and assess statistical significance. We\nevaluate our test on both simulated and real data. Compared to the only\nexisting private test for this situation, that of Task and Clifton, we find\nthat our test requires less than half as much data to achieve the same\nstatistical power.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:21:52 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Couch", "Simon", ""], ["Kazan", "Zeki", ""], ["Shi", "Kaiyan", ""], ["Bray", "Andrew", ""], ["Groce", "Adam", ""]]}, {"id": "1809.01697", "submitter": "Zirui Xu", "authors": "Zirui Xu, Fuxun Yu, Chenchen Liu, Xiang Chen", "title": "HASP: A High-Performance Adaptive Mobile Security Enhancement Against\n  Malicious Speech Recognition", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SD eess.AS eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, machine learning based Automatic Speech Recognition (ASR) technique\nhas widely spread in smartphones, home devices, and public facilities. As\nconvenient as this technology can be, a considerable security issue also raises\n-- the users' speech content might be exposed to malicious ASR monitoring and\ncause severe privacy leakage. In this work, we propose HASP -- a\nhigh-performance security enhancement approach to solve this security issue on\nmobile devices. Leveraging ASR systems' vulnerability to the adversarial\nexamples, HASP is designed to cast human imperceptible adversarial noises to\nreal-time speech and effectively perturb malicious ASR monitoring by increasing\nthe Word Error Rate (WER). To enhance the practical performance on mobile\ndevices, HASP is also optimized for effective adaptation to the human speech\ncharacteristics, environmental noises, and mobile computation scenarios. The\nexperiments show that HASP can achieve optimal real-time security enhancement:\nit can lead an average WER of 84.55% for perturbing the malicious ASR\nmonitoring, and the data processing speed is 15x to 40x faster compared to the\nstate-of-the-art methods. Moreover, HASP can effectively perturb various ASR\nsystems, demonstrating a strong transferability.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 00:47:50 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Xu", "Zirui", ""], ["Yu", "Fuxun", ""], ["Liu", "Chenchen", ""], ["Chen", "Xiang", ""]]}, {"id": "1809.01703", "submitter": "Lucas Vinh Tran", "authors": "Lucas Vinh Tran, Yi Tay, Shuai Zhang, Gao Cong, Xiaoli Li", "title": "HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for\n  Recommender Systems", "comments": "Accepted at WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the notion of learning user and item representations\nin non-Euclidean space. Specifically, we study the connection between metric\nlearning in hyperbolic space and collaborative filtering by exploring Mobius\ngyrovector spaces where the formalism of the spaces could be utilized to\ngeneralize the most common Euclidean vector operations. Overall, this work aims\nto bridge the gap between Euclidean and hyperbolic geometry in recommender\nsystems through metric learning approach. We propose HyperML (Hyperbolic Metric\nLearning), a conceptually simple but highly effective model for boosting the\nperformance. Via a series of extensive experiments, we show that our proposed\nHyperML not only outperforms their Euclidean counterparts, but also achieves\nstate-of-the-art performance on multiple benchmark datasets, demonstrating the\neffectiveness of personalized recommendation in hyperbolic geometry.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 19:30:54 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 05:26:57 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 09:12:07 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Tran", "Lucas Vinh", ""], ["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Cong", "Gao", ""], ["Li", "Xiaoli", ""]]}, {"id": "1809.01706", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha and Akshay Chaturvedi", "title": "A Limitation of V-Matrix based Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate the conditional probability functions based on the direct problem\nsetting, V-matrix based method was proposed. We construct V-matrix based\nconstrained quadratic programming problems for which the inequality constraints\nare inconsistent. In particular, we would like to present that the constrained\nquadratic optimization problem for conditional probability estimation using\nV-matrix method may not have a consistent solution always.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:15:31 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Gauraha", "Niharika", ""], ["Chaturvedi", "Akshay", ""]]}, {"id": "1809.01712", "submitter": "Bhavya Kailkhura", "authors": "Gowtham Muniraju, Bhavya Kailkhura, Jayaraman J. Thiagarajan,\n  Peer-Timo Bremer, Cihan Tepedelenlioglu, Andreas Spanias", "title": "Coverage-Based Designs Improve Sample Mining and Hyper-Parameter\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling one or more effective solutions from large search spaces is a\nrecurring idea in machine learning, and sequential optimization has become a\npopular solution. Typical examples include data summarization, sample mining\nfor predictive modeling and hyper-parameter optimization. Existing solutions\nattempt to adaptively trade-off between global exploration and local\nexploitation, wherein the initial exploratory sample is critical to their\nsuccess. While discrepancy-based samples have become the de facto approach for\nexploration, results from computer graphics suggest that coverage-based\ndesigns, e.g. Poisson disk sampling, can be a superior alternative. In order to\nsuccessfully adopt coverage-based sample designs to ML applications, which were\noriginally developed for 2-d image analysis, we propose fundamental advances by\nconstructing a parameterized family of designs with provably improved coverage\ncharacteristics, and by developing algorithms for effective sample synthesis.\nUsing experiments in sample mining and hyper-parameter optimization for\nsupervised learning, we show that our approach consistently outperforms\nexisting exploratory sampling methods in both blind exploration, and sequential\nsearch with Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 19:59:38 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 23:41:03 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 18:21:15 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Muniraju", "Gowtham", ""], ["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Bremer", "Peer-Timo", ""], ["Tepedelenlioglu", "Cihan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1809.01715", "submitter": "Olga Taran", "authors": "Olga Taran, Shideh Rezaeifar, Slava Voloshynovskiy", "title": "Bridging machine learning and cryptography in defence against\n  adversarial attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, deep learning algorithms have become very popular thanks\nto the achieved performance in many machine learning and computer vision tasks.\nHowever, most of the deep learning architectures are vulnerable to so called\nadversarial examples. This questions the security of deep neural networks (DNN)\nfor many security- and trust-sensitive domains. The majority of the proposed\nexisting adversarial attacks are based on the differentiability of the DNN cost\nfunction.Defence strategies are mostly based on machine learning and signal\nprocessing principles that either try to detect-reject or filter out the\nadversarial perturbations and completely neglect the classical cryptographic\ncomponent in the defence. In this work, we propose a new defence mechanism\nbased on the second Kerckhoffs's cryptographic principle which states that the\ndefence and classification algorithm are supposed to be known, but not the key.\nTo be compliant with the assumption that the attacker does not have access to\nthe secret key, we will primarily focus on a gray-box scenario and do not\naddress a white-box one. More particularly, we assume that the attacker does\nnot have direct access to the secret block, but (a) he completely knows the\nsystem architecture, (b) he has access to the data used for training and\ntesting and (c) he can observe the output of the classifier for each given\ninput. We show empirically that our system is efficient against most famous\nstate-of-the-art attacks in black-box and gray-box scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 20:16:14 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Taran", "Olga", ""], ["Rezaeifar", "Shideh", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1809.01728", "submitter": "George Sterpu", "authors": "George Sterpu, Christian Saam, Naomi Harte", "title": "Attention-based Audio-Visual Fusion for Robust Automatic Speech\n  Recognition", "comments": "In ICMI'18, October 16-20, 2018, Boulder, CO, USA. Equation (2)\n  corrected on this version", "journal-ref": null, "doi": "10.1145/3242969.3243014", "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition can potentially benefit from the lip motion\npatterns, complementing acoustic speech to improve the overall recognition\nperformance, particularly in noise. In this paper we propose an audio-visual\nfusion strategy that goes beyond simple feature concatenation and learns to\nautomatically align the two modalities, leading to enhanced representations\nwhich increase the recognition accuracy in both clean and noisy conditions. We\ntest our strategy on the TCD-TIMIT and LRS2 datasets, designed for large\nvocabulary continuous speech recognition, applying three types of noise at\ndifferent power ratios. We also exploit state of the art Sequence-to-Sequence\narchitectures, showing that our method can be easily integrated. Results show\nrelative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality\nalone, depending on the acoustic noise level. We anticipate that the fusion\nstrategy can easily generalise to many other multimodal tasks which involve\ncorrelated modalities. Code available online on GitHub:\nhttps://github.com/georgesterpu/Sigmedia-AVSR\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 20:38:48 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 16:35:16 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 11:21:28 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Sterpu", "George", ""], ["Saam", "Christian", ""], ["Harte", "Naomi", ""]]}, {"id": "1809.01733", "submitter": "David Burth Kurka", "authors": "Eirina Bourtsoulatze, David Burth Kurka and Deniz Gunduz", "title": "Deep Joint Source-Channel Coding for Wireless Image Transmission", "comments": "To appear in IEEE Transactions on Cognitive Communications and\n  Networking", "journal-ref": null, "doi": "10.1109/TCCN.2019.2919300", "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint source and channel coding (JSCC) technique for wireless\nimage transmission that does not rely on explicit codes for either compression\nor error correction; instead, it directly maps the image pixel values to the\ncomplex-valued channel input symbols. We parameterize the encoder and decoder\nfunctions by two convolutional neural networks (CNNs), which are trained\njointly, and can be considered as an autoencoder with a non-trainable layer in\nthe middle that represents the noisy communication channel. Our results show\nthat the proposed deep JSCC scheme outperforms digital transmission\nconcatenating JPEG or JPEG2000 compression with a capacity achieving channel\ncode at low signal-to-noise ratio (SNR) and channel bandwidth values in the\npresence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC\ndoes not suffer from the ``cliff effect'', and it provides a graceful\nperformance degradation as the channel SNR varies with respect to the SNR value\nassumed during training. In the case of a slow Rayleigh fading channel, deep\nJSCC learns noise resilient coded representations and significantly outperforms\nseparation-based digital communication at all SNR and channel bandwidth values.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:28:51 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 22:15:22 GMT"}, {"version": "v3", "created": "Sat, 16 Mar 2019 17:41:45 GMT"}, {"version": "v4", "created": "Mon, 17 Jun 2019 19:07:59 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bourtsoulatze", "Eirina", ""], ["Kurka", "David Burth", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1809.01738", "submitter": "Hussein Saad", "authors": "Hussein Saad and Aria Nosratinia", "title": "Recovering a Single Community with Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of the quality and quantity of side information on the\nrecovery of a hidden community of size $K=o(n)$ in a graph of size $n$. Side\ninformation for each node in the graph is modeled by a random vector with the\nfollowing features: either the dimension of the vector is allowed to vary with\n$n$, while log-likelihood ratio (LLR) of each component with respect to the\nnode label is fixed, or the LLR is allowed to vary and the vector dimension is\nfixed. These two models represent the variation in quality and quantity of side\ninformation. Under maximum likelihood detection, we calculate tight necessary\nand sufficient conditions for exact recovery of the labels. We demonstrate how\nside information needs to evolve with $n$ in terms of either its quantity, or\nquality, to improve the exact recovery threshold. A similar set of results are\nobtained for weak recovery. Under belief propagation, tight necessary and\nsufficient conditions for weak recovery are calculated when the LLRs are\nconstant, and sufficient conditions when the LLRs vary with $n$. Moreover, we\ndesign and analyze a local voting procedure using side information that can\nachieve exact recovery when applied after belief propagation. The results for\nbelief propagation are validated via simulations on finite synthetic data-sets,\nshowing that the asymptotic results of this paper can also shed light on the\nperformance at finite $n$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 21:27:19 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Saad", "Hussein", ""], ["Nosratinia", "Aria", ""]]}, {"id": "1809.01740", "submitter": "Matthew Engelhard", "authors": "Matthew Engelhard, Hongteng Xu, Lawrence Carin, Jason A Oliver,\n  Matthew Hallyburton, F Joseph McClernon", "title": "Predicting Smoking Events with a Time-Varying Semi-Parametric Hawkes\n  Process Model", "comments": "Presented at Machine Learning for Healthcare 2018, Stanford, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health risks from cigarette smoking -- the leading cause of preventable death\nin the United States -- can be substantially reduced by quitting. Although most\nsmokers are motivated to quit, the majority of quit attempts fail. A number of\nstudies have explored the role of self-reported symptoms, physiologic\nmeasurements, and environmental context on smoking risk, but less work has\nfocused on the temporal dynamics of smoking events, including daily patterns\nand related nicotine effects. In this work, we examine these dynamics and\nimprove risk prediction by modeling smoking as a self-triggering process, in\nwhich previous smoking events modify current risk. Specifically, we fit smoking\nevents self-reported by 42 smokers to a time-varying semi-parametric Hawkes\nprocess (TV-SPHP) developed for this purpose. Results show that the TV-SPHP\nachieves superior prediction performance compared to related and existing\nmodels, with the incorporation of time-varying predictors having greatest\nbenefit over longer prediction windows. Moreover, the impact function\nillustrates previously unknown temporal dynamics of smoking, with possible\nconnections to nicotine metabolism to be explored in future work through a\nrandomized study design. By more effectively predicting smoking events and\nexploring a self-triggering component of smoking risk, this work supports\ndevelopment of novel or improved cessation interventions that aim to reduce\ndeath from smoking.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 21:37:27 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Engelhard", "Matthew", ""], ["Xu", "Hongteng", ""], ["Carin", "Lawrence", ""], ["Oliver", "Jason A", ""], ["Hallyburton", "Matthew", ""], ["McClernon", "F Joseph", ""]]}, {"id": "1809.01749", "submitter": "Mohammad Golbabaee", "authors": "Mohammad Golbabaee, Dongdong Chen, Pedro A. G\\'omez, Marion I. Menzel,\n  Mike E. Davies", "title": "Geometry of Deep Learning for Magnetic Resonance Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are\nbottlenecked by the heavy storage and computation requirements of a\ndictionary-matching (DM) step due to the growing size and complexity of the\nfingerprint dictionaries in multi-parametric quantitative MRI applications. In\nthis paper we study a deep learning approach to address these shortcomings.\nCoupled with a dimensionality reduction first layer, the proposed MRF-Net is\nable to reconstruct quantitative maps by saving more than 60 times in memory\nand computations required for a DM baseline. Fine-grid manifold enumeration\ni.e. the MRF dictionary is only used for training the network and not during\nimage reconstruction. We show that the MRF-Net provides a piece-wise affine\napproximation to the Bloch response manifold projection and that rather than\nmemorizing the dictionary, the network efficiently clusters this manifold and\nlearns a set of hierarchical matched-filters for affine regression of the NMR\ncharacteristics in each segment.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 22:10:16 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 20:46:12 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Golbabaee", "Mohammad", ""], ["Chen", "Dongdong", ""], ["G\u00f3mez", "Pedro A.", ""], ["Menzel", "Marion I.", ""], ["Davies", "Mike E.", ""]]}, {"id": "1809.01765", "submitter": "Tomoya Murata", "authors": "Tomoya Murata, Taiji Suzuki", "title": "Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method\n  for Stochastic Sparse Linear Regression with Limited Attribute Observation", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new stochastic gradient methods for efficiently solving sparse\nlinear regression in a partial attribute observation setting, where learners\nare only allowed to observe a fixed number of actively chosen attributes per\nexample at training and prediction times. It is shown that the methods achieve\nessentially a sample complexity of $O(1/\\varepsilon)$ to attain an error of\n$\\varepsilon$ under a variant of restricted eigenvalue condition, and the rate\nhas better dependency on the problem dimension than existing methods.\nParticularly, if the smallest magnitude of the non-zero components of the\noptimal solution is not too small, the rate of our proposed {\\it Hybrid}\nalgorithm can be boosted to near the minimax optimal sample complexity of {\\it\nfull information} algorithms. The core ideas are (i) efficient construction of\nan unbiased gradient estimator by the iterative usage of the hard thresholding\noperator for configuring an exploration algorithm; and (ii) an adaptive\ncombination of the exploration and an exploitation algorithms for quickly\nidentifying the support of the optimum and efficiently searching the optimal\nparameter in its support. Experimental results are presented to validate our\ntheoretical findings and the superiority of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 23:54:33 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 13:11:35 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 04:43:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Murata", "Tomoya", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1809.01772", "submitter": "Tianle Ma", "authors": "Tianle Ma and Aidong Zhang", "title": "Multi-view Factorization AutoEncoder with Network Constraints for\n  Multi-omic Integrative Analysis", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-omic data provides multiple views of the same patients. Integrative\nanalysis of multi-omic data is crucial to elucidate the molecular underpinning\nof disease etiology. However, multi-omic data has the \"big p, small N\" problem\n(the number of features is large, but the number of samples is small), it is\nchallenging to train a complicated machine learning model from the multi-omic\ndata alone and make it generalize well. Here we propose a framework termed\nMulti-view Factorization AutoEncoder with network constraints to integrate\nmulti-omic data with domain knowledge (biological interactions networks). Our\nframework employs deep representation learning to learn feature embeddings and\npatient embeddings simultaneously, enabling us to integrate feature interaction\nnetwork and patient view similarity network constraints into the training\nobjective. The whole framework is end-to-end differentiable. We applied our\napproach to the TCGA Pan-cancer dataset and achieved satisfactory results to\npredict disease progression-free interval (PFI) and patient overall survival\n(OS) events. Code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 00:34:38 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ma", "Tianle", ""], ["Zhang", "Aidong", ""]]}, {"id": "1809.01774", "submitter": "Mahmoud Nabil", "authors": "Mahmoud Nabil, Muhammad Ismail, Mohamed Mahmoud, Mostafa Shahin,\n  Khalid Qaraqe, Erchin Serpedin", "title": "Deep Recurrent Electricity Theft Detection in AMI Networks with Random\n  Tuning of Hyper-parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern smart grids rely on advanced metering infrastructure (AMI) networks\nfor monitoring and billing purposes. However, such an approach suffers from\nelectricity theft cyberattacks. Different from the existing research that\nutilizes shallow, static, and customer-specific-based electricity theft\ndetectors, this paper proposes a generalized deep recurrent neural network\n(RNN)-based electricity theft detector that can effectively thwart these\ncyberattacks. The proposed model exploits the time series nature of the\ncustomers' electricity consumption to implement a gated recurrent unit\n(GRU)-RNN, hence, improving the detection performance. In addition, the\nproposed RNN-based detector adopts a random search analysis in its learning\nstage to appropriately fine-tune its hyper-parameters. Extensive test studies\nare carried out to investigate the detector's performance using publicly\navailable real data of 107,200 energy consumption days from 200 customers.\nSimulation results demonstrate the superior performance of the proposed\ndetector compared with state-of-the-art electricity theft detectors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 00:41:01 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Nabil", "Mahmoud", ""], ["Ismail", "Muhammad", ""], ["Mahmoud", "Mohamed", ""], ["Shahin", "Mostafa", ""], ["Qaraqe", "Khalid", ""], ["Serpedin", "Erchin", ""]]}, {"id": "1809.01796", "submitter": "Anru Zhang", "authors": "Anru Zhang and Rungang Han", "title": "Optimal Sparse Singular Value Decomposition for High-dimensional\n  High-order Data", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we consider the sparse tensor singular value decomposition,\nwhich aims for dimension reduction on high-dimensional high-order data with\ncertain sparsity structure. A method named Sparse Tensor Alternating\nThresholding for Singular Value Decomposition (STAT-SVD) is proposed. The\nproposed procedure features a novel double projection \\& thresholding scheme,\nwhich provides a sharp criterion for thresholding in each iteration. Compared\nwith regular tensor SVD model, STAT-SVD permits more robust estimation under\nweaker assumptions. Both the upper and lower bounds for estimation accuracy are\ndeveloped. The proposed procedure is shown to be minimax rate-optimal in a\ngeneral class of situations. Simulation studies show that STAT-SVD performs\nwell under a variety of configurations. We also illustrate the merits of the\nproposed procedure on a longitudinal tensor dataset on European country\nmortality rates.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 02:55:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhang", "Anru", ""], ["Han", "Rungang", ""]]}, {"id": "1809.01804", "submitter": "Shiqi Liu", "authors": "Shiqi Liu, Jingxin Liu, Qian Zhao, Xiangyong Cao, Huibin Li, Hongying\n  Meng, Sheng Liu, Deyu Meng", "title": "Discovering Influential Factors in Variational Autoencoder", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of machine learning, it is still a critical issue to identify\nand supervise the learned representation without manually intervening or\nintuition assistance to extract useful knowledge or serve for the downstream\ntasks. In this work, we focus on supervising the influential factors extracted\nby the variational autoencoder(VAE). The VAE is proposed to learn independent\nlow dimension representation while facing the problem that sometimes pre-set\nfactors are ignored. We argue that the mutual information of the input and each\nlearned factor of the representation plays a necessary indicator of discovering\nthe influential factors. We find the VAE objective inclines to induce mutual\ninformation sparsity in factor dimension over the data intrinsic dimension and\nresults in some non-influential factors whose function on data reconstruction\ncould be ignored. We show mutual information also influences the lower bound of\nVAE's reconstruction error and downstream classification task. To make such\nindicator applicable, we design an algorithm for calculating the mutual\ninformation for VAE and prove its consistency. Experimental results on MNIST,\nCelebA and DEAP datasets show that mutual information can help determine\ninfluential factors, of which some are interpretable and can be used to further\ngeneration and classification tasks, and help discover the variant that\nconnects with emotion on DEAP dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 03:33:06 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 07:39:12 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liu", "Shiqi", ""], ["Liu", "Jingxin", ""], ["Zhao", "Qian", ""], ["Cao", "Xiangyong", ""], ["Li", "Huibin", ""], ["Meng", "Hongying", ""], ["Liu", "Sheng", ""], ["Meng", "Deyu", ""]]}, {"id": "1809.01817", "submitter": "Brian Moore", "authors": "Brian E. Moore, Saiprasad Ravishankar, Raj Rao Nadakuditi, and Jeffrey\n  A. Fessler", "title": "Online Adaptive Image Reconstruction (OnAIR) Using Dictionary Models", "comments": "To appear in IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity and low-rank models have been popular for reconstructing images and\nvideos from limited or corrupted measurements. Dictionary or transform learning\nmethods are useful in applications such as denoising, inpainting, and medical\nimage reconstruction. This paper proposes a framework for online (or\ntime-sequential) adaptive reconstruction of dynamic image sequences from linear\n(typically undersampled) measurements. We model the spatiotemporal patches of\nthe underlying dynamic image sequence as sparse in a dictionary, and we\nsimultaneously estimate the dictionary and the images sequentially from\nstreaming measurements. Multiple constraints on the adapted dictionary are also\nconsidered such as a unitary matrix, or low-rank dictionary atoms that provide\nadditional efficiency or robustness. The proposed online algorithms are memory\nefficient and involve simple updates of the dictionary atoms, sparse\ncoefficients, and images. Numerical experiments demonstrate the usefulness of\nthe proposed methods in inverse problems such as video reconstruction or\ninpainting from noisy, subsampled pixels, and dynamic magnetic resonance image\nreconstruction from very limited measurements.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:40:50 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 06:51:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 02:43:46 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Moore", "Brian E.", ""], ["Ravishankar", "Saiprasad", ""], ["Nadakuditi", "Raj Rao", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1809.01818", "submitter": "Chin-Wei Huang", "authors": "Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, Aaron Courville", "title": "Improving Explorability in Variational Inference with Annealed\n  Variational Objectives", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advances in the representational capacity of approximate\ndistributions for variational inference, the optimization process can still\nlimit the density that is ultimately learned. We demonstrate the drawbacks of\nbiasing the true posterior to be unimodal, and introduce Annealed Variational\nObjectives (AVO) into the training of hierarchical variational methods.\nInspired by Annealed Importance Sampling, the proposed method facilitates\nlearning by incorporating energy tempering into the optimization objective. In\nour experiments, we demonstrate our method's robustness to deterministic warm\nup, and the benefits of encouraging exploration in the latent space.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:41:21 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 02:17:05 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 01:33:11 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Huang", "Chin-Wei", ""], ["Tan", "Shawn", ""], ["Lacoste", "Alexandre", ""], ["Courville", "Aaron", ""]]}, {"id": "1809.01819", "submitter": "Saachi Jain", "authors": "Saachi Jain, David Hallac, Rok Sosic, Jure Leskovec", "title": "MASA: Motif-Aware State Assignment in Noisy Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems, such as airplanes, cars, or financial markets, produce\nmultivariate time series data consisting of a large number of system\nmeasurements over a period of time. Such data can be interpreted as a sequence\nof states, where each state represents a prototype of system behavior. An\nimportant problem in this domain is to identify repeated sequences of states,\nknown as motifs. Such motifs correspond to complex behaviors that capture\ncommon sequences of state transitions. For example, in automotive data, a motif\nof \"making a turn\" might manifest as a sequence of states: slowing down,\nturning the wheel, and then speeding back up. However, discovering these motifs\nis challenging, because the individual states and state assignments are\nunknown, have different durations, and need to be jointly learned from the\nnoisy time series. Here we develop motif-aware state assignment (MASA), a\nmethod to discover common motifs in noisy time series data and leverage those\nmotifs to more robustly assign states to measurements. We formulate the problem\nof motif discovery as a large optimization problem, which we solve using an\nexpectation-maximization type approach. MASA performs well in the presence of\nnoise in the input data and is scalable to very large datasets. Experiments on\nsynthetic data show that MASA outperforms state-of-the-art baselines by up to\n38.2%, and two case studies demonstrate how our approach discovers insightful\nmotifs in the presence of noise in real-world time series data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:50:08 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 00:24:56 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Jain", "Saachi", ""], ["Hallac", "David", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1809.01829", "submitter": "Paarth Neekhara", "authors": "Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, Farinaz Koushanfar", "title": "Adversarial Reprogramming of Text Classification Neural Networks", "comments": "Published as a conference paper at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Reprogramming has demonstrated success in utilizing pre-trained\nneural network classifiers for alternative classification tasks without\nmodification to the original network. An adversary in such an attack scenario\ntrains an additive contribution to the inputs to repurpose the neural network\nfor the new classification task. While this reprogramming approach works for\nneural networks with a continuous input space such as that of images, it is not\ndirectly applicable to neural networks trained for tasks such as text\nclassification, where the input space is discrete. Repurposing such\nclassification networks would require the attacker to learn an adversarial\nprogram that maps inputs from one discrete space to the other. In this work, we\nintroduce a context-based vocabulary remapping model to reprogram neural\nnetworks trained on a specific sequence classification task, for a new sequence\nclassification task desired by the adversary. We propose training procedures\nfor this adversarial program in both white-box and black-box settings. We\ndemonstrate the application of our model by adversarially repurposing various\ntext-classification models including LSTM, bi-directional LSTM and CNN for\nalternate classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:29:46 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 11:26:02 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 07:49:02 GMT"}, {"version": "v4", "created": "Thu, 15 Aug 2019 05:16:18 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Neekhara", "Paarth", ""], ["Hussain", "Shehzeen", ""], ["Dubnov", "Shlomo", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1809.01833", "submitter": "Shahab Asoodeh", "authors": "Tingran Gao, Shahab Asoodeh, Yi Huang, and James Evans", "title": "Wasserstein Soft Label Propagation on Hypergraphs: Algorithm and\n  Generalization Error Bounds", "comments": "To appear in Proc. AAAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent interests of developing machine learning and data mining\nalgorithms on hypergraphs, we investigate in this paper the semi-supervised\nlearning algorithm of propagating \"soft labels\" (e.g. probability\ndistributions, class membership scores) over hypergraphs, by means of optimal\ntransportation. Borrowing insights from Wasserstein propagation on graphs\n[Solomon et al. 2014], we re-formulate the label propagation procedure as a\nmessage-passing algorithm, which renders itself naturally to a generalization\napplicable to hypergraphs through Wasserstein barycenters. Furthermore, in a\nPAC learning framework, we provide generalization error bounds for propagating\none-dimensional distributions on graphs and hypergraphs using 2-Wasserstein\ndistance, by establishing the \\textit{algorithmic stability} of the proposed\nsemi-supervised learning algorithm. These theoretical results also shed new\nlights upon deeper understandings of the Wasserstein propagation on graphs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:48:50 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 22:39:46 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Gao", "Tingran", ""], ["Asoodeh", "Shahab", ""], ["Huang", "Yi", ""], ["Evans", "James", ""]]}, {"id": "1809.01843", "submitter": "Jonathan Efroni", "authors": "Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor", "title": "How to Combine Tree-Search Methods in Reinforcement Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite-horizon lookahead policies are abundantly used in Reinforcement\nLearning and demonstrate impressive empirical success. Usually, the lookahead\npolicies are implemented with specific planning methods such as Monte Carlo\nTree Search (e.g. in AlphaZero). Referring to the planning problem as tree\nsearch, a reasonable practice in these implementations is to back up the value\nonly at the leaves while the information obtained at the root is not leveraged\nother than for updating the policy. Here, we question the potency of this\napproach. Namely, the latter procedure is non-contractive in general, and its\nconvergence is not guaranteed. Our proposed enhancement is straightforward and\nsimple: use the return from the optimal tree path to back up the values at the\ndescendants of the root. This leads to a $\\gamma^h$-contracting procedure,\nwhere $\\gamma$ is the discount factor and $h$ is the tree depth. To establish\nour results, we first introduce a notion called \\emph{multiple-step greedy\nconsistency}. We then provide convergence rates for two algorithmic\ninstantiations of the above enhancement in the presence of noise injected to\nboth the tree search stage and value estimation stage.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 06:40:08 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 07:26:42 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Efroni", "Yonathan", ""], ["Dalal", "Gal", ""], ["Scherrer", "Bruno", ""], ["Mannor", "Shie", ""]]}, {"id": "1809.01845", "submitter": "Matthew Blaschko", "authors": "Maxim Berman, Matthew B. Blaschko, Amal Rannen Triki, Jiaqian Yu", "title": "Yes, IoU loss is submodular - as a function of the mispredictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is a response to [7] in which it is claimed that [13, Proposition\n11] is false. We demonstrate here that this assertion in [7] is false, and is\nbased on a misreading of the notion of set membership in [13, Proposition 11].\nWe maintain that [13, Proposition 11] is true.\n  ([7] = arXiv:1809.00593, [13] = arXiv:1512.07797)\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 06:57:36 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Berman", "Maxim", ""], ["Blaschko", "Matthew B.", ""], ["Triki", "Amal Rannen", ""], ["Yu", "Jiaqian", ""]]}, {"id": "1809.01852", "submitter": "Junyuan Shang", "authors": "Junyuan Shang, Cao Xiao, Tengfei Ma, Hongyan Li, Jimeng Sun", "title": "GAMENet: Graph Augmented MEmory Networks for Recommending Medication\n  Combination", "comments": "AAAI 2019; change the template and fix some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent progress in deep learning is revolutionizing the healthcare domain\nincluding providing solutions to medication recommendations, especially\nrecommending medication combination for patients with complex health\nconditions. Existing approaches either do not customize based on patient health\nhistory, or ignore existing knowledge on drug-drug interactions (DDI) that\nmight lead to adverse outcomes. To fill this gap, we propose the Graph\nAugmented Memory Networks (GAMENet), which integrates the drug-drug\ninteractions knowledge graph by a memory module implemented as a graph\nconvolutional networks, and models longitudinal patient records as the query.\nIt is trained end-to-end to provide safe and personalized recommendation of\nmedication combination. We demonstrate the effectiveness and safety of GAMENet\nby comparing with several state-of-the-art methods on real EHR data. GAMENet\noutperformed all baselines in all effectiveness measures, and also achieved\n3.60% DDI rate reduction from existing EHR data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 07:30:13 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 06:56:54 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 04:27:19 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Shang", "Junyuan", ""], ["Xiao", "Cao", ""], ["Ma", "Tengfei", ""], ["Li", "Hongyan", ""], ["Sun", "Jimeng", ""]]}, {"id": "1809.01859", "submitter": "Congzhe Cao", "authors": "Congzhe Cao, Duanshun Li and Ivan Fair", "title": "Deep Learning-Based Decoding for Constrained Sequence Codes", "comments": "7 pages, 6 figures, accepted by IEEE Global Communications Conference\n  Workshop - Machine learning for communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constrained sequence codes have been widely used in modern communication and\ndata storage systems. Sequences encoded with constrained sequence codes satisfy\nconstraints imposed by the physical channel, hence enabling efficient and\nreliable transmission of coded symbols. Traditional encoding and decoding of\nconstrained sequence codes rely on table look-up, which is prone to errors that\noccur during transmission. In this paper, we introduce constrained sequence\ndecoding based on deep learning. With multiple layer perception (MLP) networks\nand convolutional neural networks (CNNs), we are able to achieve low bit error\nrates that are close to maximum a posteriori probability (MAP) decoding as well\nas improve the system throughput. Moreover, implementation of\ncapacity-achieving fixed-length codes, where the complexity is prohibitively\nhigh with table look-up decoding, becomes practical with deep learning-based\ndecoding.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 07:44:33 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Cao", "Congzhe", ""], ["Li", "Duanshun", ""], ["Fair", "Ivan", ""]]}, {"id": "1809.01887", "submitter": "Wei Wang", "authors": "Wei Wang (1) and Xucheng Li (2) ((1) Atkins (SNC-Lavalin), UK, (2)\n  Shenzhen Urban Transport Planning Center Co. Ltd, China)", "title": "Travel Speed Prediction with a Hierarchical Convolutional Neural Network\n  and Long Short-Term Memory Model Framework", "comments": "17 pages, 10 Figures, 4 Tables; To be presented in European Transport\n  Conference in Dublin, Oct 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced travel information and warning, if provided accurately, can help\nroad users avoid traffic congestion through dynamic route planning and behavior\nchange. It also enables traffic control centres mitigate the impact of\ncongestion by activating Intelligent Transport System (ITS) proactively. Deep\nlearning has become increasingly popular in recent years, following a surge of\ninnovative GPU technology, high-resolution, big datasets and thriving machine\nlearning algorithms. However, there are few examples exploiting this emerging\ntechnology to develop applications for traffic prediction. This is largely due\nto the difficulty in capturing random, seasonal, non-linear, and\nspatio-temporal correlated nature of traffic data. In this paper, we propose a\ndata-driven modelling approach with a novel hierarchical D-CLSTM-t deep\nlearning model for short-term traffic speed prediction, a framework combined\nwith convolutional neural network (CNN) and long short-term memory (LSTM)\nmodels. A deep CNN model is employed to learn the spatio-temporal traffic\npatterns of the input graphs, which are then fed into a deep LSTM model for\nsequence learning. To capture traffic seasonal variations, time of the day and\nday of the week indicators are fused with trained features. The model is\ntrained end-to-end to predict travel speed in 15 to 90 minutes in the future.\nWe compare the model performance against other baseline models including CNN,\nLGBM, LSTM, and traditional speed-flow curves. Experiment results show that the\nD-CLSTM-t outperforms other models considerably. Model tests show that speed\nupstream also responds sensibly to a sudden accident occurring downstream. Our\nD-CLSTM-t model framework is also highly scalable for future extension such as\nfor network-wide traffic prediction, which can also be improved by including\nadditional features such as weather, long term seasonality and accident\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:00:57 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 07:39:40 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Xucheng", ""]]}, {"id": "1809.01890", "submitter": "Koichi Hamada", "authors": "Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, Yusuke\n  Uchida", "title": "Full-body High-resolution Anime Generation with Progressive\n  Structure-conditional Generative Adversarial Networks", "comments": "Accepted to ECCV 2018 Workshop: Computer Vision for Fashion, Art and\n  Design. Project page is at https://dena.com/intl/anime-generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Progressive Structure-conditional Generative Adversarial Networks\n(PSGAN), a new framework that can generate full-body and high-resolution\ncharacter images based on structural information. Recent progress in generative\nadversarial networks with progressive training has made it possible to generate\nhigh-resolution images. However, existing approaches have limitations in\nachieving both high image quality and structural consistency at the same time.\nOur method tackles the limitations by progressively increasing the resolution\nof both generated images and structural conditions during training. In this\npaper, we empirically demonstrate the effectiveness of this method by showing\nthe comparison with existing approaches and video generation results of diverse\nanime characters at 1024x1024 based on target pose sequences. We also create a\nnovel dataset containing full-body 1024x1024 high-resolution images and exact\n2D pose keypoints using Unity 3D Avatar models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:09:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Hamada", "Koichi", ""], ["Tachibana", "Kentaro", ""], ["Li", "Tianqi", ""], ["Honda", "Hiroto", ""], ["Uchida", "Yusuke", ""]]}, {"id": "1809.01898", "submitter": "Jo\\~ao R. Campos", "authors": "Jo\\~ao R. Campos, Marco Vieira, Ernesto Costa", "title": "Propheticus: Generalizable Machine Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent technological developments, Machine Learning (ML), a subfield\nof Artificial Intelligence (AI), has been successfully used to process and\nextract knowledge from a variety of complex problems. However, a thorough ML\napproach is complex and highly dependent on the problem at hand. Additionally,\nimplementing the logic required to execute the experiments is no small nor\ntrivial deed, consequentially increasing the probability of faulty code which\ncan compromise the results. Propheticus is a data-driven framework which\nresults of the need for a tool that abstracts some of the inherent complexity\nof ML, whilst being easy to understand and use, as well as to adapt and expand\nto assist the user's specific needs. Propheticus systematizes and enforces\nvarious complex concepts of an ML experiment workflow, taking into account the\nnature of both the problem and the data. It contains functionalities to execute\nall the different tasks, from data preprocessing, to results analysis and\ncomparison. Notwithstanding, it can be fairly easily adapted to different\nproblems due to its flexible architecture, and customized as needed to address\nthe user's needs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:26:03 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Campos", "Jo\u00e3o R.", ""], ["Vieira", "Marco", ""], ["Costa", "Ernesto", ""]]}, {"id": "1809.01906", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Peter Vrancx", "title": "Model-Based Regularization for Deep Reinforcement Learning with\n  Transcoder Networks", "comments": "Presented at the NIPS Deep Reinforcement Learning Workshop, Montreal,\n  Canada, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new optimization objective for value-based deep\nreinforcement learning. We extend conventional Deep Q-Networks (DQNs) by adding\na model-learning component yielding a transcoder network. The prediction errors\nfor the model are included in the basic DQN loss as additional regularizers.\nThis augmented objective leads to a richer training signal that provides\nfeedback at every time step. Moreover, because learning an environment model\nshares a common structure with the RL problem, we hypothesize that the\nresulting objective improves both sample efficiency and performance. We\nempirically confirm our hypothesis on a range of 20 games from the Atari\nbenchmark attaining superior results over vanilla DQN without model-based\nregularization.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:49:18 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 13:30:16 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Leibfried", "Felix", ""], ["Vrancx", "Peter", ""]]}, {"id": "1809.01913", "submitter": "Kshitij Tiwari", "authors": "Kshitij Tiwari", "title": "Hands-on Experience with Gaussian Processes (GPs): Implementing GPs in\n  Python - I", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document serves to complement our website which was developed with the\naim of exposing the students to Gaussian Processes (GPs). GPs are\nnon-parametric Bayesian regression models that are largely used by\nstatisticians and geospatial data scientists for modeling spatial data. Several\nopen source libraries spanning from Matlab [1], Python [2], R [3] etc., are\nalready available for simple plug-and-use. The objective of this handout and in\nturn the website was to allow the users to develop stand-alone GPs in Python by\nrelying on minimal external dependencies. To this end, we only use the default\npython modules and assist the users in developing their own GPs from scratch\ngiving them an in-depth knowledge of what goes on under the hood. The module\ncovers GP inference using maximum likelihood estimation (MLE) and gives\nexamples of 1D (dummy) spatial data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 10:19:50 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Tiwari", "Kshitij", ""]]}, {"id": "1809.01921", "submitter": "Shenda Hong", "authors": "Shenda Hong, Cao Xiao, Trong Nghia Hoang, Tengfei Ma, Hongyan Li,\n  Jimeng Sun", "title": "RDPD: Rich Data Helps Poor Data via Imitation", "comments": "Published in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many situations, we need to build and deploy separate models in related\nenvironments with different data qualities. For example, an environment with\nstrong observation equipments (e.g., intensive care units) often provides\nhigh-quality multi-modal data, which are acquired from multiple sensory devices\nand have rich-feature representations. On the other hand, an environment with\npoor observation equipment (e.g., at home) only provides low-quality, uni-modal\ndata with poor-feature representations. To deploy a competitive model in a\npoor-data environment without requiring direct access to multi-modal data\nacquired from a rich-data environment, this paper develops and presents a\nknowledge distillation (KD) method (RDPD) to enhance a predictive model trained\non poor data using knowledge distilled from a high-complexity model trained on\nrich, private data. We evaluated RDPD on three real-world datasets and shown\nthat its distilled model consistently outperformed all baselines across all\ndatasets, especially achieving the greatest performance improvement over a\nmodel trained only on low-quality data by 24.56% on PR-AUC and 12.21% on\nROC-AUC, and over that of a state-of-the-art KD model by 5.91% on PR-AUC and\n4.44% on ROC-AUC.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 10:57:51 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 02:38:48 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 16:48:36 GMT"}, {"version": "v4", "created": "Sat, 24 Aug 2019 14:50:17 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hong", "Shenda", ""], ["Xiao", "Cao", ""], ["Hoang", "Trong Nghia", ""], ["Ma", "Tengfei", ""], ["Li", "Hongyan", ""], ["Sun", "Jimeng", ""]]}, {"id": "1809.01926", "submitter": "Alessio Burrello", "authors": "Alessio Burrello, Kaspar Schindler, Luca Benini, Abbas Rahimi", "title": "One-shot Learning for iEEG Seizure Detection Using End-to-end Binary\n  Operations: Local Binary Patterns with Hyperdimensional Computing", "comments": "Published as a conference paper at the IEEE BioCAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient binarized algorithm for both learning and\nclassification of human epileptic seizures from intracranial\nelectroencephalography (iEEG). The algorithm combines local binary patterns\nwith brain-inspired hyperdimensional computing to enable end-to-end learning\nand inference with binary operations. The algorithm first transforms iEEG time\nseries from each electrode into local binary pattern codes. Then atomic\nhigh-dimensional binary vectors are used to construct composite representations\nof seizures across all electrodes. For the majority of our patients (10 out of\n16), the algorithm quickly learns from one or two seizures (i.e., one-/few-shot\nlearning) and perfectly generalizes on 27 further seizures. For other patients,\nthe algorithm requires three to six seizures for learning. Overall, our\nalgorithm surpasses the state-of-the-art methods for detecting 65 novel\nseizures with higher specificity and sensitivity, and lower memory footprint.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 11:39:12 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Burrello", "Alessio", ""], ["Schindler", "Kaspar", ""], ["Benini", "Luca", ""], ["Rahimi", "Abbas", ""]]}, {"id": "1809.01991", "submitter": "Fabrizio Sebastiani", "authors": "Fabrizio Sebastiani", "title": "Evaluation Measures for Quantification: An Axiomatic Approach", "comments": "36 pages, 2 figures. Submitted for publication in the Information\n  Retrieval Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification is the task of estimating, given a set $\\sigma$ of unlabelled\nitems and a set of classes $\\mathcal{C}=\\{c_{1}, \\ldots, c_{|\\mathcal{C}|}\\}$,\nthe prevalence (or `relative frequency') in $\\sigma$ of each class $c_{i}\\in\n\\mathcal{C}$. While quantification may in principle be solved by classifying\neach item in $\\sigma$ and counting how many such items have been labelled with\n$c_{i}$, it has long been shown that this `classify and count' (CC) method\nyields suboptimal quantification accuracy. As a result, quantification is no\nlonger considered a mere byproduct of classification, and has evolved as a task\nof its own. While the scientific community has devoted a lot of attention to\ndevising more accurate quantification methods, it has not devoted much to\ndiscussing what properties an \\emph{evaluation measure for quantification}\n(EMQ) should enjoy, and which EMQs should be adopted as a result. This paper\nlies down a number of interesting properties that an EMQ may or may not enjoy,\ndiscusses if (and when) each of these properties is desirable, surveys the EMQs\nthat have been used so far, and discusses whether they enjoy or not the above\nproperties. As a result of this investigation, some of the EMQs that have been\nused in the literature turn out to be severely unfit, while others emerge as\ncloser to what the quantification community actually needs. However, a\nsignificant result is that no existing EMQ satisfies all the properties\nidentified as desirable, thus indicating that more research is needed in order\nto identify (or synthesize) a truly adequate EMQ.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 13:47:53 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Sebastiani", "Fabrizio", ""]]}, {"id": "1809.01999", "submitter": "David Ha", "authors": "David Ha and J\\\"urgen Schmidhuber", "title": "Recurrent World Models Facilitate Policy Evolution", "comments": "To appear at NIPS 2018, selected for an oral presentation. arXiv\n  admin note: substantial text overlap with arXiv:1803.10122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generative recurrent neural network is quickly trained in an unsupervised\nmanner to model popular reinforcement learning environments through compressed\nspatio-temporal representations. The world model's extracted features are fed\ninto compact and simple policies trained by evolution, achieving state of the\nart results in various environments. We also train our agent entirely inside of\nan environment generated by its own internal world model, and transfer this\npolicy back into the actual environment. Interactive version of paper at\nhttps://worldmodels.github.io\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 22:25:12 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ha", "David", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1809.02010", "submitter": "Michael Smith", "authors": "Michael Thomas Smith, Mauricio A Alvarez, Neil D Lawrence", "title": "Gaussian Process Regression for Binned Data", "comments": "10 pages (+1 supp), 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets are in the form of tables of binned data. Performing regression\non these data usually involves either reading off bin heights, ignoring data\nfrom neighbouring bins or interpolating between bins thus over or\nunderestimating the true bin integrals.\n  In this paper we propose an elegant method for performing Gaussian Process\n(GP) regression given such binned data, allowing one to make probabilistic\npredictions of the latent function which produced the binned data.\n  We look at several applications. First, for differentially private\nregression; second, to make predictions over other integrals; and third when\nthe input regions are irregularly shaped collections of polytopes.\n  In summary, our method provides an effective way of analysing binned data\nsuch that one can use more information from the histogram representation, and\nthus reconstruct a more useful and precise density for making predictions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 14:28:26 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:09:32 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Smith", "Michael Thomas", ""], ["Alvarez", "Mauricio A", ""], ["Lawrence", "Neil D", ""]]}, {"id": "1809.02052", "submitter": "Maurizio Ferrari Dacrema", "authors": "Maurizio Ferrari Dacrema and Paolo Cremonesi", "title": "Eigenvalue analogy for confidence estimation in item-based recommender\n  systems", "comments": null, "journal-ref": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys 2018)", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item-item collaborative filtering (CF) models are a well known and studied\nfamily of recommender systems, however current literature does not provide any\ntheoretical explanation of the conditions under which item-based\nrecommendations will succeed or fail.\n  We investigate the existence of an ideal item-based CF method able to make\nperfect recommendations. This CF model is formalized as an eigenvalue problem,\nwhere estimated ratings are equivalent to the true (unknown) ratings multiplied\nby a user-specific eigenvalue of the similarity matrix. Preliminary experiments\nshow that the magnitude of the eigenvalue is proportional to the accuracy of\nrecommendations for that user and therefore it can provide reliable measure of\nconfidence.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:22:19 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 21:23:03 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1809.02064", "submitter": "Lionel Blond\\'e", "authors": "Lionel Blond\\'e, Alexandros Kalousis", "title": "Sample-Efficient Imitation Learning via Generative Adversarial Nets", "comments": "Published as a conference paper for AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GAIL is a recent successful imitation learning architecture that exploits the\nadversarial training procedure introduced in GANs. Albeit successful at\ngenerating behaviours similar to those demonstrated to the agent, GAIL suffers\nfrom a high sample complexity in the number of interactions it has to carry out\nin the environment in order to achieve satisfactory performance. We\ndramatically shrink the amount of interactions with the environment necessary\nto learn well-behaved imitation policies, by up to several orders of magnitude.\nOur framework, operating in the model-free regime, exhibits a significant\nincrease in sample-efficiency over previous methods by simultaneously a)\nlearning a self-tuned adversarially-trained surrogate reward and b) leveraging\nan off-policy actor-critic architecture. We show that our approach is simple to\nimplement and that the learned agents remain remarkably stable, as shown in our\nexperiments that span a variety of continuous control tasks. Video\nvisualisations available at: \\url{https://youtu.be/-nCsqUJnRKU}.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:55:16 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 15:31:32 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 12:00:07 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Blond\u00e9", "Lionel", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1809.02066", "submitter": "Dianhui Wang", "authors": "Ming Li and Dianhui Wang", "title": "Two Dimensional Stochastic Configuration Networks for Image Data\n  Analytics", "comments": "This paper has been submitted to IEEE Transactions on Cybernetics, on\n  August 30, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic configuration networks (SCNs) as a class of randomized learner\nmodel have been successfully employed in data analytics due to its universal\napproximation capability and fast modelling property. The technical essence\nlies in stochastically configuring hidden nodes (or basis functions) based on a\nsupervisory mechanism rather than data-independent randomization as usually\nadopted for building randomized neural networks. Given image data modelling\ntasks, the use of one-dimensional SCNs potentially demolishes the spatial\ninformation of images, and may result in undesirable performance. This paper\nextends the original SCNs to two-dimensional version, termed 2DSCNs, for fast\nbuilding randomized learners with matrix-inputs. Some theoretical analyses on\nthe goodness of 2DSCNs against SCNs, including the complexity of the random\nparameter space, and the superiority of generalization, are presented.\nEmpirical results over one regression, four benchmark handwritten digits\nclassification, and two human face recognition datasets demonstrate that the\nproposed 2DSCNs perform favourably and show good potential for image data\nanalytics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:59:06 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Li", "Ming", ""], ["Wang", "Dianhui", ""]]}, {"id": "1809.02069", "submitter": "Yilong Yang", "authors": "Yilong Yang, Zhuyifan Ye, Yan Su, Qianqian Zhao, Xiaoshan Li, Defang\n  Ouyang", "title": "Deep learning for in vitro prediction of pharmaceutical formulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.apsb.2018.09.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current pharmaceutical formulation development still strongly relies on the\ntraditional trial-and-error approach by individual experiences of\npharmaceutical scientists, which is laborious, time-consuming and costly.\nRecently, deep learning has been widely applied in many challenging domains\nbecause of its important capability of automatic feature extraction. The aim of\nthis research is to use deep learning to predict pharmaceutical formulations.\nIn this paper, two different types of dosage forms were chosen as model\nsystems. Evaluation criteria suitable for pharmaceutics were applied to\nassessing the performance of the models. Moreover, an automatic dataset\nselection algorithm was developed for selecting the representative data as\nvalidation and test datasets. Six machine learning methods were compared with\ndeep learning. The result shows the accuracies of both two deep neural networks\nwere above 80% and higher than other machine learning models, which showed good\nprediction in pharmaceutical formulations. In summary, deep learning with the\nautomatic data splitting algorithm and the evaluation criteria suitable for\npharmaceutical formulation data was firstly developed for the prediction of\npharmaceutical formulations. The cross-disciplinary integration of\npharmaceutics and artificial intelligence may shift the paradigm of\npharmaceutical researches from experience-dependent studies to data-driven\nmethodologies.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 16:03:18 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Yang", "Yilong", ""], ["Ye", "Zhuyifan", ""], ["Su", "Yan", ""], ["Zhao", "Qianqian", ""], ["Li", "Xiaoshan", ""], ["Ouyang", "Defang", ""]]}, {"id": "1809.02070", "submitter": "Tianfu Wu", "authors": "Sameera Lanka and Tianfu Wu", "title": "ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience\n  Replay", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay is an important technique for addressing\nsample-inefficiency in deep reinforcement learning (RL), but faces difficulty\nin learning from binary and sparse rewards due to disproportionately few\nsuccessful experiences in the replay buffer. Hindsight experience replay (HER)\nwas recently proposed to tackle this difficulty by manipulating unsuccessful\ntransitions, but in doing so, HER introduces a significant bias in the replay\nbuffer experiences and therefore achieves a suboptimal improvement in\nsample-efficiency. In this paper, we present an analysis on the source of bias\nin HER, and propose a simple and effective method to counter the bias, to most\neffectively harness the sample-efficiency provided by HER. Our method,\nmotivated by counter-factual reasoning and called ARCHER, extends HER with a\ntrade-off to make rewards calculated for hindsight experiences numerically\ngreater than real rewards. We validate our algorithm on two continuous control\nenvironments from DeepMind Control Suite - Reacher and Finger, which simulate\nmanipulation tasks with a robotic arm - in combination with various reward\nfunctions, task complexities and goal sampling strategies. Our experiments\nconsistently demonstrate that countering bias using more aggressive hindsight\nrewards increases sample efficiency, thus establishing the greater benefit of\nARCHER in RL applications with limited computing budget.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 16:08:39 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 00:31:16 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Lanka", "Sameera", ""], ["Wu", "Tianfu", ""]]}, {"id": "1809.02104", "submitter": "Tom Goldstein", "authors": "Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, Tom\n  Goldstein", "title": "Are adversarial examples inevitable?", "comments": null, "journal-ref": "International Conference on Learning Representations, 2019.\n  https://openreview.net/forum?id=r1lWUoA9FQ", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of defenses have been proposed to harden neural networks against\nadversarial attacks. However, a pattern has emerged in which the majority of\nadversarial defenses are quickly broken by new attacks. Given the lack of\nsuccess at generating robust defenses, we are led to ask a fundamental\nquestion: Are adversarial attacks inevitable? This paper analyzes adversarial\nexamples from a theoretical perspective, and identifies fundamental bounds on\nthe susceptibility of a classifier to adversarial attacks. We show that, for\ncertain classes of problems, adversarial examples are inescapable. Using\nexperiments, we explore the implications of theoretical guarantees for\nreal-world problems and discuss how factors such as dimensionality and image\ncomplexity limit a classifier's robustness against adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:26:58 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 19:34:04 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 21:18:27 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Shafahi", "Ali", ""], ["Huang", "W. Ronny", ""], ["Studer", "Christoph", ""], ["Feizi", "Soheil", ""], ["Goldstein", "Tom", ""]]}, {"id": "1809.02105", "submitter": "Yen-Yu Chang", "authors": "Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, Shou-De Lin", "title": "A Memory-Network Based Solution for Multivariate Time-Series Forecasting", "comments": "8 pages, 4 figures, submitted to AAAI 2019. arXiv admin note: text\n  overlap with arXiv:1703.07015 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series forecasting is extensively studied throughout the\nyears with ubiquitous applications in areas such as finance, traffic,\nenvironment, etc. Still, concerns have been raised on traditional methods for\nincapable of modeling complex patterns or dependencies lying in real word data.\nTo address such concerns, various deep learning models, mainly Recurrent Neural\nNetwork (RNN) based methods, are proposed. Nevertheless, capturing extremely\nlong-term patterns while effectively incorporating information from other\nvariables remains a challenge for time-series forecasting. Furthermore,\nlack-of-explainability remains one serious drawback for deep neural network\nmodels. Inspired by Memory Network proposed for solving the question-answering\ntask, we propose a deep learning based model named Memory Time-series network\n(MTNet) for time series forecasting. MTNet consists of a large memory\ncomponent, three separate encoders, and an autoregressive component to train\njointly. Additionally, the attention mechanism designed enable MTNet to be\nhighly interpretable. We can easily tell which part of the historic data is\nreferenced the most.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:29:10 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Chang", "Yen-Yu", ""], ["Sun", "Fan-Yun", ""], ["Wu", "Yueh-Hua", ""], ["Lin", "Shou-De", ""]]}, {"id": "1809.02112", "submitter": "Yueh-Hua Wu", "authors": "Yueh-Hua Wu, Fan-Yun Sun, Yen-Yu Chang, Shou-De Lin", "title": "ANS: Adaptive Network Scaling for Deep Rectifier Reinforcement Learning\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a thorough study on how reward scaling can affect\nperformance of deep reinforcement learning agents. In particular, we would like\nto answer the question that how does reward scaling affect non-saturating ReLU\nnetworks in RL? This question matters because ReLU is one of the most effective\nactivation functions for deep learning models. We also propose an Adaptive\nNetwork Scaling framework to find a suitable scale of the rewards during\nlearning for better performance. We conducted empirical studies to justify the\nsolution.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:39:18 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 03:27:13 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 08:00:32 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Wu", "Yueh-Hua", ""], ["Sun", "Fan-Yun", ""], ["Chang", "Yen-Yu", ""], ["Lin", "Shou-De", ""]]}, {"id": "1809.02121", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz and Shie\n  Mannor", "title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement\n  Learning", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (pp. 3566-3577).\n  2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to act when there are many available actions in each state is a\nchallenging task for Reinforcement Learning (RL) agents, especially when many\nof the actions are redundant or irrelevant. In such cases, it is sometimes\neasier to learn which actions not to take. In this work, we propose the\nAction-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL\nalgorithm with an Action Elimination Network (AEN) that eliminates sub-optimal\nactions. The AEN is trained to predict invalid actions, supervised by an\nexternal elimination signal provided by the environment. Simulations\ndemonstrate a considerable speedup and added robustness over vanilla DQN in\ntext-based games with over a thousand discrete actions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:52:41 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 08:21:20 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 14:30:13 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Zahavy", "Tom", ""], ["Haroush", "Matan", ""], ["Merlis", "Nadav", ""], ["Mankowitz", "Daniel J.", ""], ["Mannor", "Shie", ""]]}, {"id": "1809.02130", "submitter": "Simen Eide", "authors": "Simen Eide and Ning Zhou", "title": "Deep neural network marketplace recommenders in online experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations are broadly used in marketplaces to match users with items\nrelevant to their interests and needs. To understand user intent and tailor\nrecommendations to their needs, we use deep learning to explore various\nheterogeneous data available in marketplaces. This paper focuses on the\nchallenge of measuring recommender performance and summarizes the online\nexperiment results with several promising types of deep neural network\nrecommenders - hybrid item representation models combining features from user\nengagement and content, sequence-based models, and multi-armed bandit models\nthat optimize user engagement by re-ranking proposals from multiple submodels.\nThe recommenders are currently running in production at the leading Norwegian\nmarketplace FINN.no and serves over one million visitors everyday.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 07:56:33 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Eide", "Simen", ""], ["Zhou", "Ning", ""]]}, {"id": "1809.02131", "submitter": "Audun Mathias {\\O}ygard", "authors": "Simen Eide, Audun M. {\\O}ygard, Ning Zhou", "title": "Five lessons from building a deep neural network recommender", "comments": "Fixed typos. Removed \"staged training strategy\" result, as it will\n  vary a lot depending on how the stages are designed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms are widely adopted in marketplaces to help users\nfind the items they are looking for. The sparsity of the items by user matrix\nand the cold-start issue in marketplaces pose challenges for the off-the-shelf\nmatrix factorization based recommender systems. To understand user intent and\ntailor recommendations to their needs, we use deep learning to explore various\nheterogeneous data available in marketplaces. This paper summarizes five\nlessons we learned from experimenting with state-of-the-art deep learning\nrecommenders at the leading Norwegian marketplace FINN.no. We design a hybrid\nrecommender system that takes the user-generated contents of a marketplace\n(including text, images and meta attributes) and combines them with user\nbehavior data such as page views and messages to provide recommendations for\nmarketplace items. Among various tactics we experimented with, the following\nfive show the best impact: staged training instead of end-to-end training,\nleveraging rich user behaviors beyond page views, using user behaviors as noisy\nlabels to train embeddings, using transfer learning to solve the unbalanced\ndata problem, and using attention mechanisms in the hybrid model. This system\nis currently running with around 20% click-through-rate in production at\nFINN.no and serves over one million visitors everyday.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 08:08:42 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 20:40:58 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Eide", "Simen", ""], ["\u00d8ygard", "Audun M.", ""], ["Zhou", "Ning", ""]]}, {"id": "1809.02145", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau", "title": "GANs beyond divergence minimization", "comments": "Associated repository:\n  https://github.com/AlexiaJM/GANsBeyondDivergenceMin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can be interpreted as an adversarial\ngame between two players, a discriminator D and a generator G, in which D\nlearns to classify real from fake data and G learns to generate realistic data\nby \"fooling\" D into thinking that fake data is actually real data. Currently, a\ndominating view is that G actually learns by minimizing a divergence given that\nthe general objective function is a divergence when D is optimal. However, this\nview has been challenged due to inconsistencies between theory and practice. In\nthis paper, we discuss of the properties associated with most loss functions\nfor G (e.g., saturating/non-saturating f-GAN, LSGAN, WGAN, etc.). We show that\nthese loss functions are not divergences and do not have the same equilibrium\nas expected of divergences. This suggests that G does not need to minimize the\nsame objective function as D maximize, nor maximize the objective of D after\nswapping real data with fake data (non-saturating GAN) but can instead use a\nwide range of possible loss functions to learn to generate realistic data. We\ndefine GANs through two separate and independent D maximization and G\nminimization steps. We generalize the generator step to four new classes of\nloss functions, most of which are actual divergences (while traditional G loss\nfunctions are not). We test a wide variety of loss functions from these four\nclasses on a synthetic dataset and on CIFAR-10. We observe that most loss\nfunctions converge well and provide comparable data generation quality to\nnon-saturating GAN, LSGAN, and WGAN-GP generator loss functions, whether we use\ndivergences or non-divergences. These results suggest that GANs do not conform\nwell to the divergence minimization theory and form a much broader range of\nmodels than previously assumed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:00:26 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""]]}, {"id": "1809.02153", "submitter": "Cole Hawkins", "authors": "Cole Hawkins and Zheng Zhang", "title": "Variational Bayesian Inference for Robust Streaming Tensor Factorization\n  and Completion", "comments": "ICDM 2018. arXiv admin note: substantial text overlap with\n  arXiv:1809.01265", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming tensor factorization is a powerful tool for processing high-volume\nand multi-way temporal data in Internet networks, recommender systems and\nimage/video data analysis. Existing streaming tensor factorization algorithms\nrely on least-squares data fitting and they do not possess a mechanism for\ntensor rank determination. This leaves them susceptible to outliers and\nvulnerable to over-fitting. This paper presents a Bayesian robust streaming\ntensor factorization model to identify sparse outliers, automatically determine\nthe underlying tensor rank and accurately fit low-rank structure. We implement\nour model in Matlab and compare it with existing algorithms on tensor datasets\ngenerated from dynamic MRI and Internet traffic.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:22:49 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 16:34:54 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Hawkins", "Cole", ""], ["Zhang", "Zheng", ""]]}, {"id": "1809.02157", "submitter": "Dino Oglic", "authors": "Dino Oglic and Thomas G\\\"artner", "title": "Scalable Learning in Reproducing Kernel Krein Spaces", "comments": "The version accepted for presentation at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first mathematically complete derivation of the Nystr\\\"om\nmethod for low-rank approximation of indefinite kernels and propose an\nefficient method for finding an approximate eigendecomposition of such kernel\nmatrices. Building on this result, we devise highly scalable methods for\nlearning in reproducing kernel Kre\\u{\\i}n spaces. The devised approaches\nprovide a principled and theoretically well-founded means to tackle large scale\nlearning problems with indefinite kernels. The main motivation for our work\ncomes from problems with structured representations (e.g., graphs, strings,\ntime-series), where it is relatively easy to devise a pairwise (dis)similarity\nfunction based on intuition and/or knowledge of domain experts. Such functions\nare typically not positive definite and it is often well beyond the expertise\nof practitioners to verify this condition. The effectiveness of the devised\napproaches is evaluated empirically using indefinite kernels defined on\nstructured and vectorial data representations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:26:06 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 22:36:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Oglic", "Dino", ""], ["G\u00e4rtner", "Thomas", ""]]}, {"id": "1809.02162", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Asuman Ozdaglar and Ali Jadbabaie", "title": "Escaping Saddle Points in Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of escaping from saddle points in smooth\nnonconvex optimization problems subject to a convex set $\\mathcal{C}$. We\npropose a generic framework that yields convergence to a second-order\nstationary point of the problem, if the convex set $\\mathcal{C}$ is simple for\na quadratic objective function. Specifically, our results hold if one can find\na $\\rho$-approximate solution of a quadratic program subject to $\\mathcal{C}$\nin polynomial time, where $\\rho<1$ is a positive constant that depends on the\nstructure of the set $\\mathcal{C}$. Under this condition, we show that the\nsequence of iterates generated by the proposed framework reaches an\n$(\\epsilon,\\gamma)$-second order stationary point (SOSP) in at most\n$\\mathcal{O}(\\max\\{\\epsilon^{-2},\\rho^{-3}\\gamma^{-3}\\})$ iterations. We\nfurther characterize the overall complexity of reaching an SOSP when the convex\nset $\\mathcal{C}$ can be written as a set of quadratic constraints and the\nobjective function Hessian has a specific structure over the convex set\n$\\mathcal{C}$. Finally, we extend our results to the stochastic setting and\ncharacterize the number of stochastic gradient and Hessian evaluations to reach\nan $(\\epsilon,\\gamma)$-SOSP.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:34:36 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 04:33:29 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ozdaglar", "Asuman", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1809.02188", "submitter": "Garrett Bernstein", "authors": "Garrett Bernstein and Daniel Sheldon", "title": "Differentially Private Bayesian Inference for Exponential Families", "comments": "NIPS 2018. Code available at\n  https://github.com/gbernstein6/private_bayesian_expfam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of private inference has been sparked by growing concern regarding\nthe analysis of data when it stems from sensitive sources. We present the first\nmethod for private Bayesian inference in exponential families that properly\naccounts for noise introduced by the privacy mechanism. It is efficient because\nit works only with sufficient statistics and not individual data. Unlike other\nmethods, it gives properly calibrated posterior beliefs in the non-asymptotic\ndata regime.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 19:40:59 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 16:31:59 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 15:22:12 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Bernstein", "Garrett", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1809.02196", "submitter": "Felipe Tobar", "authors": "Felipe Tobar", "title": "Bayesian Nonparametric Spectral Estimation", "comments": "11 pages. In Advances in Neural Information Processing Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a\ntime series) is distributed across different frequencies. This can become\nparticularly challenging when only partial and noisy observations of the signal\nare available, where current methods fail to handle uncertainty appropriately.\nIn this context, we propose a joint probabilistic model for signals,\nobservations and spectra, where SE is addressed as an exact inference problem.\nAssuming a Gaussian process prior over the signal, we apply Bayes' rule to find\nthe analytic posterior distribution of the spectrum given a set of\nobservations. Besides its expressiveness and natural account of spectral\nuncertainty, the proposed model also provides a functional-form representation\nof the power spectral density, which can be optimised efficiently. Comparison\nwith previous approaches, in particular against Lomb-Scargle, is addressed\ntheoretically and also experimentally in three different scenarios. Code and\ndemo available at https://github.com/GAMES-UChile/BayesianSpectralEstimation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 19:53:32 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 18:41:02 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Tobar", "Felipe", ""]]}, {"id": "1809.02206", "submitter": "Akshat Agarwal", "authors": "Akshat Agarwal, Ryan Hope, Katia Sycara", "title": "Challenges of Context and Time in Reinforcement Learning: Introducing\n  Space Fortress as a Benchmark", "comments": "8 pages. Code available at https://github.com/agakshat/spacefortress\n  .Supersedes arXiv:1805.06824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in deep reinforcement learning (RL) has coalesced around improving\nperformance on benchmarks like the Arcade Learning Environment. However, these\nbenchmarks conspicuously miss important characteristics like abrupt\ncontext-dependent shifts in strategy and temporal sensitivity that are often\npresent in real-world domains. As a result, RL research has not focused on\nthese challenges, resulting in algorithms which do not understand critical\nchanges in context, and have little notion of real world time. To tackle this\nissue, this paper introduces the game of Space Fortress as a RL benchmark which\nincorporates these characteristics. We show that existing state-of-the-art RL\nalgorithms are unable to learn to play the Space Fortress game. We then confirm\nthat this poor performance is due to the RL algorithms' context insensitivity\nand reward sparsity. We also identify independent axes along which to vary\ncontext and temporal sensitivity, allowing Space Fortress to be used as a\ntestbed for understanding both characteristics in combination and also in\nisolation. We release Space Fortress as an open-source Gym environment.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 20:17:44 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Agarwal", "Akshat", ""], ["Hope", "Ryan", ""], ["Sycara", "Katia", ""]]}, {"id": "1809.02209", "submitter": "Chai Wah Wu", "authors": "Chai Wah Wu", "title": "ProdSumNet: reducing model parameters in deep neural networks via\n  product-of-sums matrix decompositions", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general framework for reducing the number of trainable model\nparameters in deep learning networks by decomposing linear operators as a\nproduct of sums of simpler linear operators. Recently proposed deep learning\narchitectures such as CNN, KFC, Dilated CNN, etc. are all subsumed in this\nframework and we illustrate other types of neural network architectures within\nthis framework. We show that good accuracy on MNIST and Fashion MNIST can be\nobtained using a relatively small number of trainable parameters. In addition,\nsince implementation of the convolutional layer is resource-heavy, we consider\nan approach in the transform domain that obviates the need for convolutional\nlayers. One of the advantages of this general framework over prior approaches\nis that the number of trainable parameters is not fixed and can be varied\narbitrarily. In particular, we illustrate the tradeoff of varying the number of\ntrainable variables and the corresponding error rate. As an example, by using\nthis decomposition on a reference CNN architecture for MNIST with over 3x10^6\ntrainable parameters, we are able to obtain an accuracy of 98.44% using only\n3554 trainable parameters.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 20:50:40 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 20:34:09 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wu", "Chai Wah", ""]]}, {"id": "1809.02213", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Xiaojing Dong, Chen Dong, Yiwen Sun, Zhenyu Yan, Abhishek\n  Pani", "title": "Dynamic Hierarchical Empirical Bayes: A Predictive Model Applied to\n  Online Advertising", "comments": "AdKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting keywords performance, such as number of impressions, click-through\nrate (CTR), conversion rate (CVR), revenue per click (RPC), and cost per click\n(CPC), is critical for sponsored search in the online advertising industry. An\ninteresting phenomenon is that, despite the size of the overall data, the data\nare very sparse at the individual unit level. To overcome the sparsity and\nleverage hierarchical information across the data structure, we propose a\nDynamic Hierarchical Empirical Bayesian (DHEB) model that dynamically\ndetermines the hierarchy through a data-driven process and provides\nshrinkage-based estimations. Our method is also equipped with an efficient\nempirical approach to derive inferences through the hierarchy. We evaluate the\nproposed method in both simulated and real-world datasets and compare to\nseveral competitive models. The results favor the proposed method among all\ncomparisons in terms of both accuracy and efficiency. In the end, we design a\ntwo-phase system to serve prediction in real time.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 20:56:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Yuan", "Yuan", ""], ["Dong", "Xiaojing", ""], ["Dong", "Chen", ""], ["Sun", "Yiwen", ""], ["Yan", "Zhenyu", ""], ["Pani", "Abhishek", ""]]}, {"id": "1809.02230", "submitter": "Sai Kumar Arava", "authors": "Ning li, Sai Kumar Arava, Chen Dong, Zhenyu Yan, Abhishek Pani", "title": "Deep Neural Net with Attention for Multi-channel Multi-touch Attribution", "comments": "6 pages ; It got published in AdKDD 2018 workshop as part of KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customers are usually exposed to online digital advertisement channels, such\nas email marketing, display advertising, paid search engine marketing, along\ntheir way to purchase or subscribe products( aka. conversion). The marketers\ntrack all the customer journey data and try to measure the effectiveness of\neach advertising channel. The inference about the influence of each channel\nplays an important role in budget allocation and inventory pricing decisions.\nSeveral simplistic rule-based strategies and data-driven algorithmic strategies\nhave been widely used in marketing field, but they do not address the issues,\nsuch as channel interaction, time dependency, user characteristics. In this\npaper, we propose a novel attribution algorithm based on deep learning to\nassess the impact of each advertising channel. We present Deep Neural Net With\nAttention multi-touch attribution model (DNAMTA) model in a supervised learning\nfashion of predicting if a series of events leads to conversion, and it leads\nus to have a deep understanding of the dynamic interaction effects between\nmedia channels. DNAMTA also incorporates user-context information, such as user\ndemographics and behavior, as control variables to reduce the estimation biases\nof media effects. We used computational experiment of large real world\nmarketing dataset to demonstrate that our proposed model is superior to\nexisting methods in both conversion prediction and media channel influence\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 21:43:20 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["li", "Ning", ""], ["Arava", "Sai Kumar", ""], ["Dong", "Chen", ""], ["Yan", "Zhenyu", ""], ["Pani", "Abhishek", ""]]}, {"id": "1809.02235", "submitter": "Kevin Jamieson", "authors": "Kevin Jamieson and Lalit Jain", "title": "A Bandit Approach to Multiple Testing with False Discovery Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive sampling approach for multiple testing which aims to\nmaximize statistical power while ensuring anytime false discovery control. We\nconsider $n$ distributions whose means are partitioned by whether they are\nbelow or equal to a baseline (nulls), versus above the baseline (actual\npositives). In addition, each distribution can be sequentially and repeatedly\nsampled. Inspired by the multi-armed bandit literature, we provide an algorithm\nthat takes as few samples as possible to exceed a target true positive\nproportion (i.e. proportion of actual positives discovered) while giving\nanytime control of the false discovery proportion (nulls predicted as actual\npositives). Our sample complexity results match known information theoretic\nlower bounds and through simulations we show a substantial performance\nimprovement over uniform sampling and an adaptive elimination style algorithm.\nGiven the simplicity of the approach, and its sample efficiency, the method has\npromise for wide adoption in the biological sciences, clinical testing for drug\ndiscovery, and online A/B/n testing problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 22:08:20 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 17:57:55 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 20:35:51 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jamieson", "Kevin", ""], ["Jain", "Lalit", ""]]}, {"id": "1809.02244", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Daniel Malinsky, Ilya Shpitser", "title": "Learning Optimal Fair Policies", "comments": null, "journal-ref": "The Thirty-sixth International Conference on Machine Learning\n  (ICML 2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic discriminatory biases present in our society influence the way\ndata is collected and stored, the way variables are defined, and the way\nscientific findings are put into practice as policy. Automated decision\nprocedures and learning algorithms applied to such data may serve to perpetuate\nexisting injustice or unfairness in our society. In this paper, we consider how\nto make optimal but fair decisions, which \"break the cycle of injustice\" by\ncorrecting for the unfair dependence of both decisions and outcomes on\nsensitive features (e.g., variables that correspond to gender, race,\ndisability, or other protected attributes). We use methods from causal\ninference and constrained optimization to learn optimal policies in a way that\naddresses multiple potential biases which afflict data analysis in sensitive\ncontexts, extending the approach of (Nabi and Shpitser 2018). Our proposal\ncomes equipped with the theoretical guarantee that the chosen fair policy will\ninduce a joint distribution for new instances that satisfies given fairness\nconstraints. We illustrate our approach with both synthetic data and real\ncriminal justice data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 22:46:49 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 17:51:33 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 19:57:39 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Nabi", "Razieh", ""], ["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1809.02262", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao, Qing Pan and Chengan Du", "title": "Logistic Regression Augmented Community Detection for Network Data with\n  Application in Identifying Autism-Related Gene Pathways", "comments": null, "journal-ref": "Biometrics (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching for gene pathways leading to specific disease outcomes,\nadditional information on gene characteristics is often available that may\nfacilitate to differentiate genes related to the disease from irrelevant\nbackground when connections involving both types of genes are observed and\ntheir relationships to the disease are unknown. We propose method to single out\nirrelevant background genes with the help of auxiliary information through a\nlogistic regression, and cluster relevant genes into cohesive groups using the\nadjacency matrix. Expectation-maximization algorithm is modified to maximize a\njoint pseudo-likelihood assuming latent indicators for relevance to the disease\nand latent group memberships as well as Poisson or multinomial distributed link\nnumbers within and between groups. A robust version allowing arbitrary linkage\npatterns within the background is further derived. Asymptotic consistency of\nlabel assignments under the stochastic blockmodel is proven. Superior\nperformance and robustness in finite samples are observed in simulation\nstudies. The proposed robust method identifies previously missed gene sets\nunderlying autism related neurological diseases using diverse data sources\nincluding de novo mutations, gene expressions and protein-protein interactions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 00:38:20 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhao", "Yunpeng", ""], ["Pan", "Qing", ""], ["Du", "Chengan", ""]]}, {"id": "1809.02270", "submitter": "Hong Xu", "authors": "Kexuan Sun and Shudan Zhong and Hong Xu", "title": "Learning Embeddings of Directed Networks with Text-Associated\n  Nodes---with Applications in Software Package Dependency Networks", "comments": "10 pages, 6 figures, 3 tables. 2020 BigGraphs Workshop at IEEE\n  BigData 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network embedding consists of a vector representation for each node in the\nnetwork. Its usefulness has been shown in many real-world application domains,\nsuch as social networks and web networks. Directed networks with text\nassociated with each node, such as software package dependency networks, are\ncommonplace. However, to the best of our knowledge, their embeddings have\nhitherto not been specifically studied. In this paper, we propose PCTADW-1 and\nPCTADW-2, two algorithms based on neural networks that learn embeddings of\ndirected networks with text associated with each node. We create two new\nnode-labeled such networks: The package dependency networks in two popular\nGNU/Linux distributions, Debian and Fedora. We experimentally demonstrate that\nthe embeddings produced by our algorithms resulted in node classification with\nbetter quality than those of various baselines on these two networks. We\nobserve that there exist systematic presence of analogies (similar to those in\nword embeddings) in the network embeddings of software package dependency\nnetworks. To the best of our knowledge, this is the first time that such\nsystematic presence of analogies is observed in network and document\nembeddings. We further demonstrate that these network embeddings can be novelly\nused for better understanding software attributes, such as the development\nprocess and user interface of software, etc.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 01:33:13 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 00:04:41 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 07:44:24 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 03:15:50 GMT"}, {"version": "v5", "created": "Thu, 26 Nov 2020 09:40:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Sun", "Kexuan", ""], ["Zhong", "Shudan", ""], ["Xu", "Hong", ""]]}, {"id": "1809.02288", "submitter": "Longhao Yuan", "authors": "Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, Qibin Zhao", "title": "Tensor Ring Decomposition with Rank Minimization on Latent Space: An\n  Efficient Approach for Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tensor completion tasks, the traditional low-rank tensor decomposition\nmodels suffer from the laborious model selection problem due to their high\nmodel sensitivity. In particular, for tensor ring (TR) decomposition, the\nnumber of model possibilities grows exponentially with the tensor order, which\nmakes it rather challenging to find the optimal TR decomposition. In this\npaper, by exploiting the low-rank structure of the TR latent space, we propose\na novel tensor completion method which is robust to model selection. In\ncontrast to imposing the low-rank constraint on the data space, we introduce\nnuclear norm regularization on the latent TR factors, resulting in the\noptimization step using singular value decomposition (SVD) being performed at a\nmuch smaller scale. By leveraging the alternating direction method of\nmultipliers (ADMM) scheme, the latent TR factors with optimal rank and the\nrecovered tensor can be obtained simultaneously. Our proposed algorithm is\nshown to effectively alleviate the burden of TR-rank selection, thereby greatly\nreducing the computational cost. The extensive experimental results on both\nsynthetic and real-world data demonstrate the superior performance and\nefficiency of the proposed approach against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 03:05:08 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:03:46 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yuan", "Longhao", ""], ["Li", "Chao", ""], ["Mandic", "Danilo", ""], ["Cao", "Jianting", ""], ["Zhao", "Qibin", ""]]}, {"id": "1809.02292", "submitter": "Daoming Lyu", "authors": "Bo Liu, Tengyang Xie, Yangyang Xu, Mohammad Ghavamzadeh, Yinlam Chow,\n  Daoming Lyu, Daesub Yoon", "title": "A Block Coordinate Ascent Algorithm for Mean-Variance Optimization", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk management in dynamic decision problems is a primary concern in many\nfields, including financial investment, autonomous driving, and healthcare. The\nmean-variance function is one of the most widely used objective functions in\nrisk management due to its simplicity and interpretability. Existing algorithms\nfor mean-variance optimization are based on multi-time-scale stochastic\napproximation, whose learning rate schedules are often hard to tune, and have\nonly asymptotic convergence proof. In this paper, we develop a model-free\npolicy search framework for mean-variance optimization with finite-sample error\nbound analysis (to local optima). Our starting point is a reformulation of the\noriginal mean-variance function with its Fenchel dual, from which we propose a\nstochastic block coordinate ascent policy search algorithm. Both the asymptotic\nconvergence guarantee of the last iteration's solution and the convergence rate\nof the randomly picked solution are provided, and their applicability is\ndemonstrated on several benchmark domains.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 03:15:20 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 02:33:24 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 19:31:34 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liu", "Bo", ""], ["Xie", "Tengyang", ""], ["Xu", "Yangyang", ""], ["Ghavamzadeh", "Mohammad", ""], ["Chow", "Yinlam", ""], ["Lyu", "Daoming", ""], ["Yoon", "Daesub", ""]]}, {"id": "1809.02314", "submitter": "Kaito Fujii", "authors": "Kaito Fujii and Tasuku Soma", "title": "Fast greedy algorithms for dictionary selection with generalized\n  sparsity constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dictionary selection, several atoms are selected from finite candidates\nthat successfully approximate given data points in the sparse representation.\nWe propose a novel efficient greedy algorithm for dictionary selection. Not\nonly does our algorithm work much faster than the known methods, but it can\nalso handle more complex sparsity constraints, such as average sparsity. Using\nnumerical experiments, we show that our algorithm outperforms the known methods\nfor dictionary selection, achieving competitive performances with dictionary\nlearning algorithms in a smaller running time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 05:20:12 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Fujii", "Kaito", ""], ["Soma", "Tasuku", ""]]}, {"id": "1809.02322", "submitter": "Dmitrii Marin", "authors": "Dmitrii Marin and Meng Tang and Ismail Ben Ayed and Yuri Boykov", "title": "Beyond Gradient Descent for Regularized Segmentation Losses", "comments": "https://github.com/dmitrii-marin/adm-seg", "journal-ref": "In IEEE conference on Computer Vision and Pattern Recognition\n  (CVPR), Long Beach, CA, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplicity of gradient descent (GD) made it the default method for\ntraining ever-deeper and complex neural networks. Both loss functions and\narchitectures are often explicitly tuned to be amenable to this basic local\noptimization. In the context of weakly-supervised CNN segmentation, we\ndemonstrate a well-motivated loss function where an alternative optimizer (ADM)\nachieves the state-of-the-art while GD performs poorly. Interestingly, GD\nobtains its best result for a \"smoother\" tuning of the loss function. The\nresults are consistent across different network architectures. Our loss is\nmotivated by well-understood MRF/CRF regularization models in \"shallow\"\nsegmentation and their known global solvers. Our work suggests that network\ndesign/training should pay more attention to optimization methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 06:41:17 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 15:56:13 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Marin", "Dmitrii", ""], ["Tang", "Meng", ""], ["Ayed", "Ismail Ben", ""], ["Boykov", "Yuri", ""]]}, {"id": "1809.02337", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Christoph K\\\"ading, Joachim Denzler", "title": "Information-Theoretic Active Learning for Content-Based Image Retrieval", "comments": "GCPR 2018 paper (14 pages text + 2 pages references + 6 pages\n  appendix)", "journal-ref": "Pattern Recognition. GCPR 2018. Lecture Notes in Computer Science,\n  vol 11269, pp. 650-666", "doi": "10.1007/978-3-030-12939-2_45", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode\nactive learning method for binary classification, and apply it for acquiring\nmeaningful user feedback in the context of content-based image retrieval.\nInstead of combining different heuristics such as uncertainty, diversity, or\ndensity, our method is based on maximizing the mutual information between the\npredicted relevance of the images and the expected user feedback regarding the\nselected batch. We propose suitable approximations to this computationally\ndemanding problem and also integrate an explicit model of user behavior that\naccounts for possible incorrect labels and unnameable instances. Furthermore,\nour approach does not only take the structure of the data but also the expected\nmodel output change caused by the user feedback into account. In contrast to\nother methods, ITAL turns out to be highly flexible and provides\nstate-of-the-art performance across various datasets, such as MIRFLICKR and\nImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 07:57:26 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 15:19:35 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["K\u00e4ding", "Christoph", ""], ["Denzler", "Joachim", ""]]}, {"id": "1809.02341", "submitter": "Zhize Li", "authors": "Zhize Li, Jian Li", "title": "A Fast Anderson-Chebyshev Acceleration for Nonlinear Optimization", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anderson acceleration (or Anderson mixing) is an efficient acceleration\nmethod for fixed point iterations $x_{t+1}=G(x_t)$, e.g., gradient descent can\nbe viewed as iteratively applying the operation $G(x) \\triangleq x-\\alpha\\nabla\nf(x)$. It is known that Anderson acceleration is quite efficient in practice\nand can be viewed as an extension of Krylov subspace methods for nonlinear\nproblems. In this paper, we show that Anderson acceleration with Chebyshev\npolynomial can achieve the optimal convergence rate\n$O(\\sqrt{\\kappa}\\ln\\frac{1}{\\epsilon})$, which improves the previous result\n$O(\\kappa\\ln\\frac{1}{\\epsilon})$ provided by (Toth and Kelley, 2015) for\nquadratic functions. Moreover, we provide a convergence analysis for minimizing\ngeneral nonlinear problems. Besides, if the hyperparameters (e.g., the\nLipschitz smooth parameter $L$) are not available, we propose a guessing\nalgorithm for guessing them dynamically and also prove a similar convergence\nrate. Finally, the experimental results demonstrate that the proposed\nAnderson-Chebyshev acceleration method converges significantly faster than\nother algorithms, e.g., vanilla gradient descent (GD), Nesterov's Accelerated\nGD. Also, these algorithms combined with the proposed guessing algorithm\n(guessing the hyperparameters dynamically) achieve much better performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 08:12:56 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 13:30:50 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 17:51:19 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2020 13:47:58 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Li", "Zhize", ""], ["Li", "Jian", ""]]}, {"id": "1809.02352", "submitter": "Willem Waegeman", "authors": "Willem Waegeman, Krzysztof Dembczynski, Eyke Huellermeier", "title": "Multi-Target Prediction: A Unifying View on Problems and Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target prediction (MTP) is concerned with the simultaneous prediction\nof multiple target variables of diverse type. Due to its enormous application\npotential, it has developed into an active and rapidly expanding research field\nthat combines several subfields of machine learning, including multivariate\nregression, multi-label classification, multi-task learning, dyadic prediction,\nzero-shot learning, network inference, and matrix completion. In this paper, we\npresent a unifying view on MTP problems and methods. First, we formally discuss\ncommonalities and differences between existing MTP problems. To this end, we\nintroduce a general framework that covers the above subfields as special cases.\nAs a second contribution, we provide a structured overview of MTP methods. This\nis accomplished by identifying a number of key properties, which distinguish\nsuch methods and determine their suitability for different types of problems.\nFinally, we also discuss a few challenges for future research.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 08:38:02 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Waegeman", "Willem", ""], ["Dembczynski", "Krzysztof", ""], ["Huellermeier", "Eyke", ""]]}, {"id": "1809.02383", "submitter": "Haruo Hosoya", "authors": "Haruo Hosoya", "title": "Group-based Learning of Disentangled Representations with\n  Generalizability for Novel Contents", "comments": null, "journal-ref": "published in IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory data are often comprised of independent content and transformation\nfactors. For example, face images may have shapes as content and poses as\ntransformation. To infer separately these factors from given data, various\n``disentangling'' models have been proposed. However, many of these are\nsupervised or semi-supervised, either requiring attribute labels that are often\nunavailable or disallowing for generalization over new contents. In this study,\nwe introduce a novel deep generative model, called group-based variational\nautoencoders. In this, we assume no explicit labels, but a weaker form of\nstructure that groups together data instances having the same content but\ntransformed differently; we thereby separately estimate a group-common factor\nas content and an instance-specific factor as transformation. This approach\nallows for learning to represent a general continuous space of contents, which\ncan accommodate unseen contents. Despite the simplicity, our model succeeded in\nlearning, from five datasets, content representations that are highly separate\nfrom the transformation representation and generalizable to data with novel\ncontents. We further provide detailed analysis of the latent content code and\nshow insight into how our model obtains the notable transformation invariance\nand content generalizability.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:00:54 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 00:55:30 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hosoya", "Haruo", ""]]}, {"id": "1809.02385", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:04:39 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 23:50:33 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 17:05:05 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1809.02387", "submitter": "Yubin Deng", "authors": "Yubin Deng, Ke Yu, Dahua Lin, Xiaoou Tang, Chen Change Loy", "title": "Improving On-policy Learning with Statistical Reward Accumulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has obtained significant breakthroughs in recent\nyears. Most methods in deep-RL achieve good results via the maximization of the\nreward signal provided by the environment, typically in the form of discounted\ncumulative returns. Such reward signals represent the immediate feedback of a\nparticular action performed by an agent. However, tasks with sparse reward\nsignals are still challenging to on-policy methods. In this paper, we introduce\nan effective characterization of past reward statistics (which can be seen as\nlong-term feedback signals) to supplement this immediate reward feedback. In\nparticular, value functions are learned with multi-critics supervision,\nenabling complex value functions to be more easily approximated in on-policy\nlearning, even when the reward signals are sparse. We also introduce a novel\nexploration mechanism called \"hot-wiring\" that can give a boost to seemingly\ntrapped agents. We demonstrate the effectiveness of our advantage actor\nmulti-critic (A2MC) method across the discrete domains in Atari games as well\nas continuous domains in the MuJoCo environments. A video demo is provided at\nhttps://youtu.be/zBmpf3Yz8tc.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:10:12 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Deng", "Yubin", ""], ["Yu", "Ke", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1809.02394", "submitter": "Jiajie Peng", "authors": "Hansheng Xue, Jiajie Peng, Xuequn Shang", "title": "Deep Feature Learning of Multi-Network Topology for Node Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous structure that describes complex relationships\nbetween different entities in the real world. As a critical component of\nprediction task over nodes in networks, learning the feature representation of\nnodes has become one of the most active areas recently. Network Embedding,\naiming to learn non-linear and low-dimensional feature representation based on\nnetwork topology, has been proved to be helpful on tasks of network analysis,\nespecially node classification. For many real-world systems, multiple types of\nrelations are naturally represented by multiple networks. However, existing\nnetwork embedding methods mainly focus on single network embedding and neglect\nthe information shared among different networks. In this paper, we propose a\nnovel multiple network embedding method based on semisupervised autoencoder,\nnamed DeepMNE, which captures complex topological structures of multi-networks\nand takes the correlation among multi-networks into account. We evaluate\nDeepMNE on the task of node classification with two real-world datasets. The\nexperimental results demonstrate the superior performance of our method over\nfour state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:36:22 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Xue", "Hansheng", ""], ["Peng", "Jiajie", ""], ["Shang", "Xuequn", ""]]}, {"id": "1809.02397", "submitter": "Xavier Renard", "authors": "Xavier Renard, Thibault Laugel, Marie-Jeanne Lesot, Christophe\n  Marsala, Marcin Detyniecki", "title": "Detecting Potential Local Adversarial Examples for Human-Interpretable\n  Defense", "comments": "presented at 2018 ECML/PKDD Workshop on Recent Advances in\n  Adversarial Machine Learning (Nemesis 2018), Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are increasingly used in the industry to make\ndecisions such as credit insurance approval. Some people may be tempted to\nmanipulate specific variables, such as the age or the salary, in order to get\nbetter chances of approval. In this ongoing work, we propose to discuss, with a\nfirst proposition, the issue of detecting a potential local adversarial example\non classical tabular data by providing to a human expert the locally critical\nfeatures for the classifier's decision, in order to control the provided\ninformation and avoid a fraud.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:39:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Renard", "Xavier", ""], ["Laugel", "Thibault", ""], ["Lesot", "Marie-Jeanne", ""], ["Marsala", "Christophe", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "1809.02403", "submitter": "Kan Ren", "authors": "Kan Ren, Jiarui Qin, Lei Zheng, Zhengyu Yang, Weinan Zhang, Lin Qiu,\n  Yong Yu", "title": "Deep Recurrent Survival Analysis", "comments": "AAAI 2019. Supplemental material, slides, code:\n  https://github.com/rk2900/drsa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a hotspot in statistical research for modeling\ntime-to-event information with data censorship handling, which has been widely\nused in many applications such as clinical research, information system and\nother fields with survivorship bias. Many works have been proposed for survival\nanalysis ranging from traditional statistic methods to machine learning models.\nHowever, the existing methodologies either utilize counting-based statistics on\nthe segmented data, or have a pre-assumption on the event probability\ndistribution w.r.t. time. Moreover, few works consider sequential patterns\nwithin the feature space. In this paper, we propose a Deep Recurrent Survival\nAnalysis model which combines deep learning for conditional probability\nprediction at fine-grained level of the data, and survival analysis for\ntackling the censorship. By capturing the time dependency through modeling the\nconditional probability of the event for each sample, our method predicts the\nlikelihood of the true event occurrence and estimates the survival rate over\ntime, i.e., the probability of the non-occurrence of the event, for the\ncensored data. Meanwhile, without assuming any specific form of the event\nprobability distribution, our model shows great advantages over the previous\nworks on fitting various sophisticated data distributions. In the experiments\non the three real-world tasks from different fields, our model significantly\noutperforms the state-of-the-art solutions under various metrics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 11:13:44 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 11:40:19 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Ren", "Kan", ""], ["Qin", "Jiarui", ""], ["Zheng", "Lei", ""], ["Yang", "Zhengyu", ""], ["Zhang", "Weinan", ""], ["Qiu", "Lin", ""], ["Yu", "Yong", ""]]}, {"id": "1809.02408", "submitter": "Laura Balzer PhD", "authors": "Hachem Saddiki and Laura B. Balzer", "title": "A Primer on Causality in Data Science", "comments": "26 pages (with references); 4 figures", "journal-ref": "Journal de la Societe Francaise de Statistique, 161, 2020, 67-90", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many questions in Data Science are fundamentally causal in that our objective\nis to learn the effect of some exposure, randomized or not, on an outcome\ninterest. Even studies that are seemingly non-causal, such as those with the\ngoal of prediction or prevalence estimation, have causal elements, including\ndifferential censoring or measurement. As a result, we, as Data Scientists,\nneed to consider the underlying causal mechanisms that gave rise to the data,\nrather than simply the pattern or association observed in those data. In this\nwork, we review the 'Causal Roadmap' of Petersen and van der Laan (2014) to\nprovide an introduction to some key concepts in causal inference. Similar to\nother causal frameworks, the steps of the Roadmap include clearly stating the\nscientific question, defining of the causal model, translating the scientific\nquestion into a causal parameter, assessing the assumptions needed to express\nthe causal parameter as a statistical estimand, implementation of statistical\nestimators including parametric and semi-parametric methods, and interpretation\nof our findings. We believe that using such a framework in Data Science will\nhelp to ensure that our statistical analyses are guided by the scientific\nquestion driving our research, while avoiding over-interpreting our results. We\nfocus on the effect of an exposure occurring at a single time point and\nhighlight the use of targeted maximum likelihood estimation (TMLE) with Super\nLearner.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 11:26:51 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 04:38:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Saddiki", "Hachem", ""], ["Balzer", "Laura B.", ""]]}, {"id": "1809.02441", "submitter": "Jangho Kim", "authors": "Jangho Kim, Jeesoo Kim, Nojun Kwak", "title": "StackNet: Stacking Parameters for Continual learning", "comments": "CVPR 2020 Workshop on Continual Learning in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a neural network for a classification task typically assumes that\nthe data to train are given from the beginning. However, in the real world,\nadditional data accumulate gradually and the model requires additional training\nwithout accessing the old training data. This usually leads to the catastrophic\nforgetting problem which is inevitable for the traditional training methodology\nof neural networks. In this paper, we propose a continual learning method that\nis able to learn additional tasks while retaining the performance of previously\nlearned tasks by stacking parameters. Composed of two complementary components,\nthe index module and the StackNet, our method estimates the index of the\ncorresponding task for an input sample with the index module and utilizes a\nparticular portion of StackNet with this index. The StackNet guarantees no\ndegradation in the performance of the previously learned tasks and the index\nmodule shows high confidence in finding the origin of an input sample. Compared\nto the previous work of PackNet, our method is competitive and highly\nintuitive.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:39:13 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 08:10:25 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 01:23:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kim", "Jangho", ""], ["Kim", "Jeesoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1809.02482", "submitter": "Fragkiskos  Malliaros", "authors": "Duong Nguyen and Fragkiskos D. Malliaros", "title": "BiasedWalk: Biased Sampling for Representation Learning on Graphs", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding algorithms are able to learn latent feature representations\nof nodes, transforming networks into lower dimensional vector representations.\nTypical key applications, which have effectively been addressed using network\nembeddings, include link prediction, multilabel classification and community\ndetection. In this paper, we propose BiasedWalk, a scalable, unsupervised\nfeature learning algorithm that is based on biased random walks to sample\ncontext information about each node in the network. Our random-walk based\nsampling can behave as Breath-First-Search (BFS) and Depth-First-Search (DFS)\nsamplings with the goal to capture homophily and role equivalence between the\nnodes in the network. We have performed a detailed experimental evaluation\ncomparing the performance of the proposed algorithm against various baseline\nmethods, on several datasets and learning tasks. The experiment results show\nthat the proposed method outperforms the baseline ones in most of the tasks and\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 13:58:37 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Nguyen", "Duong", ""], ["Malliaros", "Fragkiskos D.", ""]]}, {"id": "1809.02497", "submitter": "Rudrajit Das", "authors": "Rudrajit Das, Aditya Golatkar and Suyash P. Awate", "title": "Sparse Kernel PCA for Outlier Detection", "comments": "Accepted at IEEE ICMLA 2018 for Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to perform Sparse Kernel Principal\nComponent Analysis (SKPCA) and also mathematically analyze the validity of\nSKPCA. We formulate SKPCA as a constrained optimization problem with elastic\nnet regularization (Hastie et al.) in kernel feature space and solve it. We\nconsider outlier detection (where KPCA is employed) as an application for\nSKPCA, using the RBF kernel. We test it on 5 real-world datasets and show that\nby using just 4% (or even less) of the principal components (PCs), where each\nPC has on average less than 12% non-zero elements in the worst case among all 5\ndatasets, we are able to nearly match and in 3 datasets even outperform KPCA.\nWe also compare the performance of our method with a recently proposed method\nfor SKPCA by Wang et al. and show that our method performs better in terms of\nboth accuracy and sparsity. We also provide a novel probabilistic proof to\njustify the existence of sparse solutions for KPCA using the RBF kernel. To the\nbest of our knowledge, this is the first attempt at theoretically analyzing the\nvalidity of SKPCA.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:23:03 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:35:15 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Das", "Rudrajit", ""], ["Golatkar", "Aditya", ""], ["Awate", "Suyash P.", ""]]}, {"id": "1809.02499", "submitter": "Hongyu Guo", "authors": "Hongyu Guo and Yongyi Mao and Richong Zhang", "title": "MixUp as Locally Linear Out-Of-Manifold Regularization", "comments": "Accepted by AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MixUp is a recently proposed data-augmentation scheme, which linearly\ninterpolates a random pair of training examples and correspondingly the one-hot\nrepresentations of their labels. Training deep neural networks with such\nadditional data is shown capable of significantly improving the predictive\naccuracy of the current art. The power of MixUp, however, is primarily\nestablished empirically and its working and effectiveness have not been\nexplained in any depth. In this paper, we develop an understanding for MixUp as\na form of \"out-of-manifold regularization\", which imposes certain \"local\nlinearity\" constraints on the model's input space beyond the data manifold.\nThis analysis enables us to identify a limitation of MixUp, which we call\n\"manifold intrusion\". In a nutshell, manifold intrusion in MixUp is a form of\nunder-fitting resulting from conflicts between the synthetic labels of the\nmixed-up examples and the labels of original training data. Such a phenomenon\nusually happens when the parameters controlling the generation of mixing\npolicies are not sufficiently fine-tuned on the training data. To address this\nissue, we propose a novel adaptive version of MixUp, where the mixing policies\nare automatically learned from the data using an additional network and\nobjective function designed to avoid manifold intrusion. The proposed\nregularizer, AdaMixUp, is empirically evaluated on several benchmark datasets.\nExtensive experiments demonstrate that AdaMixUp improves upon MixUp when\napplied to the current art of deep classification models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:26:17 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 01:11:46 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 19:37:01 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Guo", "Hongyu", ""], ["Mao", "Yongyi", ""], ["Zhang", "Richong", ""]]}, {"id": "1809.02505", "submitter": "Liu Liu", "authors": "Liu Liu, Ji Liu, Cho-Jui Hsieh, Dacheng Tao", "title": "Stochastically Controlled Stochastic Gradient for the Convex and\n  Non-convex Composition problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the convex and non-convex composition problem with\nthe structure $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n {{F_i}( {G( x )} )}$, where\n$G( x )=\\frac{1}{n}\\sum\\nolimits_{j = 1}^n {{G_j}( x )} $ is the inner\nfunction, and $F_i(\\cdot)$ is the outer function. We explore the variance\nreduction based method to solve the composition optimization. Due to the fact\nthat when the number of inner function and outer function are large, it is not\nreasonable to estimate them directly, thus we apply the stochastically\ncontrolled stochastic gradient (SCSG) method to estimate the gradient of the\ncomposition function and the value of the inner function. The query complexity\nof our proposed method for the convex and non-convex problem is equal to or\nbetter than the current method for the composition problem. Furthermore, we\nalso present the mini-batch version of the proposed method, which has the\nimproved the query complexity with related to the size of the mini-batch.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 12:04:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Liu", "Liu", ""], ["Liu", "Ji", ""], ["Hsieh", "Cho-Jui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1809.02512", "submitter": "Guilherme Gomes", "authors": "Guilherme Gomes and Vinayak Rao and Jennifer Neville", "title": "Multi-level hypothesis testing for populations of heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider hypothesis testing and anomaly detection on\ndatasets where each observation is a weighted network. Examples of such data\ninclude brain connectivity networks from fMRI flow data, or word co-occurrence\ncounts for populations of individuals. Current approaches to hypothesis testing\nfor weighted networks typically requires thresholding the edge-weights, to\ntransform the data to binary networks. This results in a loss of information,\nand outcomes are sensitivity to choice of threshold levels. Our work avoids\nthis, and we consider weighted-graph observations in two situations, 1) where\neach graph belongs to one of two populations, and 2) where entities belong to\none of two populations, with each entity possessing multiple graphs (indexed\ne.g. by time). Specifically, we propose a hierarchical Bayesian hypothesis\ntesting framework that models each population with a mixture of latent space\nmodels for weighted networks, and then tests populations of networks for\ndifferences in distribution over components. Our framework is capable of\npopulation-level, entity-specific, as well as edge-specific hypothesis testing.\nWe apply it to synthetic data and three real-world datasets: two social media\ndatasets involving word co-occurrences from discussions on Twitter of the\npolitical unrest in Brazil, and on Instagram concerning Attention Deficit\nHyperactivity Disorder (ADHD) and depression drugs, and one medical dataset\ninvolving fMRI brain-scans of human subjects. The results show that our\nproposed method has lower Type I error and higher statistical power compared to\nalternatives that need to threshold the edge weights. Moreover, they show our\nproposed method is better suited to deal with highly heterogeneous datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:44:11 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Gomes", "Guilherme", ""], ["Rao", "Vinayak", ""], ["Neville", "Jennifer", ""]]}, {"id": "1809.02519", "submitter": "David Madras", "authors": "David Madras, Elliot Creager, Toniann Pitassi, Richard Zemel", "title": "Fairness Through Causal Awareness: Learning Latent-Variable Models for\n  Biased Data", "comments": "Accepted as a conference paper at ACM Conference on Fairness,\n  Accountability, and Transparency (ACM FAT*) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we learn from biased data? Historical datasets often reflect\nhistorical prejudices; sensitive or protected attributes may affect the\nobserved treatments and outcomes. Classification algorithms tasked with\npredicting outcomes accurately from these datasets tend to replicate these\nbiases. We advocate a causal modeling approach to learning from biased data,\nexploring the relationship between fair classification and intervention. We\npropose a causal model in which the sensitive attribute confounds both the\ntreatment and the outcome. Building on prior work in deep learning and\ngenerative modeling, we describe how to learn the parameters of this causal\nmodel from observational data alone, even in the presence of unobserved\nconfounders. We show experimentally that fairness-aware causal modeling\nprovides better estimates of the causal effects between the sensitive\nattribute, the treatment, and the outcome. We further present evidence that\nestimating these causal effects can help learn policies that are both more\naccurate and fair, when presented with a historically biased dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 15:00:44 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 19:35:07 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 04:16:13 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Madras", "David", ""], ["Creager", "Elliot", ""], ["Pitassi", "Toniann", ""], ["Zemel", "Richard", ""]]}, {"id": "1809.02589", "submitter": "Naganand Yadati", "authors": "Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin,\n  Anand Louis, Partha Talukdar", "title": "HyperGCN: A New Method of Training Graph Convolutional Networks on\n  Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world network datasets such as co-authorship, co-citation, email\ncommunication, etc., relationships are complex and go beyond pairwise.\nHypergraphs provide a flexible and natural modeling tool to model such complex\nrelationships. The obvious existence of such complex relationships in many\nreal-world networks naturaly motivates the problem of learning with\nhypergraphs. A popular learning paradigm is hypergraph-based semi-supervised\nlearning (SSL) where the goal is to assign labels to initially unlabeled\nvertices in a hypergraph. Motivated by the fact that a graph convolutional\nnetwork (GCN) has been effective for graph-based SSL, we propose HyperGCN, a\nnovel GCN for SSL on attributed hypergraphs. Additionally, we show how HyperGCN\ncan be used as a learning-based approach for combinatorial optimisation on\nNP-hard hypergraph problems. We demonstrate HyperGCN's effectiveness through\ndetailed experimentation on real-world hypergraphs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:27:25 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 19:41:23 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 17:45:32 GMT"}, {"version": "v4", "created": "Wed, 22 May 2019 13:48:41 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Yadati", "Naganand", ""], ["Nimishakavi", "Madhav", ""], ["Yadav", "Prateek", ""], ["Nitin", "Vikram", ""], ["Louis", "Anand", ""], ["Talukdar", "Partha", ""]]}, {"id": "1809.02591", "submitter": "Remi Tachet Des Combes", "authors": "Remi Tachet, Philip Bachman and Harm van Seijen", "title": "Learning Invariances for Policy Generalization", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent progress has spawned very powerful machine learning systems,\nthose agents remain extremely specialized and fail to transfer the knowledge\nthey gain to similar yet unseen tasks. In this paper, we study a simple\nreinforcement learning problem and focus on learning policies that encode the\nproper invariances for generalization to different settings. We evaluate three\npotential methods for policy generalization: data augmentation, meta-learning\nand adversarial training. We find our data augmentation method to be effective,\nand study the potential of meta-learning and adversarial learning as\nalternative task-agnostic approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:32:19 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 12:57:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Tachet", "Remi", ""], ["Bachman", "Philip", ""], ["van Seijen", "Harm", ""]]}, {"id": "1809.02596", "submitter": "Val Andrei Fajardo", "authors": "Val Andrei Fajardo, David Findlay, Roshanak Houmanfar, Charu Jaiswal,\n  Jiaxi Liang, Honglei Xie", "title": "VOS: a Method for Variational Oversampling of Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalanced datasets are common in real-world applications that range\nfrom credit card fraud detection to rare disease diagnostics. Several popular\nclassification algorithms assume that classes are approximately balanced, and\nhence build the accompanying objective function to maximize an overall accuracy\nrate. In these situations, optimizing the overall accuracy will lead to highly\nskewed predictions towards the majority class. Moreover, the negative business\nimpact resulting from false positives (positive samples incorrectly classified\nas negative) can be detrimental. Many methods have been proposed to address the\nclass imbalance problem, including methods such as over-sampling,\nunder-sampling and cost-sensitive methods. In this paper, we consider the\nover-sampling method, where the aim is to augment the original dataset with\nsynthetically created observations of the minority classes. In particular,\ninspired by the recent advances in generative modelling techniques (e.g.,\nVariational Inference and Generative Adversarial Networks), we introduce a new\noversampling technique based on variational autoencoders. Our experiments show\nthat the new method is superior in augmenting datasets for downstream\nclassification tasks when compared to traditional oversampling methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:40:16 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Fajardo", "Val Andrei", ""], ["Findlay", "David", ""], ["Houmanfar", "Roshanak", ""], ["Jaiswal", "Charu", ""], ["Liang", "Jiaxi", ""], ["Xie", "Honglei", ""]]}, {"id": "1809.02599", "submitter": "Stephen Whitelam", "authors": "Stephen Whitelam", "title": "Improving the accuracy of nearest-neighbor classification using\n  principled construction and stochastic sampling of training-set centroids", "comments": null, "journal-ref": "Entropy 23, 149 (2021)", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conceptually simple way to classify images is to directly compare test-set\ndata and training-set data. The accuracy of this approach is limited by the\nmethod of comparison used, and by the extent to which the training-set data\ncover configuration space. Here we show that this coverage can be substantially\nincreased using coarse graining (replacing groups of images by their centroids)\nand stochastic sampling (using distinct sets of centroids in combination). We\nuse the MNIST and Fashion-MNIST data sets to show that a principled\ncoarse-graining algorithm can convert training images into fewer image\ncentroids without loss of accuracy of classification of test-set images by\nnearest-neighbor classification. Distinct batches of centroids can be used in\ncombination as a means of stochastically sampling configuration space, and can\nclassify test-set data more accurately than can the unaltered training set. On\nthe MNIST and Fashion-MNIST data sets this approach converts nearest-neighbor\nclassification from a mid-ranking- to an upper-ranking member of the set of\nclassical machine-learning techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:49:38 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 18:20:07 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 18:18:51 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Whitelam", "Stephen", ""]]}, {"id": "1809.02627", "submitter": "Arthur Juliani", "authors": "Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen,\n  Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan\n  Mattar, Danny Lange", "title": "Unity: A General Platform for Intelligent Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence have been driven by the presence\nof increasingly realistic and complex simulated environments. However, many of\nthe existing environments provide either unrealistic visuals, inaccurate\nphysics, low task complexity, restricted agent perspective, or a limited\ncapacity for interaction among artificial agents. Furthermore, many platforms\nlack the ability to flexibly configure the simulation, making the simulated\nenvironment a black-box from the perspective of the learning system. In this\nwork, we propose a novel taxonomy of existing simulation platforms and discuss\nthe highest level class of general platforms which enable the development of\nlearning environments that are rich in visual, physical, task, and social\ncomplexity. We argue that modern game engines are uniquely suited to act as\ngeneral platforms and as a case study examine the Unity engine and open source\nUnity ML-Agents Toolkit. We then survey the research enabled by Unity and the\nUnity ML-Agents Toolkit, discussing the kinds of research a flexible,\ninteractive and easily configurable general platform can facilitate.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:13:25 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:59:11 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Juliani", "Arthur", ""], ["Berges", "Vincent-Pierre", ""], ["Teng", "Ervin", ""], ["Cohen", "Andrew", ""], ["Harper", "Jonathan", ""], ["Elion", "Chris", ""], ["Goy", "Chris", ""], ["Gao", "Yuan", ""], ["Henry", "Hunter", ""], ["Mattar", "Marwan", ""], ["Lange", "Danny", ""]]}, {"id": "1809.02630", "submitter": "Jie Chen", "authors": "Tengfei Ma, Jie Chen, Cao Xiao", "title": "Constrained Generation of Semantically Valid Graphs via Regularizing\n  Variational Autoencoders", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have achieved remarkable success in various data\ndomains, including images, time series, and natural languages. There remain,\nhowever, substantial challenges for combinatorial structures, including graphs.\nOne of the key challenges lies in the difficulty of ensuring semantic validity\nin context. For examples, in molecular graphs, the number of bonding-electron\npairs must not exceed the valence of an atom; whereas in protein interaction\nnetworks, two proteins may be connected only when they belong to the same or\ncorrelated gene ontology terms. These constraints are not easy to be\nincorporated into a generative model. In this work, we propose a regularization\nframework for variational autoencoders as a step toward semantic validity. We\nfocus on the matrix representation of graphs and formulate penalty terms that\nregularize the output distribution of the decoder to encourage the satisfaction\nof validity constraints. Experimental results confirm a much higher likelihood\nof sampling valid graphs in our approach, compared with others reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:27:16 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 21:21:01 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ma", "Tengfei", ""], ["Chen", "Jie", ""], ["Xiao", "Cao", ""]]}, {"id": "1809.02652", "submitter": "Harris Chan", "authors": "Harris Chan, Atef Chaudhury, Kevin Shen", "title": "Are You Sure You Want To Do That? Classification with Verification", "comments": "9 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification systems typically act in isolation, meaning they are required\nto implicitly memorize the characteristics of all candidate classes in order to\nclassify. The cost of this is increased memory usage and poor sample\nefficiency. We propose a model which instead verifies using reference images\nduring the classification process, reducing the burden of memorization. The\nmodel uses iterative nondifferentiable queries in order to classify an image.\nWe demonstrate that such a model is feasible to train and can match baseline\naccuracy while being more parameter efficient. However, we show that finding\nthe correct balance between image recognition and verification is essential to\npushing the model towards desired behavior, suggesting that a pipeline of\nrecognition followed by verification is a more promising approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 19:59:43 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 03:04:38 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Chan", "Harris", ""], ["Chaudhury", "Atef", ""], ["Shen", "Kevin", ""]]}, {"id": "1809.02665", "submitter": "Michael Jacobs", "authors": "Sanghyun Choi, Nikita Ivkin, Vladimir Braverman, Michael A. Jacobs", "title": "DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using\n  Count Sketch Data Streaming Algorithm: Preliminary Results", "comments": "13 pages, 3 figures, US patent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting information from electronic health records (EHR) is a challenging\ntask since it requires prior knowledge of the reports and some natural language\nprocessing algorithm (NLP). With the growing number of EHR implementations,\nsuch knowledge is increasingly challenging to obtain in an efficient manner. We\naddress this challenge by proposing a novel methodology to analyze large sets\nof EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.\nBy using DreamNLP, we generate a dictionary of frequently occurring terms or\nheavy hitters in the EHRs using low computational memory compared to\nconventional counting approach other NLP programs use. We demonstrate the\nextraction of the most important breast diagnosis features from the EHRs in a\nset of patients that underwent breast imaging. Based on the analysis,\nextraction of these terms would be useful for defining important features for\ndownstream tasks such as machine learning for precision medicine.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 01:42:29 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Choi", "Sanghyun", ""], ["Ivkin", "Nikita", ""], ["Braverman", "Vladimir", ""], ["Jacobs", "Michael A.", ""]]}, {"id": "1809.02670", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, Arye Nehorai", "title": "RetGK: Graph Kernels based on Return Probabilities of Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data arise in wide applications, such as computer vision,\nbioinformatics, and social networks. Quantifying similarities among graphs is a\nfundamental problem. In this paper, we develop a framework for computing graph\nkernels, based on return probabilities of random walks. The advantages of our\nproposed kernels are that they can effectively exploit various node attributes,\nwhile being scalable to large datasets. We conduct extensive graph\nclassification experiments to evaluate our graph kernels. The experimental\nresults show that our graph kernels significantly outperform existing\nstate-of-the-art approaches in both accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 20:57:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhang", "Zhen", ""], ["Wang", "Mianzhi", ""], ["Xiang", "Yijian", ""], ["Huang", "Yan", ""], ["Nehorai", "Arye", ""]]}, {"id": "1809.02707", "submitter": "Cem Tekin", "authors": "Alihan H\\\"uy\\\"uk and Cem Tekin", "title": "Analysis of Thompson Sampling for Combinatorial Multi-armed Bandit with\n  Probabilistically Triggered Arms", "comments": "To appear in the Proceedings of the 22nd International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the regret of combinatorial Thompson sampling (CTS) for the\ncombinatorial multi-armed bandit with probabilistically triggered arms under\nthe semi-bandit feedback setting. We assume that the learner has access to an\nexact optimization oracle but does not know the expected base arm outcomes\nbeforehand. When the expected reward function is Lipschitz continuous in the\nexpected base arm outcomes, we derive $O(\\sum_{i =1}^m \\log T / (p_i\n\\Delta_i))$ regret bound for CTS, where $m$ denotes the number of base arms,\n$p_i$ denotes the minimum non-zero triggering probability of base arm $i$ and\n$\\Delta_i$ denotes the minimum suboptimality gap of base arm $i$. We also\ncompare CTS with combinatorial upper confidence bound (CUCB) via numerical\nexperiments on a cascading bandit problem.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 23:14:44 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 13:08:48 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["H\u00fcy\u00fck", "Alihan", ""], ["Tekin", "Cem", ""]]}, {"id": "1809.02709", "submitter": "Liyu Gong", "authors": "Liyu Gong, Qiang Cheng", "title": "Exploiting Edge Features in Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge features contain important information about graphs. However, current\nstate-of-the-art neural network models designed for graph learning, e.g. graph\nconvolutional networks (GCN) and graph attention networks (GAT), adequately\nutilize edge features, especially multi-dimensional edge features. In this\npaper, we build a new framework for a family of new graph neural network models\nthat can more sufficiently exploit edge features, including those of undirected\nor multi-dimensional edges. The proposed framework can consolidate current\ngraph neural network models; e.g. graph convolutional networks (GCN) and graph\nattention networks (GAT). The proposed framework and new models have the\nfollowing novelties: First, we propose to use doubly stochastic normalization\nof graph edge features instead of the commonly used row or symmetric\nnormalization approches used in current graph neural networks. Second, we\nconstruct new formulas for the operations in each individual layer so that they\ncan handle multi-dimensional edge features. Third, for the proposed new\nframework, edge features are adaptive across network layers. As a result, our\nproposed new framework and new models can exploit a rich source of graph\ninformation. We apply our new models to graph node classification on several\ncitation networks, whole graph classification, and regression on several\nmolecular datasets. Compared with the current state-of-the-art methods, i.e.\nGCNs and GAT, our models obtain better performance, which testify to the\nimportance of exploiting edge features in graph neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 23:18:59 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 18:55:36 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gong", "Liyu", ""], ["Cheng", "Qiang", ""]]}, {"id": "1809.02721", "submitter": "Marcelo Prates", "authors": "Marcelo O. R. Prates, Pedro H. C. Avelar, Henrique Lemos, Luis Lamb,\n  Moshe Vardi", "title": "Learning to Solve NP-Complete Problems - A Graph Neural Network for\n  Decision TSP", "comments": "Accepted for presentation at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNN) are a promising technique for bridging\ndifferential programming and combinatorial domains. GNNs employ trainable\nmodules which can be assembled in different configurations that reflect the\nrelational structure of each problem instance. In this paper, we show that GNNs\ncan learn to solve, with very little supervision, the decision variant of the\nTraveling Salesperson Problem (TSP), a highly relevant $\\mathcal{NP}$-Complete\nproblem. Our model is trained to function as an effective message-passing\nalgorithm in which edges (embedded with their weights) communicate with\nvertices for a number of iterations after which the model is asked to decide\nwhether a route with cost $<C$ exists. We show that such a network can be\ntrained with sets of dual examples: given the optimal tour cost $C^{*}$, we\nproduce one decision instance with target cost $x\\%$ smaller and one with\ntarget cost $x\\%$ larger than $C^{*}$. We were able to obtain $80\\%$ accuracy\ntraining with $-2\\%,+2\\%$ deviations, and the same trained model can generalize\nfor more relaxed deviations with increasing performance. We also show that the\nmodel is capable of generalizing for larger problem sizes. Finally, we provide\na method for predicting the optimal route cost within $2\\%$ deviation from the\nground truth. In summary, our work shows that Graph Neural Networks are\npowerful enough to solve $\\mathcal{NP}$-Complete problems which combine\nsymbolic and numeric data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 00:11:51 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 18:02:19 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 12:10:20 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Prates", "Marcelo O. R.", ""], ["Avelar", "Pedro H. C.", ""], ["Lemos", "Henrique", ""], ["Lamb", "Luis", ""], ["Vardi", "Moshe", ""]]}, {"id": "1809.02727", "submitter": "Richeng Jin", "authors": "Richeng Jin, Xiaofan He and Huaiyu Dai", "title": "Decentralized Differentially Private Without-Replacement Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning has achieved remarkable results in a wide variety of\ndomains, the training of models often requires large datasets that may need to\nbe collected from different individuals. As sensitive information may be\ncontained in the individual's dataset, sharing training data may lead to severe\nprivacy concerns. Therefore, there is a compelling need to develop\nprivacy-aware machine learning methods, for which one effective approach is to\nleverage the generic framework of differential privacy. Considering that\nstochastic gradient descent (SGD) is one of the mostly adopted methods for\nlarge-scale machine learning problems, two decentralized differentially private\nSGD algorithms are proposed in this work. Particularly, we focus on SGD without\nreplacement due to its favorable structure for practical implementation. In\naddition, both privacy and convergence analysis are provided for the proposed\nalgorithms. Finally, extensive experiments are performed to verify the\ntheoretical results and demonstrate the effectiveness of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 01:10:59 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 23:21:35 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 19:01:59 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Jin", "Richeng", ""], ["He", "Xiaofan", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1809.02728", "submitter": "Daniel Smolyak", "authors": "Kathryn Gray, Daniel Smolyak, Sarkhan Badirli, George Mohler", "title": "Coupled IGMM-GANs for deep multimodal anomaly detection in human\n  mobility data", "comments": "Submitted and pending notification from AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalous activity in human mobility data has a number of\napplications including road hazard sensing, telematic based insurance, and\nfraud detection in taxi services and ride sharing. In this paper we address two\nchallenges that arise in the study of anomalous human trajectories: 1) a lack\nof ground truth data on what defines an anomaly and 2) the dependence of\nexisting methods on significant pre-processing and feature engineering. While\ngenerative adversarial networks seem like a natural fit for addressing these\nchallenges, we find that existing GAN based anomaly detection algorithms\nperform poorly due to their inability to handle multimodal patterns. For this\npurpose we introduce an infinite Gaussian mixture model coupled with\n(bi-directional) generative adversarial networks, IGMM-GAN, that is able to\ngenerate synthetic, yet realistic, human mobility data and simultaneously\nfacilitates multimodal anomaly detection. Through estimation of a generative\nprobability density on the space of human trajectories, we are able to generate\nrealistic synthetic datasets that can be used to benchmark existing anomaly\ndetection methods. The estimated multimodal density also allows for a natural\ndefinition of outlier that we use for detecting anomalous trajectories. We\nillustrate our methodology and its improvement over existing GAN anomaly\ndetection on several human mobility datasets, along with MNIST.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 01:11:10 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gray", "Kathryn", ""], ["Smolyak", "Daniel", ""], ["Badirli", "Sarkhan", ""], ["Mohler", "George", ""]]}, {"id": "1809.02740", "submitter": "Tim Leathart", "authors": "Tim Leathart, Eibe Frank, Bernhard Pfahringer, Geoffrey Holmes", "title": "Ensembles of Nested Dichotomies with Multiple Subset Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system of nested dichotomies is a method of decomposing a multi-class\nproblem into a collection of binary problems. Such a system recursively applies\nbinary splits to divide the set of classes into two subsets, and trains a\nbinary classifier for each split. Many methods have been proposed to perform\nthis split, each with various advantages and disadvantages. In this paper, we\npresent a simple, general method for improving the predictive performance of\nnested dichotomies produced by any subset selection techniques that employ\nrandomness to construct the subsets. We provide a theoretical expectation for\nperformance improvements, as well as empirical results showing that our method\nimproves the root mean squared error of nested dichotomies, regardless of\nwhether they are employed as an individual model or in an ensemble setting.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 02:10:52 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 02:26:07 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Leathart", "Tim", ""], ["Frank", "Eibe", ""], ["Pfahringer", "Bernhard", ""], ["Holmes", "Geoffrey", ""]]}, {"id": "1809.02744", "submitter": "Tim Leathart", "authors": "Tim Leathart, Eibe Frank, Bernhard Pfahringer, Geoffrey Holmes", "title": "On the Calibration of Nested Dichotomies for Large Multiclass Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested dichotomies are used as a method of transforming a multiclass\nclassification problem into a series of binary problems. A tree structure is\ninduced that recursively splits the set of classes into subsets, and a binary\nclassification model learns to discriminate between the two subsets of classes\nat each node. In this paper, we demonstrate that these nested dichotomies\ntypically exhibit poor probability calibration, even when the base binary\nmodels are well calibrated. We also show that this problem is exacerbated when\nthe binary models are poorly calibrated. We discuss the effectiveness of\ndifferent calibration strategies and show that accuracy and log-loss can be\nsignificantly improved by calibrating both the internal base models and the\nfull nested dichotomy structure, especially when the number of classes is high.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 02:24:42 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 02:22:15 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 22:15:49 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Leathart", "Tim", ""], ["Frank", "Eibe", ""], ["Pfahringer", "Bernhard", ""], ["Holmes", "Geoffrey", ""]]}, {"id": "1809.02745", "submitter": "Hiroshi Kajino", "authors": "Hiroshi Kajino", "title": "Molecular Hypergraph Grammar with its Application to Molecular\n  Optimization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular optimization aims to discover novel molecules with desirable\nproperties. Two fundamental challenges are: (i) it is not trivial to generate\nvalid molecules in a controllable way due to hard chemical constraints such as\nthe valency conditions, and (ii) it is often costly to evaluate a property of a\nnovel molecule, and therefore, the number of property evaluations is limited.\nThese challenges are to some extent alleviated by a combination of a\nvariational autoencoder (VAE) and Bayesian optimization (BO). VAE converts a\nmolecule into/from its latent continuous vector, and BO optimizes a latent\ncontinuous vector (and its corresponding molecule) within a limited number of\nproperty evaluations. While the most recent work, for the first time, achieved\n100% validity, its architecture is rather complex due to auxiliary neural\nnetworks other than VAE, making it difficult to train. This paper presents a\nmolecular hypergraph grammar variational autoencoder (MHG-VAE), which uses a\nsingle VAE to achieve 100% validity. Our idea is to develop a graph grammar\nencoding the hard chemical constraints, called molecular hypergraph grammar\n(MHG), which guides VAE to always generate valid molecules. We also present an\nalgorithm to construct MHG from a set of molecules.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 02:25:52 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 04:52:44 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Kajino", "Hiroshi", ""]]}, {"id": "1809.02786", "submitter": "Dan Peng", "authors": "Dan Peng, Zizhan Zheng, Xiaofeng Zhang", "title": "Structure-Preserving Transformation: Generating Diverse and Transferable\n  Adversarial Examples", "comments": "The AAAI-2019 Workshop on Artificial Intelligence for Cyber Security\n  (AICS)", "journal-ref": null, "doi": null, "report-no": "AICS/2019/09", "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Most recent works on adversarial examples for image classification\nfocus on directly modifying pixels with minor perturbations. A common\nrequirement in all these works is that the malicious perturbations should be\nsmall enough (measured by an L_p norm for some p) so that they are\nimperceptible to humans. However, small perturbations can be unnecessarily\nrestrictive and limit the diversity of adversarial examples generated. Further,\nan L_p norm based distance metric ignores important structure patterns hidden\nin images that are important to human perception. Consequently, even the minor\nperturbation introduced in recent works often makes the adversarial examples\nless natural to humans. More importantly, they often do not transfer well and\nare therefore less effective when attacking black-box models especially for\nthose protected by a defense mechanism. In this paper, we propose a\nstructure-preserving transformation (SPT) for generating natural and diverse\nadversarial examples with extremely high transferability. The key idea of our\napproach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Empirical\nresults on the MNIST and the fashion-MNIST datasets show that adversarial\nexamples generated by our approach can easily bypass strong adversarial\ntraining. Further, they transfer well to other target models with no loss or\nlittle loss of successful attack rate.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 10:26:50 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 15:42:00 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 09:07:32 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Peng", "Dan", ""], ["Zheng", "Zizhan", ""], ["Zhang", "Xiaofeng", ""]]}, {"id": "1809.02804", "submitter": "Zhi-Hua Zhou", "authors": "Peng Zhao, Le-Wen Cai, Zhi-Hua Zhou", "title": "Handling Concept Drift via Model Reuse", "comments": null, "journal-ref": "Machine Learning, 2020, 109(3): 533-568", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, data are often collected in the form of\nstream, and thus the distribution usually changes in nature, which is referred\nas concept drift in literature. We propose a novel and effective approach to\nhandle concept drift via model reuse, leveraging previous knowledge by reusing\nmodels. Each model is associated with a weight representing its reusability\ntowards current data, and the weight is adaptively adjusted according to the\nmodel performance. We provide generalization and regret analysis. Experimental\nresults also validate the superiority of our approach on both synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 14:13:46 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhao", "Peng", ""], ["Cai", "Le-Wen", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1809.02811", "submitter": "Zacarias Curi Filho", "authors": "Zacarias Curi and Alceu de Souza Britto Jr and Emerson Cabrera Paraiso", "title": "Multi-label Classification of User Reactions in Online News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in the number of Internet users and the strong interaction\nbrought by Web 2.0 made the Opinion Mining an important task in the area of\nnatural language processing. Although several methods are capable of performing\nthis task, few use multi-label classification, where there is a group of true\nlabels for each example. This type of classification is useful for situations\nwhere the opinions are analyzed from the perspective of the reader, this\nhappens because each person can have different interpretations and opinions on\nthe same subject. This paper discuss the efficiency of problem transformation\nmethods combined with different classification algorithms for the task of\nmulti-label classification of reactions in news texts. To do that, extensive\ntests were carried out on two news corpora written in Brazilian Portuguese\nannotated with reactions. A new corpus called BFRC-PT is presented. In the\ntests performed, the highest number of correct predictions was obtained with\nthe Classifier Chains method combined with the Random Forest algorithm. When\nconsidering the class distribution, the best results were obtained with the\nBinary Relevance method combined with the LSTM and Random Forest algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 14:47:26 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:33:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Curi", "Zacarias", ""], ["Britto", "Alceu de Souza", "Jr"], ["Paraiso", "Emerson Cabrera", ""]]}, {"id": "1809.02838", "submitter": "Linfeng Liu", "authors": "Linfeng Liu, Liping Liu", "title": "Non-Parametric Variational Inference with Graph Convolutional Networks\n  for Gaussian Processes", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for GP models with non-Gaussian noises is computationally expensive\nwhen dealing with large datasets. Many recent inference methods approximate the\nposterior distribution with a simpler distribution defined on a small number of\ninducing points. The inference is accurate only when data points have strong\ncorrelation with these inducing points. In this paper, we consider the\ninference problem in a different direction: GP function values in the posterior\nare mostly correlated in short distance. We construct a variational\ndistribution such that the inference for a data point considers only its\nneighborhood. With this construction, the variational lower bound is highly\ndecomposible, hence we can run stochastic optimization with very small batches.\nWe then train Graph Convolutional Networks as a reusable model to identify\nvariational parameters for each data point. Model reuse greatly reduces the\nnumber of parameters and the number of iterations needed in optimization. The\nproposed method significantly speeds up the inference and often gets more\naccurate results than previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 17:20:45 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liu", "Linfeng", ""], ["Liu", "Liping", ""]]}, {"id": "1809.02840", "submitter": "Lisa Zhang", "authors": "Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William E.\n  Byrd, Matthew Might, Raquel Urtasun, Richard Zemel", "title": "Neural Guided Constraint Logic Programming for Program Synthesis", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing programs using example input/outputs is a classic problem in\nartificial intelligence. We present a method for solving Programming By Example\n(PBE) problems by using a neural model to guide the search of a constraint\nlogic programming system called miniKanren. Crucially, the neural model uses\nminiKanren's internal representation as input; miniKanren represents a PBE\nproblem as recursive constraints imposed by the provided examples. We explore\nRecurrent Neural Network and Graph Neural Network models. We contribute a\nmodified miniKanren, drivable by an external agent, available at\nhttps://github.com/xuexue/neuralkanren. We show that our neural-guided approach\nusing constraints can synthesize programs faster in many cases, and\nimportantly, can generalize to larger problems.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 17:25:33 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 10:51:08 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 12:10:05 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhang", "Lisa", ""], ["Rosenblatt", "Gregory", ""], ["Fetaya", "Ethan", ""], ["Liao", "Renjie", ""], ["Byrd", "William E.", ""], ["Might", "Matthew", ""], ["Urtasun", "Raquel", ""], ["Zemel", "Richard", ""]]}, {"id": "1809.02855", "submitter": "Amr El-Wakeel", "authors": "Amr S. El-Wakeel, Aboelmagd Noureldin, Hossam S. Hassanein and Nizar\n  Zorba", "title": "iDriveSense: Dynamic Route Planning Involving Roads Quality Information", "comments": "Globecom 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the expeditious growth in the information and communication\ntechnologies, smart cities have raised the expectations in terms of efficient\nfunctioning and management. One key aspect of residents' daily comfort is\nassured through affording reliable traffic management and route planning.\nComprehensively, the majority of the present trip planning applications and\nservice providers are enabling their trip planning recommendations relying on\nshortest paths and/or fastest routes. However, such suggestions may discount\ndrivers' preferences with respect to safe and less disturbing trips. Road\nanomalies such as cracks, potholes, and manholes induce risky driving scenarios\nand can lead to vehicles damages and costly repairs. Accordingly, in this\npaper, we propose a crowdsensing based dynamic route planning system.\nLeveraging both the vehicle motion sensors and the inertial sensors within the\nsmart devices, road surface types and anomalies have been detected and\ncategorized. In addition, the monitored events are geo-referenced utilizing GPS\nreceivers on both vehicles and smart devices. Consequently, road segments\nassessments are conducted using fuzzy system models based on aspects such as\nthe number of anomalies and their severity levels in each road segment.\nAfterward, another fuzzy model is adopted to recommend the best trip routes\nbased on the road segments quality in each potential route. Extensive road\nexperiments are held to build and show the potential of the proposed system.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:21:09 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["El-Wakeel", "Amr S.", ""], ["Noureldin", "Aboelmagd", ""], ["Hassanein", "Hossam S.", ""], ["Zorba", "Nizar", ""]]}, {"id": "1809.02860", "submitter": "Lu Bai", "authors": "Lixin Cui, Lu Bai, Zhihong Zhang, Yue Wang, Edwin R. Hancock", "title": "Identifying The Most Informative Features Using A Structurally\n  Interacting Elastic Net", "comments": "Neurocomputing (Accept/In Press), ISSN: 0925-2312", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection can efficiently identify the most informative features with\nrespect to the target feature used in training. However, state-of-the-art\nvector-based methods are unable to encapsulate the relationships between\nfeature samples into the feature selection process, thus leading to significant\ninformation loss. To address this problem, we propose a new graph-based\nstructurally interacting elastic net method for feature selection.\nSpecifically, we commence by constructing feature graphs that can incorporate\npairwise relationship between samples. With the feature graphs to hand, we\npropose a new information theoretic criterion to measure the joint relevance of\ndifferent pairwise feature combinations with respect to the target feature\ngraph representation. This measure is used to obtain a structural interaction\nmatrix where the elements represent the proposed information theoretic measure\nbetween feature pairs. We then formulate a new optimization model through the\ncombination of the structural interaction matrix and an elastic net regression\nmodel for the feature subset selection problem. This allows us to a) preserve\nthe information of the original vectorial space, b) remedy the information loss\nof the original feature space caused by using graph representation, and c)\npromote a sparse solution and also encourage correlated features to be\nselected. Because the proposed optimization problem is non-convex, we develop\nan efficient alternating direction multiplier method (ADMM) to locate the\noptimal solutions. Extensive experiments on various datasets demonstrate the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:40:14 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Cui", "Lixin", ""], ["Bai", "Lu", ""], ["Zhang", "Zhihong", ""], ["Wang", "Yue", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1809.02861", "submitter": "Ambra Demontis Ph.D.", "authors": "Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista\n  Biggio, Alina Oprea, Cristina Nita-Rotaru, Fabio Roli", "title": "Why Do Adversarial Attacks Transfer? Explaining Transferability of\n  Evasion and Poisoning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferability captures the ability of an attack against a machine-learning\nmodel to be effective against a different, potentially unknown, model.\nEmpirical evidence for transferability has been shown in previous work, but the\nunderlying reasons why an attack transfers or not are not yet well understood.\nIn this paper, we present a comprehensive analysis aimed to investigate the\ntransferability of both test-time evasion and training-time poisoning attacks.\nWe provide a unifying optimization framework for evasion and poisoning attacks,\nand a formal definition of transferability of such attacks. We highlight two\nmain factors contributing to attack transferability: the intrinsic adversarial\nvulnerability of the target model, and the complexity of the surrogate model\nused to optimize the attack. Based on these insights, we define three metrics\nthat impact an attack's transferability. Interestingly, our results derived\nfrom theoretical analysis hold for both evasion and poisoning attacks, and are\nconfirmed experimentally using a wide range of linear and non-linear\nclassifiers and datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:44:47 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 01:15:43 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 08:52:37 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 10:01:26 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Demontis", "Ambra", ""], ["Melis", "Marco", ""], ["Pintor", "Maura", ""], ["Jagielski", "Matthew", ""], ["Biggio", "Battista", ""], ["Oprea", "Alina", ""], ["Nita-Rotaru", "Cristina", ""], ["Roli", "Fabio", ""]]}, {"id": "1809.02864", "submitter": "Kfir Levy Yehuda", "authors": "Kfir Y. Levy, Alp Yurtsever, Volkan Cevher", "title": "Online Adaptive Methods, Universality and Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for convex unconstrained optimization that, without\nany modifications, ensures: (i) accelerated convergence rate for smooth\nobjectives, (ii) standard convergence rate in the general (non-smooth) setting,\nand (iii) standard convergence rate in the stochastic optimization setting. To\nthe best of our knowledge, this is the first method that simultaneously applies\nto all of the above settings. At the heart of our method is an adaptive\nlearning rate rule that employs importance weights, in the spirit of adaptive\nonline learning algorithms (Duchi et al., 2011; Levy, 2017), combined with an\nupdate that linearly couples two sequences, in the spirit of (Allen-Zhu and\nOrecchia, 2017). An empirical examination of our method demonstrates its\napplicability to the above mentioned scenarios and corroborates our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 20:02:20 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Levy", "Kfir Y.", ""], ["Yurtsever", "Alp", ""], ["Cevher", "Volkan", ""]]}, {"id": "1809.02869", "submitter": "Pedram Daee", "authors": "Tomi Peltola, Mustafa Mert \\c{C}elikok, Pedram Daee, Samuel Kaski", "title": "Machine Teaching of Active Sequential Learners", "comments": "24 pages, 16 figures. This version focuses more on machine teaching\n  while the previous version focused more on human-computer interaction and\n  user modelling. The title has been updated accordingly. Code and data\n  available at\n  https://github.com/AaltoPML/machine-teaching-of-active-sequential-learners .\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine teaching addresses the problem of finding the best training data that\ncan guide a learning algorithm to a target model with minimal effort. In\nconventional settings, a teacher provides data that are consistent with the\ntrue data distribution. However, for sequential learners which actively choose\ntheir queries, such as multi-armed bandits and active learners, the teacher can\nonly provide responses to the learner's queries, not design the full data. In\nthis setting, consistent teachers can be sub-optimal for finite horizons. We\nformulate this sequential teaching problem, which current techniques in machine\nteaching do not address, as a Markov decision process, with the dynamics\nnesting a model of the learner and the actions being the teacher's responses.\nFurthermore, we address the complementary problem of learning from a teacher\nthat plans: to recognise the teaching intent of the responses, the learner is\nendowed with a model of the teacher. We test the formulation with multi-armed\nbandit learners in simulated experiments and a user study. The results show\nthat learning is improved by (i) planning teaching and (ii) the learner having\na model of the teacher. The approach gives tools to taking into account\nstrategic (planning) behaviour of users of interactive intelligent systems,\nsuch as recommendation engines, by considering them as boundedly optimal\nteachers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 20:39:31 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 10:40:50 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 06:30:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Peltola", "Tomi", ""], ["\u00c7elikok", "Mustafa Mert", ""], ["Daee", "Pedram", ""], ["Kaski", "Samuel", ""]]}, {"id": "1809.02880", "submitter": "Zachary Ross", "authors": "Zachary E. Ross, Yisong Yue, Men-Andrin Meier, Egill Hauksson, Thomas\n  H. Heaton", "title": "PhaseLink: A Deep Learning Approach to Seismic Phase Association", "comments": "9 figures", "journal-ref": null, "doi": "10.1029/2018JB016674", "report-no": null, "categories": "cs.LG physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic phase association is a fundamental task in seismology that pertains\nto linking together phase detections on different sensors that originate from a\ncommon earthquake. It is widely employed to detect earthquakes on permanent and\ntemporary seismic networks, and underlies most seismicity catalogs produced\naround the world. This task can be challenging because the number of sources is\nunknown, events frequently overlap in time, or can occur simultaneously in\ndifferent parts of a network. We present PhaseLink, a framework based on recent\nadvances in deep learning for grid-free earthquake phase association. Our\napproach learns to link phases together that share a common origin, and is\ntrained entirely on tens of millions of synthetic sequences of P- and S-wave\narrival times generated using a simple 1D velocity model. Our approach is\nsimple to implement for any tectonic regime, suitable for real-time processing,\nand can naturally incorporate errors in arrival time picks. Rather than tuning\na set of ad hoc hyperparameters to improve performance, PhaseLink can be\nimproved by simply adding examples of problematic cases to the training\ndataset. We demonstrate the state-of-the-art performance of PhaseLink on a\nchallenging recent sequence from southern California, and synthesized sequences\nfrom Japan designed to test the point at which the method fails. For the\nexamined datasets, PhaseLink can precisely associate P- and S-picks to events\nthat are separated by ~12 seconds in origin time. This approach is expected to\nimprove the resolution of seismicity catalogs, add stability to real-time\nseismic monitoring, and streamline automated processing of large seismic\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 21:39:29 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 20:44:10 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ross", "Zachary E.", ""], ["Yue", "Yisong", ""], ["Meier", "Men-Andrin", ""], ["Hauksson", "Egill", ""], ["Heaton", "Thomas H.", ""]]}, {"id": "1809.02918", "submitter": "Yali Du", "authors": "Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, Dacheng Tao", "title": "Towards Query Efficient Black-box Attacks: An Input-free Perspective", "comments": "Accepted by 11th ACM Workshop on Artificial Intelligence and Security\n  (AISec) with the 25th ACM Conference on Computer and Communications Security\n  (CCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have highlighted that deep neural networks (DNNs) are\nvulnerable to adversarial attacks, even in a black-box scenario. However, most\nof the existing black-box attack algorithms need to make a huge amount of\nqueries to perform attacks, which is not practical in the real world. We note\none of the main reasons for the massive queries is that the adversarial example\nis required to be visually similar to the original image, but in many cases,\nhow adversarial examples look like does not matter much. It inspires us to\nintroduce a new attack called \\emph{input-free} attack, under which an\nadversary can choose an arbitrary image to start with and is allowed to add\nperceptible perturbations on it. Following this approach, we propose two\ntechniques to significantly reduce the query complexity. First, we initialize\nan adversarial example with a gray color image on which every pixel has roughly\nthe same importance for the target model. Then we shrink the dimension of the\nattack space by perturbing a small region and tiling it to cover the input\nimage. To make our algorithm more effective, we stabilize a projected gradient\nascent algorithm with momentum, and also propose a heuristic approach for\nregion size selection. Through extensive experiments, we show that with only\n1,701 queries on average, we can perturb a gray image to any target class of\nImageNet with a 100\\% success rate on InceptionV3. Besides, our algorithm has\nsuccessfully defeated two real-world systems, the Clarifai food detection API\nand the Baidu Animal Identification API.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 03:49:06 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Du", "Yali", ""], ["Fang", "Meng", ""], ["Yi", "Jinfeng", ""], ["Cheng", "Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1809.02925", "submitter": "Jonathan Tompson", "authors": "Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey\n  Levine, Jonathan Tompson", "title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward\n  Bias in Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify two issues with the family of algorithms based on the Adversarial\nImitation Learning framework. The first problem is implicit bias present in the\nreward functions used in these algorithms. While these biases might work well\nfor some environments, they can also lead to sub-optimal behavior in others.\nSecondly, even though these algorithms can learn from few expert\ndemonstrations, they require a prohibitively large number of interactions with\nthe environment in order to imitate the expert for many real-world\napplications. In order to address these issues, we propose a new algorithm\ncalled Discriminator-Actor-Critic that uses off-policy Reinforcement Learning\nto reduce policy-environment interaction sample complexity by an average factor\nof 10. Furthermore, since our reward function is designed to be unbiased, we\ncan apply our algorithm to many problems without making any task-specific\nadjustments.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 05:37:25 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 17:27:25 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Agrawal", "Kumar Krishna", ""], ["Dwibedi", "Debidatta", ""], ["Levine", "Sergey", ""], ["Tompson", "Jonathan", ""]]}, {"id": "1809.02926", "submitter": "Liting Sun", "authors": "Liting Sun, Wei Zhan and Masayoshi Tomizuka", "title": "Probabilistic Prediction of Interactive Driving Behavior via\n  Hierarchical Inverse Reinforcement Learning", "comments": "ITSC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) are on the road. To safely and efficiently interact\nwith other road participants, AVs have to accurately predict the behavior of\nsurrounding vehicles and plan accordingly. Such prediction should be\nprobabilistic, to address the uncertainties in human behavior. Such prediction\nshould also be interactive, since the distribution over all possible\ntrajectories of the predicted vehicle depends not only on historical\ninformation, but also on future plans of other vehicles that interact with it.\nTo achieve such interaction-aware predictions, we propose a probabilistic\nprediction approach based on hierarchical inverse reinforcement learning (IRL).\nFirst, we explicitly consider the hierarchical trajectory-generation process of\nhuman drivers involving both discrete and continuous driving decisions. Based\non this, the distribution over all future trajectories of the predicted vehicle\nis formulated as a mixture of distributions partitioned by the discrete\ndecisions. Then we apply IRL hierarchically to learn the distributions from\nreal human demonstrations. A case study for the ramp-merging driving scenario\nis provided. The quantitative results show that the proposed approach can\naccurately predict both the discrete driving decisions such as yield or pass as\nwell as the continuous trajectories.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 05:44:16 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Sun", "Liting", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1809.02927", "submitter": "Jiachen Li", "authors": "Jiachen Li, Hengbo Ma, Wei Zhan and Masayoshi Tomizuka", "title": "Generic Probabilistic Interactive Situation Recognition and Prediction:\n  From Virtual to Real", "comments": "Accepted by The 21st IEEE International Conference on Intelligent\n  Transportation Systems (2018 IEEE ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust recognition and prediction of traffic situation plays an\nimportant role in autonomous driving, which is a prerequisite for risk\nassessment and effective decision making. Although there exist a lot of works\ndealing with modeling driver behavior of a single object, it remains a\nchallenge to make predictions for multiple highly interactive agents that react\nto each other simultaneously. In this work, we propose a generic probabilistic\nhierarchical recognition and prediction framework which employs a two-layer\nHidden Markov Model (TLHMM) to obtain the distribution of potential situations\nand a learning-based dynamic scene evolution model to sample a group of future\ntrajectories. Instead of predicting motions of a single entity, we propose to\nget the joint distribution by modeling multiple interactive agents as a whole\nsystem. Moreover, due to the decoupling property of the layered structure, our\nmodel is suitable for knowledge transfer from simulation to real world\napplications as well as among different traffic scenarios, which can reduce the\ncomputational efforts of training and the demand for a large data amount. A\ncase study of highway ramp merging scenario is demonstrated to verify the\neffectiveness and accuracy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 06:02:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1809.02963", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi", "title": "Variational Approximation Error in Bayesian Non-negative Matrix\n  Factorization", "comments": "21 pages. 1 table. Revision in Neural Networks", "journal-ref": "Neural Networks, Volume 126, June 2020, pp. 65-75", "doi": "10.1016/j.neunet.2020.03.009", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a knowledge discovery method that\nis used in many fields. Variational inference and Gibbs sampling methods for it\nare also wellknown. However, the variational approximation error has not been\nclarified yet, because NMF is not statistically regular and the prior\ndistribution used in variational Bayesian NMF (VBNMF) has zero or divergence\npoints. In this paper, using algebraic geometrical methods, we theoretically\nanalyze the difference in negative log evidence (a.k.a. free energy) between\nVBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the\nvariational posterior and the true posterior. We derive an upper bound for the\nlearning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF.\nBy using the upper bound, we find a lower bound for the approximation error,\nasymptotically. The result quantitatively shows how well the VBNMF algorithm\ncan approximate Bayesian NMF; the lower bound depends on the hyperparameters\nand the true nonnegative rank. A numerical experiment demonstrates the\ntheoretical result.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 12:56:37 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 11:17:07 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 12:31:34 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 11:00:13 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hayashi", "Naoki", ""]]}, {"id": "1809.03006", "submitter": "Alexei Botchkarev", "authors": "Alexei Botchkarev", "title": "Performance Metrics (Error Measures) in Machine Learning Regression,\n  Forecasting and Prognostics: Properties and Typology", "comments": null, "journal-ref": "Interdisciplinary Journal of Information, Knowledge, and\n  Management, 2019, 14, 45-79", "doi": "10.28945/4184", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance metrics (error measures) are vital components of the evaluation\nframeworks in various fields. The intention of this study was to overview of a\nvariety of performance metrics and approaches to their classification. The main\ngoal of the study was to develop a typology that will help to improve our\nknowledge and understanding of metrics and facilitate their selection in\nmachine learning regression, forecasting and prognostics. Based on the analysis\nof the structure of numerous performance metrics, we propose a framework of\nmetrics which includes four (4) categories: primary metrics, extended metrics,\ncomposite metrics, and hybrid sets of metrics. The paper identified three (3)\nkey components (dimensions) that determine the structure and properties of\nprimary metrics: method of determining point distance, method of normalization,\nmethod of aggregation of point distances over a data set.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 16:58:33 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Botchkarev", "Alexei", ""]]}, {"id": "1809.03008", "submitter": "Kai Xiao", "authors": "Kai Y. Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, Aleksander Madry", "title": "Training for Faster Adversarial Robustness Verification via Inducing\n  ReLU Stability", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the concept of co-design in the context of neural network\nverification. Specifically, we aim to train deep neural networks that not only\nare robust to adversarial perturbations but also whose robustness can be\nverified more easily. To this end, we identify two properties of network models\n- weight sparsity and so-called ReLU stability - that turn out to significantly\nimpact the complexity of the corresponding verification task. We demonstrate\nthat improving weight sparsity alone already enables us to turn computationally\nintractable verification problems into tractable ones. Then, improving ReLU\nstability leads to an additional 4-13x speedup in verification times. An\nimportant feature of our methodology is its \"universality,\" in the sense that\nit can be used with a broad range of training procedures and verification\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 17:10:16 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:58:07 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 21:04:31 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Xiao", "Kai Y.", ""], ["Tjeng", "Vincent", ""], ["Shafiullah", "Nur Muhammad", ""], ["Madry", "Aleksander", ""]]}, {"id": "1809.03018", "submitter": "Houtao Deng", "authors": "Houtao Deng, Ganesh Krishnan, Ji Chen, Dong Liang", "title": "Leveraging Elastic Demand for Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand variance can result in a mismatch between planned supply and actual\ndemand. Demand shaping strategies such as pricing can be used to shift elastic\ndemand to reduce the imbalance. In this work, we propose to consider elastic\ndemand in the forecasting phase. We present a method to reallocate the\nhistorical elastic demand to reduce variance, thus making forecasting and\nsupply planning more effective.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 18:19:10 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Deng", "Houtao", ""], ["Krishnan", "Ganesh", ""], ["Chen", "Ji", ""], ["Liang", "Dong", ""]]}, {"id": "1809.03019", "submitter": "Samet Oymak", "authors": "Samet Oymak", "title": "Stochastic Gradient Descent Learns State Equations with Nonlinear\n  Activations", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete time dynamical systems governed by the state equation\n$h_{t+1}=\\phi(Ah_t+Bu_t)$. Here $A,B$ are weight matrices, $\\phi$ is an\nactivation function, and $u_t$ is the input data. This relation is the backbone\nof recurrent neural networks (e.g. LSTMs) which have broad applications in\nsequential learning tasks. We utilize stochastic gradient descent to learn the\nweight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$. We\nprove that SGD estimate linearly converges to the ground truth weights while\nusing near-optimal sample size. Our results apply to increasing activations\nwhose derivatives are bounded away from zero. The analysis is based on i) a\nnovel SGD convergence result with nonlinear activations and ii) careful\nstatistical characterization of the state vector. Numerical experiments verify\nthe fast convergence of SGD on ReLU and leaky ReLU in consistence with our\ntheory.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 18:22:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Oymak", "Samet", ""]]}, {"id": "1809.03041", "submitter": "Denali Molitor", "authors": "Denali Molitor and Deanna Needell", "title": "An iterative method for classification of binary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's data driven world, storing, processing, and gleaning insights from\nlarge-scale data are major challenges. Data compression is often required in\norder to store large amounts of high-dimensional data, and thus, efficient\ninference methods for analyzing compressed data are necessary. Building on a\nrecently designed simple framework for classification using binary data, we\ndemonstrate that one can improve classification accuracy of this approach\nthrough iterative applications whose output serves as input to the next\napplication. As a side consequence, we show that the original framework can be\nused as a data preprocessing step to improve the performance of other methods,\nsuch as support vector machines. For several simple settings, we showcase the\nability to obtain theoretical guarantees for the accuracy of the iterative\nclassification method. The simplicity of the underlying classification\nframework makes it amenable to theoretical analysis and studying this approach\nwill hopefully serve as a step toward developing theory for more sophisticated\ndeep learning technologies.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 20:37:10 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Molitor", "Denali", ""], ["Needell", "Deanna", ""]]}, {"id": "1809.03045", "submitter": "Agniva Chowdhury", "authors": "Agniva Chowdhury, Jiasen Yang, Petros Drineas", "title": "Randomized Iterative Algorithms for Fisher Discriminant Analysis", "comments": "23 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher discriminant analysis (FDA) is a widely used method for classification\nand dimensionality reduction. When the number of predictor variables greatly\nexceeds the number of observations, one of the alternatives for conventional\nFDA is regularized Fisher discriminant analysis (RFDA). In this paper, we\npresent a simple, iterative, sketching-based algorithm for RFDA that comes with\nprovable accuracy guarantees when compared to the conventional approach. Our\nanalysis builds upon two simple structural results that boil down to randomized\nmatrix multiplication, a fundamental and well-understood primitive of\nrandomized linear algebra. We analyze the behavior of RFDA when the ridge\nleverage and the standard leverage scores are used to select predictor\nvariables and we prove that accurate approximations can be achieved by a sample\nwhose size depends on the effective degrees of freedom of the RFDA problem. Our\nresults yield significant improvements over existing approaches and our\nempirical evaluations support our theoretical analyses.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 21:10:30 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 04:51:12 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Chowdhury", "Agniva", ""], ["Yang", "Jiasen", ""], ["Drineas", "Petros", ""]]}, {"id": "1809.03048", "submitter": "Alexander Mamonov V", "authors": "Vladimir Druskin, Alexander V. Mamonov and Mikhail Zaslavsky", "title": "Distance preserving model order reduction of graph-Laplacians and\n  cluster analysis", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-Laplacians and their spectral embeddings play an important role in\nmultiple areas of machine learning. This paper is focused on graph-Laplacian\ndimension reduction for the spectral clustering of data as a primary\napplication. Spectral embedding provides a low-dimensional parametrization of\nthe data manifold which makes the subsequent task (e.g., clustering) much\neasier. However, despite reducing the dimensionality of data, the overall\ncomputational cost may still be prohibitive for large data sets due to two\nfactors. First, computing the partial eigendecomposition of the graph-Laplacian\ntypically requires a large Krylov subspace. Second, after the spectral\nembedding is complete, one still has to operate with the same number of data\npoints. For example, clustering of the embedded data is typically performed\nwith various relaxations of k-means which computational cost scales poorly with\nrespect to the size of data set. In this work, we switch the focus from the\nentire data set to a subset of graph vertices (target subset). We develop two\nnovel algorithms for such low-dimensional representation of the original graph\nthat preserves important global distances between the nodes of the target\nsubset. In particular, it allows to ensure that target subset clustering is\nconsistent with the spectral clustering of the full data set if one would\nperform such. That is achieved by a properly parametrized reduced-order model\n(ROM) of the graph-Laplacian that approximates accurately the diffusion\ntransfer function of the original graph for inputs and outputs restricted to\nthe target subset. Working with a small target subset reduces greatly the\nrequired dimension of Krylov subspace and allows to exploit the conventional\nalgorithms (like approximations of k-means) in the regimes when they are most\nrobust and efficient.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 22:04:15 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 17:48:12 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Druskin", "Vladimir", ""], ["Mamonov", "Alexander V.", ""], ["Zaslavsky", "Mikhail", ""]]}, {"id": "1809.03060", "submitter": "S\\\"oren Mindermann", "authors": "S\\\"oren Mindermann, Rohin Shah, Adam Gleave, Dylan Hadfield-Menell", "title": "Active Inverse Reward Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designers of AI agents often iterate on the reward function in a\ntrial-and-error process until they get the desired behavior, but this only\nguarantees good behavior in the training environment. We propose structuring\nthis process as a series of queries asking the user to compare between\ndifferent reward functions. Thus we can actively select queries for maximum\ninformativeness about the true reward. In contrast to approaches asking the\ndesigner for optimal behavior, this allows us to gather additional information\nby eliciting preferences between suboptimal behaviors. After each query, we\nneed to update the posterior over the true reward function from observing the\nproxy reward function chosen by the designer. The recently proposed Inverse\nReward Design (IRD) enables this. Our approach substantially outperforms IRD in\ntest environments. In particular, it can query the designer about\ninterpretable, linear reward functions and still infer non-linear ones.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 23:30:59 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 15:52:24 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 17:41:15 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Mindermann", "S\u00f6ren", ""], ["Shah", "Rohin", ""], ["Gleave", "Adam", ""], ["Hadfield-Menell", "Dylan", ""]]}, {"id": "1809.03062", "submitter": "Julius Berner", "authors": "Julius Berner, Philipp Grohs, Arnulf Jentzen", "title": "Analysis of the Generalization Error: Empirical Risk Minimization over\n  Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the\n  Numerical Approximation of Black-Scholes Partial Differential Equations", "comments": null, "journal-ref": "SIAM Journal on Mathematics of Data Science 2(3), 2020, pp.\n  631-657", "doi": "10.1137/19M125649X", "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of new classification and regression algorithms based on\nempirical risk minimization (ERM) over deep neural network hypothesis classes,\ncoined deep learning, revolutionized the area of artificial intelligence,\nmachine learning, and data analysis. In particular, these methods have been\napplied to the numerical solution of high-dimensional partial differential\nequations with great success. Recent simulations indicate that deep\nlearning-based algorithms are capable of overcoming the curse of dimensionality\nfor the numerical solution of Kolmogorov equations, which are widely used in\nmodels from engineering, finance, and the natural sciences. The present paper\nconsiders under which conditions ERM over a deep neural network hypothesis\nclass approximates the solution of a $d$-dimensional Kolmogorov equation with\naffine drift and diffusion coefficients and typical initial values arising from\nproblems in computational finance up to error $\\varepsilon$. We establish that,\nwith high probability over draws of training samples, such an approximation can\nbe achieved with both the size of the hypothesis class and the number of\ntraining samples scaling only polynomially in $d$ and $\\varepsilon^{-1}$. It\ncan be concluded that ERM over deep neural network hypothesis classes overcomes\nthe curse of dimensionality for the numerical solution of linear Kolmogorov\nequations with affine coefficients.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 23:50:37 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 15:33:20 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 12:46:12 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Berner", "Julius", ""], ["Grohs", "Philipp", ""], ["Jentzen", "Arnulf", ""]]}, {"id": "1809.03063", "submitter": "Mohammad Mahmoody", "authors": "Saeed Mahloujifar, Dimitrios I. Diochnos, Mohammad Mahmoody", "title": "The Curse of Concentration in Robust Learning: Evasion and Poisoning\n  Attacks from Concentration of Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern machine learning classifiers are shown to be vulnerable to\nadversarial perturbations of the instances. Despite a massive amount of work\nfocusing on making classifiers robust, the task seems quite challenging. In\nthis work, through a theoretical study, we investigate the adversarial risk and\nrobustness of classifiers and draw a connection to the well-known phenomenon of\nconcentration of measure in metric measure spaces. We show that if the metric\nprobability space of the test instance is concentrated, any classifier with\nsome initial constant error is inherently vulnerable to adversarial\nperturbations.\n  One class of concentrated metric probability spaces are the so-called Levy\nfamilies that include many natural distributions. In this special case, our\nattacks only need to perturb the test instance by at most $O(\\sqrt n)$ to make\nit misclassified, where $n$ is the data dimension. Using our general result\nabout Levy instance spaces, we first recover as special case some of the\npreviously proved results about the existence of adversarial examples. However,\nmany more Levy families are known (e.g., product distribution under the Hamming\ndistance) for which we immediately obtain new attacks that find adversarial\nexamples of distance $O(\\sqrt n)$.\n  Finally, we show that concentration of measure for product spaces implies the\nexistence of forms of \"poisoning\" attacks in which the adversary tampers with\nthe training data with the goal of degrading the classifier. In particular, we\nshow that for any learning algorithm that uses $m$ training examples, there is\nan adversary who can increase the probability of any \"bad property\" (e.g.,\nfailing on a particular test instance) that initially happens with\nnon-negligible probability to $\\approx 1$ by substituting only $\\tilde{O}(\\sqrt\nm)$ of the examples with other (still correctly labeled) examples.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 23:57:29 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 04:31:04 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Mahloujifar", "Saeed", ""], ["Diochnos", "Dimitrios I.", ""], ["Mahmoody", "Mohammad", ""]]}, {"id": "1809.03073", "submitter": "Bryon Aragam", "authors": "Chen Dan, Liu Leqi, Bryon Aragam, Pradeep Ravikumar, Eric P. Xing", "title": "Sample Complexity of Nonparametric Semi-Supervised Learning", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of semi-supervised learning (SSL) and\nintroduce new assumptions based on the mismatch between a mixture model learned\nfrom unlabeled data and the true mixture model induced by the (unknown) class\nconditional distributions. Under these assumptions, we establish an\n$\\Omega(K\\log K)$ labeled sample complexity bound without imposing parametric\nassumptions, where $K$ is the number of classes. Our results suggest that even\nin nonparametric settings it is possible to learn a near-optimal classifier\nusing only a few labeled samples. Unlike previous theoretical work which\nfocuses on binary classification, we consider general multiclass classification\n($K>2$), which requires solving a difficult permutation learning problem. This\npermutation defines a classifier whose classification error is controlled by\nthe Wasserstein distance between mixing measures, and we provide finite-sample\nresults characterizing the behaviour of the excess risk of this classifier.\nFinally, we describe three algorithms for computing these estimators based on a\nconnection to bipartite graph matching, and perform experiments to illustrate\nthe superiority of the MLE over the majority vote estimator.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 01:12:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Dan", "Chen", ""], ["Leqi", "Liu", ""], ["Aragam", "Bryon", ""], ["Ravikumar", "Pradeep", ""], ["Xing", "Eric P.", ""]]}, {"id": "1809.03084", "submitter": "Kohei Yata", "authors": "Yusuke Narita, Shota Yasui, Kohei Yata", "title": "Efficient Counterfactual Learning from Bandit Feedback", "comments": "accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the most statistically efficient way to do off-policy evaluation and\noptimization with batch data from bandit feedback? For log data generated by\ncontextual bandit algorithms, we consider offline estimators for the expected\nreward from a counterfactual policy. Our estimators are shown to have lowest\nvariance in a wide class of estimators, achieving variance reduction relative\nto standard estimators. We then apply our estimators to improve advertisement\ndesign by a major advertisement company. Consistent with the theoretical\nresult, our estimators allow us to improve on the existing bandit algorithm\nwith more statistical confidence compared to a state-of-the-art benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:08:14 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 21:07:33 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 23:41:04 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Narita", "Yusuke", ""], ["Yasui", "Shota", ""], ["Yata", "Kohei", ""]]}, {"id": "1809.03090", "submitter": "Jason Klusowski M", "authors": "Andrew R. Barron, Jason M. Klusowski", "title": "Approximation and Estimation for High-Dimensional Deep Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been experimentally observed in recent years that multi-layer\nartificial neural networks have a surprising ability to generalize, even when\ntrained with far more parameters than observations. Is there a theoretical\nbasis for this? The best available bounds on their metric entropy and\nassociated complexity measures are essentially linear in the number of\nparameters, which is inadequate to explain this phenomenon. Here we examine the\nstatistical risk (mean squared predictive error) of multi-layer networks with\n$\\ell^1$-type controls on their parameters and with ramp activation functions\n(also called lower-rectified linear units). In this setting, the risk is shown\nto be upper bounded by $[(L^3 \\log d)/n]^{1/2}$, where $d$ is the input\ndimension to each layer, $L$ is the number of layers, and $n$ is the sample\nsize. In this way, the input dimension can be much larger than the sample size\nand the estimator can still be accurate, provided the target function has such\n$\\ell^1$ controls and that the sample size is at least moderately large\ncompared to $L^3\\log d$. The heart of the analysis is the development of a\nsampling strategy that demonstrates the accuracy of a sparse covering of deep\nramp networks. Lower bounds show that the identified risk is close to being\noptimal.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:21:40 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 17:49:00 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Barron", "Andrew R.", ""], ["Klusowski", "Jason M.", ""]]}, {"id": "1809.03113", "submitter": "Changyou Chen", "authors": "Bai Li and Changyou Chen and Wenlin Wang and Lawrence Carin", "title": "Certified Adversarial Robustness with Additive Noise", "comments": "NIPS 2019; Code: https://github.com/Bai-Li/STN-Code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of adversarial data examples has drawn significant attention in\nthe deep-learning community; such data are seemingly minimally perturbed\nrelative to the original data, but lead to very different outputs from a\ndeep-learning algorithm. Although a significant body of work on developing\ndefensive models has been considered, most such models are heuristic and are\noften vulnerable to adaptive attacks. Defensive methods that provide\ntheoretical robustness guarantees have been studied intensively, yet most fail\nto obtain non-trivial robustness when a large-scale model and data are present.\nTo address these limitations, we introduce a framework that is scalable and\nprovides certified bounds on the norm of the input manipulation for\nconstructing adversarial examples. We establish a connection between robustness\nagainst adversarial perturbation and additive random noise, and propose a\ntraining strategy that can significantly improve the certified bounds. Our\nevaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is\nscalable to complicated models and large data sets, while providing competitive\nrobustness to state-of-the-art provable defense methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 03:03:06 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 00:59:24 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 21:05:10 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 03:35:55 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 01:12:08 GMT"}, {"version": "v6", "created": "Sun, 10 Nov 2019 06:24:53 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Li", "Bai", ""], ["Chen", "Changyou", ""], ["Wang", "Wenlin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1809.03137", "submitter": "Zhen He", "authors": "Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber", "title": "Tracking by Animation: Unsupervised Learning of Multi-Object Attentive\n  Trackers", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Multi-Object Tracking (MOT) from videos is a challenging computer\nvision task which has been extensively studied for decades. Most of the\nexisting MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm\ncombined with popular machine learning approaches which largely reduce the\nhuman effort to tune algorithm parameters. However, the commonly used\nsupervised learning approaches require the labeled data (e.g., bounding boxes),\nwhich is expensive for videos. Also, the TBD framework is usually suboptimal\nsince it is not end-to-end, i.e., it considers the task as detection and\ntracking, but not jointly. To achieve both label-free and end-to-end learning\nof MOT, we propose a Tracking-by-Animation framework, where a differentiable\nneural model first tracks objects from input frames and then animates these\nobjects into reconstructed frames. Learning is then driven by the\nreconstruction error through backpropagation. We further propose a\nReprioritized Attentive Tracking to improve the robustness of data association.\nExperiments conducted on both synthetic and real video datasets show the\npotential of the proposed model. Our project page is publicly available at:\nhttps://github.com/zhen-he/tracking-by-animation\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 04:59:25 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 05:39:56 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 02:02:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["He", "Zhen", ""], ["Li", "Jian", ""], ["Liu", "Daxue", ""], ["He", "Hangen", ""], ["Barber", "David", ""]]}, {"id": "1809.03140", "submitter": "Venkateswararao Cherukuri", "authors": "Venkateswararao Cherukuri, Tiantong Guo, Steven J. Schiff, Vishal\n  Monga", "title": "Deep MR Image Super-Resolution Using Structural Priors", "comments": "Accepted to IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution magnetic resonance (MR) images are desired for accurate\ndiagnostics. In practice, image resolution is restricted by factors like\nhardware, cost and processing constraints. Recently, deep learning methods have\nbeen shown to produce compelling state of the art results for image\nsuper-resolution. Paying particular attention to desired hi-resolution MR image\nstructure, we propose a new regularized network that exploits image priors,\nnamely a low-rank structure and a sharpness prior to enhance deep MR image\nsuperresolution. Our contributions are then incorporating these priors in an\nanalytically tractable fashion in the learning of a convolutional neural\nnetwork (CNN) that accomplishes the super-resolution task. This is particularly\nchallenging for the low rank prior, since the rank is not a differentiable\nfunction of the image matrix (and hence the network parameters), an issue we\naddress by pursuing differentiable approximations of the rank. Sharpness is\nemphasized by the variance of the Laplacian which we show can be implemented by\na fixed {\\em feedback} layer at the output of the network. Experiments\nperformed on two publicly available MR brain image databases exhibit promising\nresults particularly when training imagery is limited.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:20:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Cherukuri", "Venkateswararao", ""], ["Guo", "Tiantong", ""], ["Schiff", "Steven J.", ""], ["Monga", "Vishal", ""]]}, {"id": "1809.03149", "submitter": "Weixun Wang", "authors": "Weixun Wang, Junqi Jin, Jianye Hao, Chunjie Chen, Chuan Yu, Weinan\n  Zhang, Jun Wang, Xiaotian Hao, Yixi Wang, Han Li, Jian Xu, Kun Gai", "title": "Learning Adaptive Display Exposure for Real-Time Advertising", "comments": "accepted by CIKM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In E-commerce advertising, where product recommendations and product ads are\npresented to users simultaneously, the traditional setting is to display ads at\nfixed positions. However, under such a setting, the advertising system loses\nthe flexibility to control the number and positions of ads, resulting in\nsub-optimal platform revenue and user experience. Consequently, major\ne-commerce platforms (e.g., Taobao.com) have begun to consider more flexible\nways to display ads. In this paper, we investigate the problem of advertising\nwith adaptive exposure: can we dynamically determine the number and positions\nof ads for each user visit under certain business constraints so that the\nplatform revenue can be increased? More specifically, we consider two types of\nconstraints: request-level constraint ensures user experience for each user\nvisit, and platform-level constraint controls the overall platform monetization\nrate. We model this problem as a Constrained Markov Decision Process with\nper-state constraint (psCMDP) and propose a constrained two-level reinforcement\nlearning approach to decompose the original problem into two relatively\nindependent sub-problems. To accelerate policy learning, we also devise a\nconstrained hindsight experience replay mechanism. Experimental evaluations on\nindustry-scale real-world datasets demonstrate the merits of our approach in\nboth obtaining higher revenue under the constraints and the effectiveness of\nthe constrained hindsight experience replay mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 06:15:42 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 01:55:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Weixun", ""], ["Jin", "Junqi", ""], ["Hao", "Jianye", ""], ["Chen", "Chunjie", ""], ["Yu", "Chuan", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""], ["Hao", "Xiaotian", ""], ["Wang", "Yixi", ""], ["Li", "Han", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""]]}, {"id": "1809.03185", "submitter": "Francesco La Rosa", "authors": "Francesco La Rosa, M\\'ario Jo\\~ao Fartaria, Tobias Kober, Jonas\n  Richiardi, Cristina Granziera, Jean-Philippe Thiran, Meritxell Bach Cuadra", "title": "Shallow vs deep learning architectures for white matter lesion\n  segmentation in the early stages of multiple sclerosis", "comments": "Accepted to the MICCAI 2018 Brain Lesion (BrainLes) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a comparison of a shallow and a deep learning\narchitecture for the automated segmentation of white matter lesions in MR\nimages of multiple sclerosis patients. In particular, we train and test both\nmethods on early stage disease patients, to verify their performance in\nchallenging conditions, more similar to a clinical setting than what is\ntypically provided in multiple sclerosis segmentation challenges. Furthermore,\nwe evaluate a prototype naive combination of the two methods, which refines the\nfinal segmentation. All methods were trained on 32 patients, and the evaluation\nwas performed on a pure test set of 73 cases. Results show low lesion-wise\nfalse positives (30%) for the deep learning architecture, whereas the shallow\narchitecture yields the best Dice coefficient (63%) and volume difference\n(19%). Combining both shallow and deep architectures further improves the\nlesion-wise metrics (69% and 26% lesion-wise true and false positive rate,\nrespectively).\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:50:34 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["La Rosa", "Francesco", ""], ["Fartaria", "M\u00e1rio Jo\u00e3o", ""], ["Kober", "Tobias", ""], ["Richiardi", "Jonas", ""], ["Granziera", "Cristina", ""], ["Thiran", "Jean-Philippe", ""], ["Cuadra", "Meritxell Bach", ""]]}, {"id": "1809.03207", "submitter": "Jessa Bekker", "authors": "Jessa Bekker, Pieter Robberechts, and Jesse Davis", "title": "Beyond the Selected Completely At Random Assumption for Learning from\n  Positive and Unlabeled Data", "comments": null, "journal-ref": "European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases 2019 (ECMLPKDD 2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most positive and unlabeled data is subject to selection biases. The labeled\nexamples can, for example, be selected from the positive set because they are\neasier to obtain or more obviously positive. This paper investigates how\nlearning can be ena BHbled in this setting. We propose and theoretically\nanalyze an empirical-risk-based method for incorporating the labeling\nmechanism. Additionally, we investigate under which assumptions learning is\npossible when the labeling mechanism is not fully understood and propose a\npractical method to enable this. Our empirical analysis supports the\ntheoretical results and shows that taking into account the possibility of a\nselection bias, even when the labeling mechanism is unknown, improves the\ntrained classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 09:23:32 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 19:23:38 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 05:36:10 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 09:41:04 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Bekker", "Jessa", ""], ["Robberechts", "Pieter", ""], ["Davis", "Jesse", ""]]}, {"id": "1809.03214", "submitter": "Karl Kurzer", "authors": "Peter Wolf, Karl Kurzer, Tobias Wingert, Florian Kuhnt, J. Marius\n  Z\\\"ollner", "title": "Adaptive Behavior Generation for Autonomous Driving using Deep\n  Reinforcement Learning with Compact Semantic States", "comments": null, "journal-ref": null, "doi": "10.1109/IVS.2018.8500427", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making the right decision in traffic is a challenging task that is highly\ndependent on individual preferences as well as the surrounding environment.\nTherefore it is hard to model solely based on expert knowledge. In this work we\nuse Deep Reinforcement Learning to learn maneuver decisions based on a compact\nsemantic state representation. This ensures a consistent model of the\nenvironment across scenarios as well as a behavior adaptation function,\nenabling on-line changes of desired behaviors without re-training. The input\nfor the neural network is a simulated object list similar to that of Radar or\nLidar sensors, superimposed by a relational semantic scene description. The\nstate as well as the reward are extended by a behavior adaptation function and\na parameterization respectively. With little expert knowledge and a set of\nmid-level actions, it can be seen that the agent is capable to adhere to\ntraffic rules and learns to drive safely in a variety of situations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 09:35:27 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wolf", "Peter", ""], ["Kurzer", "Karl", ""], ["Wingert", "Tobias", ""], ["Kuhnt", "Florian", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "1809.03267", "submitter": "Sebastian Bischoff", "authors": "Sebastian Bischoff", "title": "Feature Learning for Meta-Paths in Knowledge Graphs", "comments": "Bachelor's Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we study the problem of feature learning on heterogeneous\nknowledge graphs. These features can be used to perform tasks such as link\nprediction, classification and clustering on graphs. Knowledge graphs provide\nrich semantics encoded in the edge and node types. Meta-paths consist of these\ntypes and abstract paths in the graph. Until now, meta-paths can only be used\nas categorical features with high redundancy and are therefore unsuitable for\nmachine learning models. We propose meta-path embeddings to solve this problem\nby learning semantical and compact vector representations of them. Current\ngraph embedding methods only embed nodes and edge types and therefore miss\nsemantics encoded in the combination of them. Our method embeds meta-paths\nusing the skipgram model with an extension to deal with the redundancy and high\namount of meta-paths in big knowledge graphs. We critically evaluate our\nembedding approach by predicting links on Wikidata. The experiments indicate\nthat we learn a sensible embedding of the meta-paths but can improve it\nfurther.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 09:31:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Bischoff", "Sebastian", ""]]}, {"id": "1809.03272", "submitter": "Le Trieu Phong", "authors": "Le Trieu Phong and Tran Thi Phuong", "title": "Privacy-Preserving Deep Learning via Weight Transmission", "comments": "Full version of a conference paper at NSS 2017", "journal-ref": "IEEE Transactions on Information Forensics and Security (Volume:\n  14, Issue: 11, Nov. 2019)", "doi": "10.1109/TIFS.2019.2911169", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the scenario that multiple data owners wish to apply a\nmachine learning method over the combined dataset of all owners to obtain the\nbest possible learning output but do not want to share the local datasets owing\nto privacy concerns. We design systems for the scenario that the stochastic\ngradient descent (SGD) algorithm is used as the machine learning method because\nSGD (or its variants) is at the heart of recent deep learning techniques over\nneural networks. Our systems differ from existing systems in the following\nfeatures: {\\bf (1)} any activation function can be used, meaning that no\nprivacy-preserving-friendly approximation is required; {\\bf (2)} gradients\ncomputed by SGD are not shared but the weight parameters are shared instead;\nand {\\bf (3)} robustness against colluding parties even in the extreme case\nthat only one honest party exists. We prove that our systems, while\nprivacy-preserving, achieve the same learning accuracy as SGD and hence retain\nthe merit of deep learning with respect to accuracy. Finally, we conduct\nseveral experiments using benchmark datasets, and show that our systems\noutperform previous system in terms of learning accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:36:05 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 06:43:25 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 06:44:53 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Phong", "Le Trieu", ""], ["Phuong", "Tran Thi", ""]]}, {"id": "1809.03291", "submitter": "Elena Smirnova", "authors": "Elena Smirnova", "title": "Action-conditional Sequence Modeling for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many online applications interactions between a user and a web-service are\norganized in a sequential way, e.g., user browsing an e-commerce website. In\nthis setting, recommendation system acts throughout user navigation by showing\nitems. Previous works have addressed this recommendation setup through the task\nof predicting the next item user will interact with. In particular, Recurrent\nNeural Networks (RNNs) has been shown to achieve substantial improvements over\ncollaborative filtering baselines. In this paper, we consider interactions\ntriggered by the recommendations of deployed recommender system in addition to\nbrowsing behavior. Indeed, it is reported that in online services interactions\nwith recommendations represent up to 30\\% of total interactions. Moreover, in\npractice, recommender system can greatly influence user behavior by promoting\nspecific items. In this paper, we extend the RNN modeling framework by taking\ninto account user interaction with recommended items. We propose and evaluate\nRNN architectures that consist of the recommendation action module and the\nstate-action fusion module. Using real-world large-scale datasets we\ndemonstrate improved performance on the next item prediction task compared to\nthe baselines.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:37:30 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Smirnova", "Elena", ""]]}, {"id": "1809.03306", "submitter": "Kuntoro Adi Nugroho", "authors": "Kuntoro Adi Nugroho", "title": "A Comparison of Handcrafted and Deep Neural Network Feature Extraction\n  for Classifying Optical Coherence Tomography (OCT) Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Coherence Tomography allows ophthalmologist to obtain cross-section\nimaging of eye retina. Assisted with digital image analysis methods, effective\ndisease detection could be performed. Various methods exist to extract feature\nfrom OCT images. The proposed study aims to compare the effectiveness of\nhandcrafted and deep neural network features. The evaluated dataset consist of\n32339 instances distributed in four classes, namely CNV, DME, DRUSEN, and\nNORMAL. The methods are Histogram of Oriented Gradient (HOG), Local Binary\nPattern (LBP), DenseNet-169, and ResNet50. As a result, the deep neural network\nbased methods outperformed the handcrafted feature with 88% and 89% accuracy\nfor DenseNet and ResNet compared to 50 % and 42 % for HOG and LBP respectively.\nThe deep neural network based methods also demonstrated better result on the\nunder represented class.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 03:18:17 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Nugroho", "Kuntoro Adi", ""]]}, {"id": "1809.03316", "submitter": "Farzaneh Mahdisoltani", "authors": "Farzaneh Mahdisoltani, Roland Memisevic, David Fleet", "title": "Hierarchical Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:29:06 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Mahdisoltani", "Farzaneh", ""], ["Memisevic", "Roland", ""], ["Fleet", "David", ""]]}, {"id": "1809.03322", "submitter": "Jonathan Heras", "authors": "\\'Angela Casado and J\\'onathan Heras", "title": "Guiding the Creation of Deep Learning-based Object Detectors", "comments": "To be published in I Workshop en Deep Learning of the CAEPIA\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 07:07:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Casado", "\u00c1ngela", ""], ["Heras", "J\u00f3nathan", ""]]}, {"id": "1809.03323", "submitter": "Michael Lash", "authors": "Michael T. Lash and Min Zhang and Xun Zhou and W. Nick Street and\n  Charles F. Lynch", "title": "Deriving Enhanced Geographical Representations via Similarity-based\n  Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.04714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are capable of learning rich, nonlinear feature\nrepresentations shown to be beneficial in many predictive tasks. In this work,\nwe use such models to explore different geographical feature representations in\nthe context of predicting colorectal cancer survival curves for patients in the\nstate of Iowa, spanning the years 1989 to 2013. Specifically, we compare model\nperformance using \"area between the curves\" (ABC) to assess (a) whether\nsurvival curves can be reasonably predicted for colorectal cancer patients in\nthe state of Iowa, (b) whether geographical features improve predictive\nperformance, (c) whether a simple binary representation, or a richer, spectral\nanalysis-elicited representation perform better, and (d) whether spectral\nanalysis-based representations can be improved upon by leveraging\ngeographically-descriptive features. In exploring (d), we devise a\nsimilarity-based spectral analysis procedure, which allows for the combination\nof geographically relational and geographically descriptive features. Our\nfindings suggest that survival curves can be reasonably estimated on average,\nwith predictive performance deviating at the five-year survival mark among all\nmodels. We also find that geographical features improve predictive performance,\nand that better performance is obtained using richer, spectral\nanalysis-elicited features. Furthermore, we find that similarity-based spectral\nanalysis-elicited representations improve upon the original spectral analysis\nresults by approximately 40%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:04:33 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lash", "Michael T.", ""], ["Zhang", "Min", ""], ["Zhou", "Xun", ""], ["Street", "W. Nick", ""], ["Lynch", "Charles F.", ""]]}, {"id": "1809.03343", "submitter": "Wenqing Li", "authors": "Wenqing Li, Chunhui Zhao, Biao Huang", "title": "Distributed dynamic modeling and monitoring for large-scale industrial\n  processes under closed-loop control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale industrial processes under closed-loop control, process\ndynamics directly resulting from control action are typical characteristics and\nmay show different behaviors between real faults and normal changes of\noperating conditions. However, conventional distributed monitoring approaches\ndo not consider the closed-loop control mechanism and only explore static\ncharacteristics, which thus are incapable of distinguishing between real\nprocess faults and nominal changes of operating conditions, leading to\nunnecessary alarms. In this regard, this paper proposes a distributed\nmonitoring method for closed-loop industrial processes by concurrently\nexploring static and dynamic characteristics. First, the large-scale\nclosed-loop process is decomposed into several subsystems by developing a\nsparse slow feature analysis (SSFA) algorithm which capture changes of both\nstatic and dynamic information. Second, distributed models are developed to\nseparately capture static and dynamic characteristics from the local and global\naspects. Based on the distributed monitoring system, a two-level monitoring\nstrategy is proposed to check different influences on process characteristics\nresulting from changes of the operating conditions and control action, and thus\nthe two changes can be well distinguished from each other. Case studies are\nconducted based on both benchmark data and real industrial process data to\nillustrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 06:06:54 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Li", "Wenqing", ""], ["Zhao", "Chunhui", ""], ["Huang", "Biao", ""]]}, {"id": "1809.03363", "submitter": "Ethan Harris", "authors": "Ethan Harris, Matthew Painter and Jonathon Hare", "title": "Torchbearer: A Model Fitting Library for PyTorch", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce torchbearer, a model fitting library for pytorch aimed at\nresearchers working on deep learning or differentiable programming. The\ntorchbearer library provides a high level metric and callback API that can be\nused for a wide range of applications. We also include a series of built in\ncallbacks that can be used for: model persistence, learning rate decay,\nlogging, data visualization and more. The extensive documentation includes an\nexample library for deep learning and dynamic programming problems and can be\nfound at http://torchbearer.readthedocs.io. The code is licensed under the MIT\nLicense and available at https://github.com/ecs-vlc/torchbearer.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:46:35 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Harris", "Ethan", ""], ["Painter", "Matthew", ""], ["Hare", "Jonathon", ""]]}, {"id": "1809.03368", "submitter": "Jorn Peters", "authors": "Jorn W.T. Peters and Max Welling", "title": "Probabilistic Binary Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low bit-width weights and activations are an effective way of combating the\nincreasing need for both memory and compute power of Deep Neural Networks. In\nthis work, we present a probabilistic training method for Neural Network with\nboth binary weights and activations, called BLRNet. By embracing stochasticity\nduring training, we circumvent the need to approximate the gradient of\nnon-differentiable functions such as sign(), while still obtaining a fully\nBinary Neural Network at test time. Moreover, it allows for anytime ensemble\npredictions for improved performance and uncertainty estimates by sampling from\nthe weight distribution. Since all operations in a layer of the BLRNet operate\non random variables, we introduce stochastic versions of Batch Normalization\nand max pooling, which transfer well to a deterministic network at test time.\nWe evaluate the BLRNet on multiple standardized benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:51:08 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Peters", "Jorn W. T.", ""], ["Welling", "Max", ""]]}, {"id": "1809.03371", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain", "title": "Revisiting Inaccuracies of Time Series Averaging under Dynamic Time\n  Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article revisits an analysis on inaccuracies of time series averaging\nunder dynamic time warping conducted by \\cite{Niennattrakul2007}. The authors\npresented a correctness-criterion and introduced drift-outs of averages from\nclusters. They claimed that averages are inaccurate if they are incorrect or\ndrift-outs. Furthermore, they conjectured that such inaccuracies are caused by\nthe lack of triangle inequality. We show that a rectified version of the\ncorrectness-criterion is unsatisfiable and that the concept of drift-out is\ngeometrically and operationally inconclusive. Satisfying the triangle\ninequality is insufficient to achieve correctness and unnecessary to overcome\nthe drift-out phenomenon. We place the concept of drift-out on a principled\nbasis and show that sample means as global minimizers of a Fr\\'echet function\nnever drift out. The adjusted drift-out is a way to test to which extent an\napproximation is coherent. Empirical results show that solutions obtained by\nthe state-of-the-art methods SSG and DBA are incoherent approximations of a\nsample mean in over a third of all trials.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 15:32:28 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Jain", "Brijnesh", ""]]}, {"id": "1809.03385", "submitter": "Dicong Qiu", "authors": "Dicong Qiu", "title": "SPASS: Scientific Prominence Active Search System with Deep Image\n  Captioning Network", "comments": "9 pages, 5 figures, 1 table. Preprint. Work in progress", "journal-ref": "Planetary and Space Science, 2020, 118: 104943", "doi": "10.1016/j.pss.2020.104943", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Planetary exploration missions with Mars rovers are complicated, which\ngenerally require elaborated task planning by human experts, from the path to\ntake to the images to capture. NASA has been using this process to acquire over\n22 million images from the planet Mars. In order to improve the degree of\nautomation and thus efficiency in this process, we propose a system for\nplanetary rovers to actively search for prominence of prespecified scientific\nfeatures in captured images. Scientists can prespecify such search tasks in\nnatural language and upload them to a rover, on which the deployed system\nconstantly captions captured images with a deep image captioning network and\ncompare the auto-generated captions to the prespecified search tasks by certain\nmetrics so as to prioritize those images for transmission. As a beneficial side\neffect, the proposed system can also be deployed to ground-based planetary data\nsystems as a content-based search engine.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:18:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Qiu", "Dicong", ""]]}, {"id": "1809.03400", "submitter": "Hoda Heidari", "authors": "Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause", "title": "A Moral Framework for Understanding of Fair ML through Economic Models\n  of Equality of Opportunity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.TH stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We map the recently proposed notions of algorithmic fairness to economic\nmodels of Equality of opportunity (EOP)---an extensively studied ideal of\nfairness in political philosophy. We formally show that through our conceptual\nmapping, many existing definition of algorithmic fairness, such as predictive\nvalue parity and equality of odds, can be interpreted as special cases of EOP.\nIn this respect, our work serves as a unifying moral framework for\nunderstanding existing notions of algorithmic fairness. Most importantly, this\nframework allows us to explicitly spell out the moral assumptions underlying\neach notion of fairness, and interpret recent fairness impossibility results in\na new light. Last but not least and inspired by luck egalitarian models of EOP,\nwe propose a new family of measures for algorithmic fairness. We illustrate our\nproposal empirically and show that employing a measure of algorithmic\n(un)fairness when its underlying moral assumptions are not satisfied, can have\ndevastating consequences for the disadvantaged group's welfare.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:33:51 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 14:54:00 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Heidari", "Hoda", ""], ["Loi", "Michele", ""], ["Gummadi", "Krishna P.", ""], ["Krause", "Andreas", ""]]}, {"id": "1809.03402", "submitter": "John Peruzzi", "authors": "John Peruzzi, Phillip Andrew Wingard, David Zucker", "title": "Does Your Phone Know Your Touch?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores supervised techniques for continuous anomaly detection\nfrom biometric touch screen data. A capacitive sensor array used to mimic a\ntouch screen as used to collect touch and swipe gestures from participants. The\ngestures are recorded over fixed segments of time, with position and force\nmeasured for each gesture. Support Vector Machine, Logistic Regression, and\nGaussian mixture models were tested to learn individual touch patterns. Test\nresults showed true negative and true positive scores of over 95% accuracy for\nall gesture types, with logistic regression models far outperforming the other\nmethods. A more expansive and varied data collection over longer periods of\ntime is needed to determine pragmatic usage of these results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:39:07 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Peruzzi", "John", ""], ["Wingard", "Phillip Andrew", ""], ["Zucker", "David", ""]]}, {"id": "1809.03416", "submitter": "Nisansa de Silva", "authors": "Gathika Ratnayaka, Thejan Rupasinghe, Nisansa de Silva, Menuka\n  Warushavithana, Viraj Gamage, Amal Shehan Perera", "title": "Identifying Relationships Among Sentences in Court Case Transcripts\n  Using Discourse Relations", "comments": "Conference: 2018 International Conference on Advances in ICT for\n  Emerging Regions (ICTer)", "journal-ref": null, "doi": "10.1109/ICTER.2018.8615485", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case Law has a significant impact on the proceedings of legal cases.\nTherefore, the information that can be obtained from previous court cases is\nvaluable to lawyers and other legal officials when performing their duties.\nThis paper describes a methodology of applying discourse relations between\nsentences when processing text documents related to the legal domain. In this\nstudy, we developed a mechanism to classify the relationships that can be\nobserved among sentences in transcripts of United States court cases. First, we\ndefined relationship types that can be observed between sentences in court case\ntranscripts. Then we classified pairs of sentences according to the\nrelationship type by combining a machine learning model and a rule-based\napproach. The results obtained through our system were evaluated using human\njudges. To the best of our knowledge, this is the first study where discourse\nrelationships between sentences have been used to determine relationships among\nsentences in legal court case transcripts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:55:15 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 02:36:07 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Ratnayaka", "Gathika", ""], ["Rupasinghe", "Thejan", ""], ["de Silva", "Nisansa", ""], ["Warushavithana", "Menuka", ""], ["Gamage", "Viraj", ""], ["Perera", "Amal Shehan", ""]]}, {"id": "1809.03428", "submitter": "Ji Wang", "authors": "Ji Wang and Jianguo Zhang and Weidong Bao and Xiaomin Zhu and Bokai\n  Cao and Philip S. Yu", "title": "Not Just Privacy: Improving Performance of Private Deep Learning in\n  Mobile Cloud", "comments": "Conference version accepted by KDD'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand for on-device deep learning services calls for a highly\nefficient manner to deploy deep neural networks (DNNs) on mobile devices with\nlimited capacity. The cloud-based solution is a promising approach to enabling\ndeep learning applications on mobile devices where the large portions of a DNN\nare offloaded to the cloud. However, revealing data to the cloud leads to\npotential privacy risk. To benefit from the cloud data center without the\nprivacy risk, we design, evaluate, and implement a cloud-based framework ARDEN\nwhich partitions the DNN across mobile devices and cloud data centers. A simple\ndata transformation is performed on the mobile device, while the\nresource-hungry training and the complex inference rely on the cloud data\ncenter. To protect the sensitive information, a lightweight privacy-preserving\nmechanism consisting of arbitrary data nullification and random noise addition\nis introduced, which provides strong privacy guarantee. A rigorous privacy\nbudget analysis is given. Nonetheless, the private perturbation to the original\ndata inevitably has a negative impact on the performance of further inference\non the cloud side. To mitigate this influence, we propose a noisy training\nmethod to enhance the cloud-side network robustness to perturbed data. Through\nthe sophisticated design, ARDEN can not only preserve privacy but also improve\nthe inference performance. To validate the proposed ARDEN, a series of\nexperiments based on three image datasets and a real mobile application are\nconducted. The experimental results demonstrate the effectiveness of ARDEN.\nFinally, we implement ARDEN on a demo system to verify its practicality.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:09:58 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 02:50:41 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 11:21:17 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wang", "Ji", ""], ["Zhang", "Jianguo", ""], ["Bao", "Weidong", ""], ["Zhu", "Xiaomin", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.03447", "submitter": "Michal Garmulewicz", "authors": "Micha{\\l} Garmulewicz and Henryk Michalewski and Piotr Mi{\\l}o\\'s", "title": "Expert-augmented actor-critic for ViZDoom and Montezumas Revenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an expert-augmented actor-critic algorithm, which we evaluate on\ntwo environments with sparse rewards: Montezumas Revenge and a demanding maze\nfrom the ViZDoom suite. In the case of Montezumas Revenge, an agent trained\nwith our method achieves very good results consistently scoring above 27,000\npoints (in many experiments beating the first world). With an appropriate\nchoice of hyperparameters, our algorithm surpasses the performance of the\nexpert data. In a number of experiments, we have observed an unreported bug in\nMontezumas Revenge which allowed the agent to score more than 800,000 points.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:36:22 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Garmulewicz", "Micha\u0142", ""], ["Michalewski", "Henryk", ""], ["Mi\u0142o\u015b", "Piotr", ""]]}, {"id": "1809.03461", "submitter": "Xiu Yang", "authors": "Xiu Yang and Guzel Tartakovsky and Alexandre Tartakovsky", "title": "Physics-Informed Kriging: A Physics-Informed Gaussian Process Regression\n  Method for Data-Model Convergence", "comments": "Updated Figure 2(b),(c), Figure 3(c), Figure 4 and Figure 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new Gaussian process regression (GPR) method:\nphysics-informed Kriging (PhIK). In the standard data-driven Kriging, the\nunknown function of interest is usually treated as a Gaussian process with\nassumed stationary covariance with hyperparameters estimated from data. In\nPhIK, we compute the mean and covariance function from realizations of\navailable stochastic models, e.g., from realizations of governing stochastic\npartial differential equations solutions. Such a constructed Gaussian process\ngenerally is non-stationary, and does not assume a specific form of the\ncovariance function. Our approach avoids the costly optimization step in\ndata-driven GPR methods to identify the hyperparameters. More importantly, we\nprove that the physical constraints in the form of a deterministic linear\noperator are guaranteed in the resulting prediction. We also provide an error\nestimate in preserving the physical constraints when errors are included in the\nstochastic model realizations. To reduce the computational cost of obtaining\nstochastic model realizations, we propose a multilevel Monte Carlo estimate of\nthe mean and covariance functions. Further, we present an active learning\nalgorithm that guides the selection of additional observation locations. The\nefficiency and accuracy of PhIK are demonstrated for reconstructing a partially\nknown modified Branin function and learning a conservative tracer distribution\nfrom sparse concentration measurements.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:04:31 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 17:12:49 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yang", "Xiu", ""], ["Tartakovsky", "Guzel", ""], ["Tartakovsky", "Alexandre", ""]]}, {"id": "1809.03470", "submitter": "Marek Wydmuch", "authors": "Marek Wydmuch, Micha{\\l} Kempka, Wojciech Ja\\'skowski", "title": "ViZDoom Competitions: Playing Doom from Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first two editions of Visual Doom AI Competition,\nheld in 2016 and 2017. The challenge was to create bots that compete in a\nmulti-player deathmatch in a first-person shooter (FPS) game, Doom. The bots\nhad to make their decisions based solely on visual information, i.e., a raw\nscreen buffer. To play well, the bots needed to understand their surroundings,\nnavigate, explore, and handle the opponents at the same time. These aspects,\ntogether with the competitive multi-agent aspect of the game, make the\ncompetition a unique platform for evaluating the state of the art reinforcement\nlearning algorithms. The paper discusses the rules, solutions, results, and\nstatistics that give insight into the agents' behaviors. Best-performing agents\nare described in more detail. The results of the competition lead to the\nconclusion that, although reinforcement learning can produce capable Doom bots,\nthey still are not yet able to successfully compete against humans in this\ngame. The paper also revisits the ViZDoom environment, which is a flexible,\neasy to use, and efficient 3D platform for research for vision-based\nreinforcement learning, based on a well-recognized first-person perspective\ngame Doom.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:41:39 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Wydmuch", "Marek", ""], ["Kempka", "Micha\u0142", ""], ["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1809.03474", "submitter": "Mohammad Mahmoody", "authors": "Saeed Mahloujifar, Mohammad Mahmoody, Ameer Mohammed", "title": "Multi-party Poisoning through Generalized $p$-Tampering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a poisoning attack against a learning algorithm, an adversary tampers with\na fraction of the training data $T$ with the goal of increasing the\nclassification error of the constructed hypothesis/model over the final test\ndistribution. In the distributed setting, $T$ might be gathered gradually from\n$m$ data providers $P_1,\\dots,P_m$ who generate and submit their shares of $T$\nin an online way.\n  In this work, we initiate a formal study of $(k,p)$-poisoning attacks in\nwhich an adversary controls $k\\in[n]$ of the parties, and even for each\ncorrupted party $P_i$, the adversary submits some poisoned data $T'_i$ on\nbehalf of $P_i$ that is still \"$(1-p)$-close\" to the correct data $T_i$ (e.g.,\n$1-p$ fraction of $T'_i$ is still honestly generated). For $k=m$, this model\nbecomes the traditional notion of poisoning, and for $p=1$ it coincides with\nthe standard notion of corruption in multi-party computation.\n  We prove that if there is an initial constant error for the generated\nhypothesis $h$, there is always a $(k,p)$-poisoning attacker who can decrease\nthe confidence of $h$ (to have a small error), or alternatively increase the\nerror of $h$, by $\\Omega(p \\cdot k/m)$. Our attacks can be implemented in\npolynomial time given samples from the correct data, and they use no wrong\nlabels if the original distributions are not noisy.\n  At a technical level, we prove a general lemma about biasing bounded\nfunctions $f(x_1,\\dots,x_n)\\in[0,1]$ through an attack model in which each\nblock $x_i$ might be controlled by an adversary with marginal probability $p$\nin an online way. When the probabilities are independent, this coincides with\nthe model of $p$-tampering attacks, thus we call our model generalized\n$p$-tampering. We prove the power of such attacks by incorporating ideas from\nthe context of coin-flipping attacks into the $p$-tampering model and\ngeneralize the results in both of these areas.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:47:24 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 22:49:33 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Mahloujifar", "Saeed", ""], ["Mahmoody", "Mohammad", ""], ["Mohammed", "Ameer", ""]]}, {"id": "1809.03497", "submitter": "Dan Shiebler", "authors": "Dan Shiebler", "title": "A Correlation Maximization Approach for Cross Domain Co-Embeddings", "comments": "Submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although modern recommendation systems can exploit the structure in users'\nitem feedback, most are powerless in the face of new users who provide no\nstructure for them to exploit. In this paper we introduce ImplicitCE, an\nalgorithm for recommending items to new users during their sign-up flow.\nImplicitCE works by transforming users' implicit feedback towards auxiliary\ndomain items into an embedding in the target domain item embedding space.\nImplicitCE learns these embedding spaces and transformation function in an\nend-to-end fashion and can co-embed users and items with any differentiable\nsimilarity function. To train ImplicitCE we explore methods for maximizing the\ncorrelations between model predictions and users' affinities and introduce\nSample Correlation Update, a novel and extremely simple training strategy.\nFinally, we show that ImplicitCE trained with Sample Correlation Update\noutperforms a variety of state of the art algorithms and loss functions on both\na large scale Twitter dataset and the DBLP dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:20:04 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Shiebler", "Dan", ""]]}, {"id": "1809.03534", "submitter": "Mahdi Khodayar", "authors": "Mahdi Khodayar, Jianhui Wang, Zhaoyu Wang", "title": "Energy Disaggregation via Deep Temporal Dictionary Learning", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the energy disaggregation problem, i.e. decomposing the\nelectricity signal of a whole home to its operating devices. First, we cast the\nproblem as a dictionary learning (DL) problem where the key electricity\npatterns representing consumption behaviors are extracted for each device and\nstored in a dictionary matrix. The electricity signal of each device is then\nmodeled by a linear combination of such patterns with sparse coefficients that\ndetermine the contribution of each device in the total electricity. Although\npopular, the classic DL approach is prone to high error in real-world\napplications including energy disaggregation, as it merely finds linear\ndictionaries. Moreover, this method lacks a recurrent structure; thus, it is\nunable to leverage the temporal structure of energy signals. Motivated by such\nshortcomings, we propose a novel optimization program where the dictionary and\nits sparse coefficients are optimized simultaneously with a deep neural model\nextracting powerful nonlinear features from the energy signals. A long\nshort-term memory auto-encoder (LSTM-AE) is proposed with tunable time\ndependent states to capture the temporal behavior of energy signals for each\ndevice. We learn the dictionary in the space of temporal features captured by\nthe LSTM-AE rather than the original space of the energy signals; hence, in\ncontrast to the traditional DL, here, a nonlinear dictionary is learned using\npowerful temporal features extracted from our deep model. Real experiments on\nthe publicly available Reference Energy Disaggregation Dataset (REDD) show\nsignificant improvement compared to the state-of-the-art methodologies in terms\nof the disaggregation accuracy and F-score metrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:26:39 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Khodayar", "Mahdi", ""], ["Wang", "Jianhui", ""], ["Wang", "Zhaoyu", ""]]}, {"id": "1809.03538", "submitter": "Mahdi Khodayar", "authors": "Mahdi Khodayar, Saeed Mohammadi, Mohammad Khodayar, Jianhui Wang,\n  Guangyi Liu", "title": "Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture\n  for Probabilistic Spatio-temporal Solar Irradiance Forecasting", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning on graph-structured data is an important and omnipresent\ntask for a vast variety of applications including anomaly detection and dynamic\nnetwork analysis. In this paper, a deep generative model is introduced to\ncapture continuous probability densities corresponding to the nodes of an\narbitrary graph. In contrast to all learning formulations in the area of\ndiscriminative pattern recognition, we propose a scalable generative\noptimization/algorithm theoretically proved to capture distributions at the\nnodes of a graph. Our model is able to generate samples from the probability\ndensities learned at each node. This probabilistic data generation model, i.e.\nconvolutional graph auto-encoder (CGAE), is devised based on the localized\nfirst-order approximation of spectral graph convolutions, deep learning, and\nthe variational Bayesian inference. We apply our CGAE to a new problem, the\nspatio-temporal probabilistic solar irradiance prediction. Multiple solar\nradiation measurement sites in a wide area in northern states of the US are\nmodeled as an undirected graph. Using our proposed model, the distribution of\nfuture irradiance given historical radiation observations is estimated for\nevery site/node. Numerical results on the National Solar Radiation Database\nshow state-of-the-art performance for probabilistic radiation prediction on\ngeographically distributed irradiance data in terms of reliability, sharpness,\nand continuous ranked probability score.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:31:53 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Khodayar", "Mahdi", ""], ["Mohammadi", "Saeed", ""], ["Khodayar", "Mohammad", ""], ["Wang", "Jianhui", ""], ["Liu", "Guangyi", ""]]}, {"id": "1809.03541", "submitter": "Ramin Moghaddass", "authors": "Ramin Moghaddass and Cynthia Rudin", "title": "Bayesian Patchworks: An Approach to Case-Based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doctors often rely on their past experience in order to diagnose patients.\nFor a doctor with enough experience, almost every patient would have\nsimilarities to key cases seen in the past, and each new patient could be\nviewed as a mixture of these key past cases. Because doctors often tend to\nreason this way, an efficient computationally aided diagnostic tool that thinks\nin the same way might be helpful in locating key past cases of interest that\ncould assist with diagnosis. This article develops a novel mathematical model\nto mimic the type of logical thinking that physicians use when considering past\ncases. The proposed model can also provide physicians with explanations that\nwould be similar to the way they would naturally reason about cases. The\nproposed method is designed to yield predictive accuracy, computational\nefficiency, and insight into medical data; the key element is the insight into\nmedical data, in some sense we are automating a complicated process that\nphysicians might perform manually. We finally implemented the result of this\nwork on two publicly available healthcare datasets, for heart disease\nprediction and breast cancer prediction.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:40:46 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Moghaddass", "Ramin", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1809.03548", "submitter": "Isac Arnekvist", "authors": "Isac Arnekvist, Danica Kragic and Johannes A. Stork", "title": "VPE: Variational Policy Embedding for Transfer Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning methods are capable of solving complex problems, but\nresulting policies might perform poorly in environments that are even slightly\ndifferent. In robotics especially, training and deployment conditions often\nvary and data collection is expensive, making retraining undesirable.\nSimulation training allows for feasible training times, but on the other hand\nsuffers from a reality-gap when applied in real-world settings. This raises the\nneed of efficient adaptation of policies acting in new environments. We\nconsider this as a problem of transferring knowledge within a family of similar\nMarkov decision processes.\n  For this purpose we assume that Q-functions are generated by some\nlow-dimensional latent variable. Given such a Q-function, we can find a master\npolicy that can adapt given different values of this latent variable. Our\nmethod learns both the generative mapping and an approximate posterior of the\nlatent variables, enabling identification of policies for new tasks by\nsearching only in the latent space, rather than the space of all policies. The\nlow-dimensional space, and master policy found by our method enables policies\nto quickly adapt to new environments. We demonstrate the method on both a\npendulum swing-up task in simulation, and for simulation-to-real transfer on a\npushing task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:55:19 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 11:39:07 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Arnekvist", "Isac", ""], ["Kragic", "Danica", ""], ["Stork", "Johannes A.", ""]]}, {"id": "1809.03561", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic\n  Load Forecasting", "comments": "accepted for International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1400-1408", "doi": "10.1016/j.ijforecast.2018.07.004", "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple quantile regression-based forecasting method that was\napplied in a probabilistic load forecasting framework of the Global Energy\nForecasting Competition 2017 (GEFCom2017). The hourly load data is log\ntransformed and split into a long-term trend component and a remainder term.\nThe key forecasting element is the quantile regression approach for the\nremainder term that takes into account weekly and annual seasonalities such as\ntheir interactions. Temperature information is only used to stabilize the\nforecast of the long-term trend component. Public holidays information is\nignored. Still, the forecasting method placed second in the open data track and\nfourth in the definite data track with our forecasting method, which is\nremarkable given simplicity of the model. The method also outperforms the\nVanilla benchmark consistently.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:31:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1809.03566", "submitter": "Sikun Yang", "authors": "Sikun Yang, Heinz Koeppl", "title": "Collapsed Variational Inference for Nonparametric Bayesian Group Factor\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group factor analysis (GFA) methods have been widely used to infer the common\nstructure and the group-specific signals from multiple related datasets in\nvarious fields including systems biology and neuroimaging. To date, most\navailable GFA models require Gibbs sampling or slice sampling to perform\ninference, which prevents the practical application of GFA to large-scale data.\nIn this paper we present an efficient collapsed variational inference (CVI)\nalgorithm for the nonparametric Bayesian group factor analysis (NGFA) model\nbuilt upon an hierarchical beta Bernoulli process. Our CVI algorithm proceeds\nby marginalizing out the group-specific beta process parameters, and then\napproximating the true posterior in the collapsed space using mean field\nmethods. Experimental results on both synthetic and real-world data demonstrate\nthe effectiveness of our CVI algorithm for the NGFA compared with\nstate-of-the-art GFA methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:50:56 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 15:38:44 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Yang", "Sikun", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1809.03576", "submitter": "Xia Zhu", "authors": "Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat\n  Kaul, Theodore L. Willke", "title": "Out-of-Distribution Detection Using an Ensemble of Self Supervised\n  Leave-out Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning methods form a critical part in commercially important\napplications such as autonomous driving and medical diagnostics, it is\nimportant to reliably detect out-of-distribution (OOD) inputs while employing\nthese algorithms. In this work, we propose an OOD detection algorithm which\ncomprises of an ensemble of classifiers. We train each classifier in a\nself-supervised manner by leaving out a random subset of training data as OOD\ndata and the rest as in-distribution (ID) data. We propose a novel margin-based\nloss over the softmax output which seeks to maintain at least a margin $m$\nbetween the average entropy of the OOD and in-distribution samples. In\nconjunction with the standard cross-entropy loss, we minimize the novel loss to\ntrain an ensemble of classifiers. We also propose a novel method to combine the\noutputs of the ensemble of classifiers to obtain OOD detection score and class\nprediction. Overall, our method convincingly outperforms Hendrycks et al.[7]\nand the current state-of-the-art ODIN[13] on several OOD detection benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 16:00:08 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Vyas", "Apoorv", ""], ["Jammalamadaka", "Nataraj", ""], ["Zhu", "Xia", ""], ["Das", "Dipankar", ""], ["Kaul", "Bharat", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1809.03627", "submitter": "Sudipto Mukherjee", "authors": "Sudipto Mukherjee, Himanshu Asnani, Eugene Lin, Sreeram Kannan", "title": "ClusterGAN : Latent Space Clustering in Generative Adversarial Networks", "comments": "GANs, Clustering, Latent Space, Interpolation (v2 : Typos fixed, some\n  new experiments added, reported metrics on best validated model.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial networks (GANs) have obtained remarkable success in\nmany unsupervised learning tasks and unarguably, clustering is an important\nunsupervised learning problem. While one can potentially exploit the\nlatent-space back-projection in GANs to cluster, we demonstrate that the\ncluster structure is not retained in the GAN latent space.\n  In this paper, we propose ClusterGAN as a new mechanism for clustering using\nGANs. By sampling latent variables from a mixture of one-hot encoded variables\nand continuous latent variables, coupled with an inverse network (which\nprojects the data to the latent space) trained jointly with a clustering\nspecific loss, we are able to achieve clustering in the latent space. Our\nresults show a remarkable phenomenon that GANs can preserve latent space\ninterpolation across categories, even though the discriminator is never exposed\nto such vectors. We compare our results with various clustering baselines and\ndemonstrate superior performance on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 23:00:37 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 23:28:35 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Mukherjee", "Sudipto", ""], ["Asnani", "Himanshu", ""], ["Lin", "Eugene", ""], ["Kannan", "Sreeram", ""]]}, {"id": "1809.03655", "submitter": "Tao Sun", "authors": "Lei Guan, Linbo Qiao, Dongsheng Li, Tao Sun, Keshi Ge, Xicheng Lu", "title": "An Efficient ADMM-Based Algorithm to Nonconvex Penalized Support Vector\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) with sparsity-inducing nonconvex penalties\nhave received considerable attentions for the characteristics of automatic\nclassification and variable selection. However, it is quite challenging to\nsolve the nonconvex penalized SVMs due to their nondifferentiability,\nnonsmoothness and nonconvexity. In this paper, we propose an efficient\nADMM-based algorithm to the nonconvex penalized SVMs. The proposed algorithm\ncovers a large class of commonly used nonconvex regularization terms including\nthe smooth clipped absolute deviation (SCAD) penalty, minimax concave penalty\n(MCP), log-sum penalty (LSP) and capped-$\\ell_1$ penalty. The computational\ncomplexity analysis shows that the proposed algorithm enjoys low computational\ncost. Moreover, the convergence of the proposed algorithm is guaranteed.\nExtensive experimental evaluations on five benchmark datasets demonstrate the\nsuperior performance of the proposed algorithm to other three state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:08:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Guan", "Lei", ""], ["Qiao", "Linbo", ""], ["Li", "Dongsheng", ""], ["Sun", "Tao", ""], ["Ge", "Keshi", ""], ["Lu", "Xicheng", ""]]}, {"id": "1809.03659", "submitter": "Boris Beranger", "authors": "Boris Beranger, Huan Lin, and Scott A. Sisson", "title": "New models for symbolic data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:36:10 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:13:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Beranger", "Boris", ""], ["Lin", "Huan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1809.03672", "submitter": "Guorui Zhou", "authors": "Guorui Zhou and Na Mou and Ying Fan and Qi Pi and Weijie Bian and\n  Chang Zhou and Xiaoqiang Zhu and Kun Gai", "title": "Deep Interest Evolution Network for Click-Through Rate Prediction", "comments": "9 pages. Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate~(CTR) prediction, whose goal is to estimate the\nprobability of the user clicks, has become one of the core tasks in advertising\nsystems. For CTR prediction model, it is necessary to capture the latent user\ninterest behind the user behavior data. Besides, considering the changing of\nthe external environment and the internal cognition, user interest evolves over\ntime dynamically. There are several CTR prediction methods for interest\nmodeling, while most of them regard the representation of behavior as the\ninterest directly, and lack specially modeling for latent interest behind the\nconcrete behavior. Moreover, few work consider the changing trend of interest.\nIn this paper, we propose a novel model, named Deep Interest Evolution\nNetwork~(DIEN), for CTR prediction. Specifically, we design interest extractor\nlayer to capture temporal interests from history behavior sequence. At this\nlayer, we introduce an auxiliary loss to supervise interest extracting at each\nstep. As user interests are diverse, especially in the e-commerce system, we\npropose interest evolving layer to capture interest evolving process that is\nrelative to the target item. At interest evolving layer, attention mechanism is\nembedded into the sequential structure novelly, and the effects of relative\ninterests are strengthened during interest evolution. In the experiments on\nboth public and industrial datasets, DIEN significantly outperforms the\nstate-of-the-art solutions. Notably, DIEN has been deployed in the display\nadvertisement system of Taobao, and obtained 20.7\\% improvement on CTR.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 03:52:37 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 04:08:27 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 05:28:48 GMT"}, {"version": "v4", "created": "Wed, 7 Nov 2018 09:45:31 GMT"}, {"version": "v5", "created": "Fri, 16 Nov 2018 06:48:37 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zhou", "Guorui", ""], ["Mou", "Na", ""], ["Fan", "Ying", ""], ["Pi", "Qi", ""], ["Bian", "Weijie", ""], ["Zhou", "Chang", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "1809.03702", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas,\n  Michael C. Mozer, Chris Pal, Yoshua Bengio", "title": "Sparse Attentive Backtracking: Temporal CreditAssignment Through\n  Reminding", "comments": "To appear as a Spotlight presentation at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long-term dependencies in extended temporal sequences requires\ncredit assignment to events far back in the past. The most common method for\ntraining recurrent neural networks, back-propagation through time (BPTT),\nrequires credit information to be propagated backwards through every single\nstep of the forward computation, potentially over thousands or millions of time\nsteps. This becomes computationally expensive or even infeasible when used with\nlong sequences. Importantly, biological brains are unlikely to perform such\ndetailed reverse replay over very long sequences of internal states (consider\ndays, months, or years.) However, humans are often reminded of past memories or\nmental states which are associated with the current mental state. We consider\nthe hypothesis that such memory associations between past and present could be\nused for credit assignment through arbitrarily long sequences, propagating the\ncredit assigned to the current state to the associated past state. Based on\nthis principle, we study a novel algorithm which only back-propagates through a\nfew of these temporal skip connections, realized by a learned attention\nmechanism that associates current states with relevant past states. We\ndemonstrate in experiments that our method matches or outperforms regular BPTT\nand truncated BPTT in tasks involving particularly long-term dependencies, but\nwithout requiring the biologically implausible backward replay through the\nwhole history of states. Additionally, we demonstrate that the proposed method\ntransfers to longer sequences significantly better than LSTMs trained with BPTT\nand LSTMs trained with full self-attention.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 07:04:47 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bilaniuk", "Olexa", ""], ["Binas", "Jonathan", ""], ["Mozer", "Michael C.", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1809.03721", "submitter": "Seungjoon Yang", "authors": "Jinhyeok Jang, Hyunjoong Cho, Jaehong Kim, Jaeyeon Lee, and Seungjoon\n  Yang", "title": "Deep Asymmetric Networks with a Set of Node-wise Variant Activation\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents deep asymmetric networks with a set of node-wise variant\nactivation functions. The nodes' sensitivities are affected by activation\nfunction selections such that the nodes with smaller indices become\nincreasingly more sensitive. As a result, features learned by the nodes are\nsorted by the node indices in the order of their importance. Asymmetric\nnetworks not only learn input features but also the importance of those\nfeatures. Nodes of lesser importance in asymmetric networks can be pruned to\nreduce the complexity of the networks, and the pruned networks can be retrained\nwithout incurring performance losses. We validate the feature-sorting property\nusing both shallow and deep asymmetric networks as well as deep asymmetric\nnetworks transferred from famous networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:09:25 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 07:24:15 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Jang", "Jinhyeok", ""], ["Cho", "Hyunjoong", ""], ["Kim", "Jaehong", ""], ["Lee", "Jaeyeon", ""], ["Yang", "Seungjoon", ""]]}, {"id": "1809.03776", "submitter": "Ryota Suzuki", "authors": "Ryota Suzuki, Shingo Takahashi, Murtuza Petladwala, and Shigeru\n  Kohmoto", "title": "Solving Non-identifiable Latent Feature Models", "comments": "Submitted to NIPS 2018 (https://nips.cc/). 15 pages , 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent feature models (LFM)s are widely employed for extracting latent\nstructures of data. While offering high, parameter estimation is difficult with\nLFMs because of the combinational nature of latent features, and\nnon-identifiability is a particularly difficult problem when parameter\nestimation is not unique and there exists equivalent solutions. In this paper,\na necessary and sufficient condition for non-identifiability is shown. The\ncondition is significantly related to dependency of features, and this implies\nthat non-identifiability may often occur in real-world applications. A novel\nmethod for parameter estimation that solves the non-identifiability problem is\nalso proposed. This method can be combined as a post-process with existing\nmethods and can find an appropriate solution by hopping efficiently through\nequivalent solutions. We have evaluated the effectiveness of the method on both\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:11:48 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 05:46:55 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Suzuki", "Ryota", ""], ["Takahashi", "Shingo", ""], ["Petladwala", "Murtuza", ""], ["Kohmoto", "Shigeru", ""]]}, {"id": "1809.03779", "submitter": "Zenith Purisha", "authors": "Zenith Purisha, Carl Jidling, Niklas Wahlstr\\\"om, Simo S\\\"arkk\\\"a,\n  Thomas B. Sch\\\"on", "title": "Probabilistic approach to limited-data computed tomography\n  reconstruction", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab2e2a", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the inverse problem of reconstructing the internal\nstructure of an object from limited x-ray projections. We use a Gaussian\nprocess prior to model the target function and estimate its (hyper)parameters\nfrom measured data. In contrast to other established methods, this comes with\nthe advantage of not requiring any manual parameter tuning, which usually\narises in classical regularization strategies. Our method uses a basis function\nexpansion technique for the Gaussian process which significantly reduces the\ncomputational complexity and avoids the need for numerical integration. The\napproach also allows for reformulation of come classical regularization methods\nas Laplacian and Tikhonov regularization as Gaussian process regression, and\nhence provides an efficient algorithm and principled means for their parameter\ntuning. Results from simulated and real data indicate that this approach is\nless sensitive to streak artifacts as compared to the commonly used method of\nfiltered backprojection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:16:44 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 09:07:08 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 08:30:06 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Purisha", "Zenith", ""], ["Jidling", "Carl", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1809.03817", "submitter": "Qingnan Sun", "authors": "Qingnan Sun, Marko V. Jankovic, Lia Bally, Stavroula G. Mougiakakou", "title": "Predicting Blood Glucose with an LSTM and Bi-LSTM Based Deep Neural\n  Network", "comments": "5 pages, submitted to 2018 14th Symposium on Neural Networks and\n  Applications (NEUREL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning network was used to predict future blood glucose levels, as\nthis can permit diabetes patients to take action before imminent hyperglycaemia\nand hypoglycaemia. A sequential model with one long-short-term memory (LSTM)\nlayer, one bidirectional LSTM layer and several fully connected layers was used\nto predict blood glucose levels for different prediction horizons. The method\nwas trained and tested on 26 datasets from 20 real patients. The proposed\nnetwork outperforms the baseline methods in terms of all evaluation criteria.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 12:26:46 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Sun", "Qingnan", ""], ["Jankovic", "Marko V.", ""], ["Bally", "Lia", ""], ["Mougiakakou", "Stavroula G.", ""]]}, {"id": "1809.03832", "submitter": "Antti Koskela", "authors": "Antti Koskela and Antti Honkela", "title": "Learning Rate Adaptation for Federated and Differentially Private\n  Learning", "comments": "17 pages, 9 figures", "journal-ref": "AISTATS (2020) 2465-2475", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm for the adaptation of the learning rate for\nstochastic gradient descent (SGD) that avoids the need for validation set use.\nThe idea for the adaptiveness comes from the technique of extrapolation: to get\nan estimate for the error against the gradient flow which underlies SGD, we\ncompare the result obtained by one full step and two half-steps. The algorithm\nis applied in two separate frameworks: federated and differentially private\nlearning. Using examples of deep neural networks we empirically show that the\nadaptive algorithm is competitive with manually tuned commonly used\noptimisation methods for differentially privately training. We also show that\nit works robustly in the case of federated learning unlike commonly used\noptimisation methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:04:19 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 13:34:05 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 10:30:52 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Koskela", "Antti", ""], ["Honkela", "Antti", ""]]}, {"id": "1809.03839", "submitter": "Seiichi Kuroki", "authors": "Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei\n  Sato, Masashi Sugiyama", "title": "Unsupervised Domain Adaptation Based on Source-guided Discrepancy", "comments": "To appear in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is the problem setting where data generating\ndistributions in the source and target domains are different, and labels in the\ntarget domain are unavailable. One important question in unsupervised domain\nadaptation is how to measure the difference between the source and target\ndomains. A previously proposed discrepancy that does not use the source domain\nlabels requires high computational cost to estimate and may lead to a loose\ngeneralization error bound in the target domain. To mitigate these problems, we\npropose a novel discrepancy called source-guided discrepancy (S-disc), which\nexploits labels in the source domain. As a consequence, S-disc can be computed\nefficiently with a finite sample convergence guarantee. In addition, we show\nthat S-disc can provide a tighter generalization error bound than the one based\non an existing discrepancy. Finally, we report experimental results that\ndemonstrate the advantages of S-disc over the existing discrepancies.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:11:30 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 11:36:35 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 09:54:32 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kuroki", "Seiichi", ""], ["Charoenphakdee", "Nontawat", ""], ["Bao", "Han", ""], ["Honda", "Junya", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.03864", "submitter": "Ramin M. Hasani", "authors": "Ramin M. Hasani, Alexander Amini, Mathias Lechner, Felix Naser, Radu\n  Grosu, Daniela Rus", "title": "Response Characterization for Auditing Cell Dynamics in Long Short-term\n  Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel method to interpret recurrent neural\nnetworks (RNNs), particularly long short-term memory networks (LSTMs) at the\ncellular level. We propose a systematic pipeline for interpreting individual\nhidden state dynamics within the network using response characterization\nmethods. The ranked contribution of individual cells to the network's output is\ncomputed by analyzing a set of interpretable metrics of their decoupled step\nand sinusoidal responses. As a result, our method is able to uniquely identify\nneurons with insightful dynamics, quantify relationships between dynamical\nproperties and test accuracy through ablation analysis, and interpret the\nimpact of network capacity on a network's dynamical distribution. Finally, we\ndemonstrate generalizability and scalability of our method by evaluating a\nseries of different benchmark sequential datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:27:36 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Hasani", "Ramin M.", ""], ["Amini", "Alexander", ""], ["Lechner", "Mathias", ""], ["Naser", "Felix", ""], ["Grosu", "Radu", ""], ["Rus", "Daniela", ""]]}, {"id": "1809.03868", "submitter": "Hao Zhang", "authors": "Hao Zhang, Stephen Zahorian, Xiao Chen, Peter Guzewich, Xiaoyu Liu", "title": "Dual-label Deep LSTM Dereverberation For Speaker Verification", "comments": "4 pages, 3 figures, submitted to Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a reverberation removal approach for speaker\nverification, utilizing dual-label deep neural networks (DNNs). The networks\nperform feature mapping between the spectral features of reverberant and clean\nspeech. Long short term memory recurrent neural networks (LSTMs) are trained to\nmap corrupted Mel filterbank (MFB) features to two sets of labels: i) the clean\nMFB features, and ii) either estimated pitch tracks or the fast Fourier\ntransform (FFT) spectrogram of clean speech. The performance of reverberation\nremoval is evaluated by equal error rates (EERs) of speaker verification\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 04:55:24 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Zhang", "Hao", ""], ["Zahorian", "Stephen", ""], ["Chen", "Xiao", ""], ["Guzewich", "Peter", ""], ["Liu", "Xiaoyu", ""]]}, {"id": "1809.03986", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis and Themis Gouleakis and Christos Tzamos and\n  Manolis Zampetakis", "title": "Efficient Statistics, in High Dimensions, from Truncated Samples", "comments": "Appeared at 59th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:42:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:39:09 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1809.04019", "submitter": "Emilia Apostolova PhD", "authors": "Emilia Apostolova and R. Andrew Kreek", "title": "Training and Prediction Data Discrepancies: Challenges of Text\n  Classification with Noisy, Historical Data", "comments": "2018 The 4th Workshop on Noisy User-generated Text (W-NUT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry datasets used for text classification are rarely created for that\npurpose. In most cases, the data and target predictions are a by-product of\naccumulated historical data, typically fraught with noise, present in both the\ntext-based document, as well as in the targeted labels. In this work, we\naddress the question of how well performance metrics computed on noisy,\nhistorical data reflect the performance on the intended future machine learning\nmodel input. The results demonstrate the utility of dirty training datasets\nused to build prediction models for cleaner (and different) prediction inputs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 16:43:52 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Apostolova", "Emilia", ""], ["Kreek", "R. Andrew", ""]]}, {"id": "1809.04069", "submitter": "Ruobing Wang", "authors": "Zhiyuan Ma, Ping Wang, Zehui Gao, Ruobing Wang, Koroush Khalighi", "title": "Estimate the Warfarin Dose by Ensemble of Machine Learning Algorithms", "comments": "other authors do not agree to submit to arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warfarin dosing remains challenging due to narrow therapeutic index and\nhighly individual variability. Incorrect warfarin dosing is associated with\ndevastating adverse events. Remarkable efforts have been made to develop the\nmachine learning based warfarin dosing algorithms incorporating clinical\nfactors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The\nmost widely validated pharmacogenetic algorithm is the IWPC algorithm based on\nmultivariate linear regression (MLR). However, with only a single algorithm,\nthe prediction performance may reach an upper limit even with optimal\nparameters. Here, we present novel algorithms using stacked generalization\nframeworks to estimate the warfarin dose, within which different types of\nmachine learning algorithms function together through a meta-machine learning\nmodel to maximize the prediction accuracy. Compared to the IWPC-derived MLR\nalgorithm, Stack 1 and 2 based on stacked generalization frameworks performed\nsignificantly better overall. Subgroup analysis revealed that the mean of the\npercentage of patients whose predicted dose of warfarin within 20% of the\nactual stable therapeutic dose (mean percentage within 20%) for Stack 1 was\nimproved by 12.7% (from 42.47% to 47.86%) in Asians and by 13.5% (from 22.08%\nto 25.05%) in the low-dose group compared to that for MLR, respectively. These\ndata suggest that our algorithms would especially benefit patients required low\nwarfarin maintenance dose, as subtle changes in warfarin dose could lead to\nadverse clinical events (thrombosis or bleeding) in patients with low dose. Our\nstudy offers novel pharmacogenetic algorithms for clinical trials and practice.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 22:18:37 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 14:36:49 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Ma", "Zhiyuan", ""], ["Wang", "Ping", ""], ["Gao", "Zehui", ""], ["Wang", "Ruobing", ""], ["Khalighi", "Koroush", ""]]}, {"id": "1809.04091", "submitter": "Pooya Ronagh", "authors": "Behrooz Sepehry, Ehsan Iranmanesh, Michael P. Friedlander and Pooya\n  Ronagh", "title": "Quantum Algorithms for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.OC quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two quantum algorithms for solving structured prediction\nproblems. We first show that a stochastic gradient descent that uses the\nquantum minimum finding algorithm and takes its probabilistic failure into\naccount solves the structured prediction problem with a runtime that scales\nwith the square root of the size of the label space, and in $\\widetilde\nO\\left(1/\\epsilon\\right)$ with respect to the precision, $\\epsilon$, of the\nsolution. Motivated by robust inference techniques in machine learning, we then\nintroduce another quantum algorithm that solves a smooth approximation of the\nstructured prediction problem with a similar quantum speedup in the size of the\nlabel space and a similar scaling in the precision parameter. In doing so, we\nanalyze a variant of stochastic gradient descent for convex optimization in the\npresence of an additive error in the calculation of the gradients, and show\nthat its convergence rate does not deteriorate if the additive errors are of\nthe order $O(\\sqrt\\epsilon)$. This algorithm uses quantum Gibbs sampling at\ntemperature $\\Omega (\\epsilon)$ as a subroutine. Based on these theoretical\nobservations, we propose a method for using quantum Gibbs samplers to combine\nfeedforward neural networks with probabilistic graphical models for quantum\nmachine learning. Our numerical results using Monte Carlo simulations on an\nimage tagging task demonstrate the benefit of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:04:11 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 18:00:21 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 19:13:05 GMT"}, {"version": "v4", "created": "Thu, 28 Feb 2019 00:45:51 GMT"}, {"version": "v5", "created": "Thu, 1 Jul 2021 20:43:29 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Sepehry", "Behrooz", ""], ["Iranmanesh", "Ehsan", ""], ["Friedlander", "Michael P.", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1809.04110", "submitter": "Lichao Sun", "authors": "Lichao Sun, Lifang He, Zhipeng Huang, Bokai Cao, Congying Xia, Xiaokai\n  Wei and Philip S. Yu", "title": "Joint Embedding of Meta-Path and Meta-Graph for Heterogeneous\n  Information Networks", "comments": "accepted by ICBK 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-graph is currently the most powerful tool for similarity search on\nheterogeneous information networks,where a meta-graph is a composition of\nmeta-paths that captures the complex structural information. However, current\nrelevance computing based on meta-graph only considers the complex structural\ninformation, but ignores its embedded meta-paths information. To address this\nproblem, we proposeMEta-GrAph-based network embedding models, called MEGA and\nMEGA++, respectively. The MEGA model uses normalized relevance or similarity\nmeasures that are derived from a meta-graph and its embedded meta-paths between\nnodes simultaneously, and then leverages tensor decomposition method to perform\nnode embedding. The MEGA++ further facilitates the use of coupled tensor-matrix\ndecomposition method to obtain a joint embedding for nodes, which\nsimultaneously considers the hidden relations of all meta information of a\nmeta-graph.Extensive experiments on two real datasets demonstrate thatMEGA and\nMEGA++ are more effective than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:03:31 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sun", "Lichao", ""], ["He", "Lifang", ""], ["Huang", "Zhipeng", ""], ["Cao", "Bokai", ""], ["Xia", "Congying", ""], ["Wei", "Xiaokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.04121", "submitter": "Cameron Hoerig", "authors": "Cameron Hoerig, Jamshid Ghaboussi, and Michael F. Insana", "title": "Cartesian Neural Network Constitutive Models for Data-driven Elasticity\n  Imaging", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticity images map biomechanical properties of soft tissues to aid in the\ndetection and diagnosis of pathological states. In particular, quasi-static\nultrasonic (US) elastography techniques use force-displacement measurements\nacquired during an US scan to parameterize the spatio-temporal stress-strain\nbehavior. Current methods use a model-based inverse approach to estimate the\nparameters associated with a chosen constitutive model. However, model-based\nmethods rely on simplifying assumptions of tissue biomechanical properties,\noften limiting elastography to imaging one or two linear-elastic parameters.\n  We previously described a data-driven method for building neural network\nconstitutive models (NNCMs) that learn stress-strain relationships from\nforce-displacement data. Using measurements acquired on gelatin phantoms, we\ndemonstrated the ability of NNCMs to characterize linear-elastic mechanical\nproperties without an initial model assumption and thus circumvent the\nmathematical constraints typically encountered in classic model-based\napproaches to the inverse problem. While successful, we were required to use a\npriori knowledge of the internal object shape to define the spatial\ndistribution of regions exhibiting different material properties.\n  Here, we introduce Cartesian neural network constitutive models (CaNNCMs)\nthat are capable of using data to model both linear-elastic mechanical\nproperties and their distribution in space. We demonstrate the ability of\nCaNNCMs to capture arbitrary material property distributions using\nstress-strain data from simulated phantoms. Furthermore, we show that a trained\nCaNNCM can be used to reconstruct a Young's modulus image. CaNNCMs are an\nimportant step toward data-driven modeling and imaging the complex mechanical\nproperties of soft tissues.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:41:30 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Hoerig", "Cameron", ""], ["Ghaboussi", "Jamshid", ""], ["Insana", "Michael F.", ""]]}, {"id": "1809.04127", "submitter": "Minghong Fang", "authors": "Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, Jia Liu", "title": "Poisoning Attacks to Graph-Based Recommender Systems", "comments": "34th Annual Computer Security Applications Conference (ACSAC), 2018;\n  Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "journal-ref": null, "doi": "10.1145/3274694.3274706", "report-no": null, "categories": "cs.IR cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system is an important component of many web services to help\nusers locate items that match their interests. Several studies showed that\nrecommender systems are vulnerable to poisoning attacks, in which an attacker\ninjects fake data to a given system such that the system makes recommendations\nas the attacker desires. However, these poisoning attacks are either agnostic\nto recommendation algorithms or optimized to recommender systems that are not\ngraph-based. Like association-rule-based and matrix-factorization-based\nrecommender systems, graph-based recommender system is also deployed in\npractice, e.g., eBay, Huawei App Store. However, how to design optimized\npoisoning attacks for graph-based recommender systems is still an open problem.\nIn this work, we perform a systematic study on poisoning attacks to graph-based\nrecommender systems. Due to limited resources and to avoid detection, we assume\nthe number of fake users that can be injected into the system is bounded. The\nkey challenge is how to assign rating scores to the fake users such that the\ntarget item is recommended to as many normal users as possible. To address the\nchallenge, we formulate the poisoning attacks as an optimization problem,\nsolving which determines the rating scores for the fake users. We also propose\ntechniques to solve the optimization problem. We evaluate our attacks and\ncompare them with existing attacks under white-box (recommendation algorithm\nand its parameters are known), gray-box (recommendation algorithm is known but\nits parameters are unknown), and black-box (recommendation algorithm is\nunknown) settings using two real-world datasets. Our results show that our\nattack is effective and outperforms existing attacks for graph-based\nrecommender systems. For instance, when 1% fake users are injected, our attack\ncan make a target item recommended to 580 times more normal users in certain\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:50:41 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Fang", "Minghong", ""], ["Yang", "Guolei", ""], ["Gong", "Neil Zhenqiang", ""], ["Liu", "Jia", ""]]}, {"id": "1809.04157", "submitter": "Xu Zhang", "authors": "Xu Zhang, Felix Xinnan Yu, Svebor Karaman, Wei Zhang, Shih-Fu Chang", "title": "Heated-Up Softmax Embedding", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning aims at learning a distance which is consistent with the\nsemantic meaning of the samples. The problem is generally solved by learning an\nembedding for each sample such that the embeddings of samples of the same\ncategory are compact while the embeddings of samples of different categories\nare spread-out in the feature space. We study the features extracted from the\nsecond last layer of a deep neural network based classifier trained with the\ncross entropy loss on top of the softmax layer. We show that training\nclassifiers with different temperature values of softmax function leads to\nfeatures with different levels of compactness. Leveraging these insights, we\npropose a \"heating-up\" strategy to train a classifier with increasing\ntemperatures, leading the corresponding embeddings to achieve state-of-the-art\nperformance on a variety of metric learning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:56:02 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Zhang", "Xu", ""], ["Yu", "Felix Xinnan", ""], ["Karaman", "Svebor", ""], ["Zhang", "Wei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1809.04176", "submitter": "Seyedehsara Nayer", "authors": "Seyedehsara Nayer and Namrata Vaswani", "title": "Phaseless Subspace Tracking", "comments": "To be appeared in GlobalSIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work takes the first steps towards solving the \"phaseless subspace\ntracking\" (PST) problem. PST involves recovering a time sequence of signals (or\nimages) from phaseless linear projections of each signal under the following\nstructural assumption: the signal sequence is generated from a much lower\ndimensional subspace (than the signal dimension) and this subspace can change\nover time, albeit gradually. It can be simply understood as a dynamic\n(time-varying subspace) extension of the low-rank phase retrieval problem\nstudied in recent work.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 21:30:56 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Nayer", "Seyedehsara", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1809.04184", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George Papandreou,\n  Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens", "title": "Searching for Efficient Multi-Scale Architectures for Dense Image\n  Prediction", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural network architectures is an important component for\nachieving state-of-the-art performance with machine learning systems across a\nbroad array of tasks. Much work has endeavored to design and build\narchitectures automatically through clever construction of a search space\npaired with simple learning algorithms. Recent progress has demonstrated that\nsuch meta-learning methods may exceed scalable human-invented architectures on\nimage classification tasks. An open question is the degree to which such\nmethods may generalize to new domains. In this work we explore the construction\nof meta-learning techniques for dense image prediction focused on the tasks of\nscene parsing, person-part segmentation, and semantic image segmentation.\nConstructing viable search spaces in this domain is challenging because of the\nmulti-scale representation of visual information and the necessity to operate\non high resolution imagery. Based on a survey of techniques in dense image\nprediction, we construct a recursive search space and demonstrate that even\nwith efficient random search, we can identify architectures that outperform\nhuman-invented architectures and achieve state-of-the-art performance on three\ndense prediction tasks including 82.7\\% on Cityscapes (street scene parsing),\n71.3\\% on PASCAL-Person-Part (person-part segmentation), and 87.9\\% on PASCAL\nVOC 2012 (semantic image segmentation). Additionally, the resulting\narchitecture is more computationally efficient, requiring half the parameters\nand half the computational cost as previous state of the art systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:36:01 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Papandreou", "George", ""], ["Zoph", "Barret", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1809.04188", "submitter": "Jianguo Zhang", "authors": "Jianguo Zhang and Ji Wang and Lifang He and Zhao Li and Philip S. Yu", "title": "Layerwise Perturbation-Based Adversarial Training for Hard Drive Health\n  Degree Prediction", "comments": "The 2018 IEEE International Conference on Data Mining (ICDM'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of cloud computing and big data, the reliability of data\nstorage systems becomes increasingly important. Previous researchers have shown\nthat machine learning algorithms based on SMART attributes are effective\nmethods to predict hard drive failures. In this paper, we use SMART attributes\nto predict hard drive health degrees which are helpful for taking different\nfault tolerant actions in advance. Given the highly imbalanced SMART datasets,\nit is a nontrivial work to predict the health degree precisely. The proposed\nmodel would encounter overfitting and biased fitting problems if it is trained\nby the traditional methods. In order to resolve this problem, we propose two\nstrategies to better utilize imbalanced data and improve performance. Firstly,\nwe design a layerwise perturbation-based adversarial training method which can\nadd perturbations to any layers of a neural network to improve the\ngeneralization of the network. Secondly, we extend the training method to the\nsemi-supervised settings. Then, it is possible to utilize unlabeled data that\nhave a potential of failure to further improve the performance of the model.\nOur extensive experiments on two real-world hard drive datasets demonstrate the\nsuperiority of the proposed schemes for both supervised and semi-supervised\nclassification. The model trained by the proposed method can correctly predict\nthe hard drive health status 5 and 15 days in advance. Finally, we verify the\ngenerality of the proposed training method in other similar anomaly detection\ntasks where the dataset is imbalanced. The results argue that the proposed\nmethods are applicable to other domains.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:43:19 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 02:31:51 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 01:41:21 GMT"}, {"version": "v4", "created": "Fri, 28 Sep 2018 20:00:03 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zhang", "Jianguo", ""], ["Wang", "Ji", ""], ["He", "Lifang", ""], ["Li", "Zhao", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.04197", "submitter": "Pablo Moreno-Mu\\~noz", "authors": "Pablo Moreno-Mu\\~noz, David Ram\\'irez and Antonio Art\\'es-Rodr\\'iguez", "title": "Change-Point Detection on Hierarchical Circadian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of change-point detection on sequences of\nhigh-dimensional and heterogeneous observations, which also possess a periodic\ntemporal structure. Due to the dimensionality problem, when the time between\nchange-points is on the order of the dimension of the model parameters, drifts\nin the underlying distribution can be misidentified as changes. To overcome\nthis limitation, we assume that the observations lie in a lower-dimensional\nmanifold that admits a latent variable representation. In particular, we\npropose a hierarchical model that is computationally feasible, widely\napplicable to heterogeneous data and robust to missing instances. Additionally,\nthe observations' periodic dependencies are captured by non-stationary periodic\ncovariance functions. The proposed technique is particularly fitted to (and\nmotivated by) the problem of detecting changes in human behavior using\nsmartphones and its application to relapse detection in psychiatric patients.\nFinally, we validate the technique on synthetic examples and we demonstrate its\nutility in the detection of behavioral changes using real data acquired by\nsmartphones.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:36:31 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 14:12:48 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Moreno-Mu\u00f1oz", "Pablo", ""], ["Ram\u00edrez", "David", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "1809.04198", "submitter": "Heinrich Jiang", "authors": "Andrew Cotter, Heinrich Jiang, Serena Wang, Taman Narayan, Maya Gupta,\n  Seungil You, Karthik Sridharan", "title": "Optimization with Non-Differentiable Constraints with Applications to\n  Fairness, Recall, Churn, and Other Goals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that many machine learning goals, such as improved fairness metrics,\ncan be expressed as constraints on the model's predictions, which we call rate\nconstraints. We study the problem of training non-convex models subject to\nthese rate constraints (or any non-convex and non-differentiable constraints).\nIn the non-convex setting, the standard approach of Lagrange multipliers may\nfail. Furthermore, if the constraints are non-differentiable, then one cannot\noptimize the Lagrangian with gradient-based methods. To solve these issues, we\nintroduce the proxy-Lagrangian formulation. This new formulation leads to an\nalgorithm that produces a stochastic classifier by playing a two-player\nnon-zero-sum game solving for what we call a semi-coarse correlated\nequilibrium, which in turn corresponds to an approximately optimal and feasible\nsolution to the constrained optimization problem. We then give a procedure\nwhich shrinks the randomized solution down to one that is a mixture of at most\n$m+1$ deterministic solutions, given $m$ constraints. This culminates in\nalgorithms that can solve non-convex constrained optimization problems with\npossibly non-differentiable and non-convex constraints with theoretical\nguarantees. We provide extensive experimental results enforcing a wide range of\npolicy goals including different fairness metrics, and other goals on accuracy,\ncoverage, recall, and churn.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:41:47 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Cotter", "Andrew", ""], ["Jiang", "Heinrich", ""], ["Wang", "Serena", ""], ["Narayan", "Taman", ""], ["Gupta", "Maya", ""], ["You", "Seungil", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1809.04206", "submitter": "Fan-Keng Sun", "authors": "Shun-Yao Shih, Fan-Keng Sun, Hung-yi Lee", "title": "Temporal Pattern Attention for Multivariate Time Series Forecasting", "comments": "Journal track of ECML/PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting multivariate time series data, such as prediction of electricity\nconsumption, solar power production, and polyphonic piano pieces, has numerous\nvaluable applications. However, complex and non-linear interdependencies\nbetween time steps and series complicate the task. To obtain accurate\nprediction, it is crucial to model long-term dependency in time series data,\nwhich can be achieved to some good extent by recurrent neural network (RNN)\nwith attention mechanism. Typical attention mechanism reviews the information\nat each previous time step and selects the relevant information to help\ngenerate the outputs, but it fails to capture the temporal patterns across\nmultiple time steps. In this paper, we propose to use a set of filters to\nextract time-invariant temporal patterns, which is similar to transforming time\nseries data into its \"frequency domain\". Then we proposed a novel attention\nmechanism to select relevant time series, and use its \"frequency domain\"\ninformation for forecasting. We applied the proposed model on several\nreal-world tasks and achieved state-of-the-art performance in all of them with\nonly one exception.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 00:40:40 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 06:09:10 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 23:17:57 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shih", "Shun-Yao", ""], ["Sun", "Fan-Keng", ""], ["Lee", "Hung-yi", ""]]}, {"id": "1809.04216", "submitter": "Tao Sun", "authors": "Tao Sun, Yuejiao Sun, Wotao Yin", "title": "On Markov Chain Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient methods are the workhorse (algorithms) of large-scale\noptimization problems in machine learning, signal processing, and other\ncomputational sciences and engineering. This paper studies Markov chain\ngradient descent, a variant of stochastic gradient descent where the random\nsamples are taken on the trajectory of a Markov chain. Existing results of this\nmethod assume convex objectives and a reversible Markov chain and thus have\ntheir limitations. We establish new non-ergodic convergence under wider step\nsizes, for nonconvex problems, and for non-reversible finite-state Markov\nchains. Nonconvexity makes our method applicable to broader problem classes.\nNon-reversible finite-state Markov chains, on the other hand, can mix\nsubstatially faster. To obtain these results, we introduce a new technique that\nvaries the mixing levels of the Markov chains. The reported numerical results\nvalidate our contributions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 01:39:13 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sun", "Tao", ""], ["Sun", "Yuejiao", ""], ["Yin", "Wotao", ""]]}, {"id": "1809.04249", "submitter": "Lei Yang", "authors": "Lei Yang, Jia Li, Defeng Sun, Kim-Chuan Toh", "title": "A Fast Globally Linearly Convergent Algorithm for the Computation of\n  Wasserstein Barycenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a Wasserstein barycenter for a set of\ndiscrete probability distributions with finite supports, which finds many\napplications in areas such as statistics, machine learning and image\nprocessing. When the support points of the barycenter are pre-specified, this\nproblem can be modeled as a linear programming (LP) problem whose size can be\nextremely large. To handle this large-scale LP, we analyse the structure of its\ndual problem, which is conceivably more tractable and can be reformulated as a\nwell-structured convex problem with 3 kinds of block variables and a coupling\nlinear equality constraint. We then adapt a symmetric Gauss-Seidel based\nalternating direction method of multipliers (sGS-ADMM) to solve the resulting\ndual problem and establish its global convergence and global linear convergence\nrate. As a critical component for efficient computation, we also show how all\nthe subproblems involved can be solved exactly and efficiently. This makes our\nmethod suitable for computing a Wasserstein barycenter on a large-scale data\nset, without introducing an entropy regularization term as is commonly\npracticed. In addition, our sGS-ADMM can be used as a subroutine in an\nalternating minimization method to compute a barycenter when its support points\nare not pre-specified. Numerical results on synthetic data sets and image data\nsets demonstrate that our method is highly competitive for solving large-scale\nWasserstein barycenter problems, in comparison to two existing representative\nmethods and the commercial software Gurobi.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 04:13:48 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 02:36:49 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2019 07:24:30 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 08:56:56 GMT"}, {"version": "v5", "created": "Sat, 26 Dec 2020 11:35:30 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Yang", "Lei", ""], ["Li", "Jia", ""], ["Sun", "Defeng", ""], ["Toh", "Kim-Chuan", ""]]}, {"id": "1809.04262", "submitter": "Samiulla Shaikh", "authors": "Rashmi Nagpal, Chetna Wadhwa, Mallika Gupta, Samiulla Shaikh, Sameep\n  Mehta, Vikram Goyal", "title": "Extracting Fairness Policies from Legal Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning community is recently exploring the implications of bias and\nfairness with respect to the AI applications. The definition of fairness for\nsuch applications varies based on their domain of application. The policies\ngoverning the use of such machine learning system in a given context are\ndefined by the constitutional laws of nations and regulatory policies enforced\nby the organizations that are involved in the usage. Fairness related laws and\npolicies are often spread across the large documents like constitution,\nagreements, and organizational regulations. These legal documents have long\ncomplex sentences in order to achieve rigorousness and robustness. Automatic\nextraction of fairness policies, or in general, any specific kind of policies\nfrom large legal corpus can be very useful for the study of bias and fairness\nin the context of AI applications.\n  We attempted to automatically extract fairness policies from publicly\navailable law documents using two approaches based on semantic relatedness. The\nexperiments reveal how classical Wordnet-based similarity and vector-based\nsimilarity differ in addressing this task. We have shown that similarity based\non word vectors beats the classical approach with a large margin, whereas other\nvector representations of senses and sentences fail to even match the classical\nbaseline. Further, we have presented thorough error analysis and reasoning to\nexplain the results with appropriate examples from the dataset for deeper\ninsights.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 05:44:23 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Nagpal", "Rashmi", ""], ["Wadhwa", "Chetna", ""], ["Gupta", "Mallika", ""], ["Shaikh", "Samiulla", ""], ["Mehta", "Sameep", ""], ["Goyal", "Vikram", ""]]}, {"id": "1809.04270", "submitter": "Stratos Idreos", "authors": "Abdul Wasay, Brian Hentschel, Yuze Liao, Sanyuan Chen, Stratos Idreos", "title": "MotherNets: Rapid Deep Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of deep neural networks significantly improve generalization\naccuracy. However, training neural network ensembles requires a large amount of\ncomputational resources and time. State-of-the-art approaches either train all\nnetworks from scratch leading to prohibitive training cost that allows only\nvery small ensemble sizes in practice, or generate ensembles by training a\nmonolithic architecture, which results in lower model diversity and decreased\nprediction accuracy. We propose MotherNets to enable higher accuracy and\npractical training cost for large and diverse neural network ensembles: A\nMotherNet captures the structural similarity across some or all members of a\ndeep neural network ensemble which allows us to share data movement and\ncomputation costs across these networks. We first train a single or a small set\nof MotherNets and, subsequently, we generate the target ensemble networks by\ntransferring the function from the trained MotherNet(s). Then, we continue to\ntrain these ensemble networks, which now converge drastically faster compared\nto training from scratch. MotherNets handle ensembles with diverse\narchitectures by clustering ensemble networks of similar architecture and\ntraining a separate MotherNet for every cluster. MotherNets also use clustering\nto control the accuracy vs. training cost tradeoff. We show that compared to\nstate-of-the-art approaches such as Snapshot Ensembles, Knowledge Distillation,\nand TreeNets, MotherNets provide a new Pareto frontier for the\naccuracy-training cost tradeoff. Crucially, training cost and accuracy\nimprovements continue to scale as we increase the ensemble size (2 to 3 percent\nreduced absolute test error rate and up to 35 percent faster training compared\nto Snapshot Ensembles). We verify these benefits over numerous neural network\narchitectures and large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 06:36:31 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 02:53:18 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wasay", "Abdul", ""], ["Hentschel", "Brian", ""], ["Liao", "Yuze", ""], ["Chen", "Sanyuan", ""], ["Idreos", "Stratos", ""]]}, {"id": "1809.04279", "submitter": "Trefor Evans", "authors": "Trefor W. Evans, Prasanth B. Nair", "title": "Discretely Relaxing Continuous Variables for tractable Variational\n  Inference", "comments": "Appears in the proceedings of the Advances in Neural Information\n  Processing Systems (NeurIPS), 2018. Full code is available at\n  https://github.com/treforevans/direct", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new research direction in Bayesian variational inference with\ndiscrete latent variable priors where we exploit Kronecker matrix algebra for\nefficient and exact computations of the evidence lower bound (ELBO). The\nproposed \"DIRECT\" approach has several advantages over its predecessors; (i) it\ncan exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient\nestimates), eliminating the need for high-variance stochastic gradient\nestimators and enabling the use of quasi-Newton optimization methods; (ii) its\ntraining complexity is independent of the number of training points, permitting\ninference on large datasets; and (iii) its posterior samples consist of sparse\nand low-precision quantized integers which permit fast inference on hardware\nlimited devices. In addition, our DIRECT models can exactly compute statistical\nmoments of the parameterized predictive posterior without relying on Monte\nCarlo sampling. The DIRECT approach is not practical for all likelihoods,\nhowever, we identify a popular model structure which is practical, and\ndemonstrate accurate inference using latent variables discretized as extremely\nlow-precision 4-bit quantized integers. While the ELBO computations considered\nin the numerical studies require over $10^{2352}$ log-likelihood evaluations,\nwe train on datasets with over two-million points in just seconds.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 07:05:30 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 20:06:10 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 22:29:41 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Evans", "Trefor W.", ""], ["Nair", "Prasanth B.", ""]]}, {"id": "1809.04281", "submitter": "Cheng-Zhi Anna Huang", "authors": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer,\n  Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica\n  Dinculescu, Douglas Eck", "title": "Music Transformer", "comments": "Improved skewing section and accompanying figures. Previous titles\n  are \"An Improved Relative Self-Attention Mechanism for Transformer with\n  Application to Music Generation\" and \"Music Transformer\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music relies heavily on repetition to build structure and meaning.\nSelf-reference occurs on multiple timescales, from motifs to phrases to reusing\nof entire sections of music, such as in pieces with ABA structure. The\nTransformer (Vaswani et al., 2017), a sequence model based on self-attention,\nhas achieved compelling results in many generation tasks that require\nmaintaining long-range coherence. This suggests that self-attention might also\nbe well-suited to modeling music. In musical composition and performance,\nhowever, relative timing is critically important. Existing approaches for\nrepresenting relative positional information in the Transformer modulate\nattention based on pairwise distance (Shaw et al., 2018). This is impractical\nfor long sequences such as musical compositions since their memory complexity\nfor intermediate relative information is quadratic in the sequence length. We\npropose an algorithm that reduces their intermediate memory requirement to\nlinear in the sequence length. This enables us to demonstrate that a\nTransformer with our modified relative attention mechanism can generate\nminute-long compositions (thousands of steps, four times the length modeled in\nOore et al., 2018) with compelling structure, generate continuations that\ncoherently elaborate on a given motif, and in a seq2seq setup generate\naccompaniments conditioned on melodies. We evaluate the Transformer with our\nrelative attention mechanism on two datasets, JSB Chorales and\nPiano-e-Competition, and obtain state-of-the-art results on the latter.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 07:15:26 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 20:23:04 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 07:42:08 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Huang", "Cheng-Zhi Anna", ""], ["Vaswani", "Ashish", ""], ["Uszkoreit", "Jakob", ""], ["Shazeer", "Noam", ""], ["Simon", "Ian", ""], ["Hawthorne", "Curtis", ""], ["Dai", "Andrew M.", ""], ["Hoffman", "Matthew D.", ""], ["Dinculescu", "Monica", ""], ["Eck", "Douglas", ""]]}, {"id": "1809.04294", "submitter": "Dominik Linzner", "authors": "Dominik Linzner and Heinz Koeppl", "title": "Cluster Variational Approximations for Structure Learning of\n  Continuous-Time Bayesian Networks from Incomplete Data", "comments": "Accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time Bayesian networks (CTBNs) constitute a general and powerful\nframework for modeling continuous-time stochastic processes on networks. This\nmakes them particularly attractive for learning the directed structures among\ninteracting entities. However, if the available data is incomplete, one needs\nto simulate the prohibitively complex CTBN dynamics. Existing approximation\ntechniques, such as sampling and low-order variational methods, either scale\nunfavorably in system size, or are unsatisfactory in terms of accuracy.\nInspired by recent advances in statistical physics, we present a new\napproximation scheme based on cluster-variational methods significantly\nimproving upon existing variational approximations. We can analytically\nmarginalize the parameters of the approximate CTBN, as these are of secondary\nimportance for structure learning. This recovers a scalable scheme for direct\nstructure learning from incomplete and noisy time-series data. Our approach\noutperforms existing methods in terms of scalability.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 07:56:01 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 14:36:35 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 09:02:36 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 14:44:36 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Linzner", "Dominik", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1809.04356", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane\n  Idoumghar, Pierre-Alain Muller", "title": "Deep learning for time series classification: a review", "comments": "Accepted at Data Mining and Knowledge Discovery", "journal-ref": null, "doi": "10.1007/s10618-019-00619-1", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Series Classification (TSC) is an important and challenging problem in\ndata mining. With the increase of time series data availability, hundreds of\nTSC algorithms have been proposed. Among these methods, only a few have\nconsidered Deep Neural Networks (DNNs) to perform this task. This is surprising\nas deep learning has seen very successful applications in the last years. DNNs\nhave indeed revolutionized the field of computer vision especially with the\nadvent of novel deeper architectures such as Residual and Convolutional Neural\nNetworks. Apart from images, sequential data such as text and audio can also be\nprocessed with DNNs to reach state-of-the-art performance for document\nclassification and speech recognition. In this article, we study the current\nstate-of-the-art performance of deep learning algorithms for TSC by presenting\nan empirical study of the most recent DNN architectures for TSC. We give an\noverview of the most successful deep learning applications in various time\nseries domains under a unified taxonomy of DNNs for TSC. We also provide an\nopen source deep learning framework to the TSC community where we implemented\neach of the compared approaches and evaluated them on a univariate TSC\nbenchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By\ntraining 8,730 deep learning models on 97 time series datasets, we propose the\nmost exhaustive study of DNNs for TSC to date.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:55:33 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 17:49:17 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 08:14:31 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 14:41:18 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1809.04359", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Stefanos Zafeiriou", "title": "Training Deep Neural Networks with Different Datasets In-the-wild: The\n  Emotion Recognition Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel procedure is presented in this paper, for training a deep\nconvolutional and recurrent neural network, taking into account both the\navailable training data set and some information extracted from similar\nnetworks trained with other relevant data sets. This information is included in\nan extended loss function used for the network training, so that the network\ncan have an improved performance when applied to the other data sets, without\nforgetting the learned knowledge from the original data set. Facial expression\nand emotion recognition in-the-wild is the test bed application that is used to\ndemonstrate the improved performance achieved using the proposed approach. In\nthis framework, we provide an experimental study on categorical emotion\nrecognition using datasets from a very recent related emotion recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 11:16:31 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1809.04379", "submitter": "Yin Cheng Ng", "authors": "Yin Cheng Ng, Nicolo Colombo, Ricardo Silva", "title": "Bayesian Semi-supervised Learning with Graph Gaussian Processes", "comments": "To appear in NIPS 2018 Fixed an error in Figure 2. The previous arxiv\n  version contains two identical sub-figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-efficient Gaussian process-based Bayesian approach to the\nsemi-supervised learning problem on graphs. The proposed model shows extremely\ncompetitive performance when compared to the state-of-the-art graph neural\nnetworks on semi-supervised learning benchmark experiments, and outperforms the\nneural networks in active learning experiments where labels are scarce.\nFurthermore, the model does not require a validation data set for early\nstopping to control over-fitting. Our model can be viewed as an instance of\nempirical distribution regression weighted locally by network connectivity. We\nfurther motivate the intuitive construction of the model with a Bayesian linear\nmodel interpretation where the node features are filtered by an operator\nrelated to the graph Laplacian. The method can be easily implemented by\nadapting off-the-shelf scalable variational inference algorithms for Gaussian\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 12:26:00 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 15:30:20 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 15:30:26 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Ng", "Yin Cheng", ""], ["Colombo", "Nicolo", ""], ["Silva", "Ricardo", ""]]}, {"id": "1809.04400", "submitter": "Martin Trapp", "authors": "Martin Trapp, Robert Peharz, Carl E. Rasmussen and Franz Pernkopf", "title": "Learning Deep Mixtures of Gaussian Process Experts Using Sum-Product\n  Networks", "comments": "Presented at the Workshop on Tractable Probabilistic Models (TPM\n  2018), ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Gaussian processes (GPs) are the method of choice for regression tasks,\nthey also come with practical difficulties, as inference cost scales cubic in\ntime and quadratic in memory. In this paper, we introduce a natural and\nexpressive way to tackle these problems, by incorporating GPs in sum-product\nnetworks (SPNs), a recently proposed tractable probabilistic model allowing\nexact and efficient inference. In particular, by using GPs as leaves of an SPN\nwe obtain a novel flexible prior over functions, which implicitly represents an\nexponentially large mixture of local GPs. Exact and efficient posterior\ninference in this model can be done in a natural interplay of the inference\nmechanisms in GPs and SPNs. Thereby, each GP is -- similarly as in a mixture of\nexperts approach -- responsible only for a subset of data points, which\neffectively reduces inference cost in a divide and conquer fashion. We show\nthat integrating GPs into the SPN framework leads to a promising probabilistic\nregression model which is: (1) computational and memory efficient, (2) allows\nefficient and exact posterior inference, (3) is flexible enough to mix\ndifferent kernel functions, and (4) naturally accounts for non-stationarities\nin time series. In a variate of experiments, we show that the SPN-GP model can\nlearn input dependent parameters and hyper-parameters and is on par with or\noutperforms the traditional GPs as well as state of the art approximations on\nreal-world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:12:21 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Trapp", "Martin", ""], ["Peharz", "Robert", ""], ["Rasmussen", "Carl E.", ""], ["Pernkopf", "Franz", ""]]}, {"id": "1809.04423", "submitter": "Ramin M. Hasani", "authors": "Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, Radu\n  Grosu", "title": "Can a Compact Neuronal Circuit Policy be Re-purposed to Learn Simple\n  Robotic Control?", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.08554", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural information processing system which is obtained by\nre-purposing the function of a biological neural circuit model, to govern\nsimulated and real-world control tasks. Inspired by the structure of the\nnervous system of the soil-worm, C. elegans, we introduce Neuronal Circuit\nPolicies (NCPs), defined as the model of biological neural circuits\nreparameterized for the control of an alternative task. We learn instances of\nNCPs to control a series of robotic tasks, including the autonomous parking of\na real-world rover robot. For reconfiguration of the purpose of the neural\ncircuit, we adopt a search-based optimization algorithm. Neuronal circuit\npolicies perform on par and in some cases surpass the performance of\ncontemporary deep learning models with the advantage leveraging significantly\nfewer learnable parameters and realizing interpretable dynamics at the\ncell-level.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:05:12 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 13:12:37 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hasani", "Ramin", ""], ["Lechner", "Mathias", ""], ["Amini", "Alexander", ""], ["Rus", "Daniela", ""], ["Grosu", "Radu", ""]]}, {"id": "1809.04429", "submitter": "Muhammad Yousefnezhad", "authors": "Xiaoliang Sheng, Muhammad Yousefnezhad, Tonglin Xu, Ning Yuan,\n  Daoqiang Zhang", "title": "Gradient-based Representational Similarity Analysis with Searchlight for\n  Analyzing fMRI Data", "comments": "Conference: Chinese Conference on Pattern Recognition and Computer\n  Vision 2018 (PRCV18), 23-26/Nov, Guangzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representational Similarity Analysis (RSA) aims to explore similarities\nbetween neural activities of different stimuli. Classical RSA techniques employ\nthe inverse of the covariance matrix to explore a linear model between the\nneural activities and task events. However, calculating the inverse of a\nlarge-scale covariance matrix is time-consuming and can reduce the stability\nand robustness of the final analysis. Notably, it becomes severe when the\nnumber of samples is too large. For facing this shortcoming, this paper\nproposes a novel RSA method called gradient-based RSA (GRSA). Moreover, the\nproposed method is not restricted to a linear model. In fact, there is a\ngrowing interest in finding more effective ways of using multi-subject and\nwhole-brain fMRI data. Searchlight technique can extend RSA from the localized\nbrain regions to the whole-brain regions with smaller memory footprint in each\nprocess. Based on Searchlight, we propose a new method called Spatiotemporal\nSearchlight GRSA (SSL-GRSA) that generalizes our ROI-based GRSA algorithm to\nthe whole-brain data. Further, our approach can handle some computational\nchallenges while dealing with large-scale, multi-subject fMRI data.\nExperimental studies on multi-subject datasets confirm that both proposed\napproaches achieve superior performance to other state-of-the-art RSA\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:40:59 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sheng", "Xiaoliang", ""], ["Yousefnezhad", "Muhammad", ""], ["Xu", "Tonglin", ""], ["Yuan", "Ning", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1809.04430", "submitter": "Stanislav Nikolov", "authors": "Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes,\n  Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham,\n  Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton\n  Chu, Dawn Carnell, Cheng Boon, Derek D'Souza, Syed Ali Moinuddin, Bethany\n  Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh\n  Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, C\\'ian Hughes,\n  Joseph R. Ledsam, Olaf Ronneberger", "title": "Deep learning to achieve clinically applicable segmentation of head and\n  neck anatomy for radiotherapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over half a million individuals are diagnosed with head and neck cancer each\nyear worldwide. Radiotherapy is an important curative treatment for this\ndisease, but it requires manual time consuming delineation of radio-sensitive\norgans at risk (OARs). This planning process can delay treatment, while also\nintroducing inter-operator variability with resulting downstream radiation dose\ndifferences. While auto-segmentation algorithms offer a potentially time-saving\nsolution, the challenges in defining, quantifying and achieving expert\nperformance remain. Adopting a deep learning approach, we demonstrate a 3D\nU-Net architecture that achieves expert-level performance in delineating 21\ndistinct head and neck OARs commonly segmented in clinical practice. The model\nwas trained on a dataset of 663 deidentified computed tomography (CT) scans\nacquired in routine clinical practice and with both segmentations taken from\nclinical practice and segmentations created by experienced radiographers as\npart of this research, all in accordance with consensus OAR definitions. We\ndemonstrate the model's clinical applicability by assessing its performance on\na test set of 21 CT scans from clinical practice, each with the 21 OARs\nsegmented by two independent experts. We also introduce surface Dice similarity\ncoefficient (surface DSC), a new metric for the comparison of organ\ndelineation, to quantify deviation between OAR surface contours rather than\nvolumes, better reflecting the clinical task of correcting errors in the\nautomated organ segmentations. The model's generalisability is then\ndemonstrated on two distinct open source datasets, reflecting different centres\nand countries to model training. With appropriate validation studies and\nregulatory approvals, this system could improve the efficiency, consistency,\nand safety of radiotherapy pathways.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:42:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 11:09:59 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:43:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nikolov", "Stanislav", ""], ["Blackwell", "Sam", ""], ["Zverovitch", "Alexei", ""], ["Mendes", "Ruheena", ""], ["Livne", "Michelle", ""], ["De Fauw", "Jeffrey", ""], ["Patel", "Yojan", ""], ["Meyer", "Clemens", ""], ["Askham", "Harry", ""], ["Romera-Paredes", "Bernardino", ""], ["Kelly", "Christopher", ""], ["Karthikesalingam", "Alan", ""], ["Chu", "Carlton", ""], ["Carnell", "Dawn", ""], ["Boon", "Cheng", ""], ["D'Souza", "Derek", ""], ["Moinuddin", "Syed Ali", ""], ["Garie", "Bethany", ""], ["McQuinlan", "Yasmin", ""], ["Ireland", "Sarah", ""], ["Hampton", "Kiarna", ""], ["Fuller", "Krystle", ""], ["Montgomery", "Hugh", ""], ["Rees", "Geraint", ""], ["Suleyman", "Mustafa", ""], ["Back", "Trevor", ""], ["Hughes", "C\u00edan", ""], ["Ledsam", "Joseph R.", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1809.04432", "submitter": "Isaac Karth", "authors": "Isaac Karth, Adam M. Smith", "title": "Addressing the Fundamental Tension of PCGML with Discriminative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural content generation via machine learning (PCGML) is typically\nframed as the task of fitting a generative model to full-scale examples of a\ndesired content distribution. This approach presents a fundamental tension: the\nmore design effort expended to produce detailed training examples for shaping a\ngenerator, the lower the return on investment from applying PCGML in the first\nplace. In response, we propose the use of discriminative models (which capture\nthe validity of a design rather the distribution of the content) trained on\npositive and negative examples. Through a modest modification of\nWaveFunctionCollapse, a commercially-adopted PCG approach that we characterize\nas using elementary machine learning, we demonstrate a new mode of control for\nlearning-based generators. We demonstrate how an artist might craft a focused\nset of additional positive and negative examples by critique of the generator's\nprevious outputs. This interaction mode bridges PCGML with mixed-initiative\ndesign assistance tools by working with a machine to define a space of valid\ndesigns rather than just one new design.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 20:04:59 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Karth", "Isaac", ""], ["Smith", "Adam M.", ""]]}, {"id": "1809.04440", "submitter": "Yunsheng Bai", "authors": "Yunsheng Bai, Hao Ding, Yizhou Sun, Wei Wang", "title": "Learning-based Efficient Graph Similarity Computation via Multi-Scale\n  Convolutional Set Matching", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity computation is one of the core operations in many\ngraph-based applications, such as graph similarity search, graph database\nanalysis, graph clustering, etc. Since computing the exact distance/similarity\nbetween two graphs is typically NP-hard, a series of approximate methods have\nbeen proposed with a trade-off between accuracy and speed. Recently, several\ndata-driven approaches based on neural networks have been proposed, most of\nwhich model the graph-graph similarity as the inner product of their\ngraph-level representations, with different techniques proposed for generating\none embedding per graph. However, using one fixed-dimensional embedding per\ngraph may fail to fully capture graphs in varying sizes and link structures, a\nlimitation that is especially problematic for the task of graph similarity\ncomputation, where the goal is to find the fine-grained difference between two\ngraphs. In this paper, we address the problem of graph similarity computation\nfrom another perspective, by directly matching two sets of node embeddings\nwithout the need to use fixed-dimensional vectors to represent whole graphs for\ntheir similarity computation. The model, GraphSim, achieves the\nstate-of-the-art performance on four real-world graph datasets under six out of\neight settings (here we count a specific dataset and metric combination as one\nsetting), compared to existing popular methods for approximate Graph Edit\nDistance (GED) and Maximum Common Subgraph (MCS) computation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 22:48:49 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 20:23:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bai", "Yunsheng", ""], ["Ding", "Hao", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "1809.04441", "submitter": "Zhuqing Liu", "authors": "Zhuqing Liu, Liyuanjun Lai, Lin Zhang", "title": "An empirical learning-based validation procedure for simulation workflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation workflow is a top-level model for the design and control of\nsimulation process. It connects multiple simulation components with time and\ninteraction restrictions to form a complete simulation system. Before the\nconstruction and evaluation of the component models, the validation of\nupper-layer simulation workflow is of the most importance in a simulation\nsystem. However, the methods especially for validating simulation workflow is\nvery limit. Many of the existing validation techniques are domain-dependent\nwith cumbersome questionnaire design and expert scoring. Therefore, this paper\npresent an empirical learning-based validation procedure to implement a\nsemi-automated evaluation for simulation workflow. First, representative\nfeatures of general simulation workflow and their relations with validation\nindices are proposed. The calculation process of workflow credibility based on\nAnalytic Hierarchy Process (AHP) is then introduced. In order to make full use\nof the historical data and implement more efficient validation, four learning\nalgorithms, including back propagation neural network (BPNN), extreme learning\nmachine (ELM), evolving new-neuron (eNFN) and fast incremental gaussian mixture\nmodel (FIGMN), are introduced for constructing the empirical relation between\nthe workflow credibility and its features. A case study on a landing-process\nsimulation workflow is established to test the feasibility of the proposed\nprocedure. The experimental results also provide some useful overview of the\nstate-of-the-art learning algorithms on the credibility evaluation of\nsimulation models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 00:48:04 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Liu", "Zhuqing", ""], ["Lai", "Liyuanjun", ""], ["Zhang", "Lin", ""]]}, {"id": "1809.04445", "submitter": "Vishnu Menon", "authors": "Vishnu Menon, Sheetal Kalyani", "title": "Structured and Unstructured Outlier Identification for Robust PCA: A Non\n  iterative, Parameter free Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.04791", "journal-ref": null, "doi": "10.1109/TSP.2019.2905826", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA, the problem of PCA in the presence of outliers has been\nextensively investigated in the last few years. Here we focus on Robust PCA in\nthe outlier model where each column of the data matrix is either an inlier or\nan outlier. Most of the existing methods for this model assumes either the\nknowledge of the dimension of the lower dimensional subspace or the fraction of\noutliers in the system. However in many applications knowledge of these\nparameters is not available. Motivated by this we propose a parameter free\noutlier identification method for robust PCA which a) does not require the\nknowledge of outlier fraction, b) does not require the knowledge of the\ndimension of the underlying subspace, c) is computationally simple and fast d)\ncan handle structured and unstructured outliers. Further, analytical guarantees\nare derived for outlier identification and the performance of the algorithm is\ncompared with the existing state of the art methods in both real and synthetic\ndata for various outlier structures.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 07:47:57 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Menon", "Vishnu", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1809.04461", "submitter": "Vinayakumar R", "authors": "Anu Vazhayil, Vinayakumar R and Soman KP", "title": "DeepProteomics: Protein family classification using Shallow and Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge regarding the function of proteins is necessary as it gives a\nclear picture of biological processes. Nevertheless, there are many protein\nsequences found and added to the databases but lacks functional annotation. The\nlaboratory experiments take a considerable amount of time for annotation of the\nsequences. This arises the need to use computational techniques to classify\nproteins based on their functions. In our work, we have collected the data from\nSwiss-Prot containing 40433 proteins which is grouped into 30 families. We pass\nit to recurrent neural network(RNN), long short term memory(LSTM) and gated\nrecurrent unit(GRU) model and compare it by applying trigram with deep neural\nnetwork and shallow neural network on the same dataset. Through this approach,\nwe could achieve maximum of around 78% accuracy for the classification of\nprotein families.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:48:01 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Vazhayil", "Anu", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "1809.04474", "submitter": "Matteo Hessel", "authors": "Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon\n  Schmitt, Hado van Hasselt", "title": "Multi-task Deep Reinforcement Learning with PopArt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reinforcement learning community has made great strides in designing\nalgorithms capable of exceeding human performance on specific tasks. These\nalgorithms are mostly trained one task at the time, each new task requiring to\ntrain a brand new agent instance. This means the learning algorithm is general,\nbut each solution is not; each agent can only solve the one task it was trained\non. In this work, we study the problem of learning to master not one but\nmultiple sequential-decision tasks at once. A general issue in multi-task\nlearning is that a balance must be found between the needs of multiple tasks\ncompeting for the limited resources of a single learning system. Many learning\nalgorithms can get distracted by certain tasks in the set of tasks to solve.\nSuch tasks appear more salient to the learning process, for instance because of\nthe density or magnitude of the in-task rewards. This causes the algorithm to\nfocus on those salient tasks at the expense of generality. We propose to\nautomatically adapt the contribution of each task to the agent's updates, so\nthat all tasks have a similar impact on the learning dynamics. This resulted in\nstate of the art performance on learning to play all games in a set of 57\ndiverse Atari games. Excitingly, our method learned a single trained policy -\nwith a single set of weights - that exceeds median human performance. To our\nknowledge, this was the first time a single agent surpassed human-level\nperformance on this multi-task domain. The same approach also demonstrated\nstate of the art performance on a set of 30 tasks in the 3D reinforcement\nlearning platform DeepMind Lab.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:17:00 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Hessel", "Matteo", ""], ["Soyer", "Hubert", ""], ["Espeholt", "Lasse", ""], ["Czarnecki", "Wojciech", ""], ["Schmitt", "Simon", ""], ["van Hasselt", "Hado", ""]]}, {"id": "1809.04481", "submitter": "Yitong Sun", "authors": "Yitong Sun, Anna Gilbert, Ambuj Tewari", "title": "But How Does It Work in Theory? Linear SVM with Random Features", "comments": "Accepted by NeurIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that, under low noise assumptions, the support vector machine with\n$N\\ll m$ random features (RFSVM) can achieve the learning rate faster than\n$O(1/\\sqrt{m})$ on a training set with $m$ samples when an optimized feature\nmap is used. Our work extends the previous fast rate analysis of random\nfeatures method from least square loss to 0-1 loss. We also show that the\nreweighted feature selection method, which approximates the optimized feature\nmap, helps improve the performance of RFSVM in experiments on a synthetic data\nset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:29:05 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 00:41:55 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 06:25:08 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Sun", "Yitong", ""], ["Gilbert", "Anna", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1809.04487", "submitter": "Jayesh Choudhari", "authors": "Srikanta Bedathur, Indrajit Bhattacharya, Jayesh Choudhari and Anirban\n  Dasgupta", "title": "Discovering Topical Interactions in Text-based Cascades using Hidden\n  Markov Hawkes Processes", "comments": "Accepted as a short paper at ICDM-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media conversations unfold based on complex interactions between\nusers, topics and time. While recent models have been proposed to capture\nnetwork strengths between users, users' topical preferences and temporal\npatterns between posting and response times, interaction patterns between\ntopics has not been studied. We propose the Hidden Markov Hawkes Process (HMHP)\nthat incorporates topical Markov Chains within Hawkes processes to jointly\nmodel topical interactions along with user-user and user-topic patterns. We\npropose a Gibbs sampling algorithm for HMHP that jointly infers the network\nstrengths, diffusion paths, the topics of the posts as well as the topic-topic\ninteractions. We show using experiments on real and semi-synthetic data that\nHMHP is able to generalize better and recover the network strengths, topics and\ndiffusion paths more accurately than state-of-the-art baselines. More\ninterestingly, HMHP finds insightful interactions between topics in real tweets\nwhich no existing model is able to do.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:36:27 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bedathur", "Srikanta", ""], ["Bhattacharya", "Indrajit", ""], ["Choudhari", "Jayesh", ""], ["Dasgupta", "Anirban", ""]]}, {"id": "1809.04497", "submitter": "Abdul Fatir Ansari", "authors": "Abdul Fatir Ansari and Harold Soh", "title": "Hyperprior Induced Unsupervised Disentanglement of Latent\n  Representations", "comments": "AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of unsupervised disentanglement of latent\nrepresentations learnt via deep generative models. In contrast to current\napproaches that operate on the evidence lower bound (ELBO), we argue that\nstatistical independence in the latent space of VAEs can be enforced in a\nprincipled hierarchical Bayesian manner. To this effect, we augment the\nstandard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the\nlatent code. By tuning the IW parameters, we are able to encourage (or\ndiscourage) independence in the learnt latent dimensions. Extensive\nexperimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and\nCelebA) show our approach to outperform the $\\beta$-VAE and is competitive with\nthe state-of-the-art FactorVAE. Our approach achieves significantly better\ndisentanglement and reconstruction on a new dataset (CorrelatedEllipses) which\nintroduces correlations between the factors of variation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:53:19 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 10:17:22 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 09:30:19 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ansari", "Abdul Fatir", ""], ["Soh", "Harold", ""]]}, {"id": "1809.04506", "submitter": "Vincent Francois-Lavet", "authors": "Vincent Fran\\c{c}ois-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau", "title": "Combined Reinforcement Learning via Abstract Representations", "comments": "Accepted to the Thirty-Third AAAI Conference On Artificial\n  Intelligence, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest for efficient and robust reinforcement learning methods, both\nmodel-free and model-based approaches offer advantages. In this paper we\npropose a new way of explicitly bridging both approaches via a shared\nlow-dimensional learned encoding of the environment, meant to capture\nsummarizing abstractions. We show that the modularity brought by this approach\nleads to good generalization while being computationally efficient, with\nplanning happening in a smaller latent state space. In addition, this approach\nrecovers a sufficient low-dimensional representation of the environment, which\nopens up new strategies for interpretable AI, exploration and transfer\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 15:12:49 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 23:47:15 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Fran\u00e7ois-Lavet", "Vincent", ""], ["Bengio", "Yoshua", ""], ["Precup", "Doina", ""], ["Pineau", "Joelle", ""]]}, {"id": "1809.04542", "submitter": "Shuang Liu", "authors": "Shuang Liu and Kamalika Chaudhuri", "title": "The Inductive Bias of Restricted f-GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks are a novel method for statistical inference\nthat have achieved much empirical success; however, the factors contributing to\nthis success remain ill-understood. In this work, we attempt to analyze\ngenerative adversarial learning -- that is, statistical inference as the result\nof a game between a generator and a discriminator -- with the view of\nunderstanding how it differs from classical statistical inference solutions\nsuch as maximum likelihood inference and the method of moments.\n  Specifically, we provide a theoretical characterization of the distribution\ninferred by a simple form of generative adversarial learning called restricted\nf-GANs -- where the discriminator is a function in a given function class, the\ndistribution induced by the generator is restricted to lie in a pre-specified\ndistribution class and the objective is similar to a variational form of the\nf-divergence. A consequence of our result is that for linear KL-GANs -- that\nis, when the discriminator is a linear function over some feature space and f\ncorresponds to the KL-divergence -- the distribution induced by the optimal\ngenerator is neither the maximum likelihood nor the method of moments solution,\nbut an interesting combination of both.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:19:49 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Liu", "Shuang", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1809.04547", "submitter": "Ole-Christoffer Granmo", "authors": "Geir Thore Berge, Ole-Christoffer Granmo, Tor Oddbj{\\o}rn Tveit,\n  Morten Goodwin, Lei Jiao, Bernt Viggo Matheussen", "title": "Using the Tsetlin Machine to Learn Human-Interpretable Rules for\n  High-Accuracy Text Categorization with Medical Applications", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical applications challenge today's text categorization techniques by\ndemanding both high accuracy and ease-of-interpretation. Although deep learning\nhas provided a leap ahead in accuracy, this leap comes at the sacrifice of\ninterpretability. To address this accuracy-interpretability challenge, we here\nintroduce, for the first time, a text categorization approach that leverages\nthe recently introduced Tsetlin Machine. In all brevity, we represent the terms\nof a text as propositional variables. From these, we capture categories using\nsimple propositional formulae, such as: if \"rash\" and \"reaction\" and\n\"penicillin\" then Allergy. The Tsetlin Machine learns these formulae from a\nlabelled text, utilizing conjunctive clauses to represent the particular facets\nof each category. Indeed, even the absence of terms (negated features) can be\nused for categorization purposes. Our empirical comparison with Na\\\"ive Bayes,\ndecision trees, linear support vector machines (SVMs), random forest, long\nshort-term memory (LSTM) neural networks, and other techniques, is quite\nconclusive. The Tsetlin Machine either performs on par with or outperforms all\nof the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well\nas on a non-public clinical dataset. On average, the Tsetlin Machine delivers\nthe best recall and precision scores across the datasets. Finally, our GPU\nimplementation of the Tsetlin Machine executes 5 to 15 times faster than the\nCPU implementation, depending on the dataset. We thus believe that our novel\napproach can have a significant impact on a wide range of text analysis\napplications, forming a promising starting point for deeper natural language\nunderstanding with the Tsetlin Machine.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:34:44 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 21:59:17 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Berge", "Geir Thore", ""], ["Granmo", "Ole-Christoffer", ""], ["Tveit", "Tor Oddbj\u00f8rn", ""], ["Goodwin", "Morten", ""], ["Jiao", "Lei", ""], ["Matheussen", "Bernt Viggo", ""]]}, {"id": "1809.04559", "submitter": "Andreea Anghel", "authors": "Andreea Anghel, Nikolaos Papandreou, Thomas Parnell, Alessandro De\n  Palma, Haralampos Pozidis", "title": "Benchmarking and Optimization of Gradient Boosting Decision Tree\n  Algorithms", "comments": "Workshop on Systems for ML and Open Source Software at NeurIPS 2018,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting decision trees (GBDTs) have seen widespread adoption in\nacademia, industry and competitive data science due to their state-of-the-art\nperformance in many machine learning tasks. One relative downside to these\nmodels is the large number of hyper-parameters that they expose to the\nend-user. To maximize the predictive power of GBDT models, one must either\nmanually tune the hyper-parameters, or utilize automated techniques such as\nthose based on Bayesian optimization. Both of these approaches are\ntime-consuming since they involve repeatably training the model for different\nsets of hyper-parameters. A number of software GBDT packages have started to\noffer GPU acceleration which can help to alleviate this problem. In this paper,\nwe consider three such packages: XGBoost, LightGBM and Catboost. Firstly, we\nevaluate the performance of the GPU acceleration provided by these packages\nusing large-scale datasets with varying shapes, sparsities and learning tasks.\nThen, we compare the packages in the context of hyper-parameter optimization,\nboth in terms of how quickly each package converges to a good validation score,\nand in terms of generalization performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:51:18 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 16:38:05 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 12:40:35 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Anghel", "Andreea", ""], ["Papandreou", "Nikolaos", ""], ["Parnell", "Thomas", ""], ["De Palma", "Alessandro", ""], ["Pozidis", "Haralampos", ""]]}, {"id": "1809.04564", "submitter": "Ali Ramezani-Kebrya", "authors": "Ali Ramezani-Kebrya, Ashish Khisti, Ben Liang", "title": "On the Stability and Convergence of Stochastic Gradient Descent with\n  Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While momentum-based methods, in conjunction with the stochastic gradient\ndescent, are widely used when training machine learning models, there is little\ntheoretical understanding on the generalization error of such methods. In\npractice, the momentum parameter is often chosen in a heuristic fashion with\nlittle theoretical guidance. In the first part of this paper, for the case of\ngeneral loss functions, we analyze a modified momentum-based update rule, i.e.,\nthe method of early momentum, and develop an upper-bound on the generalization\nerror using the framework of algorithmic stability. Our results show that\nmachine learning models can be trained for multiple epochs of this method while\ntheir generalization errors are bounded. We also study the convergence of the\nmethod of early momentum by establishing an upper-bound on the expected norm of\nthe gradient. In the second part of the paper, we focus on the case of strongly\nconvex loss functions and the classical heavy-ball momentum update rule. We use\nthe framework of algorithmic stability to provide an upper-bound on the\ngeneralization error of the stochastic gradient method with momentum. We also\ndevelop an upper-bound on the expected true risk, in terms of the number of\ntraining steps, the size of the training set, and the momentum parameter.\nExperimental evaluations verify the consistency between the numerical results\nand our theoretical bounds and the effectiveness of the method of early\nmomentum for the case of non-convex loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:02:08 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Ramezani-Kebrya", "Ali", ""], ["Khisti", "Ashish", ""], ["Liang", "Ben", ""]]}, {"id": "1809.04578", "submitter": "Jon Kleinberg", "authors": "Jon Kleinberg and Sendhil Mullainathan", "title": "Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and\n  Interpretability", "comments": "Updated version incorporating additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are increasingly used to aid, or in some cases supplant, human\ndecision-making, particularly for decisions that hinge on predictions. As a\nresult, two additional features in addition to prediction quality have\ngenerated interest: (i) to facilitate human interaction and understanding with\nthese algorithms, we desire prediction functions that are in some fashion\nsimple or interpretable; and (ii) because they influence consequential\ndecisions, we also want them to produce equitable allocations. We develop a\nformal model to explore the relationship between the demands of simplicity and\nequity. Although the two concepts appear to be motivated by qualitatively\ndistinct goals, we show a fundamental inconsistency between them. Specifically,\nwe formalize a general framework for producing simple prediction functions, and\nin this framework we establish two basic results. First, every simple\nprediction function is strictly improvable: there exists a more complex\nprediction function that is both strictly more efficient and also strictly more\nequitable. Put another way, using a simple prediction function both reduces\nutility for disadvantaged groups and reduces overall welfare relative to other\noptions. Second, we show that simple prediction functions necessarily create\nincentives to use information about individuals' membership in a disadvantaged\ngroup --- incentives that weren't present before simplification, and that work\nagainst these individuals. Thus, simplicity transforms disadvantage into bias\nagainst the disadvantaged group. Our results are not only about algorithms but\nabout any process that produces simple models, and as such they connect to the\npsychology of stereotypes and to an earlier economics literature on statistical\ndiscrimination.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:40:18 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 02:50:47 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""]]}, {"id": "1809.04587", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Stefano Marano", "title": "Distributed Chernoff Test: Optimal decision systems over networks", "comments": "A part of this work has been accepted in ISIT 2018 and CDC 2018;\n  Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.MA math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \"active\" decision making over sensor networks where the sensors'\nsequential probing actions are actively chosen by continuously learning from\npast observations. We consider two network settings: with and without central\ncoordination. In the first case, the network nodes interact with each other\nthrough a central entity, which plays the role of a fusion center. In the\nsecond case, the network nodes interact in a fully distributed fashion. In both\nof these scenarios, we propose sequential and adaptive hypothesis tests\nextending the classic Chernoff test. We compare the performance of the proposed\ntests to the optimal sequential test. In the presence of a fusion center, our\ntest achieves the same asymptotic optimality of the Chernoff test, minimizing\nthe risk, expressed by the expected cost required to reach a decision plus the\nexpected cost of making a wrong decision, when the observation cost per unit\ntime tends to zero. The test is also asymptotically optimal in the higher\nmoments of the time required to reach a decision. Additionally, the test is\nparsimonious in terms of communications, and the expected number of channel\nuses per network node tends to a small constant. In the distributed setup, our\ntest achieves the same asymptotic optimality of Chernoff's test, up to a\nmultiplicative constant in terms of both risk and the higher moments of the\ndecision time. Additionally, the test is parsimonious in terms of\ncommunications in comparison to state-of-the-art schemes proposed in the\nliterature. The analysis of these tests is also extended to account for message\nquantization and communication over channels with random erasures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:51:30 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 14:31:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Marano", "Stefano", ""]]}, {"id": "1809.04598", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Michael Hobson, Anthony Lasenby", "title": "Bayesian sparse reconstruction: a brute-force approach to astronomical\n  imaging and machine learning", "comments": "18 pages + appendix, 19 figures, minor updates to text and layout", "journal-ref": "Mon. Notices Royal Astron. Soc. 483, 4 (2019) p4828-4846", "doi": "10.1093/mnras/sty3307", "report-no": null, "categories": "astro-ph.IM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled Bayesian framework for signal reconstruction, in\nwhich the signal is modelled by basis functions whose number (and form, if\nrequired) is determined by the data themselves. This approach is based on a\nBayesian interpretation of conventional sparse reconstruction and\nregularisation techniques, in which sparsity is imposed through priors via\nBayesian model selection. We demonstrate our method for noisy 1- and\n2-dimensional signals, including astronomical images. Furthermore, by using a\nproduct-space approach, the number and type of basis functions can be treated\nas integer parameters and their posterior distributions sampled directly. We\nshow that order-of-magnitude increases in computational efficiency are possible\nfrom this technique compared to calculating the Bayesian evidences separately,\nand that further computational gains are possible using it in combination with\ndynamic nested sampling. Our approach can also be readily applied to neural\nnetworks, where it allows the network architecture to be determined by the data\nin a principled Bayesian manner by treating the number of nodes and hidden\nlayers as parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 17:59:37 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Michael", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1809.04663", "submitter": "Stephen Pfohl", "authors": "Stephen Pfohl, Ben Marafino, Adrien Coulet, Fatima Rodriguez, Latha\n  Palaniappan, Nigam H. Shah", "title": "Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guidelines for the management of atherosclerotic cardiovascular disease\n(ASCVD) recommend the use of risk stratification models to identify patients\nmost likely to benefit from cholesterol-lowering and other therapies. These\nmodels have differential performance across race and gender groups with\ninconsistent behavior across studies, potentially resulting in an inequitable\ndistribution of beneficial therapy. In this work, we leverage adversarial\nlearning and a large observational cohort extracted from electronic health\nrecords (EHRs) to develop a \"fair\" ASCVD risk prediction model with reduced\nvariability in error rates across groups. We empirically demonstrate that our\napproach is capable of aligning the distribution of risk predictions\nconditioned on the outcome across several groups simultaneously for models\nbuilt from high-dimensional EHR data. We also discuss the relevance of these\nresults in the context of the empirical trade-off between fairness and model\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 20:28:29 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 00:04:36 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 04:50:04 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Pfohl", "Stephen", ""], ["Marafino", "Ben", ""], ["Coulet", "Adrien", ""], ["Rodriguez", "Fatima", ""], ["Palaniappan", "Latha", ""], ["Shah", "Nigam H.", ""]]}, {"id": "1809.04668", "submitter": "Balaji Sesha Sarath Pokuri", "authors": "Balaji Sesha Sarath Pokuri, Alec Lofquist, Chad M Risko and Baskar\n  Ganapathysubramanian", "title": "PARyOpt: A software for Parallel Asynchronous Remote Bayesian\n  Optimization", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PARyOpt is a python based implementation of the Bayesian optimization routine\ndesigned for remote and asynchronous function evaluations. Bayesian\noptimization is especially attractive for computational optimization due to its\nlow cost function footprint as well as the ability to account for uncertainties\nin data. A key challenge to efficiently deploy any optimization strategy on\ndistributed computing systems is the synchronization step, where data from\nmultiple function calls is assimilated to identify the next campaign of\nfunction calls. Bayesian optimization provides an elegant approach to overcome\nthis issue via asynchronous updates. We formulate, develop and implement a\nparallel, asynchronous variant of Bayesian optimization. The framework is\nrobust and resilient to external failures. We show how such asynchronous\nevaluations help reduce the total optimization wall clock time for a suite of\ntest problems. Additionally, we show how the software design of the framework\nallows easy extension to response surface reconstruction (Kriging), providing a\nhigh performance software for autonomous exploration. The software is available\non PyPI, with examples and documentation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 20:50:19 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Pokuri", "Balaji Sesha Sarath", ""], ["Lofquist", "Alec", ""], ["Risko", "Chad M", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "1809.04673", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Nimit Acharya, Tanuja Bompada, Denis Charles, Eren\n  Manavoglu", "title": "A Unified Batch Online Learning Framework for Click Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for Batch Online Learning (OL) for Click\nPrediction in Search Advertisement. Machine Learning models once deployed, show\nnon-trivial accuracy and calibration degradation over time due to model\nstaleness. It is therefore necessary to regularly update models, and do so\nautomatically. This paper presents two paradigms of Batch Online Learning, one\nwhich incrementally updates the model parameters via an early stopping\nmechanism, and another which does so through a proximal regularization. We\nargue how both these schemes naturally trade-off between old and new data. We\nthen theoretically and empirically show that these two seemingly different\nschemes are closely related. Through extensive experiments, we demonstrate the\nutility of of our OL framework; how the two OL schemes relate to each other and\nhow they trade-off between the new and historical data. We then compare batch\nOL to full model retrains, and show how online learning is more robust to data\nissues. We also demonstrate the long term impact of Online Learning, the role\nof the initial Models in OL, the impact of delays in the update, and finally\nconclude with some implementation details and challenges in deploying a real\nworld online learning system in production. While this paper mostly focuses on\napplication of click prediction for search advertisement, we hope that the\nlessons learned here can be carried over to other problem domains.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:01:55 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Iyer", "Rishabh", ""], ["Acharya", "Nimit", ""], ["Bompada", "Tanuja", ""], ["Charles", "Denis", ""], ["Manavoglu", "Eren", ""]]}, {"id": "1809.04682", "submitter": "Amit Zohar", "authors": "Amit Zohar, Lior Wolf", "title": "Automatic Program Synthesis of Long Programs with a Learned Garbage\n  Collector", "comments": "Published at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating automatic code given sample\ninput-output pairs. We train a neural network to map from the current state and\nthe outputs to the program's next statement. The neural network optimizes\nmultiple tasks concurrently: the next operation out of a set of high level\ncommands, the operands of the next statement, and which variables can be\ndropped from memory. Using our method we are able to create programs that are\nmore than twice as long as existing state-of-the-art solutions, while improving\nthe success rate for comparable lengths, and cutting the run-time by two orders\nof magnitude. Our code, including an implementation of various literature\nbaselines, is publicly available at https://github.com/amitz25/PCCoder\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:25:28 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 15:58:09 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zohar", "Amit", ""], ["Wolf", "Lior", ""]]}, {"id": "1809.04683", "submitter": "Shuhan Yuan", "authors": "Panpan Zheng, Shuhan Yuan, Xintao Wu", "title": "SAFE: A Neural Survival Analysis Model for Fraud Early Detection", "comments": "To appear in AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online platforms have deployed anti-fraud systems to detect and prevent\nfraudulent activities. However, there is usually a gap between the time that a\nuser commits a fraudulent action and the time that the user is suspended by the\nplatform. How to detect fraudsters in time is a challenging problem. Most of\nthe existing approaches adopt classifiers to predict fraudsters given their\nactivity sequences along time. The main drawback of classification models is\nthat the prediction results between consecutive timestamps are often\ninconsistent. In this paper, we propose a survival analysis based fraud early\ndetection model, SAFE, which maps dynamic user activities to survival\nprobabilities that are guaranteed to be monotonically decreasing along time.\nSAFE adopts recurrent neural network (RNN) to handle user activity sequences\nand directly outputs hazard values at each timestamp, and then, survival\nprobability derived from hazard values is deployed to achieve consistent\npredictions. Because we only observe the user suspended time instead of the\nfraudulent activity time in the training data, we revise the loss function of\nthe regular survival model to achieve fraud early detection. Experimental\nresults on two real world datasets demonstrate that SAFE outperforms both the\nsurvival analysis model and recurrent neural network model alone as well as\nstate-of-the-art fraud early detection approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:28:26 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:12:08 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zheng", "Panpan", ""], ["Yuan", "Shuhan", ""], ["Wu", "Xintao", ""]]}, {"id": "1809.04684", "submitter": "Jiahao Chen", "authors": "Jiahao Chen", "title": "Fair lending needs explainable models for responsible recommendation", "comments": "4 pages, position paper accepted for FATREC 2018 conference at ACM\n  RecSys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The financial services industry has unique explainability and fairness\nchallenges arising from compliance and ethical considerations in credit\ndecisioning. These challenges complicate the use of model machine learning and\nartificial intelligence methods in business decision processes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:29:20 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Chen", "Jiahao", ""]]}, {"id": "1809.04705", "submitter": "Hongteng Xu", "authors": "Hongteng Xu, Wenlin Wang, Wei Liu, Lawrence Carin", "title": "Distilled Wasserstein Learning for Word Embedding and Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Wasserstein method with a distillation mechanism, yielding\njoint learning of word embeddings and topics. The proposed method is based on\nthe fact that the Euclidean distance between word embeddings may be employed as\nthe underlying distance in the Wasserstein topic model. The word distributions\nof topics, their optimal transports to the word distributions of documents, and\nthe embeddings of words are learned in a unified framework. When learning the\ntopic model, we leverage a distilled underlying distance matrix to update the\ntopic distributions and smoothly calculate the corresponding optimal\ntransports. Such a strategy provides the updating of word embeddings with\nrobust guidance, improving the algorithmic convergence. As an application, we\nfocus on patient admission records, in which the proposed method embeds the\ncodes of diseases and procedures and learns the topics of admissions, obtaining\nsuperior performance on clinically-meaningful disease network construction,\nmortality prediction as a function of admission codes, and procedure\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 23:10:23 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Xu", "Hongteng", ""], ["Wang", "Wenlin", ""], ["Liu", "Wei", ""], ["Carin", "Lawrence", ""]]}, {"id": "1809.04720", "submitter": "Jeroen van Baar", "authors": "Jeroen van Baar, Alan Sullivan, Radu Cordorel, Devesh Jha, Diego\n  Romeres and Daniel Nikovski", "title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic\n  Tasks involving Complex Dynamics", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robot tasks or controllers using deep reinforcement learning has\nbeen proven effective in simulations. Learning in simulation has several\nadvantages. For example, one can fully control the simulated environment,\nincluding halting motions while performing computations. Another advantage when\nrobots are involved, is that the amount of time a robot is occupied learning a\ntask---rather than being productive---can be reduced by transferring the\nlearned task to the real robot. Transfer learning requires some amount of\nfine-tuning on the real robot. For tasks which involve complex (non-linear)\ndynamics, the fine-tuning itself may take a substantial amount of time. In\norder to reduce the amount of fine-tuning we propose to learn robustified\ncontrollers in simulation. Robustified controllers are learned by exploiting\nthe ability to change simulation parameters (both appearance and dynamics) for\nsuccessive training episodes. An additional benefit for this approach is that\nit alleviates the precise determination of physics parameters for the\nsimulator, which is a non-trivial task. We demonstrate our proposed approach on\na real setup in which a robot aims to solve a maze game, which involves complex\ndynamics due to static friction and potentially large accelerations. We show\nthat the amount of fine-tuning in transfer learning for a robustified\ncontroller is substantially reduced compared to a non-robustified controller.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 00:27:31 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 14:26:13 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["van Baar", "Jeroen", ""], ["Sullivan", "Alan", ""], ["Cordorel", "Radu", ""], ["Jha", "Devesh", ""], ["Romeres", "Diego", ""], ["Nikovski", "Daniel", ""]]}, {"id": "1809.04729", "submitter": "Alireza Shafaei", "authors": "Alireza Shafaei, Mark Schmidt, James J. Little", "title": "A Less Biased Evaluation of Out-of-distribution Sample Detectors", "comments": "to appear in BMVC 2019; v2 is more compact, with more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, a learning system could receive an input that is unlike\nanything it has seen during training. Unfortunately, out-of-distribution\nsamples can lead to unpredictable behaviour. We need to know whether any given\ninput belongs to the population distribution of the training/evaluation data to\nprevent unpredictable behaviour in deployed systems. A recent surge of interest\nin this problem has led to the development of sophisticated techniques in the\ndeep learning literature. However, due to the absence of a standard problem\ndefinition or an exhaustive evaluation, it is not evident if we can rely on\nthese methods. What makes this problem different from a typical supervised\nlearning setting is that the distribution of outliers used in training may not\nbe the same as the distribution of outliers encountered in the application.\nClassical approaches that learn inliers vs. outliers with only two datasets can\nyield optimistic results. We introduce OD-test, a three-dataset evaluation\nscheme as a more reliable strategy to assess progress on this problem. We\npresent an exhaustive evaluation of a broad set of methods from related areas\non image classification tasks. Contrary to the existing results, we show that\nfor realistic applications of high-dimensional images the previous techniques\nhave low accuracy and are not reliable in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:15:49 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 17:46:05 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Shafaei", "Alireza", ""], ["Schmidt", "Mark", ""], ["Little", "James J.", ""]]}, {"id": "1809.04737", "submitter": "Xintao Wu", "authors": "Yongkai Wu and Lu Zhang and Xintao Wu", "title": "Fairness-aware Classification: Criterion, Convexity, and Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness-aware classification is receiving increasing attention in the\nmachine learning fields. Recently research proposes to formulate the\nfairness-aware classification as constrained optimization problems. However,\nseveral limitations exist in previous works due to the lack of a theoretical\nframework for guiding the formulation. In this paper, we propose a general\nframework for learning fair classifiers which addresses previous limitations.\nThe framework formulates various commonly-used fairness metrics as convex\nconstraints that can be directly incorporated into classic classification\nmodels. Within the framework, we propose a constraint-free criterion on the\ntraining data which ensures that any classifier learned from the data is fair.\nWe also derive the constraints which ensure that the real fairness metric is\nsatisfied when surrogate functions are used to achieve convexity. Our framework\ncan be used to for formulating fairness-aware classification with fairness\nguarantee and computational efficiency. The experiments using real-world\ndatasets demonstrate our theoretical results and show the effectiveness of\nproposed framework and methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:56:57 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wu", "Yongkai", ""], ["Zhang", "Lu", ""], ["Wu", "Xintao", ""]]}, {"id": "1809.04747", "submitter": "Tao Yang", "authors": "Tao Yang, Georgios Arvanitidis, Dongmei Fu, Xiaogang Li, and S{\\o}ren\n  Hauberg", "title": "Geodesic Clustering in Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are tremendously successful in learning\nlow-dimensional latent representations that well-describe the data. These\nrepresentations, however, tend to much distort relationships between points,\ni.e. pairwise distances tend to not reflect semantic similarities well. This\nrenders unsupervised tasks, such as clustering, difficult when working with the\nlatent representations. We demonstrate that taking the geometry of the\ngenerative model into account is sufficient to make simple clustering\nalgorithms work well over latent representations. Leaning on the recent finding\nthat deep generative models constitute stochastically immersed Riemannian\nmanifolds, we propose an efficient algorithm for computing geodesics (shortest\npaths) and computing distances in the latent space, while taking its distortion\ninto account. We further propose a new architecture for modeling uncertainty in\nvariational autoencoders, which is essential for understanding the geometry of\ndeep generative models. Experiments show that the geodesic distance is very\nlikely to reflect the internal structure of the data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 02:37:53 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Yang", "Tao", ""], ["Arvanitidis", "Georgios", ""], ["Fu", "Dongmei", ""], ["Li", "Xiaogang", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1809.04758", "submitter": "Dan Li", "authors": "Dan Li and Dacheng Chen and Jonathan Goh and See-kiong Ng", "title": "Anomaly Detection with Generative Adversarial Networks for Multivariate\n  Time Series", "comments": "This paper was presented in the 7th International Workshop on Big\n  Data, Streams and Heterogeneous Source Mining: Algorithms, Systems,\n  Programming Models and Applications on the ACM Knowledge Discovery and Data\n  Mining conference, August 2018, London, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with\nnetworked sensors and actuators that are targets for cyber-attacks.\nConventional detection techniques are unable to deal with the increasingly\ndynamic and complex nature of the CPSs. On the other hand, the networked\nsensors and actuators generate large amounts of data streams that can be\ncontinuously monitored for intrusion events. Unsupervised machine learning\ntechniques can be used to model the system behaviour and classify deviant\nbehaviours as possible attacks. In this work, we proposed a novel Generative\nAdversarial Networks-based Anomaly Detection (GAN-AD) method for such complex\nnetworked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the\nmultivariate time series of the sensors and actuators under normal working\nconditions of a CPS. Instead of treating each sensor's and actuator's time\nseries independently, we model the time series of multiple sensors and\nactuators in the CPS concurrently to take into account of potential latent\ninteractions between them. To exploit both the generator and the discriminator\nof our GAN, we deployed the GAN-trained discriminator together with the\nresiduals between generator-reconstructed data and the actual samples to detect\npossible anomalies in the complex CPS. We used our GAN-AD to distinguish\nabnormal attacked situations from normal working conditions for a complex\nsix-stage Secure Water Treatment (SWaT) system. Experimental results showed\nthat the proposed strategy is effective in identifying anomalies caused by\nvarious attacks with high detection rate and low false positive rate as\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 03:54:22 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 05:50:34 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 05:13:19 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Li", "Dan", ""], ["Chen", "Dacheng", ""], ["Goh", "Jonathan", ""], ["Ng", "See-kiong", ""]]}, {"id": "1809.04790", "submitter": "Jiliang Zhang", "authors": "Jiliang Zhang and Chen Li", "title": "Adversarial Examples: Opportunities and Challenges", "comments": "16 pages, 13 figures, 5 tables", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2019)", "doi": "10.1109/TNNLS.2019.2933524", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 06:09:32 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 11:09:01 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 04:54:27 GMT"}, {"version": "v4", "created": "Mon, 23 Sep 2019 13:04:37 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhang", "Jiliang", ""], ["Li", "Chen", ""]]}, {"id": "1809.04828", "submitter": "Raanan Rohekar", "authors": "Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Guy Koren, Gal Novik", "title": "Bayesian Structure Learning by Recursive Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We address the problem of Bayesian structure learning for domains with\nhundreds of variables by employing non-parametric bootstrap, recursively. We\npropose a method that covers both model averaging and model selection in the\nsame framework. The proposed method deals with the main weakness of\nconstraint-based learning---sensitivity to errors in the independence\ntests---by a novel way of combining bootstrap with constraint-based learning.\nEssentially, we provide an algorithm for learning a tree, in which each node\nrepresents a scored CPDAG for a subset of variables and the level of the node\ncorresponds to the maximal order of conditional independencies that are encoded\nin the graph. As higher order independencies are tested in deeper recursive\ncalls, they benefit from more bootstrap samples, and therefore more resistant\nto the curse-of-dimensionality. Moreover, the re-use of stable low order\nindependencies allows greater computational efficiency. We also provide an\nalgorithm for sampling CPDAGs efficiently from their posterior given the\nlearned tree. We empirically demonstrate that the proposed algorithm scales\nwell to hundreds of variables, and learns better MAP models and more reliable\ncausal relationships between variables, than other state-of-the-art-methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 08:21:16 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Rohekar", "Raanan Y.", ""], ["Gurwicz", "Yaniv", ""], ["Nisimov", "Shami", ""], ["Koren", "Guy", ""], ["Novik", "Gal", ""]]}, {"id": "1809.04855", "submitter": "Thomas Bird", "authors": "Thomas Bird, Julius Kunze and David Barber", "title": "Stochastic Variational Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Optimization forms a differentiable upper bound on an objective.\nWe show that approaches such as Natural Evolution Strategies and Gaussian\nPerturbation, are special cases of Variational Optimization in which the\nexpectations are approximated by Gaussian sampling. These approaches are of\nparticular interest because they are parallelizable. We calculate the\napproximate bias and variance of the corresponding gradient estimators and\ndemonstrate that using antithetic sampling or a baseline is crucial to mitigate\ntheir problems. We contrast these methods with an alternative parallelizable\nmethod, namely Directional Derivatives. We conclude that, for differentiable\nobjectives, using Directional Derivatives is preferable to using Variational\nOptimization to perform parallel Stochastic Gradient Descent.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 09:34:04 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Bird", "Thomas", ""], ["Kunze", "Julius", ""], ["Barber", "David", ""]]}, {"id": "1809.04913", "submitter": "Pengcheng Li", "authors": "Pengcheng Li, Jinfeng Yi, Lijun Zhang", "title": "Query-Efficient Black-Box Attack by Active Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) as a popular machine learning model is found to be\nvulnerable to adversarial attack. This attack constructs adversarial examples\nby adding small perturbations to the raw input, while appearing unmodified to\nhuman eyes but will be misclassified by a well-trained classifier. In this\npaper, we focus on the black-box attack setting where attackers have almost no\naccess to the underlying models. To conduct black-box attack, a popular\napproach aims to train a substitute model based on the information queried from\nthe target DNN. The substitute model can then be attacked using existing\nwhite-box attack approaches, and the generated adversarial examples will be\nused to attack the target DNN. Despite its encouraging results, this approach\nsuffers from poor query efficiency, i.e., attackers usually needs to query a\nhuge amount of times to collect enough information for training an accurate\nsubstitute model. To this end, we first utilize state-of-the-art white-box\nattack methods to generate samples for querying, and then introduce an active\nlearning strategy to significantly reduce the number of queries needed.\nBesides, we also propose a diversity criterion to avoid the sampling bias. Our\nextensive experimental results on MNIST and CIFAR-10 show that the proposed\nmethod can reduce more than $90\\%$ of queries while preserve attacking success\nrates and obtain an accurate substitute model which is more than $85\\%$ similar\nwith the target oracle.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 12:35:18 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Li", "Pengcheng", ""], ["Yi", "Jinfeng", ""], ["Zhang", "Lijun", ""]]}, {"id": "1809.04933", "submitter": "Alejandro Baldominos", "authors": "Alejandro Baldominos, Iv\\'an Blanco, Antonio Jos\\'e Moreno, Rub\\'en\n  Iturrarte, \\'Oscar Bern\\'ardez and Carlos Afonso", "title": "Identifying Real Estate Opportunities using Machine Learning", "comments": "24 pages, 13 figures, 5 tables", "journal-ref": "Baldominos, A.; Blanco, I.; Moreno, A.J.; Iturrarte, R.;\n  Bern\\'ardez, \\'O.; Afonso, C. Identifying Real Estate Opportunities Using\n  Machine Learning. Appl. Sci. 2018, 8, 2321", "doi": "10.3390/app8112321", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The real estate market is exposed to many fluctuations in prices because of\nexisting correlations with many variables, some of which cannot be controlled\nor might even be unknown. Housing prices can increase rapidly (or in some\ncases, also drop very fast), yet the numerous listings available online where\nhouses are sold or rented are not likely to be updated that often. In some\ncases, individuals interested in selling a house (or apartment) might include\nit in some online listing, and forget about updating the price. In other cases,\nsome individuals might be interested in deliberately setting a price below the\nmarket price in order to sell the home faster, for various reasons. In this\npaper, we aim at developing a machine learning application that identifies\nopportunities in the real estate market in real time, i.e., houses that are\nlisted with a price substantially below the market price. This program can be\nuseful for investors interested in the housing market. We have focused in a use\ncase considering real estate assets located in the Salamanca district in Madrid\n(Spain) and listed in the most relevant Spanish online site for home sales and\nrentals. The application is formally implemented as a regression problem that\ntries to estimate the market price of a house given features retrieved from\npublic online listings. For building this application, we have performed a\nfeature engineering stage in order to discover relevant features that allows\nfor attaining a high predictive performance. Several machine learning\nalgorithms have been tested, including regression trees, k-nearest neighbors,\nsupport vector machines and neural networks, identifying advantages and\nhandicaps of each of them.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 13:19:23 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 11:53:32 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Baldominos", "Alejandro", ""], ["Blanco", "Iv\u00e1n", ""], ["Moreno", "Antonio Jos\u00e9", ""], ["Iturrarte", "Rub\u00e9n", ""], ["Bern\u00e1rdez", "\u00d3scar", ""], ["Afonso", "Carlos", ""]]}, {"id": "1809.04951", "submitter": "Philipp Bach", "authors": "Philipp Bach, Victor Chernozhukov, Martin Spindler", "title": "Valid Simultaneous Inference in High-Dimensional Settings (with the hdm\n  package for R)", "comments": "25 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing availability of high-dimensional empirical applications\nin many research disciplines, valid simultaneous inference becomes more and\nmore important. For instance, high-dimensional settings might arise in economic\nstudies due to very rich data sets with many potential covariates or in the\nanalysis of treatment heterogeneities. Also the evaluation of potentially more\ncomplicated (non-linear) functional forms of the regression relationship leads\nto many potential variables for which simultaneous inferential statements might\nbe of interest. Here we provide a review of classical and modern methods for\nsimultaneous inference in (high-dimensional) settings and illustrate their use\nby a case study using the R package hdm. The R package hdm implements valid\njoint powerful and efficient hypothesis tests for a potentially large number of\ncoeffcients as well as the construction of simultaneous confidence intervals\nand, therefore, provides useful methods to perform valid post-selection\ninference based on the LASSO.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 13:41:03 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Bach", "Philipp", ""], ["Chernozhukov", "Victor", ""], ["Spindler", "Martin", ""]]}, {"id": "1809.04967", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Filip Tronarp, Simo S\\\"arkk\\\"a", "title": "Gaussian process classification using posterior linearisation", "comments": "\\'A. F. Garc\\'ia-Fern\\'andez, F. Tronarp and S. S\\\"arkk\\\"a, \"Gaussian\n  Process Classification Using Posterior Linearization,\" in IEEE Signal\n  Processing Letters, vol. 26, no. 5, pp. 735-739, May 2019", "journal-ref": null, "doi": "10.1109/LSP.2019.2906929", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for Gaussian process classification based\non posterior linearisation (PL). In PL, a Gaussian approximation to the\nposterior density is obtained iteratively using the best possible linearisation\nof the conditional mean of the labels and accounting for the linearisation\nerror. PL has some theoretical advantages over expectation propagation (EP):\nall calculated covariance matrices are positive definite and there is a local\nconvergence theorem. In experimental data, PL has better performance than EP\nwith the noisy threshold likelihood and the parallel implementation of the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 13:54:16 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:35:21 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 07:05:12 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Tronarp", "Filip", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1809.04988", "submitter": "Eric Crawford", "authors": "Eric Crawford, Guillaume Rabusseau, Joelle Pineau", "title": "Sequential Coordination of Deep Models for Learning Visual Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving machine intelligence requires a smooth integration of perception\nand reasoning, yet models developed to date tend to specialize in one or the\nother; sophisticated manipulation of symbols acquired from rich perceptual\nspaces has so far proved elusive. Consider a visual arithmetic task, where the\ngoal is to carry out simple arithmetical algorithms on digits presented under\nnatural conditions (e.g. hand-written, placed randomly). We propose a\ntwo-tiered architecture for tackling this problem. The lower tier consists of a\nheterogeneous collection of information processing modules, which can include\npre-trained deep neural networks for locating and extracting characters from\nthe image, as well as modules performing symbolic transformations on the\nrepresentations extracted by perception. The higher tier consists of a\ncontroller, trained using reinforcement learning, which coordinates the modules\nin order to solve the high-level task. For instance, the controller may learn\nin what contexts to execute the perceptual networks and what symbolic\ntransformations to apply to their outputs. The resulting model is able to solve\na variety of tasks in the visual arithmetic domain, and has several advantages\nover standard, architecturally homogeneous feedforward networks including\nimproved sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:27:25 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Crawford", "Eric", ""], ["Rabusseau", "Guillaume", ""], ["Pineau", "Joelle", ""]]}, {"id": "1809.04993", "submitter": "Diego Romeres", "authors": "Diego Romeres, Devesh Jha, Alberto Dalla Libera, William Yerazunis and\n  Daniel Nikovski", "title": "Semiparametrical Gaussian Processes Learning of Forward Dynamical Models\n  for Navigating in a Circular Maze", "comments": "7 pages including the references, 5 figures. Changed title, improved\n  the structure of the article and the images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a problem of model learning for the purpose of learning\nhow to navigate a ball to a goal state in a circular maze environment with two\ndegrees of freedom. The motion of the ball in the maze environment is\ninfluenced by several non-linear effects such as dry friction and contacts,\nwhich are difficult to model physically. We propose a semiparametric model to\nestimate the motion dynamics of the ball based on Gaussian Process Regression\nequipped with basis functions obtained from physics first principles. The\naccuracy of this semiparametric model is shown not only in estimation but also\nin prediction at n-steps ahead and its compared with standard algorithms for\nmodel learning. The learned model is then used in a trajectory optimization\nalgorithm to compute ball trajectories. We propose the system presented in the\npaper as a benchmark problem for reinforcement and robot learning, for its\ninteresting and challenging dynamics and its relative ease of reproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:44:05 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 18:22:48 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Romeres", "Diego", ""], ["Jha", "Devesh", ""], ["Libera", "Alberto Dalla", ""], ["Yerazunis", "William", ""], ["Nikovski", "Daniel", ""]]}, {"id": "1809.04997", "submitter": "Takeshi Teshima", "authors": "Takeshi Teshima, Miao Xu, Issei Sato and Masashi Sugiyama", "title": "Clipped Matrix Completion: A Remedy for Ceiling Effects", "comments": "36 pages, 3 figures, The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a low-rank matrix from its clipped\nobservations. Clipping is conceivable in many scientific areas that obstructs\nstatistical analyses. On the other hand, matrix completion (MC) methods can\nrecover a low-rank matrix from various information deficits by using the\nprinciple of low-rank completion. However, the current theoretical guarantees\nfor low-rank MC do not apply to clipped matrices, as the deficit depends on the\nunderlying values. Therefore, the feasibility of clipped matrix completion\n(CMC) is not trivial. In this paper, we first provide a theoretical guarantee\nfor the exact recovery of CMC by using a trace-norm minimization algorithm.\nFurthermore, we propose practical CMC algorithms by extending ordinary MC\nmethods. Our extension is to use the squared hinge loss in place of the squared\nloss for reducing the penalty of over-estimation on clipped entries. We also\npropose a novel regularization term tailored for CMC. It is a combination of\ntwo trace-norm terms, and we theoretically bound the recovery error under the\nregularization. We demonstrate the effectiveness of the proposed methods\nthrough experiments using both synthetic and benchmark data for recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:46:54 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 04:44:31 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 11:39:24 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Teshima", "Takeshi", ""], ["Xu", "Miao", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.05014", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer and Aryeh Kontorovich", "title": "Statistical Estimation of Ergodic Markov Chain Kernel over Discrete\n  State Space", "comments": "Journal version of the extended abstract (ALT'19), to appear in\n  Bernoulli 2020+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the statistical complexity of estimating the parameters of a\ndiscrete-state Markov chain kernel from a single long sequence of state\nobservations. In the finite case, we characterize (modulo logarithmic factors)\nthe minimax sample complexity of estimation with respect to the operator\ninfinity norm, while in the countably infinite case, we analyze the problem\nwith respect to a natural entry-wise norm derived from total variation. We show\nthat in both cases, the sample complexity is governed by the mixing properties\nof the unknown chain, for which, in the finite-state case, there are known\nfinite-sample estimators with fully empirical confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 15:37:19 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 13:57:14 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:42:03 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2020 08:13:15 GMT"}, {"version": "v5", "created": "Wed, 1 Jul 2020 09:37:22 GMT"}, {"version": "v6", "created": "Thu, 13 Aug 2020 09:04:36 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wolfer", "Geoffrey", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1809.05032", "submitter": "Yoshimasa Uematsu", "authors": "Yingying Fan, Jinchi Lv, Mahrad Sharifvaghefi and Yoshimasa Uematsu", "title": "IPAD: Stable Interpretable Forecasting with Knockoffs Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability and stability are two important features that are desired in\nmany contemporary big data applications arising in economics and finance. While\nthe former is enjoyed to some extent by many existing forecasting approaches,\nthe latter in the sense of controlling the fraction of wrongly discovered\nfeatures which can enhance greatly the interpretability is still largely\nunderdeveloped in the econometric settings. To this end, in this paper we\nexploit the general framework of model-X knockoffs introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2018), which is nonconventional for\nreproducible large-scale inference in that the framework is completely free of\nthe use of p-values for significance testing, and suggest a new method of\nintertwined probabilistic factors decoupling (IPAD) for stable interpretable\nforecasting with knockoffs inference in high-dimensional models. The recipe of\nthe method is constructing the knockoff variables by assuming a latent factor\nmodel that is exploited widely in economics and finance for the association\nstructure of covariates. Our method and work are distinct from the existing\nliterature in that we estimate the covariate distribution from data instead of\nassuming that it is known when constructing the knockoff variables, our\nprocedure does not require any sample splitting, we provide theoretical\njustifications on the asymptotic false discovery rate control, and the theory\nfor the power analysis is also established. Several simulation examples and the\nreal data analysis further demonstrate that the newly suggested method has\nappealing finite-sample performance with desired interpretability and stability\ncompared to some popularly used forecasting methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 08:08:10 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""], ["Sharifvaghefi", "Mahrad", ""], ["Uematsu", "Yoshimasa", ""]]}, {"id": "1809.05042", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan O'Donoghue,\n  Arnaud Doucet", "title": "Hamiltonian Descent Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of optimization methods that achieve linear convergence\nusing first-order gradient information and constant step sizes on a class of\nconvex functions much larger than the smooth and strongly convex ones. This\nlarger class includes functions whose second derivatives may be singular or\nunbounded at their minima. Our methods are discretizations of conformal\nHamiltonian dynamics, which generalize the classical momentum method to model\nthe motion of a particle with non-standard kinetic energy exposed to a\ndissipative force and the gradient field of the function of interest. They are\nfirst-order in the sense that they require only gradient computation. Yet,\ncrucially the kinetic gradient map can be designed to incorporate information\nabout the convex conjugate in a fashion that allows for linear convergence on\nconvex functions that may be non-smooth or non-strongly convex. We study in\ndetail one implicit and two explicit methods. For one explicit method, we\nprovide conditions under which it converges to stationary points of non-convex\nfunctions. For all, we provide conditions on the convex function and kinetic\nenergy pair that guarantee linear convergence, and show that these conditions\ncan be satisfied by functions with power growth. In sum, these methods expand\nthe class of convex functions on which linear convergence is possible with\nfirst-order computation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:21:11 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Maddison", "Chris J.", ""], ["Paulin", "Daniel", ""], ["Teh", "Yee Whye", ""], ["O'Donoghue", "Brendan", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1809.05043", "submitter": "Amichai Painsky", "authors": "Amichai Painsky", "title": "PhD Dissertation: Generalized Independent Components Analysis Over\n  Finite Alphabets", "comments": "PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a statistical method for transforming\nan observable multi-dimensional random vector into components that are as\nstatistically independent as possible from each other. Usually the ICA\nframework assumes a model according to which the observations are generated\n(such as a linear transformation with additive noise). ICA over finite fields\nis a special case of ICA in which both the observations and the independent\ncomponents are over a finite alphabet. In this thesis we consider a formulation\nof the finite-field case in which an observation vector is decomposed to its\nindependent components (as much as possible) with no prior assumption on the\nway it was generated. This generalization is also known as Barlow's minimal\nredundancy representation and is considered an open problem. We propose several\ntheorems and show that this hard problem can be accurately solved with a branch\nand bound search tree algorithm, or tightly approximated with a series of\nlinear problems. Moreover, we show that there exists a simple transformation\n(namely, order permutation) which provides a greedy yet very effective\napproximation of the optimal solution. We further show that while not every\nrandom vector can be efficiently decomposed into independent components, the\nvast majority of vectors do decompose very well (that is, within a small\nconstant cost), as the dimension increases. In addition, we show that we may\npractically achieve this favorable constant cost with a complexity that is\nasymptotically linear in the alphabet size. Our contribution provides the first\nefficient set of solutions to Barlow's problem with theoretical and\ncomputational guarantees. Finally, we demonstrate our suggested framework in\nmultiple source coding applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:22:19 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 04:36:26 GMT"}, {"version": "v3", "created": "Sat, 17 Nov 2018 14:47:45 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 12:00:20 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Painsky", "Amichai", ""]]}, {"id": "1809.05074", "submitter": "Diego Romeres", "authors": "Diego Romeres, Mattia Zorzi, Raffaello Camoriano, Silvio Traversaro\n  and Alessandro Chiuso", "title": "Derivative-free online learning of inverse dynamics models", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses online algorithms for inverse dynamics modelling in\nrobotics. Several model classes including rigid body dynamics (RBD) models,\ndata-driven models and semiparametric models (which are a combination of the\nprevious two classes) are placed in a common framework. While model classes\nused in the literature typically exploit joint velocities and accelerations,\nwhich need to be approximated resorting to numerical differentiation schemes,\nin this paper a new `derivative-free' framework is proposed that does not\nrequire this preprocessing step. An extensive experimental study with real data\nfrom the right arm of the iCub robot is presented, comparing different model\nclasses and estimation procedures, showing that the proposed `derivative-free'\nmethods outperform existing methodologies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:29:35 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Romeres", "Diego", ""], ["Zorzi", "Mattia", ""], ["Camoriano", "Raffaello", ""], ["Traversaro", "Silvio", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1809.05077", "submitter": "Amichai Painsky", "authors": "Amichai Painsky", "title": "MSc Dissertation: Exclusive Row Biclustering for Gene Expression Using a\n  Combinatorial Auction Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large microarray data has led to a growing interest in\nbiclustering methods in the past decade. Several algorithms have been proposed\nto identify subsets of genes and conditions according to different similarity\nmeasures and under varying constraints. In this paper we focus on the exclusive\nrow biclustering problem for gene expression data sets, in which each row can\nonly be a member of a single bicluster while columns can participate in\nmultiple ones. This type of biclustering may be adequate, for example, for\nclustering groups of cancer patients where each patient (row) is expected to be\ncarrying only a single type of cancer, while each cancer type is associated\nwith multiple (and possibly overlapping) genes (columns). We present a novel\nmethod to identify these exclusive row biclusters through a combination of\nexisting biclustering algorithms and combinatorial auction techniques. We\ndevise an approach for tuning the threshold for our algorithm based on\ncomparison to a null model in the spirit of the Gap statistic approach. We\ndemonstrate our approach on both synthetic and real-world gene expression data\nand show its power in identifying large span non-overlapping rows sub matrices,\nwhile considering their unique nature. The Gap statistic approach succeeds in\nidentifying appropriate thresholds in all our examples.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:37:16 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 04:42:48 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Painsky", "Amichai", ""]]}, {"id": "1809.05127", "submitter": "Garrett Goh", "authors": "Khushmeen Sakloth, Wesley Beckner, Jim Pfaendtner, Garrett B. Goh", "title": "IL-Net: Using Expert Knowledge to Guide the Design of Furcated Neural\n  Networks", "comments": "Submitted to peer-reviewed ML conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) excel at extracting patterns. Through\nrepresentation learning and automated feature engineering on large datasets,\nsuch models have been highly successful in computer vision and natural language\napplications. Designing optimal network architectures from a principled or\nrational approach however has been less than successful, with the best\nsuccessful approaches utilizing an additional machine learning algorithm to\ntune the network hyperparameters. However, in many technical fields, there\nexist established domain knowledge and understanding about the subject matter.\nIn this work, we develop a novel furcated neural network architecture that\nutilizes domain knowledge as high-level design principles of the network. We\ndemonstrate proof-of-concept by developing IL-Net, a furcated network for\npredicting the properties of ionic liquids, which is a class of complex\nmulti-chemicals entities. Compared to existing state-of-the-art approaches, we\nshow that furcated networks can improve model accuracy by approximately 20-35%,\nwithout using additional labeled data. Lastly, we distill two key design\nprinciples for furcated networks that can be adapted to other domains.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 18:22:04 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Sakloth", "Khushmeen", ""], ["Beckner", "Wesley", ""], ["Pfaendtner", "Jim", ""], ["Goh", "Garrett B.", ""]]}, {"id": "1809.05139", "submitter": "Johan Ugander", "authors": "Stephen Ragain, Johan Ugander", "title": "Choosing to Rank", "comments": "39 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking data arises in a wide variety of application areas but remains\ndifficult to model, learn from, and predict. Datasets often exhibit\nmultimodality, intransitivity, or incomplete rankings---particularly when\ngenerated by humans---yet popular probabilistic models are often too rigid to\ncapture such complexities. In this work we leverage recent progress on similar\nchallenges in discrete choice modeling to form flexible and tractable\nchoice-based models for ranking data. We study choice representations, maps\nfrom rankings (complete or top-$k$) to collections of choices, as a way of\nforming ranking models from choice models. We focus on the repeated selection\n(RS) choice representation, first used to form the Plackett-Luce ranking model\nfrom the conditional multinomial logit choice model. We fully characterize, for\na prime number of alternatives, the choice representations that admit ranking\ndistributions with unit normalization, a desirably property that greatly\nsimplifies maximum likelihood estimation. We further show that only specific\nminor variations on repeated selection exhibit this property. Our choice-based\nranking models provide higher out-of-sample likelihood when compared to\nPlackett-Luce and Mallows models on a broad collection of ranking tasks\nincluding food preferences, ranked-choice elections, car racing, and search\nengine relevance tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 18:43:55 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 05:15:30 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Ragain", "Stephen", ""], ["Ugander", "Johan", ""]]}, {"id": "1809.05142", "submitter": "Ioannis C. Konstantakopoulos", "authors": "Ioannis C. Konstantakopoulos, Andrew R. Barkan, Shiying He, Tanya\n  Veeravalli, Huihan Liu, Costas Spanos", "title": "A Deep Learning and Gamification Approach to Energy Conservation at\n  Nanyang Technological University", "comments": "16 double pages, shorter version submitted to Applied Energy Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The implementation of smart building technology in the form of smart\ninfrastructure applications has great potential to improve sustainability and\nenergy efficiency by leveraging humans-in-the-loop strategy. However, human\npreference in regard to living conditions is usually unknown and heterogeneous\nin its manifestation as control inputs to a building. Furthermore, the\noccupants of a building typically lack the independent motivation necessary to\ncontribute to and play a key role in the control of smart building\ninfrastructure. Moreover, true human actions and their integration with\nsensing/actuation platforms remains unknown to the decision maker tasked with\nimproving operational efficiency. By modeling user interaction as a sequential\ndiscrete game between non-cooperative players, we introduce a gamification\napproach for supporting user engagement and integration in a human-centric\ncyber-physical system. We propose the design and implementation of a\nlarge-scale network game with the goal of improving the energy efficiency of a\nbuilding through the utilization of cutting-edge Internet of Things (IoT)\nsensors and cyber-physical systems sensing/actuation platforms. A benchmark\nutility learning framework that employs robust estimations for classical\ndiscrete choice models provided for the derived high dimensional imbalanced\ndata. To improve forecasting performance, we extend the benchmark utility\nlearning scheme by leveraging Deep Learning end-to-end training with Deep\nbi-directional Recurrent Neural Networks. We apply the proposed methods to high\ndimensional data from a social game experiment designed to encourage energy\nefficient behavior among smart building occupants in Nanyang Technological\nUniversity (NTU) residential housing. Using occupant-retrieved actions for\nresources such as lighting and A/C, we simulate the game defined by the\nestimated utility functions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 18:52:16 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 19:09:19 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Konstantakopoulos", "Ioannis C.", ""], ["Barkan", "Andrew R.", ""], ["He", "Shiying", ""], ["Veeravalli", "Tanya", ""], ["Liu", "Huihan", ""], ["Spanos", "Costas", ""]]}, {"id": "1809.05143", "submitter": "Nikita Klyuchnikov", "authors": "Nikita Klyuchnikov and Evgeny Burnaev", "title": "Gaussian Process Classification for Variable Fidelity Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address a classification problem where two sources of labels\nwith different levels of fidelity are available. Our approach is to combine\ndata from both sources by applying a co-kriging schema on latent functions,\nwhich allows the model to account item-dependent labeling discrepancy. We\nprovide an extension of Laplace inference for Gaussian process classification,\nthat takes into account multi-fidelity data. We evaluate the proposed method on\nreal and synthetic datasets and show that it is more resistant to different\nlevels of discrepancy between sources than other approaches for data fusion.\nOur method can provide accuracy/cost trade-off for a number of practical tasks\nsuch as crowd-sourced data annotation and feasibility regions construction in\nengineering design.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 18:59:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 10:24:10 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 20:48:04 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Klyuchnikov", "Nikita", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1809.05165", "submitter": "Siyue Wang", "authors": "Siyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin,\n  Xue Lin", "title": "Defensive Dropout for Hardening Deep Neural Networks under Adversarial\n  Attacks", "comments": "Accepted as conference paper on ICCAD 2018", "journal-ref": null, "doi": "10.1145/3240765.3264699", "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. This work provides a solution to hardening DNNs under adversarial\nattacks through defensive dropout. Besides using dropout during training for\nthe best test accuracy, we propose to use dropout also at test time to achieve\nstrong defense effects. We consider the problem of building robust DNNs as an\nattacker-defender two-player game, where the attacker and the defender know\neach others' strategies and try to optimize their own strategies towards an\nequilibrium. Based on the observations of the effect of test dropout rate on\ntest accuracy and attack success rate, we propose a defensive dropout algorithm\nto determine an optimal test dropout rate given the neural network model and\nthe attacker's strategy for generating adversarial examples.We also investigate\nthe mechanism behind the outstanding defense effects achieved by the proposed\ndefensive dropout. Comparing with stochastic activation pruning (SAP), another\ndefense method through introducing randomness into the DNN model, we find that\nour defensive dropout achieves much larger variances of the gradients, which is\nthe key for the improved defense effects (much lower attack success rate). For\nexample, our defensive dropout can reduce the attack success rate from 100% to\n13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:26:32 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wang", "Siyue", ""], ["Wang", "Xiao", ""], ["Zhao", "Pu", ""], ["Wen", "Wujie", ""], ["Kaeli", "David", ""], ["Chin", "Peter", ""], ["Lin", "Xue", ""]]}, {"id": "1809.05172", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla", "title": "Deterministic Inequalities for Smooth M-estimators", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever since the proof of asymptotic normality of maximum likelihood estimator\nby Cramer (1946), it has been understood that a basic technique of the Taylor\nseries expansion suffices for asymptotics of $M$-estimators with\nsmooth/differentiable loss function. Although the Taylor series expansion is a\npurely deterministic tool, the realization that the asymptotic normality\nresults can also be made deterministic (and so finite sample) received far less\nattention. With the advent of big data and high-dimensional statistics, the\nneed for finite sample results has increased. In this paper, we use the\n(well-known) Banach fixed point theorem to derive various deterministic\ninequalities that lead to the classical results when studied under randomness.\nIn addition, we provide applications of these deterministic inequalities for\ncrossvalidation/subsampling, marginal screening and uniform-in-submodel results\nthat are very useful for post-selection inference and in the study of\npost-regularization estimators. Our results apply to many classical estimators,\nin particular, generalized linear models, non-linear regression and cox\nproportional hazards model. Extensions to non-smooth and constrained problems\nare also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:39:37 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "1809.05173", "submitter": "Jan Van Haaren", "authors": "Bart Aalbers, Jan Van Haaren", "title": "Distinguishing Between Roles of Football Players in Play-by-play Match\n  Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, the player recruitment process in professional\nfootball has evolved into a multi-billion industry and has thus become of vital\nimportance. To gain insights into the general level of their candidate\nreinforcements, many professional football clubs have access to extensive video\nfootage and advanced statistics. However, the question whether a given player\nwould fit the team's playing style often still remains unanswered. In this\npaper, we aim to bridge that gap by proposing a set of 21 player roles and\nintroducing a method for automatically identifying the most applicable roles\nfor each player from play-by-play event data collected during matches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:43:40 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Aalbers", "Bart", ""], ["Van Haaren", "Jan", ""]]}, {"id": "1809.05183", "submitter": "Isak Karlsson", "authors": "Isak Karlsson, Jonathan Rebane, Panagiotis Papapetrou, Aristides\n  Gionis", "title": "Explainable time series tweaking via irreversible and reversible\n  temporal transformations", "comments": "To appear in International Conference on Data Mining, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification has received great attention over the past decade\nwith a wide range of methods focusing on predictive performance by exploiting\nvarious types of temporal features. Nonetheless, little emphasis has been\nplaced on interpretability and explainability. In this paper, we formulate the\nnovel problem of explainable time series tweaking, where, given a time series\nand an opaque classifier that provides a particular classification decision for\nthe time series, we want to find the minimum number of changes to be performed\nto the given time series so that the classifier changes its decision to another\nclass. We show that the problem is NP-hard, and focus on two instantiations of\nthe problem, which we refer to as reversible and irreversible time series\ntweaking. The classifier under investigation is the random shapelet forest\nclassifier. Moreover, we propose two algorithmic solutions for the two problems\nalong with simple optimizations, as well as a baseline solution using the\nnearest neighbor classifier. An extensive experimental evaluation on a variety\nof real datasets demonstrates the usefulness and effectiveness of our problem\nformulation and solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 21:35:04 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Karlsson", "Isak", ""], ["Rebane", "Jonathan", ""], ["Papapetrou", "Panagiotis", ""], ["Gionis", "Aristides", ""]]}, {"id": "1809.05188", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, Hongyuan\n  Zha", "title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement\n  Learning", "comments": "Published at International Conference on Learning Representations\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of cooperative multi-agent control problems require agents to\nachieve individual goals while contributing to collective success. This\nmulti-goal multi-agent setting poses difficulties for recent algorithms, which\nprimarily target settings with a single global reward, due to two new\nchallenges: efficient exploration for learning both individual goal attainment\nand cooperation for others' success, and credit-assignment for interactions\nbetween actions and goals of different agents. To address both challenges, we\nrestructure the problem into a novel two-stage curriculum, in which\nsingle-agent goal attainment is learned prior to learning multi-agent\ncooperation, and we derive a new multi-goal multi-agent policy gradient with a\ncredit function for localized credit assignment. We use a function augmentation\nscheme to bridge value and policy functions across the curriculum. The complete\narchitecture, called CM3, learns significantly faster than direct adaptations\nof existing algorithms on three challenging multi-goal multi-agent problems:\ncooperative navigation in difficult formations, negotiating multi-vehicle lane\nchanges in the SUMO traffic simulator, and strategic cooperation in a Checkers\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 21:46:54 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 05:04:45 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 21:24:17 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yang", "Jiachen", ""], ["Nakhaei", "Alireza", ""], ["Isele", "David", ""], ["Fujimura", "Kikuo", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1809.05193", "submitter": "Rohan Bavishi", "authors": "Rohan Bavishi, Michael Pradel, Koushik Sen", "title": "Context2Name: A Deep Learning-Based Approach to Infer Natural Variable\n  Names from Usage Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the JavaScript code deployed in the wild has been minified, a process\nin which identifier names are replaced with short, arbitrary and meaningless\nnames. Minified code occupies less space, but also makes the code extremely\ndifficult to manually inspect and understand. This paper presents Context2Name,\na deep learningbased technique that partially reverses the effect of\nminification by predicting natural identifier names for minified names. The\ncore idea is to predict from the usage context of a variable a name that\ncaptures the meaning of the variable. The approach combines a lightweight,\ntoken-based static analysis with an auto-encoder neural network that summarizes\nusage contexts and a recurrent neural network that predict natural names for a\ngiven usage context. We evaluate Context2Name with a large corpus of real-world\nJavaScript code and show that it successfully predicts 47.5% of all minified\nidentifiers while taking only 2.9 milliseconds on average to predict a name. A\ncomparison with the state-of-the-art tools JSNice and JSNaughty shows that our\napproach performs comparably in terms of accuracy while improving in terms of\nefficiency. Moreover, Context2Name complements the state-of-the-art by\npredicting 5.3% additional identifiers that are missed by both existing tools.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 20:52:10 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Bavishi", "Rohan", ""], ["Pradel", "Michael", ""], ["Sen", "Koushik", ""]]}, {"id": "1809.05214", "submitter": "Ignasi Clavera", "authors": "Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim\n  Asfour, Pieter Abbeel", "title": "Model-Based Reinforcement Learning via Meta-Policy Optimization", "comments": "First 2 authors contributed equally. Accepted for Conference on Robot\n  Learning (CoRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning approaches carry the promise of being data\nefficient. However, due to challenges in learning dynamics models that\nsufficiently match the real-world dynamics, they struggle to achieve the same\nasymptotic performance as model-free methods. We propose Model-Based\nMeta-Policy-Optimization (MB-MPO), an approach that foregoes the strong\nreliance on accurate learned dynamics models. Using an ensemble of learned\ndynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model\nin the ensemble with one policy gradient step. This steers the meta-policy\ntowards internalizing consistent dynamics predictions among the ensemble while\nshifting the burden of behaving optimally w.r.t. the model discrepancies\ntowards the adaptation step. Our experiments show that MB-MPO is more robust to\nmodel imperfections than previous model-based approaches. Finally, we\ndemonstrate that our approach is able to match the asymptotic performance of\nmodel-free methods while requiring significantly less experience.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 01:15:28 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Clavera", "Ignasi", ""], ["Rothfuss", "Jonas", ""], ["Schulman", "John", ""], ["Fujita", "Yasuhiro", ""], ["Asfour", "Tamim", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1809.05241", "submitter": "Carlos H. C. Teixeira", "authors": "Carlos H. C. Teixeira, Leonardo Cotta, Bruno Ribeiro, Wagner Meira Jr", "title": "Graph Pattern Mining and Learning through User-defined Relations\n  (Extended Version)", "comments": "Extended version of the paper published in the ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose R-GPM, a parallel computing framework for graph\npattern mining (GPM) through a user-defined subgraph relation. More\nspecifically, we enable the computation of statistics of patterns through their\nsubgraph classes, generalizing traditional GPM methods. R-GPM provides\nefficient estimators for these statistics by employing a MCMC sampling\nalgorithm combined with several optimizations. We provide both theoretical\nguarantees and empirical evaluations of our estimators in application scenarios\nsuch as stochastic optimization of deep high-order graph neural network models\nand pattern (motif) counting. We also propose and evaluate optimizations that\nenable improvements of our estimators accuracy, while reducing their\ncomputational costs in up to 3-orders-of-magnitude. Finally,we show that R-GPM\nis scalable, providing near-linear speedups on 44 cores in all of our tests.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 03:16:09 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 20:45:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Teixeira", "Carlos H. C.", ""], ["Cotta", "Leonardo", ""], ["Ribeiro", "Bruno", ""], ["Meira", "Wagner", "Jr"]]}, {"id": "1809.05242", "submitter": "Ryan Robinett", "authors": "Ryan A. Robinett and Jeremy Kepner", "title": "Neural Network Topologies for Sparse Training", "comments": "4 Pages, 4 figures; accepted at MIT IEEE Undergraduate Research\n  Technology Conference 2018. Publication pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sizes of deep neural networks (DNNs) are rapidly outgrowing the capacity\nof hardware to store and train them. Research over the past few decades has\nexplored the prospect of sparsifying DNNs before, during, and after training by\npruning edges from the underlying topology. The resulting neural network is\nknown as a sparse neural network. More recent work has demonstrated the\nremarkable result that certain sparse DNNs can train to the same precision as\ndense DNNs at lower runtime and storage cost. An intriguing class of these\nsparse DNNs is the X-Nets, which are initialized and trained upon a sparse\ntopology with neither reference to a parent dense DNN nor subsequent pruning.\nWe present an algorithm that deterministically generates sparse DNN topologies\nthat, as a whole, are much more diverse than X-Net topologies, while preserving\nX-Nets' desired characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 03:19:04 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Robinett", "Ryan A.", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1809.05247", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Ian E.H. Yen, Jie Chen, and Rui Yan", "title": "Revisiting Random Binning Features: Fast Convergence and Strong\n  Parallelizability", "comments": "KDD16, Oral Paper, Add Code Link for generating Random Binning\n  Features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel method has been developed as one of the standard approaches for\nnonlinear learning, which however, does not scale to large data set due to its\nquadratic complexity in the number of samples. A number of kernel approximation\nmethods have thus been proposed in the recent years, among which the random\nfeatures method gains much popularity due to its simplicity and direct\nreduction of nonlinear problem to a linear one. The Random Binning (RB)\nfeature, proposed in the first random-feature paper \\cite{rahimi2007random},\nhas drawn much less attention than the Random Fourier (RF) feature. In this\nwork, we observe that the RB features, with right choice of optimization\nsolver, could be orders-of-magnitude more efficient than other random features\nand kernel approximation methods under the same requirement of accuracy. We\nthus propose the first analysis of RB from the perspective of optimization,\nwhich by interpreting RB as a Randomized Block Coordinate Descent in the\ninfinite-dimensional space, gives a faster convergence rate compared to that of\nother random features. In particular, we show that by drawing $R$ random grids\nwith at least $\\kappa$ number of non-empty bins per grid in expectation, RB\nmethod achieves a convergence rate of $O(1/(\\kappa R))$, which not only\nsharpens its $O(1/\\sqrt{R})$ rate from Monte Carlo analysis, but also shows a\n$\\kappa$ times speedup over other random features under the same analysis\nframework. In addition, we demonstrate another advantage of RB in the\nL1-regularized setting, where unlike other random features, a RB-based\nCoordinate Descent solver can be parallelized with guaranteed speedup\nproportional to $\\kappa$. Our extensive experiments demonstrate the superior\nperformance of the RB features over other random features and kernel\napproximation methods. Our code and data is available at {\n\\url{https://github.com/teddylfwu/RB_GEN}}.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 04:06:35 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 02:49:53 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Wu", "Lingfei", ""], ["Yen", "Ian E. H.", ""], ["Chen", "Jie", ""], ["Yan", "Rui", ""]]}, {"id": "1809.05250", "submitter": "Mehmet Necip Kurt", "authors": "Mehmet Necip Kurt and Yasin Yilmaz and Xiaodong Wang", "title": "Real-Time Nonparametric Anomaly Detection in High-Dimensional Settings", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.2970410", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely detection of abrupt anomalies is crucial for real-time monitoring and\nsecurity of modern systems producing high-dimensional data. With this goal, we\npropose effective and scalable algorithms. Proposed algorithms are\nnonparametric as both the nominal and anomalous multivariate data distributions\nare assumed unknown. We extract useful univariate summary statistics and\nperform anomaly detection in a single-dimensional space. We model anomalies as\npersistent outliers and propose to detect them via a cumulative sum-like\nalgorithm. In case the observed data have a low intrinsic dimensionality, we\nlearn a submanifold in which the nominal data are embedded and evaluate whether\nthe sequentially acquired data persistently deviate from the nominal\nsubmanifold. Further, in the general case, we learn an acceptance region for\nnominal data via Geometric Entropy Minimization and evaluate whether the\nsequentially observed data persistently fall outside the acceptance region. We\nprovide an asymptotic lower bound and an asymptotic approximation for the\naverage false alarm period of the proposed algorithm. Moreover, we provide a\nsufficient condition to asymptotically guarantee that the decision statistic of\nthe proposed algorithm does not diverge in the absence of anomalies.\nExperiments illustrate the effectiveness of the proposed schemes in quick and\naccurate anomaly detection in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 04:21:58 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 21:59:43 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kurt", "Mehmet Necip", ""], ["Yilmaz", "Yasin", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1809.05258", "submitter": "Mehmet Necip Kurt", "authors": "Mehmet Necip Kurt and Oyetunji Ogundijo and Chong Li and Xiaodong Wang", "title": "Online Cyber-Attack Detection in Smart Grid: A Reinforcement Learning\n  Approach", "comments": null, "journal-ref": "IEEE Transactions on Smart Grid, 2018", "doi": "10.1109/TSG.2018.2878570", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of cyber-attacks is crucial for a safe and reliable operation\nof the smart grid. In the literature, outlier detection schemes making\nsample-by-sample decisions and online detection schemes requiring perfect\nattack models have been proposed. In this paper, we formulate the online\nattack/anomaly detection problem as a partially observable Markov decision\nprocess (POMDP) problem and propose a universal robust online detection\nalgorithm using the framework of model-free reinforcement learning (RL) for\nPOMDPs. Numerical studies illustrate the effectiveness of the proposed RL-based\nalgorithm in timely and accurate detection of cyber-attacks targeting the smart\ngrid.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 05:17:36 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kurt", "Mehmet Necip", ""], ["Ogundijo", "Oyetunji", ""], ["Li", "Chong", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1809.05259", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Ian En-Hsu Yen, Jinfeng Yi, Fangli Xu, Qi Lei, and Michael\n  Witbrock", "title": "Random Warping Series: A Random Features Method for Time-Series\n  Embedding", "comments": "AIStats18, Oral Paper, Add code link for generating RWS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data analytics has been a problem of substantial interests for\ndecades, and Dynamic Time Warping (DTW) has been the most widely adopted\ntechnique to measure dissimilarity between time series. A number of\nglobal-alignment kernels have since been proposed in the spirit of DTW to\nextend its use to kernel-based estimation method such as support vector\nmachine. However, those kernels suffer from diagonal dominance of the Gram\nmatrix and a quadratic complexity w.r.t. the sample size. In this work, we\nstudy a family of alignment-aware positive definite (p.d.) kernels, with its\nfeature embedding given by a distribution of \\emph{Random Warping Series\n(RWS)}. The proposed kernel does not suffer from the issue of diagonal\ndominance while naturally enjoys a \\emph{Random Features} (RF) approximation,\nwhich reduces the computational complexity of existing DTW-based techniques\nfrom quadratic to linear in terms of both the number and the length of\ntime-series. We also study the convergence of the RF approximation for the\ndomain of time series of unbounded length. Our extensive experiments on 16\nbenchmark datasets demonstrate that RWS outperforms or matches state-of-the-art\nclassification and clustering methods in both accuracy and computational time.\nOur code and data is available at {\n\\url{https://github.com/IBM/RandomWarpingSeries}}.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 05:19:51 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wu", "Lingfei", ""], ["Yen", "Ian En-Hsu", ""], ["Yi", "Jinfeng", ""], ["Xu", "Fangli", ""], ["Lei", "Qi", ""], ["Witbrock", "Michael", ""]]}, {"id": "1809.05262", "submitter": "Joonsang Yu", "authors": "Joonsang Yu, Sungbum Kang and Kiyoung Choi", "title": "Network Recasting: A Universal Method for Network Architecture\n  Transformation", "comments": "AAAI 2019 Oral presentation, source codes are available on github:\n  https://github.com/joonsang-yu/Network-Recasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes network recasting as a general method for network\narchitecture transformation. The primary goal of this method is to accelerate\nthe inference process through the transformation, but there can be many other\npractical applications. The method is based on block-wise recasting; it recasts\neach source block in a pre-trained teacher network to a target block in a\nstudent network. For the recasting, a target block is trained such that its\noutput activation approximates that of the source block. Such a block-by-block\nrecasting in a sequential manner transforms the network architecture while\npreserving the accuracy. This method can be used to transform an arbitrary\nteacher network type to an arbitrary student network type. It can even generate\na mixed-architecture network that consists of two or more types of block. The\nnetwork recasting can generate a network with fewer parameters and/or\nactivations, which reduce the inference time significantly. Naturally, it can\nbe used for network compression by recasting a trained network into a smaller\nnetwork of the same type. Our experiments show that it outperforms previous\ncompression approaches in terms of actual speedup on a GPU.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 05:39:15 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 09:38:15 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Yu", "Joonsang", ""], ["Kang", "Sungbum", ""], ["Choi", "Kiyoung", ""]]}, {"id": "1809.05274", "submitter": "Liyuan Xu", "authors": "Liyuan Xu, Junya Honda, Masashi Sugiyama", "title": "Dueling Bandits with Qualitative Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and study a novel multi-armed bandit problem called the\nqualitative dueling bandit (QDB) problem, where an agent observes not numeric\nbut qualitative feedback by pulling each arm. We employ the same regret as the\ndueling bandit (DB) problem where the duel is carried out by comparing the\nqualitative feedback. Although we can naively use classic DB algorithms for\nsolving the QDB problem, this reduction significantly worsens the\nperformance---actually, in the QDB problem, the probability that one arm wins\nthe duel over another arm can be directly estimated without carrying out actual\nduels. In this paper, we propose such direct algorithms for the QDB problem.\nOur theoretical analysis shows that the proposed algorithms significantly\noutperform DB algorithms by incorporating the qualitative feedback, and\nexperimental results also demonstrate vast improvement over the existing DB\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 06:40:35 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 00:25:28 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Xu", "Liyuan", ""], ["Honda", "Junya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.05284", "submitter": "Hiroshi Takahashi", "authors": "Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada,\n  Satoshi Yagi", "title": "Variational Autoencoder with Implicit Optimal Priors", "comments": "9 pages, 9 figures, accepted at AAAI 2019. Code is available at\n  https://github.com/takahashihiroshi/vae_iop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) is a powerful generative model that can\nestimate the probability of a data point by using latent variables. In the VAE,\nthe posterior of the latent variable given the data point is regularized by the\nprior of the latent variable using Kullback Leibler (KL) divergence. Although\nthe standard Gaussian distribution is usually used for the prior, this simple\nprior incurs over-regularization. As a sophisticated prior, the aggregated\nposterior has been introduced, which is the expectation of the posterior over\nthe data distribution. This prior is optimal for the VAE in terms of maximizing\nthe training objective function. However, KL divergence with the aggregated\nposterior cannot be calculated in a closed form, which prevents us from using\nthis optimal prior. With the proposed method, we introduce the density ratio\ntrick to estimate this KL divergence without modeling the aggregated posterior\nexplicitly. Since the density ratio trick does not work well in high\ndimensions, we rewrite this KL divergence that contains the high-dimensional\ndensity ratio into the sum of the analytically calculable term and the\nlow-dimensional density ratio term, to which the density ratio trick is\napplied. Experiments on various datasets show that the VAE with this implicit\noptimal prior achieves high density estimation performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 07:30:31 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 00:43:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Takahashi", "Hiroshi", ""], ["Iwata", "Tomoharu", ""], ["Yamanaka", "Yuki", ""], ["Yamada", "Masanori", ""], ["Yagi", "Satoshi", ""]]}, {"id": "1809.05292", "submitter": "Zaiyi Chen", "authors": "Zaiyi Chen", "title": "Efficient Rank Minimization via Solving Non-convexPenalties by Iterative\n  Shrinkage-Thresholding Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank minimization (RM) is a wildly investigated task of finding solutions by\nexploiting low-rank structure of parameter matrices. Recently, solving RM\nproblem by leveraging non-convex relaxations has received significant\nattention. It has been demonstrated by some theoretical and experimental work\nthat non-convex relaxation, e.g. Truncated Nuclear Norm Regularization (TNNR)\nand Reweighted Nuclear Norm Regularization (RNNR), can provide a better\napproximation of original problems than convex relaxations. However, designing\nan efficient algorithm with theoretical guarantee remains a challenging\nproblem. In this paper, we propose a simple but efficient proximal-type method,\nnamely Iterative Shrinkage-Thresholding Algorithm(ISTA), with concrete analysis\nto solve rank minimization problems with both non-convex weighted and\nreweighted nuclear norm as low-rank regularizers. Theoretically, the proposed\nmethod could converge to the critical point under very mild assumptions with\nthe rate in the order of $O(1/T)$. Moreover, the experimental results on both\nsynthetic data and real world data sets show that proposed algorithm\noutperforms state-of-arts in both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 07:58:03 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Chen", "Zaiyi", ""]]}, {"id": "1809.05465", "submitter": "Mingyuan Wang", "authors": "Mingyuan Wang, Adrian Barbu", "title": "Are screening methods useful in feature selection? An empirical study", "comments": "29 pages, 4 figures, 21 tables", "journal-ref": "PLoS One, 09/11/2019", "doi": "10.1371/journal.pone.0220842", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter or screening methods are often used as a preprocessing step for\nreducing the number of variables used by a learning algorithm in obtaining a\nclassification or regression model. While there are many such filter methods,\nthere is a need for an objective evaluation of these methods. Such an\nevaluation is needed to compare them with each other and also to answer whether\nthey are at all useful, or a learning algorithm could do a better job without\nthem. For this purpose, many popular screening methods are partnered in this\npaper with three regression learners and five classification learners and\nevaluated on ten real datasets to obtain accuracy criteria such as R-square and\narea under the ROC curve (AUC). The obtained results are compared through curve\nplots and comparison tables in order to find out whether screening methods help\nimprove the performance of learning algorithms and how they fare with each\nother. Our findings revealed that the screening methods were useful in\nimproving the prediction of the best learner on two regression and two\nclassification datasets out of the ten datasets evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 15:21:53 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 15:36:17 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 02:12:27 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wang", "Mingyuan", ""], ["Barbu", "Adrian", ""]]}, {"id": "1809.05476", "submitter": "Dimitrios Stamoulis", "authors": "Diana Marculescu, Dimitrios Stamoulis, Ermao Cai", "title": "Hardware-Aware Machine Learning: Modeling and Optimization", "comments": "ICCAD'18 Invited Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Deep Learning (DL) applications have made DL models a\nkey component in almost every modern computing system. The increased popularity\nof DL applications deployed on a wide-spectrum of platforms have resulted in a\nplethora of design challenges related to the constraints introduced by the\nhardware itself. What is the latency or energy cost for an inference made by a\nDeep Neural Network (DNN)? Is it possible to predict this latency or energy\nconsumption before a model is trained? If yes, how can machine learners take\nadvantage of these models to design the hardware-optimal DNN for deployment?\nFrom lengthening battery life of mobile devices to reducing the runtime\nrequirements of DL models executing in the cloud, the answers to these\nquestions have drawn significant attention.\n  One cannot optimize what isn't properly modeled. Therefore, it is important\nto understand the hardware efficiency of DL models during serving for making an\ninference, before even training the model. This key observation has motivated\nthe use of predictive models to capture the hardware performance or energy\nefficiency of DL applications. Furthermore, DL practitioners are challenged\nwith the task of designing the DNN model, i.e., of tuning the hyper-parameters\nof the DNN architecture, while optimizing for both accuracy of the DL model and\nits hardware efficiency. Therefore, state-of-the-art methodologies have\nproposed hardware-aware hyper-parameter optimization techniques. In this paper,\nwe provide a comprehensive assessment of state-of-the-art work and selected\nresults on the hardware-aware modeling and optimization for DL applications. We\nalso highlight several open questions that are poised to give rise to novel\nhardware-aware designs in the next few years, as DL applications continue to\nsignificantly impact associated hardware systems and platforms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 15:53:14 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Marculescu", "Diana", ""], ["Stamoulis", "Dimitrios", ""], ["Cai", "Ermao", ""]]}, {"id": "1809.05483", "submitter": "Leonardo Gabrielli", "authors": "Leonardo Gabrielli, Stefano Tomassetti, Stefano Squartini, Carlo\n  Zinato, Stefano Guaiana", "title": "A Multi-Stage Algorithm for Acoustic Physical Model Parameters\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in computational acoustics is the identification of\nmodels that can simulate and predict the physical behavior of a system\ngenerating an acoustic signal. Whenever such models are used for commercial\napplications an additional constraint is the time-to-market, making automation\nof the sound design process desirable. In previous works, a computational sound\ndesign approach has been proposed for the parameter estimation problem\ninvolving timbre matching by deep learning, which was applied to the synthesis\nof pipe organ tones. In this work we refine previous results by introducing the\nformer approach in a multi-stage algorithm that also adds heuristics and a\nstochastic optimization method operating on objective cost functions based on\npsychoacoustics. The optimization method shows to be able to refine the first\nestimate given by the deep learning approach and substantially improve the\nobjective metrics, with the additional benefit of reducing the sound design\nprocess time. Subjective listening tests are also conducted to gather\nadditional insights on the results.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:05:51 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 14:39:56 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Gabrielli", "Leonardo", ""], ["Tomassetti", "Stefano", ""], ["Squartini", "Stefano", ""], ["Zinato", "Carlo", ""], ["Guaiana", "Stefano", ""]]}, {"id": "1809.05504", "submitter": "Bryan Wilder", "authors": "Bryan Wilder, Bistra Dilkina, Milind Tambe", "title": "Melding the Data-Decisions Pipeline: Decision-Focused Learning for\n  Combinatorial Optimization", "comments": "Full version of paper accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating impact in real-world settings requires artificial intelligence\ntechniques to span the full pipeline from data, to predictive models, to\ndecisions. These components are typically approached separately: a machine\nlearning model is first trained via a measure of predictive accuracy, and then\nits predictions are used as input into an optimization algorithm which produces\na decision. However, the loss function used to train the model may easily be\nmisaligned with the end goal, which is to make the best decisions possible.\nHand-tuning the loss function to align with optimization is a difficult and\nerror-prone process (which is often skipped entirely).\n  We focus on combinatorial optimization problems and introduce a general\nframework for decision-focused learning, where the machine learning model is\ndirectly trained in conjunction with the optimization algorithm to produce\nhigh-quality decisions. Technically, our contribution is a means of integrating\ncommon classes of discrete optimization problems into deep learning or other\npredictive models, which are typically trained via gradient descent. The main\nidea is to use a continuous relaxation of the discrete problem to propagate\ngradients through the optimization procedure. We instantiate this framework for\ntwo broad classes of combinatorial problems: linear programs and submodular\nmaximization. Experimental results across a variety of domains show that\ndecision-focused learning often leads to improved optimization performance\ncompared to traditional methods. We find that standard measures of accuracy are\nnot a reliable proxy for a predictive model's utility in optimization, and our\nmethod's ability to specify the true goal as the model's training objective\nyields substantial dividends across a range of decision problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 17:08:04 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 00:14:56 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wilder", "Bryan", ""], ["Dilkina", "Bistra", ""], ["Tambe", "Milind", ""]]}, {"id": "1809.05525", "submitter": "Pantita Palittapongarnpim", "authors": "Pantita Palittapongarnpim and Barry C. Sanders", "title": "Robustness of Quantum-Enhanced Adaptive Phase Estimation", "comments": "15 pages, 2 figures, 2 tables", "journal-ref": "Phys. Rev. A 100, 012106 (2019)", "doi": "10.1103/PhysRevA.100.012106", "report-no": null, "categories": "quant-ph cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As all physical adaptive quantum-enhanced metrology schemes operate under\nnoisy conditions with only partially understood noise characteristics, so a\npractical control policy must be robust even for unknown noise. We aim to\ndevise a test to evaluate the robustness of AQEM policies and assess the\nresource used by the policies. The robustness test is performed on QEAPE by\nsimulating the scheme under four phase-noise models corresponding to\nnormal-distribution noise, random-telegraph noise, skew-normal-distribution\nnoise, and log-normal-distribution noise. Control policies are devised either\nby an evolutionary algorithm under the same noisy conditions, albeit ignorant\nof its properties, or a Bayesian-based feedback method that assumes no noise.\nOur robustness test and resource comparison method can be used to determining\nthe efficacy and selecting a suitable policy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 17:56:25 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 02:44:02 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Palittapongarnpim", "Pantita", ""], ["Sanders", "Barry C.", ""]]}, {"id": "1809.05527", "submitter": "Y Cooper", "authors": "Y. Cooper", "title": "Gradient descent in higher codimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the behavior of gradient flow and of discrete and noisy gradient\ndescent. It is commonly noted that the addition of noise to the process of\ndiscrete gradient descent can affect the trajectory of gradient descent. In\nprevious work, we observed such effects. There, we considered the case where\nthe minima had codimension 1. In this note, we do some computer experiments and\nobserve the behavior of noisy gradient descent in the more complex setting of\nminima of higher codimension.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 17:58:27 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 14:14:44 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Cooper", "Y.", ""]]}, {"id": "1809.05550", "submitter": "Heejin Choi", "authors": "Heejin Choi", "title": "Efficient Structured Surrogate Loss and Regularization in Structured\n  Prediction", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we focus on several important problems in structured\nprediction. In structured prediction, the label has a rich intrinsic\nsubstructure, and the loss varies with respect to the predicted label and the\ntrue label pair. Structured SVM is an extension of binary SVM to adapt to such\nstructured tasks.\n  In the first part of the dissertation, we study the surrogate losses and its\nefficient methods. To minimize the empirical risk, a surrogate loss which upper\nbounds the loss, is used as a proxy to minimize the actual loss. Since the\nobjective function is written in terms of the surrogate loss, the choice of the\nsurrogate loss is important, and the performance depends on it. Another issue\nregarding the surrogate loss is the efficiency of the argmax label inference\nfor the surrogate loss. Efficient inference is necessary for the optimization\nsince it is often the most time-consuming step. We present a new class of\nsurrogate losses named bi-criteria surrogate loss, which is a generalization of\nthe popular surrogate losses. We first investigate an efficient method for a\nslack rescaling formulation as a starting point utilizing decomposability of\nthe model. Then, we extend the algorithm to the bi-criteria surrogate loss,\nwhich is very efficient and also shows performance improvements.\n  In the second part of the dissertation, another important issue of\nregularization is studied. Specifically, we investigate a problem of\nregularization in hierarchical classification when a structural imbalance\nexists in the label structure. We present a method to normalize the structure,\nas well as a new norm, namely shared Frobenius norm. It is suitable for\nhierarchical classification that adapts to the data in addition to the label\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:11:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Choi", "Heejin", ""]]}, {"id": "1809.05578", "submitter": "Fran\\c{c}ois Th\\'eberge", "authors": "Val\\'erie Poulin and Fran\\c{c}ois Th\\'eberge", "title": "Ensemble Clustering for Graphs", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-05411-3_19", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an ensemble clustering algorithm for graphs (ECG), which is based\non the Louvain algorithm and the concept of consensus clustering. We validate\nour approach by replicating a recently published study comparing graph\nclustering algorithms over artificial networks, showing that ECG outperforms\nthe leading algorithms from that study. We also illustrate how the ensemble\nobtained with ECG can be used to quantify the presence of community structure\nin the graph.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:38:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Poulin", "Val\u00e9rie", ""], ["Th\u00e9berge", "Fran\u00e7ois", ""]]}, {"id": "1809.05606", "submitter": "Yimin Yang", "authors": "Yimin Yang, Q.M.Jonathan Wu, Xiexing Feng, Thangarajah Akilan", "title": "Non-iterative recomputation of dense layers for performance improvement\n  of DCNN", "comments": "11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An iterative method of learning has become a paradigm for training deep\nconvolutional neural networks (DCNN). However, utilizing a non-iterative\nlearning strategy can accelerate the training process of the DCNN and\nsurprisingly such approach has been rarely explored by the deep learning (DL)\ncommunity. It motivates this paper to introduce a non-iterative learning\nstrategy that eliminates the backpropagation (BP) at the top dense or fully\nconnected (FC) layers of DCNN, resulting in, lower training time and higher\nperformance. The proposed method exploits the Moore-Penrose Inverse to pull\nback the current residual error to each FC layer, generating well-generalized\nfeatures. Then using the recomputed features, i.e., the new generalized\nfeatures the weights of each FC layer is computed according to the\nMoore-Penrose Inverse. We evaluate the proposed approach on six widely accepted\nobject recognition benchmark datasets: Scene-15, CIFAR-10, CIFAR-100, SUN-397,\nPlaces365, and ImageNet. The experimental results show that the proposed method\nobtains significant improvements over 30 state-of-the-art methods.\nInterestingly, it also indicates that any DCNN with the proposed method can\nprovide better performance than the same network with its original training\nbased on BP.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 22:24:52 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yang", "Yimin", ""], ["Wu", "Q. M. Jonathan", ""], ["Feng", "Xiexing", ""], ["Akilan", "Thangarajah", ""]]}, {"id": "1809.05630", "submitter": "Raghuram Mandyam Annasamy", "authors": "Raghuram Mandyam Annasamy, Katia Sycara", "title": "Towards Better Interpretability in Deep Q-Networks", "comments": "Accepted at AAAI-19; (16 pages, 18 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning techniques have demonstrated superior performance\nin a wide variety of environments. As improvements in training algorithms\ncontinue at a brisk pace, theoretical or empirical studies on understanding\nwhat these networks seem to learn, are far behind. In this paper we propose an\ninterpretable neural network architecture for Q-learning which provides a\nglobal explanation of the model's behavior using key-value memories, attention\nand reconstructible embeddings. With a directed exploration strategy, our model\ncan reach training rewards comparable to the state-of-the-art deep Q-learning\nmodels. However, results suggest that the features extracted by the neural\nnetwork are extremely shallow and subsequent testing using out-of-sample\nexamples shows that the agent can easily overfit to trajectories seen during\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 01:34:27 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 03:06:56 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Annasamy", "Raghuram Mandyam", ""], ["Sycara", "Katia", ""]]}, {"id": "1809.05693", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Charlie Soh, Lihui Chen, Yang Liu and Lipo Wang", "title": "apk2vec: Semi-supervised multi-view representation learning for\n  profiling Android applications", "comments": "International Conference on Data Mining, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building behavior profiles of Android applications (apps) with holistic, rich\nand multi-view information (e.g., incorporating several semantic views of an\napp such as API sequences, system calls, etc.) would help catering downstream\nanalytics tasks such as app categorization, recommendation and malware analysis\nsignificantly better. Towards this goal, we design a semi-supervised\nRepresentation Learning (RL) framework named apk2vec to automatically generate\na compact representation (aka profile/embedding) for a given app. More\nspecifically, apk2vec has the three following unique characteristics which make\nit an excellent choice for largescale app profiling: (1) it encompasses\ninformation from multiple semantic views such as API sequences, permissions,\netc., (2) being a semi-supervised embedding technique, it can make use of\nlabels associated with apps (e.g., malware family or app category labels) to\nbuild high quality app profiles, and (3) it combines RL and feature hashing\nwhich allows it to efficiently build profiles of apps that stream over time\n(i.e., online learning). The resulting semi-supervised multi-view hash\nembeddings of apps could then be used for a wide variety of downstream tasks\nsuch as the ones mentioned above. Our extensive evaluations with more than\n42,000 apps demonstrate that apk2vec's app profiles could significantly\noutperform state-of-the-art techniques in four app analytics tasks namely,\nmalware detection, familial clustering, app clone detection and app\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 10:37:06 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Soh", "Charlie", ""], ["Chen", "Lihui", ""], ["Liu", "Yang", ""], ["Wang", "Lipo", ""]]}, {"id": "1809.05710", "submitter": "Masahito Kato", "authors": "Masahiro Kato, Liyuan Xu, Gang Niu, and Masashi Sugiyama", "title": "Alternate Estimation of a Classifier and the Class-Prior from Positive\n  and Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of learning a binary classifier only from positive data\nand unlabeled data (PU learning) and estimating the class-prior in unlabeled\ndata under the case-control scenario. Most of the recent methods of PU learning\nrequire an estimate of the class-prior probability in unlabeled data, and it is\nestimated in advance with another method. However, such a two-step approach\nwhich first estimates the class prior and then trains a classifier may not be\nthe optimal approach since the estimation error of the class-prior is not taken\ninto account when a classifier is trained. In this paper, we propose a novel\nunified approach to estimating the class-prior and training a classifier\nalternately. Our proposed method is simple to implement and computationally\nefficient. Through experiments, we demonstrate the practical usefulness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 12:49:41 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Kato", "Masahiro", ""], ["Xu", "Liyuan", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.05781", "submitter": "Bilal Farooq", "authors": "Melvin Wong and Bilal Farooq", "title": "Modelling Latent Travel Behaviour Characteristics with Generative\n  Machine Learning", "comments": "Published in the proceedings of IEEE Intelligent Transportation\n  Systems Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we implement an information-theoretic approach to travel\nbehaviour analysis by introducing a generative modelling framework to identify\ninformative latent characteristics in travel decision making. It involves\ndeveloping a joint tri-partite Bayesian graphical network model using a\nRestricted Boltzmann Machine (RBM) generative modelling framework. We apply\nthis framework on a mode choice survey data to identify abstract latent\nvariables and compare the performance with a traditional latent variable model\nwith specific latent preferences -- safety, comfort, and environmental. Data\ncollected from a joint stated and revealed preference mode choice survey in\nQuebec, Canada were used to calibrate the RBM model. Results show that a\nsignficant impact on model likelihood statistics and suggests that machine\nlearning tools are highly suitable for modelling complex networks of\nconditional independent behaviour interactions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 23:57:34 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Wong", "Melvin", ""], ["Farooq", "Bilal", ""]]}, {"id": "1809.05786", "submitter": "Yasin Almalioglu", "authors": "Yasin Almalioglu, Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao,\n  Andrew Markham, Niki Trigoni", "title": "GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation\n  with Generative Adversarial Networks", "comments": "ICRA 2019 - accepted", "journal-ref": "2019 International Conference on Robotics and Automation (ICRA)", "doi": "10.1109/ICRA.2019.8793512", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, supervised deep learning approaches have been extensively\nemployed in visual odometry (VO) applications, which is not feasible in\nenvironments where labelled data is not abundant. On the other hand,\nunsupervised deep learning approaches for localization and mapping in unknown\nenvironments from unlabelled data have received comparatively less attention in\nVO research. In this study, we propose a generative unsupervised learning\nframework that predicts 6-DoF pose camera motion and monocular depth map of the\nscene from unlabelled RGB image sequences, using deep convolutional Generative\nAdversarial Networks (GANs). We create a supervisory signal by warping view\nsequences and assigning the re-projection minimization to the objective loss\nfunction that is adopted in multi-view pose estimation and single-view depth\ngeneration network. Detailed quantitative and qualitative evaluations of the\nproposed framework on the KITTI and Cityscapes datasets show that the proposed\nmethod outperforms both existing traditional and unsupervised deep VO methods\nproviding better results for both pose estimation and depth recovery.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 00:27:09 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 14:01:53 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 12:45:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Saputra", "Muhamad Risqi U.", ""], ["de Gusmao", "Pedro P. B.", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1809.05788", "submitter": "Bilal Farooq", "authors": "Arash Kalatian and Bilal Farooq", "title": "Mobility Mode Detection Using WiFi Signals", "comments": "Published in the proceedings of IEEE International Smart Cities\n  Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize Wi-Fi communications from smartphones to predict their mobility\nmode, i.e. walking, biking and driving. Wi-Fi sensors were deployed at four\nstrategic locations in a closed loop on streets in downtown Toronto. Deep\nneural network (Multilayer Perceptron) along with three decision tree based\nclassifiers (Decision Tree, Bagged Decision Tree and Random Forest) are\ndeveloped. Results show that the best prediction accuracy is achieved by\nMultilayer Perceptron, with 86.52% correct predictions of mobility modes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 00:49:24 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Kalatian", "Arash", ""], ["Farooq", "Bilal", ""]]}, {"id": "1809.05814", "submitter": "Boyi Yang", "authors": "Boyi Yang, Adam Wright", "title": "Development of deep learning algorithms to categorize free-text notes\n  pertaining to diabetes: convolution neural networks achieve higher accuracy\n  than support vector machines", "comments": "9 pages, 4 figures, submitted to Journal of the American Medical\n  Informatics Association (JAMIA) on September 15th, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health professionals can use natural language processing (NLP) technologies\nwhen reviewing electronic health records (EHR). Machine learning free-text\nclassifiers can help them identify problems and make critical decisions. We aim\nto develop deep learning neural network algorithms that identify EHR progress\nnotes pertaining to diabetes and validate the algorithms at two institutions.\nThe data used are 2,000 EHR progress notes retrieved from patients with\ndiabetes and all notes were annotated manually as diabetic or non-diabetic.\nSeveral deep learning classifiers were developed, and their performances were\nevaluated with the area under the ROC curve (AUC). The convolutional neural\nnetwork (CNN) model with a separable convolution layer accurately identified\ndiabetes-related notes in the Brigham and Womens Hospital testing set with the\nhighest AUC of 0.975. Deep learning classifiers can be used to identify EHR\nprogress notes pertaining to diabetes. In particular, the CNN-based classifier\ncan achieve a higher AUC than an SVM-based classifier.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 04:21:38 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yang", "Boyi", ""], ["Wright", "Adam", ""]]}, {"id": "1809.05815", "submitter": "Amichai Painsky", "authors": "Amichai Painsky, Saharon Rosset, Meir Feder", "title": "Linear Independent Component Analysis over Finite Fields: Algorithms and\n  Bounds", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2872006", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a statistical tool that decomposes an\nobserved random vector into components that are as statistically independent as\npossible. ICA over finite fields is a special case of ICA, in which both the\nobservations and the decomposed components take values over a finite alphabet.\nThis problem is also known as minimal redundancy representation or factorial\ncoding. In this work we focus on linear methods for ICA over finite fields. We\nintroduce a basic lower bound which provides a fundamental limit to the ability\nof any linear solution to solve this problem. Based on this bound, we present a\ngreedy algorithm that outperforms all currently known methods. Importantly, we\nshow that the overhead of our suggested algorithm (compared with the lower\nbound) typically decreases, as the scale of the problem grows. In addition, we\nprovide a sub-optimal variant of our suggested method that significantly\nreduces the computational complexity at a relatively small cost in performance.\nFinally, we discuss the universal abilities of linear transformations in\ndecomposing random vectors, compared with existing non-linear solutions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 04:33:21 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Painsky", "Amichai", ""], ["Rosset", "Saharon", ""], ["Feder", "Meir", ""]]}, {"id": "1809.05822", "submitter": "Wenhui Yu", "authors": "Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, Zheng Qin", "title": "Aesthetic-based Clothing Recommendation", "comments": "WWW 2018", "journal-ref": null, "doi": "10.1145/3178876.3186146", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, product images have gained increasing attention in clothing\nrecommendation since the visual appearance of clothing products has a\nsignificant impact on consumers' decision. Most existing methods rely on\nconventional features to represent an image, such as the visual features\nextracted by convolutional neural networks (CNN features) and the\nscale-invariant feature transform algorithm (SIFT features), color histograms,\nand so on. Nevertheless, one important type of features, the \\emph{aesthetic\nfeatures}, is seldom considered. It plays a vital role in clothing\nrecommendation since a users' decision depends largely on whether the clothing\nis in line with her aesthetics, however the conventional image features cannot\nportray this directly. To bridge this gap, we propose to introduce the\naesthetic information, which is highly relevant with user preference, into\nclothing recommender systems. To achieve this, we first present the aesthetic\nfeatures extracted by a pre-trained neural network, which is a brain-inspired\ndeep structure trained for the aesthetic assessment task. Considering that the\naesthetic preference varies significantly from user to user and by time, we\nthen propose a new tensor factorization model to incorporate the aesthetic\nfeatures in a personalized manner. We conduct extensive experiments on\nreal-world datasets, which demonstrate that our approach can capture the\naesthetic preference of users and significantly outperform several\nstate-of-the-art recommendation methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 06:20:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yu", "Wenhui", ""], ["Zhang", "Huidi", ""], ["He", "Xiangnan", ""], ["Chen", "Xu", ""], ["Xiong", "Li", ""], ["Qin", "Zheng", ""]]}, {"id": "1809.05839", "submitter": "Gautham Krishna Gudur", "authors": "Gautham Krishna G, Karthik Subramanian Nathan, Yogesh Kumar B, Ankith\n  A Prabhu, Ajay Kannan, Vineeth Vijayaraghavan", "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine\n  Learning", "comments": "Accepted at IEEE Future of Information and Communications Conference\n  (FICC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computer interaction facilitates intelligent communication between\nhumans and computers, in which gesture recognition plays a prominent role. This\npaper proposes a machine learning system to identify dynamic gestures using\ntri-axial acceleration data acquired from two public datasets. These datasets,\nuWave and Sony, were acquired using accelerometers embedded in Wii remotes and\nsmartwatches, respectively. A dynamic gesture signed by the user is\ncharacterized by a generic set of features extracted across time and frequency\ndomains. The system was analyzed from an end-user perspective and was modelled\nto operate in three modes. The modes of operation determine the subsets of data\nto be used for training and testing the system. From an initial set of seven\nclassifiers, three were chosen to evaluate each dataset across all modes\nrendering the system towards mode-neutrality and dataset-independence. The\nproposed system is able to classify gestures performed at varying speeds with\nminimum preprocessing, making it computationally efficient. Moreover, this\nsystem was found to run on a low-cost embedded platform - Raspberry Pi Zero\n(USD 5), making it economically viable.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:51:05 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["G", "Gautham Krishna", ""], ["Nathan", "Karthik Subramanian", ""], ["B", "Yogesh Kumar", ""], ["Prabhu", "Ankith A", ""], ["Kannan", "Ajay", ""], ["Vijayaraghavan", "Vineeth", ""]]}, {"id": "1809.05861", "submitter": "Jianlin Su", "authors": "Jianlin Su, Guang Wu", "title": "f-VAEs: Improve VAEs with Conditional Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we integrate VAEs and flow-based generative models\nsuccessfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid\nimages, solved the blurred-image problem of VAEs. Compared with flow-based\nmodels such as Glow, f-VAE is more lightweight and converges faster, achieving\nthe same performance under smaller-size architecture.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 12:23:09 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Su", "Jianlin", ""], ["Wu", "Guang", ""]]}, {"id": "1809.05872", "submitter": "Nir Baram", "authors": "Nir Baram, Shie Mannor", "title": "Inspiration Learning through Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current imitation learning techniques are too restrictive because they\nrequire the agent and expert to share the same action space. However,\noftentimes agents that act differently from the expert can solve the task just\nas good. For example, a person lifting a box can be imitated by a ceiling\nmounted robot or a desktop-based robotic-arm. In both cases, the end goal of\nlifting the box is achieved, perhaps using different strategies. We denote this\nsetup as \\textit{Inspiration Learning} - knowledge transfer between agents that\noperate in different action spaces. Since state-action expert demonstrations\ncan no longer be used, Inspiration learning requires novel methods to guide the\nagent towards the end goal. In this work, we rely on ideas of Preferential\nbased Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms\nfor solving inspiration learning tasks. Unlike classic actor-critic\narchitectures, the critic we use consists of two parts: a) a state-value\nestimation as in common actor-critic algorithms and b) a single step reward\nfunction derived from an expert/agent classifier. We show that our method is\ncapable of extending the current imitation framework to new horizons. This\nincludes continuous-to-discrete action imitation, as well as primitive-to-macro\naction imitation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 13:37:45 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Baram", "Nir", ""], ["Mannor", "Shie", ""]]}, {"id": "1809.05877", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Nag Mani, Anand Ramasubramanian, Jaskiran Kalsi", "title": "A Data Analytics Framework for Aggregate Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contexts, we have access to aggregate data, but individual level data\nis unavailable. For example, medical studies sometimes report only aggregate\nstatistics about disease prevalence because of privacy concerns. Even so, many\na time it is desirable, and in fact could be necessary to infer individual\nlevel characteristics from aggregate data. For instance, other researchers who\nwant to perform more detailed analysis of disease characteristics would require\nindividual level data. Similar challenges arise in other fields too including\npolitics, and marketing.\n  In this paper, we present an end-to-end pipeline for processing of aggregate\ndata to derive individual level statistics, and then using the inferred data to\ntrain machine learning models to answer questions of interest. We describe a\nnovel algorithm for reconstructing fine-grained data from summary statistics.\nThis step will create multiple candidate datasets which will form the input to\nthe machine learning models. The advantage of the highly parallel architecture\nwe propose is that uncertainty in the generated fine-grained data will be\ncompensated by the use of multiple candidate fine-grained datasets.\nConsequently, the answers derived from the machine learning models will be more\nvalid and usable. We validate our approach using data from a challenging\nmedical problem called Acute Traumatic Coagulopathy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:13:34 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Mani", "Nag", ""], ["Ramasubramanian", "Anand", ""], ["Kalsi", "Jaskiran", ""]]}, {"id": "1809.05879", "submitter": "Abdallah Moussawi", "authors": "Abdallah Moussawi, Kamal Haddad and Anthony Chahine", "title": "An FPGA-Accelerated Design for Deep Learning Pedestrian Detection in\n  Self-Driving Vehicles", "comments": "7 pages. American University of Beirut, Faculty of Engineering and\n  Architecture Student and Alumni Conference 2017 FEASAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of self-driving vehicles comes the risk of accidents and the\nneed for higher safety, and protection for pedestrian detection in the\nfollowing scenarios: imminent crashes, thus the car should crash into an object\nand avoid the pedestrian, and in the case of road intersections, where it is\nimportant for the car to stop when pedestrians are crossing. Currently, a\nspecial topology of deep neural networks called Fused Deep Neural Network\n(F-DNN) is considered to be the state of the art in pedestrian detection, as it\nhas the lowest miss rate, yet it is very slow. Therefore, acceleration is\nneeded to speed up the performance. This project proposes two contributions to\naddress this problem, by using a deep neural network used for object detection,\ncalled Single Shot Multi-Box Detector (SSD). The first contribution is training\nand tuning the hyperparameters of SSD to improve pedestrian detection. The\nsecond contribution is a new FPGA design for accelerating the model on the\nAltera Arria 10 platform. The final system will be used in self-driving\nvehicles in real-time. Preliminary results of the improved SSD shows 3% higher\nmiss-rate than F-DNN on Caltech pedestrian detection benchmark, but 4x\nperformance improvement. The acceleration design is expected to achieve an\nadditional performance improvement significantly outweighing the minimal\ndifference in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:16:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Moussawi", "Abdallah", ""], ["Haddad", "Kamal", ""], ["Chahine", "Anthony", ""]]}, {"id": "1809.05896", "submitter": "Markku Hinkka", "authors": "Markku Hinkka, Teemu Lehto, Keijo Heljanko, Alexander Jung", "title": "Classifying Process Instances Using Recurrent Neural Networks", "comments": "Proceedings of the BPM 2018 Workshops", "journal-ref": null, "doi": "10.1007/978-3-030-11641-5_25", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process Mining consists of techniques where logs created by operative systems\nare transformed into process models. In process mining tools it is often\ndesired to be able to classify ongoing process instances, e.g., to predict how\nlong the process will still require to complete, or to classify process\ninstances to different classes based only on the activities that have occurred\nin the process instance thus far. Recurrent neural networks and its subclasses,\nsuch as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been\ndemonstrated to be able to learn relevant temporal features for subsequent\nclassification tasks. In this paper we apply recurrent neural networks to\nclassifying process instances. The proposed model is trained in a supervised\nfashion using labeled process instances extracted from event log traces. This\nis the first time we know of GRU having been used in classifying business\nprocess instances. Our main experimental results shows that GRU outperforms\nLSTM remarkably in training time while giving almost identical accuracies to\nLSTM models. Additional contributions of our paper are improving the\nclassification model training time by filtering infrequent activities, which is\na technique commonly used, e.g., in Natural Language Processing (NLP).\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 15:33:24 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Hinkka", "Markku", ""], ["Lehto", "Teemu", ""], ["Heljanko", "Keijo", ""], ["Jung", "Alexander", ""]]}, {"id": "1809.05910", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman and\n  Daniel Cohen-Or", "title": "MeshCNN: A Network with an Edge", "comments": "For a two-minute explanation video see https://bit.ly/meshcnnvideo", "journal-ref": null, "doi": "10.1145/3306346.3322959", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes provide an efficient representation for 3D shapes. They\nexplicitly capture both shape surface and topology, and leverage non-uniformity\nto represent large flat regions as well as sharp, intricate features. This\nnon-uniformity and irregularity, however, inhibits mesh analysis efforts using\nneural networks that combine convolution and pooling operations. In this paper,\nwe utilize the unique properties of the mesh for a direct analysis of 3D shapes\nusing MeshCNN, a convolutional neural network designed specifically for\ntriangular meshes. Analogous to classic CNNs, MeshCNN combines specialized\nconvolution and pooling layers that operate on the mesh edges, by leveraging\ntheir intrinsic geodesic connections. Convolutions are applied on edges and the\nfour edges of their incident triangles, and pooling is applied via an edge\ncollapse operation that retains surface topology, thereby, generating new mesh\nconnectivity for the subsequent convolutions. MeshCNN learns which edges to\ncollapse, thus forming a task-driven process where the network exposes and\nexpands the important features while discarding the redundant ones. We\ndemonstrate the effectiveness of our task-driven pooling on various learning\ntasks applied to 3D meshes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 16:32:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 11:30:57 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hanocka", "Rana", ""], ["Hertz", "Amir", ""], ["Fish", "Noa", ""], ["Giryes", "Raja", ""], ["Fleishman", "Shachar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1809.05916", "submitter": "James O' Neill", "authors": "James O' Neill and Danushka Bollegala", "title": "Curriculum-Based Neighborhood Sampling For Sequence Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of multi-step ahead prediction in language models is challenging\nconsidering the discrepancy between training and testing. At test time, a\nlanguage model is required to make predictions given past predictions as input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time.\n  In order to improve generalization in neural language models and address\ncompounding errors, we propose a curriculum learning based method that\ngradually changes an initially deterministic teacher policy to a gradually more\nstochastic policy, which we refer to as \\textit{Nearest-Neighbor Replacement\nSampling}. A chosen input at a given timestep is replaced with a sampled\nnearest neighbor of the past target with a truncated probability proportional\nto the cosine similarity between the original word and its top $k$ most similar\nwords. This allows the teacher to explore alternatives when the teacher\nprovides a sub-optimal policy or when the initial policy is difficult for the\nlearner to model. The proposed strategy is straightforward, online and requires\nlittle additional memory requirements. We report our main findings on two\nlanguage modelling benchmarks and find that the proposed approach performs\nparticularly well when used in conjunction with scheduled sampling, that too\nattempts to mitigate compounding errors in language models.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 17:17:56 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1809.05922", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes, Nathan D. Cahill, Christopher Kanan", "title": "Memory Efficient Experience Replay for Streaming Learning", "comments": "To appear in the IEEE International Conference on Robotics and\n  Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised machine learning, an agent is typically trained once and then\ndeployed. While this works well for static settings, robots often operate in\nchanging environments and must quickly learn new things from data streams. In\nthis paradigm, known as streaming learning, a learner is trained online, in a\nsingle pass, from a data stream that cannot be assumed to be independent and\nidentically distributed (iid). Streaming learning will cause conventional deep\nneural networks (DNNs) to fail for two reasons: 1) they need multiple passes\nthrough the entire dataset; and 2) non-iid data will cause catastrophic\nforgetting. An old fix to both of these issues is rehearsal. To learn a new\nexample, rehearsal mixes it with previous examples, and then this mixture is\nused to update the DNN. Full rehearsal is slow and memory intensive because it\nstores all previously observed examples, and its effectiveness for preventing\ncatastrophic forgetting has not been studied in modern DNNs. Here, we describe\nthe ExStream algorithm for memory efficient rehearsal and compare it to\nalternatives. We find that full rehearsal can eliminate catastrophic forgetting\nin a variety of streaming learning settings, with ExStream performing well\nusing far less memory and computation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 18:04:33 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 23:32:51 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Cahill", "Nathan D.", ""], ["Kanan", "Christopher", ""]]}, {"id": "1809.05929", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Solving for multi-class: a survey and synthesis", "comments": "Tried to cut out the fat and improve wording and organization.\n  Returned title to the original", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the best statistical classification algorithms are binary classifiers\nthat can only distinguish between one of two classes. The number of possible\nways of generalizing binary classification to multi-class increases\nexponentially with the number of classes. There is some indication that the\nbest method will depend on the dataset. Hence, we are particularly interested\nin data-driven solution design, whether based on prior considerations or on\nempirical examination of the data. Here we demonstrate how a recursive control\nlanguage can be used to describe a multitude of different partitioning\nstrategies in multi-class classification, including those in most common use.\nWe use it both to manually construct new partitioning configurations as well as\nto examine those that have been automatically designed.\n  Eight different strategies were tested on eight different datasets using a\nsupport vector machine (SVM) as the base binary classifier. Numerical results\nsuggest that a one-size-fits-all solution consisting of one-versus-one is\nappropriate for most datasets. Three datasets showed better accuracy using\ndifferent methods. The best solution for the most improved dataset exploited a\nproperty of the data to produce an uncertainty coefficient 36\\% higher (0.016\nabsolute gain) than one-vs.-one. For the same dataset, an adaptive solution\nthat empirically examined the data was also more accurate than one-vs.-one\nwhile being faster.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 18:28:08 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 01:18:02 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 17:49:01 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 00:05:43 GMT"}, {"version": "v5", "created": "Fri, 21 Jun 2019 02:27:58 GMT"}, {"version": "v6", "created": "Sat, 2 Nov 2019 18:58:18 GMT"}, {"version": "v7", "created": "Sun, 24 Jan 2021 18:04:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1809.05957", "submitter": "Jeffrey Regier", "authors": "Maxime Langevin, Edouard Mehlman, Jeffrey Regier, Romain Lopez,\n  Michael I. Jordan, and Nir Yosef", "title": "A Deep Generative Model for Semi-Supervised Classification with Noisy\n  Labels", "comments": "accepted to BayLearn 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels are often imperfectly observed, due to mistakes and to genuine\nambiguity among classes. We propose a new semi-supervised deep generative model\nthat explicitly models noisy labels, called the Mislabeled VAE (M-VAE). The\nM-VAE can perform better than existing deep generative models which do not\naccount for label noise. Additionally, the derivation of M-VAE gives new\ntheoretical insights into the popular M1+M2 semi-supervised model.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 21:04:47 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Langevin", "Maxime", ""], ["Mehlman", "Edouard", ""], ["Regier", "Jeffrey", ""], ["Lopez", "Romain", ""], ["Jordan", "Michael I.", ""], ["Yosef", "Nir", ""]]}, {"id": "1809.05964", "submitter": "Huidong Liu", "authors": "Huidong Liu, Yang Guo, Na Lei, Zhixin Shu, Shing-Tung Yau, Dimitris\n  Samaras, Xianfeng Gu", "title": "Latent Space Optimal Transport for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Auto-Encoders enforce their learned intermediate latent-space\ndata distribution to be a simple distribution, such as an isotropic Gaussian.\nHowever, this causes the posterior collapse problem and loses manifold\nstructure which can be important for datasets such as facial images. A GAN can\ntransform a simple distribution to a latent-space data distribution and thus\npreserve the manifold structure, but optimizing a GAN involves solving a\nMin-Max optimization problem, which is difficult and not well understood so\nfar. Therefore, we propose a GAN-like method to transform a simple distribution\nto a data distribution in the latent space by solving only a minimization\nproblem. This minimization problem comes from training a discriminator between\na simple distribution and a latent-space data distribution. Then, we can\nexplicitly formulate an Optimal Transport (OT) problem that computes the\ndesired mapping between the two distributions. This means that we can transform\na distribution without solving the difficult Min-Max optimization problem.\nExperimental results on an eight-Gaussian dataset show that the proposed OT can\nhandle multi-cluster distributions. Results on the MNIST and the CelebA\ndatasets validate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 21:57:56 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Huidong", ""], ["Guo", "Yang", ""], ["Lei", "Na", ""], ["Shu", "Zhixin", ""], ["Yau", "Shing-Tung", ""], ["Samaras", "Dimitris", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1809.06009", "submitter": "Jessica Titensky", "authors": "Jessica S. Titensky, Hayden Jananthan, Jeremy Kepner", "title": "Uncertainty Propagation in Deep Neural Networks Using Extended Kalman\n  Filtering", "comments": "4 Pages, 8 figures. Accepted at MIT IEEE Undergraduate Research\n  Technology Conference 2018. Publication pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended Kalman Filtering (EKF) can be used to propagate and quantify input\nuncertainty through a Deep Neural Network (DNN) assuming mild hypotheses on the\ninput distribution. This methodology yields results comparable to existing\nmethods of uncertainty propagation for DNNs while lowering the computational\noverhead considerably. Additionally, EKF allows model error to be naturally\nincorporated into the output uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 03:30:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Titensky", "Jessica S.", ""], ["Jananthan", "Hayden", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1809.06018", "submitter": "Xi Zhang", "authors": "Xi Sheryl Zhang, Jingyuan Chou, Fei Wang", "title": "Integrative Analysis of Patient Health Records and Neuroimages via\n  Memory-based Graph Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the arrival of the big data era, more and more data are becoming readily\navailable in various real-world applications and those data are usually highly\nheterogeneous. Taking computational medicine as an example, we have both\nElectronic Health Records (EHR) and medical images for each patient. For\ncomplicated diseases such as Parkinson's and Alzheimer's, both EHR and\nneuroimaging information are very important for disease understanding because\nthey contain complementary aspects of the disease. However, EHR and neuroimage\nare completely different. So far the existing research has been mainly focusing\non one of them. In this paper, we proposed a framework, Memory-Based Graph\nConvolution Network (MemGCN), to perform integrative analysis with such\nmulti-modal data. Specifically, GCN is used to extract useful information from\nthe patients' neuroimages. The information contained in the patient EHRs before\nthe acquisition of each brain image is captured by a memory network because of\nits sequential nature. The information contained in each brain image is\ncombined with the information read out from the memory network to infer the\ndisease state at the image acquisition timestamp. To further enhance the\nanalytical power of MemGCN, we also designed a multi-hop strategy that allows\nmultiple reading and updating on the memory can be performed at each iteration.\nWe conduct experiments using the patient data from the Parkinson's Progression\nMarkers Initiative (PPMI) with the task of classification of Parkinson's\nDisease (PD) cases versus controls. We demonstrate that superior classification\nperformance can be achieved with our proposed framework, comparing with\nexisting approaches involving a single type of data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 04:45:39 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 14:02:12 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 06:06:25 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 04:11:54 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhang", "Xi Sheryl", ""], ["Chou", "Jingyuan", ""], ["Wang", "Fei", ""]]}, {"id": "1809.06019", "submitter": "Guang Cheng", "authors": "Meimei Liu, Jean Honorio, Guang Cheng", "title": "Statistically and Computationally Efficient Variance Estimator for\n  Kernel Ridge Regression", "comments": "To Appear in 2018 Allerton", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a random projection approach to estimate variance\nin kernel ridge regression. Our approach leads to a consistent estimator of the\ntrue variance, while being computationally more efficient. Our variance\nestimator is optimal for a large family of kernels, including cubic splines and\nGaussian kernels. Simulation analysis is conducted to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 04:53:46 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Meimei", ""], ["Honorio", "Jean", ""], ["Cheng", "Guang", ""]]}, {"id": "1809.06024", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Zhaoran Wang, Tong Zhang, Han Liu, R. Dennis Cook", "title": "A convex formulation for high-dimensional sparse sliced inverse\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced inverse regression is a popular tool for sufficient dimension\nreduction, which replaces covariates with a minimal set of their linear\ncombinations without loss of information on the conditional distribution of the\nresponse given the covariates. The estimated linear combinations include all\ncovariates, making results difficult to interpret and perhaps unnecessarily\nvariable, particularly when the number of covariates is large. In this paper,\nwe propose a convex formulation for fitting sparse sliced inverse regression in\nhigh dimensions. Our proposal estimates the subspace of the linear combinations\nof the covariates directly and performs variable selection simultaneously. We\nsolve the resulting convex optimization problem via the linearized alternating\ndirection methods of multiplier algorithm, and establish an upper bound on the\nsubspace distance between the estimated and the true subspaces. Through\nnumerical studies, we show that our proposal is able to identify the correct\ncovariates in the high-dimensional setting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 05:23:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Tan", "Kean Ming", ""], ["Wang", "Zhaoran", ""], ["Zhang", "Tong", ""], ["Liu", "Han", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1809.06025", "submitter": "Louis Ly", "authors": "Louis Ly and Yen-Hsi Richard Tsai", "title": "Greedy Algorithms for Sparse Sensor Placement via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the exploration problem: an agent equipped with a depth sensor\nmust map out a previously unknown environment using as few sensor measurements\nas possible. We propose an approach based on supervised learning of a greedy\nalgorithm. We provide a bound on the optimality of the greedy algorithm using\nsubmodularity theory. Using a level set representation, we train a\nconvolutional neural network to determine vantage points that maximize\nvisibility. We show that this method drastically reduces the on-line\ncomputational cost and determines a small set of vantage points that solve the\nproblem. This enables us to efficiently produce highly-resolved and\ntopologically accurate maps of complex 3D environments. Unlike traditional\nnext-best-view and frontier-based strategies, the proposed method accounts for\ngeometric priors while evaluating potential vantage points. While existing deep\nlearning approaches focus on obstacle avoidance and local navigation, our\nmethod aims at finding near-optimal solutions to the more global exploration\nproblem. We present realistic simulations on 2D and 3D urban environments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 05:24:14 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 07:47:57 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 14:58:54 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 04:25:57 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 19:40:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ly", "Louis", ""], ["Tsai", "Yen-Hsi Richard", ""]]}, {"id": "1809.06035", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Julien Mairal (Thoth), Bertrand Thirion\n  (PARIETAL), Ga\\\"el Varoquaux (PARIETAL)", "title": "Extracting representations of cognition across neuroimaging studies\n  improves brain decoding", "comments": null, "journal-ref": "PLoS Computational Biology, Public Library of Science, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive brain imaging is accumulating datasets about the neural substrate\nof many different mental processes. Yet, most studies are based on few subjects\nand have low statistical power. Analyzing data across studies could bring more\nstatistical power; yet the current brain-imaging analytic framework cannot be\nused at scale as it requires casting all cognitive tasks in a unified\ntheoretical framework. We introduce a new methodology to analyze brain\nresponses across tasks without a joint model of the psychological processes.\nThe method boosts statistical power in small studies with specific cognitive\nfocus by analyzing them jointly with large studies that probe less focal mental\nprocesses. Our approach improves decoding performance for 80% of 35\nwidely-different functional-imaging studies. It finds commonalities across\ntasks in a data-driven way, via common brain representations that predict\nmental processes. These are brain networks tuned to psychological\nmanipulations. They outline interpretable and plausible brain structures. The\nextracted networks have been made available; they can be readily reused in new\nneuro-imaging studies. We provide a multi-study decoding tool to adapt to new\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:19:11 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:50:24 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:37:14 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Mairal", "Julien", "", "Thoth"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1809.06040", "submitter": "Manjesh Kumar Hanawal", "authors": "Manjesh K. Hanawal and Sumit J. Darak", "title": "Multi-Player Bandits: A Trekking Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic multi-armed bandits with many players. The players do not\nknow the number of players, cannot communicate with each other and if multiple\nplayers select a common arm they collide and none of them receive any reward.\nWe consider the static scenario, where the number of players remains fixed, and\nthe dynamic scenario, where the players enter and leave at any time. We provide\nalgorithms based on a novel `trekking approach' that guarantees constant regret\nfor the static case and sub-linear regret for the dynamic case with high\nprobability. The trekking approach eliminates the need to estimate the number\nof players resulting in fewer collisions and improved regret performance\ncompared to the state-of-the-art algorithms. We also develop an epoch-less\nalgorithm that eliminates any requirement of time synchronization across the\nplayers provided each player can detect the presence of other players on an\narm. We validate our theoretical guarantees using simulation based and real\ntest-bed based experiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:29:43 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Hanawal", "Manjesh K.", ""], ["Darak", "Sumit J.", ""]]}, {"id": "1809.06045", "submitter": "Dominique Vaufreydaz", "authors": "Pavan Vasishta (UGA, CHROMA), Dominique Vaufreydaz (PERVASIVE), Anne\n  Spalanzani (CHROMA)", "title": "Building Prior Knowledge: A Markov Based Pedestrian Prediction Model\n  Using Urban Environmental Data", "comments": "15 th International Conference on Control, Automation, Robotics and\n  Vision (ICARCV 2018), Nov 2018, Singapore, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Vehicles navigating in urban areas have a need to understand and\npredict future pedestrian behavior for safer navigation. This high level of\nsituational awareness requires observing pedestrian behavior and extrapolating\ntheir positions to know future positions. While some work has been done in this\nfield using Hidden Markov Models (HMMs), one of the few observed drawbacks of\nthe method is the need for informed priors for learning behavior. In this work,\nan extension to the Growing Hidden Markov Model (GHMM) method is proposed to\nsolve some of these drawbacks. This is achieved by building on existing work\nusing potential cost maps and the principle of Natural Vision. As a\nconsequence, the proposed model is able to predict pedestrian positions more\nprecisely over a longer horizon compared to the state of the art. The method is\ntested over \"legal\" and \"illegal\" behavior of pedestrians, having trained the\nmodel with sparse observations and partial trajectories. The method, with no\ntraining data, is compared against a trained state of the art model. It is\nobserved that the proposed method is robust even in new, previously unseen\nareas.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:06:44 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Vasishta", "Pavan", "", "UGA, CHROMA"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"], ["Spalanzani", "Anne", "", "CHROMA"]]}, {"id": "1809.06061", "submitter": "Katia Sycara", "authors": "Rahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, Katia\n  Sycara", "title": "Transparency and Explanation in Deep Reinforcement Learning Neural\n  Networks", "comments": "8 pages, 5 figures, Accepted at AAAI/ACM Conference on AI, Ethics,\n  and Society 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous AI systems will be entering human society in the near future to\nprovide services and work alongside humans. For those systems to be accepted\nand trusted, the users should be able to understand the reasoning process of\nthe system, i.e. the system should be transparent. System transparency enables\nhumans to form coherent explanations of the system's decisions and actions.\nTransparency is important not only for user trust, but also for software\ndebugging and certification. In recent years, Deep Neural Networks have made\ngreat advances in multiple application areas. However, deep neural networks are\nopaque. In this paper, we report on work in transparency in Deep Reinforcement\nLearning Networks (DRLN). Such networks have been extremely successful in\naccurately learning action control in image input domains, such as Atari games.\nIn this paper, we propose a novel and general method that (a) incorporates\nexplicit object recognition processing into deep reinforcement learning models,\n(b) forms the basis for the development of \"object saliency maps\", to provide\nvisualization of internal states of DRLNs, thus enabling the formation of\nexplanations and (c) can be incorporated in any existing deep reinforcement\nlearning framework. We present computational results and human experiments to\nevaluate our approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:56:35 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Iyer", "Rahul", ""], ["Li", "Yuezhang", ""], ["Li", "Huao", ""], ["Lewis", "Michael", ""], ["Sundar", "Ramitha", ""], ["Sycara", "Katia", ""]]}, {"id": "1809.06064", "submitter": "Katia Sycara", "authors": "Yuezhang Li, Katia Sycara, Rahul Iyer", "title": "Object-sensitive Deep Reinforcement Learning", "comments": "15 pages, 6 figures, Accepted at 3rd Global Conference on Artificial\n  Intelligence (GCAI-17), Miami, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:59:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Li", "Yuezhang", ""], ["Sycara", "Katia", ""], ["Iyer", "Rahul", ""]]}, {"id": "1809.06098", "submitter": "Alberto Maria Metelli", "authors": "Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello\n  Restelli", "title": "Policy Optimization via Importance Sampling", "comments": null, "journal-ref": "32nd Conference on Neural Information Processing Systems (NIPS\n  2018), Montr\\'eal, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy optimization is an effective reinforcement learning approach to solve\ncontinuous control tasks. Recent achievements have shown that alternating\nonline and offline optimization is a successful choice for efficient trajectory\nreuse. However, deciding when to stop optimizing and collect new trajectories\nis non-trivial, as it requires to account for the variance of the objective\nfunction estimate. In this paper, we propose a novel, model-free, policy search\nalgorithm, POIS, applicable in both action-based and parameter-based settings.\nWe first derive a high-confidence bound for importance sampling estimation;\nthen we define a surrogate objective function, which is optimized offline\nwhenever a new batch of trajectories is collected. Finally, the algorithm is\ntested on a selection of continuous control tasks, with both linear and deep\npolicies, and compared with state-of-the-art policy optimization methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:42:26 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 10:47:21 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Metelli", "Alberto Maria", ""], ["Papini", "Matteo", ""], ["Faccio", "Francesco", ""], ["Restelli", "Marcello", ""]]}, {"id": "1809.06101", "submitter": "Mikael Mieskolainen", "authors": "Mikael Mieskolainen", "title": "DeepEfficiency - optimal efficiency inversion in higher dimensions at\n  the LHC", "comments": "2 pages, double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an hep-ex hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new high dimensional algorithm for efficiency corrected,\nmaximally Monte Carlo event generator independent fiducial measurements at the\nLHC and beyond. The approach is driven probabilistically using a Deep Neural\nNetwork on an event-by-event basis, trained using detector simulation and even\nonly pure phase space distributed events. This approach gives also a glimpse\ninto the future of high energy physics, where experiments publish new type of\nmeasurements in a radically multidimensional way.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:51:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Mieskolainen", "Mikael", ""]]}, {"id": "1809.06121", "submitter": "Amir Abdi", "authors": "Amir H. Abdi, Pramit Saha, Praneeth Srungarapu, Sidney Fels", "title": "Muscle Excitation Estimation in Biomechanical Simulation Using NAF\n  Reinforcement Learning", "comments": "9 pages, 3 figures. Computational Biomechanics for Medicine. MICCAI\n  2019. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-15923-8_11", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor control is a set of time-varying muscle excitations which generate\ndesired motions for a biomechanical system. Muscle excitations cannot be\ndirectly measured from live subjects. An alternative approach is to estimate\nmuscle activations using inverse motion-driven simulation. In this article, we\npropose a deep reinforcement learning method to estimate the muscle excitations\nin simulated biomechanical systems. Here, we introduce a custom-made reward\nfunction which incentivizes faster point-to-point tracking of target motion.\nMoreover, we deploy two new techniques, namely, episode-based hard update and\ndual buffer experience replay, to avoid feedback training loops. The proposed\nmethod is tested in four simulated 2D and 3D environments with 6 to 24 axial\nmuscles. The results show that the models were able to learn muscle excitations\nfor given motions after nearly 100,000 simulated steps. Moreover, the root mean\nsquare error in point-to-point reaching of the target across experiments was\nless than 1% of the length of the domain of motion. Our reinforcement learning\nmethod is far from the conventional dynamic approaches as the muscle control is\nderived functionally by a set of distributed neurons. This can open paths for\nneural activity interpretation of this phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 10:38:20 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 21:12:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Abdi", "Amir H.", ""], ["Saha", "Pramit", ""], ["Srungarapu", "Praneeth", ""], ["Fels", "Sidney", ""]]}, {"id": "1809.06124", "submitter": "Ioannis Sarafis", "authors": "Ioannis Sarafis, Christos Diou, Anastasios Delopoulos", "title": "Span error bound for weighted SVM with applications in hyperparameter\n  selection", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted SVM (or fuzzy SVM) is the most widely used SVM variant owning its\neffectiveness to the use of instance weights. Proper selection of the instance\nweights can lead to increased generalization performance. In this work, we\nextend the span error bound theory to weighted SVM and we introduce effective\nhyperparameter selection methods for the weighted SVM algorithm. The\nsignificance of the presented work is that enables the application of span\nbound and span-rule with weighted SVM. The span bound is an upper bound of the\nleave-one-out error that can be calculated using a single trained SVM model.\nThis is important since leave-one-out error is an almost unbiased estimator of\nthe test error. Similarly, the span-rule gives the actual value of the\nleave-one-out error. Thus, one can apply span bound and span-rule as\ncomputationally lightweight alternatives of leave-one-out procedure for\nhyperparameter selection. The main theoretical contributions are: (a) we prove\nthe necessary and sufficient condition for the existence of the span of a\nsupport vector in weighted SVM; and (b) we prove the extension of span bound\nand span-rule to weighted SVM. We experimentally evaluate the span bound and\nthe span-rule for hyperparameter selection and we compare them with other\nmethods that are applicable to weighted SVM: the $K$-fold cross-validation and\nthe ${\\xi}-{\\alpha}$ bound. Experiments on 14 benchmark data sets and data sets\nwith importance scores for the training instances show that: (a) the condition\nfor the existence of span in weighted SVM is satisfied almost always; (b) the\nspan-rule is the most effective method for weighted SVM hyperparameter\nselection; (c) the span-rule is the best predictor of the test error in the\nmean square error sense; and (d) the span-rule is efficient and, for certain\nproblems, it can be calculated faster than $K$-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 10:50:25 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sarafis", "Ioannis", ""], ["Diou", "Christos", ""], ["Delopoulos", "Anastasios", ""]]}, {"id": "1809.06127", "submitter": "Dimos Makris", "authors": "Dimos Makris, Maximos Kaliakatsos-Papakostas, Katia Lida Kermanidis", "title": "DeepDrum: An Adaptive Conditional Neural Network", "comments": "2018 Joint Workshop on Machine Learning for Music. The Federated\n  Artificial Intelligence Meeting (FAIM), a joint workshop program of ICML,\n  IJCAI/ECAI, and AAMAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering music as a sequence of events with multiple complex dependencies,\nthe Long Short-Term Memory (LSTM) architecture has proven very efficient in\nlearning and reproducing musical styles. However, the generation of rhythms\nrequires additional information regarding musical structure and accompanying\ninstruments. In this paper we present DeepDrum, an adaptive Neural Network\ncapable of generating drum rhythms under constraints imposed by Feed-Forward\n(Conditional) Layers which contain musical parameters along with given\ninstrumentation information (e.g. bass and guitar notes). Results on generated\ndrum sequences are presented indicating that DeepDrum is effective in producing\nrhythms that resemble the learned style, while at the same time conforming to\ngiven constraints that were unknown during the training process.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:08:52 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 21:32:38 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Makris", "Dimos", ""], ["Kaliakatsos-Papakostas", "Maximos", ""], ["Kermanidis", "Katia Lida", ""]]}, {"id": "1809.06146", "submitter": "Manfred Eppe", "authors": "Manfred Eppe, Sven Magg and Stefan Wermter", "title": "Curriculum goal masking for continuous deep reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has recently gained a focus on problems where\npolicy or value functions are independent of goals. Evidence exists that the\nsampling of goals has a strong effect on the learning performance, but there is\na lack of general mechanisms that focus on optimizing the goal sampling\nprocess. In this work, we present a simple and general goal masking method that\nalso allows us to estimate a goal's difficulty level and thus realize a\ncurriculum learning approach for deep RL. Our results indicate that focusing on\ngoals with a medium difficulty level is appropriate for deep deterministic\npolicy gradient (DDPG) methods, while an \"aim for the stars and reach the\nmoon-strategy\", where hard goals are sampled much more often than simple goals,\nleads to the best learning performance in cases where DDPG is combined with for\nhindsight experience replay (HER). We demonstrate that the approach\nsignificantly outperforms standard goal sampling for different robotic object\nmanipulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:01:02 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 12:02:59 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Eppe", "Manfred", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1809.06166", "submitter": "Nicholas Choma", "authors": "Nicholas Choma, Federico Monti, Lisa Gerhardt, Tomasz Palczewski,\n  Zahra Ronaghi, Prabhat, Wahid Bhimji, Michael M. Bronstein, Spencer R. Klein,\n  Joan Bruna", "title": "Graph Neural Networks for IceCube Signal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks involving the analysis of geometric (graph- and manifold-structured)\ndata have recently gained prominence in the machine learning community, giving\nbirth to a rapidly developing field of geometric deep learning. In this work,\nwe leverage graph neural networks to improve signal detection in the IceCube\nneutrino observatory. The IceCube detector array is modeled as a graph, where\nvertices are sensors and edges are a learned function of the sensors' spatial\ncoordinates. As only a subset of IceCube's sensors is active during a given\nobservation, we note the adaptive nature of our GNN, wherein computation is\nrestricted to the input signal support. We demonstrate the effectiveness of our\nGNN architecture on a task classifying IceCube events, where it outperforms\nboth a traditional physics-based method as well as classical 3D convolution\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:45:01 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Choma", "Nicholas", ""], ["Monti", "Federico", ""], ["Gerhardt", "Lisa", ""], ["Palczewski", "Tomasz", ""], ["Ronaghi", "Zahra", ""], ["Prabhat", "", ""], ["Bhimji", "Wahid", ""], ["Bronstein", "Michael M.", ""], ["Klein", "Spencer R.", ""], ["Bruna", "Joan", ""]]}, {"id": "1809.06186", "submitter": "Md. Abu Bakr Siddique", "authors": "Mohammad Mahmudur Rahman Khan, Rezoana Bente Arif, Md. Abu Bakr\n  Siddique, Mahjabin Rahman Oishe", "title": "Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN,\n  ENN Algorithms on Eleven Different Datasets from UCI Machine Learning\n  Repository", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628041", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning qualifies computers to assimilate with data, without being\nsolely programmed [1, 2]. Machine learning can be classified as supervised and\nunsupervised learning. In supervised learning, computers learn an objective\nthat portrays an input to an output hinged on training input-output pairs [3].\nMost efficient and widely used supervised learning algorithms are K-Nearest\nNeighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor\n(LMNN), and Extended Nearest Neighbor (ENN). The main contribution of this\npaper is to implement these elegant learning algorithms on eleven different\ndatasets from the UCI machine learning repository to observe the variation of\naccuracies for each of the algorithms on all datasets. Analyzing the accuracy\nof the algorithms will give us a brief idea about the relationship of the\nmachine learning algorithms and the data dimensionality. All the algorithms are\ndeveloped in Matlab. Upon such accuracy observation, the comparison can be\nbuilt among KNN, SVM, LMNN, and ENN regarding their performances on each\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:27:43 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:11:41 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:57:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Khan", "Mohammad Mahmudur Rahman", ""], ["Arif", "Rezoana Bente", ""], ["Siddique", "Md. Abu Bakr", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06187", "submitter": "Md. Abu Bakr Siddique", "authors": "Rezoana Bente Arif, Md. Abu Bakr Siddique, Mohammad Mahmudur Rahman\n  Khan, Mahjabin Rahman Oishe", "title": "Study and Observation of the Variations of Accuracies for Handwritten\n  Digits Recognition with Various Hidden Layers and Epochs using Convolutional\n  Neural Network", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628078", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, deep learning can be employed to a wide ranges of fields including\nmedicine, engineering, etc. In deep learning, Convolutional Neural Network\n(CNN) is extensively used in the pattern and sequence recognition, video\nanalysis, natural language processing, spam detection, topic categorization,\nregression analysis, speech recognition, image classification, object\ndetection, segmentation, face recognition, robotics, and control. The benefits\nassociated with its near human level accuracies in large applications lead to\nthe growing acceptance of CNN in recent years. The primary contribution of this\npaper is to analyze the impact of the pattern of the hidden layers of a CNN\nover the overall performance of the network. To demonstrate this influence, we\napplied neural network with different layers on the Modified National Institute\nof Standards and Technology (MNIST) dataset. Also, is to observe the variations\nof accuracies of the network for various numbers of hidden layers and epochs\nand to make comparison and contrast among them. The system is trained utilizing\nstochastic gradient and backpropagation algorithm and tested with feedforward\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:28:02 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:08:57 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:56:11 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Arif", "Rezoana Bente", ""], ["Siddique", "Md. Abu Bakr", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06189", "submitter": "Md. Abu Bakr Siddique", "authors": "Mohammad Mahmudur Rahman Khan, Md. Abu Bakr Siddique, Rezoana Bente\n  Arif, Mahjabin Rahman Oishe", "title": "ADBSCAN: Adaptive Density-Based Spatial Clustering of Applications with\n  Noise for Identifying Clusters with Varying Densities", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628138", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Density-based spatial clustering of applications with noise (DBSCAN) is a\ndata clustering algorithm which has the high-performance rate for dataset where\nclusters have the constant density of data points. One of the significant\nattributes of this algorithm is noise cancellation. However, DBSCAN\ndemonstrates reduced performances for clusters with different densities.\nTherefore, in this paper, an adaptive DBSCAN is proposed which can work\nsignificantly well for identifying clusters with varying densities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:28:35 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:03:42 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:51:05 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Khan", "Mohammad Mahmudur Rahman", ""], ["Siddique", "Md. Abu Bakr", ""], ["Arif", "Rezoana Bente", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06213", "submitter": "Zhen Cui", "authors": "Zhen Cui, Chunyan Xu, Wenming Zheng and Jian Yang", "title": "Context-Dependent Diffusion Network for Visual Relationship Detection", "comments": "8 pages, 3 figures, 2018 ACM Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240668", "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection can bridge the gap between computer vision and\nnatural language for scene understanding of images. Different from pure object\nrecognition tasks, the relation triplets of subject-predicate-object lie on an\nextreme diversity space, such as \\textit{person-behind-person} and\n\\textit{car-behind-building}, while suffering from the problem of combinatorial\nexplosion. In this paper, we propose a context-dependent diffusion network\n(CDDN) framework to deal with visual relationship detection. To capture the\ninteractions of different object instances, two types of graphs, word semantic\ngraph and visual scene graph, are constructed to encode global context\ninterdependency. The semantic graph is built through language priors to model\nsemantic correlations across objects, whilst the visual scene graph defines the\nconnections of scene objects so as to utilize the surrounding scene\ninformation. For the graph-structured data, we design a diffusion network to\nadaptively aggregate information from contexts, which can effectively learn\nlatent representations of visual relationships and well cater to visual\nrelationship detection in view of its isomorphic invariance to graphs.\nExperiments on two widely-used datasets demonstrate that our proposed method is\nmore effective and achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:13:45 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Zheng", "Wenming", ""], ["Yang", "Jian", ""]]}, {"id": "1809.06219", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu", "title": "Ensemble learning with 3D convolutional neural networks for\n  connectome-based prediction", "comments": "45 pages, 9 figures, 4 supplementary figures (To appear in\n  Neuroimage)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The specificty and sensitivity of resting state functional MRI (rs-fMRI)\nmeasurements depend on pre-processing choices, such as the parcellation scheme\nused to define regions of interest (ROIs). In this study, we critically\nevaluate the effect of brain parcellations on machine learning models applied\nto rs-fMRI data. Our experiments reveal a remarkable trend: On average, models\nwith stochastic parcellations consistently perform as well as models with\nwidely used atlases at the same spatial scale. We thus propose an ensemble\nlearning strategy to combine the predictions from models trained on\nconnectivity data extracted using different (e.g., stochastic) parcellations.\nWe further present an implementation of our ensemble learning strategy with a\nnovel 3D Convolutional Neural Network (CNN) approach. The proposed CNN approach\ntakes advantage of the full-resolution 3D spatial structure of rs-fMRI data and\nfits non-linear predictive models. Our ensemble CNN framework overcomes the\nlimitations of traditional machine learning models for connectomes that often\nrely on region-based summary statistics and/or linear models. We showcase our\napproach on a classification (autism patients versus healthy controls) and a\nregression problem (prediction of subject's age), and report promising results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:55:10 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 21:55:28 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1809.06222", "submitter": "Christoph Baur", "authors": "Salome Kazeminia, Christoph Baur, Arjan Kuijper, Bram van Ginneken,\n  Nassir Navab, Shadi Albarqouni, Anirban Mukhopadhyay", "title": "GANs for Medical Image Analysis", "comments": "Salome Kazeminia and Christoph Baur contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) and their extensions have carved open\nmany exciting ways to tackle well known and challenging medical image analysis\nproblems such as medical image de-noising, reconstruction, segmentation, data\nsimulation, detection or classification. Furthermore, their ability to\nsynthesize images at unprecedented levels of realism also gives hope that the\nchronic scarcity of labeled data in the medical field can be resolved with the\nhelp of these generative models. In this review paper, a broad overview of\nrecent literature on GANs for medical applications is given, the shortcomings\nand opportunities of the proposed methods are thoroughly discussed and\npotential future work is elaborated. We review the most relevant papers\npublished until the submission date. For quick access, important details such\nas the underlying method, datasets and performance are tabulated. An\ninteractive visualization which categorizes all papers to keep the review\nalive, is available at\nhttp://livingreview.in.tum.de/GANs_for_Medical_Applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 21:38:29 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 23:34:44 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 06:31:07 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kazeminia", "Salome", ""], ["Baur", "Christoph", ""], ["Kuijper", "Arjan", ""], ["van Ginneken", "Bram", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "1809.06227", "submitter": "Tszhang Guo", "authors": "Tszhang Guo, Shiyu Chang, Mo Yu and Kun Bai", "title": "Improving Reinforcement Learning Based Image Captioning with Natural\n  Language Prior", "comments": "8 pages, 5 figures, EMNLP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Reinforcement Learning (RL) approaches have demonstrated advanced\nperformance in image captioning by directly optimizing the metric used for\ntesting. However, this shaped reward introduces learning biases, which reduces\nthe readability of generated text. In addition, the large sample space makes\ntraining unstable and slow. To alleviate these issues, we propose a simple\ncoherent solution that constrains the action space using an n-gram language\nprior. Quantitative and qualitative evaluations on benchmarks show that RL with\nthe simple add-on module performs favorably against its counterpart in terms of\nboth readability and speed of convergence. Human evaluation results show that\nour model is more human readable and graceful. The implementation will become\npublicly available upon the acceptance of the paper.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:21:56 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Guo", "Tszhang", ""], ["Chang", "Shiyu", ""], ["Yu", "Mo", ""], ["Bai", "Kun", ""]]}, {"id": "1809.06247", "submitter": "Mai Nguyen", "authors": "Ehab Abdelmaguid, Jolene Huang, Sanjay Kenchareddy, Disha Singla,\n  Laura Wilke, Mai H. Nguyen, Ilkay Altintas", "title": "Left Ventricle Segmentation and Volume Estimation on Cardiac MRI using\n  Deep Learning", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the United States, heart disease is the leading cause of death for both\nmen and women, accounting for 610,000 deaths each year [1]. Physicians use\nMagnetic Resonance Imaging (MRI) scans to take images of the heart in order to\nnon-invasively estimate its structural and functional parameters for\ncardiovascular diagnosis and disease management. The end-systolic volume (ESV)\nand end-diastolic volume (EDV) of the left ventricle (LV), and the ejection\nfraction (EF) are indicators of heart disease. These measures can be derived\nfrom the segmented contours of the LV; thus, consistent and accurate\nsegmentation of the LV from MRI images are critical to the accuracy of the ESV,\nEDV, and EF, and to non-invasive cardiac disease detection.\n  In this work, various image preprocessing techniques, model configurations\nusing the U-Net deep learning architecture, postprocessing methods, and\napproaches for volume estimation are investigated. An end-to-end analytics\npipeline with multiple stages is provided for automated LV segmentation and\nvolume estimation. First, image data are reformatted and processed from DICOM\nand NIfTI formats to raw images in array format. Secondly, raw images are\nprocessed with multiple image preprocessing methods and cropped to include only\nthe Region of Interest (ROI). Thirdly, preprocessed images are segmented using\nU-Net models. Lastly, post processing of segmented images to remove extra\ncontours along with intelligent slice and frame selection are applied, followed\nby calculation of the ESV, EDV, and EF. This analytics pipeline is implemented\nand runs on a distributed computing environment with a GPU cluster at the San\nDiego Supercomputer Center at UCSD.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 06:40:07 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:58:27 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Abdelmaguid", "Ehab", ""], ["Huang", "Jolene", ""], ["Kenchareddy", "Sanjay", ""], ["Singla", "Disha", ""], ["Wilke", "Laura", ""], ["Nguyen", "Mai H.", ""], ["Altintas", "Ilkay", ""]]}, {"id": "1809.06253", "submitter": "Leonardo Gutierrez", "authors": "Leonardo Gutierrez Gomez, Jean-Charles Delvenne", "title": "Multi-hop assortativities for networks classification", "comments": "20 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1705.10817", "journal-ref": null, "doi": "10.1093/comnet/cny034", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:33:35 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:28:51 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Gomez", "Leonardo Gutierrez", ""], ["Delvenne", "Jean-Charles", ""]]}, {"id": "1809.06304", "submitter": "Andre Manoel", "authors": "Andre Manoel, Florent Krzakala, Ga\\\"el Varoquaux, Bertrand Thirion,\n  Lenka Zdeborov\\'a", "title": "Approximate message-passing for convex optimization with non-separable\n  penalties", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an iterative optimization scheme for convex objectives\nconsisting of a linear loss and a non-separable penalty, based on the\nexpectation-consistent approximation and the vector approximate message-passing\n(VAMP) algorithm. Specifically, the penalties we approach are convex on a\nlinear transformation of the variable to be determined, a notable example being\ntotal variation (TV). We describe the connection between message-passing\nalgorithms -- typically used for approximate inference -- and proximal methods\nfor optimization, and show that our scheme is, as VAMP, similar in nature to\nthe Peaceman-Rachford splitting, with the important difference that stepsizes\nare set adaptively. Finally, we benchmark the performance of our VAMP-like\niteration in problems where TV penalties are useful, namely classification in\ntask fMRI and reconstruction in tomography, and show faster convergence than\nthat of state-of-the-art approaches such as FISTA and ADMM in most settings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 16:14:55 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Manoel", "Andre", ""], ["Krzakala", "Florent", ""], ["Varoquaux", "Ga\u00ebl", ""], ["Thirion", "Bertrand", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1809.06334", "submitter": "Edward Pyzer-Knapp", "authors": "Clyde Fare, Lukas Turcani, Edward O. Pyzer-Knapp", "title": "Powerful, transferable representations for molecules through intelligent\n  task selection in deep multitask networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical representations derived from deep learning are emerging as a\npowerful tool in areas such as drug discovery and materials innovation.\nCurrently, this methodology has three major limitations - the cost of\nrepresentation generation, risk of inherited bias, and the requirement for\nlarge amounts of data. We propose the use of multi-task learning in tandem with\ntransfer learning to address these limitations directly. In order to avoid\nintroducing unknown bias into multi-task learning through the task selection\nitself, we calculate task similarity through pairwise task affinity, and use\nthis measure to programmatically select tasks. We test this methodology on\nseveral real-world data sets to demonstrate its potential for execution in\ncomplex and low-data environments. Finally, we utilise the task similarity to\nfurther probe the expressiveness of the learned representation through a\ncomparison to a commonly used cheminformatics fingerprint, and show that the\ndeep representation is able to capture more expressive task-based information.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 17:06:06 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Fare", "Clyde", ""], ["Turcani", "Lukas", ""], ["Pyzer-Knapp", "Edward O.", ""]]}, {"id": "1809.06364", "submitter": "Eli Friedman", "authors": "Eli Friedman and Fred Fontaine", "title": "Generalizing Across Multi-Objective Reward Functions in Deep\n  Reinforcement Learning", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many reinforcement-learning researchers treat the reward function as a part\nof the environment, meaning that the agent can only know the reward of a state\nif it encounters that state in a trial run. However, we argue that this is an\nunnecessary limitation and instead, the reward function should be provided to\nthe learning algorithm. The advantage is that the algorithm can then use the\nreward function to check the reward for states that the agent hasn't even\nencountered yet. In addition, the algorithm can simultaneously learn policies\nfor multiple reward functions. For each state, the algorithm would calculate\nthe reward using each of the reward functions and add the rewards to its\nexperience replay dataset. The Hindsight Experience Replay algorithm developed\nby Andrychowicz et al. (2017) does just this, and learns to generalize across a\ndistribution of sparse, goal-based rewards. We extend this algorithm to\nlinearly-weighted, multi-objective rewards and learn a single policy that can\ngeneralize across all linear combinations of the multi-objective reward.\nWhereas other multi-objective algorithms teach the Q-function to generalize\nacross the reward weights, our algorithm enables the policy to generalize, and\ncan thus be used with continuous actions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 17:59:13 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Friedman", "Eli", ""], ["Fontaine", "Fred", ""]]}, {"id": "1809.06367", "submitter": "Eugene Belilovsky", "authors": "Edouard Oyallon (CVN, GALEN), Sergey Zagoruyko (ENPC, LIGM), Gabriel\n  Huang (DIRO, MILA), Nikos Komodakis (ENPC, CSD-UOC, LIGM), Simon\n  Lacoste-Julien (DIRO, MILA), Matthew Blaschko (ESAT), Eugene Belilovsky\n  (DIRO, MILA)", "title": "Scattering Networks for Hybrid Representation Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.08961", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Institute of Electrical and Electronics Engineers, 2018, pp.11", "doi": "10.1109/TPAMI.2018.2855738", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:27:40 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Oyallon", "Edouard", "", "CVN, GALEN"], ["Zagoruyko", "Sergey", "", "ENPC, LIGM"], ["Huang", "Gabriel", "", "DIRO, MILA"], ["Komodakis", "Nikos", "", "ENPC, CSD-UOC, LIGM"], ["Lacoste-Julien", "Simon", "", "DIRO, MILA"], ["Blaschko", "Matthew", "", "ESAT"], ["Belilovsky", "Eugene", "", "DIRO, MILA"]]}, {"id": "1809.06401", "submitter": "Hyung-Jin Yoon", "authors": "Hyung-Jin Yoon, Donghwan Lee, and Naira Hovakimyan", "title": "Hidden Markov Model Estimation-Based Q-learning for Partially Observable\n  Markov Decision Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to study an on-line Hidden Markov model (HMM)\nestimation-based Q-learning algorithm for partially observable Markov decision\nprocess (POMDP) on finite state and action sets. When the full state\nobservation is available, Q-learning finds the optimal action-value function\ngiven the current action (Q function). However, Q-learning can perform poorly\nwhen the full state observation is not available. In this paper, we formulate\nthe POMDP estimation into a HMM estimation problem and propose a recursive\nalgorithm to estimate both the POMDP parameter and Q function concurrently.\nAlso, we show that the POMDP estimation converges to a set of stationary points\nfor the maximum likelihood estimate, and the Q function estimation converges to\na fixed point that satisfies the Bellman optimality equation weighted on the\ninvariant distribution of the state belief determined by the HMM estimation\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:40:48 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 14:35:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Yoon", "Hyung-Jin", ""], ["Lee", "Donghwan", ""], ["Hovakimyan", "Naira", ""]]}, {"id": "1809.06404", "submitter": "Ahmed Qureshi", "authors": "Ahmed H. Qureshi, Byron Boots and Michael C. Yip", "title": "Adversarial Imitation via Variational Inverse Reinforcement Learning", "comments": "Paper published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of learning the reward and policy from expert examples\nunder unknown dynamics. Our proposed method builds on the framework of\ngenerative adversarial networks and introduces the empowerment-regularized\nmaximum-entropy inverse reinforcement learning to learn near-optimal rewards\nand policies. Empowerment-based regularization prevents the policy from\noverfitting to expert demonstrations, which advantageously leads to more\ngeneralized behaviors that result in learning near-optimal rewards. Our method\nsimultaneously learns empowerment through variational information maximization\nalong with the reward and policy under the adversarial learning formulation. We\nevaluate our approach on various high-dimensional complex control tasks. We\nalso test our learned rewards in challenging transfer learning problems where\ntraining and testing environments are made to be different from each other in\nterms of dynamics or structure. The results show that our proposed method not\nonly learns near-optimal rewards and policies that are matching expert behavior\nbut also performs significantly better than state-of-the-art inverse\nreinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:47:47 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 19:27:41 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 23:32:23 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Qureshi", "Ahmed H.", ""], ["Boots", "Byron", ""], ["Yip", "Michael C.", ""]]}, {"id": "1809.06432", "submitter": "Pedro Mercado", "authors": "Pedro Mercado, Jessica Bosch, Martin Stoll", "title": "Node Classification for Signed Social Networks Using Diffuse Interface\n  Methods", "comments": "Accepted at ECML-PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signed networks contain both positive and negative kinds of interactions like\nfriendship and enmity. The task of node classification in non-signed graphs has\nproven to be beneficial in many real world applications, yet extensions to\nsigned networks remain largely unexplored. In this paper we introduce the first\nanalysis of node classification in signed social networks via diffuse interface\nmethods based on the Ginzburg-Landau functional together with different\nextensions of the graph Laplacian to signed networks. We show that blending the\ninformation from both positive and negative interactions leads to performance\nimprovement in real signed social networks, consistently outperforming the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 11:41:37 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 18:39:37 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Mercado", "Pedro", ""], ["Bosch", "Jessica", ""], ["Stoll", "Martin", ""]]}, {"id": "1809.06452", "submitter": "Luca Laurenti", "authors": "Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Andrea Patane", "title": "Robustness Guarantees for Bayesian Inference with Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference and Gaussian processes are widely used in applications\nranging from robotics and control to biological systems. Many of these\napplications are safety-critical and require a characterization of the\nuncertainty associated with the learning model and formal guarantees on its\npredictions. In this paper we define a robustness measure for Bayesian\ninference against input perturbations, given by the probability that, for a\ntest point and a compact set in the input space containing the test point, the\nprediction of the learning model will remain $\\delta-$close for all the points\nin the set, for $\\delta>0.$ Such measures can be used to provide formal\nguarantees for the absence of adversarial examples. By employing the theory of\nGaussian processes, we derive tight upper bounds on the resulting robustness by\nutilising the Borell-TIS inequality, and propose algorithms for their\ncomputation. We evaluate our techniques on two examples, a GP regression\nproblem and a fully-connected deep neural network, where we rely on weak\nconvergence to GPs to study adversarial examples on the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 21:32:26 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 22:00:04 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Cardelli", "Luca", ""], ["Kwiatkowska", "Marta", ""], ["Laurenti", "Luca", ""], ["Patane", "Andrea", ""]]}, {"id": "1809.06463", "submitter": "Eugene Wong", "authors": "Eugene Wong", "title": "Self Configuration in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first present a class of algorithms for training multi-level\nneural networks with a quadratic cost function one layer at a time starting\nfrom the input layer. The algorithm is based on the fact that for any layer to\nbe trained, the effect of a direct connection to an optimized linear output\nlayer can be computed without the connection being made. Thus, starting from\nthe input layer, we can train each layer in succession in isolation from the\nother layers. Once trained, the weights are kept fixed and the outputs of the\ntrained layer then serve as the inputs to the next layer to be trained. The\nresult is a very fast algorithm. The simplicity of this training arrangement\nallows the activation function and step size in weight adjustment to be\nadaptive and self-adjusting. Furthermore, the stability of the training process\nallows relatively large steps to be taken and thereby achieving in even greater\nspeeds. Finally, in our context configuring the network means determining the\nnumber of outputs for each layer. By decomposing the overall cost function into\nseparate components related to approximation and estimation, we obtain an\noptimization formula for determining the number of outputs for each layer. With\nthe ability to self-configure and set parameters, we now have more than a fast\ntraining algorithm, but the ability to build automatically a fully trained deep\nneural network starting with nothing more than data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 22:29:28 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Wong", "Eugene", ""]]}, {"id": "1809.06473", "submitter": "Sahin Geyik", "authors": "Rohan Ramanath, Hakan Inan, Gungor Polatkan, Bo Hu, Qi Guo, Cagri\n  Ozcaglar, Xianren Wu, Krishnaram Kenthapadi, Sahin Cem Geyik", "title": "Towards Deep and Representation Learning for Talent Search at LinkedIn", "comments": "This paper has been accepted for publication in ACM CIKM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Talent search and recommendation systems at LinkedIn strive to match the\npotential candidates to the hiring needs of a recruiter or a hiring manager\nexpressed in terms of a search query or a job posting. Recent work in this\ndomain has mainly focused on linear models, which do not take complex\nrelationships between features into account, as well as ensemble tree models,\nwhich introduce non-linearity but are still insufficient for exploring all the\npotential feature interactions, and strictly separate feature generation from\nmodeling. In this paper, we present the results of our application of deep and\nrepresentation learning models on LinkedIn Recruiter. Our key contributions\ninclude: (i) Learning semantic representations of sparse entities within the\ntalent search domain, such as recruiter ids, candidate ids, and skill entity\nids, for which we utilize neural network models that take advantage of LinkedIn\nEconomic Graph, and (ii) Deep models for learning recruiter engagement and\ncandidate response in talent search applications. We also explore learning to\nrank approaches applied to deep models, and show the benefits for the talent\nsearch use case. Finally, we present offline and online evaluation results for\nLinkedIn talent search and recommendation systems, and discuss potential\nchallenges along the path to a fully deep model architecture. The challenges\nand approaches discussed generalize to any multi-faceted search engine.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:11:50 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Ramanath", "Rohan", ""], ["Inan", "Hakan", ""], ["Polatkan", "Gungor", ""], ["Hu", "Bo", ""], ["Guo", "Qi", ""], ["Ozcaglar", "Cagri", ""], ["Wu", "Xianren", ""], ["Kenthapadi", "Krishnaram", ""], ["Geyik", "Sahin Cem", ""]]}, {"id": "1809.06474", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Saeed Ghadimi", "title": "Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints,\n  High-Dimensionality and Saddle-Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze zeroth-order stochastic approximation\nalgorithms for nonconvex and convex optimization, with a focus on addressing\nconstrained optimization, high-dimensional setting and saddle-point avoiding.\nTo handle constrained optimization, we first propose generalizations of the\nconditional gradient algorithm achieving rates similar to the standard\nstochastic gradient algorithm using only zeroth-order information. To\nfacilitate zeroth-order optimization in high-dimensions, we explore the\nadvantages of structural sparsity assumptions. Specifically, (i) we highlight\nan implicit regularization phenomenon where the standard stochastic gradient\nalgorithm with zeroth-order information adapts to the sparsity of the problem\nat hand by just varying the step-size and (ii) propose a truncated stochastic\ngradient algorithm with zeroth-order information, whose rate of convergence\ndepends only poly-logarithmically on the dimensionality. We next focus on\navoiding saddle-points in non-convex setting. Towards that, we interpret the\nGaussian smoothing technique for estimating gradient based on zeroth-order\ninformation as an instantiation of first-order Stein's identity. Based on this,\nwe provide a novel linear-(in dimension) time estimator of the Hessian matrix\nof a function using only zeroth-order information, which is based on\nsecond-order Stein's identity. We then provide an algorithm for avoiding\nsaddle-points, which is based on a zeroth-order cubic regularization Newton's\nmethod and discuss its convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:30:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 02:53:46 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""]]}, {"id": "1809.06477", "submitter": "Shubhomoy Das", "authors": "Shubhomoy Das, Md Rakibul Islam, Nitthilan Kannappan Jayakodi,\n  Janardhan Rao Doppa", "title": "Active Anomaly Detection via Ensembles", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In critical applications of anomaly detection including computer security and\nfraud prevention, the anomaly detector must be configurable by the analyst to\nminimize the effort on false positives. One important way to configure the\nanomaly detector is by providing true labels for a few instances. We study the\nproblem of label-efficient active learning to automatically tune anomaly\ndetection ensembles and make four main contributions. First, we present an\nimportant insight into how anomaly detector ensembles are naturally suited for\nactive learning. This insight allows us to relate the greedy querying strategy\nto uncertainty sampling, with implications for label-efficiency. Second, we\npresent a novel formalism called compact description to describe the discovered\nanomalies and show that it can also be employed to improve the diversity of the\ninstances presented to the analyst without loss in the anomaly discovery rate.\nThird, we present a novel data drift detection algorithm that not only detects\nthe drift robustly, but also allows us to take corrective actions to adapt the\ndetector in a principled manner. Fourth, we present extensive experiments to\nevaluate our insights and algorithms in both batch and streaming settings. Our\nresults show that in addition to discovering significantly more anomalies than\nstate-of-the-art unsupervised baselines, our active learning algorithms under\nthe streaming-data setup are competitive with the batch setup.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:53:39 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Das", "Shubhomoy", ""], ["Islam", "Md Rakibul", ""], ["Jayakodi", "Nitthilan Kannappan", ""], ["Doppa", "Janardhan Rao", ""]]}, {"id": "1809.06498", "submitter": "Deqiang Li", "authors": "Deqiang Li and Ramesh Baral and Tao Li and Han Wang and Qianmu Li and\n  Shouhuai Xu", "title": "HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural\n  Networks against Adversarial Malware Samples", "comments": "13 pages (included references), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial machine learning in the context of image processing and related\napplications has received a large amount of attention. However, adversarial\nmachine learning, especially adversarial deep learning, in the context of\nmalware detection has received much less attention despite its apparent\nimportance. In this paper, we present a framework for enhancing the robustness\nof Deep Neural Networks (DNNs) against adversarial malware samples, dubbed\nHashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is\nto use hash functions with a certain locality-preserving property to transform\nsamples to enhance the robustness of DNNs in malware classification. The\nframework further uses a Denoising Auto-Encoder (DAE) regularizer to\nreconstruct the hash representations of samples, making the resulting DNN\nclassifiers capable of attaining the locality information in the latent space.\nWe experiment with two concrete instantiations of the HashTran-DNN framework to\nclassify Android malware. Experimental results show that four known attacks can\nrender standard DNNs useless in classifying Android malware, that known\ndefenses can at most defend three of the four attacks, and that HashTran-DNN\ncan effectively defend against all of the four attacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 01:39:04 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Li", "Deqiang", ""], ["Baral", "Ramesh", ""], ["Li", "Tao", ""], ["Wang", "Han", ""], ["Li", "Qianmu", ""], ["Xu", "Shouhuai", ""]]}, {"id": "1809.06514", "submitter": "Berk Ustun", "authors": "Berk Ustun, Alexander Spangher, Yang Liu", "title": "Actionable Recourse in Linear Classification", "comments": "Extended version. ACM Conference on Fairness, Accountability and\n  Transparency [FAT2019]", "journal-ref": null, "doi": "10.1145/3287560.3287566", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are increasingly used to automate decisions that\naffect humans - deciding who should receive a loan, a job interview, or a\nsocial service. In such applications, a person should have the ability to\nchange the decision of a model. When a person is denied a loan by a credit\nscore, for example, they should be able to alter its input variables in a way\nthat guarantees approval. Otherwise, they will be denied the loan as long as\nthe model is deployed. More importantly, they will lack the ability to\ninfluence a decision that affects their livelihood.\n  In this paper, we frame these issues in terms of recourse, which we define as\nthe ability of a person to change the decision of a model by altering\nactionable input variables (e.g., income vs. age or marital status). We present\ninteger programming tools to ensure recourse in linear classification problems\nwithout interfering in model development. We demonstrate how our tools can\ninform stakeholders through experiments on credit scoring problems. Our results\nshow that recourse can be significantly affected by standard practices in model\ndevelopment, and motivate the need to evaluate recourse in practice.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 03:08:02 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 01:32:32 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ustun", "Berk", ""], ["Spangher", "Alexander", ""], ["Liu", "Yang", ""]]}, {"id": "1809.06517", "submitter": "Youhei Akimoto", "authors": "Kouhei Nishida, Hernan Aguirre, Shota Saito, Shinichi Shirakawa,\n  Youhei Akimoto", "title": "Parameterless Stochastic Natural Gradient Method for Discrete\n  Optimization and its Application to Hyper-Parameter Optimization for Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box discrete optimization (BBDO) appears in wide range of engineering\ntasks. Evolutionary or other BBDO approaches have been applied, aiming at\nautomating necessary tuning of system parameters, such as hyper parameter\ntuning of machine learning based systems when being installed for a specific\ntask. However, automation is often jeopardized by the need of strategy\nparameter tuning for BBDO algorithms. An expert with the domain knowledge must\nundergo time-consuming strategy parameter tuning. This paper proposes a\nparameterless BBDO algorithm based on information geometric optimization, a\nrecent framework for black box optimization using stochastic natural gradient.\nInspired by some theoretical implications, we develop an adaptation mechanism\nfor strategy parameters of the stochastic natural gradient method for discrete\nsearch domains. The proposed algorithm is evaluated on commonly used test\nproblems. It is further extended to two examples of simultaneous optimization\nof the hyper parameters and the connection weights of deep learning models,\nleading to a faster optimization than the existing approaches without any\neffort of parameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 03:27:49 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Nishida", "Kouhei", ""], ["Aguirre", "Hernan", ""], ["Saito", "Shota", ""], ["Shirakawa", "Shinichi", ""], ["Akimoto", "Youhei", ""]]}, {"id": "1809.06546", "submitter": "Jian Liang", "authors": "Jian Liang, Ziqi Liu, Jiayu Zhou, Xiaoqian Jiang, Changshui Zhang, Fei\n  Wang", "title": "Model-Protected Multi-Task Learning", "comments": "Supplemental materials are attached at the end of the main paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) refers to the paradigm of learning multiple related\ntasks together. In contrast, in single-task learning (STL) each individual task\nis learned independently. MTL often leads to better trained models because they\ncan leverage the commonalities among related tasks. However, because MTL\nalgorithms can ``leak\" information from different models across different\ntasks, MTL poses a potential security risk. Specifically, an adversary may\nparticipate in the MTL process through one task and thereby acquire the model\ninformation for another task. The previously proposed privacy-preserving MTL\nmethods protect data instances rather than models, and some of them may\nunderperform in comparison with STL methods. In this paper, we propose a\nprivacy-preserving MTL framework to prevent information from each model leaking\nto other models based on a perturbation of the covariance matrix of the model\nmatrix. We study two popular MTL approaches for instantiation, namely, learning\nthe low-rank and group-sparse patterns of the model matrix. Our algorithms can\nbe guaranteed not to underperform compared with STL methods. We build our\nmethods based upon tools for differential privacy, and privacy guarantees,\nutility bounds are provided, and heterogeneous privacy budgets are considered.\nThe experiments demonstrate that our algorithms outperform the baseline methods\nconstructed by existing privacy-preserving MTL methods on the proposed\nmodel-protection problem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 06:16:38 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 05:40:27 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 13:52:39 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Liang", "Jian", ""], ["Liu", "Ziqi", ""], ["Zhou", "Jiayu", ""], ["Jiang", "Xiaoqian", ""], ["Zhang", "Changshui", ""], ["Wang", "Fei", ""]]}, {"id": "1809.06569", "submitter": "Yu-Hsun Lin", "authors": "Yu-Hsun Lin, Chun-Nan Chou, Edward Y. Chang", "title": "MBS: Macroblock Scaling for CNN Model Reduction", "comments": "8 pages (Accepted by CVPR'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the macroblock scaling (MBS) algorithm, which can be\napplied to various CNN architectures to reduce their model size. MBS adaptively\nreduces each CNN macroblock depending on its information redundancy measured by\nour proposed effective flops. Empirical studies conducted with ImageNet and\nCIFAR-10 attest that MBS can reduce the model size of some already compact CNN\nmodels, e.g., MobileNetV2 (25.03% further reduction) and ShuffleNet (20.74%),\nand even ultra-deep ones such as ResNet-101 (51.67%) and ResNet-1202 (72.71%)\nwith negligible accuracy degradation. MBS also performs better reduction at a\nmuch lower cost than the state-of-the-art optimization-based methods do. MBS's\nsimplicity and efficiency, its flexibility to work with any CNN model, and its\nscalability to work with models of any depth make it an attractive choice for\nCNN model size reduction.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:40:46 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 09:33:59 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lin", "Yu-Hsun", ""], ["Chou", "Chun-Nan", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1809.06570", "submitter": "Izumi Karino", "authors": "Izumi Karino, Kazutoshi Tanaka, Ryuma Niiyama, and Yasuo Kuniyoshi", "title": "Switching Isotropic and Directional Exploration with Parameter Space\n  Noise in Deep Reinforcement Learning", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an exploration method for deep reinforcement learning\nbased on parameter space noise. Recent studies have experimentally shown that\nparameter space noise results in better exploration than the commonly used\naction space noise. Previous methods devised a way to update the diagonal\ncovariance matrix of a noise distribution and did not consider the direction of\nthe noise vector and its correlation. In addition, fast updates of the noise\ndistribution are required to facilitate policy learning. We propose a method\nthat deforms the noise distribution according to the accumulated returns and\nthe noises that have led to the returns. Moreover, this method switches\nisotropic exploration and directional exploration in parameter space with\nregard to obtained rewards. We validate our exploration strategy in the OpenAI\nGym continuous environments and modified environments with sparse rewards. The\nproposed method achieves results that are competitive with a previous method at\nbaseline tasks. Moreover, our approach exhibits better performance in sparse\nreward environments by exploration with the switching strategy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:43:00 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 04:33:12 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Karino", "Izumi", ""], ["Tanaka", "Kazutoshi", ""], ["Niiyama", "Ryuma", ""], ["Kuniyoshi", "Yasuo", ""]]}, {"id": "1809.06573", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Georg N\\\"uhrenberg, Hirotoshi Yasuoka", "title": "Runtime Monitoring Neuron Activation Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For using neural networks in safety critical domains, it is important to know\nif a decision made by a neural network is supported by prior similarities in\ntraining. We propose runtime neuron activation pattern monitoring - after the\nstandard training process, one creates a monitor by feeding the training data\nto the network again in order to store the neuron activation patterns in\nabstract form. In operation, a classification decision over an input is further\nsupplemented by examining if a pattern similar (measured by Hamming distance)\nto the generated pattern is contained in the monitor. If the monitor does not\ncontain any pattern similar to the generated pattern, it raises a warning that\nthe decision is not based on the training data. Our experiments show that, by\nadjusting the similarity-threshold for activation patterns, the monitors can\nreport a significant portion of misclassfications to be not supported by\ntraining with a small false-positive rate, when evaluated on a test set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:54:43 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 06:53:27 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["N\u00fchrenberg", "Georg", ""], ["Yasuoka", "Hirotoshi", ""]]}, {"id": "1809.06636", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer and Marta Pittavino", "title": "Comparison between Suitable Priors for Additive Bayesian Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive Bayesian networks are types of graphical models that extend the\nusual Bayesian generalized linear model to multiple dependent variables through\nthe factorisation of the joint probability distribution of the underlying\nvariables. When fitting an ABN model, the choice of the prior of the parameters\nis of crucial importance. If an inadequate prior - like a too weakly\ninformative one - is used, data separation and data sparsity lead to issues in\nthe model selection process. In this work a simulation study between two weakly\nand a strongly informative priors is presented. As weakly informative prior we\nuse a zero mean Gaussian prior with a large variance, currently implemented in\nthe R-package abn. The second prior belongs to the Student's t-distribution,\nspecifically designed for logistic regressions and, finally, the strongly\ninformative prior is again Gaussian with mean equal to true parameter value and\na small variance. We compare the impact of these priors on the accuracy of the\nlearned additive Bayesian network in function of different parameters. We\ncreate a simulation study to illustrate Lindley's paradox based on the prior\nchoice. We then conclude by highlighting the good performance of the\ninformative Student's t-prior and the limited impact of the Lindley's paradox.\nFinally, suggestions for further developments are provided.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 10:53:51 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""], ["Pittavino", "Marta", ""]]}, {"id": "1809.06674", "submitter": "Philipp-Immanuel Schneider", "authors": "Philipp-Immanuel Schneider, Xavier Garcia Santiago, Victor Soltwisch,\n  Martin Hammerschmidt, Sven Burger, Carsten Rockstuhl", "title": "Benchmarking five global optimization approaches for nano-optical shape\n  optimization and parameter reconstruction", "comments": "11 pages, 4 figures", "journal-ref": "ACS Photonics 6, 2726 (2019)", "doi": "10.1021/acsphotonics.9b00706", "report-no": null, "categories": "physics.comp-ph physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical optimization is an important tool in the field of computational\nphysics in general and in nano-optics in specific. It has attracted attention\nwith the increase in complexity of structures that can be realized with\nnowadays nano-fabrication technologies for which a rational design is no longer\nfeasible. Also, numerical resources are available to enable the computational\nphotonic material design and to identify structures that meet predefined\noptical properties for specific applications. However, the optimization\nobjective function is in general non-convex and its computation remains\nresource demanding such that the right choice for the optimization method is\ncrucial to obtain excellent results. Here, we benchmark five global\noptimization methods for three typical nano-optical optimization problems:\n\\removed{downhill simplex optimization, the limited-memory\nBroyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, particle swarm\noptimization, differential evolution, and Bayesian optimization}\n\\added{particle swarm optimization, differential evolution, and Bayesian\noptimization as well as multi-start versions of downhill simplex optimization\nand the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm}. In\nthe shown examples from the field of shape optimization and parameter\nreconstruction, Bayesian optimization, mainly known from machine learning\napplications, obtains significantly better results in a fraction of the run\ntimes of the other optimization methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:42:29 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 12:18:33 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 12:52:16 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Schneider", "Philipp-Immanuel", ""], ["Santiago", "Xavier Garcia", ""], ["Soltwisch", "Victor", ""], ["Hammerschmidt", "Martin", ""], ["Burger", "Sven", ""], ["Rockstuhl", "Carsten", ""]]}, {"id": "1809.06679", "submitter": "Thomas Klausch", "authors": "Thomas Klausch, Peter van de Ven, Tim van de Brug, Mark A. van de\n  Wiel, Johannes Berkhof", "title": "Estimating Bayesian Optimal Treatment Regimes for Dichotomous Outcomes\n  using Observational Data", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal treatment regimes (OTR) are individualised treatment assignment\nstrategies that identify a medical treatment as optimal given all background\ninformation available on the individual. We discuss Bayes optimal treatment\nregimes estimated using a loss function defined on the bivariate distribution\nof dichotomous potential outcomes. The proposed approach allows considering\nmore general objectives for the OTR than maximization of an expected outcome\n(e.g., survival probability) by taking into account, for example, unnecessary\ntreatment burden. As a motivating example we consider the case of oropharynx\ncancer treatment where unnecessary burden due to chemotherapy is to be avoided\nwhile maximizing survival chances. Assuming ignorable treatment assignment we\ndescribe Bayesian inference about the OTR including a sensitivity analysis on\nthe unobserved partial association of the potential outcomes. We evaluate the\nmethodology by simulations that apply Bayesian parametric and more flexible\nnon-parametric outcome models. The proposed OTR for oropharynx cancer reduces\nthe frequency of the more burdensome chemotherapy assignment by approximately\n75% without reducing the average survival probability. This regime thus offers\na strong increase in expected quality of life of patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:03:02 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 20:18:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Klausch", "Thomas", ""], ["van de Ven", "Peter", ""], ["van de Brug", "Tim", ""], ["van de Wiel", "Mark A.", ""], ["Berkhof", "Johannes", ""]]}, {"id": "1809.06686", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim, Ethan Vizitei, Varun Ganapathi", "title": "Domain Adaptation for Real-Time Student Performance Prediction", "comments": "arXiv admin note: text overlap with arXiv:1804.07405", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly fast development and update cycle of online course contents, and\ndiverse demographics of students in each online classroom, make student\nperformance prediction in real-time (before the course finishes) and/or on\ncurriculum without specific historical performance data available interesting\ntopics for both industrial research and practical needs. In this research, we\ntackle the problem of real-time student performance prediction with on-going\ncourses in a domain adaptation framework, which is a system trained on\nstudents' labeled outcome from one set of previous coursework but is meant to\nbe deployed on another. In particular, we first introduce recently-developed\nGritNet architecture which is the current state of the art for student\nperformance prediction problem, and develop a new \\emph{unsupervised} domain\nadaptation method to transfer a GritNet trained on a past course to a new\ncourse without any (students' outcome) label. Our results for real Udacity\nstudents' graduation predictions show that the GritNet not only\n\\emph{generalizes} well from one course to another across different Nanodegree\nprograms, but enhances real-time predictions explicitly in the first few weeks\nwhen accurate predictions are most challenging.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:47:57 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 02:12:36 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 14:48:12 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Vizitei", "Ethan", ""], ["Ganapathi", "Varun", ""]]}, {"id": "1809.06693", "submitter": "Yuri G. Gordienko", "authors": "Nikita Gordienko, Yuriy Kochura, Vlad Taran, Gang Peng, Yuri Gordienko\n  and Sergii Stirenko", "title": "Capsule Deep Neural Network for Recognition of Historical Graffiti\n  Handwriting", "comments": "6 pages, 8 figures, accepted for 2018 IEEE Ukraine Student, Young\n  Professional and Women in Engineering Congress (UKRSYW), October 2-6, 2018\n  (Kyiv, Ukraine). arXiv admin note: text overlap with arXiv:1808.10862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of the historical letters (XI-XVIII centuries) carved\non the stoned walls of St.Sophia cathedral in Kyiv (Ukraine) was demonstrated\nby means of capsule deep learning neural network. It was applied to the image\ndataset of the carved Glagolitic and Cyrillic letters (CGCL), which was\nassembled and pre-processed recently for recognition and prediction by machine\nlearning methods\n(https://www.kaggle.com/yoctoman/graffiti-st-sophia-cathedral-kyiv). CGCL\ndataset contains >4000 images for glyphs of 34 letters which are hardly\nrecognized by experts even in contrast to notMNIST dataset with the better\nimages of 10 letters taken from different fonts. Despite the much worse quality\nof CGCL dataset and extremely low number of samples (in comparison to notMNIST\ndataset) the capsule network model demonstrated much better results than the\npreviously used convolutional neural network (CNN). The validation accuracy\n(and validation loss) was higher (lower) for capsule network model than for CNN\nwithout data augmentation even. The area under curve (AUC) values for receiver\noperating characteristic (ROC) were also higher for the capsule network model\nthan for CNN model: 0.88-0.93 (capsule network) and 0.50 (CNN) without data\naugmentation, 0.91-0.95 (capsule network) and 0.51 (CNN) with lossless data\naugmentation, and similar results of 0.91-0.93 (capsule network) and 0.9 (CNN)\nin the regime of lossless data augmentation only. The confusion matrixes were\nmuch better for capsule network than for CNN model and gave the much lower type\nI (false positive) and type II (false negative) values in all three regimes of\ndata augmentation. These results supports the previous claims that capsule-like\nnetworks allow to reduce error rates not only on MNIST digit dataset, but on\nthe other notMNIST letter dataset and the more complex CGCL handwriting\ngraffiti letter dataset also.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:02:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Gordienko", "Nikita", ""], ["Kochura", "Yuriy", ""], ["Taran", "Vlad", ""], ["Peng", "Gang", ""], ["Gordienko", "Yuri", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1809.06705", "submitter": "Anthony Bagnall Dr", "authors": "A. Bagnall, M. Flynn, J. Large, J. Line, A. Bostrom and G. Cawley", "title": "Is rotation forest the best classifier for problems with continuous\n  features?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In short, our experiments suggest that yes, on average, rotation forest is\nbetter than the most common alternatives when all the attributes are\nreal-valued. Rotation forest is a tree based ensemble that performs transforms\non subsets of attributes prior to constructing each tree. We present an\nempirical comparison of classifiers for problems with only real-valued\nfeatures. We evaluate classifiers from three families of algorithms: support\nvector machines; tree-based ensembles; and neural networks tuned with a large\ngrid search. We compare classifiers on unseen data based on the quality of the\ndecision rule (using classification error) the ability to rank cases (area\nunder the receiver operating characteristic) and the probability estimates\n(using negative log likelihood). We conclude that, in answer to the question\nposed in the title, yes, rotation forest is significantly more accurate on\naverage than competing techniques when compared on three distinct sets of\ndatasets. Further, we assess the impact of the design features of rotation\nforest through an ablative study that transforms random forest into rotation\nforest. We identify the major limitation of rotation forest as its scalability,\nparticularly in number of attributes. To overcome this problem we develop a\nmodel to predict the train time of the algorithm and hence propose a contract\nversion of rotation forest where a run time cap is imposed {\\em a priori}. We\ndemonstrate that on large problems rotation forest can be made an order of\nmagnitude faster without significant loss of accuracy. We also show that there\nis no real benefit (on average) from tuning rotation forest. We maintain that\nwithout any domain knowledge to indicate an algorithm preference, rotation\nforest should be the default algorithm of choice for problems with continuous\nattributes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:33:45 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 14:44:32 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 12:29:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bagnall", "A.", ""], ["Flynn", "M.", ""], ["Large", "J.", ""], ["Line", "J.", ""], ["Bostrom", "A.", ""], ["Cawley", "G.", ""]]}, {"id": "1809.06719", "submitter": "Ameet Deshpande", "authors": "Ameet Deshpande, Srikanth Sarma, Ashutosh Jha, Balaraman Ravindran", "title": "Improvements on Hindsight Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reward problems are one of the biggest challenges in Reinforcement\nLearning. Goal-directed tasks are one such sparse reward problems where a\nreward signal is received only when the goal is reached. One promising way to\ntrain an agent to perform goal-directed tasks is to use Hindsight Learning\napproaches. In these approaches, even when an agent fails to reach the desired\ngoal, the agent learns to reach the goal it achieved instead. Doing this over\nmultiple trajectories while generalizing the policy learned from the achieved\ngoals, the agent learns a goal conditioned policy to reach any goal. One such\napproach is Hindsight Experience replay which uses an off-policy Reinforcement\nLearning algorithm to learn a goal conditioned policy. In this approach, a\nreplay of the past transitions happens in a uniformly random fashion. Another\napproach is to use a Hindsight version of the policy gradients to directly\nlearn a policy. In this work, we discuss different ways to replay past\ntransitions to improve learning in hindsight experience replay focusing on\nprioritized variants in particular. Also, we implement the Hindsight Policy\ngradient methods to robotic tasks.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 17:07:33 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 19:40:31 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Deshpande", "Ameet", ""], ["Sarma", "Srikanth", ""], ["Jha", "Ashutosh", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1809.06751", "submitter": "Anthony Bagnall Dr", "authors": "James Large, Anthony Bagnall, Simon Malinowski and Romain Tavenard", "title": "From BOP to BOSS and Beyond: Time Series Classification with Dictionary\n  Based Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of algorithms for time series classification (TSC) involve running a\nsliding window across each series, discretising the window to form a word,\nforming a histogram of word counts over the dictionary, then constructing a\nclassifier on the histograms. A recent evaluation of two of this type of\nalgorithm, Bag of Patterns (BOP) and Bag of Symbolic Fourier Approximation\nSymbols (BOSS) found a significant difference in accuracy between these\nseemingly similar algorithms. We investigate this phenomenon by deconstructing\nthe classifiers and measuring the relative importance of the four key\ncomponents between BOP and BOSS. We find that whilst ensembling is a key\ncomponent for both algorithms, the effect of the other components is mixed and\nmore complex. We conclude that BOSS represents the state of the art for\ndictionary based TSC. Both BOP and BOSS can be classed as bag of words\napproaches. These are particularly popular in Computer Vision for tasks such as\nimage classification. Converting approaches from vision requires careful\nengineering. We adapt three techniques used in Computer Vision for TSC: Scale\nInvariant Feature Transform; Spatial Pyramids; and Histrogram Intersection. We\nfind that using Spatial Pyramids in conjunction with BOSS (SP) produces a\nsignificantly more accurate classifier. SP is significantly more accurate than\nstandard benchmarks and the original BOSS algorithm. It is not significantly\nworse than the best shapelet based approach, and is only outperformed by\nHIVE-COTE, an ensemble that includes BOSS as a constituent module.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 14:04:33 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Large", "James", ""], ["Bagnall", "Anthony", ""], ["Malinowski", "Simon", ""], ["Tavenard", "Romain", ""]]}, {"id": "1809.06781", "submitter": "Sam Green", "authors": "Jieliang Luo, Sam Green, Peter Feghali, George Legrady, and \\c{C}etin\n  Kaya Ko\\c{c}", "title": "Visual Diagnostics for Deep Reinforcement Learning Policy Development", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vision-based reinforcement learning techniques often use convolutional\nneural networks (CNN) as universal function approximators to choose which\naction to take for a given visual input. Until recently, CNNs have been treated\nlike black-box functions, but this mindset is especially dangerous when used\nfor control in safety-critical settings. In this paper, we present our\nextensions of CNN visualization algorithms to the domain of vision-based\nreinforcement learning. We use a simulated drone environment as an example\nscenario. These visualization algorithms are an important tool for behavior\nintrospection and provide insight into the qualities and flaws of trained\npolicies when interacting with the physical world. A video may be seen at\nhttps://sites.google.com/view/drlvisual .\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:59:12 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 22:21:09 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Luo", "Jieliang", ""], ["Green", "Sam", ""], ["Feghali", "Peter", ""], ["Legrady", "George", ""], ["Ko\u00e7", "\u00c7etin Kaya", ""]]}, {"id": "1809.06784", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta and Zhaoyuan Yang", "title": "Adversarial Reinforcement Learning for Observer Design in Autonomous\n  Systems under Cyber Attacks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex autonomous control systems are subjected to sensor failures,\ncyber-attacks, sensor noise, communication channel failures, etc. that\nintroduce errors in the measurements. The corrupted information, if used for\nmaking decisions, can lead to degraded performance. We develop a framework for\nusing adversarial deep reinforcement learning to design observer strategies\nthat are robust to adversarial errors in information channels. We further show\nthrough simulation studies that the learned observation strategies perform\nremarkably well when the adversary's injected errors are bounded in some sense.\nWe use neural network as function approximator in our studies with the\nunderstanding that any other suitable function approximating class can be used\nwithin our framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 12:28:02 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Gupta", "Abhishek", ""], ["Yang", "Zhaoyuan", ""]]}, {"id": "1809.06796", "submitter": "Jialin Dong", "authors": "Jialin Dong and Yuanming Shi", "title": "Nonconvex Demixing From Bilinear Measurements", "comments": "This paper has been accepted by IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2018.2864660", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of demixing a sequence of source signals from the sum\nof noisy bilinear measurements. It is a generalized mathematical model for\nblind demixing with blind deconvolution, which is prevalent across the areas of\ndictionary learning, image processing, and communications. However, state-of-\nthe-art convex methods for blind demixing via semidefinite programming are\ncomputationally infeasible for large-scale problems. Although the existing\nnonconvex algorithms are able to address the scaling issue, they normally\nrequire proper regularization to establish optimality guarantees. The\nadditional regularization yields tedious algorithmic parameters and pessimistic\nconvergence rates with conservative step sizes. To address the limitations of\nexisting methods, we thus develop a provable nonconvex demixing procedure\nviaWirtinger flow, much like vanilla gradient descent, to harness the benefits\nof regularization-free fast convergence rate with aggressive step size and\ncomputational optimality guarantees. This is achieved by exploiting the benign\ngeometry of the blind demixing problem, thereby revealing that Wirtinger flow\nenforces the regularization-free iterates in the region of strong convexity and\nqualified level of smoothness, where the step size can be chosen aggressively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 15:31:43 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 09:48:31 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Dong", "Jialin", ""], ["Shi", "Yuanming", ""]]}, {"id": "1809.06798", "submitter": "Emre Yilmaz", "authors": "Longting Xu, Rohan Kumar Das, Emre Y{\\i}lmaz, Jichen Yang, Haizhou Li", "title": "Generative x-vectors for text-independent speaker verification", "comments": "Accepted for publication at SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker verification (SV) systems using deep neural network embeddings,\nso-called the x-vector systems, are becoming popular due to its good\nperformance superior to the i-vector systems. The fusion of these systems\nprovides improved performance benefiting both from the discriminatively trained\nx-vectors and generative i-vectors capturing distinct speaker characteristics.\nIn this paper, we propose a novel method to include the complementary\ninformation of i-vector and x-vector, that is called generative x-vector. The\ngenerative x-vector utilizes a transformation model learned from the i-vector\nand x-vector representations of the background data. Canonical correlation\nanalysis is applied to derive this transformation model, which is later used to\ntransform the standard x-vectors of the enrollment and test segments to the\ncorresponding generative x-vectors. The SV experiments performed on the NIST\nSRE 2010 dataset demonstrate that the system using generative x-vectors\nprovides considerably better performance than the baseline i-vector and\nx-vector systems. Furthermore, the generative x-vectors outperform the fusion\nof i-vector and x-vector systems for long-duration utterances, while yielding\ncomparable results for short-duration utterances.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:04:54 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Xu", "Longting", ""], ["Das", "Rohan Kumar", ""], ["Y\u0131lmaz", "Emre", ""], ["Yang", "Jichen", ""], ["Li", "Haizhou", ""]]}, {"id": "1809.06827", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom van Bussel, Tom Claassen, Tom Heskes", "title": "A Bayesian Approach for Inferring Local Causal Structure in Gene\n  Regulatory Networks", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": "PMLR 72 (2018) 37-48", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene regulatory networks play a crucial role in controlling an organism's\nbiological processes, which is why there is significant interest in developing\ncomputational methods that are able to extract their structure from\nhigh-throughput genetic data. A typical approach consists of a series of\nconditional independence tests on the covariance structure meant to\nprogressively reduce the space of possible causal models. We propose a novel\nefficient Bayesian method for discovering the local causal relationships among\ntriplets of (normally distributed) variables. In our approach, we score the\npatterns in the covariance matrix in one go and we incorporate the available\nbackground knowledge in the form of priors over causal structures. Our method\nis flexible in the sense that it allows for different types of causal\nstructures and assumptions. We apply the approach to the task of inferring gene\nregulatory networks by learning regulatory relationships between gene\nexpression levels. We show that our algorithm produces stable and conservative\nposterior probability estimates over local causal structures that can be used\nto derive an honest ranking of the most meaningful regulatory relationships. We\ndemonstrate the stability and efficacy of our method both on simulated data and\non real-world data from an experiment on yeast.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 16:51:48 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["van Bussel", "Tom", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "1809.06848", "submitter": "Remi Tachet Des Combes", "authors": "Remi Tachet, Mohammad Pezeshki, Samira Shabanian, Aaron Courville,\n  Yoshua Bengio", "title": "On the Learning Dynamics of Deep Neural Networks", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a lot of progress has been made in recent years, the dynamics of\nlearning in deep nonlinear neural networks remain to this day largely\nmisunderstood. In this work, we study the case of binary classification and\nprove various properties of learning in such networks under strong assumptions\nsuch as linear separability of the data. Extending existing results from the\nlinear case, we confirm empirical observations by proving that the\nclassification error also follows a sigmoidal shape in nonlinear architectures.\nWe show that given proper initialization, learning expounds parallel\nindependent modes and that certain regions of parameter space might lead to\nfailed training. We also demonstrate that input norm and features' frequency in\nthe dataset lead to distinct convergence speeds which might shed some light on\nthe generalization capabilities of deep neural networks. We provide a\ncomparison between the dynamics of learning with cross-entropy and hinge\nlosses, which could prove useful to understand recent progress in the training\nof generative adversarial networks. Finally, we identify a phenomenon that we\nbaptize gradient starvation where the most frequent features in a dataset\nprevent the learning of other less frequent but equally informative features.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:58:49 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 13:55:46 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 22:06:39 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Tachet", "Remi", ""], ["Pezeshki", "Mohammad", ""], ["Shabanian", "Samira", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1809.06858", "submitter": "Di He", "authors": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu", "title": "FRAGE: Frequency-Agnostic Word Representation", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous word representation (aka word embedding) is a basic building block\nin many neural network-based models used in natural language processing tasks.\nAlthough it is widely accepted that words with similar semantics should be\nclose to each other in the embedding space, we find that word embeddings\nlearned in several tasks are biased towards word frequency: the embeddings of\nhigh-frequency and low-frequency words lie in different subregions of the\nembedding space, and the embedding of a rare word and a popular word can be far\nfrom each other even if they are semantically similar. This makes learned word\nembeddings ineffective, especially for rare words, and consequently limits the\nperformance of these neural network models. In this paper, we develop a neat,\nsimple yet effective way to learn \\emph{FRequency-AGnostic word Embedding}\n(FRAGE) using adversarial training. We conducted comprehensive studies on ten\ndatasets across four natural language processing tasks, including word\nsimilarity, language modeling, machine translation and text classification.\nResults show that with FRAGE, we achieve higher performance than the baselines\nin all tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:31:22 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 04:28:27 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Gong", "Chengyue", ""], ["He", "Di", ""], ["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1809.06913", "submitter": "Markus Sch\\\"oberl", "authors": "Markus Sch\\\"oberl, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis", "title": "Predictive Collective Variable Discovery with Deep Bayesian Models", "comments": null, "journal-ref": "J. Chem. Phys. 150, 024109 (2019)", "doi": "10.1063/1.5058063", "report-no": null, "categories": "stat.ML cs.LG physics.chem-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending spatio-temporal scale limitations of models for complex atomistic\nsystems considered in biochemistry and materials science necessitates the\ndevelopment of enhanced sampling methods. The potential acceleration in\nexploring the configurational space by enhanced sampling methods depends on the\nchoice of collective variables (CVs). In this work, we formulate the discovery\nof CVs as a Bayesian inference problem and consider the CVs as hidden\ngenerators of the full-atomistic trajectory. The ability to generate samples of\nthe fine-scale atomistic configurations using limited training data allows us\nto compute estimates of observables as well as our probabilistic confidence on\nthem. The methodology is based on emerging methodological advances in machine\nlearning and variational inference. The discovered CVs are related to\nphysicochemical properties which are essential for understanding mechanisms\nespecially in unexplored complex systems. We provide a quantitative assessment\nof the CVs in terms of their predictive ability for alanine dipeptide (ALA-2)\nand ALA-15 peptide.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 20:11:22 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 21:27:02 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Sch\u00f6berl", "Markus", ""], ["Zabaras", "Nicholas", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1809.06937", "submitter": "Hannah Li", "authors": "Ramesh Johari, Vijay Kamble, Anilesh K. Krishnaswamy, Hannah Li", "title": "Exploration vs. Exploitation in Team Formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An online labor platform faces an online learning problem in matching workers\nwith jobs and using the performance on these jobs to create better future\nmatches. This learning problem is complicated by the rise of complex tasks on\nthese platforms, such as web development and product design, that require a\nteam of workers to complete. The success of a job is now a function of the\nskills and contributions of all workers involved, which may be unknown to both\nthe platform and the client who posted the job. These team matchings result in\na structured correlation between what is known about the individuals and this\ninformation can be utilized to create better future matches. We analyze two\nnatural settings where the performance of a team is dictated by its strongest\nand its weakest member, respectively. We find that both problems pose an\nexploration-exploitation tradeoff between learning the performance of untested\nteams and repeating previously tested teams that resulted in a good\nperformance. We establish fundamental regret bounds and design near-optimal\nalgorithms that uncover several insights into these tradeoffs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 21:15:45 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 22:58:12 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Johari", "Ramesh", ""], ["Kamble", "Vijay", ""], ["Krishnaswamy", "Anilesh K.", ""], ["Li", "Hannah", ""]]}, {"id": "1809.06958", "submitter": "Patrick Rebeschini", "authors": "Dominic Richards and Patrick Rebeschini", "title": "Graph-Dependent Implicit Regularisation for Distributed Stochastic\n  Subgradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose graph-dependent implicit regularisation strategies for distributed\nstochastic subgradient descent (Distributed SGD) for convex problems in\nmulti-agent learning. Under the standard assumptions of convexity, Lipschitz\ncontinuity, and smoothness, we establish statistical learning rates that\nretain, up to logarithmic terms, centralised statistical guarantees through\nimplicit regularisation (step size tuning and early stopping) with appropriate\ndependence on the graph topology. Our approach avoids the need for explicit\nregularisation in decentralised learning problems, such as adding constraints\nto the empirical risk minimisation rule. Particularly for distributed methods,\nthe use of implicit regularisation allows the algorithm to remain simple,\nwithout projections or dual methods. To prove our results, we establish\ngraph-independent generalisation bounds for Distributed SGD that match the\ncentralised setting (using algorithmic stability), and we establish\ngraph-dependent optimisation bounds that are of independent interest. We\npresent numerical experiments to show that the qualitative nature of the upper\nbounds we derive can be representative of real behaviours.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 22:51:32 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Richards", "Dominic", ""], ["Rebeschini", "Patrick", ""]]}, {"id": "1809.06970", "submitter": "Shuochao Yao", "authors": "Shuochao Yao, Yiran Zhao, Huajie Shao, Shengzhong Liu, Dongxin Liu, Lu\n  Su, Tarek Abdelzaher", "title": "FastDeepIoT: Towards Understanding and Optimizing Neural Network\n  Execution Time on Mobile and Embedded Devices", "comments": "Accepted by SenSys '18", "journal-ref": null, "doi": "10.1145/3274783.3274840", "report-no": null, "categories": "cs.LG cs.NI cs.PF cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks show great potential as solutions to many sensing\napplication problems, but their excessive resource demand slows down execution\ntime, pausing a serious impediment to deployment on low-end devices. To address\nthis challenge, recent literature focused on compressing neural network size to\nimprove performance. We show that changing neural network size does not\nproportionally affect performance attributes of interest, such as execution\ntime. Rather, extreme run-time nonlinearities exist over the network\nconfiguration space. Hence, we propose a novel framework, called FastDeepIoT,\nthat uncovers the non-linear relation between neural network structure and\nexecution time, then exploits that understanding to find network configurations\nthat significantly improve the trade-off between execution time and accuracy on\nmobile and embedded devices. FastDeepIoT makes two key contributions. First,\nFastDeepIoT automatically learns an accurate and highly interpretable execution\ntime model for deep neural networks on the target device. This is done without\nprior knowledge of either the hardware specifications or the detailed\nimplementation of the used deep learning library. Second, FastDeepIoT informs a\ncompression algorithm how to minimize execution time on the profiled device\nwithout impacting accuracy. We evaluate FastDeepIoT using three different\nsensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus.\nFastDeepIoT further reduces the neural network execution time by $48\\%$ to\n$78\\%$ and energy consumption by $37\\%$ to $69\\%$ compared with the\nstate-of-the-art compression algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 00:43:26 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Yao", "Shuochao", ""], ["Zhao", "Yiran", ""], ["Shao", "Huajie", ""], ["Liu", "Shengzhong", ""], ["Liu", "Dongxin", ""], ["Su", "Lu", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "1809.06992", "submitter": "Md Fayeem Bin Aziz", "authors": "Fayeem Aziz, Aaron S. W. Wong, James S. Welsh and Stephan K. Chalup", "title": "Aligning Manifolds of Double Pendulum Dynamics Under the Influence of\n  Noise", "comments": "The final version will appear in ICONIP 2018. A DOI identifier to the\n  final version will be added to the preprint, as soon as it is available", "journal-ref": null, "doi": "10.1007/978-3-030-04239-4_7", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents the results of a series of simulation experiments that\nevaluate and compare four different manifold alignment methods under the\ninfluence of noise. The data was created by simulating the dynamics of two\nslightly different double pendulums in three-dimensional space. The method of\nsemi-supervised feature-level manifold alignment using global distance resulted\nin the most convincing visualisations. However, the semi-supervised\nfeature-level local alignment methods resulted in smaller alignment errors.\nThese local alignment methods were also more robust to noise and faster than\nthe other methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 03:13:22 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 04:06:59 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Aziz", "Fayeem", ""], ["Wong", "Aaron S. W.", ""], ["Welsh", "James S.", ""], ["Chalup", "Stephan K.", ""]]}, {"id": "1809.06995", "submitter": "Alexander Brown", "authors": "Alexander Brown and Marek Petrik", "title": "Interpretable Reinforcement Learning with Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use boosted regression trees as a way to compute\nhuman-interpretable solutions to reinforcement learning problems. Boosting\ncombines several regression trees to improve their accuracy without\nsignificantly reducing their inherent interpretability. Prior work has focused\nindependently on reinforcement learning and on interpretable machine learning,\nbut there has been little progress in interpretable reinforcement learning. Our\nexperimental results show that boosted regression trees compute solutions that\nare both interpretable and match the quality of leading reinforcement learning\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 03:23:35 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Brown", "Alexander", ""], ["Petrik", "Marek", ""]]}, {"id": "1809.07006", "submitter": "Andrew Skabar", "authors": "Andrew Skabar", "title": "Using Eigencentrality to Estimate Joint, Conditional and Marginal\n  Probabilities from Mixed-Variable Data: Method and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to estimate joint, conditional and marginal probability\ndistributions over some set of variables is of great utility for many common\nmachine learning tasks. However, estimating these distributions can be\nchallenging, particularly in the case of data containing a mix of discrete and\ncontinuous variables. This paper presents a non-parametric method for\nestimating these distributions directly from a dataset. The data are first\nrepresented as a graph consisting of object nodes and attribute value nodes.\nDepending on the distribution to be estimated, an appropriate eigenvector\nequation is then constructed. This equation is then solved to find the\ncorresponding stationary distribution of the graph, from which the required\ndistributions can then be estimated and sampled from. The paper demonstrates\nhow the method can be applied to many common machine learning tasks including\nclassification, regression, missing value imputation, outlier detection, random\nvector generation, and clustering.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 04:01:55 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Skabar", "Andrew", ""]]}, {"id": "1809.07011", "submitter": "Nontawat Charoenphakdee", "authors": "Nontawat Charoenphakdee and Masashi Sugiyama", "title": "Positive-Unlabeled Classification under Class Prior Shift and Asymmetric\n  Error", "comments": "Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottlenecks of binary classification from positive and unlabeled data (PU\nclassification) are the requirements that given unlabeled patterns are drawn\nfrom the test marginal distribution, and the penalty of the false positive\nerror is identical to the false negative error. However, such requirements are\noften not fulfilled in practice. In this paper, we generalize PU classification\nto the class prior shift and asymmetric error scenarios. Based on the analysis\nof the Bayes optimal classifier, we show that given a test class prior, PU\nclassification under class prior shift is equivalent to PU classification with\nasymmetric error. Then, we propose two different frameworks to handle these\nproblems, namely, a risk minimization framework and density ratio estimation\nframework. Finally, we demonstrate the effectiveness of the proposed frameworks\nand compare both frameworks through experiments using benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 04:29:03 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 14:56:14 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 03:14:42 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2020 10:26:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Charoenphakdee", "Nontawat", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.07023", "submitter": "Zijun Zhang", "authors": "Zijun Zhang, Yining Zhang, Zongpeng Li", "title": "Removing the Feature Correlation Effect of Multiplicative Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplicative noise, including dropout, is widely used to regularize deep\nneural networks (DNNs), and is shown to be effective in a wide range of\narchitectures and tasks. From an information perspective, we consider injecting\nmultiplicative noise into a DNN as training the network to solve the task with\nnoisy information pathways, which leads to the observation that multiplicative\nnoise tends to increase the correlation between features, so as to increase the\nsignal-to-noise ratio of information pathways. However, high feature\ncorrelation is undesirable, as it increases redundancy in representations. In\nthis work, we propose non-correlating multiplicative noise (NCMN), which\nexploits batch normalization to remove the correlation effect in a simple yet\neffective way. We show that NCMN significantly improves the performance of\nstandard multiplicative noise on image classification tasks, providing a better\nalternative to dropout for batch-normalized networks. Additionally, we present\na unified view of NCMN and shake-shake regularization, which explains the\nperformance gain of the latter.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 06:23:46 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zhang", "Zijun", ""], ["Zhang", "Yining", ""], ["Li", "Zongpeng", ""]]}, {"id": "1809.07048", "submitter": "Jose Carballo", "authors": "Jose A. Carballo, Javier Bonilla, Manuel Berenguel, Jes\\'us\n  Fern\\'andez-Reche, Gin\\'es Garc\\'ia", "title": "New approach for solar tracking systems based on computer vision, low\n  cost hardware and deep learning", "comments": "12 pages, 10 figures,", "journal-ref": "Carballo, J. A., Bonilla, J., Berenguel, M., Fernandez-Reche, J.,\n  & Garcia, G. (2018). New approach for solar tracking systems based on\n  computer vision, low cost hardware and deep learning. Renewable Energy", "doi": "10.1016/j.renene.2018.08.101", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new approach for Sun tracking systems is presented. Due to\nthe current system limitations regarding costs and operational problems, a new\napproach based on low cost, computer vision open hardware and deep learning has\nbeen developed. The preliminary tests carried out successfully in Plataforma\nsolar de Almeria (PSA), reveal the great potential and show the new approach as\na good alternative to traditional systems. The proposed approach can provide\nkey variables for the Sun tracking system control like cloud movements\nprediction, block and shadow detection, atmospheric attenuation or measures of\nconcentrated solar radiation, which can improve the control strategies of the\nsystem and therefore the system performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:09:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Carballo", "Jose A.", ""], ["Bonilla", "Javier", ""], ["Berenguel", "Manuel", ""], ["Fern\u00e1ndez-Reche", "Jes\u00fas", ""], ["Garc\u00eda", "Gin\u00e9s", ""]]}, {"id": "1809.07066", "submitter": "Vishal Sunder", "authors": "Vishal Sunder, Lovekesh Vig, Arnab Chatterjee, Gautam Shroff", "title": "Prosocial or Selfish? Agents with different behaviors for Contract\n  Negotiation using Reinforcement Learning", "comments": "Proceedings of the 11th International Workshop on Automated\n  Negotiations (held in conjunction with IJCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective technique for training deep learning agents capable\nof negotiating on a set of clauses in a contract agreement using a simple\ncommunication protocol. We use Multi Agent Reinforcement Learning to train both\nagents simultaneously as they negotiate with each other in the training\nenvironment. We also model selfish and prosocial behavior to varying degrees in\nthese agents. Empirical evidence is provided showing consistency in agent\nbehaviors. We further train a meta agent with a mixture of behaviors by\nlearning an ensemble of different models using reinforcement learning. Finally,\nto ascertain the deployability of the negotiating agents, we conducted\nexperiments pitting the trained agents against human players. Results\ndemonstrate that the agents are able to hold their own against human players,\noften emerging as winners in the negotiation. Our experiments demonstrate that\nthe meta agent is able to reasonably emulate human behavior.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:46:34 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Sunder", "Vishal", ""], ["Vig", "Lovekesh", ""], ["Chatterjee", "Arnab", ""], ["Shroff", "Gautam", ""]]}, {"id": "1809.07102", "submitter": "Kirubin Pillay", "authors": "Kirubin Pillay and Maarten De Vos", "title": "A unifying Bayesian approach for preterm brain-age prediction that\n  models EEG sleep transitions over age", "comments": "5 pages, 2 figures. Submitted for Neural Information Processing\n  (NIPS) conference workshop on Machine Learning for Health (ML4H), Long Beach,\n  CA, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm newborns undergo various stresses that may materialize as learning\nproblems at school-age. Sleep staging of the Electroencephalogram (EEG),\nfollowed by prediction of their brain-age from these sleep states can quantify\ndeviations from normal brain development early (when compared to the known\nage). Current automation of this approach relies on explicit sleep state\nclassification, optimizing algorithms using clinician visually labelled sleep\nstages, which remains a subjective gold-standard. Such models fail to perform\nconsistently over a wide age range and impacts the subsequent brain-age\nestimates that could prevent identification of subtler developmental\ndeviations. We introduce a Bayesian Network utilizing multiple Gaussian Mixture\nModels, as a novel, unified approach for directly estimating brain-age,\nsimultaneously modelling for both age and sleep dependencies on the EEG, to\nimprove the accuracy of prediction over a wider age range.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:52:53 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Pillay", "Kirubin", ""], ["De Vos", "Maarten", ""]]}, {"id": "1809.07109", "submitter": "Young-Jin Park", "authors": "Young-Jin Park, and Han-Lim Choi", "title": "InfoSSM: Interpretable Unsupervised Learning of Nonparametric\n  State-Space Model for Multi-modal Dynamics", "comments": "Submitted to AIAA Intelligent Systems Student Paper Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of system identification is to learn about underlying physics\ndynamics behind the time-series data. To model the probabilistic and\nnonparametric dynamics model, Gaussian process (GP) have been widely used; GP\ncan estimate the uncertainty of prediction and avoid over-fitting. Traditional\nGPSSMs, however, are based on Gaussian transition model, thus often have\ndifficulty in describing a more complex transition model, e.g. aircraft\nmotions. To resolve the challenge, this paper proposes a framework using\nmultiple GP transition models which is capable of describing multi-modal\ndynamics. Furthermore, we extend the model to the information-theoretic\nframework, the so-called InfoSSM, by introducing a mutual information\nregularizer helping the model to learn interpretable and distinguishable\nmultiple dynamics models. Two illustrative numerical experiments in simple\nDubins vehicle and high-fidelity flight simulator are presented to demonstrate\nthe performance and interpretability of the proposed model. Finally, this paper\nintroduces a framework using InfoSSM with Bayesian filtering for air traffic\ncontrol tracking.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:16:00 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 10:05:23 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Park", "Young-Jin", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1809.07122", "submitter": "Shuxin Zheng", "authors": "Shuxin Zheng, Qi Meng, Huishuai Zhang, Wei Chen, Nenghai Yu, Tie-Yan\n  Liu", "title": "Capacity Control of ReLU Neural Networks by Basis-path Norm", "comments": null, "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, path norm was proposed as a new capacity measure for neural\nnetworks with Rectified Linear Unit (ReLU) activation function, which takes the\nrescaling-invariant property of ReLU into account. It has been shown that the\ngeneralization error bound in terms of the path norm explains the empirical\ngeneralization behaviors of the ReLU neural networks better than that of other\ncapacity measures. Moreover, optimization algorithms which take path norm as\nthe regularization term to the loss function, like Path-SGD, have been shown to\nachieve better generalization performance. However, the path norm counts the\nvalues of all paths, and hence the capacity measure based on path norm could be\nimproperly influenced by the dependency among different paths. It is also known\nthat each path of a ReLU network can be represented by a small group of\nlinearly independent basis paths with multiplication and division operation,\nwhich indicates that the generalization behavior of the network only depends on\nonly a few basis paths. Motivated by this, we propose a new norm\n\\emph{Basis-path Norm} based on a group of linearly independent paths to\nmeasure the capacity of neural networks more accurately. We establish a\ngeneralization error bound based on this basis path norm, and show it explains\nthe generalization behaviors of ReLU networks more accurately than previous\ncapacity measures via extensive experiments. In addition, we develop\noptimization algorithms which minimize the empirical risk regularized by the\nbasis-path norm. Our experiments on benchmark datasets demonstrate that the\nproposed regularization method achieves clearly better performance on the test\nset than the previous regularization approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 11:08:31 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zheng", "Shuxin", ""], ["Meng", "Qi", ""], ["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Yu", "Nenghai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1809.07192", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Yang Weng, Guangyi Liu, Zhongyang Zhao, Chin-woo Tan,\n  Ram Rajagopal", "title": "Unbalanced Multi-Phase Distribution Grid Topology Estimation and Bus\n  Phase Identification", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing need for monitoring and controlling uncertainties\nbrought by distributed energy resources in distribution grids. For such goal,\naccurate multi-phase topology is the basis for correlating measurements in\nunbalanced distribution networks. Unfortunately, such topology knowledge is\noften unavailable due to limited investment, especially for \\revv{low-voltage}\ndistribution grids. Also, the bus phase labeling information is inaccurate due\nto human errors or outdated records. For this challenge, this paper utilizes\nsmart meter data for an information-theoretic approach to learn the topology of\ndistribution grids. Specifically, multi-phase unbalanced systems are converted\ninto symmetrical components, namely positive, negative, and zero sequences.\nThen, this paper proves that the Chow-Liu algorithm finds the topology by\nutilizing power flow equations and the conditional independence relationships\nimplied by the radial multi-phase structure of distribution grids with the\npresence of incorrect bus phase labels. At last, by utilizing Carson's\nequation, this paper proves that the bus phase connection can be correctly\nidentified using voltage measurements. For validation, IEEE systems are\nsimulated using three real data sets. The simulation results demonstrate that\nthe algorithm is highly accurate for finding multi-phase topology even with\nstrong load unbalancing condition and DERs. This ensures close monitoring and\ncontrolling DERs in distribution grids.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 05:22:49 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 05:00:44 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 07:05:35 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Liao", "Yizheng", ""], ["Weng", "Yang", ""], ["Liu", "Guangyi", ""], ["Zhao", "Zhongyang", ""], ["Tan", "Chin-woo", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1809.07196", "submitter": "Elliot J. Crowley", "authors": "Jack Turner, Jos\\'e Cano, Valentin Radu, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks", "comments": "IISWC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 13:52:49 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Turner", "Jack", ""], ["Cano", "Jos\u00e9", ""], ["Radu", "Valentin", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "1809.07222", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil and Sheetal Kalyani", "title": "Noise Statistics Oblivious GARD For Robust Regression With Sparse\n  Outliers", "comments": "16 pages, 24 figures", "journal-ref": null, "doi": "10.1109/TSP.2018.2883025", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression models contaminated by Gaussian noise (inlier) and possibly\nunbounded sparse outliers are common in many signal processing applications.\nSparse recovery inspired robust regression (SRIRR) techniques are shown to\ndeliver high quality estimation performance in such regression models.\nUnfortunately, most SRIRR techniques assume \\textit{a priori} knowledge of\nnoise statistics like inlier noise variance or outlier statistics like number\nof outliers. Both inlier and outlier noise statistics are rarely known\n\\textit{a priori} and this limits the efficient operation of many SRIRR\nalgorithms. This article proposes a novel noise statistics oblivious algorithm\ncalled residual ratio thresholding GARD (RRT-GARD) for robust regression in the\npresence of sparse outliers. RRT-GARD is developed by modifying the recently\nproposed noise statistics dependent greedy algorithm for robust de-noising\n(GARD). Both finite sample and asymptotic analytical results indicate that\nRRT-GARD performs nearly similar to GARD with \\textit{a priori} knowledge of\nnoise statistics. Numerical simulations in real and synthetic data sets also\npoint to the highly competitive performance of RRT-GARD.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:36:11 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1809.07257", "submitter": "Oliver Nina", "authors": "Oliver Nina and Washington Garcia and Scott Clouse and Alper Yilmaz", "title": "MTLE: A Multitask Learning Encoder of Visual Feature Representations for\n  Video and Movie Description", "comments": "This is a pre-print version of our soon to be released paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning visual feature representations for video analysis is a daunting task\nthat requires a large amount of training samples and a proper generalization\nframework. Many of the current state of the art methods for video captioning\nand movie description rely on simple encoding mechanisms through recurrent\nneural networks to encode temporal visual information extracted from video\ndata. In this paper, we introduce a novel multitask encoder-decoder framework\nfor automatic semantic description and captioning of video sequences. In\ncontrast to current approaches, our method relies on distinct decoders that\ntrain a visual encoder in a multitask fashion. Our system does not depend\nsolely on multiple labels and allows for a lack of training data working even\nwith datasets where only one single annotation is viable per video. Our method\nshows improved performance over current state of the art methods in several\nmetrics on multi-caption and single-caption datasets. To the best of our\nknowledge, our method is the first method to use a multitask approach for\nencoding video features. Our method demonstrates its robustness on the Large\nScale Movie Description Challenge (LSMDC) 2017 where our method won the movie\ndescription task and its results were ranked among other competitors as the\nmost helpful for the visually impaired.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:50:18 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Nina", "Oliver", ""], ["Garcia", "Washington", ""], ["Clouse", "Scott", ""], ["Yilmaz", "Alper", ""]]}, {"id": "1809.07258", "submitter": "Guillaume Gautier", "authors": "Guillaume Gautier, Guillermo Polito, R\\'emi Bardenet, Michal Valko", "title": "DPPy: Sampling DPPs with Python", "comments": "Code at http://github.com/guilgautier/DPPy/ Documentation at\n  http://dppy.readthedocs.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are specific probability distributions\nover clouds of points that are used as models and computational tools across\nphysics, probability, statistics, and more recently machine learning. Sampling\nfrom DPPs is a challenge and therefore we present DPPy, a Python toolbox that\ngathers known exact and approximate sampling algorithms for both finite and\ncontinuous DPPs. The project is hosted on GitHub and equipped with an extensive\ndocumentation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:53:00 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 16:58:41 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gautier", "Guillaume", ""], ["Polito", "Guillermo", ""], ["Bardenet", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "1809.07260", "submitter": "Santu Rana", "authors": "Pratibha Vellanki, Santu Rana, Sunil Gupta, David Rubin de Celis Leal,\n  Alessandra Sutti, Murray Height, Svetha Venkatesh", "title": "Bayesian functional optimisation with shape prior", "comments": "Submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world experiments are expensive, and thus it is important to reach a\ntarget in minimum number of experiments. Experimental processes often involve\ncontrol variables that changes over time. Such problems can be formulated as a\nfunctional optimisation problem. We develop a novel Bayesian optimisation\nframework for such functional optimisation of expensive black-box processes. We\nrepresent the control function using Bernstein polynomial basis and optimise in\nthe coefficient space. We derive the theory and practice required to\ndynamically adjust the order of the polynomial degree, and show how prior\ninformation about shape can be integrated. We demonstrate the effectiveness of\nour approach for short polymer fibre design and optimising learning rate\nschedules for deep networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:00:24 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 00:24:26 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Vellanki", "Pratibha", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Leal", "David Rubin de Celis", ""], ["Sutti", "Alessandra", ""], ["Height", "Murray", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1809.07276", "submitter": "Romain Hennequin", "authors": "R\\'emi Delbouys and Romain Hennequin and Francesco Piccoli and Jimena\n  Royo-Letelier and Manuel Moussallam", "title": "Music Mood Detection Based On Audio And Lyrics With Deep Neural Net", "comments": "Published in ISMIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the task of multimodal music mood prediction based on the audio\nsignal and the lyrics of a track. We reproduce the implementation of\ntraditional feature engineering based approaches and propose a new model based\non deep learning. We compare the performance of both approaches on a database\ncontaining 18,000 tracks with associated valence and arousal values and show\nthat our approach outperforms classical models on the arousal detection task,\nand that both approaches perform equally on the valence prediction task. We\nalso compare the a posteriori fusion with fusion of modalities optimized\nsimultaneously with each unimodal model, and observe a significant improvement\nof valence prediction. We release part of our database for comparison purposes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:16:57 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Delbouys", "R\u00e9mi", ""], ["Hennequin", "Romain", ""], ["Piccoli", "Francesco", ""], ["Royo-Letelier", "Jimena", ""], ["Moussallam", "Manuel", ""]]}, {"id": "1809.07282", "submitter": "Nikita Srivatsan", "authors": "Nikita Srivatsan, Zachary Wojtowicz, Taylor Berg-Kirkpatrick", "title": "Modeling Online Discourse with Coupled Distributed Topics", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep, globally normalized topic model that\nincorporates structural relationships connecting documents in socially\ngenerated corpora, such as online forums. Our model (1) captures discursive\ninteractions along observed reply links in addition to traditional topic\ninformation, and (2) incorporates latent distributed representations arranged\nin a deep architecture, which enables a GPU-based mean-field inference\nprocedure that scales efficiently to large data. We apply our model to a new\nsocial media dataset consisting of 13M comments mined from the popular internet\nforum Reddit, a domain that poses significant challenges to models that do not\naccount for relationships connecting user comments. We evaluate against\nexisting methods across multiple metrics including perplexity and metadata\nprediction, and qualitatively analyze the learned interaction patterns.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:21:12 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:47:25 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 05:21:13 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Srivatsan", "Nikita", ""], ["Wojtowicz", "Zachary", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1809.07310", "submitter": "Yann Guermeur", "authors": "Yann Guermeur", "title": "Combinatorial and Structural Results for gamma-Psi-dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the generalization performance of margin\nmulti-category classifiers, when minimal learnability hypotheses are made. In\nthat context, the derivation of a guaranteed risk is based on the handling of\ncapacity measures belonging to three main families: Rademacher/Gaussian\ncomplexities, metric entropies and scale-sensitive combinatorial dimensions.\nThe scale-sensitive combinatorial dimensions dedicated to the classifiers of\nthis kind are the gamma-Psi-dimensions. We introduce the combinatorial and\nstructural results needed to involve them in the derivation of guaranteed risks\nand establish the corresponding upper bounds on the metric entropies and the\nRademacher complexity. Two major conclusions can be drawn: 1. the\ngamma-Psi-dimensions always bring an improvement compared to the use of the\nfat-shattering dimension of the class of margin functions; 2. thanks to their\ncapacity to take into account basic features of the classifier, they represent\na promising alternative to performing the transition from the multi-class case\nto the binary one with covering numbers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 17:35:43 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:22:21 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 17:35:56 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Guermeur", "Yann", ""]]}, {"id": "1809.07347", "submitter": "Sanket Diwale", "authors": "Sanket Diwale and Colin Jones", "title": "A Generalized Representer Theorem for Hilbert Space - Valued Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The necessary and sufficient conditions for existence of a generalized\nrepresenter theorem are presented for learning Hilbert space-valued functions.\nRepresenter theorems involving explicit basis functions and Reproducing Kernels\nare a common occurrence in various machine learning algorithms like generalized\nleast squares, support vector machines, Gaussian process regression and kernel\nbased deep neural networks to name a few. Due to the more general structure of\nthe underlying variational problems, the theory is also relevant to other\napplication areas like optimal control, signal processing and decision making.\nWe present the generalized representer as a unified view for supervised and\nsemi-supervised learning methods, using the theory of linear operators and\nsubspace valued maps. The implications of the theorem are presented with\nexamples of multi input-multi output regression, kernel based deep neural\nnetworks, stochastic regression and sparsity learning problems as being special\ncases in this unified view.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 18:00:51 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Diwale", "Sanket", ""], ["Jones", "Colin", ""]]}, {"id": "1809.07394", "submitter": "Jessica Hwang", "authors": "Jessica Hwang and Paulo Orenstein and Judah Cohen and Karl Pfeiffer\n  and Lester Mackey", "title": "Improving Subseasonal Forecasting in the Western U.S. with Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water managers in the western United States (U.S.) rely on longterm forecasts\nof temperature and precipitation to prepare for droughts and other wet weather\nextremes. To improve the accuracy of these longterm forecasts, the U.S. Bureau\nof Reclamation and the National Oceanic and Atmospheric Administration (NOAA)\nlaunched the Subseasonal Climate Forecast Rodeo, a year-long real-time\nforecasting challenge in which participants aimed to skillfully predict\ntemperature and precipitation in the western U.S. two to four weeks and four to\nsix weeks in advance. Here we present and evaluate our machine learning\napproach to the Rodeo and release our SubseasonalRodeo dataset, collected to\ntrain and evaluate our forecasting system.\n  Our system is an ensemble of two regression models. The first integrates the\ndiverse collection of meteorological measurements and dynamic model forecasts\nin the SubseasonalRodeo dataset and prunes irrelevant predictors using a\ncustomized multitask model selection procedure. The second uses only historical\nmeasurements of the target variable (temperature or precipitation) and\nintroduces multitask nearest neighbor features into a weighted local linear\nregression. Each model alone is significantly more accurate than the debiased\noperational U.S. Climate Forecasting System (CFSv2), and our ensemble skill\nexceeds that of the top Rodeo competitor for each target variable and forecast\nhorizon. Moreover, over 2011-2018, an ensemble of our regression models and\ndebiased CFSv2 improves debiased CFSv2 skill by 40-50% for temperature and\n129-169% for precipitation. We hope that both our dataset and our methods will\nhelp to advance the state of the art in subseasonal forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:08:26 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 02:42:49 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 10:16:23 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Hwang", "Jessica", ""], ["Orenstein", "Paulo", ""], ["Cohen", "Judah", ""], ["Pfeiffer", "Karl", ""], ["Mackey", "Lester", ""]]}, {"id": "1809.07402", "submitter": "Huan Wang", "authors": "Huan Wang, Nitish Shirish Keskar, Caiming Xiong, Richard Socher", "title": "Identifying Generalization Properties in Neural Networks", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it has not yet been proven, empirical evidence suggests that model\ngeneralization is related to local properties of the optima which can be\ndescribed via the Hessian. We connect model generalization with the local\nproperty of a solution under the PAC-Bayes paradigm. In particular, we prove\nthat model generalization ability is related to the Hessian, the higher-order\n\"smoothness\" terms characterized by the Lipschitz constant of the Hessian, and\nthe scales of the parameters. Guided by the proof, we propose a metric to score\nthe generalization capability of the model, as well as an algorithm that\noptimizes the perturbed model accordingly.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:37:42 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Wang", "Huan", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1809.07405", "submitter": "Tom Hanika", "authors": "Bastian Sch\\\"afermeier and Tom Hanika and Gerd Stumme", "title": "Distances for WiFi Based Topological Indoor Mapping", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3360774.3360780", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For localization and mapping of indoor environments through WiFi signals,\nlocations are often represented as likelihoods of the received signal strength\nindicator. In this work we compare various measures of distance between such\nlikelihoods in combination with different methods for estimation and\nrepresentation. In particular, we show that among the considered distance\nmeasures the Earth Mover's Distance seems the most beneficial for the\nlocalization task. Combined with kernel density estimation we were able to\nretain the topological structure of rooms in a real-world office scenario.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:45:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Sch\u00e4fermeier", "Bastian", ""], ["Hanika", "Tom", ""], ["Stumme", "Gerd", ""]]}, {"id": "1809.07424", "submitter": "Besmira Nushi", "authors": "Besmira Nushi, Ece Kamar, Eric Horvitz", "title": "Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing\n  System Failure", "comments": null, "journal-ref": "AAAI Conference on Human Computation and Crowdsourcing 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems move from computer-science laboratories into the\nopen world, their accountability becomes a high priority problem.\nAccountability requires deep understanding of system behavior and its failures.\nCurrent evaluation methods such as single-score error metrics and confusion\nmatrices provide aggregate views of system performance that hide important\nshortcomings. Understanding details about failures is important for identifying\npathways for refinement, communicating the reliability of systems in different\nsettings, and for specifying appropriate human oversight and engagement.\nCharacterization of failures and shortcomings is particularly complex for\nsystems composed of multiple machine learned components. For such systems,\nexisting evaluation methods have limited expressiveness in describing and\nexplaining the relationship among input content, the internal states of system\ncomponents, and final output quality. We present Pandora, a set of hybrid\nhuman-machine methods and tools for describing and explaining system failures.\nPandora leverages both human and system-generated observations to summarize\nconditions of system malfunction with respect to the input content and system\narchitecture. We share results of a case study with a machine learning pipeline\nfor image captioning that show how detailed performance views can be beneficial\nfor analysis and debugging.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:53:46 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Nushi", "Besmira", ""], ["Kamar", "Ece", ""], ["Horvitz", "Eric", ""]]}, {"id": "1809.07428", "submitter": "Jiaxi Tang", "authors": "Jiaxi Tang, Ke Wang", "title": "Ranking Distillation: Learning Compact Ranking Models With High\n  Performance for Recommender System", "comments": "Accepted at KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel way to train ranking models, such as recommender systems,\nthat are both effective and efficient. Knowledge distillation (KD) was shown to\nbe successful in image recognition to achieve both effectiveness and\nefficiency. We propose a KD technique for learning to rank problems, called\n\\emph{ranking distillation (RD)}. Specifically, we train a smaller student\nmodel to learn to rank documents/items from both the training data and the\nsupervision of a larger teacher model. The student model achieves a similar\nranking performance to that of the large teacher model, but its smaller model\nsize makes the online inference more efficient. RD is flexible because it is\northogonal to the choices of ranking models for the teacher and student. We\naddress the challenges of RD for ranking problems. The experiments on public\ndata sets and state-of-the-art recommendation models showed that RD achieves\nits design purposes: the student model learnt with RD has a model size less\nthan half of the teacher model while achieving a ranking performance similar to\nthe teacher model and much better than the student model learnt without RD.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 23:25:24 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Tang", "Jiaxi", ""], ["Wang", "Ke", ""]]}, {"id": "1809.07480", "submitter": "Vibhavari Dasagi", "authors": "Vibhavari Dasagi, Robert Lee, Serena Mou, Jake Bruce, Niko\n  S\\\"underhauf and J\\\"urgen Leitner", "title": "Sim-to-Real Transfer of Robot Learning with Variable Length Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current end-to-end deep Reinforcement Learning (RL) approaches require\njointly learning perception, decision-making and low-level control from very\nsparse reward signals and high-dimensional inputs, with little capability of\nincorporating prior knowledge. This results in prohibitively long training\ntimes for use on real-world robotic tasks. Existing algorithms capable of\nextracting task-level representations from high-dimensional inputs, e.g. object\ndetection, often produce outputs of varying lengths, restricting their use in\nRL methods due to the need for neural networks to have fixed length inputs. In\nthis work, we propose a framework that combines deep sets encoding, which\nallows for variable-length abstract representations, with modular RL that\nutilizes these representations, decoupling high-level decision making from\nlow-level control. We successfully demonstrate our approach on the robot\nmanipulation task of object sorting, showing that this method can learn\neffective policies within mere minutes of highly simplified simulation. The\nlearned policies can be directly deployed on a robot without further training,\nand generalize to variations of the task unseen during training.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 05:09:00 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 00:21:04 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Dasagi", "Vibhavari", ""], ["Lee", "Robert", ""], ["Mou", "Serena", ""], ["Bruce", "Jake", ""], ["S\u00fcnderhauf", "Niko", ""], ["Leitner", "J\u00fcrgen", ""]]}, {"id": "1809.07500", "submitter": "Simon Duque Anton", "authors": "Simon Duque Anton, Lia Ahrens, Daniel Fraunholz, Hans Dieter Schotten", "title": "Time is of the Essence: Machine Learning-based Intrusion Detection in\n  Industrial Time Series Data", "comments": "Extended version of a publication in the 2018 IEEE International\n  Conference on Data Mining Workshops (ICDMW)", "journal-ref": null, "doi": "10.1109/ICDMW.2018.00008", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Industrial Internet of Things drastically increases connectivity of\ndevices in industrial applications. In addition to the benefits in efficiency,\nscalability and ease of use, this creates novel attack surfaces. Historically,\nindustrial networks and protocols do not contain means of security, such as\nauthentication and encryption, that are made necessary by this development.\nThus, industrial IT-security is needed. In this work, emulated industrial\nnetwork data is transformed into a time series and analysed with three\ndifferent algorithms. The data contains labeled attacks, so the performance can\nbe evaluated. Matrix Profiles perform well with almost no parameterisation\nneeded. Seasonal Autoregressive Integrated Moving Average performs well in the\npresence of noise, requiring parameterisation effort. Long Short Term\nMemory-based neural networks perform mediocre while requiring a high training-\nand parameterisation effort.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:15:43 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Anton", "Simon Duque", ""], ["Ahrens", "Lia", ""], ["Fraunholz", "Daniel", ""], ["Schotten", "Hans Dieter", ""]]}, {"id": "1809.07575", "submitter": "Gino Brunner", "authors": "Gino Brunner, Yuyi Wang, Roger Wattenhofer and Sumu Zhao", "title": "Symbolic Music Genre Transfer with CycleGAN", "comments": "Paper accepted at the 30th International Conference on Tools with\n  Artificial Intelligence, ICTAI 2018, Volos, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models such as Variational Autoencoders (VAEs) and Generative\nAdversarial Networks (GANs) have recently been applied to style and domain\ntransfer for images, and in the case of VAEs, music. GAN-based models employing\nseveral generators and some form of cycle consistency loss have been among the\nmost successful for image domain transfer. In this paper we apply such a model\nto symbolic music and show the feasibility of our approach for music genre\ntransfer. Evaluations using separate genre classifiers show that the style\ntransfer works well. In order to improve the fidelity of the transformed music,\nwe add additional discriminators that cause the generators to keep the\nstructure of the original music mostly intact, while still achieving strong\ngenre transfer. Visual and audible results further show the potential of our\napproach. To the best of our knowledge, this paper represents the first\napplication of GANs to symbolic music domain transfer.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 11:20:11 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Brunner", "Gino", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""], ["Zhao", "Sumu", ""]]}, {"id": "1809.07599", "submitter": "Jean-Baptiste Cordonnier", "authors": "Sebastian U. Stich, Jean-Baptiste Cordonnier and Martin Jaggi", "title": "Sparsified SGD with Memory", "comments": "to appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge scale machine learning problems are nowadays tackled by distributed\noptimization algorithms, i.e. algorithms that leverage the compute power of\nmany devices for training. The communication overhead is a key bottleneck that\nhinders perfect scalability. Various recent works proposed to use quantization\nor sparsification techniques to reduce the amount of data that needs to be\ncommunicated, for instance by only sending the most significant entries of the\nstochastic gradient (top-k sparsification). Whilst such schemes showed very\npromising performance in practice, they have eluded theoretical analysis so\nfar.\n  In this work we analyze Stochastic Gradient Descent (SGD) with\nk-sparsification or compression (for instance top-k or random-k) and show that\nthis scheme converges at the same rate as vanilla SGD when equipped with error\ncompensation (keeping track of accumulated errors in memory). That is,\ncommunication can be reduced by a factor of the dimension of the problem\n(sometimes even more) whilst still converging at the same rate. We present\nnumerical experiments to illustrate the theoretical findings and the better\nscalability for distributed applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:02:14 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:13:10 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Cordonnier", "Jean-Baptiste", ""], ["Jaggi", "Martin", ""]]}, {"id": "1809.07600", "submitter": "Gino Brunner", "authors": "Gino Brunner, Andres Konrad, Yuyi Wang, Roger Wattenhofer", "title": "MIDI-VAE: Modeling Dynamics and Instrumentation of Music with\n  Applications to Style Transfer", "comments": "Paper accepted at the 19th International Society for Music\n  Information Retrieval Conference, ISMIR 2018, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MIDI-VAE, a neural network model based on Variational\nAutoencoders that is capable of handling polyphonic music with multiple\ninstrument tracks, as well as modeling the dynamics of music by incorporating\nnote durations and velocities. We show that MIDI-VAE can perform style transfer\non symbolic music by automatically changing pitches, dynamics and instruments\nof a music piece from, e.g., a Classical to a Jazz style. We evaluate the\nefficacy of the style transfer by training separate style validation\nclassifiers. Our model can also interpolate between short pieces of music,\nproduce medleys and create mixtures of entire songs. The interpolations\nsmoothly change pitches, dynamics and instrumentation to create a harmonic\nbridge between two music pieces. To the best of our knowledge, this work\nrepresents the first successful attempt at applying neural style transfer to\ncomplete musical compositions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:02:30 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Brunner", "Gino", ""], ["Konrad", "Andres", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1809.07609", "submitter": "Xavier Warin", "authors": "Quentin Chan-Wai-Nam, Joseph Mikael, Xavier Warin", "title": "Machine Learning for semi linear PDEs", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning algorithms dedicated to solving semi-linear PDEs are\nimproved by using different neural network architectures and different\nparameterizations. These algorithms are compared to a new one that solves a\nfixed point problem by using deep learning techniques. This new algorithm\nappears to be competitive in terms of accuracy with the best existing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:26:09 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 12:39:29 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chan-Wai-Nam", "Quentin", ""], ["Mikael", "Joseph", ""], ["Warin", "Xavier", ""]]}, {"id": "1809.07688", "submitter": "Peiyuan Sun", "authors": "Peiyuan Suny, Jianxin Li, Yongyi Mao, Richong Zhang, Lihong Wang", "title": "Inferring Multiplex Diffusion Network via Multivariate Marked Hawkes\n  Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the diffusion in social network is an important task. However,\nthis task is challenging since (1) the network structure is usually hidden with\nonly observations of events like \"post\" or \"repost\" associated with each node,\nand (2) the interactions between nodes encompass multiple distinct patterns\nwhich in turn affect the diffusion patterns. For instance, social interactions\nseldom develop on a single channel, and multiple relationships can bind pairs\nof people due to their various common interests. Most previous work considers\nonly one of these two challenges which is apparently unrealistic. In this\npaper, we study the problem of \\emph{inferring multiplex network} in social\nnetworks. We propose the Multiplex Diffusion Model (MDM) which incorporates the\nmultivariate marked Hawkes process and topic model to infer the multiplex\nstructure of social network. A MCMC based algorithm is developed to infer the\nlatent multiplex structure and to estimate the node-related parameters. We\nevaluate our model based on both synthetic and real-world datasets. The results\nshow that our model is more effective in terms of uncovering the multiplex\nnetwork structure.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 02:56:39 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Suny", "Peiyuan", ""], ["Li", "Jianxin", ""], ["Mao", "Yongyi", ""], ["Zhang", "Richong", ""], ["Wang", "Lihong", ""]]}, {"id": "1809.07691", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao", "title": "A Survey on Theoretical Advances of Community Detection in Networks", "comments": "Wire Computational Statistics, 2017", "journal-ref": null, "doi": "10.1002/wics.1403", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world networks usually have community structure, that is, nodes are\ngrouped into densely connected communities. Community detection is one of the\nmost popular and best-studied research topics in network science and has\nattracted attention in many different fields, including computer science,\nstatistics, social sciences, among others. Numerous approaches for community\ndetection have been proposed in literature, from ad-hoc algorithms to\nsystematic model-based approaches. The large number of available methods leads\nto a fundamental question: whether a certain method can provide consistent\nestimates of community labels. The stochastic blockmodel (SBM) and its variants\nprovide a convenient framework for the study of such problems. This article is\na survey on the recent theoretical advances of community detection. The authors\nreview a number of community detection methods and their theoretical\nproperties, including graph cut methods, profile likelihoods, the\npseudo-likelihood method, the variational method, belief propagation, spectral\nclustering, and semidefinite relaxations of the SBM. The authors also briefly\ndiscuss other research topics in community detection such as robust community\ndetection, community detection with nodal covariates and model selection, as\nwell as suggest a few possible directions for future research.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 03:52:19 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Zhao", "Yunpeng", ""]]}, {"id": "1809.07695", "submitter": "Pedro Henrique da Costa Avelar", "authors": "Pedro H. C. Avelar and Henrique Lemos and Marcelo O. R. Prates and\n  Luis Lamb", "title": "Multitask Learning on Graph Neural Networks: Learning Multiple Graph\n  Centrality Measures with a Unified Network", "comments": "Published at ICANN2019. 10 pages, 3 Figures", "journal-ref": null, "doi": "10.1007/978-3-030-30493-5_63", "report-no": null, "categories": "cs.SI cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning to symbolic domains remains an active\nresearch endeavour. Graph neural networks (GNN), consisting of trained neural\nmodules which can be arranged in different topologies at run time, are sound\nalternatives to tackle relational problems which lend themselves to graph\nrepresentations. In this paper, we show that GNNs are capable of multitask\nlearning, which can be naturally enforced by training the model to refine a\nsingle set of multidimensional embeddings $\\in \\mathbb{R}^d$ and decode them\ninto multiple outputs by connecting MLPs at the end of the pipeline. We\ndemonstrate the multitask learning capability of the model in the relevant\nrelational problem of estimating network centrality measures, focusing\nprimarily on producing rankings based on these measures, i.e. is vertex $v_1$\nmore central than vertex $v_2$ given centrality $c$?. We then show that a GNN\ncan be trained to develop a \\emph{lingua franca} of vertex embeddings from\nwhich all relevant information about any of the trained centrality measures can\nbe decoded. The proposed model achieves $89\\%$ accuracy on a test dataset of\nrandom instances with up to 128 vertices and is shown to generalise to larger\nproblem sizes. The model is also shown to obtain reasonable accuracy on a\ndataset of real world instances with up to 4k vertices, vastly surpassing the\nsizes of the largest instances with which the model was trained ($n=128$).\nFinally, we believe that our contributions attest to the potential of GNNs in\nsymbolic domains in general and in relational learning in particular.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 12:01:37 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 13:04:48 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 17:56:26 GMT"}, {"version": "v4", "created": "Thu, 28 Nov 2019 18:39:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Avelar", "Pedro H. C.", ""], ["Lemos", "Henrique", ""], ["Prates", "Marcelo O. R.", ""], ["Lamb", "Luis", ""]]}, {"id": "1809.07703", "submitter": "Dan Shiebler", "authors": "Dan Shiebler, Luca Belli, Jay Baxter, Hanchen Xiong, Abhishek Tayal", "title": "Fighting Redundancy and Model Decay with Embeddings", "comments": "Presented at the Common Model Infrastructure Workshop at KDD 2018\n  (link: https://cmi2018.sdsc.edu/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day, hundreds of millions of new Tweets containing over 40 languages of\never-shifting vernacular flow through Twitter. Models that attempt to extract\ninsight from this firehose of information must face the torrential covariate\nshift that is endemic to the Twitter platform. While regularly-retrained\nalgorithms can maintain performance in the face of this shift, fixed model\nfeatures that fail to represent new trends and tokens can quickly become stale,\nresulting in performance degradation. To mitigate this problem we employ\nlearned features, or embedding models, that can efficiently represent the most\nrelevant aspects of a data distribution. Sharing these embedding models across\nteams can also reduce redundancy and multiplicatively increase cross-team\nmodeling productivity. In this paper, we detail the commoditized tools,\nalgorithms and pipelines that we have developed and are developing at Twitter\nto regularly generate high quality, up-to-date embeddings and share them\nbroadly across the company.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 15:58:13 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Shiebler", "Dan", ""], ["Belli", "Luca", ""], ["Baxter", "Jay", ""], ["Xiong", "Hanchen", ""], ["Tayal", "Abhishek", ""]]}, {"id": "1809.07731", "submitter": "A. Rupam Mahmood", "authors": "A. Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, James\n  Bergstra", "title": "Benchmarking Reinforcement Learning Algorithms on Real-World Robots", "comments": "Appears in Proceedings of the Second Conference on Robot Learning\n  (CoRL 2018). Companion video at https://youtu.be/ovDfhvjpQd8 and source code\n  at https://github.com/kindredresearch/SenseAct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through many recent successes in simulation, model-free reinforcement\nlearning has emerged as a promising approach to solving continuous control\nrobotic tasks. The research community is now able to reproduce, analyze and\nbuild quickly on these results due to open source implementations of learning\nalgorithms and simulated benchmark tasks. To carry forward these successes to\nreal-world applications, it is crucial to withhold utilizing the unique\nadvantages of simulations that do not transfer to the real world and experiment\ndirectly with physical robots. However, reinforcement learning research with\nphysical robots faces substantial resistance due to the lack of benchmark tasks\nand supporting source code. In this work, we introduce several reinforcement\nlearning tasks with multiple commercially available robots that present varying\nlevels of learning difficulty, setup, and repeatability. On these tasks, we\ntest the learning performance of off-the-shelf implementations of four\nreinforcement learning algorithms and analyze sensitivity to their\nhyper-parameters to determine their readiness for applications in various\nreal-world tasks. Our results show that with a careful setup of the task\ninterface and computations, some of these implementations can be readily\napplicable to physical robots. We find that state-of-the-art learning\nalgorithms are highly sensitive to their hyper-parameters and their relative\nordering does not transfer across tasks, indicating the necessity of re-tuning\nthem for each task for best performance. On the other hand, the best\nhyper-parameter configuration from one task may often result in effective\nlearning on held-out tasks even with different robots, providing a reasonable\ndefault. We make the benchmark tasks publicly available to enhance\nreproducibility in real-world reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 16:46:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Mahmood", "A. Rupam", ""], ["Korenkevych", "Dmytro", ""], ["Vasan", "Gautham", ""], ["Ma", "William", ""], ["Bergstra", "James", ""]]}, {"id": "1809.07748", "submitter": "Shing Chan", "authors": "Shing Chan and Ahmed H. Elsheikh", "title": "Exemplar-based synthesis of geology using kernel discrepancies and\n  generative neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for synthesis of geological images based on an\nexemplar image. We synthesize new realizations such that the discrepancy in the\npatch distribution between the realizations and the exemplar image is\nminimized. Such discrepancy is quantified using a kernel method for two-sample\ntest called maximum mean discrepancy. To enable fast synthesis, we train a\ngenerative neural network in an offline phase to sample realizations\nefficiently during deployment, while also providing a parametrization of the\nsynthesis process. We assess the framework on a classical binary image\nrepresenting channelized subsurface reservoirs, finding that the method\nreproduces the visual patterns and spatial statistics (image histogram and\ntwo-point probability functions) of the exemplar image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 17:33:20 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 09:31:45 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1809.07751", "submitter": "Brian Lucena", "authors": "Brian Lucena", "title": "Spline-Based Probability Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems it is desirable to output well-calibrated\nprobabilities on the different classes. We propose a robust, non-parametric\nmethod of calibrating probabilities called SplineCalib that utilizes smoothing\nsplines to determine a calibration function. We demonstrate how applying\ncertain transformations as part of the calibration process can improve\nperformance on problems in deep learning and other domains where the scores\ntend to be \"overconfident\". We adapt the approach to multi-class problems and\nfind that better calibration can improve accuracy as well as log-loss by better\nresolving uncertain cases. Finally, we present a cross-validated approach to\ncalibration which conserves data. Significant improvements to log-loss and\naccuracy are shown on several different problems. We also introduce the\nml-insights python package which contains an implementation of the SplineCalib\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 17:36:24 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Lucena", "Brian", ""]]}, {"id": "1809.07763", "submitter": "Alicja Gosiewska", "authors": "Alicja Gosiewska, Przemyslaw Biecek", "title": "auditor: an R Package for Model-Agnostic Visual Validation and\n  Diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have spread to almost every area of life. They are\nsuccessfully applied in biology, medicine, finance, physics, and other fields.\nWith modern software it is easy to train even a~complex model that fits the\ntraining data and results in high accuracy on the test set. The problem arises\nwhen models fail confronted with real-world data.\n  This paper describes methodology and tools for model-agnostic audit.\nIntroduced techniques facilitate assessing and comparing the goodness of fit\nand performance of models. In~addition, they may be used for the analysis of\nthe similarity of residuals and for identification of~outliers and influential\nobservations. The examination is carried out by diagnostic scores and visual\nverification.\n  Presented methods were implemented in the auditor package for R. Due to\nflexible and~consistent grammar, it is simple to validate models of any\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 19:14:46 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 06:28:36 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 18:01:43 GMT"}, {"version": "v4", "created": "Tue, 26 May 2020 15:15:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Gosiewska", "Alicja", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1809.07802", "submitter": "Mateusz Malinowski", "authors": "Julien Perolat and Mateusz Malinowski and Bilal Piot and Olivier\n  Pietquin", "title": "Playing the Game of Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning classifiers robust to universal adversarial\nperturbations. While prior work approaches this problem via robust\noptimization, adversarial training, or input transformation, we instead phrase\nit as a two-player zero-sum game. In this new formulation, both players\nsimultaneously play the same game, where one player chooses a classifier that\nminimizes a classification loss whilst the other player creates an adversarial\nperturbation that increases the same loss when applied to every sample in the\ntraining set. By observing that performing a classification (respectively\ncreating adversarial samples) is the best response to the other player, we\npropose a novel extension of a game-theoretic algorithm, namely fictitious\nplay, to the domain of training robust classifiers. Finally, we empirically\nshow the robustness and versatility of our approach in two defence scenarios\nwhere universal attacks are performed on several image classification datasets\n-- CIFAR10, CIFAR100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:48:36 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 20:16:45 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Perolat", "Julien", ""], ["Malinowski", "Mateusz", ""], ["Piot", "Bilal", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1809.07803", "submitter": "Axel Abels", "authors": "Axel Abels, Diederik M. Roijers, Tom Lenaerts, Ann Now\\'e, Denis\n  Steckelmacher", "title": "Dynamic Weights in Multi-Objective Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world decision problems are characterized by multiple conflicting\nobjectives which must be balanced based on their relative importance. In the\ndynamic weights setting the relative importance changes over time and\nspecialized algorithms that deal with such change, such as a tabular\nReinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are\nrequired. However, this earlier work is not feasible for RL settings that\nnecessitate the use of function approximators. We generalize across weight\nchanges and high-dimensional inputs by proposing a multi-objective Q-network\nwhose outputs are conditioned on the relative importance of objectives and we\nintroduce Diverse Experience Replay (DER) to counter the inherent\nnon-stationarity of the Dynamic Weights setting. We perform an extensive\nexperimental evaluation and compare our methods to adapted algorithms from Deep\nMulti-Task/Multi-Objective Reinforcement Learning and show that our proposed\nnetwork in combination with DER dominates these adapted algorithms across\nweight change scenarios and problem domains.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:52:15 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 14:51:55 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Abels", "Axel", ""], ["Roijers", "Diederik M.", ""], ["Lenaerts", "Tom", ""], ["Now\u00e9", "Ann", ""], ["Steckelmacher", "Denis", ""]]}, {"id": "1809.07806", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Deepta Rajan and Prasanna Sattigeri", "title": "Understanding Behavior of Clinical Models under Domain Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypothesis that computational models can be reliable enough to be adopted\nin prognosis and patient care is revolutionizing healthcare. Deep learning, in\nparticular, has been a game changer in building predictive models, thus leading\nto community-wide data curation efforts. However, due to inherent variabilities\nin population characteristics and biological systems, these models are often\nbiased to the training datasets. This can be limiting when models are deployed\nin new environments, when there are systematic domain shifts not known a\npriori. In this paper, we propose to emulate a large class of domain shifts,\nthat can occur in clinical settings, with a given dataset, and argue that\nevaluating the behavior of predictive models in light of those shifts is an\neffective way to quantify their reliability. More specifically, we develop an\napproach for building realistic scenarios, based on analysis of \\textit{disease\nlandscapes} in multi-label classification. Using the openly available MIMIC-III\nEHR dataset for phenotyping, for the first time, our work sheds light into data\nregimes where deep clinical models can fail to generalize. This work emphasizes\nthe need for novel validation mechanisms driven by real-world domain shifts in\nAI for healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:03:14 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 01:39:12 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Rajan", "Deepta", ""], ["Sattigeri", "Prasanna", ""]]}, {"id": "1809.07823", "submitter": "Mohammadhosein Hasanbeig", "authors": "Mohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening", "title": "Logically-Constrained Neural Fitted Q-Iteration", "comments": "AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for efficient training of Q-functions for\ncontinuous-state Markov Decision Processes (MDPs) such that the traces of the\nresulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a\nmodal logic, can express a wide range of time-dependent logical properties\n(including \"safety\") that are quite similar to patterns in natural language. We\nconvert the LTL property into a limit deterministic Buchi automaton and\nconstruct an on-the-fly synchronised product MDP. The control policy is then\nsynthesised by defining an adaptive reward function and by applying a modified\nneural fitted Q-iteration algorithm to the synchronised structure, assuming\nthat no prior knowledge is available from the original MDP. The proposed method\nis evaluated in a numerical study to test the quality of the generated control\npolicy and is compared with conventional methods for policy synthesis such as\nMDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted\nvalue iteration).\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:52:06 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 19:39:49 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2019 12:04:34 GMT"}, {"version": "v4", "created": "Thu, 14 Mar 2019 11:17:57 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Hasanbeig", "Mohammadhosein", ""], ["Abate", "Alessandro", ""], ["Kroening", "Daniel", ""]]}, {"id": "1809.07824", "submitter": "Yair Lakretz", "authors": "Yair Lakretz, Gal Chechik, Evan-Gary Cohen, Alessandro Treves, Naama\n  Friedmann", "title": "Metric Learning for Phoneme Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric functions for phoneme perception capture the similarity structure\namong phonemes in a given language and therefore play a central role in\nphonology and psycho-linguistics. Various phenomena depend on phoneme\nsimilarity, such as spoken word recognition or serial recall from verbal\nworking memory. This study presents a new framework for learning a metric\nfunction for perceptual distances among pairs of phonemes. Previous studies\nhave proposed various metric functions, from simple measures counting the\nnumber of phonetic dimensions that two phonemes share (place-,\nmanner-of-articulation and voicing), to more sophisticated ones such as\nderiving perceptual distances based on the number of natural classes that both\nphonemes belong to. However, previous studies have manually constructed the\nmetric function, which may lead to unsatisfactory account of the empirical\ndata. This study presents a framework to derive the metric function from\nbehavioral data on phoneme perception using learning algorithms. We first show\nthat this approach outperforms previous metrics suggested in the literature in\npredicting perceptual distances among phoneme pairs. We then study several\nmetric functions derived by the learning algorithms and show how perceptual\nsaliencies of phonological features can be derived from them. For English, we\nshow that the derived perceptual saliencies are in accordance with a previously\ndescribed order among phonological features and show how the framework extends\nthe results to more features. Finally, we explore how the metric function and\nperceptual saliencies of phonological features may vary across languages. To\nthis end, we compare results based on two English datasets and a new dataset\nthat we have collected for Hebrew.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:53:33 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Lakretz", "Yair", ""], ["Chechik", "Gal", ""], ["Cohen", "Evan-Gary", ""], ["Treves", "Alessandro", ""], ["Friedmann", "Naama", ""]]}, {"id": "1809.07828", "submitter": "Qinghan Xue", "authors": "Qinghan Xue, Xiaoran Wang, Samuel Meehan, Jilong Kuang, Alex Gao, Mooi\n  Choo Chuah", "title": "Recurrent Neural Networks based Obesity Status Prediction Using Activity\n  Data", "comments": "8 pages, 6 figures, ICMLA 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity is a serious public health concern world-wide, which increases the\nrisk of many diseases, including hypertension, stroke, and type 2 diabetes. To\ntackle this problem, researchers across the health ecosystem are collecting\ndiverse types of data, which includes biomedical, behavioral and activity, and\nutilizing machine learning techniques to mine hidden patterns for obesity\nstatus improvement prediction. While existing machine learning methods such as\nRecurrent Neural Networks (RNNs) can provide exceptional results, it is\nchallenging to discover hidden patterns of the sequential data due to the\nirregular observation time instances. Meanwhile, the lack of understanding of\nwhy those learning models are effective also limits further improvements on\ntheir architectures. Thus, in this work, we develop a RNN based time-aware\narchitecture to tackle the challenging problem of handling irregular\nobservation times and relevant feature extractions from longitudinal patient\nrecords for obesity status improvement prediction. To improve the prediction\nperformance, we train our model using two data sources: (i) electronic medical\nrecords containing information regarding lab tests, diagnoses, and\ndemographics; (ii) continuous activity data collected from popular wearables.\nEvaluations of real-world data demonstrate that our proposed method can capture\nthe underlying structures in users' time sequences with irregularities, and\nachieve an accuracy of 77-86% in predicting the obesity status improvement.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:55:22 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Xue", "Qinghan", ""], ["Wang", "Xiaoran", ""], ["Meehan", "Samuel", ""], ["Kuang", "Jilong", ""], ["Gao", "Alex", ""], ["Chuah", "Mooi Choo", ""]]}, {"id": "1809.07856", "submitter": "Nino Antulov-Fantulin", "authors": "Nino Antulov-Fantulin and Dijana Tolic and Matija Piskorec and Zhang\n  Ce and Irena Vodenska", "title": "Inferring short-term volatility indicators from Bitcoin blockchain", "comments": null, "journal-ref": "7th International Conference on Complex Networks and their\n  Applications 2018", "doi": "10.1007/978-3-030-05414-4_41", "report-no": null, "categories": "q-fin.ST cs.CE cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the possibility of inferring early warning indicators\n(EWIs) for periods of extreme bitcoin price volatility using features obtained\nfrom Bitcoin daily transaction graphs. We infer the low-dimensional\nrepresentations of transaction graphs in the time period from 2012 to 2017\nusing Bitcoin blockchain, and demonstrate how these representations can be used\nto predict extreme price volatility events. Our EWI, which is obtained with a\nnon-negative decomposition, contains more predictive information than those\nobtained with singular value decomposition or scalar value of the total Bitcoin\ntransaction volume.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:19:09 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Antulov-Fantulin", "Nino", ""], ["Tolic", "Dijana", ""], ["Piskorec", "Matija", ""], ["Ce", "Zhang", ""], ["Vodenska", "Irena", ""]]}, {"id": "1809.07861", "submitter": "Avraam Tsantekidis", "authors": "Paraskevi Nousi, Avraam Tsantekidis, Nikolaos Passalis, Adamantios\n  Ntakaris, Juho Kanniainen, Anastasios Tefas, Moncef Gabbouj, Alexandros\n  Iosifidis", "title": "Machine Learning for Forecasting Mid Price Movement using Limit Order\n  Book Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the movements of stock prices is one the most challenging\nproblems in financial markets analysis. In this paper, we use Machine Learning\n(ML) algorithms for the prediction of future price movements using limit order\nbook data. Two different sets of features are combined and evaluated:\nhandcrafted features based on the raw order book data and features extracted by\nML algorithms, resulting in feature vectors with highly variant\ndimensionalities. Three classifiers are evaluated using combinations of these\nsets of features on two different evaluation setups and three prediction\nscenarios. Even though the large scale and high frequency nature of the limit\norder book poses several challenges, the scope of the conducted experiments and\nthe significance of the experimental results indicate that Machine Learning\nhighly befits this task carving the path towards future research in this field.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:05:30 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 11:26:49 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nousi", "Paraskevi", ""], ["Tsantekidis", "Avraam", ""], ["Passalis", "Nikolaos", ""], ["Ntakaris", "Adamantios", ""], ["Kanniainen", "Juho", ""], ["Tefas", "Anastasios", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1809.07904", "submitter": "Dmitriy Korchev", "authors": "Dmitriy Korchev, Aruna Jammalamadaka, and Rajan Bhattacharyya", "title": "Automatic Rule Learning for Autonomous Driving Using Semantic Memory", "comments": "8 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic rule learning applicable\nto an autonomous driving system using real driving data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 01:02:48 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 20:08:42 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Korchev", "Dmitriy", ""], ["Jammalamadaka", "Aruna", ""], ["Bhattacharyya", "Rajan", ""]]}, {"id": "1809.07945", "submitter": "Kamel Alrashedy", "authors": "Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German, Venkatesh\n  Srinivasan and T. Aaron Gulliver", "title": "SCC: Automatic Classification of Code Snippets", "comments": null, "journal-ref": "Working Conference on Source Code Analysis & Manipulation 2018", "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the programming language of a source code file has been\nconsidered in the research community; it has been shown that Machine Learning\n(ML) and Natural Language Processing (NLP) algorithms can be effective in\nidentifying the programming language of source code files. However, determining\nthe programming language of a code snippet or a few lines of source code is\nstill a challenging task. Online forums such as Stack Overflow and code\nrepositories such as GitHub contain a large number of code snippets. In this\npaper, we describe Source Code Classification (SCC), a classifier that can\nidentify the programming language of code snippets written in 21 different\nprogramming languages. A Multinomial Naive Bayes (MNB) classifier is employed\nwhich is trained using Stack Overflow posts. It is shown to achieve an accuracy\nof 75% which is higher than that with Programming Languages Identification (PLI\na proprietary online classifier of snippets) whose accuracy is only 55.5%. The\naverage score for precision, recall and the F1 score with the proposed tool are\n0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code\nsnippets from a family of programming languages such as C, C++ and C#, and can\nalso identify the programming language version such as C# 3.0, C# 4.0 and C#\n5.0.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 04:50:40 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Alreshedy", "Kamel", ""], ["Dharmaretnam", "Dhanush", ""], ["German", "Daniel M.", ""], ["Srinivasan", "Venkatesh", ""], ["Gulliver", "T. Aaron", ""]]}, {"id": "1809.07952", "submitter": "Yusuke Tanaka", "authors": "Yusuke Tanaka, Tomoharu Iwata, Toshiyuki Tanaka, Takeshi Kurashima,\n  Maya Okawa, Hiroyuki Toda", "title": "Refining Coarse-grained Spatial Data using Auxiliary Spatial Data Sets\n  with Various Granularities", "comments": "Appears in Proceedings of the Thirty-Third AAAI Conference on\n  Artificial Intelligence (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model for refining coarse-grained spatial data by\nutilizing auxiliary spatial data sets. Existing methods require that the\nspatial granularities of the auxiliary data sets are the same as the desired\ngranularity of target data. The proposed model can effectively make use of\nauxiliary data sets with various granularities by hierarchically incorporating\nGaussian processes. With the proposed model, a distribution for each auxiliary\ndata set on the continuous space is modeled using a Gaussian process, where the\nrepresentation of uncertainty considers the levels of granularity. The\nfine-grained target data are modeled by another Gaussian process that considers\nboth the spatial correlation and the auxiliary data sets with their\nuncertainty. We integrate the Gaussian process with a spatial aggregation\nprocess that transforms the fine-grained target data into the coarse-grained\ntarget data, by which we can infer the fine-grained target Gaussian process\nfrom the coarse-grained data. Our model is designed such that the inference of\nmodel parameters based on the exact marginal likelihood is possible, in which\nthe variables of fine-grained target and auxiliary data are analytically\nintegrated out. Our experiments on real-world spatial data sets demonstrate the\neffectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 05:54:18 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 00:11:57 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tanaka", "Yusuke", ""], ["Iwata", "Tomoharu", ""], ["Tanaka", "Toshiyuki", ""], ["Kurashima", "Takeshi", ""], ["Okawa", "Maya", ""], ["Toda", "Hiroyuki", ""]]}, {"id": "1809.07954", "submitter": "Kamel Alrashedy", "authors": "Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German, Venkatesh\n  Srinivasan and T. Aaron Gulliver", "title": "Predicting the Programming Language of Questions and Snippets of\n  StackOverflow Using Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Overflow is the most popular Q&A website among software developers. As\na platform for knowledge sharing and acquisition, the questions posted in Stack\nOverflow usually contain a code snippet. Stack Overflow relies on users to\nproperly tag the programming language of a question and it simply assumes that\nthe programming language of the snippets inside a question is the same as the\ntag of the question itself. In this paper, we propose a classifier to predict\nthe programming language of questions posted in Stack Overflow using Natural\nLanguage Processing (NLP) and Machine Learning (ML). The classifier achieves an\naccuracy of 91.1% in predicting the 24 most popular programming languages by\ncombining features from the title, body and the code snippets of the question.\nWe also propose a classifier that only uses the title and body of the question\nand has an accuracy of 81.1%. Finally, we propose a classifier of code snippets\nonly that achieves an accuracy of 77.7%. These results show that deploying\nMachine Learning techniques on the combination of text and the code snippets of\na question provides the best performance. These results demonstrate also that\nit is possible to identify the programming language of a snippet of few lines\nof source code. We visualize the feature space of two programming languages\nJava and SQL in order to identify some special properties of information inside\nthe questions in Stack Overflow corresponding to these languages.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 06:02:25 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Alreshedy", "Kamel", ""], ["Dharmaretnam", "Dhanush", ""], ["German", "Daniel M.", ""], ["Srinivasan", "Venkatesh", ""], ["Gulliver", "T. Aaron", ""]]}, {"id": "1809.08031", "submitter": "Silvia Makowski", "authors": "Silvia Makowski, Lena J\\\"ager, Ahmed Abdelwahab, Niels Landwehr,\n  Tobias Scheffer", "title": "A Discriminative Model for Identifying Readers and Assessing Text\n  Comprehension from Eye Movements", "comments": "Proceedings of the European Conference on Machine Learning, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of inferring readers' identities and estimating their\nlevel of text comprehension from observations of their eye movements during\nreading. We develop a generative model of individual gaze patterns (scanpaths)\nthat makes use of lexical features of the fixated words. Using this generative\nmodel, we derive a Fisher-score representation of eye-movement sequences. We\nstudy whether a Fisher-SVM with this Fisher kernel and several reference\nmethods are able to identify readers and estimate their level of text\ncomprehension based on eye-tracking data. While none of the methods are able to\nestimate text comprehension accurately, we find that the SVM with Fisher kernel\nexcels at identifying readers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:46:21 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Makowski", "Silvia", ""], ["J\u00e4ger", "Lena", ""], ["Abdelwahab", "Ahmed", ""], ["Landwehr", "Niels", ""], ["Scheffer", "Tobias", ""]]}, {"id": "1809.08045", "submitter": "Dominik Dold", "authors": "Dominik Dold, Ilja Bytschok, Akos F. Kungl, Andreas Baumbach, Oliver\n  Breitwieser, Walter Senn, Johannes Schemmel, Karlheinz Meier, Mihai A.\n  Petrovici", "title": "Stochasticity from function -- why the Bayesian brain may need no noise", "comments": null, "journal-ref": "Neural Networks 119C (2019) pp. 200-213", "doi": "10.1016/j.neunet.2019.08.002", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing body of evidence suggests that the trial-to-trial variability\nof spiking activity in the brain is not mere noise, but rather the reflection\nof a sampling-based encoding scheme for probabilistic computing. Since the\nprecise statistical properties of neural activity are important in this\ncontext, many models assume an ad-hoc source of well-behaved, explicit noise,\neither on the input or on the output side of single neuron dynamics, most often\nassuming an independent Poisson process in either case. However, these\nassumptions are somewhat problematic: neighboring neurons tend to share\nreceptive fields, rendering both their input and their output correlated; at\nthe same time, neurons are known to behave largely deterministically, as a\nfunction of their membrane potential and conductance. We suggest that spiking\nneural networks may, in fact, have no need for noise to perform sampling-based\nBayesian inference. We study analytically the effect of auto- and\ncross-correlations in functionally Bayesian spiking networks and demonstrate\nhow their effect translates to synaptic interaction strengths, rendering them\ncontrollable through synaptic plasticity. This allows even small ensembles of\ninterconnected deterministic spiking networks to simultaneously and\nco-dependently shape their output activity through learning, enabling them to\nperform complex Bayesian computation without any need for noise, which we\ndemonstrate in silico, both in classical simulation and in neuromorphic\nemulation. These results close a gap between the abstract models and the\nbiology of functionally Bayesian spiking networks, effectively reducing the\narchitectural constraints imposed on physical neural substrates required to\nperform probabilistic computing, be they biological or artificial.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 11:37:14 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 15:53:49 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 12:58:19 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Dold", "Dominik", ""], ["Bytschok", "Ilja", ""], ["Kungl", "Akos F.", ""], ["Baumbach", "Andreas", ""], ["Breitwieser", "Oliver", ""], ["Senn", "Walter", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "1809.08067", "submitter": "Mostafa Tavassolipour", "authors": "Mostafa Tavassolipour, Seyed Abolfazl Motahari, and Mohammad-Taghi\n  Manzuri Shalmani", "title": "Learning of Tree-Structured Gaussian Graphical Models on Distributed\n  Data under Communication Constraints", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2876325", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, learning of tree-structured Gaussian graphical models from\ndistributed data is addressed. In our model, samples are stored in a set of\ndistributed machines where each machine has access to only a subset of\nfeatures. A central machine is then responsible for learning the structure\nbased on received messages from the other nodes. We present a set of\ncommunication efficient strategies, which are theoretically proved to convey\nsufficient information for reliable learning of the structure. In particular,\nour analyses show that even if each machine sends only the signs of its local\ndata samples to the central node, the tree structure can still be recovered\nwith high accuracy. Our simulation results on both synthetic and real-world\ndatasets show that our strategies achieve a desired accuracy in inferring the\nunderlying structure, while spending a small budget on communication.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 12:49:27 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Tavassolipour", "Mostafa", ""], ["Motahari", "Seyed Abolfazl", ""], ["Shalmani", "Mohammad-Taghi Manzuri", ""]]}, {"id": "1809.08079", "submitter": "Fei Jiang", "authors": "Fei Jiang and Lei Zheng and Jin Xu and Philip S. Yu", "title": "FI-GRL: Fast Inductive Graph Representation Learning via Projection-Cost\n  Preservation", "comments": "ICDM 2018, Full Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning aims at transforming graph data into meaningful\nlow-dimensional vectors to facilitate the employment of machine learning and\ndata mining algorithms designed for general data. Most current graph\nrepresentation learning approaches are transductive, which means that they\nrequire all the nodes in the graph are known when learning graph\nrepresentations and these approaches cannot naturally generalize to unseen\nnodes. In this paper, we present a Fast Inductive Graph Representation Learning\nframework (FI-GRL) to learn nodes' low-dimensional representations. Our\napproach can obtain accurate representations for seen nodes with provable\ntheoretical guarantees and can easily generalize to unseen nodes. Specifically,\nin order to explicitly decouple nodes' relations expressed by the graph, we\ntransform nodes into a randomized subspace spanned by a random projection\nmatrix. This stage is guaranteed to preserve the projection-cost of the\nnormalized random walk matrix which is highly related to the normalized cut of\nthe graph. Then feature extraction is achieved by conducting singular value\ndecomposition on the obtained matrix sketch. By leveraging the property of\nprojection-cost preservation on the matrix sketch, the obtained representation\nresult is nearly optimal. To deal with unseen nodes, we utilize folding-in\ntechnique to learn their meaningful representations. Empirically, when the\namount of seen nodes are larger than that of unseen nodes, FI-GRL always\nachieves excellent results. Our algorithm is fast, simple to implement and\ntheoretically guaranteed. Extensive experiments on real datasets demonstrate\nthe superiority of our algorithm on both efficacy and efficiency over both\nmacroscopic level (clustering) and microscopic level (structural hole\ndetection) applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:06:00 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Jiang", "Fei", ""], ["Zheng", "Lei", ""], ["Xu", "Jin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.08085", "submitter": "Gonzalo N\\'apoles", "authors": "Gonzalo N\\'apoles, Frank Vanhoenshoven, Koen Vanhoof", "title": "Short-term Cognitive Networks, Flexible Reasoning and Nonsynaptic\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the machine learning literature dedicated to fully automated reasoning\nalgorithms is abundant, the number of methods enabling the inference process on\nthe basis of previously defined knowledge structures is scanter. Fuzzy\nCognitive Maps (FCMs) are neural networks that can be exploited towards this\ngoal because of their flexibility to handle external knowledge. However, FCMs\nsuffer from a number of issues that range from the limited prediction horizon\nto the absence of theoretically sound learning algorithms able to produce\naccurate predictions. In this paper, we propose a neural network system named\nShort-term Cognitive Networks that tackle some of these limitations. In our\nmodel weights are not constricted and may have a causal nature or not. As a\nsecond contribution, we present a nonsynaptic learning algorithm to improve the\nnetwork performance without modifying the previously defined weights. Moreover,\nwe derive a stop condition to prevent the learning algorithm from iterating\nwithout decreasing the simulation error.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:02:30 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["N\u00e1poles", "Gonzalo", ""], ["Vanhoenshoven", "Frank", ""], ["Vanhoof", "Koen", ""]]}, {"id": "1809.08092", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Songtao Lu, Renchang Dai, Guangyi Liu, Zhiwei Wang", "title": "Power Market Price Forecasting via Deep Learning", "comments": "This manuscript has been accepted by the incoming conference IECON\n  2018 at Washington DC, USA, Oct. 21-23, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study on power market price forecasting by deep learning is presented. As\none of the most successful deep learning frameworks, the LSTM (Long short-term\nmemory) neural network is utilized. The hourly prices data from the New England\nand PJM day-ahead markets are used in this study. First, a LSTM network is\nformulated and trained. Then the raw input and output data are preprocessed by\nunit scaling, and the trained network is tested on the real price data under\ndifferent input lengths, forecasting horizons and data sizes. Its performance\nis also compared with other existing methods. The forecasted results\ndemonstrate that, the LSTM deep neural network can outperform the others under\ndifferent application settings in this problem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 19:00:18 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 13:49:04 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhu", "Yongli", ""], ["Lu", "Songtao", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.08097", "submitter": "Amar Prakash Azad", "authors": "Amar Prakash Azad, Dinesh Garg, Priyanka Agrawal, Arun Kumar", "title": "Deep Domain Adaptation under Deep Label Scarcity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal behind Domain Adaptation (DA) is to leverage the labeled examples\nfrom a source domain so as to infer an accurate model in a target domain where\nlabels are not available or in scarce at the best. A state-of-the-art approach\nfor the DA is due to (Ganin et al. 2016), known as DANN, where they attempt to\ninduce a common representation of source and target domains via adversarial\ntraining. This approach requires a large number of labeled examples from the\nsource domain to be able to infer a good model for the target domain. However,\nin many situations obtaining labels in the source domain is expensive which\nresults in deteriorated performance of DANN and limits its applicability in\nsuch scenarios. In this paper, we propose a novel approach to overcome this\nlimitation. In our work, we first establish that DANN reduces the original DA\nproblem into a semi-supervised learning problem over the space of common\nrepresentation. Next, we propose a learning approach, namely TransDANN, that\namalgamates adversarial learning and transductive learning to mitigate the\ndetrimental impact of limited source labels and yields improved performance.\nExperimental results (both on text and images) show a significant boost in the\nperformance of TransDANN over DANN under such scenarios. We also provide\ntheoretical justification for the performance boost.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 12:32:47 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Azad", "Amar Prakash", ""], ["Garg", "Dinesh", ""], ["Agrawal", "Priyanka", ""], ["Kumar", "Arun", ""]]}, {"id": "1809.08098", "submitter": "Shiqi Wang", "authors": "Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana", "title": "Efficient Formal Safety Analysis of Neural Networks", "comments": "Accepted to NIPS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are increasingly deployed in real-world safety-critical\ndomains such as autonomous driving, aircraft collision avoidance, and malware\ndetection. However, these networks have been shown to often mispredict on\ninputs with minor adversarial or even accidental perturbations. Consequences of\nsuch errors can be disastrous and even potentially fatal as shown by the recent\nTesla autopilot crash. Thus, there is an urgent need for formal analysis\nsystems that can rigorously check neural networks for violations of different\nsafety properties such as robustness against adversarial perturbations within a\ncertain $L$-norm of a given image. An effective safety analysis system for a\nneural network must be able to either ensure that a safety property is\nsatisfied by the network or find a counterexample, i.e., an input for which the\nnetwork will violate the property. Unfortunately, most existing techniques for\nperforming such analysis struggle to scale beyond very small networks and the\nones that can scale to larger networks suffer from high false positives and\ncannot produce concrete counterexamples in case of a property violation. In\nthis paper, we present a new efficient approach for rigorously checking\ndifferent safety properties of neural networks that significantly outperforms\nexisting approaches by multiple orders of magnitude. Our approach can check\ndifferent safety properties and find concrete counterexamples for networks that\nare 10$\\times$ larger than the ones supported by existing analysis techniques.\nWe believe that our approach to estimating tight output bounds of a network for\na given input range can also help improve the explainability of neural networks\nand guide the training process of more robust neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:21:28 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 02:29:30 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 22:30:38 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Wang", "Shiqi", ""], ["Pei", "Kexin", ""], ["Whitehouse", "Justin", ""], ["Yang", "Junfeng", ""], ["Jana", "Suman", ""]]}, {"id": "1809.08106", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "title": "Distribution Networks for Open Set Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open set learning, a model must be able to generalize to novel classes\nwhen it encounters a sample that does not belong to any of the classes it has\nseen before. Open set learning poses a realistic learning scenario that is\nreceiving growing attention. Existing studies on open set learning mainly\nfocused on detecting novel classes, but few studies tried to model them for\ndifferentiating novel classes. In this paper, we recognize that novel classes\nshould be different from each other, and propose distribution networks for open\nset learning that can model different novel classes based on probability\ndistributions. We hypothesize that, through a certain mapping, samples from\ndifferent classes with the same classification criterion should follow\ndifferent probability distributions from the same distribution family. A deep\nneural network is learned to map the samples in the original feature space to a\nlatent space where the distributions of known classes can be jointly learned\nwith the network. We additionally propose a distribution parameter transfer and\nupdating strategy for novel class modeling when a novel class is detected in\nthe latent space. By novel class modeling, the detected novel classes can serve\nas known classes to the subsequent classification. Our experimental results on\nimage datasets MNIST and CIFAR10 show that the distribution networks can detect\nnovel classes accurately, and model them well for the subsequent classification\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 00:06:56 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 01:13:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mao", "Chengsheng", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1809.08113", "submitter": "Yong Zhang", "authors": "Yong Zhang, Yu Zhang, Zhao Zhang, Jie Bao, Yunpeng Song", "title": "Human activity recognition based on time series analysis using U-Net", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional human activity recognition (HAR) based on time series adopts\nsliding window analysis method. This method faces the multi-class window\nproblem which mistakenly labels different classes of sampling points within a\nwindow as a class. In this paper, a HAR algorithm based on U-Net is proposed to\nperform activity labeling and prediction at each sampling point. The activity\ndata of the triaxial accelerometer is mapped into an image with the single\npixel column and multi-channel which is input into the U-Net network for\ntraining and recognition. Our proposal can complete the pixel-level gesture\nrecognition function. The method does not need manual feature extraction and\ncan effectively identify short-term behaviors in long-term activity sequences.\nWe collected the Sanitation dataset and tested the proposed scheme with four\nopen data sets. The experimental results show that compared with Support Vector\nMachine (SVM), k-Nearest Neighbor (kNN), Decision Tree(DT), Quadratic\nDiscriminant Analysis (QDA), Convolutional Neural Network (CNN) and Fully\nConvolutional Networks (FCN) methods, our proposal has the highest accuracy and\nF1-socre in each dataset, and has stable performance and high robustness. At\nthe same time, after the U-Net has finished training, our proposal can achieve\nfast enough recognition speed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:16:33 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Zhang", "Yong", ""], ["Zhang", "Yu", ""], ["Zhang", "Zhao", ""], ["Bao", "Jie", ""], ["Song", "Yunpeng", ""]]}, {"id": "1809.08151", "submitter": "Etienne Boursier", "authors": "Etienne Boursier and Vianney Perchet", "title": "SIC-MMAB: Synchronisation Involves Communication in Multiplayer\n  Multi-Armed Bandits", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by cognitive radio networks, we consider the stochastic multiplayer\nmulti-armed bandit problem, where several players pull arms simultaneously and\ncollisions occur if one of them is pulled by several players at the same stage.\nWe present a decentralized algorithm that achieves the same performance as a\ncentralized one, contradicting the existing lower bounds for that problem. This\nis possible by \"hacking\" the standard model by constructing a communication\nprotocol between players that deliberately enforces collisions, allowing them\nto share their information at a negligible cost. This motivates the\nintroduction of a more appropriate dynamic setting without sensing, where\nsimilar communication protocols are no longer possible. However, we show that\nthe logarithmic growth of the regret is still achievable for this model with a\nnew algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:43:46 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 10:30:41 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 09:28:36 GMT"}, {"version": "v4", "created": "Tue, 19 Nov 2019 09:51:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Boursier", "Etienne", ""], ["Perchet", "Vianney", ""]]}, {"id": "1809.08159", "submitter": "Keiichi Kisamori", "authors": "Keiichi Kisamori, Motonobu Kanagawa, Keisuke Yamazaki", "title": "Simulator Calibration under Covariate Shift with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel calibration method for computer simulators, dealing with\nthe problem of covariate shift. Covariate shift is the situation where input\ndistributions for training and test are different, and ubiquitous in\napplications of simulations. Our approach is based on Bayesian inference with\nkernel mean embedding of distributions, and on the use of an\nimportance-weighted reproducing kernel for covariate shift adaptation. We\nprovide a theoretical analysis for the proposed method, including a novel\ntheoretical result for conditional mean embedding, as well as empirical\ninvestigations suggesting its effectiveness in practice. The experiments\ninclude calibration of a widely used simulator for industrial manufacturing\nprocesses, where we also demonstrate how the proposed method may be useful for\nsensitivity analysis of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:51:39 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 06:24:52 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 12:28:43 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2020 03:18:24 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kisamori", "Keiichi", ""], ["Kanagawa", "Motonobu", ""], ["Yamazaki", "Keisuke", ""]]}, {"id": "1809.08196", "submitter": "Xiongfeng Yan", "authors": "Xiongfeng Yan and Tinghua Ai", "title": "Analysis of Irregular Spatial Data with Machine Learning: Classification\n  of Building Patterns with a Graph Convolutional Neural Network", "comments": "7 pages, 6 figures, GIScience 2018", "journal-ref": null, "doi": "10.4230/LIPIcs.GIScience.2018.69", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods such as convolutional neural networks (CNNs) are\nbecoming an integral part of scientific research in many disciplines, spatial\nvector data often fail to be analyzed using these powerful learning methods\nbecause of its irregularities. With the aid of graph Fourier transform and\nconvolution theorem, it is possible to convert the convolution as a point-wise\nproduct in Fourier domain and construct a learning architecture of CNN on graph\nfor the analysis task of irregular spatial data. In this study, we used the\nclassification task of building patterns as a case study to test this method,\nand experiments showed that this method has achieved outstanding results in\nidentifying regular and irregular patterns, and has significantly improved in\ncomparing with other methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 16:37:24 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Yan", "Xiongfeng", ""], ["Ai", "Tinghua", ""]]}, {"id": "1809.08204", "submitter": "Yuan Cao", "authors": "Yuan Cao, Matey Neykov, Han Liu", "title": "High-Temperature Structure Detection in Ferromagnets", "comments": "51 pages, 4 figures. version 2: a new computational lower bound\n  result is added. version 3: citations are updated", "journal-ref": "Information and Inference: A Journal of the IMA (2020)", "doi": "10.1093/imaiai/iaaa032", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies structure detection problems in high temperature\nferromagnetic (positive interaction only) Ising models. The goal is to\ndistinguish whether the underlying graph is empty, i.e., the model consists of\nindependent Rademacher variables, versus the alternative that the underlying\ngraph contains a subgraph of a certain structure. We give matching upper and\nlower minimax bounds under which testing this problem is possible/impossible\nrespectively. Our results reveal that a key quantity called graph arboricity\ndrives the testability of the problem. On the computational front, under a\nconjecture of the computational hardness of sparse principal component\nanalysis, we prove that, unless the signal is strong enough, there are no\npolynomial time tests which are capable of testing this problem. In order to\nprove this result we exhibit a way to give sharp inequalities for the even\nmoments of sums of i.i.d. Rademacher random variables which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 16:49:23 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 09:02:50 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 03:47:46 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 18:28:44 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cao", "Yuan", ""], ["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1809.08327", "submitter": "Dongkun Zhang", "authors": "Dongkun Zhang, Lu Lu, Ling Guo, George Em Karniadakis", "title": "Quantifying total uncertainty in physics-informed neural networks for\n  solving forward and inverse stochastic problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.07.048", "report-no": null, "categories": "math.AP physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) have recently emerged as an\nalternative way of solving partial differential equations (PDEs) without the\nneed of building elaborate grids, instead, using a straightforward\nimplementation. In particular, in addition to the deep neural network (DNN) for\nthe solution, a second DNN is considered that represents the residual of the\nPDE. The residual is then combined with the mismatch in the given data of the\nsolution in order to formulate the loss function. This framework is effective\nbut is lacking uncertainty quantification of the solution due to the inherent\nrandomness in the data or due to the approximation limitations of the DNN\narchitecture. Here, we propose a new method with the objective of endowing the\nDNN with uncertainty quantification for both sources of uncertainty, i.e., the\nparametric uncertainty and the approximation uncertainty. We first account for\nthe parametric uncertainty when the parameter in the differential equation is\nrepresented as a stochastic process. Multiple DNNs are designed to learn the\nmodal functions of the arbitrary polynomial chaos (aPC) expansion of its\nsolution by using stochastic data from sparse sensors. We can then make\npredictions from new sensor measurements very efficiently with the trained\nDNNs. Moreover, we employ dropout to correct the over-fitting and also to\nquantify the uncertainty of DNNs in approximating the modal functions. We then\ndesign an active learning strategy based on the dropout uncertainty to place\nnew sensors in the domain to improve the predictions of DNNs. Several numerical\ntests are conducted for both the forward and the inverse problems to quantify\nthe effectiveness of PINNs combined with uncertainty quantification. This\nNN-aPC new paradigm of physics-informed deep learning with uncertainty\nquantification can be readily applied to other types of stochastic PDEs in\nmulti-dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 21:51:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Dongkun", ""], ["Lu", "Lu", ""], ["Guo", "Ling", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1809.08336", "submitter": "Konstantina Christakopoulou", "authors": "Konstantina Christakopoulou, Arindam Banerjee", "title": "Adversarial Recommendation: Attack of the Learned Fake Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can machine learning models for recommendation be easily fooled? While the\nquestion has been answered for hand-engineered fake user profiles, it has not\nbeen explored for machine learned adversarial attacks. This paper attempts to\nclose this gap.\n  We propose a framework for generating fake user profiles which, when\nincorporated in the training of a recommendation system, can achieve an\nadversarial intent, while remaining indistinguishable from real user profiles.\nWe formulate this procedure as a repeated general-sum game between two players:\nan oblivious recommendation system $R$ and an adversarial fake user generator\n$A$ with two goals: (G1) the rating distribution of the fake users needs to be\nclose to the real users, and (G2) some objective $f_A$ encoding the attack\nintent, such as targeting the top-K recommendation quality of $R$ for a subset\nof users, needs to be optimized. We propose a learning framework to achieve\nboth goals, and offer extensive experiments considering multiple types of\nattacks highlighting the vulnerability of recommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 23:00:39 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Christakopoulou", "Konstantina", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1809.08343", "submitter": "Nicholas Mattei", "authors": "Ritesh Noothigattu, Djallel Bouneffouf, Nicholas Mattei, Rachita\n  Chandra, Piyush Madan, Kush Varshney, Murray Campbell, Moninder Singh,\n  Francesca Rossi", "title": "Interpretable Multi-Objective Reinforcement Learning through Policy\n  Orchestration", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous cyber-physical agents and systems play an increasingly large role\nin our lives. To ensure that agents behave in ways aligned with the values of\nthe societies in which they operate, we must develop techniques that allow\nthese agents to not only maximize their reward in an environment, but also to\nlearn and follow the implicit constraints of society. These constraints and\nnorms can come from any number of sources including regulations, business\nprocess guidelines, laws, ethical principles, social norms, and moral values.\nWe detail a novel approach that uses inverse reinforcement learning to learn a\nset of unspecified constraints from demonstrations of the task, and\nreinforcement learning to learn to maximize the environment rewards. More\nprecisely, we assume that an agent can observe traces of behavior of members of\nthe society but has no access to the explicit set of constraints that give rise\nto the observed behavior. Inverse reinforcement learning is used to learn such\nconstraints, that are then combined with a possibly orthogonal value function\nthrough the use of a contextual bandit-based orchestrator that picks a\ncontextually-appropriate choice between the two policies (constraint-based and\nenvironment reward-based) when taking actions. The contextual bandit\norchestrator allows the agent to mix policies in novel ways, taking the best\nactions from either a reward maximizing or constrained policy. In addition, the\norchestrator is transparent on which policy is being employed at each time\nstep. We test our algorithms using a Pac-Man domain and show that the agent is\nable to learn to act optimally, act within the demonstrated constraints, and\nmix these two functions in complex ways.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 23:38:17 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Noothigattu", "Ritesh", ""], ["Bouneffouf", "Djallel", ""], ["Mattei", "Nicholas", ""], ["Chandra", "Rachita", ""], ["Madan", "Piyush", ""], ["Varshney", "Kush", ""], ["Campbell", "Murray", ""], ["Singh", "Moninder", ""], ["Rossi", "Francesca", ""]]}, {"id": "1809.08346", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar, Mohammad Saeed Abrishami, David Eigen, Massoud\n  Pedram", "title": "A Meta-Learning Approach for Custom Model Training", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer-learning and meta-learning are two effective methods to apply\nknowledge learned from large data sources to new tasks. In few-class, few-shot\ntarget task settings (i.e. when there are only a few classes and training\nexamples available in the target task), meta-learning approaches that optimize\nfor future task learning have outperformed the typical transfer approach of\ninitializing model weights from a pre-trained starting point. But as we\nexperimentally show, meta-learning algorithms that work well in the few-class\nsetting do not generalize well in many-shot and many-class cases. In this\npaper, we propose a joint training approach that combines both\ntransfer-learning and meta-learning. Benefiting from the advantages of each,\nour method obtains improved generalization performance on unseen target tasks\nin both few- and many-class and few- and many-shot scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 23:47:34 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 04:32:50 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Abrishami", "Mohammad Saeed", ""], ["Eigen", "David", ""], ["Pedram", "Massoud", ""]]}, {"id": "1809.08350", "submitter": "Nicholas Mattei", "authors": "Andrea Loreggia, Nicholas Mattei, Francesca Rossi, K. Brent Venable", "title": "CPMetric: Deep Siamese Networks for Learning Distances Between\n  Structured Preferences", "comments": null, "journal-ref": "Artificial Intelligence. IJCAI 2019 International Workshops. IJCAI\n  2019. Lecture Notes in Computer Science, vol 12158. Springer, Cham", "doi": "10.1007/978-3-030-56150-5_11", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference are central to decision making by both machines and humans.\nRepresenting, learning, and reasoning with preferences is an important area of\nstudy both within computer science and across the sciences. When working with\npreferences it is necessary to understand and compute the distance between sets\nof objects, e.g., the preferences of a user and a the descriptions of objects\nto be recommended. We present CPDist, a novel neural network to address the\nproblem of learning to measure the distance between structured preference\nrepresentations. We use the popular CP-net formalism to represent preferences\nand then leverage deep neural networks to learn a recently proposed metric\nfunction that is computationally hard to compute directly. CPDist is a novel\nmetric learning approach based on the use of deep siamese networks which learn\nthe Kendal Tau distance between partial orders that are induced by compact\npreference representations. We find that CPDist is able to learn the distance\nfunction with high accuracy and outperform existing approximation algorithms on\nboth the regression and classification task using less computation time.\nPerformance remains good even when CPDist is trained with only a small number\nof samples compared to the dimension of the solution space, indicating the\nnetwork generalizes well.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 23:56:53 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 19:47:20 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Loreggia", "Andrea", ""], ["Mattei", "Nicholas", ""], ["Rossi", "Francesca", ""], ["Venable", "K. Brent", ""]]}, {"id": "1809.08352", "submitter": "Tom B Brown", "authors": "Tom B. Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul\n  Christiano, Ian Goodfellow", "title": "Unrestricted Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two-player contest for evaluating the safety and robustness of\nmachine learning systems, with a large prize pool. Unlike most prior work in ML\nrobustness, which studies norm-constrained adversaries, we shift our focus to\nunconstrained adversaries. Defenders submit machine learning models, and try to\nachieve high accuracy and coverage on non-adversarial data while making no\nconfident mistakes on adversarial inputs. Attackers try to subvert defenses by\nfinding arbitrary unambiguous inputs where the model assigns an incorrect label\nwith high confidence. We propose a simple unambiguous dataset (\"bird-or-\nbicycle\") to use as part of this contest. We hope this contest will help to\nmore comprehensively evaluate the worst-case adversarial risk of machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 00:16:18 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Brown", "Tom B.", ""], ["Carlini", "Nicholas", ""], ["Zhang", "Chiyuan", ""], ["Olsson", "Catherine", ""], ["Christiano", "Paul", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1809.08353", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Ahmed S. Zamzam, Georgios B. Giannakis, and\n  Nicholas D. Sidiropoulos", "title": "Coupled Graphs and Tensor Factorization for Recommender Systems and\n  Community Detection", "comments": "This paper is submitted to the IEEE Transactions on Knowledge and\n  Data Engineering. A preliminary version of this work was accepted for\n  presentation in the special track of GlobalSIP on Tensor Methods for Signal\n  Processing and Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint analysis of data from multiple information repositories facilitates\nuncovering the underlying structure in heterogeneous datasets. Single and\ncoupled matrix-tensor factorization (CMTF) has been widely used in this context\nfor imputation-based recommendation from ratings, social network, and other\nuser-item data. When this side information is in the form of item-item\ncorrelation matrices or graphs, existing CMTF algorithms may fall short.\nAlleviating current limitations, we introduce a novel model coined coupled\ngraph-tensor factorization (CGTF) that judiciously accounts for graph-related\nside information. The CGTF model has the potential to overcome practical\nchallenges, such as missing slabs from the tensor and/or missing rows/columns\nfrom the correlation matrices. A novel alternating direction method of\nmultipliers (ADMM) is also developed that recovers the nonnegative factors of\nCGTF. Our algorithm enjoys closed-form updates that result in reduced\ncomputational complexity and allow for convergence claims. A novel direction is\nfurther explored by employing the interpretable factors to detect graph\ncommunities having the tensor as side information. The resulting community\ndetection approach is successful even when some links in the graphs are\nmissing. Results with real data sets corroborate the merits of the proposed\nmethods relative to state-of-the-art competing factorization techniques in\nproviding recommendations and detecting communities.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 00:25:20 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:22:27 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Zamzam", "Ahmed S.", ""], ["Giannakis", "Georgios B.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1809.08360", "submitter": "Xiuqing Wei", "authors": "Haiqing Wei, Gang Huang, Xiuqing Wei, Yanlong Sun, Hongbin Wang", "title": "Comment on \"All-optical machine learning using diffractive deep neural\n  networks\"", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lin et al. (Reports, 7 September 2018, p. 1004) reported a remarkable\nproposal that employs a passive, strictly linear optical setup to perform\npattern classifications. But interpreting the multilayer diffractive setup as a\ndeep neural network and advocating it as an all-optical deep learning framework\nare not well justified and represent a mischaracterization of the system by\noverlooking its defining characteristics of perfect linearity and strict\npassivity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 01:06:16 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 04:52:11 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wei", "Haiqing", ""], ["Huang", "Gang", ""], ["Wei", "Xiuqing", ""], ["Sun", "Yanlong", ""], ["Wang", "Hongbin", ""]]}, {"id": "1809.08400", "submitter": "Kenan Cui", "authors": "Kenan Cui and Xu Chen and Jiangchao Yao and Ya Zhang", "title": "Variational Collaborative Learning for User Probabilistic Representation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) has been successfully employed by many modern\nrecommender systems. Conventional CF-based methods use the user-item\ninteraction data as the sole information source to recommend items to users.\nHowever, CF-based methods are known for suffering from cold start problems and\ndata sparsity problems. Hybrid models that utilize auxiliary information on top\nof interaction data have increasingly gained attention. A few \"collaborative\nlearning\"-based models, which tightly bridges two heterogeneous learners\nthrough mutual regularization, are recently proposed for the hybrid\nrecommendation. However, the \"collaboration\" in the existing methods are\nactually asynchronous due to the alternative optimization of the two learners.\nLeveraging the recent advances in variational autoencoder~(VAE), we here\npropose a model consisting of two streams of mutual linked VAEs, named\nvariational collaborative model (VCM). Unlike the mutual regularization used in\nprevious works where two learners are optimized asynchronously, VCM enables a\nsynchronous collaborative learning mechanism. Besides, the two stream VAEs\nsetup allows VCM to fully leverages the Bayesian probabilistic representations\nin collaborative learning. Extensive experiments on three real-life datasets\nhave shown that VCM outperforms several state-of-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 07:38:30 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Cui", "Kenan", ""], ["Chen", "Xu", ""], ["Yao", "Jiangchao", ""], ["Zhang", "Ya", ""]]}, {"id": "1809.08427", "submitter": "Lewis Mitchell", "authors": "Jonathan Tuke, Andrew Nguyen, Mehwish Nasim, Drew Mellor, Asanga\n  Wickramasinghe, Nigel Bean, Lewis Mitchell", "title": "Pachinko Prediction: A Bayesian method for event prediction from social\n  media data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of large open data sources with machine learning approaches\npresents a potentially powerful way to predict events such as protest or social\nunrest. However, accounting for uncertainty in such models, particularly when\nusing diverse, unstructured datasets such as social media, is essential to\nguarantee the appropriate use of such methods. Here we develop a Bayesian\nmethod for predicting social unrest events in Australia using social media\ndata. This method uses machine learning methods to classify individual postings\nto social media as being relevant, and an empirical Bayesian approach to\ncalculate posterior event probabilities. We use the method to predict events in\nAustralian cities over a period in 2017/18.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 11:40:22 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Tuke", "Jonathan", ""], ["Nguyen", "Andrew", ""], ["Nasim", "Mehwish", ""], ["Mellor", "Drew", ""], ["Wickramasinghe", "Asanga", ""], ["Bean", "Nigel", ""], ["Mitchell", "Lewis", ""]]}, {"id": "1809.08438", "submitter": "Ravi Kiran Raman", "authors": "Ravi Kiran Raman, Roman Vaculin, Michael Hind, Sekou L. Remy,\n  Eleftheria K. Pissadaki, Nelson Kibichii Bore, Roozbeh Daneshvar, Biplav\n  Srivastava, Kush R. Varshney", "title": "Trusted Multi-Party Computation and Verifiable Simulations: A Scalable\n  Blockchain Approach", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale computational experiments, often running over weeks and over\nlarge datasets, are used extensively in fields such as epidemiology,\nmeteorology, computational biology, and healthcare to understand phenomena, and\ndesign high-stakes policies affecting everyday health and economy. For\ninstance, the OpenMalaria framework is a computationally-intensive simulation\nused by various non-governmental and governmental agencies to understand\nmalarial disease spread and effectiveness of intervention strategies, and\nsubsequently design healthcare policies. Given that such shared results form\nthe basis of inferences drawn, technological solutions designed, and day-to-day\npolicies drafted, it is essential that the computations are validated and\ntrusted. In particular, in a multi-agent environment involving several\nindependent computing agents, a notion of trust in results generated by peers\nis critical in facilitating transparency, accountability, and collaboration.\nUsing a novel combination of distributed validation of atomic computation\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\nuniversal framework for distributed trust in computations. In particular we\naddress the scalaibility problem by reducing the storage and communication\ncosts using a lossy compression scheme. This framework guarantees not only\nverifiability of final results, but also the validity of local computations,\nand its cost-benefit tradeoffs are studied using a synthetic example of\ntraining a neural network.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 14:03:06 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Raman", "Ravi Kiran", ""], ["Vaculin", "Roman", ""], ["Hind", "Michael", ""], ["Remy", "Sekou L.", ""], ["Pissadaki", "Eleftheria K.", ""], ["Bore", "Nelson Kibichii", ""], ["Daneshvar", "Roozbeh", ""], ["Srivastava", "Biplav", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1809.08510", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan, Xia Song, Saurabh Tiwary", "title": "Towards Language Agnostic Universal Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a bilingual student learns to solve word problems in math, we expect the\nstudent to be able to solve these problem in both languages the student is\nfluent in,even if the math lessons were only taught in one language. However,\ncurrent representations in machine learning are language dependent. In this\nwork, we present a method to decouple the language from the problem by learning\nlanguage agnostic representations and therefore allowing training a model in\none language and applying to a different one in a zero shot fashion. We learn\nthese representations by taking inspiration from linguistics and formalizing\nUniversal Grammar as an optimization process (Chomsky, 2014; Montague, 1970).\nWe demonstrate the capabilities of these representations by showing that the\nmodels trained on a single language using language agnostic representations\nachieve very similar accuracies in other languages.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 01:55:46 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Aghajanyan", "Armen", ""], ["Song", "Xia", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "1809.08516", "submitter": "Bao Wang", "authors": "Bao Wang, Alex T. Lin, Wei Zhu, Penghang Yin, Andrea L. Bertozzi,\n  Stanley J. Osher", "title": "Adversarial Defense via Data Dependent Activation Function and Total\n  Variation Minimization", "comments": "17 pages, 6 figures", "journal-ref": "Inverse Problems and Imaging, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the robustness of Deep Neural Net (DNN) to adversarial attacks by\nusing an interpolating function as the output activation. This data-dependent\nactivation remarkably improves both the generalization and robustness of DNN.\nIn the CIFAR10 benchmark, we raise the robust accuracy of the adversarially\ntrained ResNet20 from $\\sim 46\\%$ to $\\sim 69\\%$ under the state-of-the-art\nIterative Fast Gradient Sign Method (IFGSM) based adversarial attack. When we\ncombine this data-dependent activation with total variation minimization on\nadversarial images and training data augmentation, we achieve an improvement in\nrobust accuracy by 38.9$\\%$ for ResNet56 under the strongest IFGSM attack.\nFurthermore, We provide an intuitive explanation of our defense by analyzing\nthe geometry of the feature space.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 02:33:31 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 17:54:21 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 07:05:16 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wang", "Bao", ""], ["Lin", "Alex T.", ""], ["Zhu", "Wei", ""], ["Yin", "Penghang", ""], ["Bertozzi", "Andrea L.", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1809.08530", "submitter": "Jason Lee", "authors": "Sham Kakade and Jason D. Lee", "title": "Provably Correct Automatic Subdifferentiation for Qualified Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cheap Gradient Principle (Griewank 2008) --- the computational cost of\ncomputing the gradient of a scalar-valued function is nearly the same (often\nwithin a factor of $5$) as that of simply computing the function itself --- is\nof central importance in optimization; it allows us to quickly obtain (high\ndimensional) gradients of scalar loss functions which are subsequently used in\nblack box gradient-based optimization procedures. The current state of affairs\nis markedly different with regards to computing subderivatives: widely used ML\nlibraries, including TensorFlow and PyTorch, do not correctly compute\n(generalized) subderivatives even on simple examples. This work considers the\nquestion: is there a Cheap Subgradient Principle? Our main result shows that,\nunder certain restrictions on our library of nonsmooth functions (standard in\nnonlinear programming), provably correct generalized subderivatives can be\ncomputed at a computational cost that is within a (dimension-free) factor of\n$6$ of the cost of computing the scalar function itself.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 04:22:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 06:09:08 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kakade", "Sham", ""], ["Lee", "Jason D.", ""]]}, {"id": "1809.08541", "submitter": "Jianzhe Lin", "authors": "Jianzhe Lin, Qi Wang, Rabab Ward, Z. Jane Wang", "title": "DT-LET: Deep Transfer Learning by Exploring where to Transfer", "comments": "Conference paper submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous transfer learning methods based on deep network assume the knowledge\nshould be transferred between the same hidden layers of the source domain and\nthe target domains. This assumption doesn't always hold true, especially when\nthe data from the two domains are heterogeneous with different resolutions. In\nsuch case, the most suitable numbers of layers for the source domain data and\nthe target domain data would differ. As a result, the high level knowledge from\nthe source domain would be transferred to the wrong layer of target domain.\nBased on this observation, \"where to transfer\" proposed in this paper should be\na novel research frontier. We propose a new mathematic model named DT-LET to\nsolve this heterogeneous transfer learning problem. In order to select the best\nmatching of layers to transfer knowledge, we define specific loss function to\nestimate the corresponding relationship between high-level features of data in\nthe source domain and the target domain. To verify this proposed cross-layer\nmodel, experiments for two cross-domain recognition/classification tasks are\nconducted, and the achieved superior results demonstrate the necessity of layer\ncorrespondence searching.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 06:06:01 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Lin", "Jianzhe", ""], ["Wang", "Qi", ""], ["Ward", "Rabab", ""], ["Wang", "Z. Jane", ""]]}, {"id": "1809.08560", "submitter": "Shoubo Hu", "authors": "Shoubo Hu, Zhitang Chen, Laiwan Chan", "title": "A Kernel Embedding-based Approach for Nonstationary Causal Model\n  Inference", "comments": "Published at Neural Computation", "journal-ref": "Neural computation, 30(5), 1394-1425, 2018", "doi": "10.1162/neco_a_01064", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although nonstationary data are more common in the real world, most existing\ncausal discovery methods do not take nonstationarity into consideration. In\nthis letter, we propose a kernel embedding-based approach, ENCI, for\nnonstationary causal model inference where data are collected from multiple\ndomains with varying distributions. In ENCI, we transform the complicated\nrelation of a cause-effect pair into a linear model of variables of which\nobservations correspond to the kernel embeddings of the cause-and-effect\ndistributions in different domains. In this way, we are able to estimate the\ncausal direction by exploiting the causal asymmetry of the transformed linear\nmodel. Furthermore, we extend ENCI to causal graph discovery for multiple\nvariables by transforming the relations among them into a linear nongaussian\nacyclic model. We show that by exploiting the nonstationarity of distributions,\nboth cause-effect pairs and two kinds of causal graphs are identifiable under\nmild conditions. Experiments on synthetic and real-world data are conducted to\njustify the efficacy of ENCI over major existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 09:28:46 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hu", "Shoubo", ""], ["Chen", "Zhitang", ""], ["Chan", "Laiwan", ""]]}, {"id": "1809.08567", "submitter": "Jordi De La Torre", "authors": "Jordi de la Torre, Aida Valls, Domenec Puig, Pere Romero-Aroca", "title": "Identification and Visualization of the Underlying Independent Causes of\n  the Diagnostic of Diabetic Retinopathy made by a Deep Learning Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is a key factor in the design of automatic classifiers for\nmedical diagnosis. Deep learning models have been proven to be a very effective\nclassification algorithm when trained in a supervised way with enough data. The\nmain concern is the difficulty of inferring rationale interpretations from\nthem. Different attempts have been done in last years in order to convert deep\nlearning classifiers from high confidence statistical black box machines into\nself-explanatory models. In this paper we go forward into the generation of\nexplanations by identifying the independent causes that use a deep learning\nmodel for classifying an image into a certain class. We use a combination of\nIndependent Component Analysis with a Score Visualization technique. In this\npaper we study the medical problem of classifying an eye fundus image into 5\nlevels of Diabetic Retinopathy. We conclude that only 3 independent components\nare enough for the differentiation and correct classification between the 5\ndisease standard classes. We propose a method for visualizing them and\ndetecting lesions from the generated visual maps.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 09:51:12 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["de la Torre", "Jordi", ""], ["Valls", "Aida", ""], ["Puig", "Domenec", ""], ["Romero-Aroca", "Pere", ""]]}, {"id": "1809.08568", "submitter": "Shoubo Hu", "authors": "Shoubo Hu, Zhitang Chen, Vahid Partovi Nia, Laiwan Chan, Yanhui Geng", "title": "Causal Inference and Mechanism Clustering of A Mixture of Additive Noise\n  Models", "comments": "Published at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of the causal relationship between a pair of observed variables\nis a fundamental problem in science, and most existing approaches are based on\none single causal model. In practice, however, observations are often collected\nfrom multiple sources with heterogeneous causal models due to certain\nuncontrollable factors, which renders causal analysis results obtained by a\nsingle model skeptical. In this paper, we generalize the Additive Noise Model\n(ANM) to a mixture model, which consists of a finite number of ANMs, and\nprovide the condition of its causal identifiability. To conduct model\nestimation, we propose Gaussian Process Partially Observable Model (GPPOM), and\nincorporate independence enforcement into it to learn latent parameter\nassociated with each observation. Causal inference and clustering according to\nthe underlying generating mechanisms of the mixture model are addressed in this\nwork. Experiments on synthetic and real data demonstrate the effectiveness of\nour proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 09:57:14 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 18:11:35 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2018 13:04:29 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hu", "Shoubo", ""], ["Chen", "Zhitang", ""], ["Nia", "Vahid Partovi", ""], ["Chan", "Laiwan", ""], ["Geng", "Yanhui", ""]]}, {"id": "1809.08587", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Exponential Convergence Time of Gradient Descent for One-Dimensional\n  Deep Linear Neural Networks", "comments": "Comparison to previous version: Fixed a bug in lemma 1 part 3 (does\n  not affect any other part of the paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamics of gradient descent on objective functions of the form\n$f(\\prod_{i=1}^{k} w_i)$ (with respect to scalar parameters $w_1,\\ldots,w_k$),\nwhich arise in the context of training depth-$k$ linear neural networks. We\nprove that for standard random initializations, and under mild assumptions on\n$f$, the number of iterations required for convergence scales exponentially\nwith the depth $k$. We also show empirically that this phenomenon can occur in\nhigher dimensions, where each $w_i$ is a matrix. This highlights a potential\nobstacle in understanding the convergence of gradient-based methods for deep\nlinear neural networks, where $k$ is large.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 12:32:45 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 08:37:45 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 08:31:56 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 07:23:22 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1809.08613", "submitter": "Namiko Saito", "authors": "Namiko Saito, Kitae Kim, Shingo Murata, Tetsuya Ogata and Shigeki\n  Sugano", "title": "Detecting Features of Tools, Objects, and Actions from Effects in a\n  Robot using Deep Learning", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tool-use model that can detect the features of tools, target\nobjects, and actions from the provided effects of object manipulation. We\nconstruct a model that enables robots to manipulate objects with tools, using\ninfant learning as a concept. To realize this, we train sensory-motor data\nrecorded during a tool-use task performed by a robot with deep learning.\nExperiments include four factors: (1) tools, (2) objects, (3) actions, and (4)\neffects, which the model considers simultaneously. For evaluation, the robot\ngenerates predicted images and motions given information of the effects of\nusing unknown tools and objects. We confirm that the robot is capable of\ndetecting features of tools, objects, and actions by learning the effects and\nexecuting the task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 15:24:21 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Saito", "Namiko", ""], ["Kim", "Kitae", ""], ["Murata", "Shingo", ""], ["Ogata", "Tetsuya", ""], ["Sugano", "Shigeki", ""]]}, {"id": "1809.08660", "submitter": "Vahid Moosavi", "authors": "Lukas Fuhrimann, Vahid Moosavi, Patrick Ole Ohlbrock, Pierluigi\n  Dacunto", "title": "Data-Driven Design: Exploring new Structural Forms using Machine\n  Learning and Graphic Statics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this research is to introduce a novel structural design process\nthat allows architects and engineers to extend their typical design space\nhorizon and thereby promoting the idea of creativity in structural design. The\ntheoretical base of this work builds on the combination of structural\nform-finding and state-of-the-art machine learning algorithms. In the first\nstep of the process, Combinatorial Equilibrium Modelling (CEM) is used to\ngenerate a large variety of spatial networks in equilibrium for given input\nparameters. In the second step, these networks are clustered and represented in\na form-map through the implementation of a Self Organizing Map (SOM) algorithm.\nIn the third step, the solution space is interpreted with the help of a Uniform\nManifold Approximation and Projection algorithm (UMAP). This allows gaining\nimportant insights in the structure of the solution space. A specific case\nstudy is used to illustrate how the infinite equilibrium states of a given\ntopology can be defined and represented by clusters. Furthermore, three\nclasses, related to the non-linear interaction between the input parameters and\nthe form space, are verified and a statement about the entire manifold of the\nsolution space of the case study is made. To conclude, this work presents an\ninnovative approach on how the manifold of a solution space can be grasped with\na minimum amount of data and how to operate within the manifold in order to\nincrease the diversity of solutions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 19:00:40 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fuhrimann", "Lukas", ""], ["Moosavi", "Vahid", ""], ["Ohlbrock", "Patrick Ole", ""], ["Dacunto", "Pierluigi", ""]]}, {"id": "1809.08696", "submitter": "Zeljko Kereta", "authors": "Ernesto de Vito and Zeljko Kereta and Valeria Naumova", "title": "Unsupervised parameter selection for denoising with the elastic net", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in regularisation theory, the issue of parameter\nselection still remains a challenge for most applications. In a recent work the\nframework of statistical learning was used to approximate the optimal Tikhonov\nregularisation parameter from noisy data. In this work, we improve their\nresults and extend the analysis to the elastic net regularisation, providing\nexplicit error bounds on the accuracy of the approximated parameter and the\ncorresponding regularisation solution in a simplified case. Furthermore, in the\ngeneral case we design a data-driven, automated algorithm for the computation\nof an approximate regularisation parameter. Our analysis combines statistical\nlearning theory with insights from regularisation theory. We compare our\napproach with state-of-the-art parameter selection criteria and illustrate its\nsuperiority in terms of accuracy and computational time on simulated and real\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 23:31:17 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 08:52:04 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 14:56:15 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["de Vito", "Ernesto", ""], ["Kereta", "Zeljko", ""], ["Naumova", "Valeria", ""]]}, {"id": "1809.08700", "submitter": "Ritesh Noothigattu", "authors": "Maria-Florina Balcan, Travis Dick, Ritesh Noothigattu and Ariel D.\n  Procaccia", "title": "Envy-Free Classification", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 2019, pp.\n  1240-1250", "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic fair division problems such as cake cutting and rent division,\nenvy-freeness requires that each individual (weakly) prefer his allocation to\nanyone else's. On a conceptual level, we argue that envy-freeness also provides\na compelling notion of fairness for classification tasks. Our technical focus\nis the generalizability of envy-free classification, i.e., understanding\nwhether a classifier that is envy free on a sample would be almost envy free\nwith respect to the underlying distribution with high probability. Our main\nresult establishes that a small sample is sufficient to achieve such\nguarantees, when the classifier in question is a mixture of deterministic\nclassifiers that belong to a family of low Natarajan dimension.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 23:59:20 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 05:35:16 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Dick", "Travis", ""], ["Noothigattu", "Ritesh", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "1809.08705", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Meisam Razaviyayn", "title": "On the Behavior of the Expectation-Maximization Algorithm for Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models are among the most popular statistical models used in\ndifferent data science disciplines. Despite their broad applicability,\ninference under these models typically leads to computationally challenging\nnon-convex problems. While the Expectation-Maximization (EM) algorithm is the\nmost popular approach for solving these non-convex problems, the behavior of\nthis algorithm is not well understood. In this work, we focus on the case of\nmixture of Laplacian (or Gaussian) distribution. We start by analyzing a simple\nequally weighted mixture of two single dimensional Laplacian distributions and\nshow that every local optimum of the population maximum likelihood estimation\nproblem is globally optimal. Then, we prove that the EM algorithm converges to\nthe ground truth parameters almost surely with random initialization. Our\nresult extends the existing results for Gaussian distribution to Laplacian\ndistribution. Then we numerically study the behavior of mixture models with\nmore than two components. Motivated by our extensive numerical experiments, we\npropose a novel stochastic method for estimating the mean of components of a\nmixture model. Our numerical experiments show that our algorithm outperforms\nthe Naive EM algorithm in almost all scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 00:12:28 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Barazandeh", "Babak", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "1809.08706", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Bhanukiran Vinzamuri, Sijia Liu", "title": "Is Ordered Weighted $\\ell_1$ Regularized Regression Robust to\n  Adversarial Perturbation? A Case Study on OSCAR", "comments": "Accepted to IEEE GlobalSIP 2018. Pin-Yu Chen and Bhanukiran Vinzamuri\n  contribute equally to this work; v2 fixes missing citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art machine learning models such as deep neural networks\nhave recently shown to be vulnerable to adversarial perturbations, especially\nin classification tasks. Motivated by adversarial machine learning, in this\npaper we investigate the robustness of sparse regression models with strongly\ncorrelated covariates to adversarially designed measurement noises.\nSpecifically, we consider the family of ordered weighted $\\ell_1$ (OWL)\nregularized regression methods and study the case of OSCAR (octagonal shrinkage\nclustering algorithm for regression) in the adversarial setting. Under a\nnorm-bounded threat model, we formulate the process of finding a maximally\ndisruptive noise for OWL-regularized regression as an optimization problem and\nillustrate the steps towards finding such a noise in the case of OSCAR.\nExperimental results demonstrate that the regression performance of grouping\nstrongly correlated features can be severely degraded under our adversarial\nsetting, even when the noise budget is significantly smaller than the\nground-truth signals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 00:20:03 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 03:50:41 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Vinzamuri", "Bhanukiran", ""], ["Liu", "Sijia", ""]]}, {"id": "1809.08717", "submitter": "Alexander Stec", "authors": "Alexander Stec, Diego Klabjan, Jean Utke", "title": "Unified recurrent neural network for many feature types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are time series that are amenable to recurrent neural network (RNN)\nsolutions when treated as sequences, but some series, e.g. asynchronous time\nseries, provide a richer variation of feature types than current RNN cells take\ninto account. In order to address such situations, we introduce a unified RNN\nthat handles five different feature types, each in a different manner. Our RNN\nframework separates sequential features into two groups dependent on their\nfrequency, which we call sparse and dense features, and which affect cell\nupdates differently. Further, we also incorporate time features at the\nsequential level that relate to the time between specified events in the\nsequence and are used to modify the cell's memory state. We also include two\ntypes of static (whole sequence level) features, one related to time and one\nnot, which are combined with the encoder output. The experiments show that the\nmodeling framework proposed does increase performance compared to standard\ncells.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 01:37:26 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Stec", "Alexander", ""], ["Klabjan", "Diego", ""], ["Utke", "Jean", ""]]}, {"id": "1809.08738", "submitter": "Mikhail Yurochkin", "authors": "Mikhail Yurochkin, Zhiwei Fan, Aritra Guha, Paraschos Koutris and\n  XuanLong Nguyen", "title": "Scalable inference of topic evolution via models for latent geometric\n  structures", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new models and algorithms for learning the temporal dynamics of\nthe topic polytopes and related geometric objects that arise in topic model\nbased inference. Our model is nonparametric Bayesian and the corresponding\ninference algorithm is able to discover new topics as the time progresses. By\nexploiting the connection between the modeling of topic polytope evolution,\nBeta-Bernoulli process and the Hungarian matching algorithm, our method is\nshown to be several orders of magnitude faster than existing topic modeling\napproaches, as demonstrated by experiments working with several million\ndocuments in under two dozens of minutes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 03:23:07 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 23:59:08 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 03:49:37 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yurochkin", "Mikhail", ""], ["Fan", "Zhiwei", ""], ["Guha", "Aritra", ""], ["Koutris", "Paraschos", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1809.08746", "submitter": "Weining Shen", "authors": "Wei Hu, Weining Shen, Hua Zhou, and Dehan Kong", "title": "Matrix Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel linear discriminant analysis approach for the\nclassification of high-dimensional matrix-valued data that commonly arises from\nimaging studies. Motivated by the equivalence of the conventional linear\ndiscriminant analysis and the ordinary least squares, we consider an efficient\nnuclear norm penalized regression that encourages a low-rank structure.\nTheoretical properties including a non-asymptotic risk bound and a rank\nconsistency result are established. Simulation studies and an application to\nelectroencephalography data show the superior performance of the proposed\nmethod over the existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:07:06 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 16:50:25 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Hu", "Wei", ""], ["Shen", "Weining", ""], ["Zhou", "Hua", ""], ["Kong", "Dehan", ""]]}, {"id": "1809.08771", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Trevor Hastie", "title": "Longitudinal data analysis using matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of defect\nof vision, or assessment of gait in patients with neurological disorders. Since\nthe data collection is often costly and inconvenient, estimation of progression\nfrom sparse observations is of great interest for practitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both random and fixed effect.\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions and require\nvery careful implementation.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields point\nestimates of progression curves by iterative application of the SVD. Our\nframework covers multivariate longitudinal data, regression and can be easily\nextended to other settings.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables discovering that subtypes of Cerebral Palsy exhibit\ndifferent progression trends.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 06:18:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hastie", "Trevor", ""]]}, {"id": "1809.08782", "submitter": "Xiao Yan", "authors": "Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, James Cheng", "title": "Norm-Ranging LSH for Maximum Inner Product Search", "comments": "NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neyshabur and Srebro proposed Simple-LSH, which is the state-of-the-art\nhashing method for maximum inner product search (MIPS) with performance\nguarantee. We found that the performance of Simple-LSH, in both theory and\npractice, suffers from long tails in the 2-norm distribution of real datasets.\nWe propose Norm-ranging LSH, which addresses the excessive normalization\nproblem caused by long tails in Simple-LSH by partitioning a dataset into\nmultiple sub-datasets and building a hash index for each sub-dataset\nindependently. We prove that Norm-ranging LSH has lower query time complexity\nthan Simple-LSH. We also show that the idea of partitioning the dataset can\nimprove other hashing based methods for MIPS. To support efficient query\nprocessing on the hash indexes of the sub-datasets, a novel similarity metric\nis formulated. Experiments show that Norm-ranging LSH achieves an order of\nmagnitude speedup over Simple-LSH for the same recall, thus significantly\nbenefiting applications that involve MIPS.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 07:20:45 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 06:29:44 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yan", "Xiao", ""], ["Li", "Jinfeng", ""], ["Dai", "Xinyan", ""], ["Chen", "Hongzhi", ""], ["Cheng", "James", ""]]}, {"id": "1809.08783", "submitter": "Bodhibrata Mukhopadhyay", "authors": "Bodhibrata Mukhopadhyay, Sahil Anchal, Subrat Kar", "title": "Person Identification using Seismic Signals generated from Footfalls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Footfall based biometric system is perhaps the only person identification\ntechnique which does not hinder the natural movement of an individual. This is\na clear edge over all other biometric systems which require a formidable amount\nof human intervention and encroach upon an individual's privacy to some extent\nor the other. This paper presents a Fog computing architecture for implementing\nfootfall based biometric system using widespread geographically distributed\ngeophones (vibration sensor). Results were stored in an Internet of Things\n(IoT) cloud. We have tested our biometric system on an indigenous database\n(created by us) containing 46000 footfall events from 8 individuals and\nachieved an accuracy of 73%, 90% and 95% in case of 1, 5 and 10 footsteps per\nsample. We also proposed a basis pursuit based data compression technique DS8BP\nfor wireless transmission of footfall events to the Fog. DS8BP compresses the\noriginal footfall events (sampled at 8 kHz) by a factor of 108 and also acts as\na smoothing filter. These experimental results depict the high viability of our\ntechnique in the realm of person identification and access control systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 07:35:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Mukhopadhyay", "Bodhibrata", ""], ["Anchal", "Sahil", ""], ["Kar", "Subrat", ""]]}, {"id": "1809.08820", "submitter": "Hugh Salimbeni", "authors": "Hugh Salimbeni, Ching-An Cheng, Byron Boots, Marc Deisenroth", "title": "Orthogonally Decoupled Variational Gaussian Processes", "comments": "Appearing NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a powerful non-parametric framework for\nreasoning over functions. Despite appealing theory, its superlinear\ncomputational and memory complexities have presented a long-standing challenge.\nState-of-the-art sparse variational inference methods trade modeling accuracy\nagainst complexity. However, the complexities of these methods still scale\nsuperlinearly in the number of basis functions, implying that that sparse GP\nmethods are able to learn from large datasets only when a small model is used.\nRecently, a decoupled approach was proposed that removes the unnecessary\ncoupling between the complexities of modeling the mean and the covariance\nfunctions of a GP. It achieves a linear complexity in the number of mean\nparameters, so an expressive posterior mean function can be modeled. While\npromising, this approach suffers from optimization difficulties due to\nill-conditioning and non-convexity. In this work, we propose an alternative\ndecoupled parametrization. It adopts an orthogonal basis in the mean function\nto model the residues that cannot be learned by the standard coupled approach.\nTherefore, our method extends, rather than replaces, the coupled approach to\nachieve strictly better performance. This construction admits a straightforward\nnatural gradient update rule, so the structure of the information manifold that\nis lost during decoupling can be leveraged to speed up learning. Empirically,\nour algorithm demonstrates significantly faster convergence in multiple\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 09:52:42 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 09:43:18 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 16:26:28 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Salimbeni", "Hugh", ""], ["Cheng", "Ching-An", ""], ["Boots", "Byron", ""], ["Deisenroth", "Marc", ""]]}, {"id": "1809.08830", "submitter": "Soroosh Shafieezadeh-Abadeh", "authors": "Soroosh Shafieezadeh-Abadeh, Viet Anh Nguyen, Daniel Kuhn, Peyman\n  Mohajerin Esfahani", "title": "Wasserstein Distributionally Robust Kalman Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributionally robust mean square error estimation problem over\na nonconvex Wasserstein ambiguity set containing only normal distributions. We\nshow that the optimal estimator and the least favorable distribution form a\nNash equilibrium. Despite the non-convex nature of the ambiguity set, we prove\nthat the estimation problem is equivalent to a tractable convex program. We\nfurther devise a Frank-Wolfe algorithm for this convex program whose\ndirection-searching subproblem can be solved in a quasi-closed form. Using\nthese ingredients, we introduce a distributionally robust Kalman filter that\nhedges against model risk.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 10:22:03 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 14:31:59 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 14:49:46 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Shafieezadeh-Abadeh", "Soroosh", ""], ["Nguyen", "Viet Anh", ""], ["Kuhn", "Daniel", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "1809.08848", "submitter": "Piotr Warcho{\\l}", "authors": "Wojciech Tarnowski, Piotr Warcho{\\l}, Stanis{\\l}aw Jastrz\\k{e}bski,\n  Jacek Tabor, Maciej A. Nowak", "title": "Dynamical Isometry is Achieved in Residual Networks in a Universal Way\n  for any Activation Function", "comments": null, "journal-ref": "AISTATS 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that in residual neural networks (ResNets) dynamical isometry\nis achievable irrespectively of the activation function used. We do that by\nderiving, with the help of Free Probability and Random Matrix Theories, a\nuniversal formula for the spectral density of the input-output Jacobian at\ninitialization, in the large network width and depth limit. The resulting\nsingular value spectrum depends on a single parameter, which we calculate for a\nvariety of popular activation functions, by analyzing the signal propagation in\nthe artificial neural network. We corroborate our results with numerical\nsimulations of both random matrices and ResNets applied to the CIFAR-10\nclassification problem. Moreover, we study the consequence of this universal\nbehavior for the initial and late phases of the learning processes. We conclude\nby drawing attention to the simple fact, that initialization acts as a\nconfounding factor between the choice of activation function and the rate of\nlearning. We propose that in ResNets this can be resolved based on our results,\nby ensuring the same level of dynamical isometry at initialization.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 11:20:50 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 17:17:27 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 15:43:56 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Tarnowski", "Wojciech", ""], ["Warcho\u0142", "Piotr", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Tabor", "Jacek", ""], ["Nowak", "Maciej A.", ""]]}, {"id": "1809.08871", "submitter": "Edouard Fournier", "authors": "Edouard Fournier (ENAC), St\\'ephane Grihon, Thierry Klein (IMT)", "title": "Semi Parametric Estimations of rotating and scaling parameters for\n  aeronautic loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform registration of noisy curves. We provide an\nappropriate model in estimating the rotation and scaling parameters to adjust a\nset of curves through a M-estimation procedure. We prove the consistency and\nthe asymptotic normality of our estimators. Numerical simulation and a real\nlife aeronautic example are given to illustrate our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 12:26:15 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:00:42 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Fournier", "Edouard", "", "ENAC"], ["Grihon", "St\u00e9phane", "", "IMT"], ["Klein", "Thierry", "", "IMT"]]}, {"id": "1809.08899", "submitter": "Christopher Ormerod", "authors": "Christopher M. Ormerod and Amy E. Harris", "title": "Neural network approach to classifying alarming student responses to\n  online assessment", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated scoring engines are increasingly being used to score the free-form\ntext responses that students give to questions. Such engines are not designed\nto appropriately deal with responses that a human reader would find alarming\nsuch as those that indicate an intention to self-harm or harm others, responses\nthat allude to drug abuse or sexual abuse or any response that would elicit\nconcern for the student writing the response. Our neural network models have\nbeen designed to help identify these anomalous responses from a large\ncollection of typical responses that students give. The responses identified by\nthe neural network can be assessed for urgency, severity, and validity more\nquickly by a team of reviewers than otherwise possible. Given the anomalous\nnature of these types of responses, our goal is to maximize the chance of\nflagging these responses for review given the constraint that only a fixed\npercentage of responses can viably be assessed by a team of reviewers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 14:29:22 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ormerod", "Christopher M.", ""], ["Harris", "Amy E.", ""]]}, {"id": "1809.08908", "submitter": "Gopal Nataraj", "authors": "Gopal Nataraj, Jon-Fredrik Nielsen, Mingjie Gao, and Jeffrey A.\n  Fessler", "title": "Fast, Precise Myelin Water Quantification using DESS MRI and Kernel\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To investigate the feasibility of myelin water content\nquantification using fast dual-echo steady-state (DESS) scans and machine\nlearning with kernels.\n  Methods: We optimized combinations of steady-state (SS) scans for precisely\nestimating the fast-relaxing signal fraction ff of a two-compartment signal\nmodel, subject to a scan time constraint. We estimated ff from the optimized\nDESS acquisition using a recently developed method for rapid parameter\nestimation via regression with kernels (PERK). We compared DESS PERK ff\nestimates to conventional myelin water fraction (MWF) estimates from a longer\nmulti-echo spin-echo (MESE) acquisition in simulation, in vivo, and ex vivo\nstudies.\n  Results: Simulations demonstrate that DESS PERK ff estimators and MESE MWF\nestimators achieve comparable error levels. In vivo and ex vivo experiments\ndemonstrate that MESE MWF and DESS PERK ff estimates are quantitatively\ncomparable measures of WM myelin water content. To our knowledge, these\nexperiments are the first to demonstrate myelin water images from a SS\nacquisition that are quantitatively similar to conventional MESE MWF images.\n  Conclusion: Combinations of fast DESS scans can be designed to enable precise\nff estimation. PERK is well-suited for ff estimation. DESS PERK ff and MESE MWF\nestimates are quantitatively similar measures of WM myelin water content.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 13:36:21 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Nataraj", "Gopal", ""], ["Nielsen", "Jon-Fredrik", ""], ["Gao", "Mingjie", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1809.08911", "submitter": "Xiao Chen", "authors": "Xiao Chen, Peter Kairouz, Ram Rajagopal", "title": "Understanding Compressive Adversarial Privacy", "comments": null, "journal-ref": "2018 IEEE Conference on Decision and Control (CDC)", "doi": "10.1109/CDC.2018.8619455", "report-no": null, "categories": "cs.LG cs.CY cs.SY eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Designing a data sharing mechanism without sacrificing too much privacy can\nbe considered as a game between data holders and malicious attackers. This\npaper describes a compressive adversarial privacy framework that captures the\ntrade-off between the data privacy and utility. We characterize the optimal\ndata releasing mechanism through convex optimization when assuming that both\nthe data holder and attacker can only modify the data using linear\ntransformations. We then build a more realistic data releasing mechanism that\ncan rely on a nonlinear compression model while the attacker uses a neural\nnetwork. We demonstrate in a series of empirical applications that this\nframework, consisting of compressive adversarial privacy, can preserve\nsensitive information.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 07:39:50 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 04:18:23 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Chen", "Xiao", ""], ["Kairouz", "Peter", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1809.08922", "submitter": "Moin Nadeem", "authors": "Moin Nadeem, Dustin Stansbury, Shane Mooney", "title": "Context-Aware Systems for Sequential Item Recommendation", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2018.00056", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quizlet is the most popular online learning tool in the United States, and is\nused by over 2/3 of high school students, and 1/2 of college students. With\nmore than 95% of Quizlet users reporting improved grades as a result, the\nplatform has become the de-facto tool used in millions of classrooms. In this\npaper, we explore the task of recommending suitable content for a student to\nstudy, given their prior interests, as well as what their peers are studying.\nWe propose a novel approach, i.e. Neural Educational Recommendation Engine\n(NERE), to recommend educational content by leveraging student behaviors rather\nthan ratings. We have found that this approach better captures social factors\nthat are more aligned with learning. NERE is based on a recurrent neural\nnetwork that includes collaborative and content-based approaches for\nrecommendation, and takes into account any particular student's speed, mastery,\nand experience to recommend the appropriate task. We train NERE by jointly\nlearning the user embeddings and content embeddings, and attempt to predict the\ncontent embedding for the final timestamp. We also develop a confidence\nestimator for our neural network, which is a crucial requirement for\nproductionizing this model. We apply NERE to Quizlet's proprietary dataset, and\npresent our results. We achieved an R^2 score of 0.81 in the content embedding\nspace, and a recall score of 54% on our 100 nearest neighbors. This vastly\nexceeds the recall@100 score of 12% that a standard matrix-factorization\napproach provides. We conclude with a discussion on how NERE will be deployed,\nand position our work as one of the first educational recommender systems for\nthe K-12 space.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 03:48:52 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 21:30:12 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Nadeem", "Moin", ""], ["Stansbury", "Dustin", ""], ["Mooney", "Shane", ""]]}, {"id": "1809.08923", "submitter": "Yue Wang", "authors": "Yue Wang, Qi Meng, Wei Cheng, Yuting Liug, Zhi-Ming Ma, Tie-Yan Liu", "title": "Target Transfer Q-Learning and Its Convergence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning is one of the most popular methods in Reinforcement Learning (RL).\nTransfer Learning aims to utilize the learned knowledge from source tasks to\nhelp new tasks to improve the sample complexity of the new tasks. Considering\nthat data collection in RL is both more time and cost consuming and Q-learning\nconverges slowly comparing to supervised learning, different kinds of transfer\nRL algorithms are designed. However, most of them are heuristic with no\ntheoretical guarantee of the convergence rate. Therefore, it is important for\nus to clearly understand when and how will transfer learning help RL method and\nprovide the theoretical guarantee for the improvement of the sample complexity.\nIn this paper, we propose to transfer the Q-function learned in the source task\nto the target of the Q-learning in the new task when certain safe conditions\nare satisfied. We call this new transfer Q-learning method target transfer\nQ-Learning. The safe conditions are necessary to avoid the harm to the new\ntasks and thus ensure the convergence of the algorithm. We study the\nconvergence rate of the target transfer Q-learning. We prove that if the two\ntasks are similar with respect to the MDPs, the optimal Q-functions in the\nsource and new RL tasks are similar which means the error of the transferred\ntarget Q-function in new MDP is small. Also, the convergence rate analysis\nshows that the target transfer Q-Learning will converge faster than Q-learning\nif the error of the transferred target Q-function is smaller than the current\nQ-function in the new task. Based on our theoretical results, we design the\nsafe condition as the Bellman error of the transferred target Q-function is\nless than the current Q-function. Our experiments are consistent with our\ntheoretical founding and verified the effectiveness of our proposed target\ntransfer Q-learning method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 05:31:26 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Wang", "Yue", ""], ["Meng", "Qi", ""], ["Cheng", "Wei", ""], ["Liug", "Yuting", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1809.08926", "submitter": "Yue Wang", "authors": "Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, Tie-Yan Liu", "title": "Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov\n  Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning (RL) , one of the key components is policy\nevaluation, which aims to estimate the value function (i.e., expected long-term\naccumulated reward) of a policy. With a good policy evaluation method, the RL\nalgorithms will estimate the value function more accurately and find a better\npolicy. When the state space is large or continuous \\emph{Gradient-based\nTemporal Difference(GTD)} policy evaluation algorithms with linear function\napproximation are widely used. Considering that the collection of the\nevaluation data is both time and reward consuming, a clear understanding of the\nfinite sample performance of the policy evaluation algorithms is very important\nto reinforcement learning. Under the assumption that data are i.i.d. generated,\nprevious work provided the finite sample analysis of the GTD algorithms with\nconstant step size by converting them into convex-concave saddle point\nproblems. However, it is well-known that, the data are generated from Markov\nprocesses rather than i.i.d. in RL problems.. In this paper, in the realistic\nMarkov setting, we derive the finite sample bounds for the general\nconvex-concave saddle point problems, and hence for the GTD algorithms. We have\nthe following discussions based on our bounds. (1) With variants of step size,\nGTD algorithms converge. (2) The convergence rate is determined by the step\nsize, with the mixing time of the Markov process as the coefficient. The faster\nthe Markov processes mix, the faster the convergence. (3) We explain that the\nexperience replay trick is effective by improving the mixing property of the\nMarkov process. To the best of our knowledge, our analysis is the first to\nprovide finite sample bounds for the GTD algorithms in Markov setting.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 06:09:21 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Wang", "Yue", ""], ["Chen", "Wei", ""], ["Liu", "Yuting", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1809.08999", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi", "title": "Fast Geometrically-Perturbed Adversarial Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art performance of deep learning algorithms has led to a\nconsiderable increase in the utilization of machine learning in\nsecurity-sensitive and critical applications. However, it has recently been\nshown that a small and carefully crafted perturbation in the input space can\ncompletely fool a deep model. In this study, we explore the extent to which\nface recognition systems are vulnerable to geometrically-perturbed adversarial\nfaces. We propose a fast landmark manipulation method for generating\nadversarial faces, which is approximately 200 times faster than the previous\ngeometric attacks and obtains 99.86% success rate on the state-of-the-art face\nrecognition models. To further force the generated samples to be natural, we\nintroduce a second attack constrained on the semantic structure of the face\nwhich has the half speed of the first attack with the success rate of 99.96%.\nBoth attacks are extremely robust against the state-of-the-art defense methods\nwith the success rate of equal or greater than 53.59%. Code is available at\nhttps://github.com/alldbi/FLM\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:26:13 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 17:20:54 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1809.09003", "submitter": "Ting Yu Mu", "authors": "Ting-Yu Mu, Ala Al-Fuqaha, Khaled Shuaib, Farag M. Sallabi, Junaid\n  Qadir", "title": "SDN Flow Entry Management Using Reinforcement Learning", "comments": "19 pages, 11 figures, published on ACM Transactions on Autonomous and\n  Adaptive Systems (TAAS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information technology services largely depend on cloud\ninfrastructures to provide their services. These cloud infrastructures are\nbuilt on top of datacenter networks (DCNs) constructed with high-speed links,\nfast switching gear, and redundancy to offer better flexibility and resiliency.\nIn this environment, network traffic includes long-lived (elephant) and\nshort-lived (mice) flows with partitioned and aggregated traffic patterns.\nAlthough SDN-based approaches can efficiently allocate networking resources for\nsuch flows, the overhead due to network reconfiguration can be significant.\nWith limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in\nan OpenFlow enabled switch, it is crucial to determine which forwarding rules\nshould remain in the flow table, and which rules should be processed by the SDN\ncontroller in case of a table-miss on the SDN switch. This is needed in order\nto obtain the flow entries that satisfy the goal of reducing the long-term\ncontrol plane overhead introduced between the controller and the switches. To\nachieve this goal, we propose a machine learning technique that utilizes two\nvariations of reinforcement learning (RL) algorithms-the first of which is\ntraditional reinforcement learning algorithm based while the other is deep\nreinforcement learning based. Emulation results using the RL algorithm show\naround 60% improvement in reducing the long-term control plane overhead, and\naround 14% improvement in the table-hit ratio compared to the Multiple Bloom\nFilters (MBF) method given a fixed size flow table of 4KB.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:29:06 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Mu", "Ting-Yu", ""], ["Al-Fuqaha", "Ala", ""], ["Shuaib", "Khaled", ""], ["Sallabi", "Farag M.", ""], ["Qadir", "Junaid", ""]]}, {"id": "1809.09030", "submitter": "Golnoosh Farnadi", "authors": "Golnoosh Farnadi and Pigi Kouki and Spencer K. Thompson and Sriram\n  Srinivasan and Lise Getoor", "title": "A Fairness-aware Hybrid Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recommender systems are used in variety of domains affecting people's lives.\nThis has raised concerns about possible biases and discrimination that such\nsystems might exacerbate. There are two primary kinds of biases inherent in\nrecommender systems: observation bias and bias stemming from imbalanced data.\nObservation bias exists due to a feedback loop which causes the model to learn\nto only predict recommendations similar to previous ones. Imbalance in data\noccurs when systematic societal, historical, or other ambient bias is present\nin the data. In this paper, we address both biases by proposing a hybrid\nfairness-aware recommender system. Our model provides efficient and accurate\nrecommendations by incorporating multiple user-user and item-item similarity\nmeasures, content, and demographic information, while addressing recommendation\nbiases. We implement our model using a powerful and expressive probabilistic\nprogramming language called probabilistic soft logic. We experimentally\nevaluate our approach on a popular movie recommendation dataset, showing that\nour proposed model can provide more accurate and fairer recommendations,\ncompared to a state-of-the art fair recommender system.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 00:30:52 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Farnadi", "Golnoosh", ""], ["Kouki", "Pigi", ""], ["Thompson", "Spencer K.", ""], ["Srinivasan", "Sriram", ""], ["Getoor", "Lise", ""]]}, {"id": "1809.09035", "submitter": "Mohammad Shojafar", "authors": "Deepa K, Radhamani G, Vinod P, Mohammad Shojafar, Neeraj Kumar, Mauro\n  Conti", "title": "FeatureAnalytics: An approach to derive relevant attributes for\n  analyzing Android Malware", "comments": "26 pages, 6 figures, 9 tables, Journal Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ever increasing number of Android malware, has always been a concern for\ncybersecurity professionals. Even though plenty of anti-malware solutions\nexist, a rational and pragmatic approach for the same is rare and has to be\ninspected further. In this paper, we propose a novel two-set feature selection\napproach based on Rough Set and Statistical Test named as RSST to extract\nrelevant system calls. To address the problem of higher dimensional attribute\nset, we derived suboptimal system call space by applying the proposed feature\nselection method to maximize the separability between malware and benign\nsamples. Comprehensive experiments conducted on a dataset consisting of 3500\nsamples with 30 RSST derived essential system calls resulted in an accuracy of\n99.9%, Area Under Curve (AUC) of 1.0, with 1% False Positive Rate (FPR).\nHowever, other feature selectors (Information Gain, CFsSubsetEval, ChiSquare,\nFreqSel and Symmetric Uncertainty) used in the domain of malware analysis\nresulted in the accuracy of 95.5% with 8.5% FPR. Besides, empirical analysis of\nRSST derived system calls outperform other attributes such as permissions,\nopcodes, API, methods, call graphs, Droidbox attributes and network traces.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 15:08:36 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["K", "Deepa", ""], ["G", "Radhamani", ""], ["P", "Vinod", ""], ["Shojafar", "Mohammad", ""], ["Kumar", "Neeraj", ""], ["Conti", "Mauro", ""]]}, {"id": "1809.09060", "submitter": "Isidro Cortes-Ciriano PhD", "authors": "Isidro Cortes-Ciriano and Andreas Bender", "title": "Deep Confidence: A Computationally Efficient Framework for Calculating\n  Reliable Errors for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.8b00542", "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning architectures have proved versatile in a number of drug\ndiscovery applications, including the modelling of in vitro compound activity.\nWhile controlling for prediction confidence is essential to increase the trust,\ninterpretability and usefulness of virtual screening models in drug discovery,\ntechniques to estimate the reliability of the predictions generated with deep\nlearning networks remain largely underexplored. Here, we present Deep\nConfidence, a framework to compute valid and efficient confidence intervals for\nindividual predictions using the deep learning technique Snapshot Ensembling\nand conformal prediction. Specifically, Deep Confidence generates an ensemble\nof deep neural networks by recording the network parameters throughout the\nlocal minima visited during the optimization phase of a single neural network.\nThis approach serves to derive a set of base learners (i.e., snapshots) with\ncomparable predictive power on average, that will however generate slightly\ndifferent predictions for a given instance. The variability across base\nlearners and the validation residuals are in turn harnessed to compute\nconfidence intervals using the conformal prediction framework. Using a set of\n24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles\nperform on par with Random Forest (RF) and ensembles of independently trained\ndeep neural networks. In addition, we find that the confidence regions\npredicted using the Deep Confidence framework span a narrower set of values.\nOverall, Deep Confidence represents a highly versatile error prediction\nframework that can be applied to any deep learning-based application at no\nextra computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:08:08 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Cortes-Ciriano", "Isidro", ""], ["Bender", "Andreas", ""]]}, {"id": "1809.09081", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Andri Ashfahani, Yew Soon Ong, Savitha Ramasamy\n  and Edwin Lughofer", "title": "Autonomous Deep Learning: Incremental Learning of Denoising Autoencoder\n  for Evolving Data Streams", "comments": "have been submitted to AAAI 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generative learning phase of Autoencoder (AE) and its successor Denosing\nAutoencoder (DAE) enhances the flexibility of data stream method in exploiting\nunlabelled samples. Nonetheless, the feasibility of DAE for data stream\nanalytic deserves in-depth study because it characterizes a fixed network\ncapacity which cannot adapt to rapidly changing environments. An automated\nconstruction of a denoising autoeconder, namely deep evolving denoising\nautoencoder (DEVDAN), is proposed in this paper. DEVDAN features an open\nstructure both in the generative phase and in the discriminative phase where\ninput features can be automatically added and discarded on the fly. A network\nsignificance (NS) method is formulated in this paper and is derived from the\nbias-variance concept. This method is capable of estimating the statistical\ncontribution of the network structure and its hidden units which precursors an\nideal state to add or prune input features. Furthermore, DEVDAN is free of the\nproblem- specific threshold and works fully in the single-pass learning\nfashion. The efficacy of DEVDAN is numerically validated using nine\nnon-stationary data stream problems simulated under the prequential\ntest-then-train protocol where DEVDAN is capable of delivering an improvement\nof classification accuracy to recently published online learning works while\nhaving flexibility in the automatic extraction of robust input features and in\nadapting to rapidly changing environments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:49:09 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Ashfahani", "Andri", ""], ["Ong", "Yew Soon", ""], ["Ramasamy", "Savitha", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1809.09087", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Implicit Maximum Likelihood Estimation", "comments": "21 pages, 4 figures. In the interest of promoting discussion, we make\n  the reviews available at\n  https://people.eecs.berkeley.edu/~ke.li/papers/imle_reviews.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit probabilistic models are models defined naturally in terms of a\nsampling procedure and often induces a likelihood function that cannot be\nexpressed explicitly. We develop a simple method for estimating parameters in\nimplicit models that does not require knowledge of the form of the likelihood\nfunction or any derived quantities, but can be shown to be equivalent to\nmaximizing likelihood under some conditions. Our result holds in the\nnon-asymptotic parametric setting, where both the capacity of the model and the\nnumber of data examples are finite. We also demonstrate encouraging\nexperimental results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:57:25 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 17:56:37 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1809.09095", "submitter": "Yang Yu", "authors": "Zhen-Jia Pang, Ruo-Ze Liu, Zhou-Yu Meng, Yi Zhang, Yang Yu, Tong Lu", "title": "On Reinforcement Learning for Full-length Game of StarCraft", "comments": "Appeared in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StarCraft II poses a grand challenge for reinforcement learning. The main\ndifficulties of it include huge state and action space and a long-time horizon.\nIn this paper, we investigate a hierarchical reinforcement learning approach\nfor StarCraft II. The hierarchy involves two levels of abstraction. One is the\nmacro-action automatically extracted from expert's trajectories, which reduces\nthe action space in an order of magnitude yet remains effective. The other is a\ntwo-layer hierarchical architecture which is modular and easy to scale,\nenabling a curriculum transferring from simpler tasks to more complex tasks.\nThe reinforcement training algorithm for this architecture is also\ninvestigated. On a 64x64 map and using restrictive units, we achieve a winning\nrate of more than 99\\% against the difficulty level-1 built-in AI. Through the\ncurriculum transfer learning algorithm and a mixture of combat model, we can\nachieve over 93\\% winning rate of Protoss against the most difficult\nnon-cheating built-in AI (level-7) of Terran, training within two days using a\nsingle machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong\ngeneralization performance, when tested against never seen opponents including\ncheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We\nhope this study could shed some light on the future research of large-scale\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 15:48:28 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 18:00:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Pang", "Zhen-Jia", ""], ["Liu", "Ruo-Ze", ""], ["Meng", "Zhou-Yu", ""], ["Zhang", "Yi", ""], ["Yu", "Yang", ""], ["Lu", "Tong", ""]]}, {"id": "1809.09096", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Antonio Bruno", "title": "Text Summarization as Tree Transduction by Top-Down TreeLSTM", "comments": "To appear in IEEE SCCI Deep Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive compression is a challenging natural language processing problem.\nThis work contributes by formulating neural extractive compression as a parse\ntree transduction problem, rather than a sequence transduction task. Motivated\nby this, we introduce a deep neural model for learning\nstructure-to-substructure tree transductions by extending the standard Long\nShort-Term Memory, considering the parent-child relationships in the structural\nrecursion. The proposed model can achieve state of the art performance on\nsentence compression benchmarks, both in terms of accuracy and compression\nrate.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 11:00:57 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Bacciu", "Davide", ""], ["Bruno", "Antonio", ""]]}, {"id": "1809.09143", "submitter": "Kexin Huang", "authors": "Kexin Huang, Rodrigo Nogueira", "title": "EpiRL: A Reinforcement Learning Agent to Facilitate Epistasis Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epistasis (gene-gene interaction) is crucial to predicting genetic disease.\nOur work tackles the computational challenges faced by previous works in\nepistasis detection by modeling it as a one-step Markov Decision Process where\nthe state is genome data, the actions are the interacted genes, and the reward\nis an interaction measurement for the selected actions. A reinforcement\nlearning agent using policy gradient method then learns to discover a set of\nhighly interacted genes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:10:17 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Huang", "Kexin", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "1809.09147", "submitter": "Akshat Agarwal", "authors": "Akshat Agarwal, Abhinau Kumar V, Kyle Dunovan, Erik Peterson, Timothy\n  Verstynen, Katia Sycara", "title": "Better Safe than Sorry: Evidence Accumulation Allows for Safe\n  Reinforcement Learning", "comments": "8 pages, 3 figures. Code available at\n  https://github.com/agakshat/evidence-accumulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, agents often have to operate in situations with incomplete\ninformation, limited sensing capabilities, and inherently stochastic\nenvironments, making individual observations incomplete and unreliable.\nMoreover, in many situations it is preferable to delay a decision rather than\nrun the risk of making a bad decision. In such situations it is necessary to\naggregate information before taking an action; however, most state of the art\nreinforcement learning (RL) algorithms are biased towards taking actions\n\\textit{at every time step}, even if the agent is not particularly confident in\nits chosen action. This lack of caution can lead the agent to make critical\nmistakes, regardless of prior experience and acclimation to the environment.\nMotivated by theories of dynamic resolution of uncertainty during decision\nmaking in biological brains, we propose a simple accumulator module which\naccumulates evidence in favor of each possible decision, encodes uncertainty as\na dynamic competition between actions, and acts on the environment only when it\nis sufficiently confident in the chosen action. The agent makes no decision by\ndefault, and the burden of proof to make a decision falls on the policy to\naccrue evidence strongly in favor of a single decision. Our results show that\nthis accumulator module achieves near-optimal performance on a simple guessing\ngame, far outperforming deep recurrent networks using traditional, forced\naction selection policies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:13:01 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Agarwal", "Akshat", ""], ["Kumar", "Abhinau", "V"], ["Dunovan", "Kyle", ""], ["Peterson", "Erik", ""], ["Verstynen", "Timothy", ""], ["Sycara", "Katia", ""]]}, {"id": "1809.09165", "submitter": "Vitaly Feldman", "authors": "Amit Daniely and Vitaly Feldman", "title": "Locally Private Learning without Interaction Requires Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning under the constraint of local differential privacy\n(LDP). For many learning problems known efficient algorithms in this model\nrequire many rounds of communication between the server and the clients holding\nthe data points. Yet multi-round protocols are prohibitively slow in practice\ndue to network latency and, as a result, currently deployed large-scale systems\nare limited to a single round. Despite significant research interest, very\nlittle is known about which learning problems can be solved by such\nnon-interactive systems. The only lower bound we are aware of is for PAC\nlearning an artificial class of functions with respect to a uniform\ndistribution (Kasiviswanathan et al. 2011).\n  We show that the margin complexity of a class of Boolean functions is a lower\nbound on the complexity of any non-interactive LDP algorithm for\ndistribution-independent PAC learning of the class. In particular, the classes\nof linear separators and decision lists require exponential number of samples\nto learn non-interactively even though they can be learned in polynomial time\nby an interactive LDP algorithm. This gives the first example of a natural\nproblem that is significantly harder to solve without interaction and also\nresolves an open problem of Kasiviswanathan et al. (2011). We complement this\nlower bound with a new efficient learning algorithm whose complexity is\npolynomial in the margin complexity of the class. Our algorithm is\nnon-interactive on labeled samples but still needs interactive access to\nunlabeled samples. All of our results also apply to the statistical query model\nand any model in which the number of bits communicated about each data point is\nconstrained.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:57:36 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 02:35:29 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 06:20:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Daniely", "Amit", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1809.09170", "submitter": "Kailiang Wu", "authors": "Kailiang Wu, Dongbin Xiu", "title": "Numerical Aspects for Approximating Governing Equations Using Data", "comments": "26 pages, 17 figures", "journal-ref": "Journal of Computational Physics, 384, 200-221, 2019", "doi": "10.1016/j.jcp.2019.01.030", "report-no": null, "categories": "math.NA cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present effective numerical algorithms for locally recovering unknown\ngoverning differential equations from measurement data. We employ a set of\nstandard basis functions, e.g., polynomials, to approximate the governing\nequation with high accuracy. Upon recasting the problem into a function\napproximation problem, we discuss several important aspects for accurate\napproximation. Most notably, we discuss the importance of using a large number\nof short bursts of trajectory data, rather than using data from a single long\ntrajectory. Several options for the numerical algorithms to perform accurate\napproximation are then presented, along with an error estimate of the final\nequation approximation. We then present an extensive set of numerical examples\nof both linear and nonlinear systems to demonstrate the properties and\neffectiveness of our equation recovery algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 19:11:32 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1809.09215", "submitter": "Tavpritesh Sethi", "authors": "Tavpritesh Sethi, Anant Mittal, Shubham Maheshwari, Samarth Chugh", "title": "Learning to Address Health Inequality in the United States with a\n  Bayesian Decision Network", "comments": "8 pages, 4 figures, 1 table (excluding the supplementary material),\n  accepted for publication in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Life-expectancy is a complex outcome driven by genetic, socio-demographic,\nenvironmental and geographic factors. Increasing socio-economic and health\ndisparities in the United States are propagating the longevity-gap, making it a\ncause for concern. Earlier studies have probed individual factors but an\nintegrated picture to reveal quantifiable actions has been missing. There is a\ngrowing concern about a further widening of healthcare inequality caused by\nArtificial Intelligence (AI) due to differential access to AI-driven services.\nHence, it is imperative to explore and exploit the potential of AI for\nilluminating biases and enabling transparent policy decisions for positive\nsocial and health impact. In this work, we reveal actionable interventions for\ndecreasing the longevity-gap in the United States by analyzing a County-level\ndata resource containing healthcare, socio-economic, behavioral, education and\ndemographic features. We learn an ensemble-averaged structure, draw inferences\nusing the joint probability distribution and extend it to a Bayesian Decision\nNetwork for identifying policy actions. We draw quantitative estimates for the\nimpact of diversity, preventive-care quality and stable-families within the\nunified framework of our decision network. Finally, we make this analysis and\ndashboard available as an interactive web-application for enabling users and\npolicy-makers to validate our reported findings and to explore the impact of\nones beyond reported in this work.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 00:24:06 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 04:20:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Sethi", "Tavpritesh", ""], ["Mittal", "Anant", ""], ["Maheshwari", "Shubham", ""], ["Chugh", "Samarth", ""]]}, {"id": "1809.09219", "submitter": "Xiaolin Huang", "authors": "Fan He, Xiaolin Huang, Yipeng Liu, Ming Yan", "title": "Fast Signal Recovery from Saturated Measurements by Linear Loss and\n  Nonconvex Penalties", "comments": null, "journal-ref": "IEEE Signal Processing Letters, 25 (2018) 1374-1378", "doi": "10.1109/LSP.2018.2860242", "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign information is the key to overcoming the inevitable saturation error in\ncompressive sensing systems, which causes information loss and results in bias.\nFor sparse signal recovery from saturation, we propose to use a linear loss to\nimprove the effectiveness from existing methods that utilize hard\nconstraints/hinge loss for sign consistency. Due to the use of linear loss, an\nanalytical solution in the update progress is obtained, and some nonconvex\npenalties are applicable, e.g., the minimax concave penalty, the $\\ell_0$ norm,\nand the sorted $\\ell_1$ norm. Theoretical analysis reveals that the estimation\nerror can still be bounded. Generally, with linear loss and nonconvex\npenalties, the recovery performance is significantly improved, and the\ncomputational time is largely saved, which is verified by the numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 01:05:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["He", "Fan", ""], ["Huang", "Xiaolin", ""], ["Liu", "Yipeng", ""], ["Yan", "Ming", ""]]}, {"id": "1809.09238", "submitter": "Putu Ayu Sudyanti", "authors": "Putu Ayu Sudyanti and Vinayak Rao", "title": "Flexible Mixture Modeling on Constrained Spaces", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses challenges in flexibly modeling multimodal data that lie\non constrained spaces. Such data are commonly found in spatial applications,\nsuch as climatology and criminology, where measurements are restricted to a\ngeographical area. Other settings include domains where unsuitable recordings\nare discarded, such as flow-cytometry measurements. A simple approach to\nmodeling such data is through the use of mixture models, especially\nnonparametric mixture models. Mixture models, while flexible and theoretically\nwell-understood, are unsuitable for settings involving complicated constraints,\nleading to difficulties in specifying the component distributions and in\nevaluating normalization constants. Bayesian inference over the parameters of\nthese models results in posterior distributions that are doubly-intractable. We\naddress this problem via an algorithm based on rejection sampling and data\naugmentation. We view samples from a truncated distribution as outcomes of a\nrejection sampling scheme, where proposals are made from a simple mixture model\nand are rejected if they violate the constraints. Our scheme proceeds by\nimputing the rejected samples given mixture parameters and then resampling\nparameters given all samples. We study two modeling approaches: mixtures of\ntruncated Gaussians and truncated mixtures of Gaussians, along with their\nassociated Markov chain Monte Carlo sampling algorithms. We also discuss\nvariations of the models, as well as approximations to improve mixing, reduce\ncomputational cost, and lower variance. We present results on simulated data\nand apply our algorithms to two problems; one involving flow-cytometry data,\nand the other, crime recorded in the city of Chicago.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 22:13:03 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 02:41:51 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sudyanti", "Putu Ayu", ""], ["Rao", "Vinayak", ""]]}, {"id": "1809.09244", "submitter": "Michele Covell", "authors": "Shumeet Baluja and David Marwood and Michele Covell and Nick Johnston", "title": "No Multiplication? No Floating Point? No Problem! Training Networks for\n  Efficient Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For successful deployment of deep neural networks on\nhighly--resource-constrained devices (hearing aids, earbuds, wearables), we\nmust simplify the types of operations and the memory/power resources used\nduring inference. Completely avoiding inference-time floating-point operations\nis one of the simplest ways to design networks for these highly-constrained\nenvironments. By discretizing both our in-network non-linearities and our\nnetwork weights, we can move to simple, compact networks without floating point\noperations, without multiplications, and avoid all non-linear function\ncomputations. Our approach allows us to explore the spectrum of possible\nnetworks, ranging from fully continuous versions down to networks with bi-level\nweights and activations. Our results show that discretization can be done\nwithout loss of performance and that we can train a network that will\nsuccessfully operate without floating-point, without multiplication, and with\nless RAM on both regression tasks (auto encoding) and multi-class\nclassification tasks (ImageNet). The memory needed to deploy our discretized\nnetworks is less than one third of the equivalent architecture that does use\nfloating-point operations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 22:29:24 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 16:11:32 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Baluja", "Shumeet", ""], ["Marwood", "David", ""], ["Covell", "Michele", ""], ["Johnston", "Nick", ""]]}, {"id": "1809.09245", "submitter": "John Henry Hinnefeld", "authors": "J. Henry Hinnefeld, Peter Cooman, Nat Mammo, Rupert Deese", "title": "Evaluating Fairness Metrics in the Presence of Dataset Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven algorithms play a large role in decision making across a variety\nof industries. Increasingly, these algorithms are being used to make decisions\nthat have significant ramifications for people's social and economic\nwell-being, e.g. in sentencing, loan approval, and policing. Amid the\nproliferation of such systems there is a growing concern about their potential\ndiscriminatory impact. In particular, machine learning systems which are\ntrained on biased data have the potential to learn and perpetuate those biases.\nA central challenge for practitioners is thus to determine whether their models\ndisplay discriminatory bias. Here we present a case study in which we frame the\nissue of bias detection as a causal inference problem with observational data.\nWe enumerate two main causes of bias, sampling bias and label bias, and we\ninvestigate the abilities of six different fairness metrics to detect each bias\ntype. Based on these investigations, we propose a set of best practice\nguidelines to select the fairness metric that is most likely to detect bias if\nit is present. Additionally, we aim to identify the conditions in which certain\nfairness metrics may fail to detect bias and instead give practitioners a false\nbelief that their biased model is making fair decisions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 22:32:05 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Hinnefeld", "J. Henry", ""], ["Cooman", "Peter", ""], ["Mammo", "Nat", ""], ["Deese", "Rupert", ""]]}, {"id": "1809.09260", "submitter": "Jeffrey McKinstry", "authors": "Jeffrey L Mckinstry, Davis R. Barch, Deepika Bablani, Michael V.\n  Debole, Steven K. Esser, Jeffrey A. Kusnitz, John V. Arthur, Dharmendra S.\n  Modha", "title": "Low Precision Policy Distillation with Application to Low-Power,\n  Real-time Sensation-Cognition-Action Loop with Neuromorphic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low precision networks in the reinforcement learning (RL) setting are\nrelatively unexplored because of the limitations of binary activations for\nfunction approximation. Here, in the discrete action ATARI domain, we\ndemonstrate, for the first time, that low precision policy distillation from a\nhigh precision network provides a principled, practical way to train an RL\nagent. As an application, on 10 different ATARI games, we demonstrate real-time\nend-to-end game playing on low-power neuromorphic hardware by converting a\nsequence of game frames into discrete actions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 00:03:33 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Mckinstry", "Jeffrey L", ""], ["Barch", "Davis R.", ""], ["Bablani", "Deepika", ""], ["Debole", "Michael V.", ""], ["Esser", "Steven K.", ""], ["Kusnitz", "Jeffrey A.", ""], ["Arthur", "John V.", ""], ["Modha", "Dharmendra S.", ""]]}, {"id": "1809.09262", "submitter": "Luca de Alfaro", "authors": "Luca de Alfaro", "title": "Neural Networks with Structural Resistance to Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adversarial attacks to machine-learning classifiers, small perturbations\nare added to input that is correctly classified. The perturbations yield\nadversarial examples, which are virtually indistinguishable from the\nunperturbed input, and yet are misclassified. In standard neural networks used\nfor deep learning, attackers can craft adversarial examples from most input to\ncause a misclassification of their choice.\n  We introduce a new type of network units, called RBFI units, whose non-linear\nstructure makes them inherently resistant to adversarial attacks. On\npermutation-invariant MNIST, in absence of adversarial attacks, networks using\nRBFI units match the performance of networks using sigmoid units, and are\nslightly below the accuracy of networks with ReLU units. When subjected to\nadversarial attacks, networks with RBFI units retain accuracies above 90% for\nattacks that degrade the accuracy of networks with ReLU or sigmoid units to\nbelow 2%. RBFI networks trained with regular input are superior in their\nresistance to adversarial attacks even to ReLU and sigmoid networks trained\nwith the help of adversarial examples.\n  The non-linear structure of RBFI units makes them difficult to train using\nstandard gradient descent. We show that networks of RBFI units can be\nefficiently trained to high accuracies using pseudogradients, computed using\nfunctions especially crafted to facilitate learning instead of their true\nderivatives. We show that the use of pseudogradients makes training deep RBFI\nnetworks practical, and we compare several structural alternatives of RBFI\nnetworks for their accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 00:08:10 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["de Alfaro", "Luca", ""]]}, {"id": "1809.09266", "submitter": "Ioannis Schizas", "authors": "Ioannis D. Schizas", "title": "Graph filtering for data reduction and reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach is put forth that utilizes data similarity, quantified on a\ngraph, to improve upon the reconstruction performance of principal component\nanalysis. The tasks of data dimensionality reduction and reconstruction are\nformulated as graph filtering operations, that enable the exploitation of data\nnode connectivity in a graph via the adjacency matrix. The unknown reducing and\nreconstruction filters are determined by optimizing a mean-square error cost\nthat entails the data, as well as their graph adjacency matrix. Working in the\ngraph spectral domain enables the derivation of simple gradient descent\nrecursions used to update the matrix filter taps. Numerical tests in real image\ndatasets demonstrate the better reconstruction performance of the novel method\nover standard principal component analysis.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 00:20:40 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Schizas", "Ioannis D.", ""]]}, {"id": "1809.09307", "submitter": "Daeyoung Choi", "authors": "Daeyoung Choi and Wonjong Rhee", "title": "Utilizing Class Information for Deep Network Representation Shaping", "comments": "Published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical characteristics of deep network representations, such as sparsity\nand correlation, are known to be relevant to the performance and\ninterpretability of deep learning. When a statistical characteristic is\ndesired, often an adequate regularizer can be designed and applied during the\ntraining phase. Typically, such a regularizer aims to manipulate a statistical\ncharacteristic over all classes together. For classification tasks, however, it\nmight be advantageous to enforce the desired characteristic per class such that\ndifferent classes can be better distinguished. Motivated by the idea, we design\ntwo class-wise regularizers that explicitly utilize class information:\nclass-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer\n(cw-VR). cw-CR targets to reduce the covariance of representations calculated\nfrom the same class samples for encouraging feature independence. cw-VR is\nsimilar, but variance instead of covariance is targeted to improve feature\ncompactness. For the sake of completeness, their counterparts without using\nclass information, Covariance Regularizer (CR) and Variance Regularizer (VR),\nare considered together. The four regularizers are conceptually simple and\ncomputationally very efficient, and the visualization shows that the\nregularizers indeed perform distinct representation shaping. In terms of\nclassification performance, significant improvements over the baseline and\nL1/L2 weight regularization methods were found for 21 out of 22 tasks over\npopular benchmark datasets. In particular, cw-VR achieved the best performance\nfor 13 tasks including ResNet-32/110.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:50:59 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 03:58:50 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Choi", "Daeyoung", ""], ["Rhee", "Wonjong", ""]]}, {"id": "1809.09318", "submitter": "Vikas Dhiman", "authors": "Vikas Dhiman, Shurjo Banerjee, Jeffrey M. Siskind and Jason J. Corso", "title": "Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to\n  Reach New Goals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider mutli-goal tasks that involve static environments and dynamic goals.\nExamples of such tasks, such as goal-directed navigation and pick-and-place in\nrobotics, abound. Two types of Reinforcement Learning (RL) algorithms are used\nfor such tasks: model-free or model-based. Each of these approaches has\nlimitations. Model-free RL struggles to transfer learned information when the\ngoal location changes, but achieves high asymptotic accuracy in single goal\ntasks. Model-based RL can transfer learned information to new goal locations by\nretaining the explicitly learned state-dynamics, but is limited by the fact\nthat small errors in modelling these dynamics accumulate over long-term\nplanning. In this work, we improve upon the limitations of model-free RL in\nmulti-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL\nand call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns\na goal-conditioned action-value function by constraining the value of the\noptimal path between any two states to be greater than or equal to the value of\npaths via intermediary states. Experimentally, we show that FWRL is more\nsample-efficient and learns higher reward strategies in multi-goal tasks as\ncompared to Q-learning, model-based RL and other relevant baselines in a\ntabular domain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:09:32 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 20:23:32 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 18:33:20 GMT"}, {"version": "v4", "created": "Fri, 4 Jan 2019 20:53:40 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Dhiman", "Vikas", ""], ["Banerjee", "Shurjo", ""], ["Siskind", "Jeffrey M.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1809.09321", "submitter": "Bryan Daniels", "authors": "Bryan C. Daniels, William S. Ryu, and Ilya Nemenman", "title": "Automated, predictive, and interpretable inference of C. elegans escape\n  dynamics", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": "10.1073/pnas.1816531116", "report-no": null, "categories": "q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The roundworm C. elegans exhibits robust escape behavior in response to\nrapidly rising temperature. The behavior lasts for a few seconds, shows history\ndependence, involves both sensory and motor systems, and is too complicated to\nmodel mechanistically using currently available knowledge. Instead we model the\nprocess phenomenologically, and we use the Sir Isaac dynamical inference\nplatform to infer the model in a fully automated fashion directly from\nexperimental data. The inferred model requires incorporation of an unobserved\ndynamical variable, and is biologically interpretable. The model makes accurate\npredictions about the dynamics of the worm behavior, and it can be used to\ncharacterize the functional logic of the dynamical system underlying the escape\nresponse. This work illustrates the power of modern artificial intelligence to\naid in discovery of accurate and interpretable models of complex natural\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:15:40 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Daniels", "Bryan C.", ""], ["Ryu", "William S.", ""], ["Nemenman", "Ilya", ""]]}, {"id": "1809.09350", "submitter": "Chaobing Song", "authors": "Chaobing Song, Ji Liu, Han Liu, Yong Jiang, Tong Zhang", "title": "Fully Implicit Online Learning", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized online learning is widely used in machine learning applications.\nIn online learning, performing exact minimization ($i.e.,$ implicit update) is\nknown to be beneficial to the numerical stability and structure of solution. In\nthis paper we study a class of regularized online algorithms without\nlinearizing the loss function or the regularizer, which we call \\emph{fully\nimplicit online learning} (FIOL). We show that for arbitrary Bregman\ndivergence, FIOL has the $O(\\sqrt{T})$ regret for general convex setting and\n$O(\\log T)$ regret for strongly convex setting, and the regret has an one-step\nimprovement effect because it avoids the approximation error of linearization.\nThen we propose efficient algorithms to solve the subproblem of FIOL. We show\nthat even if the solution of the subproblem has no closed form, it can be\nsolved with complexity comparable to the linearized online algoritms.\nExperiments validate the proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 07:16:17 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 15:47:11 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 19:35:42 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Song", "Chaobing", ""], ["Liu", "Ji", ""], ["Liu", "Han", ""], ["Jiang", "Yong", ""], ["Zhang", "Tong", ""]]}, {"id": "1809.09367", "submitter": "Edgar Steiger", "authors": "Edgar Steiger, Martin Vingron", "title": "Sparse-Group Bayesian Feature Selection Using Expectation Propagation\n  for Signal Recovery and Network Reconstruction", "comments": "44 pages, 15 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian method for feature selection in the presence of\ngrouping information with sparsity on the between- and within group level.\nInstead of using a stochastic algorithm for parameter inference, we employ\nexpectation propagation, which is a deterministic and fast algorithm. Available\nmethods for feature selection in the presence of grouping information have a\nnumber of short-comings: on one hand, lasso methods, while being fast,\nunderestimate the regression coefficients and do not make good use of the\ngrouping information, and on the other hand, Bayesian approaches, while\naccurate in parameter estimation, often rely on the stochastic and slow Gibbs\nsampling procedure to recover the parameters, rendering them infeasible e.g.\nfor gene network reconstruction. Our approach of a Bayesian sparse-group\nframework with expectation propagation enables us to not only recover accurate\nparameter estimates in signal recovery problems, but also makes it possible to\napply this Bayesian framework to large-scale network reconstruction problems.\nThe presented method is generic but in terms of application we focus on gene\nregulatory networks. We show on simulated and experimental data that the method\nconstitutes a good choice for network reconstruction regarding the number of\ncorrectly selected features, prediction on new data and reasonable computing\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 09:08:26 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Steiger", "Edgar", ""], ["Vingron", "Martin", ""]]}, {"id": "1809.09369", "submitter": "Antonin Raffin", "authors": "Antonin Raffin and Ashley Hill and Ren\\'e Traor\\'e and Timoth\\'ee\n  Lesort and Natalia D\\'iaz-Rodr\\'iguez and David Filliat", "title": "S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State\n  Representation Learning", "comments": "Github repo: https://github.com/araffin/robotics-rl-srl\n  Documentation: https://s-rl-toolbox.readthedocs.io/en/latest/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State representation learning aims at learning compact representations from\nraw observations in robotics and control applications. Approaches used for this\nobjective are auto-encoders, learning forward models, inverse dynamics or\nlearning using generic priors on the state characteristics. However, the\ndiversity in applications and methods makes the field lack standard evaluation\ndatasets, metrics and tasks. This paper provides a set of environments, data\ngenerators, robotic control tasks, metrics and tools to facilitate iterative\nstate representation learning and evaluation in reinforcement learning\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 09:11:49 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 09:40:45 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Raffin", "Antonin", ""], ["Hill", "Ashley", ""], ["Traor\u00e9", "Ren\u00e9", ""], ["Lesort", "Timoth\u00e9e", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Filliat", "David", ""]]}, {"id": "1809.09399", "submitter": "Sergey Sukhov", "authors": "Mikhail Iu. Leontev, Viktoriia Islenteva, Sergey V. Sukhov", "title": "Non-Iterative Knowledge Fusion in Deep Convolutional Neural Networks", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": "10.1007/s11063-019-10074-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporation of a new knowledge into neural networks with simultaneous\npreservation of the previous one is known to be a nontrivial problem. This\nproblem becomes even more complex when new knowledge is contained not in new\ntraining examples, but inside the parameters (connection weights) of another\nneural network. Here we propose and test two methods allowing combining the\nknowledge contained in separate networks. One method is based on a simple\noperation of summation of weights of constituent neural networks. Another\nmethod assumes incorporation of a new knowledge by modification of weights\nnonessential for the preservation of already stored information. We show that\nwith these methods the knowledge from one network can be transferred into\nanother one non-iteratively without requiring training sessions. The fused\nnetwork operates efficiently, performing classification far better than a\nchance level. The efficiency of the methods is quantified on several publicly\navailable data sets in classification tasks both for shallow and deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 10:29:18 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Leontev", "Mikhail Iu.", ""], ["Islenteva", "Viktoriia", ""], ["Sukhov", "Sergey V.", ""]]}, {"id": "1809.09401", "submitter": "Yifan Feng", "authors": "Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, Yue Gao", "title": "Hypergraph Neural Networks", "comments": "Accepted in AAAI'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a hypergraph neural networks (HGNN) framework for\ndata representation learning, which can encode high-order data correlation in a\nhypergraph structure. Confronting the challenges of learning representation for\ncomplex data in real practice, we propose to incorporate such data structure in\na hypergraph, which is more flexible on data modeling, especially when dealing\nwith complex data. In this method, a hyperedge convolution operation is\ndesigned to handle the data correlation during representation learning. In this\nway, traditional hypergraph learning procedure can be conducted using hyperedge\nconvolution operations efficiently. HGNN is able to learn the hidden layer\nrepresentation considering the high-order data structure, which is a general\nframework considering the complex data correlations. We have conducted\nexperiments on citation network classification and visual object recognition\ntasks and compared HGNN with graph convolutional networks and other traditional\nmethods. Experimental results demonstrate that the proposed HGNN method\noutperforms recent state-of-the-art methods. We can also reveal from the\nresults that the proposed HGNN is superior when dealing with multi-modal data\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 10:42:28 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 03:13:42 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 02:48:42 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Feng", "Yifan", ""], ["You", "Haoxuan", ""], ["Zhang", "Zizhao", ""], ["Ji", "Rongrong", ""], ["Gao", "Yue", ""]]}, {"id": "1809.09445", "submitter": "Yousra El-Bachir", "authors": "Yousra El-Bachir and Anthony C. Davison", "title": "Fast Automatic Smoothing for Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple generalized additive models (GAMs) are a type of distributional\nregression wherein parameters of probability distributions depend on predictors\nthrough smooth functions, with selection of the degree of smoothness via $L_2$\nregularization. Multiple GAMs allow finer statistical inference by\nincorporating explanatory information in any or all of the parameters of the\ndistribution. Owing to their nonlinearity, flexibility and interpretability,\nGAMs are widely used, but reliable and fast methods for automatic smoothing in\nlarge datasets are still lacking, despite recent advances. We develop a general\nmethodology for automatically learning the optimal degree of $L_2$\nregularization for multiple GAMs using an empirical Bayes approach. The smooth\nfunctions are penalized by different amounts, which are learned simultaneously\nby maximization of a marginal likelihood through an approximate\nexpectation-maximization algorithm that involves a double Laplace approximation\nat the E-step, and leads to an efficient M-step. Empirical analysis shows that\nthe resulting algorithm is numerically stable, faster than all existing methods\nand achieves state-of-the-art accuracy. For illustration, we apply it to an\nimportant and challenging problem in the analysis of extremal data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 12:59:01 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["El-Bachir", "Yousra", ""], ["Davison", "Anthony C.", ""]]}, {"id": "1809.09446", "submitter": "Jacques Wainer", "authors": "Jacques Wainer and Gavin Cawley", "title": "Nested cross-validation when selecting classifiers is overzealous for\n  most practical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When selecting a classification algorithm to be applied to a particular\nproblem, one has to simultaneously select the best algorithm for that dataset\n\\emph{and} the best set of hyperparameters for the chosen model. The usual\napproach is to apply a nested cross-validation procedure; hyperparameter\nselection is performed in the inner cross-validation, while the outer\ncross-validation computes an unbiased estimate of the expected accuracy of the\nalgorithm \\emph{with cross-validation based hyperparameter tuning}. The\nalternative approach, which we shall call `flat cross-validation', uses a\nsingle cross-validation step both to select the optimal hyperparameter values\nand to provide an estimate of the expected accuracy of the algorithm, that\nwhile biased may nevertheless still be used to select the best learning\nalgorithm. We tested both procedures using 12 different algorithms on 115 real\nlife binary datasets and conclude that using the less computationally expensive\nflat cross-validation procedure will generally result in the selection of an\nalgorithm that is, for all practical purposes, of similar quality to that\nselected via nested cross-validation, provided the learning algorithms have\nrelatively few hyperparameters to be optimised.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 12:59:05 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Wainer", "Jacques", ""], ["Cawley", "Gavin", ""]]}, {"id": "1809.09501", "submitter": "Matthieu Geist", "authors": "Matthieu Geist and Bruno Scherrer", "title": "Anderson Acceleration for Reinforcement Learning", "comments": "European Workshop on Reinforcement Learning (EWRL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anderson acceleration is an old and simple method for accelerating the\ncomputation of a fixed point. However, as far as we know and quite\nsurprisingly, it has never been applied to dynamic programming or reinforcement\nlearning. In this paper, we explain briefly what Anderson acceleration is and\nhow it can be applied to value iteration, this being supported by preliminary\nexperiments showing a significant speed up of convergence, that we critically\ndiscuss. We also discuss how this idea could be applied more generally to\n(deep) reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:04:25 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Geist", "Matthieu", ""], ["Scherrer", "Bruno", ""]]}, {"id": "1809.09505", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Miko{\\l}aj Kasprzak, Tamara\n  Broderick", "title": "Practical bounds on the error of Bayesian posterior approximations: A\n  nonasymptotic approach", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:11:32 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:33:00 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1809.09533", "submitter": "Faik Boray Tek", "authors": "F. Boray Tek", "title": "An Adaptive Locally Connected Neuron Model: Focusing Neuron", "comments": "45 pages, a national patent filed, submitted to Turkish Patent\n  Office, No: -2017/17601, Date: 09.11.2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new artificial neuron model capable of learning its\nreceptive field in the topological domain of inputs. The model provides\nadaptive and differentiable local connectivity (plasticity) applicable to any\ndomain. It requires no other tool than the backpropagation algorithm to learn\nits parameters which control the receptive field locations and apertures. This\nresearch explores whether this ability makes the neuron focus on informative\ninputs and yields any advantage over fully connected neurons. The experiments\ninclude tests of focusing neuron networks of one or two hidden layers on\nsynthetic and well-known image recognition data sets. The results demonstrated\nthat the focusing neurons can move their receptive fields towards more\ninformative inputs. In the simple two-hidden layer networks, the focusing\nlayers outperformed the dense layers in the classification of the 2D spatial\ndata sets. Moreover, the focusing networks performed better than the dense\nnetworks even when 70$\\%$ of the weights were pruned. The tests on\nconvolutional networks revealed that using focusing layers instead of dense\nlayers for the classification of convolutional features may work better in some\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:15:32 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 13:29:37 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 11:11:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Tek", "F. Boray", ""]]}, {"id": "1809.09569", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Dan Moldovan and Alexander B Wiltschko", "title": "Tangent: Automatic differentiation using source-code transformation for\n  dynamically typed array programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to efficiently calculate first- and higher-order derivatives of\nincreasingly complex models expressed in Python has stressed or exceeded the\ncapabilities of available tools. In this work, we explore techniques from the\nfield of automatic differentiation (AD) that can give researchers expressive\npower, performance and strong usability. These include source-code\ntransformation (SCT), flexible gradient surgery, efficient in-place array\noperations, higher-order derivatives as well as mixing of forward and reverse\nmode AD. We implement and demonstrate these ideas in the Tangent software\nlibrary for Python, the first AD framework for a dynamic language that uses\nSCT.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:08:37 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 14:13:37 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Moldovan", "Dan", ""], ["Wiltschko", "Alexander B", ""]]}, {"id": "1809.09573", "submitter": "Yuxin Chen", "authors": "Yuejie Chi, Yue M. Lu, Yuxin Chen", "title": "Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview", "comments": "Invited overview article", "journal-ref": "IEEE Transactions on Signal Processing, vol. 67, no. 20, pp.\n  5239-5269, October 2019", "doi": "10.1109/TSP.2019.2937282", "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial progress has been made recently on developing provably accurate\nand efficient algorithms for low-rank matrix factorization via nonconvex\noptimization. While conventional wisdom often takes a dim view of nonconvex\noptimization algorithms due to their susceptibility to spurious local minima,\nsimple iterative methods such as gradient descent have been remarkably\nsuccessful in practice. The theoretical footings, however, had been largely\nlacking until recently.\n  In this tutorial-style overview, we highlight the important role of\nstatistical models in enabling efficient nonconvex optimization with\nperformance guarantees. We review two contrasting approaches: (1) two-stage\nalgorithms, which consist of a tailored initialization step followed by\nsuccessive refinement; and (2) global landscape analysis and\ninitialization-free algorithms. Several canonical matrix factorization problems\nare discussed, including but not limited to matrix sensing, phase retrieval,\nmatrix completion, blind deconvolution, robust principal component analysis,\nphase synchronization, and joint alignment. Special care is taken to illustrate\nthe key technical insights underlying their analyses. This article serves as a\ntestament that the integrated consideration of optimization and statistics\nleads to fruitful research findings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:21:07 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 22:56:03 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 17:00:59 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chi", "Yuejie", ""], ["Lu", "Yue M.", ""], ["Chen", "Yuxin", ""]]}, {"id": "1809.09574", "submitter": "Jaehoon Koo", "authors": "Jaehoon Koo, Diego Klabjan, Jean Utke", "title": "Combined convolutional and recurrent neural networks for hierarchical\n  classification of images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models based on CNNs are predominantly used in image\nclassification tasks. Such approaches, assuming independence of object\ncategories, normally use a CNN as a feature learner and apply a flat classifier\non top of it. Object classes in many settings have hierarchical relations, and\nclassifiers exploiting these relations should perform better. We propose\nhierarchical classification models combining a CNN to extract hierarchical\nrepresentations of images, and an RNN or sequence-to-sequence model to capture\na hierarchical tree of classes. In addition, we apply residual learning to the\nRNN part in oder to facilitate training our compound model and improve\ngeneralization of the model. Experimental results on a real world proprietary\ndataset of images show that our hierarchical networks perform better than\nstate-of-the-art CNNs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:23:45 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 22:47:07 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 17:32:54 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Koo", "Jaehoon", ""], ["Klabjan", "Diego", ""], ["Utke", "Jean", ""]]}, {"id": "1809.09582", "submitter": "Negin Golrezaei", "authors": "Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni,\n  Jon Schneider", "title": "Contextual Bandits with Cross-learning", "comments": "48 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical contextual bandits problem, in each round $t$, a learner\nobserves some context $c$, chooses some action $a$ to perform, and receives\nsome reward $r_{a,t}(c)$. We consider the variant of this problem where in\naddition to receiving the reward $r_{a,t}(c)$, the learner also learns the\nvalues of $r_{a,t}(c')$ for all other contexts $c'$; i.e., the rewards that\nwould have been achieved by performing that action under different contexts.\nThis variant arises in several strategic settings, such as learning how to bid\nin non-truthful repeated auctions (in this setting the context is the decision\nmaker's private valuation for each auction). We call this problem the\ncontextual bandits problem with cross-learning. The best algorithms for the\nclassical contextual bandits problem achieve $\\tilde{O}(\\sqrt{CKT})$ regret\nagainst all stationary policies, where $C$ is the number of contexts, $K$ the\nnumber of actions, and $T$ the number of rounds. We demonstrate algorithms for\nthe contextual bandits problem with cross-learning that remove the dependence\non $C$ and achieve regret $O(\\sqrt{KT})$ (when contexts are stochastic with\nknown distribution), $\\tilde{O}(K^{1/3}T^{2/3})$ (when contexts are stochastic\nwith unknown distribution), and $\\tilde{O}(\\sqrt{KT})$ (when contexts are\nadversarial but rewards are stochastic).\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:40:44 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 23:04:28 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Balseiro", "Santiago", ""], ["Golrezaei", "Negin", ""], ["Mahdian", "Mohammad", ""], ["Mirrokni", "Vahab", ""], ["Schneider", "Jon", ""]]}, {"id": "1809.09621", "submitter": "Ilya Trofimov", "authors": "Ilya Trofimov", "title": "Inferring Complementary Products from Baskets and Browsing Sessions", "comments": "Workshop on Intelligent Recommender Systems by Knowledge Transfer and\n  Learning (RecSysKTL'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary products recommendation is an important problem in e-commerce.\nSuch recommendations increase the average order price and the number of\nproducts in baskets. Complementary products are typically inferred from basket\ndata. In this study, we propose the BB2vec model. The BB2vec model learns\nvector representations of products by analyzing jointly two types of data -\nBaskets and Browsing sessions (visiting web pages of products). These vector\nrepresentations are used for making complementary products recommendation. The\nproposed model alleviates the cold start problem by delivering better\nrecommendations for products having few or no purchases. We show that the\nBB2vec model has better performance than other models which use only basket\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 08:38:05 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Trofimov", "Ilya", ""]]}, {"id": "1809.09703", "submitter": "Klaus Broelemann", "authors": "Klaus Broelemann and Gjergji Kasneci", "title": "A Gradient-Based Split Criterion for Highly Accurate and Transparent\n  Model Trees", "comments": null, "journal-ref": "Proceedings of the Twenty-Eighth International Joint Conference on\n  Artificial Intelligence, {IJCAI} 2019", "doi": "10.24963/ijcai.2019/281", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms aim at minimizing the number of false decisions\nand increasing the accuracy of predictions. However, the high predictive power\nof advanced algorithms comes at the costs of transparency. State-of-the-art\nmethods, such as neural networks and ensemble methods, often result in highly\ncomplex models that offer little transparency.\n  We propose shallow model trees as a way to combine simple and highly\ntransparent predictive models for higher predictive power without losing the\ntransparency of the original models. We present a novel split criterion for\nmodel trees that allows for significantly higher predictive power than\nstate-of-the-art model trees while maintaining the same level of simplicity.\nThis novel approach finds split points which allow the underlying simple models\nto make better predictions on the corresponding data. In addition, we introduce\nmultiple mechanisms to increase the transparency of the resulting trees.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 20:22:40 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 09:03:55 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Broelemann", "Klaus", ""], ["Kasneci", "Gjergji", ""]]}, {"id": "1809.09853", "submitter": "Liu Liu", "authors": "Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, Dacheng Tao", "title": "Stochastic Second-order Methods for Non-convex Optimization with Inexact\n  Hessian and Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust region and cubic regularization methods have demonstrated good\nperformance in small scale non-convex optimization, showing the ability to\nescape from saddle points. Each iteration of these methods involves computation\nof gradient, Hessian and function value in order to obtain the search direction\nand adjust the radius or cubic regularization parameter. However, exactly\ncomputing those quantities are too expensive in large-scale problems such as\ntraining deep networks. In this paper, we study a family of stochastic trust\nregion and cubic regularization methods when gradient, Hessian and function\nvalues are computed inexactly, and show the iteration complexity to achieve\n$\\epsilon$-approximate second-order optimality is in the same order with\nprevious work for which gradient and function values are computed exactly. The\nmild conditions on inexactness can be achieved in finite-sum minimization using\nrandom sampling. We show the algorithm performs well on training convolutional\nneural networks compared with previous second-order methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 08:59:09 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Liu", "Liu", ""], ["Liu", "Xuanqing", ""], ["Hsieh", "Cho-Jui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1809.09910", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Lei Shi, Xiaolin Huang, Jie Yang, and Johan A.K. Suykens", "title": "Generalization Properties of hyper-RKHS and its Applications", "comments": "38 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes regularized regression problems in a hyper-reproducing\nkernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning\nand out-of-sample extensions, and proves asymptotic convergence results for the\nintroduced regression models in an approximation theory view. Algorithmically,\nwe consider two regularized regression models with bivariate forms in this\nspace, including kernel ridge regression (KRR) and support vector regression\n(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with\nNystr\\\"{o}m approximation for scalability in large sample cases. This framework\nis general: the underlying kernel is learned from a broad class, and can be\npositive definite or not, which adapts to various requirements in kernel\nlearning. Theoretically, we study the convergence behavior of regularized\nregression algorithms in hyper-RKHS and derive the learning rates, which goes\nbeyond the classical analysis on RKHS due to the non-trivial independence of\npairwise samples and the characterisation of hyper-RKHS. Experimentally,\nresults on several benchmarks suggest that the employed framework is able to\nlearn a general kernel function form an arbitrary similarity matrix, and thus\nachieves a satisfactory performance on classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 11:11:09 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 16:38:52 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 18:37:13 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liu", "Fanghui", ""], ["Shi", "Lei", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1809.09925", "submitter": "Yawei Luo", "authors": "Yawei Luo, Tao Guan, Junqing Yu, Ping Liu, Yi Yang", "title": "Every Node Counts: Self-Ensembling Graph Convolutional Networks for\n  Semi-Supervised Learning", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) provides a powerful means for graph-based\nsemi-supervised tasks. However, as a localized first-order approximation of\nspectral graph convolution, the classic GCN can not take full advantage of\nunlabeled data, especially when the unlabeled node is far from labeled ones. To\ncapitalize on the information from unlabeled nodes to boost the training for\nGCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which\nmarries GCN with Mean Teacher - another powerful model in semi-supervised\nlearning. SEGCN contains a student model and a teacher model. As a student, it\nnot only learns to correctly classify the labeled nodes, but also tries to be\nconsistent with the teacher on unlabeled nodes in more challenging situations,\nsuch as a high dropout rate and graph collapse. As a teacher, it averages the\nstudent model weights and generates more accurate predictions to lead the\nstudent. In such a mutual-promoting process, both labeled and unlabeled samples\ncan be fully utilized for backpropagating effective gradients to train GCN. In\nthree article classification tasks, i.e. Citeseer, Cora and Pubmed, we validate\nthat the proposed method matches the state of the arts in the classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 11:59:00 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Luo", "Yawei", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Liu", "Ping", ""], ["Yang", "Yi", ""]]}, {"id": "1809.09953", "submitter": "Max Farrell", "authors": "Max H. Farrell and Tengyuan Liang and Sanjog Misra", "title": "Deep Neural Networks for Estimation and Inference", "comments": null, "journal-ref": "Econometrica, vol 89, no 1, 181-213, 2021", "doi": "10.3982/ECTA16901", "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deep neural networks and their use in semiparametric inference. We\nestablish novel rates of convergence for deep feedforward neural nets. Our new\nrates are sufficiently fast (in some cases minimax optimal) to allow us to\nestablish valid second-step inference after first-step estimation with deep\nlearning, a result also new to the literature. Our estimation rates and\nsemiparametric inference results handle the current standard architecture:\nfully connected feedforward neural networks (multi-layer perceptrons), with the\nnow-common rectified linear unit activation function and a depth explicitly\ndiverging with the sample size. We discuss other architectures as well,\nincluding fixed-width, very deep networks. We establish nonasymptotic bounds\nfor these deep nets for a general class of nonparametric regression-type loss\nfunctions, which includes as special cases least squares, logistic regression,\nand other generalized linear models. We then apply our theory to develop\nsemiparametric inference, focusing on causal parameters for concreteness, such\nas treatment effects, expected welfare, and decomposition effects. Inference in\nmany other semiparametric contexts can be readily obtained. We demonstrate the\neffectiveness of deep learning with a Monte Carlo analysis and an empirical\napplication to direct mail marketing.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:04:23 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 00:03:51 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 14:23:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Farrell", "Max H.", ""], ["Liang", "Tengyuan", ""], ["Misra", "Sanjog", ""]]}, {"id": "1809.09994", "submitter": "Alican B\\\"uy\\\"uk\\c{c}ak{\\i}r", "authors": "Alican B\\\"uy\\\"uk\\c{c}ak{\\i}r, Hamed Bonab, Fazli Can", "title": "A Novel Online Stacked Ensemble for Multi-Label Stream Classification", "comments": "10 pages, 4 figures. To be appeared in ACM CIKM 2018, in Torino,\n  Italy", "journal-ref": null, "doi": "10.1145/3269206.3271774", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data streams become more prevalent, the necessity for online algorithms\nthat mine this transient and dynamic data becomes clearer. Multi-label data\nstream classification is a supervised learning problem where each instance in\nthe data stream is classified into one or more pre-defined sets of labels. Many\nmethods have been proposed to tackle this problem, including but not limited to\nensemble-based methods. Some of these ensemble-based methods are specifically\ndesigned to work with certain multi-label base classifiers; some others employ\nonline bagging schemes to build their ensembles. In this study, we introduce a\nnovel online and dynamically-weighted stacked ensemble for multi-label\nclassification, called GOOWE-ML, that utilizes spatial modeling to assign\noptimal weights to its component classifiers. Our model can be used with any\nexisting incremental multi-label classification algorithm as its base\nclassifier. We conduct experiments with 4 GOOWE-ML-based multi-label ensembles\nand 7 baseline models on 7 real-world datasets from diverse areas of interest.\nOur experiments show that GOOWE-ML ensembles yield consistently better results\nin terms of predictive performance in almost all of the datasets, with respect\nto the other prominent ensemble models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:40:50 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["B\u00fcy\u00fck\u00e7ak\u0131r", "Alican", ""], ["Bonab", "Hamed", ""], ["Can", "Fazli", ""]]}, {"id": "1809.10020", "submitter": "Romana Markovic", "authors": "Romana Markovic, J\\'er\\^ome Frisch, Christoph van Treeck", "title": "Learning short-term past as predictor of human behavior in commercial\n  buildings", "comments": "Preprint submitted to Energy and Buildings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the question of identifying the time-window in\nshort-term past from which the information regarding the future occupant's\nwindow opening actions and resulting window states in buildings can be\npredicted. The addressed sequence duration was in the range between 30 and 240\ntime-steps of indoor climate data, where the applied temporal discretization\nwas one minute. For that purpose, a deep neural network is trained to predict\nthe window states, where the input sequence duration is handled as an\nadditional hyperparameter. Eventually, the relationship between the prediction\naccuracy and the time-lag of the predicted window state in future is analyzed.\nThe results pointed out, that the optimal predictive performance was achieved\nfor the case where 60 time-steps of the indoor climate data were used as input.\nAdditionally, the results showed that very long sequences (120-240 time-steps)\ncould be addressed efficiently, given the right hyperprameters. Hence, the use\nof the memory over previous hours of high-resolution indoor climate data did\nnot improve the predictive performance, when compared to the case where 30/60\nminutes indoor sequences were used. The analysis of the prediction accuracy in\nthe form of F1 score for the different time-lag of future window states dropped\nfrom 0.51 to 0.27, when shifting the prediction target from 10 to 60 minutes in\nfuture.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:31:49 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Markovic", "Romana", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["van Treeck", "Christoph", ""]]}, {"id": "1809.10024", "submitter": "Russell Poldrack", "authors": "Russell A. Poldrack, Krzysztof J. Gorgolewski, and Gael Varoquaux", "title": "Computational and informatics advances for reproducible data analysis in\n  neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducibility of scientific research has become a point of critical\nconcern. We argue that openness and transparency are critical for\nreproducibility, and we outline an ecosystem for open and transparent science\nthat has emerged within the human neuroimaging community. We discuss the range\nof open data sharing resources that have been developed for neuroimaging data,\nand the role of data standards (particularly the Brain Imaging Data Structure)\nin enabling the automated sharing, processing, and reuse of large neuroimaging\ndatasets. We outline how the open-source Python language has provided the basis\nfor a data science platform that enables reproducible data analysis and\nvisualization. We also discuss how new advances in software engineering, such\nas containerization, provide the basis for greater reproducibility in data\nanalysis. The emergence of this new ecosystem provides an example for many\nareas of science that are currently struggling with reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 19:23:28 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Poldrack", "Russell A.", ""], ["Gorgolewski", "Krzysztof J.", ""], ["Varoquaux", "Gael", ""]]}, {"id": "1809.10025", "submitter": "Evan Cater", "authors": "Sam Saarinen, Evan Cater, Michael Littman", "title": "Personalized Education at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tailoring the presentation of information to the needs of individual students\nleads to massive gains in student outcomes~\\cite{bloom19842}. This finding is\nlikely due to the fact that different students learn differently, perhaps as a\nresult of variation in ability, interest or other\nfactors~\\cite{schiefele1992interest}. Adapting presentations to the educational\nneeds of an individual has traditionally been the domain of experts, making it\nexpensive and logistically challenging to do at scale, and also leading to\ninequity in educational outcomes. Increased course sizes and large MOOC\nenrollments provide an unprecedented access to student data. We propose that\nemerging technologies in reinforcement learning (RL), as well as\nsemi-supervised learning, natural language processing, and computer vision are\ncritical to leveraging this data to provide personalized education at scale.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 19:40:25 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Saarinen", "Sam", ""], ["Cater", "Evan", ""], ["Littman", "Michael", ""]]}, {"id": "1809.10073", "submitter": "Amir Emad Marvasti", "authors": "Amir Emad Marvasti, Ehsan Emad Marvasti, George Atia, Hassan Foroosh", "title": "Rediscovering Deep Neural Networks Through Finite-State Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new way of thinking about deep neural networks, in which the\nlinear and non-linear components of the network are naturally derived and\njustified in terms of principles in probability theory. In particular, the\nmodels constructed in our framework assign probabilities to uncertain\nrealizations, leading to Kullback-Leibler Divergence (KLD) as the linear layer.\nIn our model construction, we also arrive at a structure similar to ReLU\nactivation supported with Bayes' theorem. The non-linearities in our framework\nare normalization layers with ReLU and Sigmoid as element-wise approximations.\nAdditionally, the pooling function is derived as a marginalization of spatial\nrandom variables according to the mechanics of the framework. As such, Max\nPooling is an approximation to the aforementioned marginalization process.\nSince our models are comprised of finite state distributions (FSD) as variables\nand parameters, exact computation of information-theoretic quantities such as\nentropy and KLD is possible, thereby providing more objective measures to\nanalyze networks. Unlike existing designs that rely on heuristics, the proposed\nframework restricts subjective interpretations of CNNs and sheds light on the\nfunctionality of neural networks from a completely new perspective.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:46:53 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 19:29:42 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Marvasti", "Amir Emad", ""], ["Marvasti", "Ehsan Emad", ""], ["Atia", "George", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1809.10083", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Premkumar Natarajan", "title": "Unsupervised Adversarial Invariance", "comments": "To appear in Proceedings of NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data representations that contain all the information about target variables\nbut are invariant to nuisance factors benefit supervised learning algorithms by\npreventing them from learning associations between these factors and the\ntargets, thus reducing overfitting. We present a novel unsupervised invariance\ninduction framework for neural networks that learns a split representation of\ndata through competitive training between the prediction task and a\nreconstruction task coupled with disentanglement, without needing any labeled\ninformation about nuisance factors or domain knowledge. We describe an\nadversarial instantiation of this framework and provide analysis of its\nworking. Our unsupervised model outperforms state-of-the-art methods, which are\nsupervised, at inducing invariance to inherent nuisance factors, effectively\nusing synthetic data augmentation to learn invariance, and domain adaptation.\nOur method can be applied to any prediction task, eg., binary/multi-class\nclassification or regression, without loss of generality.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:55:22 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Wu", "Yue", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1809.10120", "submitter": "Yannick Le Cacheux", "authors": "Yannick Le Cacheux, Herv\\'e Le Borgne, Michel Crucianu", "title": "From Classical to Generalized Zero-Shot Learning: a Simple Adaptation\n  Process", "comments": null, "journal-ref": "Proceedings of the 25th International Conference on MultiMedia\n  Modeling (2019), 465-477", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is concerned with the recognition of previously\nunseen classes. It relies on additional semantic knowledge for which a mapping\ncan be learned with training examples of seen classes. While classical ZSL\nconsiders the recognition performance on unseen classes only, generalized\nzero-shot learning (GZSL) aims at maximizing performance on both seen and\nunseen classes. In this paper, we propose a new process for training and\nevaluation in the GZSL setting; this process addresses the gap in performance\nbetween samples from unseen and seen classes by penalizing the latter, and\nenables to select hyper-parameters well-suited to the GZSL task. It can be\napplied to any existing ZSL approach and leads to a significant performance\nboost: the experimental evaluation shows that GZSL performance, averaged over\neight state-of-the-art methods, is improved from 28.5 to 42.2 on CUB and from\n28.2 to 57.1 on AwA2.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 17:04:42 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Cacheux", "Yannick Le", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Crucianu", "Michel", ""]]}, {"id": "1809.10121", "submitter": "Sarah Dean", "authors": "Sarah Dean, Stephen Tu, Nikolai Matni, Benjamin Recht", "title": "Safely Learning to Control the Constrained Linear Quadratic Regulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the constrained linear quadratic regulator with unknown dynamics,\naddressing the tension between safety and exploration in data-driven control\ntechniques. We present a framework which allows for system identification\nthrough persistent excitation, while maintaining safety by guaranteeing the\nsatisfaction of state and input constraints. This framework involves a novel\nmethod for synthesizing robust constraint-satisfying feedback controllers,\nleveraging newly developed tools from system level synthesis. We connect\nstatistical results with cost sub-optimality bounds to give non-asymptotic\nguarantees on both estimation and controller performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 17:04:59 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 12:48:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Dean", "Sarah", ""], ["Tu", "Stephen", ""], ["Matni", "Nikolai", ""], ["Recht", "Benjamin", ""]]}, {"id": "1809.10139", "submitter": "Soudabeh Barghi", "authors": "Soudabeh Barghi, Lalet Scaria, Ali Salari, Tristan Glatard", "title": "Predicting computational reproducibility of data analysis pipelines in\n  large population studies using collaborative filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the computational reproducibility of data analysis pipelines has\nbecome a critical issue. It is, however, a cumbersome process for analyses that\ninvolve data from large populations of subjects, due to their computational and\nstorage requirements. We present a method to predict the computational\nreproducibility of data analysis pipelines in large population studies. We\nformulate the problem as a collaborative filtering process, with constraints on\nthe construction of the training set. We propose 6 different strategies to\nbuild the training set, which we evaluate on 2 datasets, a synthetic one\nmodeling a population with a growing number of subject types, and a real one\nobtained with neuroinformatics pipelines. Results show that one sampling\nmethod, \"Random File Numbers (Uniform)\" is able to predict computational\nreproducibility with a good accuracy. We also analyze the relevance of\nincluding file and subject biases in the collaborative filtering model. We\nconclude that the proposed method is able to speedup reproducibility\nevaluations substantially, with a reduced accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:24:46 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Barghi", "Soudabeh", ""], ["Scaria", "Lalet", ""], ["Salari", "Ali", ""], ["Glatard", "Tristan", ""]]}, {"id": "1809.10168", "submitter": "Viet Hung Tran", "authors": "Viet Hung Tran and Wenwu Wang", "title": "Bayesian inference for PCA and MUSIC algorithms with unknown number of\n  sources", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a popular method for projecting data\nonto uncorrelated components in lower dimension, although the optimal number of\ncomponents is not specified. Likewise, multiple signal classification (MUSIC)\nalgorithm is a popular PCA-based method for estimating directions of arrival\n(DOAs) of sinusoidal sources, yet it requires the number of sources to be known\na priori. The accurate estimation of the number of sources is hence a crucial\nissue for performance of these algorithms. In this paper, we will show that\nboth PCA and MUSIC actually return the exact joint maximum-a-posteriori (MAP)\nestimate for uncorrelated steering vectors, although they can only compute this\nMAP estimate approximately in correlated case. We then use Bayesian method to,\nfor the first time, compute the MAP estimate for the number of sources in PCA\nand MUSIC algorithms. Intuitively, this MAP estimate corresponds to the highest\nprobability that signal-plus-noise's variance still dominates projected noise's\nvariance on signal subspace. In simulations of overlapping multi-tone sources\nfor linear sensor array, our exact MAP estimate is far superior to the\nasymptotic Akaike information criterion (AIC), which is a popular method for\nestimating the number of components in PCA and MUSIC algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 18:08:25 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Tran", "Viet Hung", ""], ["Wang", "Wenwu", ""]]}, {"id": "1809.10170", "submitter": "Jiyuan Zhang", "authors": "Jiyuan Zhang, Franz Franchetti, Tze Meng Low", "title": "High Performance Zero-Memory Overhead Direct Convolutions", "comments": "the 35th International Conference on Machine Learning(ICML 2018),\n  camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of convolution layers in deep neural networks typically rely\non high performance routines that trade space for time by using additional\nmemory (either for packing purposes or required as part of the algorithm) to\nimprove performance. The problems with such an approach are two-fold. First,\nthese routines incur additional memory overhead which reduces the overall size\nof the network that can fit on embedded devices with limited memory capacity.\nSecond, these high performance routines were not optimized for performing\nconvolution, which means that the performance obtained is usually less than\nconventionally expected. In this paper, we demonstrate that direct convolution,\nwhen implemented correctly, eliminates all memory overhead, and yields\nperformance that is between 10% to 400% times better than existing high\nperformance implementations of convolution layers on conventional and embedded\nCPU architectures. We also show that a high performance direct convolution\nexhibits better scaling performance, i.e. suffers less performance drop, when\nincreasing the number of threads.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 00:48:12 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhang", "Jiyuan", ""], ["Franchetti", "Franz", ""], ["Low", "Tze Meng", ""]]}, {"id": "1809.10188", "submitter": "Linfeng Zhang", "authors": "Linfeng Zhang, Weinan E, Lei Wang", "title": "Monge-Amp\\`ere Flow for Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep generative model, named Monge-Amp\\`ere flow, which builds\non continuous-time gradient flow arising from the Monge-Amp\\`ere equation in\noptimal transport theory. The generative map from the latent space to the data\nspace follows a dynamical system, where a learnable potential function guides a\ncompressible fluid to flow towards the target density distribution. Training of\nthe model amounts to solving an optimal control problem. The Monge-Amp\\`ere\nflow has tractable likelihoods and supports efficient sampling and inference.\nOne can easily impose symmetry constraints in the generative model by designing\nsuitable scalar potential functions. We apply the approach to unsupervised\ndensity estimation of the MNIST dataset and variational calculation of the\ntwo-dimensional Ising model at the critical point. This approach brings\ninsights and techniques from Monge-Amp\\`ere equation, optimal transport, and\nfluid dynamics into reversible flow-based generative models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 18:53:51 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhang", "Linfeng", ""], ["E", "Weinan", ""], ["Wang", "Lei", ""]]}, {"id": "1809.10200", "submitter": "Edouard Oyallon", "authors": "Edouard Oyallon and Eugene Belilovsky and Sergey Zagoruyko and Michal\n  Valko", "title": "Compressing the Input for CNNs with the First-Order Scattering Transform", "comments": null, "journal-ref": "ECCV 2018", "doi": "10.1007/978-3-030-01240-3_19", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the first-order scattering transform as a candidate for reducing the\nsignal processed by a convolutional neural network (CNN). We show theoretical\nand empirical evidence that in the case of natural images and sufficiently\nsmall translation invariance, this transform preserves most of the signal\ninformation needed for classification while substantially reducing the spatial\nresolution and total signal size. We demonstrate that cascading a CNN with this\nrepresentation performs on par with ImageNet classification models, commonly\nused in downstream tasks, such as the ResNet-50. We subsequently apply our\ntrained hybrid ImageNet model as a base model on a detection system, which has\ntypically larger image inputs. On Pascal VOC and COCO detection tasks we\ndemonstrate improvements in the inference speed and training memory consumption\ncompared to models trained directly on the input image.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 10:14:46 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Oyallon", "Edouard", ""], ["Belilovsky", "Eugene", ""], ["Zagoruyko", "Sergey", ""], ["Valko", "Michal", ""]]}, {"id": "1809.10203", "submitter": "Defeng Chen", "authors": "Han Kang, Defeng Chen", "title": "Multi-Scale Fully Convolutional Network for Cardiac Left Ventricle\n  Segmentation", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphological structure of left ventricle segmented from cardiac magnetic\nresonance images can be used to calculate key clinical parameters, and it is of\ngreat significance to the accurate and efficient diagnosis of cardiovascular\ndiseases. Compared with traditional methods, the segmentation algorithms based\non fully convolutional neural network greatly improve the accuracy of semantic\nsegmentation. For the problem of left ventricular segmentation, a new fully\nconvolutional neural network structure named MS-FCN is proposed in this paper.\nThe MS-FCN network employs a multi-scale pooling module to ensure that the\nnetwork maximises the feature extraction ability and uses a dense connectivity\ndecoder to refine the boundaries of the object. Based on the Sunnybrook cine-MR\ndataset provided by the MICCAI 2009 challenge, numerical experiments\ndemonstrate that our proposed model has obtained state-of-the-art segmentation\nresults: the Dice score of our method reaches 0.93 on the endocardium, and 0.96\non the epicardium.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:13:05 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Kang", "Han", ""], ["Chen", "Defeng", ""]]}, {"id": "1809.10210", "submitter": "Cun Mu", "authors": "Guang Yang and Cun Mu", "title": "A Machine Learning Approach to Shipping Box Design", "comments": "Accepted by 2019 Intelligent Systems Conference (A shorter version of\n  the paper is presented at the 13th INFORMS Workshop on Data Mining and\n  Decision Analytics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having the right assortment of shipping boxes in the fulfillment warehouse to\npack and ship customer's online orders is an indispensable and integral part of\nnowadays eCommerce business, as it will not only help maintain a profitable\nbusiness but also create great experiences for customers. However, it is an\nextremely challenging operations task to strategically select the best\ncombination of tens of box sizes from thousands of feasible ones to be\nresponsible for hundreds of thousands of orders daily placed on millions of\ninventory products. In this paper, we present a machine learning approach to\ntackle the task by formulating the box design problem prescriptively as a\ngeneralized version of weighted $k$-medoids clustering problem, where the\nparameters are estimated through a variety of descriptive analytics. We test\nthis machine learning approach on fulfillment data collected from Walmart U.S.\neCommerce, and our approach is shown to be capable of improving the box\nutilization rate by more than $10\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 19:48:45 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 08:41:26 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 19:41:14 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Yang", "Guang", ""], ["Mu", "Cun", ""]]}, {"id": "1809.10231", "submitter": "Ren\\'e Corbet", "authors": "Ren\\'e Corbet, Ulderico Fugacci, Michael Kerber, Claudia Landi, Bei\n  Wang", "title": "A Kernel for Multi-Parameter Persistent Homology", "comments": "24 pages, 5 figures", "journal-ref": "Computers & Graphics (2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis and its main method, persistent homology, provide a\ntoolkit for computing topological information of high-dimensional and noisy\ndata sets. Kernels for one-parameter persistent homology have been established\nto connect persistent homology with machine learning techniques. We contribute\na kernel construction for multi-parameter persistence by integrating a\none-parameter kernel weighted along straight lines. We prove that our kernel is\nstable and efficiently computable, which establishes a theoretical connection\nbetween topological data analysis and machine learning for multivariate data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 21:00:58 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 11:28:45 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Corbet", "Ren\u00e9", ""], ["Fugacci", "Ulderico", ""], ["Kerber", "Michael", ""], ["Landi", "Claudia", ""], ["Wang", "Bei", ""]]}, {"id": "1809.10232", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "Preconditioner on Matrix Lie Group for SGD", "comments": "to appear on ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two types of preconditioners and preconditioned stochastic gradient\ndescent (SGD) methods in a unified framework. We call the first one the Newton\ntype due to its close relationship to the Newton method, and the second one the\nFisher type as its preconditioner is closely related to the inverse of Fisher\ninformation matrix. Both preconditioners can be derived from one framework, and\nefficiently estimated on any matrix Lie groups designated by the user using\nnatural or relative gradient descent minimizing certain preconditioner\nestimation criteria. Many existing preconditioners and methods, e.g., RMSProp,\nAdam, KFAC, equilibrated SGD, batch normalization, etc., are special cases of\nor closely related to either the Newton type or the Fisher type ones.\nExperimental results on relatively large scale machine learning problems are\nreported for performance study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 21:04:23 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 00:10:24 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1809.10237", "submitter": "Jiachen Li", "authors": "Jiachen Li, Wei Zhan and Masayoshi Tomizuka", "title": "Generic Vehicle Tracking Framework Capable of Handling Occlusions Based\n  on Modified Mixture Particle Filter", "comments": "Presented in 2018 IEEE Intelligent Vehicles Symposium (IV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust tracking of surrounding road participants plays an\nimportant role in autonomous driving. However, there is usually no prior\nknowledge of the number of tracking targets due to object emergence, object\ndisappearance and false alarms. To overcome this challenge, we propose a\ngeneric vehicle tracking framework based on modified mixture particle filter,\nwhich can make the number of tracking targets adaptive to real-time\nobservations and track all the vehicles within sensor range simultaneously in a\nuniform architecture without explicit data association. Each object corresponds\nto a mixture component whose distribution is non-parametric and approximated by\nparticle hypotheses. Most tracking approaches employ vehicle kinematic models\nas the prediction model. However, it is hard for these models to make proper\npredictions when sensor measurements are lost or become low quality due to\npartial or complete occlusions. Moreover, these models are incapable of\nforecasting sudden maneuvers. To address these problems, we propose to\nincorporate learning-based behavioral models instead of pure vehicle kinematic\nmodels to realize prediction in the prior update of recursive Bayesian state\nestimation. Two typical driving scenarios including lane keeping and lane\nchange are demonstrated to verify the effectiveness and accuracy of the\nproposed framework as well as the advantages of employing learning-based\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 05:27:47 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Li", "Jiachen", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1809.10238", "submitter": "Joseph K J", "authors": "K J Joseph, Arghya Pal, Sailaja Rajanala, Vineeth N Balasubramanian", "title": "C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis", "comments": "To appear in the proceedings of IEEE Winter Conference on\n  Applications of Computer Vision, WACV-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating an image from its description is a challenging task worth solving\nbecause of its numerous practical applications ranging from image editing to\nvirtual reality. All existing methods use one single caption to generate a\nplausible image. A single caption by itself, can be limited, and may not be\nable to capture the variety of concepts and behavior that may be present in the\nimage. We propose two deep generative models that generate an image by making\nuse of multiple captions describing it. This is achieved by ensuring\n'Cross-Caption Cycle Consistency' between the multiple captions and the\ngenerated image(s). We report quantitative and qualitative results on the\nstandard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate\nthe efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:18:57 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Joseph", "K J", ""], ["Pal", "Arghya", ""], ["Rajanala", "Sailaja", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1809.10239", "submitter": "Berta Besc\\'os Torcal", "authors": "Berta Bescos, Jos\\'e Neira, Roland Siegwart, Cesar Cadena", "title": "Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space", "comments": "Accepted for Publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present an end-to-end deep learning framework to turn images\nthat show dynamic content, such as vehicles or pedestrians, into realistic\nstatic frames. This objective encounters two main challenges: detecting all the\ndynamic objects, and inpainting the static occluded background with plausible\nimagery. The second problem is approached with a conditional generative\nadversarial model that, taking as input the original dynamic image and its\ndynamic/static binary mask, is capable of generating the final static image.\nThe former challenge is addressed by the use of a convolutional network that\nlearns a multi-class semantic segmentation of the image.\n  These generated images can be used for applications such as augmented reality\nor vision-based robot localization purposes. To validate our approach, we show\nboth qualitative and quantitative comparisons against other state-of-the-art\ninpainting methods by removing the dynamic objects and hallucinating the static\nstructure behind them. Furthermore, to demonstrate the potential of our\nresults, we carry out pilot experiments that show the benefits of our proposal\nfor visual place recognition.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 08:13:52 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 09:36:18 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Bescos", "Berta", ""], ["Neira", "Jos\u00e9", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1809.10241", "submitter": "Shanshan Wang", "authors": "Jingxu Xu, Cheng Li, Yongjin Zhou, Lisha Mou, Hairong Zheng, and\n  Shanshan Wang", "title": "Classifying Mammographic Breast Density by Residual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 06:29:50 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Xu", "Jingxu", ""], ["Li", "Cheng", ""], ["Zhou", "Yongjin", ""], ["Mou", "Lisha", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "1809.10242", "submitter": "Zhujun Xiao", "authors": "Zhujun Xiao, Yanzi Zhu, Yuxin Chen, Ben Y. Zhao, Junchen Jiang, Haitao\n  Zheng", "title": "Addressing Training Bias via Automated Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Build accurate DNN models requires training on large labeled, context\nspecific datasets, especially those matching the target scenario. We believe\nadvances in wireless localization, working in unison with cameras, can produce\nautomated annotation of targets on images and videos captured in the wild.\nUsing pedestrian and vehicle detection as examples, we demonstrate the\nfeasibility, benefits, and challenges of an automatic image annotation system.\nOur work calls for new technical development on passive localization, mobile\ndata analytics, and error-resilient ML models, as well as design issues in user\nprivacy policies.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 19:47:01 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:09:25 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Xiao", "Zhujun", ""], ["Zhu", "Yanzi", ""], ["Chen", "Yuxin", ""], ["Zhao", "Ben Y.", ""], ["Jiang", "Junchen", ""], ["Zheng", "Haitao", ""]]}, {"id": "1809.10243", "submitter": "Navid Alemi Koohbanani", "authors": "Mostafa Jahanifar, Neda Zamani Tajeddin, Navid Alemi Koohbanani, Ali\n  Gooya, and Nasir Rajpoot", "title": "Segmentation of Skin Lesions and their Attributes Using Multi-Scale\n  Convolutional Neural Networks and Domain Specific Augmentations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis systems for classification of different type of skin\nlesions have been an active field of research in recent decades. It has been\nshown that introducing lesions and their attributes masks into lesion\nclassification pipeline can greatly improve the performance. In this paper, we\npropose a framework by incorporating transfer learning for segmenting lesions\nand their attributes based on the convolutional neural networks. The proposed\nframework is based on the encoder-decoder architecture which utilizes a variety\nof pre-trained networks in the encoding path and generates the prediction map\nby combining multi-scale information in decoding path using a pyramid pooling\nmanner. To address the lack of training data and increase the proposed model\ngeneralization, an extensive set of novel domain-specific augmentation routines\nhave been applied to simulate the real variations in dermoscopy images.\nFinally, by performing broad experiments on three different data sets obtained\nfrom International Skin Imaging Collaboration archive (ISIC2016, ISIC2017, and\nISIC2018 challenges data sets), we show that the proposed method outperforms\nother state-of-the-art approaches for ISIC2016 and ISIC2017 segmentation task\nand achieved the first rank on the leader-board of ISIC2018 attribute detection\ntask.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 18:01:14 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 02:16:00 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 09:12:34 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Jahanifar", "Mostafa", ""], ["Tajeddin", "Neda Zamani", ""], ["Koohbanani", "Navid Alemi", ""], ["Gooya", "Ali", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1809.10253", "submitter": "Ryan Julian", "authors": "Ryan Julian, Eric Heiden, Zhanpeng He, Hejia Zhang, Stefan Schaal,\n  Joseph J. Lim, Gaurav Sukhatme, Karol Hausman", "title": "Scaling simulation-to-real transfer by learning composable robot skills", "comments": "Presented at ISER 2018. See\n  https://www.youtube.com/watch?v=Syr2RQTHqTs for supplemental video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel solution to the problem of simulation-to-real transfer,\nwhich builds on recent advances in robot skill decomposition. Rather than\nfocusing on minimizing the simulation-reality gap, we learn a set of diverse\npolicies that are parameterized in a way that makes them easily reusable. This\ndiversity and parameterization of low-level skills allows us to find a\ntransferable policy that is able to use combinations and variations of\ndifferent skills to solve more complex, high-level tasks. In particular, we\nfirst use simulation to jointly learn a policy for a set of low-level skills,\nand a \"skill embedding\" parameterization which can be used to compose them.\nLater, we learn high-level policies which actuate the low-level policies via\nthis skill embedding parameterization. The high-level policies encode how and\nwhen to reuse the low-level skills together to achieve specific high-level\ntasks. Importantly, our method learns to control a real robot in joint-space to\nachieve these high-level tasks with little or no on-robot time, despite the\nfact that the low-level policies may not be perfectly transferable from\nsimulation to real, and that the low-level skills were not trained on any\nexamples of high-level tasks. We illustrate the principles of our method using\ninformative simulation experiments. We then verify its usefulness for real\nrobotics problems by learning, transferring, and composing free-space and\ncontact motion skills on a Sawyer robot using only joint-space control. We\nexperiment with several techniques for composing pre-learned skills, and find\nthat our method allows us to use both learning-based approaches and efficient\nsearch-based planning to achieve high-level tasks using only pre-learned\nskills.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 22:21:02 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 00:37:06 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 21:42:51 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Julian", "Ryan", ""], ["Heiden", "Eric", ""], ["He", "Zhanpeng", ""], ["Zhang", "Hejia", ""], ["Schaal", "Stefan", ""], ["Lim", "Joseph J.", ""], ["Sukhatme", "Gaurav", ""], ["Hausman", "Karol", ""]]}, {"id": "1809.10271", "submitter": "Chi Zhang", "authors": "Chi Zhang, Thang Nguyen, Shagan Sah, Raymond Ptucha, Alexander Loui,\n  Carl Salvaggio", "title": "Batch-normalized Recurrent Highway Networks", "comments": "5 pages, 3 figures, Published in 2017 IEEE International Conference\n  on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2017.8296359", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient control plays an important role in feed-forward networks applied to\nvarious computer vision tasks. Previous work has shown that Recurrent Highway\nNetworks minimize the problem of vanishing or exploding gradients. They achieve\nthis by setting the eigenvalues of the temporal Jacobian to 1 across the time\nsteps. In this work, batch normalized recurrent highway networks are proposed\nto control the gradient flow in an improved way for network convergence.\nSpecifically, the introduced model can be formed by batch normalizing the\ninputs at each recurrence loop. The proposed model is tested on an image\ncaptioning task using MSCOCO dataset. Experimental results indicate that the\nbatch normalized recurrent highway networks converge faster and performs better\ncompared with the traditional LSTM and RHN based models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 23:56:24 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhang", "Chi", ""], ["Nguyen", "Thang", ""], ["Sah", "Shagan", ""], ["Ptucha", "Raymond", ""], ["Loui", "Alexander", ""], ["Salvaggio", "Carl", ""]]}, {"id": "1809.10274", "submitter": "Chi Zhang", "authors": "Shagan Sah, Dheeraj Peri, Ameya Shringi, Chi Zhang, Miguel Dominguez,\n  Andreas Savakis, Ray Ptucha", "title": "Semantically Invariant Text-to-Image Generation", "comments": "5 papers, 5 figures, Published in 2018 25th IEEE International\n  Conference on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451656", "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning has demonstrated models that are capable of generating\nplausible text given input images or videos. Further, recent work in image\ngeneration has shown significant improvements in image quality when text is\nused as a prior. Our work ties these concepts together by creating an\narchitecture that can enable bidirectional generation of images and text. We\ncall this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we\npropose two improvements to the text conditioned image generation. Firstly, a\nn-gram metric based cost function is introduced that generalizes the caption\nwith respect to the image. Secondly, multiple semantically similar sentences\nare shown to help in generating better images. Qualitative and quantitative\nevaluations demonstrate that MMVR improves upon existing text conditioned image\ngeneration results by over 20%, while integrating visual and text modalities.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 00:11:25 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sah", "Shagan", ""], ["Peri", "Dheeraj", ""], ["Shringi", "Ameya", ""], ["Zhang", "Chi", ""], ["Dominguez", "Miguel", ""], ["Savakis", "Andreas", ""], ["Ptucha", "Ray", ""]]}, {"id": "1809.10284", "submitter": "Kevin Schlegel", "authors": "Kevin Schlegel", "title": "When is there a Representer Theorem? Reflexive Banach spaces", "comments": "25 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1709.00084, arXiv:1804.09605", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general regularised interpolation problem for learning a\nparameter vector from data. The well known representer theorem says that under\ncertain conditions on the regulariser there exists a solution in the linear\nspan of the data points. This is at the core of kernel methods in machine\nlearning as it makes the problem computationally tractable. Most literature\ndeals only with sufficient conditions for representer theorems in Hilbert\nspaces. We prove necessary and sufficient conditions for the existence of\nrepresenter theorems in reflexive Banach spaces and illustrate why in a sense\nreflexivity is the minimal requirement on the function space. We further show\nthat if the learning relies on the linear representer theorem, then the\nsolution is independent of the regulariser and in fact determined by the\nfunction space alone. This in particular shows the value of generalising\nHilbert space learning theory to Banach spaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 12:56:09 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 16:12:31 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Schlegel", "Kevin", ""]]}, {"id": "1809.10288", "submitter": "Kou Tanaka", "authors": "Kou Tanaka, Takuhiro Kaneko, Nobukatsu Hojo, Hirokazu Kameoka", "title": "WaveCycleGAN: Synthetic-to-natural speech waveform conversion using\n  cycle-consistent adversarial networks", "comments": "SLT2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based filter that allows us to directly modify a\nsynthetic speech waveform into a natural speech waveform. Speech-processing\nsystems using a vocoder framework such as statistical parametric speech\nsynthesis and voice conversion are convenient especially for a limited number\nof data because it is possible to represent and process interpretable acoustic\nfeatures over a compact space, such as the fundamental frequency (F0) and\nmel-cepstrum. However, a well-known problem that leads to the quality\ndegradation of generated speech is an over-smoothing effect that eliminates\nsome detailed structure of generated/converted acoustic features. To address\nthis issue, we propose a synthetic-to-natural speech waveform conversion\ntechnique that uses cycle-consistent adversarial networks and which does not\nrequire any explicit assumption about speech waveform in adversarial learning.\nIn contrast to current techniques, since our modification is performed at the\nwaveform level, we expect that the proposed method will also make it possible\nto generate `vocoder-less' sounding speech even if the input speech is\nsynthesized using a vocoder framework. The experimental results demonstrate\nthat our proposed method can 1) alleviate the over-smoothing effect of the\nacoustic features despite the direct modification method used for the waveform\nand 2) greatly improve the naturalness of the generated speech sounds.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 13:03:43 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 18:25:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Tanaka", "Kou", ""], ["Kaneko", "Takuhiro", ""], ["Hojo", "Nobukatsu", ""], ["Kameoka", "Hirokazu", ""]]}, {"id": "1809.10312", "submitter": "Chi Zhang", "authors": "Shagan Sah, Chi Zhang, Thang Nguyen, Dheeraj Kumar Peri, Ameya\n  Shringi, Raymond Ptucha", "title": "Vector Learning for Cross Domain Representations", "comments": "5 pages, 7 figures, published in 2017 IEEE Applied Imagery Pattern\n  Recognition Workshop (AIPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, generative adversarial networks have gained a lot of popularity for\nimage generation tasks. However, such models are associated with complex\nlearning mechanisms and demand very large relevant datasets. This work borrows\nconcepts from image and video captioning models to form an image generative\nframework. The model is trained in a similar fashion as recurrent captioning\nmodel and uses the learned weights for image generation. This is done in an\ninverse direction, where the input is a caption and the output is an image. The\nvector representation of the sentence and frames are extracted from an\nencoder-decoder model which is initially trained on similar sentence and image\npairs. Our model conditions image generation on a natural language caption. We\nleverage a sequence-to-sequence model to generate synthetic captions that have\nthe same meaning for having a robust image generation. One key advantage of our\nmethod is that the traditional image captioning datasets can be used for\nsynthetic sentence paraphrases. Results indicate that images generated through\nmultiple captions are better at capturing the semantic meaning of the family of\ncaptions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 02:08:06 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sah", "Shagan", ""], ["Zhang", "Chi", ""], ["Nguyen", "Thang", ""], ["Peri", "Dheeraj Kumar", ""], ["Shringi", "Ameya", ""], ["Ptucha", "Raymond", ""]]}, {"id": "1809.10315", "submitter": "Jingfeng Zhang", "authors": "Jingfeng Zhang, Laura Wynter", "title": "Smooth Inter-layer Propagation of Stabilized Neural Networks for\n  Classification", "comments": "Revised Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has studied the reasons for the remarkable performance of deep\nneural networks in image classification. We examine batch normalization on the\none hand and the dynamical systems view of residual networks on the other hand.\nOur goal is in understanding the notions of stability and smoothness of the\ninter-layer propagation of ResNets so as to explain when they contribute to\nsignificantly enhanced performance. We postulate that such stability is of\nimportance for the trained ResNet to transfer.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 02:23:00 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 03:25:06 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zhang", "Jingfeng", ""], ["Wynter", "Laura", ""]]}, {"id": "1809.10326", "submitter": "Yunhao Tang", "authors": "Yunhao Tang, Shipra Agrawal", "title": "Boosting Trust Region Policy Optimization by Normalizing Flows Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to improve trust region policy search with normalizing flows\npolicy. We illustrate that when the trust region is constructed by KL\ndivergence constraints, normalizing flows policy generates samples far from the\n'center' of the previous policy iterate, which potentially enables better\nexploration and helps avoid bad local optima. Through extensive comparisons, we\nshow that the normalizing flows policy significantly improves upon baseline\narchitectures especially on high-dimensional tasks with complex dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:19:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 19:22:48 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 14:28:17 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Tang", "Yunhao", ""], ["Agrawal", "Shipra", ""]]}, {"id": "1809.10330", "submitter": "Matias Quiroz", "authors": "Ming Xu, Matias Quiroz, Robert Kohn, Scott A. Sisson", "title": "Variance reduction properties of the reparameterization trick", "comments": "Accepted for publication by AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reparameterization trick is widely used in variational inference as it\nyields more accurate estimates of the gradient of the variational objective\nthan alternative approaches such as the score function method. Although there\nis overwhelming empirical evidence in the literature showing its success, there\nis relatively little research exploring why the reparameterization trick is so\neffective. We explore this under the idealized assumptions that the variational\napproximation is a mean-field Gaussian density and that the log of the joint\ndensity of the model parameters and the data is a quadratic function that\ndepends on the variational mean. From this, we show that the marginal variances\nof the reparameterization gradient estimator are smaller than those of the\nscore function gradient estimator. We apply the result of our idealized\nanalysis to real-world examples.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:33:20 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 04:35:12 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 05:20:23 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Xu", "Ming", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1809.10333", "submitter": "Teresa Brooks", "authors": "Teresa Nicole Brooks", "title": "Using Autoencoders To Learn Interesting Features For Detecting\n  Surveillance Aircraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores using a Long short-term memory (LSTM) based sequence\nautoencoder to learn interesting features for detecting surveillance aircraft\nusing ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic\nDependent Surveillance - Broadcast) data to ground receivers. The ability of\nLSTM networks to model varying length time series data and remember\ndependencies that span across events makes it an ideal candidate for\nimplementing a sequence autoencoder for ADS-B data because of its possible\nvariable length time series, irregular sampling and dependencies that span\nacross events.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:35:00 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Brooks", "Teresa Nicole", ""]]}, {"id": "1809.10336", "submitter": "Tao Ma", "authors": "Tao Ma", "title": "Multi-task Learning for Financial Forecasting", "comments": "The methods and results of this paper have been proved to be wrong.\n  So we want to withdraw it to keep others from following the wrong results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial forecasting is challenging and attractive in machine learning.\nThere are many classic solutions, as well as many deep learning based methods,\nproposed to deal with it yielding encouraging performance. Stock time series\nforecasting is the most representative problem in financial forecasting. Due to\nthe strong connections among stocks, the information valuable for forecasting\nis not only included in individual stocks, but also included in the stocks\nrelated to them. However, most previous works focus on one single stock, which\neasily ignore the valuable information in others. To leverage more information,\nin this paper, we propose a jointly forecasting approach to process multiple\ntime series of related stocks simultaneously, using multi-task learning\nframework. Compared to the previous works, we use multiple networks to forecast\nmultiple related stocks, using the shared and private information of them\nsimultaneously through multi-task learning. Moreover, we propose an attention\nmethod learning an optimized weighted combination of shared and private\ninformation based on the idea of Capital Asset Pricing Model (CAPM) to help\nforecast. Experimental results on various data show improved forecasting\nperformance over baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 04:03:03 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 14:25:41 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 09:19:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ma", "Tao", ""]]}, {"id": "1809.10341", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, William Fedus, William L. Hamilton, Pietro\n  Li\\`o, Yoshua Bengio, R Devon Hjelm", "title": "Deep Graph Infomax", "comments": "To appear at ICLR 2019. 17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Graph Infomax (DGI), a general approach for learning node\nrepresentations within graph-structured data in an unsupervised manner. DGI\nrelies on maximizing mutual information between patch representations and\ncorresponding high-level summaries of graphs---both derived using established\ngraph convolutional network architectures. The learnt patch representations\nsummarize subgraphs centered around nodes of interest, and can thus be reused\nfor downstream node-wise learning tasks. In contrast to most prior approaches\nto unsupervised learning with GCNs, DGI does not rely on random walk\nobjectives, and is readily applicable to both transductive and inductive\nlearning setups. We demonstrate competitive performance on a variety of node\nclassification benchmarks, which at times even exceeds the performance of\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 04:53:24 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 15:44:59 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Fedus", "William", ""], ["Hamilton", "William L.", ""], ["Li\u00f2", "Pietro", ""], ["Bengio", "Yoshua", ""], ["Hjelm", "R Devon", ""]]}, {"id": "1809.10374", "submitter": "Andrew Lampinen", "authors": "Andrew K. Lampinen, Surya Ganguli", "title": "An analytic theory of generalization dynamics and transfer learning in\n  deep linear networks", "comments": "ICLR 2019, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much attention has been devoted recently to the generalization puzzle in deep\nlearning: large, deep networks can generalize well, but existing theories\nbounding generalization error are exceedingly loose, and thus cannot explain\nthis striking performance. Furthermore, a major hope is that knowledge may\ntransfer across tasks, so that multi-task learning can improve generalization\non individual tasks. However we lack analytic theories that can quantitatively\npredict how the degree of knowledge transfer depends on the relationship\nbetween the tasks. We develop an analytic theory of the nonlinear dynamics of\ngeneralization in deep linear networks, both within and across tasks. In\nparticular, our theory provides analytic solutions to the training and testing\nerror of deep networks as a function of training time, number of examples,\nnetwork size and initialization, and the task structure and SNR. Our theory\nreveals that deep networks progressively learn the most important task\nstructure first, so that generalization error at the early stopping time\nprimarily depends on task structure and is independent of network size. This\nsuggests any tight bound on generalization error must take into account task\nstructure, and explains observations about real data being learned faster than\nrandom data. Intriguingly our theory also reveals the existence of a learning\nalgorithm that proveably out-performs neural network training through gradient\ndescent. Finally, for transfer learning, our theory reveals that knowledge\ntransfer depends sensitively, but computably, on the SNRs and input feature\nalignments of pairs of tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 06:47:58 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 22:16:58 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Lampinen", "Andrew K.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1809.10388", "submitter": "Kleanthis Malialis", "authors": "Kleanthis Malialis, Christos G. Panayiotou, Marios M. Polycarpou", "title": "Queue-based Resampling for Online Class Imbalance Learning", "comments": "Keywords: online learning, class imbalance, concept drift,\n  resampling, neural networks, data streams. In: 2018 International Conference\n  on Artificial Neural Networks (ICANN)", "journal-ref": "Proceedings of the International Conference on Artificial Neural\n  Networks (ICANN), 2018", "doi": "10.1007/978-3-030-01418-6_49", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online class imbalance learning constitutes a new problem and an emerging\nresearch topic that focusses on the challenges of online learning under class\nimbalance and concept drift. Class imbalance deals with data streams that have\nvery skewed distributions while concept drift deals with changes in the class\nimbalance status. Little work exists that addresses these challenges and in\nthis paper we introduce queue-based resampling, a novel algorithm that\nsuccessfully addresses the co-existence of class imbalance and concept drift.\nThe central idea of the proposed resampling algorithm is to selectively include\nin the training set a subset of the examples that appeared in the past. Results\non two popular benchmark datasets demonstrate the effectiveness of queue-based\nresampling over state-of-the-art methods in terms of learning speed and\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 07:59:33 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 11:25:24 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Malialis", "Kleanthis", ""], ["Panayiotou", "Christos G.", ""], ["Polycarpou", "Marios M.", ""]]}, {"id": "1809.10460", "submitter": "Yutian Chen", "authors": "Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott\n  Reed, Heiga Zen, Quan Wang, Luis C. Cobo, Andrew Trask, Ben Laurie, Caglar\n  Gulcehre, A\\\"aron van den Oord, Oriol Vinyals, Nando de Freitas", "title": "Sample Efficient Adaptive Text-to-Speech", "comments": "Accepted by ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a meta-learning approach for adaptive text-to-speech (TTS) with\nfew data. During training, we learn a multi-speaker model using a shared\nconditional WaveNet core and independent learned embeddings for each speaker.\nThe aim of training is not to produce a neural network with fixed weights,\nwhich is then deployed as a TTS system. Instead, the aim is to produce a\nnetwork that requires few data at deployment time to rapidly adapt to new\nspeakers. We introduce and benchmark three strategies: (i) learning the speaker\nembedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire\narchitecture with stochastic gradient descent, and (iii) predicting the speaker\nembedding with a trained neural network encoder. The experiments show that\nthese approaches are successful at adapting the multi-speaker neural network to\nnew speakers, obtaining state-of-the-art results in both sample naturalness and\nvoice similarity with merely a few minutes of audio data from new speakers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:31:19 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 15:23:54 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 22:30:22 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Chen", "Yutian", ""], ["Assael", "Yannis", ""], ["Shillingford", "Brendan", ""], ["Budden", "David", ""], ["Reed", "Scott", ""], ["Zen", "Heiga", ""], ["Wang", "Quan", ""], ["Cobo", "Luis C.", ""], ["Trask", "Andrew", ""], ["Laurie", "Ben", ""], ["Gulcehre", "Caglar", ""], ["Oord", "A\u00e4ron van den", ""], ["Vinyals", "Oriol", ""], ["de Freitas", "Nando", ""]]}, {"id": "1809.10463", "submitter": "Joseph Bethge", "authors": "Joseph Bethge, Haojin Yang, Christian Bartz, Christoph Meinel", "title": "Learning to Train a Binary Neural Network", "comments": "Code: https://github.com/Jopyth/BMXNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved astonishing results in different\napplication areas. Various methods which allow us to use these models on mobile\nand embedded devices have been proposed. Especially binary neural networks seem\nto be a promising approach for these devices with low computational power.\nHowever, understanding binary neural networks and training accurate models for\npractical applications remains a challenge. In our work, we focus on increasing\nour understanding of the training process and making it accessible to everyone.\nWe publish our code and models based on BMXNet for everyone to use. Within this\nframework, we systematically evaluated different network architectures and\nhyperparameters to provide useful insights on how to train a binary neural\nnetwork. Further, we present how we improved accuracy by increasing the number\nof connections in the network.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:40:03 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Bethge", "Joseph", ""], ["Yang", "Haojin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1809.10477", "submitter": "Dan Garber", "authors": "Dan Garber, Atara Kaplan", "title": "Fast Stochastic Algorithms for Low-rank and Nonsmooth Matrix Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite convex optimization problems which include both a nonsmooth term\nand a low-rank promoting term have important applications in machine learning\nand signal processing, such as when one wishes to recover an unknown matrix\nthat is simultaneously low-rank and sparse. However, such problems are highly\nchallenging to solve in large-scale: the low-rank promoting term prohibits\nefficient implementations of proximal methods for composite optimization and\neven simple subgradient methods. On the other hand, methods which are tailored\nfor low-rank optimization, such as conditional gradient-type methods, which are\noften applied to a smooth approximation of the nonsmooth objective, are slow\nsince their runtime scales with both the large Lipshitz parameter of the\nsmoothed gradient vector and with $1/\\epsilon$. In this paper we develop\nefficient algorithms for \\textit{stochastic} optimization of a strongly-convex\nobjective which includes both a nonsmooth term and a low-rank promoting term.\nIn particular, to the best of our knowledge, we present the first algorithm\nthat enjoys all following critical properties for large-scale problems: i)\n(nearly) optimal sample complexity, ii) each iteration requires only a single\n\\textit{low-rank} SVD computation, and iii) overall number of thin-SVD\ncomputations scales only with $\\log{1/\\epsilon}$ (as opposed to\n$\\textrm{poly}(1/\\epsilon)$ in previous methods). We also give an algorithm for\nthe closely-related finite-sum setting. At the heart of our results lie a novel\ncombination of a variance-reduction technique and the use of a\n\\textit{weak-proximal oracle} which is key to obtaining all above three\nproperties simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:11:56 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Garber", "Dan", ""], ["Kaplan", "Atara", ""]]}, {"id": "1809.10482", "submitter": "David Gaudrie", "authors": "David Gaudrie and Rodolphe Le Riche and Victor Picheny and Benoit\n  Enaux and Vincent Herbert", "title": "Budgeted Multi-Objective Optimization with a Focus on the Central Part\n  of the Pareto Front -- Extended Version", "comments": "Submission pre-print, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing nonlinear systems involving expensive computer experiments with\nregard to conflicting objectives is a common challenge. When the number of\nexperiments is severely restricted and/or when the number of objectives\nincreases, uncovering the whole set of Pareto optimal solutions is out of\nreach, even for surrogate-based approaches: the proposed solutions are\nsub-optimal or do not cover the front well. As non-compromising optimal\nsolutions have usually little point in applications, this work restricts the\nsearch to solutions that are close to the Pareto front center. The article\nstarts by characterizing this center, which is defined for any type of front.\nNext, a Bayesian multi-objective optimization method for directing the search\ntowards it is proposed. Targeting a subset of the Pareto front allows an\nimproved optimality of the solutions and a better coverage of this zone, which\nis our main concern. A criterion for detecting convergence to the center is\ndescribed. If the criterion is triggered, a widened central part of the Pareto\nfront is targeted such that sufficiently accurate convergence to it is\nforecasted within the remaining budget. Numerical experiments show how the\nresulting algorithm, C-EHI, better locates the central part of the Pareto front\nwhen compared to state-of-the-art Bayesian algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:22:04 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 16:36:47 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 07:23:58 GMT"}, {"version": "v4", "created": "Mon, 15 Jul 2019 16:22:06 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Gaudrie", "David", ""], ["Riche", "Rodolphe Le", ""], ["Picheny", "Victor", ""], ["Enaux", "Benoit", ""], ["Herbert", "Vincent", ""]]}, {"id": "1809.10491", "submitter": "Dan Garber", "authors": "Dan Garber", "title": "On the Regret Minimization of Nonconvex Online Gradient Ascent for\n  Online PCA", "comments": "added logarithmic regret bounds, more related work, fixed some small\n  errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the problem of Online Principal Component Analysis\nin the regret minimization framework. For this problem, all existing regret\nminimization algorithms for the fully-adversarial setting are based on a\npositive semidefinite convex relaxation, and hence require quadratic memory and\nSVD computation (either thin of full) on each iteration, which amounts to at\nleast quadratic runtime per iteration. This is in stark contrast to a\ncorresponding stochastic i.i.d. variant of the problem, which was studied\nextensively lately, and admits very efficient gradient ascent algorithms that\nwork directly on the natural non-convex formulation of the problem, and hence\nrequire only linear memory and linear runtime per iteration. This raises the\nquestion: can non-convex online gradient ascent algorithms be shown to minimize\nregret in online adversarial settings? In this paper we take a step forward\ntowards answering this question. We introduce an\n\\textit{adversarially-perturbed spiked-covariance model} in which, each data\npoint is assumed to follow a fixed stochastic distribution with a non-zero\nspectral gap in the covariance matrix, but is then perturbed with some\nadversarial vector. This model is a natural extension of a well studied\nstandard stochastic setting that allows for non-stationary (adversarial)\npatterns to arise in the data and hence, might serve as a significantly better\napproximation for real-world data-streams. We show that in an interesting\nregime of parameters, when the non-convex online gradient ascent algorithm is\ninitialized with a \"warm-start\" vector, it provably minimizes the regret with\nhigh probability. We further discuss the possibility of computing such a\n\"warm-start\" vector, and also the use of regularization to obtain fast regret\nrates. Our theoretical findings are supported by empirical experiments on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:35:17 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 15:43:08 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Garber", "Dan", ""]]}, {"id": "1809.10504", "submitter": "Alexander Ecker", "authors": "Alexander S. Ecker, Fabian H. Sinz, Emmanouil Froudarakis, Paul G.\n  Fahey, Santiago A. Cadena, Edgar Y. Walker, Erick Cobos, Jacob Reimer,\n  Andreas S. Tolias, Matthias Bethge", "title": "A rotation-equivariant convolutional neural network model of primary\n  visual cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical models describe primary visual cortex (V1) as a filter bank of\norientation-selective linear-nonlinear (LN) or energy models, but these models\nfail to predict neural responses to natural stimuli accurately. Recent work\nshows that models based on convolutional neural networks (CNNs) lead to much\nmore accurate predictions, but it remains unclear which features are extracted\nby V1 neurons beyond orientation selectivity and phase invariance. Here we work\ntowards systematically studying V1 computations by categorizing neurons into\ngroups that perform similar computations. We present a framework to identify\ncommon features independent of individual neurons' orientation selectivity by\nusing a rotation-equivariant convolutional neural network, which automatically\nextracts every feature at multiple different orientations. We fit this model to\nresponses of a population of 6000 neurons to natural images recorded in mouse\nprimary visual cortex using two-photon imaging. We show that our\nrotation-equivariant network not only outperforms a regular CNN with the same\nnumber of feature maps, but also reveals a number of common features shared by\nmany V1 neurons, which deviate from the typical textbook idea of V1 as a bank\nof Gabor filters. Our findings are a first step towards a powerful new tool to\nstudy the nonlinear computations in V1.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:16:37 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Ecker", "Alexander S.", ""], ["Sinz", "Fabian H.", ""], ["Froudarakis", "Emmanouil", ""], ["Fahey", "Paul G.", ""], ["Cadena", "Santiago A.", ""], ["Walker", "Edgar Y.", ""], ["Cobos", "Erick", ""], ["Reimer", "Jacob", ""], ["Tolias", "Andreas S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1809.10505", "submitter": "Nikola Konstantinov", "authors": "Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat,\n  Nikola Konstantinov, C\\'edric Renggli", "title": "The Convergence of Sparsified Gradient Methods", "comments": "NIPS 2018 - Advances in Neural Information Processing Systems;\n  Authors in alphabetic order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of massive machine learning models, in particular deep\nneural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace.\nSeveral families of communication-reduction methods, such as quantization,\nlarge-batch methods, and gradient sparsification, have been proposed. To date,\ngradient sparsification methods - where each node sorts gradients by magnitude,\nand only communicates a subset of the components, accumulating the rest locally\n- are known to yield some of the largest practical gains. Such methods can\nreduce the amount of communication per step by up to three orders of magnitude,\nwhile preserving model accuracy. Yet, this family of methods currently has no\ntheoretical justification.\n  This is the question we address in this paper. We prove that, under analytic\nassumptions, sparsifying gradients by magnitude with local error correction\nprovides convergence guarantees, for both convex and non-convex smooth\nobjectives, for data-parallel SGD. The main insight is that sparsification\nmethods implicitly maintain bounds on the maximum impact of stale updates,\nthanks to selection by magnitude. Our analysis and empirical validation also\nreveal that these methods do require analytical conditions to converge well,\njustifying existing heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:23:35 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""], ["Johansson", "Mikael", ""], ["Khirirat", "Sarit", ""], ["Konstantinov", "Nikola", ""], ["Renggli", "C\u00e9dric", ""]]}, {"id": "1809.10535", "submitter": "Deepjyoti Deka", "authors": "Saurav Talukdar, Deepjyoti Deka, Harish Doddi, Donatello Materassi,\n  Misha Chertkov, Murti V. Salapaka", "title": "Physics Informed Topology Learning in Networks of Linear Dynamical\n  Systems", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning influence pathways of a network of dynamically related processes\nfrom observations is of considerable importance in many disciplines. In this\narticle, influence networks of agents which interact dynamically via linear\ndependencies are considered. An algorithm for the reconstruction of the\ntopology of interaction based on multivariate Wiener filtering is analyzed. It\nis shown that for a vast and important class of interactions, that respect flow\nconservation, the topology of the interactions can be exactly recovered. The\nclass of problems where reconstruction is guaranteed to be exact includes power\ndistribution networks, dynamic thermal networks and consensus networks. The\nefficacy of the approach is illustrated through simulation and experiments on\nconsensus networks, IEEE power distribution networks and thermal dynamics of\nbuildings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:19:17 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Talukdar", "Saurav", ""], ["Deka", "Deepjyoti", ""], ["Doddi", "Harish", ""], ["Materassi", "Donatello", ""], ["Chertkov", "Misha", ""], ["Salapaka", "Murti V.", ""]]}, {"id": "1809.10565", "submitter": "Yu Zhao", "authors": "Yu Zhao, Zhenhui Shi, Jingyang Zhang, Dong Chen, Lixu Gu", "title": "A novel active learning framework for classification: using weighted\n  rank aggregation to achieve multiple query criteria", "comments": "34 pages, 21 figures, 11 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple query criteria active learning (MQCAL) methods have a higher\npotential performance than conventional active learning methods in which only\none criterion is deployed for sample selection. A central issue related to\nMQCAL methods concerns the development of an integration criteria strategy\n(ICS) that makes full use of all criteria. The conventional ICS adopted in\nrelevant research all facilitate the desired effects, but several limitations\nstill must be addressed. For instance, some of the strategies are not\nsufficiently scalable during the design process, and the number and type of\ncriteria involved are dictated. Thus, it is challenging for the user to\nintegrate other criteria into the original process unless modifications are\nmade to the algorithm. Other strategies are too dependent on empirical\nparameters, which can only be acquired by experience or cross-validation and\nthus lack generality; additionally, these strategies are counter to the\nintention of active learning, as samples need to be labeled in the validation\nset before the active learning process can begin. To address these limitations,\nwe propose a novel MQCAL method for classification tasks that employs a third\nstrategy via weighted rank aggregation. The proposed method serves as a\nheuristic means to select high-value samples of high scalability and generality\nand is implemented through a three-step process: (1) the transformation of the\nsample selection to sample ranking and scoring, (2) the computation of the\nself-adaptive weights of each criterion, and (3) the weighted aggregation of\neach sample rank list. Ultimately, the sample at the top of the aggregated\nranking list is the most comprehensively valuable and must be labeled. Several\nexperiments generating 257 wins, 194 ties and 49 losses against other\nstate-of-the-art MQCALs are conducted to verify that the proposed method can\nachieve superior results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:12:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhao", "Yu", ""], ["Shi", "Zhenhui", ""], ["Zhang", "Jingyang", ""], ["Chen", "Dong", ""], ["Gu", "Lixu", ""]]}, {"id": "1809.10606", "submitter": "Dian Wu", "authors": "Dian Wu, Lei Wang, Pan Zhang", "title": "Solving Statistical Mechanics Using Variational Autoregressive Networks", "comments": null, "journal-ref": "Phys. Rev. Lett. 122, 080602 (2019), Github:\n  https://github.com/wdphy16/stat-mech-van", "doi": "10.1103/PhysRevLett.122.080602", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for solving statistical mechanics of systems\nwith finite size. The approach extends the celebrated variational mean-field\napproaches using autoregressive neural networks, which support direct sampling\nand exact calculation of normalized probability of configurations. It computes\nvariational free energy, estimates physical quantities such as entropy,\nmagnetizations and correlations, and generates uncorrelated samples all at\nonce. Training of the network employs the policy gradient approach in\nreinforcement learning, which unbiasedly estimates the gradient of variational\nparameters. We apply our approach to several classic systems, including 2D\nIsing models, the Hopfield model, the Sherrington-Kirkpatrick model, and the\ninverse Ising model, for demonstrating its advantages over existing variational\nmean-field methods. Our approach sheds light on solving statistical physics\nproblems using modern deep generative neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:18:23 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 14:52:39 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Wu", "Dian", ""], ["Wang", "Lei", ""], ["Zhang", "Pan", ""]]}, {"id": "1809.10610", "submitter": "Sahaj Garg", "authors": "Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H. Chi,\n  Alex Beutel", "title": "Counterfactual Fairness in Text Classification through Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study counterfactual fairness in text classification, which\nasks the question: How would the prediction change if the sensitive attribute\nreferenced in the example were different? Toxicity classifiers demonstrate a\ncounterfactual fairness issue by predicting that \"Some people are gay\" is toxic\nwhile \"Some people are straight\" is nontoxic. We offer a metric, counterfactual\ntoken fairness (CTF), for measuring this particular form of fairness in text\nclassifiers, and describe its relationship with group fairness. Further, we\noffer three approaches, blindness, counterfactual augmentation, and\ncounterfactual logit pairing (CLP), for optimizing counterfactual token\nfairness during training, bridging the robustness and fairness literature.\nEmpirically, we find that blindness and CLP address counterfactual token\nfairness. The methods do not harm classifier performance, and have varying\ntradeoffs with group fairness. These approaches, both for measurement and\noptimization, provide a new path forward for addressing fairness concerns in\ntext classification.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:21:39 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 19:09:40 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Garg", "Sahaj", ""], ["Perot", "Vincent", ""], ["Limtiaco", "Nicole", ""], ["Taly", "Ankur", ""], ["Chi", "Ed H.", ""], ["Beutel", "Alex", ""]]}, {"id": "1809.10611", "submitter": "David Fridovich-Keil", "authors": "Esther Rolf, David Fridovich-Keil, Max Simchowitz, Benjamin Recht,\n  Claire Tomlin", "title": "A Successive-Elimination Approach to Adaptive Robotic Sensing", "comments": null, "journal-ref": "IEEE Transactions on Robotics Research, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an adaptive source seeking problem, in which a mobile robot must\nidentify the strongest emitter(s) of a signal in an environment with background\nemissions. Background signals may be highly heterogeneous and can mislead\nalgorithms that are based on receding horizon control. We propose AdaSearch, a\ngeneral algorithm for adaptive source seeking in the face of heterogeneous\nbackground noise. AdaSearch combines global trajectory planning with principled\nconfidence intervals in order to concentrate measurements in promising regions\nwhile guaranteeing sufficient coverage of the entire area. Theoretical analysis\nshows that AdaSearch confers gains over a uniform sampling strategy when the\ndistribution of background signals is highly variable. Simulation experiments\ndemonstrate that when applied to the problem of radioactive source seeking,\nAdaSearch outperforms both uniform sampling and a receding time horizon\ninformation-maximization approach based on the current literature. We also\ndemonstrate AdaSearch in hardware, providing further evidence of its potential\nfor real-time implementation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:23:15 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 04:51:10 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 22:48:29 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Rolf", "Esther", ""], ["Fridovich-Keil", "David", ""], ["Simchowitz", "Max", ""], ["Recht", "Benjamin", ""], ["Tomlin", "Claire", ""]]}, {"id": "1809.10635", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Andreas S. Tolias", "title": "Generative replay with feedback connections as a general strategy for\n  continual learning", "comments": "17 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle to developing artificial intelligence applications capable\nof true lifelong learning is that artificial neural networks quickly or\ncatastrophically forget previously learned tasks when trained on a new one.\nNumerous methods for alleviating catastrophic forgetting are currently being\nproposed, but differences in evaluation protocols make it difficult to directly\ncompare their performance. To enable more meaningful comparisons, here we\nidentified three distinct scenarios for continual learning based on whether\ntask identity is known and, if it is not, whether it needs to be inferred.\nPerforming the split and permuted MNIST task protocols according to each of\nthese scenarios, we found that regularization-based approaches (e.g., elastic\nweight consolidation) failed when task identity needed to be inferred. In\ncontrast, generative replay combined with distillation (i.e., using class\nprobabilities as \"soft targets\") achieved superior performance in all three\nscenarios. Addressing the issue of efficiency, we reduced the computational\ncost of generative replay by integrating the generative model into the main\nmodel by equipping it with generative feedback or backward connections. This\nReplay-through-Feedback approach substantially shortened training time with no\nor negligible loss in performance. We believe this to be an important first\nstep towards making the powerful technique of generative replay scalable to\nreal-world continual learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:55:58 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:20:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1809.10652", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Preetam Nandy and Hongzhe Li", "title": "Inference for Individual Mediation Effects and Interventional Effects in\n  Sparse High-Dimensional Causal Graphical Models", "comments": "Revised version; 50 pages, 6 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying intermediate variables (or mediators)\nthat regulate the effect of a treatment on a response variable. While there has\nbeen significant research on this classical topic, little work has been done\nwhen the set of potential mediators is high-dimensional (HD). A further\ncomplication arises when these mediators are interrelated (with unknown\ndependencies). In particular, we assume that the causal structure of the\ntreatment, the confounders, the potential mediators and the response is a\n(possibly unknown) directed acyclic graph (DAG). HD DAG models have previously\nbeen used for the estimation of causal effects from observational data. In\nparticular, methods called IDA and joint-IDA have been developed for estimating\nthe effects of single and multiple simultaneous interventions, respectively. In\nthis paper, we propose an IDA-type method called MIDA for estimating so-called\nindividual mediation effects from HD observational data. Although IDA and\njoint-IDA estimators have been shown to be consistent in certain sparse HD\nsettings, their asymptotic properties such as convergence in distribution and\ninferential tools in such settings have remained unknown. In this paper, we\nprove HD consistency of MIDA for linear structural equation models with\nsub-Gaussian errors. More importantly, we derive distributional convergence\nresults for MIDA in similar HD settings, which are applicable to IDA and\njoint-IDA estimators as well. To our knowledge, these are the first such\ndistributional convergence results facilitating inference for IDA-type\nestimators. These are built on our novel theoretical results regarding uniform\nbounds for linear regression estimators over varying subsets of HD covariates\nwhich may be of independent interest. Finally, we empirically validate our\nasymptotic theory for MIDA and demonstrate its usefulness via simulations and a\nreal data application.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 17:28:07 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:21:30 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 07:27:21 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Nandy", "Preetam", ""], ["Li", "Hongzhe", ""]]}, {"id": "1809.10658", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Jannis Bulian, Massimiliano Ciaramita", "title": "Learning to Coordinate Multiple Reinforcement Learning Agents for\n  Diverse Query Reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to efficiently learn diverse strategies in reinforcement\nlearning for query reformulation in the tasks of document retrieval and\nquestion answering. In the proposed framework an agent consists of multiple\nspecialized sub-agents and a meta-agent that learns to aggregate the answers\nfrom sub-agents to produce a final answer. Sub-agents are trained on disjoint\npartitions of the training data, while the meta-agent is trained on the full\ntraining set. Our method makes learning faster, because it is highly\nparallelizable, and has better generalization performance than strong\nbaselines, such as an ensemble of agents trained on the full data. We show that\nthe improved performance is due to the increased diversity of reformulation\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 17:35:57 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 19:14:55 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Bulian", "Jannis", ""], ["Ciaramita", "Massimiliano", ""]]}, {"id": "1809.10678", "submitter": "Linara Adilova", "authors": "Linara Adilova, Nathalie Paul, and Peter Schlicht", "title": "Introducing Noise in Decentralized Training of Neural Networks", "comments": "13 pages", "journal-ref": "ECML PKDD 2018, Workshop DMLE", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that injecting noise into the neural network weights during\nthe training process leads to a better generalization of the resulting model.\nNoise injection in the distributed setup is a straightforward technique and it\nrepresents a promising approach to improve the locally trained models. We\ninvestigate the effects of noise injection into the neural networks during a\ndecentralized training process. We show both theoretically and empirically that\nnoise injection has no positive effect in expectation on linear models, though.\nHowever for non-linear neural networks we empirically show that noise injection\nsubstantially improves model quality helping to reach a generalization ability\nof a local model close to the serial baseline.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 09:45:38 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Adilova", "Linara", ""], ["Paul", "Nathalie", ""], ["Schlicht", "Peter", ""]]}, {"id": "1809.10679", "submitter": "Nasrin Sadeghianpourhamami", "authors": "Nasrin Sadeghianpourhamami, Johannes Deleu, Chris Develder", "title": "Definition and evaluation of model-free coordination of electrical\n  vehicle charging with reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initial DR studies mainly adopt model predictive control and thus require\naccurate models of the control problem (e.g., a customer behavior model), which\nare to a large extent uncertain for the EV scenario. Hence, model-free\napproaches, especially based on reinforcement learning (RL) are an attractive\nalternative. In this paper, we propose a new Markov decision process (MDP)\nformulation in the RL framework, to jointly coordinate a set of EV charging\nstations. State-of-the-art algorithms either focus on a single EV, or perform\nthe control of an aggregate of EVs in multiple steps (e.g., aggregate load\ndecisions in one step, then a step translating the aggregate decision to\nindividual connected EVs). On the contrary, we propose an RL approach to\njointly control the whole set of EVs at once. We contribute a new MDP\nformulation, with a scalable state representation that is independent of the\nnumber of EV charging stations. Further, we use a batch reinforcement learning\nalgorithm, i.e., an instance of fitted Q-iteration, to learn the optimal\ncharging policy. We analyze its performance using simulation experiments based\non a real-world EV charging data. More specifically, we (i) explore the various\nsettings in training the RL policy (e.g., duration of the period with training\ndata), (ii) compare its performance to an oracle all-knowing benchmark (which\nprovides an upper bound for performance, relying on information that is not\navailable or at least imperfect in practice), (iii) analyze performance over\ntime, over the course of a full year to evaluate possible performance\nfluctuations (e.g, across different seasons), and (iv) demonstrate the\ngeneralization capacity of a learned control policy to larger sets of charging\nstations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 10:34:41 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 19:00:36 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sadeghianpourhamami", "Nasrin", ""], ["Deleu", "Johannes", ""], ["Develder", "Chris", ""]]}, {"id": "1809.10680", "submitter": "Guoqing Chao", "authors": "Guoqing Chao, Chengsheng Mao, Fei Wang, Yuan Zhao, Yuan Luo", "title": "Supervised Nonnegative Matrix Factorization to Predict ICU Mortality\n  Risk", "comments": "7 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ICU mortality risk prediction is a tough yet important task. On one hand, due\nto the complex temporal data collected, it is difficult to identify the\neffective features and interpret them easily; on the other hand, good\nprediction can help clinicians take timely actions to prevent the mortality.\nThese correspond to the interpretability and accuracy problems. Most existing\nmethods lack of the interpretability, but recently Subgraph Augmented\nNonnegative Matrix Factorization (SANMF) has been successfully applied to time\nseries data to provide a path to interpret the features well. Therefore, we\nadopted this approach as the backbone to analyze the patient data. One\nlimitation of the raw SANMF method is its poor prediction ability due to its\nunsupervised nature. To deal with this problem, we proposed a supervised SANMF\nalgorithm by integrating the logistic regression loss function into the NMF\nframework and solved it with an alternating optimization procedure. We used the\nsimulation data to verify the effectiveness of this method, and then we applied\nit to ICU mortality risk prediction and demonstrated its superiority over other\nconventional supervised NMF methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:15:18 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 02:47:21 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Chao", "Guoqing", ""], ["Mao", "Chengsheng", ""], ["Wang", "Fei", ""], ["Zhao", "Yuan", ""], ["Luo", "Yuan", ""]]}, {"id": "1809.10717", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Mahendranath Reddy, Markku Mustonen, Kamran Paynabar,\n  and Karim Pourak", "title": "Dataset: Rare Event Classification in Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A real-world dataset is provided from a pulp-and-paper manufacturing\nindustry. The dataset comes from a multivariate time series process. The data\ncontains a rare event of paper break that commonly occurs in the industry. The\ndata contains sensor readings at regular time-intervals (x's) and the event\nlabel (y). The primary purpose of the data is thought to be building a\nclassification model for early prediction of the rare event. However, it can\nalso be used for multivariate time series data exploration and building other\nsupervised and unsupervised models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 18:38:45 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 16:46:42 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 20:26:23 GMT"}, {"version": "v4", "created": "Fri, 31 May 2019 21:11:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ranjan", "Chitta", ""], ["Reddy", "Mahendranath", ""], ["Mustonen", "Markku", ""], ["Paynabar", "Kamran", ""], ["Pourak", "Karim", ""]]}, {"id": "1809.10732", "submitter": "Nemanja Djuric", "authors": "Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin,\n  Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, Nemanja Djuric", "title": "Multimodal Trajectory Predictions for Autonomous Driving using Deep\n  Convolutional Networks", "comments": "Accepted for publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving presents one of the largest problems that the robotics and\nartificial intelligence communities are facing at the moment, both in terms of\ndifficulty and potential societal impact. Self-driving vehicles (SDVs) are\nexpected to prevent road accidents and save millions of lives while improving\nthe livelihood and life quality of many more. However, despite large interest\nand a number of industry players working in the autonomous domain, there still\nremains more to be done in order to develop a system capable of operating at a\nlevel comparable to best human drivers. One reason for this is high uncertainty\nof traffic behavior and large number of situations that an SDV may encounter on\nthe roads, making it very difficult to create a fully generalizable system. To\nensure safe and efficient operations, an autonomous vehicle is required to\naccount for this uncertainty and to anticipate a multitude of possible\nbehaviors of traffic actors in its surrounding. We address this critical\nproblem and present a method to predict multiple possible trajectories of\nactors while also estimating their probabilities. The method encodes each\nactor's surrounding context into a raster image, used as input by deep\nconvolutional networks to automatically derive relevant features for the task.\nFollowing extensive offline evaluation and comparison to state-of-the-art\nbaselines, the method was successfully tested on SDVs in closed-course tests.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 04:07:13 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:07:02 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Cui", "Henggang", ""], ["Radosavljevic", "Vladan", ""], ["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Nguyen", "Thi", ""], ["Huang", "Tzu-Kuo", ""], ["Schneider", "Jeff", ""], ["Djuric", "Nemanja", ""]]}, {"id": "1809.10749", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Mahesh Chandra Mukkamala, Matthias Hein", "title": "On the loss landscape of a class of deep neural networks with no bad\n  local valleys", "comments": "Accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 20:09:59 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 00:58:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nguyen", "Quynh", ""], ["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1809.10756", "submitter": "Jan-Willem Van De Meent", "authors": "Jan-Willem van de Meent and Brooks Paige and Hongseok Yang and Frank\n  Wood", "title": "An Introduction to Probabilistic Programming", "comments": "Under review at Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is designed to be a first-year graduate-level introduction to\nprobabilistic programming. It not only provides a thorough background for\nanyone wishing to use a probabilistic programming system, but also introduces\nthe techniques needed to design and build these systems. It is aimed at people\nwho have an undergraduate-level understanding of either or, ideally, both\nprobabilistic machine learning and programming languages.\n  We start with a discussion of model-based reasoning and explain why\nconditioning as a foundational computation is central to the fields of\nprobabilistic machine learning and artificial intelligence. We then introduce a\nsimple first-order probabilistic programming language (PPL) whose programs\ndefine static-computation-graph, finite-variable-cardinality models. In the\ncontext of this restricted PPL we introduce fundamental inference algorithms\nand describe how they can be implemented in the context of models denoted by\nprobabilistic programs.\n  In the second part of this document, we introduce a higher-order\nprobabilistic programming language, with a functionality analogous to that of\nestablished programming languages. This affords the opportunity to define\nmodels with dynamic computation graphs, at the cost of requiring inference\nmethods that generate samples by repeatedly executing the program. Foundational\ninference algorithms for this kind of probabilistic programming language are\nexplained in the context of an interface between program executions and an\ninference controller.\n  This document closes with a chapter on advanced topics which we believe to\nbe, at the time of writing, interesting directions for probabilistic\nprogramming research; directions that point towards a tight integration with\ndeep neural network research and the development of systems for next-generation\nartificial intelligence applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 20:44:23 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Paige", "Brooks", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1809.10780", "submitter": "Daniel C. Castro", "authors": "Daniel C. Castro, Jeremy Tan, Bernhard Kainz, Ender Konukoglu, Ben\n  Glocker", "title": "Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation\n  Learning", "comments": "The published version of this article can be found at\n  http://jmlr.org/papers/v20/19-033.html", "journal-ref": "Journal of Machine Learning Research 20 (2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revealing latent structure in data is an active field of research, having\nintroduced exciting technologies such as variational autoencoders and\nadversarial networks, and is essential to push machine learning towards\nunsupervised knowledge discovery. However, a major challenge is the lack of\nsuitable benchmarks for an objective and quantitative evaluation of learned\nrepresentations. To address this issue we introduce Morpho-MNIST, a framework\nthat aims to answer: \"to what extent has my model learned to represent specific\nfactors of variation in the data?\" We extend the popular MNIST dataset by\nadding a morphometric analysis enabling quantitative comparison of trained\nmodels, identification of the roles of latent variables, and characterisation\nof sample diversity. We further propose a set of quantifiable perturbations to\nassess the performance of unsupervised and supervised methods on challenging\ntasks such as outlier detection and domain adaptation. Data and code are\navailable at https://github.com/dccastro/Morpho-MNIST.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:05:03 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:46:33 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 16:40:21 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Castro", "Daniel C.", ""], ["Tan", "Jeremy", ""], ["Kainz", "Bernhard", ""], ["Konukoglu", "Ender", ""], ["Glocker", "Ben", ""]]}, {"id": "1809.10784", "submitter": "Timur Takhtaganov", "authors": "Timur Takhtaganov and Juliane M\\\"uller", "title": "Adaptive Gaussian process surrogates for Bayesian inference", "comments": "38 pages, submitted to the SIAM/ASA Journal on Uncertainty\n  Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive approach to the construction of Gaussian process\nsurrogates for Bayesian inference with expensive-to-evaluate forward models.\nOur method relies on the fully Bayesian approach to training Gaussian process\nmodels and utilizes the expected improvement idea from Bayesian global\noptimization. We adaptively construct training designs by maximizing the\nexpected improvement in fit of the Gaussian process model to the noisy\nobservational data. Numerical experiments on model problems with synthetic data\ndemonstrate the effectiveness of the obtained adaptive designs compared to the\nfixed non-adaptive designs in terms of accurate posterior estimation at a\nfraction of the cost of inference with forward models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:24:05 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Takhtaganov", "Timur", ""], ["M\u00fcller", "Juliane", ""]]}, {"id": "1809.10789", "submitter": "Mark Collier", "authors": "Mark Collier and Joeran Beel", "title": "An Empirical Comparison of Syllabuses for Curriculum Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syllabuses for curriculum learning have been developed on an ad-hoc, per task\nbasis and little is known about the relative performance of different\nsyllabuses. We identify a number of syllabuses used in the literature. We\ncompare the identified syllabuses based on their effect on the speed of\nlearning and generalization ability of a LSTM network on three sequential\nlearning tasks. We find that the choice of syllabus has limited effect on the\ngeneralization ability of a trained network. In terms of speed of learning our\nresults demonstrate that the best syllabus is task dependent but that a\nrecently proposed automated curriculum learning approach - Predictive Gain,\nperforms very competitively against all identified hand-crafted syllabuses. The\nbest performing hand-crafted syllabus which we term Look Back and Forward\ncombines a syllabus which steps through tasks in the order of their difficulty\nwith a uniform distribution over all tasks. Our experimental results provide an\nempirical basis for the choice of syllabus on a new problem that could benefit\nfrom curriculum learning. Additionally, insights derived from our results shed\nlight on how to successfully design new syllabuses.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:44:33 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 18:48:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Collier", "Mark", ""], ["Beel", "Joeran", ""]]}, {"id": "1809.10791", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Phyllis Kanki, Ilya Shpitser", "title": "Estimation of Personalized Effects Associated With Causal Pathways", "comments": null, "journal-ref": "In Proceedings of the Thirty Fourth Conference on Uncertainty in\n  Artificial Intelligence (UAI), 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of personalized decision making is to map a unit's characteristics\nto an action tailored to maximize the expected outcome for that unit. Obtaining\nhigh-quality mappings of this type is the goal of the dynamic regime\nliterature. In healthcare settings, optimizing policies with respect to a\nparticular causal pathway may be of interest as well. For example, we may wish\nto maximize the chemical effect of a drug given data from an observational\nstudy where the chemical effect of the drug on the outcome is entangled with\nthe indirect effect mediated by differential adherence. In such cases, we may\nwish to optimize the direct effect of a drug, while keeping the indirect effect\nto that of some reference treatment. [16] shows how to combine mediation\nanalysis and dynamic treatment regime ideas to defines policies associated with\ncausal pathways and counterfactual responses to these policies. In this paper,\nwe derive a variety of methods for learning high quality policies of this type\nfrom data, in a causal model corresponding to a longitudinal setting of\npractical importance. We illustrate our methods via a dataset of HIV patients\nundergoing therapy, gathered in the Nigerian PEPFAR program.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:49:29 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Nabi", "Razieh", ""], ["Kanki", "Phyllis", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1809.10794", "submitter": "Manuele Leonelli", "authors": "Christiane Goergen, Manuele Leonelli", "title": "Model-Preserving Sensitivity Analysis for Families of Gaussian\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of probability distributions inferred using machine-learning\nalgorithms heavily depends on data availability and quality. In practical\napplications it is therefore fundamental to investigate the robustness of a\nstatistical model to misspecification of some of its underlying probabilities.\nIn the context of graphical models, investigations of robustness fall under the\nnotion of sensitivity analyses. These analyses consist in varying some of the\nmodel's probabilities or parameters and then assessing how far apart the\noriginal and the varied distributions are. However, for Gaussian graphical\nmodels, such variations usually make the original graph an incoherent\nrepresentation of the model's conditional independence structure. Here we\ndevelop an approach to sensitivity analysis which guarantees the original graph\nremains valid after any probability variation and we quantify the effect of\nsuch variations using different measures. To achieve this we take advantage of\nalgebraic techniques to both concisely represent conditional independence and\nto provide a straightforward way of checking the validity of such\nrelationships. Our methods are demonstrated to be robust and comparable to\nstandard ones, which break the conditional independence structure of the model,\nusing an artificial example and a medical real-world application.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 23:12:15 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Goergen", "Christiane", ""], ["Leonelli", "Manuele", ""]]}, {"id": "1809.10804", "submitter": "Ivan Girardi", "authors": "Ivan Girardi, Pengfei Ji, An-phi Nguyen, Nora Hollenstein, Adam\n  Ivankay, Lorenz Kuhn, Chiara Marchiori and Ce Zhang", "title": "Patient Risk Assessment and Warning Symptom Detection Using Deep\n  Attention-Based Neural Networks", "comments": "10 pages, 2 figures, EMNLP workshop LOUHI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an operational component of a real-world patient triage system.\nGiven a specific patient presentation, the system is able to assess the level\nof medical urgency and issue the most appropriate recommendation in terms of\nbest point of care and time to treat. We use an attention-based convolutional\nneural network architecture trained on 600,000 doctor notes in German. We\ncompare two approaches, one that uses the full text of the medical notes and\none that uses only a selected list of medical entities extracted from the text.\nThese approaches achieve 79% and 66% precision, respectively, but on a\nconfidence threshold of 0.6, precision increases to 85% and 75%, respectively.\nIn addition, a method to detect warning symptoms is implemented to render the\nclassification task transparent from a medical perspective. The method is based\non the learning of attention scores and a method of automatic validation using\nthe same data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 00:14:10 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Girardi", "Ivan", ""], ["Ji", "Pengfei", ""], ["Nguyen", "An-phi", ""], ["Hollenstein", "Nora", ""], ["Ivankay", "Adam", ""], ["Kuhn", "Lorenz", ""], ["Marchiori", "Chiara", ""], ["Zhang", "Ce", ""]]}, {"id": "1809.10816", "submitter": "Zhe Li", "authors": "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng\n  Wang and Xiangnan He", "title": "Generative Adversarial Active Learning for Unsupervised Outlier\n  Detection", "comments": "TKDE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is an important topic in machine learning and has been used\nin a wide range of applications. In this paper, we approach outlier detection\nas a binary-classification issue by sampling potential outliers from a uniform\nreference distribution. However, due to the sparsity of data in\nhigh-dimensional space, a limited number of potential outliers may fail to\nprovide sufficient information to assist the classifier in describing a\nboundary that can separate outliers from normal data effectively. To address\nthis, we propose a novel Single-Objective Generative Adversarial Active\nLearning (SO-GAAL) method for outlier detection, which can directly generate\ninformative potential outliers based on the mini-max game between a generator\nand a discriminator. Moreover, to prevent the generator from falling into the\nmode collapsing problem, the stop node of training should be determined when\nSO-GAAL is able to provide sufficient information. But without any prior\ninformation, it is extremely difficult for SO-GAAL. Therefore, we expand the\nnetwork structure of SO-GAAL from a single generator to multiple generators\nwith different objectives (MO-GAAL), which can generate a reasonable reference\ndistribution for the whole dataset. We empirically compare the proposed\napproach with several state-of-the-art outlier detection methods on both\nsynthetic and real-world datasets. The results show that MO-GAAL outperforms\nits competitors in the majority of cases, especially for datasets with various\ncluster types or high irrelevant variable ratio.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 01:41:59 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 01:58:16 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 13:44:52 GMT"}, {"version": "v4", "created": "Mon, 18 Mar 2019 02:15:27 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Liu", "Yezheng", ""], ["Li", "Zhe", ""], ["Zhou", "Chong", ""], ["Jiang", "Yuanchun", ""], ["Sun", "Jianshan", ""], ["Wang", "Meng", ""], ["He", "Xiangnan", ""]]}, {"id": "1809.10818", "submitter": "Wenbo Wang", "authors": "Wenbo Wang and Xingye Qiao", "title": "Learning Confidence Sets using Support Vector Machines", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of confidence-set learning in the binary classification setting is\nto construct two sets, each with a specific probability guarantee to cover a\nclass. An observation outside the overlap of the two sets is deemed to be from\none of the two classes, while the overlap is an ambiguity region which could\nbelong to either class. Instead of plug-in approaches, we propose a support\nvector classifier to construct confidence sets in a flexible manner.\nTheoretically, we show that the proposed learner can control the non-coverage\nrates and minimize the ambiguity with high probability. Efficient algorithms\nare developed and numerical studies illustrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 01:47:54 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Wang", "Wenbo", ""], ["Qiao", "Xingye", ""]]}, {"id": "1809.10827", "submitter": "Ji Oon Lee", "authors": "Hye Won Chung and Ji Oon Lee", "title": "Weak detection in the spiked Wigner model", "comments": "45 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the weak detection problem in a rank-one spiked Wigner data\nmatrix where the signal-to-noise ratio is small so that reliable detection is\nimpossible. We propose a hypothesis test on the presence of the signal by\nutilizing the linear spectral statistics of the data matrix. The test is\ndata-driven and does not require prior knowledge about the distribution of the\nsignal or the noise. When the noise is Gaussian, the proposed test is optimal\nin the sense that its error matches that of the likelihood ratio test, which\nminimizes the sum of the Type-I and Type-II errors. If the density of the noise\nis known and non-Gaussian, the error of the test can be lowered by applying an\nentrywise transformation to the data matrix. We establish a central limit\ntheorem for the linear spectral statistics of general rank-one spiked Wigner\nmatrices as an intermediate step.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 02:30:27 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 07:44:17 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 02:53:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""]]}, {"id": "1809.10829", "submitter": "Yuandong Tian", "authors": "Yuandong Tian", "title": "A theoretical framework for deep locally connected ReLU network", "comments": "Submitted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding theoretical properties of deep and locally connected nonlinear\nnetwork, such as deep convolutional neural network (DCNN), is still a hard\nproblem despite its empirical success. In this paper, we propose a novel\ntheoretical framework for such networks with ReLU nonlinearity. The framework\nexplicitly formulates data distribution, favors disentangled representations\nand is compatible with common regularization techniques such as Batch Norm. The\nframework is built upon teacher-student setting, by expanding the student\nforward/backward propagation onto the teacher's computational graph. The\nresulting model does not impose unrealistic assumptions (e.g., Gaussian inputs,\nindependence of activation, etc). Our framework could help facilitate\ntheoretical analysis of many practical issues, e.g. overfitting,\ngeneralization, disentangled representations in deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 02:37:35 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Tian", "Yuandong", ""]]}, {"id": "1809.10835", "submitter": "Dung Thai", "authors": "Dung Thai, Sree Harsha Ramesh, Shikhar Murty, Luke Vilnis, Andrew\n  McCallum", "title": "Embedded-State Latent Conditional Random Fields for Sequence Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex textual information extraction tasks are often posed as sequence\nlabeling or \\emph{shallow parsing}, where fields are extracted using local\nlabels made consistent through probabilistic inference in a graphical model\nwith constrained transitions. Recently, it has become common to locally\nparametrize these models using rich features extracted by recurrent neural\nnetworks (such as LSTM), while enforcing consistent outputs through a simple\nlinear-chain model, representing Markovian dependencies between successive\nlabels. However, the simple graphical model structure belies the often complex\nnon-local constraints between output labels. For example, many fields, such as\na first name, can only occur a fixed number of times, or in the presence of\nother fields. While RNNs have provided increasingly powerful context-aware\nlocal features for sequence tagging, they have yet to be integrated with a\nglobal graphical model of similar expressivity in the output distribution. Our\nmodel goes beyond the linear chain CRF to incorporate multiple hidden states\nper output label, but parametrizes their transitions parsimoniously with\nlow-rank log-potential scoring matrices, effectively learning an embedding\nspace for hidden states. This augmented latent space of inference variables\ncomplements the rich feature representation of the RNN, and allows exact global\ninference obeying complex, learned non-local output constraints. We experiment\nwith several datasets and show that the model outperforms baseline CRF+RNN\nmodels when global output constraints are necessary at inference-time, and\nexplore the interpretable latent structure.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 03:06:31 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Thai", "Dung", ""], ["Ramesh", "Sree Harsha", ""], ["Murty", "Shikhar", ""], ["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1809.10842", "submitter": "Yi Wu", "authors": "Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari,\n  Yuandong Tian", "title": "Learning and Planning with a Semantic Model", "comments": "submitted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building deep reinforcement learning agents that can generalize and adapt to\nunseen environments remains a fundamental challenge for AI. This paper\ndescribes progresses on this challenge in the context of man-made environments,\nwhich are visually diverse but contain intrinsic semantic regularities. We\npropose a hybrid model-based and model-free approach, LEArning and Planning\nwith Semantics (LEAPS), consisting of a multi-target sub-policy that acts on\nvisual inputs, and a Bayesian model over semantic structures. When placed in an\nunseen environment, the agent plans with the semantic model to make high-level\ndecisions, proposes the next sub-target for the sub-policy to execute, and\nupdates the semantic model based on new observations. We perform experiments in\nvisual navigation tasks using House3D, a 3D environment that contains diverse\nhuman-designed indoor scenes with real-world objects. LEAPS outperforms strong\nbaselines that do not explicitly plan using the semantic content.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 03:30:37 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Wu", "Yi", ""], ["Wu", "Yuxin", ""], ["Tamar", "Aviv", ""], ["Russell", "Stuart", ""], ["Gkioxari", "Georgia", ""], ["Tian", "Yuandong", ""]]}, {"id": "1809.10847", "submitter": "T.S. Jayram", "authors": "T.S. Jayram and Tomasz Kornuta and Ryan L. McAvoy and Ahmet S. Ozcan", "title": "Using Multi-task and Transfer Learning to Solve Working Memory Tasks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture called Memory-Augmented Encoder-Solver (MAES)\nthat enables transfer learning to solve complex working memory tasks adapted\nfrom cognitive psychology. It uses dual recurrent neural network controllers,\ninside the encoder and solver, respectively, that interface with a shared\nmemory module and is completely differentiable. We study different types of\nencoders in a systematic manner and demonstrate a unique advantage of\nmulti-task learning in obtaining the best possible encoder. We show by\nextensive experimentation that the trained MAES models achieve task-size\ngeneralization, i.e., they are capable of handling sequential inputs 50 times\nlonger than seen during training, with appropriately large memory modules. We\ndemonstrate that the performance achieved by MAES far outperforms existing and\nwell-known models such as the LSTM, NTM and DNC on the entire suite of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 04:01:06 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Jayram", "T. S.", ""], ["Kornuta", "Tomasz", ""], ["McAvoy", "Ryan L.", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1809.10851", "submitter": "Dongmian Zou", "authors": "Dongmian Zou, Gilad Lerman", "title": "Encoding Robust Representation for Graph Generation", "comments": "9 pages, 7 figures, 6 tables", "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN),\n  Budapest, Hungary, 2019, pp. 1-9", "doi": "10.1109/IJCNN.2019.8851705", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative networks have made it possible to generate meaningful signals such\nas images and texts from simple noise. Recently, generative methods based on\nGAN and VAE were developed for graphs and graph signals. However, the\nmathematical properties of these methods are unclear, and training good\ngenerative models is difficult. This work proposes a graph generation model\nthat uses a recent adaptation of Mallat's scattering transform to graphs. The\nproposed model is naturally composed of an encoder and a decoder. The encoder\nis a Gaussianized graph scattering transform, which is robust to signal and\ngraph manipulation. The decoder is a simple fully connected network that is\nadapted to specific tasks, such as link prediction, signal generation on graphs\nand full graph and signal generation. The training of our proposed system is\nefficient since it is only applied to the decoder and the hardware requirements\nare moderate. Numerical results demonstrate state-of-the-art performance of the\nproposed system for both link prediction and graph and signal generation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 04:11:18 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 23:52:51 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zou", "Dongmian", ""], ["Lerman", "Gilad", ""]]}, {"id": "1809.10858", "submitter": "Chulhee Yun", "authors": "Chulhee Yun, Suvrit Sra, Ali Jadbabaie", "title": "Efficiently testing local optimality and escaping saddles for ReLU\n  networks", "comments": "23 pages, appeared at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a theoretical algorithm for checking local optimality and escaping\nsaddles at nondifferentiable points of empirical risks of two-layer ReLU\nnetworks. Our algorithm receives any parameter value and returns: local\nminimum, second-order stationary point, or a strict descent direction. The\npresence of $M$ data points on the nondifferentiability of the ReLU divides the\nparameter space into at most $2^M$ regions, which makes analysis difficult. By\nexploiting polyhedral geometry, we reduce the total computation down to one\nconvex quadratic program (QP) for each hidden node, $O(M)$ (in)equality tests,\nand one (or a few) nonconvex QP. For the last QP, we show that our specific\nproblem can be solved efficiently, in spite of nonconvexity. In the benign\ncase, we solve one equality constrained QP, and we prove that projected\ngradient descent solves it exponentially fast. In the bad case, we have to\nsolve a few more inequality constrained QPs, but we prove that the time\ncomplexity is exponential only in the number of inequality constraints. Our\nexperiments show that either benign case or bad case with very few inequality\nconstraints occurs, implying that our algorithm is efficient in most cases.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 04:53:03 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 00:22:12 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Yun", "Chulhee", ""], ["Sra", "Suvrit", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1809.10862", "submitter": "Zhiling Guo", "authors": "Zhiling Guo, Hiroaki Shengoku, Guangming Wu, Qi Chen, Wei Yuan,\n  Xiaodan Shi, Xiaowei Shao, Yongwei Xu, Ryosuke Shibasaki", "title": "Semantic Segmentation for Urban Planning Maps based on U-Net", "comments": "4 pages, 3 figures, conference, International Geoscience and Remote\n  Sensing Symposium (IGARSS 2018), Jul 2018, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic digitizing of paper maps is a significant and challenging task\nfor both academia and industry. As an important procedure of map digitizing,\nthe semantic segmentation section mainly relies on manual visual interpretation\nwith low efficiency. In this study, we select urban planning maps as a\nrepresentative sample and investigate the feasibility of utilizing U-shape\nfully convolutional based architecture to perform end-to-end map semantic\nsegmentation. The experimental results obtained from the test area in Shibuya\ndistrict, Tokyo, demonstrate that our proposed method could achieve a very high\nJaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For\nimplementation on GPGPU and cuDNN, the required processing time for the whole\nShibuya district can be less than three minutes. The results indicate the\nproposed method can serve as a viable tool for urban planning map semantic\nsegmentation task with high accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 05:32:45 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 03:00:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Guo", "Zhiling", ""], ["Shengoku", "Hiroaki", ""], ["Wu", "Guangming", ""], ["Chen", "Qi", ""], ["Yuan", "Wei", ""], ["Shi", "Xiaodan", ""], ["Shao", "Xiaowei", ""], ["Xu", "Yongwei", ""], ["Shibasaki", "Ryosuke", ""]]}, {"id": "1809.10875", "submitter": "Zhuolin Yang", "authors": "Zhuolin Yang, Bo Li, Pin-Yu Chen, Dawn Song", "title": "Characterizing Audio Adversarial Examples Using Temporal Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have highlighted adversarial examples as a ubiquitous threat\nto different neural network models and many downstream applications.\nNonetheless, as unique data properties have inspired distinct and powerful\nlearning principles, this paper aims to explore their potentials towards\nmitigating adversarial inputs. In particular, our results reveal the importance\nof using the temporal dependency in audio data to gain discriminate power\nagainst adversarial examples. Tested on the automatic speech recognition (ASR)\ntasks and three recent audio adversarial attacks, we find that (i) input\ntransformation developed from image adversarial defense provides limited\nrobustness improvement and is subtle to advanced attacks; (ii) temporal\ndependency can be exploited to gain discriminative power against audio\nadversarial examples and is resistant to adaptive attacks considered in our\nexperiments. Our results not only show promising means of improving the\nrobustness of ASR systems, but also offer novel insights in exploiting\ndomain-specific data properties to mitigate negative effects of adversarial\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 06:39:42 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 15:21:37 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Yang", "Zhuolin", ""], ["Li", "Bo", ""], ["Chen", "Pin-Yu", ""], ["Song", "Dawn", ""]]}, {"id": "1809.10877", "submitter": "Seonguk Seo", "authors": "Seonguk Seo, Paul Hongsuck Seo, Bohyung Han", "title": "Learning for Single-Shot Confidence Calibration in Deep Neural Networks\n  through Stochastic Inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic framework to calibrate accuracy and confidence of a\nprediction in deep neural networks through stochastic inferences. We interpret\nstochastic regularization using a Bayesian model, and analyze the relation\nbetween predictive uncertainty of networks and variance of the prediction\nscores obtained by stochastic inferences for a single example. Our empirical\nstudy shows that the accuracy and the score of a prediction are highly\ncorrelated with the variance of multiple stochastic inferences given by\nstochastic depth or dropout. Motivated by this observation, we design a novel\nvariance-weighted confidence-integrated loss function that is composed of two\ncross-entropy loss terms with respect to ground-truth and uniform distribution,\nwhich are balanced by variance of stochastic prediction scores. The proposed\nloss function enables us to learn deep neural networks that predict confidence\ncalibrated scores using a single inference. Our algorithm presents outstanding\nconfidence calibration performance and improves classification accuracy when\ncombined with two popular stochastic regularization techniques---stochastic\ndepth and dropout---in multiple models and datasets; it alleviates\noverconfidence issue in deep neural networks significantly by training networks\nto achieve prediction accuracy proportional to confidence of prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 06:43:26 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 07:37:17 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 07:05:15 GMT"}, {"version": "v4", "created": "Tue, 27 Nov 2018 04:20:29 GMT"}, {"version": "v5", "created": "Wed, 24 Apr 2019 17:20:34 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Seo", "Seonguk", ""], ["Seo", "Paul Hongsuck", ""], ["Han", "Bohyung", ""]]}, {"id": "1809.10889", "submitter": "Zheyi Pan", "authors": "Zheyi Pan, Yuxuan Liang, Junbo Zhang, Xiuwen Yi, Yong Yu and Yu Zheng", "title": "HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal (ST) data, which represent multiple time series data\ncorresponding to different spatial locations, are ubiquitous in real-world\ndynamic systems, such as air quality readings. Forecasting over ST data is of\ngreat importance but challenging as it is affected by many complex factors,\nincluding spatial characteristics, temporal characteristics and the intrinsic\ncausality between them. In this paper, we propose a general framework\n(HyperST-Net) based on hypernetworks for deep ST models. More specifically, it\nconsists of three major modules: a spatial module, a temporal module and a\ndeduction module. Among them, the deduction module derives the parameter\nweights of the temporal module from the spatial characteristics, which are\nextracted by the spatial module. Then, we design a general form of HyperST\nlayer as well as different forms for several basic layers in neural networks,\nincluding the dense layer (HyperST-Dense) and the convolutional layer\n(HyperST-Conv). Experiments on three types of real-world tasks demonstrate that\nthe predictive models integrated with our framework achieve significant\nimprovements, and outperform the state-of-the-art baselines as well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 07:29:21 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Pan", "Zheyi", ""], ["Liang", "Yuxuan", ""], ["Zhang", "Junbo", ""], ["Yi", "Xiuwen", ""], ["Yu", "Yong", ""], ["Zheng", "Yu", ""]]}, {"id": "1809.10932", "submitter": "Huy Phan", "authors": "Huy Phan, Fernando Andreotti, Navin Cooray, Oliver Y. Ch\\'en, Maarten\n  De Vos", "title": "SeqSleepNet: End-to-End Hierarchical Recurrent Neural Network for\n  Sequence-to-Sequence Automatic Sleep Staging", "comments": "This article has been published in IEEE Transactions on Neural\n  Systems and Rehabilitation Engineering", "journal-ref": null, "doi": "10.1109/TNSRE.2019.2896659", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic sleep staging has been often treated as a simple classification\nproblem that aims at determining the label of individual target polysomnography\n(PSG) epochs one at a time. In this work, we tackle the task as a\nsequence-to-sequence classification problem that receives a sequence of\nmultiple epochs as input and classifies all of their labels at once. For this\npurpose, we propose a hierarchical recurrent neural network named SeqSleepNet.\nAt the epoch processing level, the network consists of a filterbank layer\ntailored to learn frequency-domain filters for preprocessing and an\nattention-based recurrent layer designed for short-term sequential modelling.\nAt the sequence processing level, a recurrent layer placed on top of the\nlearned epoch-wise features for long-term modelling of sequential epochs. The\nclassification is then carried out on the output vectors at every time step of\nthe top recurrent layer to produce the sequence of output labels. Despite being\nhierarchical, we present a strategy to train the network in an end-to-end\nfashion. We show that the proposed network outperforms state-of-the-art\napproaches, achieving an overall accuracy, macro F1-score, and Cohen's kappa of\n87.1%, 83.3%, and 0.815 on a publicly available dataset with 200 subjects.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:37:48 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 08:17:21 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2019 01:46:14 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Phan", "Huy", ""], ["Andreotti", "Fernando", ""], ["Cooray", "Navin", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["De Vos", "Maarten", ""]]}, {"id": "1809.10941", "submitter": "Samir Suweis Dr.", "authors": "Alberto Testolin, Michele Piccolini, Samir Suweis", "title": "Deep learning systems as complex networks", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the availability of large scale digital datasets and massive\namounts of computational power, deep learning algorithms can learn\nrepresentations of data by exploiting multiple levels of abstraction. These\nmachine learning methods have greatly improved the state-of-the-art in many\nchallenging cognitive tasks, such as visual object recognition, speech\nprocessing, natural language understanding and automatic translation. In\nparticular, one class of deep learning models, known as deep belief networks,\ncan discover intricate statistical structure in large data sets in a completely\nunsupervised fashion, by learning a generative model of the data using\nHebbian-like learning mechanisms. Although these self-organizing systems can be\nconveniently formalized within the framework of statistical mechanics, their\ninternal functioning remains opaque, because their emergent dynamics cannot be\nsolved analytically. In this article we propose to study deep belief networks\nusing techniques commonly employed in the study of complex networks, in order\nto gain some insights into the structural and functional properties of the\ncomputational graph resulting from the learning process.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 10:06:36 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Testolin", "Alberto", ""], ["Piccolini", "Michele", ""], ["Suweis", "Samir", ""]]}, {"id": "1809.10961", "submitter": "Radu Horaud P", "authors": "Yutong Ban, Xavier Alameda-Pineda, Laurent Girin and Radu Horaud", "title": "Variational Bayesian Inference for Audio-Visual Tracking of Multiple\n  Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of tracking multiple speakers via the\nfusion of visual and auditory information. We propose to exploit the\ncomplementary nature of these two modalities in order to accurately estimate\nsmooth trajectories of the tracked persons, to deal with the partial or total\nabsence of one of the modalities over short periods of time, and to estimate\nthe acoustic status -- either speaking or silent -- of each tracked person\nalong time. We propose to cast the problem at hand into a generative\naudio-visual fusion (or association) model formulated as a latent-variable\ntemporal graphical model. This may well be viewed as the problem of maximizing\nthe posterior joint distribution of a set of continuous and discrete latent\nvariables given the past and current observations, which is intractable. We\npropose a variational inference model which amounts to approximate the joint\ndistribution with a factorized distribution. The solution takes the form of a\nclosed-form expectation maximization procedure. We describe in detail the\ninference algorithm, we evaluate its performance and we compare it with several\nbaseline methods. These experiments show that the proposed audio-visual tracker\nperforms well in informal meetings involving a time-varying number of people.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:03:03 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:54:55 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ban", "Yutong", ""], ["Alameda-Pineda", "Xavier", ""], ["Girin", "Laurent", ""], ["Horaud", "Radu", ""]]}, {"id": "1809.10962", "submitter": "Xiaofeng Cao", "authors": "Xiaofeng Cao, Ivor W. Tsang, Xiaofeng Xu, Guandong Xu", "title": "Target-Independent Active Learning via Distribution-Splitting", "comments": "This paper has been withdrawn. The first author quitted the PhD study\n  from AAI, University of Technology Sydney. The manuscript stopped updating", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the label complexity in Agnostic Active Learning (A^2 algorithm),\nvolume-splitting splits the hypothesis edges to reduce the Vapnik-Chervonenkis\n(VC) dimension in version space. However, the effectiveness of volume-splitting\ncritically depends on the initial hypothesis and this problem is also known as\ntarget-dependent label complexity gap. This paper attempts to minimize this gap\nby introducing a novel notion of number density which provides a more natural\nand direct way to describe the hypothesis distribution than volume. By\ndiscovering the connections between hypothesis and input distribution, we map\nthe volume of version space into the number density and propose a\ntarget-independent distribution-splitting strategy with the following\nadvantages: 1) provide theoretical guarantees on reducing label complexity and\nerror rate as volume-splitting; 2) break the curse of initial hypothesis; 3)\nprovide model guidance for a target-independent AL algorithm in real AL tasks.\nWith these guarantees, for AL application, we then split the input distribution\ninto more near-optimal spheres and develop an application algorithm called\nDistribution-based A^2 (DA^2). Experiments further verify the effectiveness of\nthe halving and querying abilities of DA^2. Contributions of this paper are as\nfollows.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:07:03 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 23:52:32 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Cao", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""], ["Xu", "Xiaofeng", ""], ["Xu", "Guandong", ""]]}, {"id": "1809.10979", "submitter": "Stephan Spiegel Dr.", "authors": "Stephan Spiegel, Fabian Mueller, Dorothea Weismann, John Bird", "title": "Cost-Sensitive Learning for Predictive Maintenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In predictive maintenance, model performance is usually assessed by means of\nprecision, recall, and F1-score. However, employing the model with best\nperformance, e.g. highest F1-score, does not necessarily result in minimum\nmaintenance cost, but can instead lead to additional expenses. Thus, we propose\nto perform model selection based on the economic costs associated with the\nparticular maintenance application. We show that cost-sensitive learning for\npredictive maintenance can result in significant cost reduction and fault\ntolerant policies, since it allows to incorporate various business constraints\nand requirements.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:08:51 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Spiegel", "Stephan", ""], ["Mueller", "Fabian", ""], ["Weismann", "Dorothea", ""], ["Bird", "John", ""]]}, {"id": "1809.11003", "submitter": "Hongyu Liu", "authors": "Jinhong Li, Hongyu Liu, Wing-Yan Tsui and Xianchao Wang", "title": "An inverse scattering approach for geometric body generation: a machine\n  learning perspective", "comments": "22pages, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the 2D and 3D geometric shape generation\nby prescribing a set of characteristic values of a specific geometric body. One\nof the major motivations of our study is the 3D human body generation in\nvarious applications. We develop a novel method that can generate the desired\nbody with customized characteristic values. The proposed method follows a\nmachine-learning flavour that generates the inferred geometric body with the\ninput characteristic parameters from a training dataset. One of the critical\ningredients and novelties of our method is the borrowing of inverse scattering\ntechniques in the theory of wave propagation to the body generation. This is\ndone by establishing a delicate one-to-one correspondence between a geometric\nbody and the far-field pattern of a source scattering problem governed by the\nHelmholtz system. It in turn enables us to establish a one-to-one\ncorrespondence between the geometric body space and the function space defined\nby the far-field patterns. Hence, the far-field patterns can act as the shape\ngenerators. The shape generation with prescribed characteristic parameters is\nachieved by first manipulating the shape generators and then reconstructing the\ncorresponding geometric body from the obtained shape generator by a stable\nmultiple-frequency Fourier method. Our method is easy to implement and produces\nmore efficient and stable body generations. We provide both theoretical\nanalysis and extensive numerical experiments for the proposed method. The study\nis the first attempt to introduce inverse scattering approaches in combination\nwith machine learning to the geometric body generation and it opens up many\nopportunities for further developments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:48:57 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Li", "Jinhong", ""], ["Liu", "Hongyu", ""], ["Tsui", "Wing-Yan", ""], ["Wang", "Xianchao", ""]]}, {"id": "1809.11008", "submitter": "Bo Han", "authors": "Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor Tsang,\n  Masashi Sugiyama", "title": "SIGUA: Forgetting May Make Learning with Noisy Labels More Robust", "comments": "ICML 2020 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data with noisy labels, over-parameterized deep networks can gradually\nmemorize the data, and fit everything in the end. Although equipped with\ncorrections for noisy labels, many learning methods in this area still suffer\noverfitting due to undesired memorization. In this paper, to relieve this\nissue, we propose stochastic integrated gradient underweighted ascent (SIGUA):\nin a mini-batch, we adopt gradient descent on good data as usual, and\nlearning-rate-reduced gradient ascent on bad data; the proposal is a versatile\napproach where data goodness or badness is w.r.t. desired or undesired\nmemorization given a base learning method. Technically, SIGUA pulls\noptimization back for generalization when their goals conflict with each other;\nphilosophically, SIGUA shows forgetting undesired memorization can reinforce\ndesired memorization. Experiments demonstrate that SIGUA successfully\nrobustifies two typical base learning methods, so that their performance is\noften significantly improved.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 13:08:54 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:05:16 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 03:19:31 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Han", "Bo", ""], ["Niu", "Gang", ""], ["Yu", "Xingrui", ""], ["Yao", "Quanming", ""], ["Xu", "Miao", ""], ["Tsang", "Ivor", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1809.11033", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Leonardo Cella, Nicol\\`o Cesa-Bianchi", "title": "Efficient Linear Bandits through Matrix Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that two popular linear contextual bandit algorithms, OFUL and\nThompson Sampling, can be made efficient using Frequent Directions, a\ndeterministic online sketching technique. More precisely, we show that a sketch\nof size $m$ allows a $\\mathcal{O}(md)$ update time for both algorithms, as\nopposed to $\\Omega(d^2)$ required by their non-sketched versions in general\n(where $d$ is the dimension of context vectors). This computational speedup is\naccompanied by regret bounds of order $(1+\\varepsilon_m)^{3/2}d\\sqrt{T}$ for\nOFUL and of order $\\big((1+\\varepsilon_m)d\\big)^{3/2}\\sqrt{T}$ for Thompson\nSampling, where $\\varepsilon_m$ is bounded by the sum of the tail eigenvalues\nnot covered by the sketch. In particular, when the selected contexts span a\nsubspace of dimension at most $m$, our algorithms have a regret bound matching\nthat of their slower, non-sketched counterparts. Experiments on real-world\ndatasets corroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:00:05 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 21:39:50 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Cella", "Leonardo", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1809.11044", "submitter": "Andrea Tacchetti", "authors": "Andrea Tacchetti, H. Francis Song, Pedro A. M. Mediano, Vinicius\n  Zambaldi, Neil C. Rabinowitz, Thore Graepel, Matthew Botvinick, Peter W.\n  Battaglia", "title": "Relational Forward Models for Multi-Agent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavioral dynamics of multi-agent systems have a rich and orderly\nstructure, which can be leveraged to understand these systems, and to improve\nhow artificial agents learn to operate in them. Here we introduce Relational\nForward Models (RFM) for multi-agent learning, networks that can learn to make\naccurate predictions of agents' future behavior in multi-agent environments.\nBecause these models operate on the discrete entities and relations present in\nthe environment, they produce interpretable intermediate representations which\noffer insights into what drives agents' behavior, and what events mediate the\nintensity and valence of social interactions. Furthermore, we show that\nembedding RFM modules inside agents results in faster learning systems compared\nto non-augmented baselines. As more and more of the autonomous systems we\ndevelop and interact with become multi-agent in nature, developing richer\nanalysis tools for characterizing how and why agents make decisions is\nincreasingly necessary. Moreover, developing artificial agents that quickly and\nsafely learn to coordinate with one another, and with humans in shared\nenvironments, is crucial.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:10:39 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Song", "H. Francis", ""], ["Mediano", "Pedro A. M.", ""], ["Zambaldi", "Vinicius", ""], ["Rabinowitz", "Neil C.", ""], ["Graepel", "Thore", ""], ["Botvinick", "Matthew", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1809.11084", "submitter": "Saravanan Thirumuruganathan", "authors": "Saravanan Thirumuruganathan, Shameem A Puthiya Parambath, Mourad\n  Ouzzani, Nan Tang, Shafiq Joty", "title": "Reuse and Adaptation for Entity Resolution through Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is one of the fundamental problems in data\nintegration, where machine learning (ML) based classifiers often provide the\nstate-of-the-art results. Considerable human effort goes into feature\nengineering and training data creation. In this paper, we investigate a new\nproblem: Given a dataset D_T for ER with limited or no training data, is it\npossible to train a good ML classifier on D_T by reusing and adapting the\ntraining data of dataset D_S from same or related domain? Our major\ncontributions include (1) a distributed representation based approach to encode\neach tuple from diverse datasets into a standard feature space; (2)\nidentification of common scenarios where the reuse of training data can be\nbeneficial; and (3) five algorithms for handling each of the aforementioned\nscenarios. We have performed comprehensive experiments on 12 datasets from 5\ndifferent domains (publications, movies, songs, restaurants, and books). Our\nexperiments show that our algorithms provide significant benefits such as\nproviding superior performance for a fixed training data size.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:26:17 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Thirumuruganathan", "Saravanan", ""], ["Parambath", "Shameem A Puthiya", ""], ["Ouzzani", "Mourad", ""], ["Tang", "Nan", ""], ["Joty", "Shafiq", ""]]}, {"id": "1809.11086", "submitter": "Arash Ardakani", "authors": "Arash Ardakani, Zhengyun Ji, Sean C. Smithson, Brett H. Meyer, Warren\n  J. Gross", "title": "Learning Recurrent Binary/Ternary Weights", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown excellent performance in\nprocessing sequence data. However, they are both complex and memory intensive\ndue to their recursive nature. These limitations make RNNs difficult to embed\non mobile devices requiring real-time processes with limited hardware\nresources. To address the above issues, we introduce a method that can learn\nbinary and ternary weights during the training phase to facilitate hardware\nimplementations of RNNs. As a result, using this approach replaces all\nmultiply-accumulate operations by simple accumulations, bringing significant\nbenefits to custom hardware in terms of silicon area and power consumption. On\nthe software side, we evaluate the performance (in terms of accuracy) of our\nmethod using long short-term memories (LSTMs) on various sequential models\nincluding sequence classification and language modeling. We demonstrate that\nour method achieves competitive results on the aforementioned tasks while using\nbinary/ternary weights during the runtime. On the hardware side, we present\ncustom hardware for accelerating the recurrent computations of LSTMs with\nbinary/ternary weights. Ultimately, we show that LSTMs with binary/ternary\nweights can achieve up to 12x memory saving and 10x inference speedup compared\nto the full-precision implementation on an ASIC platform.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:27:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 19:14:18 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Ardakani", "Arash", ""], ["Ji", "Zhengyun", ""], ["Smithson", "Sean C.", ""], ["Meyer", "Brett H.", ""], ["Gross", "Warren J.", ""]]}, {"id": "1809.11087", "submitter": "T.S. Jayram", "authors": "T.S. Jayram and Younes Bouhadjar and Ryan L. McAvoy and Tomasz Kornuta\n  and Alexis Asseman and Kamil Rocki and Ahmet S. Ozcan", "title": "Learning to Remember, Forget and Ignore using Attention Control in\n  Memory", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical neural networks with external memory do not effectively separate\ncapacity for episodic and working memory as is required for reasoning in\nhumans. Applying knowledge gained from psychological studies, we designed a new\nmodel called Differentiable Working Memory (DWM) in order to specifically\nemulate human working memory. As it shows the same functional characteristics\nas working memory, it robustly learns psychology inspired tasks and converges\nfaster than comparable state-of-the-art models. Moreover, the DWM model\nsuccessfully generalizes to sequences two orders of magnitude longer than the\nones used in training. Our in-depth analysis shows that the behavior of DWM is\ninterpretable and that it learns to have fine control over memory, allowing it\nto retain, ignore or forget information based on its relevance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:30:54 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Jayram", "T. S.", ""], ["Bouhadjar", "Younes", ""], ["McAvoy", "Ryan L.", ""], ["Kornuta", "Tomasz", ""], ["Asseman", "Alexis", ""], ["Rocki", "Kamil", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1809.11092", "submitter": "Stefan Klus", "authors": "Stefan Klus, Andreas Bittracher, Ingmar Schuster, Christof Sch\\\"utte", "title": "A kernel-based approach to molecular conformation analysis", "comments": null, "journal-ref": "Journal of Chemical Physics 149, 244109, 2018", "doi": "10.1063/1.5063533", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel machine learning approach to understanding conformation\ndynamics of biomolecules. The approach combines kernel-based techniques that\nare popular in the machine learning community with transfer operator theory for\nanalyzing dynamical systems in order to identify conformation dynamics based on\nmolecular dynamics simulation data. We show that many of the prominent methods\nlike Markov State Models, EDMD, and TICA can be regarded as special cases of\nthis approach and that new efficient algorithms can be constructed based on\nthis derivation. The results of these new powerful methods will be illustrated\nwith several examples, in particular the alanine dipeptide and the protein\nNTL9.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:36:11 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 14:35:26 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Klus", "Stefan", ""], ["Bittracher", "Andreas", ""], ["Schuster", "Ingmar", ""], ["Sch\u00fctte", "Christof", ""]]}, {"id": "1809.11096", "submitter": "Andrew Brock", "authors": "Andrew Brock, Jeff Donahue, Karen Simonyan", "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in generative image modeling, successfully generating\nhigh-resolution, diverse samples from complex datasets such as ImageNet remains\nan elusive goal. To this end, we train Generative Adversarial Networks at the\nlargest scale yet attempted, and study the instabilities specific to such\nscale. We find that applying orthogonal regularization to the generator renders\nit amenable to a simple \"truncation trick,\" allowing fine control over the\ntrade-off between sample fidelity and variety by reducing the variance of the\nGenerator's input. Our modifications lead to models which set the new state of\nthe art in class-conditional image synthesis. When trained on ImageNet at\n128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\nbest IS of 52.52 and FID of 18.6.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:38:49 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 21:32:06 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Brock", "Andrew", ""], ["Donahue", "Jeff", ""], ["Simonyan", "Karen", ""]]}, {"id": "1809.11115", "submitter": "Thomas Bonald", "authors": "Thomas Bonald, Alexandre Hollocou, Marc Lelarge", "title": "Weighted Spectral Embedding of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel spectral embedding of graphs that incorporates weights\nassigned to the nodes, quantifying their relative importance. This spectral\nembedding is based on the first eigenvectors of some properly normalized\nversion of the Laplacian. We prove that these eigenvectors correspond to the\nconfigurations of lowest energy of an equivalent physical system, either\nmechanical or electrical, in which the weight of each node can be interpreted\nas its mass or its capacitance, respectively. Experiments on a real dataset\nillustrate the impact of weighting on the embedding.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 16:05:03 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 15:15:11 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Bonald", "Thomas", ""], ["Hollocou", "Alexandre", ""], ["Lelarge", "Marc", ""]]}, {"id": "1809.11142", "submitter": "Chao Ma", "authors": "Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jos\\'e Miguel\n  Hern\\'andez-Lobato, Sebastian Nowozin, Cheng Zhang", "title": "EDDI: Efficient Dynamic Discovery of High-Value Information with Partial\n  VAE", "comments": "icml 2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life decision-making situations allow further relevant information\nto be acquired at a specific cost, for example, in assessing the health status\nof a patient we may decide to take additional measurements such as diagnostic\ntests or imaging scans before making a final assessment. Acquiring more\nrelevant information enables better decision making, but may be costly. How can\nwe trade off the desire to make good decisions by acquiring further information\nwith the cost of performing that acquisition? To this end, we propose a\nprincipled framework, named EDDI (Efficient Dynamic Discovery of high-value\nInformation), based on the theory of Bayesian experimental design. In EDDI, we\npropose a novel partial variational autoencoder (Partial VAE) to predict\nmissing data entries problematically given any subset of the observed ones, and\ncombine it with an acquisition function that maximizes expected information\ngain on a set of target variables. We show cost reduction at the same decision\nquality and improved decision quality at the same cost in multiple machine\nlearning benchmarks and two real-world health-care applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 16:55:26 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:29:10 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 13:28:18 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 21:49:25 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ma", "Chao", ""], ["Tschiatschek", "Sebastian", ""], ["Palla", "Konstantina", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Nowozin", "Sebastian", ""], ["Zhang", "Cheng", ""]]}, {"id": "1809.11165", "submitter": "Geoff Pleiss", "authors": "Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger,\n  Andrew Gordon Wilson", "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU\n  Acceleration", "comments": "NeurIPS 2018. Most recent version includes additional details on\n  preconditioned BBMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in scalable models, the inference tools used for Gaussian\nprocesses (GPs) have yet to fully capitalize on developments in computing\nhardware. We present an efficient and general approach to GP inference based on\nBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified\nbatched version of the conjugate gradients algorithm to derive all terms for\ntraining and inference in a single call. BBMM reduces the asymptotic complexity\nof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to\nscalable approximations and complex GP models simply requires a routine for\nefficient matrix-matrix multiplication with the kernel and its derivative. In\naddition, BBMM uses a specialized preconditioner to substantially speed up\nconvergence. In experiments we show that BBMM effectively uses GPU hardware to\ndramatically accelerate both exact GP inference and scalable approximations.\nAdditionally, we provide GPyTorch, a software platform for scalable GP\ninference via BBMM, built on PyTorch.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 17:55:16 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:52:57 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 17:30:59 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 13:32:35 GMT"}, {"version": "v5", "created": "Fri, 11 Jan 2019 03:20:19 GMT"}, {"version": "v6", "created": "Tue, 29 Jun 2021 18:58:41 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gardner", "Jacob R.", ""], ["Pleiss", "Geoff", ""], ["Bindel", "David", ""], ["Weinberger", "Kilian Q.", ""], ["Wilson", "Andrew Gordon", ""]]}]