[{"id": "1408.0043", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning From Ordered Sets and Applications in Collaborative Ranking", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012, Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking over sets arise when users choose between groups of items. For\nexample, a group may be of those movies deemed $5$ stars to them, or a\ncustomized tour package. It turns out, to model this data type properly, we\nneed to investigate the general combinatorics problem of partitioning a set and\nordering the subsets. Here we construct a probabilistic log-linear model over a\nset of ordered subsets. Inference in this combinatorial space is highly\nchallenging: The space size approaches $(N!/2)6.93145^{N+1}$ as $N$ approaches\ninfinity. We propose a \\texttt{split-and-merge} Metropolis-Hastings procedure\nthat can explore the state-space efficiently. For discovering hidden aspects in\nthe data, we enrich the model with latent binary variables so that the\nposteriors can be efficiently evaluated. Finally, we evaluate the proposed\nmodel on large-scale collaborative filtering tasks and demonstrate that it is\ncompetitive against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:30:37 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0047", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data\n  Analysis", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012; Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal data is omnipresent in almost all multiuser-generated feedback -\nquestionnaires, preferences etc. This paper investigates modelling of ordinal\ndata with Gaussian restricted Boltzmann machines (RBMs). In particular, we\npresent the model architecture, learning and inference procedures for both\nvector-variate and matrix-variate ordinal data. We show that our model is able\nto capture latent opinion profile of citizens around the world, and is\ncompetitive against state-of-art collaborative filtering techniques on\nlarge-scale public datasets. The model thus has the potential to extend\napplication of RBMs to diverse domains such as recommendation systems, product\nreviews and expert assessments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:54:16 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0055", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "comments": "Proceedings of the 30 th International Conference on Machine\n  Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture\nthat can naturally incorporate a wide range of data inputs at the same time.\nOur motivation rests in the Thurstonian view that many discrete data types can\nbe considered as being generated from a subset of underlying latent continuous\nvariables, and in the observation that each realisation of a discrete type\nimposes certain inequalities on those variables. Thus learning and inference in\nTBM reduce to making sense of a set of inequalities. Our proposed TBM naturally\nsupports the following types: Gaussian, intervals, censored, binary,\ncategorical, muticategorical, ordinal, (in)-complete rank with and without\nties. We demonstrate the versatility and capacity of the proposed model on\nthree applications of very different natures; namely handwritten digit\nrecognition, collaborative filtering and complex social survey analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 00:32:32 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0096", "submitter": "Jiankou Li", "authors": "Jiankou Li and Wei Zhang", "title": "Conditional Restricted Boltzmann Machines for Cold Start Recommendations", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzman Machines (RBMs) have been successfully used in\nrecommender systems. However, as with most of other collaborative filtering\ntechniques, it cannot solve cold start problems for there is no rating for a\nnew item. In this paper, we first apply conditional RBM (CRBM) which could take\nextra information into account and show that CRBM could solve cold start\nproblem very well, especially for rating prediction task. CRBM naturally\ncombine the content and collaborative data under a single framework which could\nbe fitted effectively. Experiments show that CRBM can be compared favourably\nwith matrix factorization models, while hidden features learned from the former\nmodels are more easy to be interpreted.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 07:51:37 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Li", "Jiankou", ""], ["Zhang", "Wei", ""]]}, {"id": "1408.0145", "submitter": "Tianwen Wei", "authors": "Tianwen Wei", "title": "A convergence and asymptotic analysis of the generalized symmetric\n  FastICA algorithm", "comments": "Final version of the paper", "journal-ref": "IEEE Transactions on Signal Processing (December, 2015) 63, 24,\n  6445-6458", "doi": "10.1109/TSP.2015.2468686", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution deals with the generalized symmetric FastICA algorithm in\nthe domain of Independent Component Analysis (ICA). The generalized symmetric\nversion of FastICA has been shown to have the potential to achieve the\nCram\\'er-Rao Bound (CRB) by allowing the usage of different nonlinearity\nfunctions in its parallel implementations of one-unit FastICA. In spite of this\nappealing property, a rigorous study of the asymptotic error of the generalized\nsymmetric FastICA algorithm is still missing in the community. In fact, all the\nexisting results exhibit certain limitations, such as ignoring the impact of\ndata standardization on the asymptotic statistics or being based on a heuristic\napproach.\n  In this work, we aim at filling this blank.\n  The first result of this contribution is the characterization of the limits\nof the generalized symmetric FastICA. It is shown that the algorithm optimizes\na function that is a sum of the contrast functions used by traditional one-unit\nFastICA with a correction of the sign. Based on this characterization, we\nderive a closed-form analytic expression of the asymptotic covariance matrix of\nthe generalized symmetric FastICA estimator using the method of estimating\nequation and M-estimator.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 12:07:53 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 03:16:04 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Wei", "Tianwen", ""]]}, {"id": "1408.0204", "submitter": "Momiao Xiong", "authors": "Nan Lin, Junhai Jiang, Shicheng Guo and Momiao Xiong", "title": "Functional Principal Component Analysis and Randomized Sparse Clustering\n  Algorithm for Medical Image Analysis", "comments": "35 pages, 2 figures, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0132945", "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to advances in sensors, growing large and complex medical image data have\nthe ability to visualize the pathological change in the cellular or even the\nmolecular level or anatomical changes in tissues and organs. As a consequence,\nthe medical images have the potential to enhance diagnosis of disease,\nprediction of clinical outcomes, characterization of disease progression,\nmanagement of health care and development of treatments, but also pose great\nmethodological and computational challenges for representation and selection of\nfeatures in image cluster analysis. To address these challenges, we first\nextend one dimensional functional principal component analysis to the two\ndimensional functional principle component analyses (2DFPCA) to fully capture\nspace variation of image signals. Image signals contain a large number of\nredundant and irrelevant features which provide no additional or no useful\ninformation for cluster analysis. Widely used methods for removing redundant\nand irrelevant features are sparse clustering algorithms using a lasso-type\npenalty to select the features. However, the accuracy of clustering using a\nlasso-type penalty depends on how to select penalty parameters and a threshold\nfor selecting features. In practice, they are difficult to determine. Recently,\nrandomized algorithms have received a great deal of attention in big data\nanalysis. This paper presents a randomized algorithm for accurate feature\nselection in image cluster analysis. The proposed method is applied to ovarian\nand kidney cancer histology image data from the TCGA database. The results\ndemonstrate that the randomized feature selection method coupled with\nfunctional principal component analysis substantially outperforms the current\nsparse clustering algorithms in image cluster analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 15:15:48 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lin", "Nan", ""], ["Jiang", "Junhai", ""], ["Guo", "Shicheng", ""], ["Xiong", "Momiao", ""]]}, {"id": "1408.0337", "submitter": "Naoki Tanaka", "authors": "Naoki Tanaka, Shohei Shimizu and Takashi Washio", "title": "A Bayesian estimation approach to analyze non-Gaussian data-generating\n  processes with latent classes", "comments": "10 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of observational data has been accumulated in various fields\nin recent times, and there is a growing need to estimate the generating\nprocesses of these data. A linear non-Gaussian acyclic model (LiNGAM) based on\nthe non-Gaussianity of external influences has been proposed to estimate the\ndata-generating processes of variables. However, the results of the estimation\ncan be biased if there are latent classes. In this paper, we first review\nLiNGAM, its extended model, as well as the estimation procedure for LiNGAM in a\nBayesian framework. We then propose a new Bayesian estimation procedure that\nsolves the problem.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 04:31:56 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Tanaka", "Naoki", ""], ["Shimizu", "Shohei", ""], ["Washio", "Takashi", ""]]}, {"id": "1408.0553", "submitter": "Majid Janzamin", "authors": "Animashree Anandkumar and Rong Ge and Majid Janzamin", "title": "Sample Complexity Analysis for Learning Overcomplete Latent Variable\n  Models through Tensor Methods", "comments": "Title changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide guarantees for learning latent variable models emphasizing on the\novercomplete regime, where the dimensionality of the latent space can exceed\nthe observed dimensionality. In particular, we consider multiview mixtures,\nspherical Gaussian mixtures, ICA, and sparse coding models. We provide tight\nconcentration bounds for empirical moments through novel covering arguments. We\nanalyze parameter recovery through a simple tensor power update algorithm. In\nthe semi-supervised setting, we exploit the label or prior information to get a\nrough estimate of the model parameters, and then refine it using the tensor\nmethod on unlabeled samples. We establish that learning is possible when the\nnumber of components scales as $k=o(d^{p/2})$, where $d$ is the observed\ndimension, and $p$ is the order of the observed moment employed in the tensor\nmethod. Our concentration bound analysis also leads to minimax sample\ncomplexity for semi-supervised learning of spherical Gaussian mixtures. In the\nunsupervised setting, we use a simple initialization algorithm based on SVD of\nthe tensor slices, and provide guarantees under the stricter condition that\n$k\\le \\beta d$ (where constant $\\beta$ can be larger than $1$), where the\ntensor method recovers the components under a polynomial running time (and\nexponential in $\\beta$). Our analysis establishes that a wide range of\novercomplete latent variable models can be learned efficiently with low\ncomputational and sample complexity through tensor decomposition methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 23:21:33 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 22:21:23 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1408.0838", "submitter": "Bjoern Andres", "authors": "Lizhen Qu and Bjoern Andres", "title": "Estimating Maximally Probable Constrained Relations by Mathematical\n  Programming", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a constrained relation is a fundamental problem in machine\nlearning. Special cases are classification (the problem of estimating a map\nfrom a set of to-be-classified elements to a set of labels), clustering (the\nproblem of estimating an equivalence relation on a set) and ranking (the\nproblem of estimating a linear order on a set). We contribute a family of\nprobability measures on the set of all relations between two finite, non-empty\nsets, which offers a joint abstraction of multi-label classification,\ncorrelation clustering and ranking by linear ordering. Estimating (learning) a\nmaximally probable measure, given (a training set of) related and unrelated\npairs, is a convex optimization problem. Estimating (inferring) a maximally\nprobable relation, given a measure, is a 01-linear program. It is solved in\nlinear time for maps. It is NP-hard for equivalence relations and linear\norders. Practical solutions for all three cases are shown in experiments with\nreal data. Finally, estimating a maximally probable measure and relation\njointly is posed as a mixed-integer nonlinear program. This formulation\nsuggests a mathematical programming approach to semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 23:30:20 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Qu", "Lizhen", ""], ["Andres", "Bjoern", ""]]}, {"id": "1408.0848", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Multilayer bootstrap networks", "comments": "accepted for publication by Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear\nnetwork from bottom up for unsupervised nonlinear dimensionality reduction.\nEach layer of the network is a nonparametric density estimator. It consists of\na group of k-centroids clusterings. Each clustering randomly selects data\npoints with randomly selected features as its centroids, and learns a one-hot\nencoder by one-nearest-neighbor optimization. Geometrically, the nonparametric\ndensity estimator at each layer projects the input data space to a\nuniformly-distributed discrete feature space, where the similarity of two data\npoints in the discrete feature space is measured by the number of the nearest\ncentroids they share in common. The multilayer network gradually reduces the\nnonlinear variations of data from bottom up by building a vast number of\nhierarchical trees implicitly on the original data space. Theoretically, the\nestimation error caused by the nonparametric density estimator is proportional\nto the correlation between the clusterings, both of which are reduced by the\nrandomization steps.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 02:13:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:26:21 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 03:11:40 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 09:09:46 GMT"}, {"version": "v5", "created": "Mon, 4 Jan 2016 17:50:27 GMT"}, {"version": "v6", "created": "Thu, 7 Jan 2016 08:07:44 GMT"}, {"version": "v7", "created": "Mon, 6 Jun 2016 18:00:32 GMT"}, {"version": "v8", "created": "Tue, 6 Mar 2018 15:59:10 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1408.0850", "submitter": "Goran Marjanovic", "authors": "Goran Marjanovic and Alfred O. Hero III", "title": "L0 Sparse Inverse Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been focus on penalized log-likelihood covariance\nestimation for sparse inverse covariance (precision) matrices. The penalty is\nresponsible for inducing sparsity, and a very common choice is the convex $l_1$\nnorm. However, the best estimator performance is not always achieved with this\npenalty. The most natural sparsity promoting \"norm\" is the non-convex $l_0$\npenalty but its lack of convexity has deterred its use in sparse maximum\nlikelihood estimation. In this paper we consider non-convex $l_0$ penalized\nlog-likelihood inverse covariance estimation and present a novel cyclic descent\nalgorithm for its optimization. Convergence to a local minimizer is proved,\nwhich is highly non-trivial, and we demonstrate via simulations the reduced\nbias and superior quality of the $l_0$ penalty as compared to the $l_1$\npenalty.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 02:15:46 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 03:56:31 GMT"}, {"version": "v3", "created": "Thu, 7 Aug 2014 02:27:02 GMT"}, {"version": "v4", "created": "Sun, 8 Mar 2015 10:43:38 GMT"}, {"version": "v5", "created": "Thu, 19 Mar 2015 02:52:03 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Marjanovic", "Goran", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1408.0853", "submitter": "Masahiro Yukawa", "authors": "Masahiro Yukawa", "title": "Adaptive Learning in Cartesian Product of Reproducing Kernel Hilbert\n  Spaces", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2463261", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive learning algorithm based on iterative orthogonal\nprojections in the Cartesian product of multiple reproducing kernel Hilbert\nspaces (RKHSs). The task is estimating/tracking nonlinear functions which are\nsupposed to contain multiple components such as (i) linear and nonlinear\ncomponents, (ii) high- and low- frequency components etc. In this case, the use\nof multiple RKHSs permits a compact representation of multicomponent functions.\nThe proposed algorithm is where two different methods of the author meet:\nmultikernel adaptive filtering and the algorithm of hyperplane projection along\naffine subspace (HYPASS). In a certain particular case, the sum space of the\nRKHSs is isomorphic to the product space and hence the proposed algorithm can\nalso be regarded as an iterative projection method in the sum space. The\nefficacy of the proposed algorithm is shown by numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 02:31:27 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 04:53:17 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Yukawa", "Masahiro", ""]]}, {"id": "1408.0856", "submitter": "Eric Chi", "authors": "Eric C. Chi, Genevera I. Allen, and Richard G. Baraniuk", "title": "Convex Biclustering", "comments": "29 pages, 3 figures", "journal-ref": "Biometrics 73 (1):10-19, 2017", "doi": "10.1111/biom.12540", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the biclustering problem, we seek to simultaneously group observations and\nfeatures. While biclustering has applications in a wide array of domains,\nranging from text mining to collaborative filtering, the problem of identifying\nstructure in high dimensional genomic data motivates this work. In this\ncontext, biclustering enables us to identify subsets of genes that are\nco-expressed only within a subset of experimental conditions. We present a\nconvex formulation of the biclustering problem that possesses a unique global\nminimizer and an iterative algorithm, COBRA, that is guaranteed to identify it.\nOur approach generates an entire solution path of possible biclusters as a\nsingle tuning parameter is varied. We also show how to reduce the problem of\nselecting this tuning parameter to solving a trivial modification of the convex\nbiclustering problem. The key contributions of our work are its simplicity,\ninterpretability, and algorithmic guarantees - features that arguably are\nlacking in the current alternative algorithms. We demonstrate the advantages of\nour approach, which includes stably and reproducibly identifying biclusterings,\non simulated and real microarray data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 03:27:17 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 21:39:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 00:09:49 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 19:57:55 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chi", "Eric C.", ""], ["Allen", "Genevera I.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1408.0881", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "Volumes of logistic regression models with applications to model\n  selection", "comments": "Improved the section on volume jumps and added a new volume bound\n  (Theorem 13) for models with generic design matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models with $n$ observations and $q$ linearly-independent\ncovariates are shown to have Fisher information volumes which are bounded below\nby $\\pi^q$ and above by ${n \\choose q} \\pi^q$. This is proved with a novel\ngeneralization of the classical theorems of Pythagoras and de Gua, which is of\nindependent interest. The finding that the volume is always finite is new, and\nit implies that the volume can be directly interpreted as a measure of model\ncomplexity. The volume is shown to be a continuous function of the design\nmatrix $X$ at generic $X$, but to be discontinuous in general. This means that\nmodels with sparse design matrices can be significantly less complex than\nnearby models, so the resulting model-selection criterion prefers sparse\nmodels. This is analogous to the way that $\\ell^1$-regularisation tends to\nprefer sparse model fits, though in our case this behaviour arises\nspontaneously from general principles. Lastly, an unusual topological duality\nis shown to exist between the ideal boundaries of the natural and expectation\nparameter spaces of logistic regression models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 07:29:02 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 06:37:46 GMT"}, {"version": "v3", "created": "Fri, 17 Oct 2014 01:56:04 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1408.0967", "submitter": "Shaina Race", "authors": "Shaina Race, Carl Meyer, Kevin Valakuzhy", "title": "Determining the Number of Clusters via Iterative Consensus Clustering", "comments": "Proceedings of the 2013 SIAM International Conference on Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611972832.11", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a cluster ensemble to determine the number of clusters, k, in a group\nof data. A consensus similarity matrix is formed from the ensemble using\nmultiple algorithms and several values for k. A random walk is induced on the\ngraph defined by the consensus matrix and the eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters. For\nnoisy or high-dimensional data, an iterative technique is presented to refine\nthis consensus matrix in way that encourages a block-diagonal form. It is shown\nthat the resulting consensus matrix is generally superior to existing\nsimilarity matrices for this type of spectral analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:40:03 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""], ["Valakuzhy", "Kevin", ""]]}, {"id": "1408.0972", "submitter": "Shaina Race Ph.D", "authors": "Shaina Race and Carl Meyer", "title": "A Flexible Iterative Framework for Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework for consensus clustering is presented which has the ability\nto determine both the number of clusters and a final solution using multiple\nalgorithms. A consensus similarity matrix is formed from an ensemble using\nmultiple algorithms and several values for k. A variety of dimension reduction\ntechniques and clustering algorithms are considered for analysis. For noisy or\nhigh-dimensional data, an iterative technique is presented to refine this\nconsensus matrix in way that encourages algorithms to agree upon a common\nsolution. We utilize the theory of nearly uncoupled Markov chains to determine\nthe number, k , of clusters in a dataset by considering a random walk on the\ngraph defined by the consensus matrix. The eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters.\nThis method succeeds at determining the number of clusters in many datasets\nwhere previous methods fail. On every considered dataset, our consensus method\nprovides a final result with accuracy well above the average of the individual\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:54:01 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""]]}, {"id": "1408.1054", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Jacek Tabor", "title": "Multithreshold Entropy Linear Classifier", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2015.03.007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear classifiers separate the data with a hyperplane. In this paper we\nfocus on the novel method of construction of multithreshold linear classifier,\nwhich separates the data with multiple parallel hyperplanes. Proposed model is\nbased on the information theory concepts -- namely Renyi's quadratic entropy\nand Cauchy-Schwarz divergence.\n  We begin with some general properties, including data scale invariance. Then\nwe prove that our method is a multithreshold large margin classifier, which\nshows the analogy to the SVM, while in the same time works with much broader\nclass of hypotheses. What is also interesting, proposed method is aimed at the\nmaximization of the balanced quality measure (such as Matthew's Correlation\nCoefficient) as opposed to very common maximization of the accuracy. This\nfeature comes directly from the optimization problem statement and is further\nconfirmed by the experiments on the UCI datasets.\n  It appears, that our Multithreshold Entropy Linear Classifier (MELC) obtaines\nsimilar or higher scores than the ones given by SVM on both synthetic and real\ndata. We show how proposed approach can be benefitial for the cheminformatics\nin the task of ligands activity prediction, where despite better classification\nresults, MELC gives some additional insight into the data structure (classes of\nunderrepresented chemical compunds).\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 18:01:29 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Tabor", "Jacek", ""]]}, {"id": "1408.1143", "submitter": "Louis-Francois Arsenault", "authors": "Louis-Fran\\c{c}ois Arsenault, Alejandro Lopez-Bezanilla, O. Anatole\n  von Lilienfeld, and Andrew J. Millis", "title": "Machine learning for many-body physics: The case of the Anderson\n  impurity model", "comments": "18 pages, 11 figures. Sections II. A and B have been modified and an\n  appendix was added", "journal-ref": "Phys. Rev. B 90, 155136 (2014)", "doi": "10.1103/PhysRevB.90.155136", "report-no": null, "categories": "cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods are applied to finding the Green's function of the\nAnderson impurity model, a basic model system of quantum many-body\ncondensed-matter physics. Different methods of parametrizing the Green's\nfunction are investigated; a representation in terms of Legendre polynomials is\nfound to be superior due to its limited number of coefficients and its\napplicability to state of the art methods of solution. The dependence of the\nerrors on the size of the training set is determined. The results indicate that\na machine learning approach to dynamical mean-field theory may be feasible.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 23:34:30 GMT"}, {"version": "v2", "created": "Sun, 2 Nov 2014 23:51:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Arsenault", "Louis-Fran\u00e7ois", ""], ["Lopez-Bezanilla", "Alejandro", ""], ["von Lilienfeld", "O. Anatole", ""], ["Millis", "Andrew J.", ""]]}, {"id": "1408.1160", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Mixed-Variate Restricted Boltzmann Machines", "comments": "Originally published in Proceedings of ACML'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern datasets are becoming heterogeneous. To this end, we present in this\npaper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling\nvariables of multiple types and modalities, including binary and continuous\nresponses, categorical options, multicategorical choices, ordinal assessment\nand category-ranked preferences. Dependency among variables is modeled using\nlatent binary variables, each of which can be interpreted as a particular\nhidden aspect of the data. The proposed model, similar to the standard RBMs,\nallows fast evaluation of the posterior for the latent variables. Hence, it is\nnaturally suitable for many common tasks including, but not limited to, (a) as\na pre-processing step to convert complex input data into a more convenient\nvectorial representation through the latent posteriors, thereby offering a\ndimensionality reduction capacity, (b) as a classifier supporting binary,\nmulticlass, multilabel, and label-ranking outputs, or a regression tool for\ncontinuous outputs and (c) as a data completion tool for multimodal and\nheterogeneous data. We evaluate the proposed model on a large-scale dataset\nusing the world opinion survey results on three tasks: feature extraction and\nvisualization, data completion and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 01:43:05 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1162", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui", "title": "MCMC for Hierarchical Semi-Markov Conditional Random Fields", "comments": "NIPS'09 Workshop on Deep Learning for Speech Recognition and Related\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep architecture such as hierarchical semi-Markov models is an important\nclass of models for nested sequential data. Current exact inference schemes\neither cost cubic time in sequence length, or exponential time in model depth.\nThese costs are prohibitive for large-scale problems with arbitrary length and\ndepth. In this contribution, we propose a new approximation technique that may\nhave the potential to achieve sub-cubic time complexity in length and linear\ntime depth, at the cost of some loss of quality. The idea is based on two\nwell-known methods: Gibbs sampling and Rao-Blackwellisation. We provide some\nsimulation-based evaluation of the quality of the RGBS with respect to run time\nand sequence length.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:04:43 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""], ["Bui", "Hung H.", ""]]}, {"id": "1408.1167", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Boosted Markov Networks for Activity Recognition", "comments": "International Conference on Intelligent Sensors, Sensor Networks and\n  Information Processing (ISSNIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a framework called boosted Markov networks to combine the learning\ncapacity of boosting and the rich modeling semantics of Markov networks and\napplying the framework for video-based activity recognition. Importantly, we\nextend the framework to incorporate hidden variables. We show how the framework\ncan be applied for both model learning and feature selection. We demonstrate\nthat boosted Markov networks with hidden variables perform comparably with the\nstandard maximum likelihood estimation. However, our framework is able to learn\nsparse models, and therefore can provide computational savings when the learned\nmodels are used for classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:45:51 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1182", "submitter": "Visar Berisha", "authors": "Visar Berisha and Alfred O. Hero", "title": "Empirical non-parametric estimation of the Fisher Information", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/LSP.2014.2378514", "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) is a foundational concept in statistical\nsignal processing. The FIM depends on the probability distribution, assumed to\nbelong to a smooth parametric family. Traditional approaches to estimating the\nFIM require estimating the probability distribution function (PDF), or its\nparameters, along with its gradient or Hessian. However, in many practical\nsituations the PDF of the data is not known but the statistician has access to\nan observation sample for any parameter value. Here we propose a method of\nestimating the FIM directly from sampled data that does not require knowledge\nof the underlying PDF. The method is based on non-parametric estimation of an\n$f$-divergence over a local neighborhood of the parameter space and a relation\nbetween curvature of the $f$-divergence and the FIM. Thus we obtain an\nempirical estimator of the FIM that does not require density estimation and is\nasymptotically consistent. We empirically evaluate the validity of our approach\nusing two experiments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 04:35:58 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 22:32:29 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Berisha", "Visar", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1408.1187", "submitter": "Mattia Ciollaro", "authors": "Mattia Ciollaro, Christopher Genovese, Jing Lei and Larry Wasserman", "title": "The functional mean-shift algorithm for mode hunting and clustering in\n  infinite dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functional mean-shift algorithm, an iterative algorithm for\nestimating the local modes of a surrogate density from functional data. We show\nthat the algorithm can be used for cluster analysis of functional data. We\npropose a test based on the bootstrap for the significance of the estimated\nlocal modes of the surrogate density. We present two applications of our\nmethodology. In the first application, we demonstrate how the functional\nmean-shift algorithm can be used to perform spike sorting, i.e. cluster neural\nactivity curves. In the second application, we use the functional mean-shift\nalgorithm to distinguish between original and fake signatures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 05:12:04 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Genovese", "Christopher", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1408.1319", "submitter": "Lewis Evans Mr", "authors": "Lewis Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "When does Active Learning Work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning (AL) methods seek to improve classifier performance when\nlabels are expensive or scarce. We consider two central questions: Where does\nAL work? How much does it help? To address these questions, a comprehensive\nexperimental simulation study of Active Learning is presented. We consider a\nvariety of tasks, classifiers and other AL factors, to present a broad\nexploration of AL performance in various settings. A precise way to quantify\nperformance is needed in order to know when AL works. Thus we also present a\ndetailed methodology for tackling the complexities of assessing AL performance\nin the context of this experimental study.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 15:27:20 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Evans", "Lewis", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1408.1336", "submitter": "Emilie Morvant", "authors": "Fran\\c{c}ois Laviolette, Emilie Morvant (LHC), Liva Ralaivola,\n  Jean-Francis Roy", "title": "On the Generalization of the C-Bound to Structured Output Ensemble\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes an important result from the PAC-Bayesian literature\nfor binary classification to the case of ensemble methods for structured\noutputs. We prove a generic version of the \\Cbound, an upper bound over the\nrisk of models expressed as a weighted majority vote that is based on the first\nand second statistical moments of the vote's margin. This bound may\nadvantageously $(i)$ be applied on more complex outputs such as multiclass\nlabels and multilabel, and $(ii)$ allow to consider margin relaxations. These\nresults open the way to develop new ensemble methods for structured output\nprediction with PAC-Bayesian guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 15:50:30 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 09:54:15 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Laviolette", "Fran\u00e7ois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"], ["Ralaivola", "Liva", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1408.1381", "submitter": "Jos\\'{e} E. Chac\\'{o}n", "authors": "Jos\\'e E. Chac\\'on", "title": "A Population Background for Nonparametric Density-Based Clustering", "comments": "Published at http://dx.doi.org/10.1214/15-STS526 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: substantial text\n  overlap with arXiv:1212.1384", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 518-532", "doi": "10.1214/15-STS526", "report-no": "IMS-STS-STS526", "categories": "math.ST math.CA math.DG math.GT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its popularity, it is widely recognized that the investigation of\nsome theoretical aspects of clustering has been relatively sparse. One of the\nmain reasons for this lack of theoretical results is surely the fact that,\nwhereas for other statistical problems the theoretical population goal is\nclearly defined (as in regression or classification), for some of the\nclustering methodologies it is difficult to specify the population goal to\nwhich the data-based clustering algorithms should try to get close. This paper\naims to provide some insight into the theoretical foundations of clustering by\nfocusing on two main objectives: to provide an explicit formulation for the\nideal population goal of the modal clustering methodology, which understands\nclusters as regions of high density; and to present two new loss functions,\napplicable in fact to any clustering methodology, to evaluate the performance\nof a data-based clustering algorithm with respect to the ideal population goal.\nIn particular, it is shown that only mild conditions on a sequence of density\nestimators are needed to ensure that the sequence of modal clusterings that\nthey induce is consistent.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 19:15:09 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 13:07:27 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1408.1717", "submitter": "Vassilis Kalofolias", "authors": "Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre\n  Vandergheynst", "title": "Matrix Completion on Graphs", "comments": "Version of NIPS 2014 workshop \"Out of the Box: Robustness in High\n  Dimension\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the missing values of a matrix given a few of its\nentries, called matrix completion, has gathered a lot of attention in the\nrecent years. Although the problem under the standard low rank assumption is\nNP-hard, Cand\\`es and Recht showed that it can be exactly relaxed if the number\nof observed entries is sufficiently large. In this work, we introduce a novel\nmatrix completion model that makes use of proximity information about rows and\ncolumns by assuming they form communities. This assumption makes sense in\nseveral real-world problems like in recommender systems, where there are\ncommunities of people sharing preferences, while products form clusters that\nreceive similar ratings. Our main goal is thus to find a low-rank solution that\nis structured by the proximities of rows and columns encoded by graphs. We\nborrow ideas from manifold learning to constrain our solution to be smooth on\nthese graphs, in order to implicitly force row and column proximities. Our\nmatrix recovery model is formulated as a convex non-smooth optimization\nproblem, for which a well-posed iterative scheme is provided. We study and\nevaluate the proposed matrix completion on synthetic and real data, showing\nthat the proposed structured low-rank recovery model outperforms the standard\nmatrix completion model in many situations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 21:33:51 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 17:00:14 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 11:12:27 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Bronstein", "Michael", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1408.2003", "submitter": "Bo Han", "authors": "Bo Han, Bo He, Rui Nian, Mengmeng Ma, Shujing Zhang, Minghui Li and\n  Amaury Lendasse", "title": "LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS\n  for Blended Data", "comments": "Accepted for publication in Neurocomputing, 01/19/2014", "journal-ref": "Neurocomputing, 2014, Elsevier. Manuscript ID: NEUCOM-D-13-01029", "doi": "10.1016/j.neucom.2014.01.069", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) as a neural network algorithm has shown its\ngood performance, such as fast speed, simple structure etc, but also, weak\nrobustness is an unavoidable defect in original ELM for blended data. We\npresent a new machine learning framework called LARSEN-ELM for overcoming this\nproblem. In our paper, we would like to show two key steps in LARSEN-ELM. In\nthe first step, preprocessing, we select the input variables highly related to\nthe output using least angle regression (LARS). In the second step, training,\nwe employ Genetic Algorithm (GA) based selective ensemble and original ELM. In\nthe experiments, we apply a sum of two sines and four datasets from UCI\nrepository to verify the robustness of our approach. The experimental results\nshow that compared with original ELM and other methods such as OP-ELM,\nGASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performance\nwhile keeping a relatively high speed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 01:31:02 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 02:54:54 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Han", "Bo", ""], ["He", "Bo", ""], ["Nian", "Rui", ""], ["Ma", "Mengmeng", ""], ["Zhang", "Shujing", ""], ["Li", "Minghui", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1408.2025", "submitter": "Cosma Shalizi", "authors": "Cosma Shalizi, Kristina Lisa Klinkner", "title": "Blind Construction of Optimal Nonlinear Recursive Predictors for\n  Discrete Sequences", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-504-511", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for nonlinear prediction of discrete random sequences\nunder minimal structural assumptions. We give a mathematical construction for\noptimal predictors of such processes, in the form of hidden Markov models. We\nthen describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which\napproximates the ideal predictor from data. We discuss the reliability of CSSR,\nits data requirements, and its performance in simulations. Finally, we compare\nour approach to existing methods using variablelength Markov models and\ncross-validated hidden Markov models, and show theoretically and experimentally\nthat our method delivers results superior to the former and at least comparable\nto the latter.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:18:20 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Shalizi", "Cosma", ""], ["Klinkner", "Kristina Lisa", ""]]}, {"id": "1408.2031", "submitter": "Alina Beygelzimer", "authors": "Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin,\n  Alexander L. Strehl", "title": "Conditional Probability Tree Estimation Analysis and Algorithms", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-51-58", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the conditional probability of a label\nin time O(log n), where n is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly 106 labels.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:25:07 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Lifshits", "Yuri", ""], ["Sorkin", "Gregory", ""], ["Strehl", "Alexander L.", ""]]}, {"id": "1408.2032", "submitter": "Hal Daume III", "authors": "Hal Daume III", "title": "Bayesian Multitask Learning with Latent Hierarchies", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-135-142", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:26:02 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Daume", "Hal", "III"]]}, {"id": "1408.2033", "submitter": "Michael A. Finegold", "authors": "Michael A. Finegold, Mathias Drton", "title": "Robust Graphical Modeling with t-Distributions", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-169-176", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models have proven to be useful tools for exploring\nnetwork structures based on multivariate data. Applications to studies of gene\nexpression have generated substantial interest in these models, and resulting\nrecent progress includes the development of fitting methodology involving\npenalization of the likelihood function. In this paper we advocate the use of\nthe multivariate t and related distributions for more robust inference of\ngraphs. In particular, we demonstrate that penalized likelihood inference\ncombined with an application of the EM algorithm provides a simple and\ncomputationally efficient approach to model selection in the t-distribution\ncase.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:26:59 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Finegold", "Michael A.", ""], ["Drton", "Mathias", ""]]}, {"id": "1408.2036", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Characterizing predictable classes of processes", "comments": "This is a duplicate submission of 0905.4341, made by UAI foundation\n  who had the brilliant idea of flooding arxiv with UAI papers 5 years after\n  the conference, without checking whether these papers were already submitted\n  to arxiv or at least asking the authors. Great job, UAI! The journal\n  (extended) version appears in JMLR, 11: 581-602, 2010; also as\n  arXiv:0912.4883", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-471-478", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is sequence prediction in the following setting. A sequence\nx1,..., xn,... of discrete-valued observations is generated according to some\nunknown probabilistic law (measure) mu. After observing each outcome, it is\nrequired to give the conditional probabilities of the next observation. The\nmeasure mu belongs to an arbitrary class C of stochastic processes. We are\ninterested in predictors ? whose conditional probabilities converge to the\n'true' mu-conditional probabilities if any mu { C is chosen to generate the\ndata. We show that if such a predictor exists, then a predictor can also be\nobtained as a convex combination of a countably many elements of C. In other\nwords, it can be obtained as a Bayesian predictor whose prior is concentrated\non a countable set. This result is established for two very different measures\nof performance of prediction, one of which is very strong, namely, total\nvariation, and the other is very weak, namely, prediction in expected average\nKullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:32:03 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 16:08:12 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1408.2037", "submitter": "Issei Sato", "authors": "Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, Seiji\n  Miyashita", "title": "Quantum Annealing for Variational Bayes Inference", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-479-486", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents studies on a deterministic annealing algorithm based on\nquantum annealing for variational Bayes (QAVB) inference, which can be seen as\nan extension of the simulated annealing for variational Bayes (SAVB) inference.\nQAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better\nlocal optimum than SAVB in terms of the variational free energy in latent\nDirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:33:21 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Sato", "Issei", ""], ["Kurihara", "Kenichi", ""], ["Tanaka", "Shu", ""], ["Nakagawa", "Hiroshi", ""], ["Miyashita", "Seiji", ""]]}, {"id": "1408.2038", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara", "title": "A direct method for estimating a causal ordering in a linear\n  non-Gaussian acyclic model", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-506-513", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models and Bayesian networks have been widely used to\nanalyze causal relations between continuous variables. In such frameworks,\nlinear acyclic models are typically used to model the datagenerating process of\nvariables. Recently, it was shown that use of non-Gaussianity identifies a\ncausal ordering of variables in a linear acyclic model without using any prior\nknowledge on the network structure, which is not the case with conventional\nmethods. However, existing estimation methods are based on iterative search\nalgorithms and may not converge to a correct solution in a finite number of\nsteps. In this paper, we propose a new direct method to estimate a causal\nordering based on non-Gaussianity. In contrast to the previous methods, our\nalgorithm requires no algorithmic parameters and is guaranteed to converge to\nthe right solution within a small fixed number of steps if the data strictly\nfollows the model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:34:21 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1408.2039", "submitter": "Ryan Prescott Adams", "authors": "Ryan Prescott Adams, George E. Dahl, Iain Murray", "title": "Incorporating Side Information in Probabilistic Matrix Factorization\n  with Gaussian Processes", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-1-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic matrix factorization (PMF) is a powerful method for modeling\ndata associ- ated with pairwise relationships, Finding use in collaborative\nFiltering, computational bi- ology, and document analysis, among other areas.\nIn many domains, there are additional covariates that can assist in prediction.\nFor example, when modeling movie ratings, we might know when the rating\noccurred, where the user lives, or what actors appear in the movie. It is\ndifficult, however, to incorporate this side information into the PMF model. We\npropose a framework for incorporating side information by coupling together\nmulti- ple PMF problems via Gaussian process priors. We replace scalar latent\nfeatures with func- tions that vary over the covariate space. The GP priors on\nthese functions require them to vary smoothly and share information. We apply\nthis new method to predict the scores of professional basketball games, where\nside information about the venue and date of the game are relevant for the\noutcome.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:35:48 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Dahl", "George E.", ""], ["Murray", "Iain", ""]]}, {"id": "1408.2040", "submitter": "Alexey Chernov", "authors": "Alexey Chernov, Vladimir Vovk", "title": "Prediction with Advice of Unknown Number of Experts", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-117-125", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of prediction with expert advice, we consider a recently\nintroduced kind of regret bounds: the bounds that depend on the effective\ninstead of nominal number of experts. In contrast to the Normal- Hedge bound,\nwhich mainly depends on the effective number of experts but also weakly depends\non the nominal one, we obtain a bound that does not contain the nominal number\nof experts at all. We use the defensive forecasting method and introduce an\napplication of defensive forecasting to multivalued supermartingales.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:36:41 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chernov", "Alexey", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1408.2042", "submitter": "Ricardo Silva", "authors": "Ricardo Silva, Robert B. Gramacy", "title": "Gaussian Process Structural Equation Models with Latent Variables", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-537-545", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of disciplines such as social sciences, psychology, medicine and\neconomics, the recorded data are considered to be noisy measurements of latent\nvariables connected by some causal structure. This corresponds to a family of\ngraphical models known as the structural equation model with latent variables.\nWhile linear non-Gaussian variants have been well-studied, inference in\nnonparametric structural equation models is still underdeveloped. We introduce\na sparse Gaussian process parameterization that defines a non-linear structure\nconnecting latent variables, unlike common formulations of Gaussian process\nlatent variable models. The sparse parameterization is given a full Bayesian\ntreatment without compromising Markov chain Monte Carlo efficiency. We compare\nthe stability of the sampling procedure and the predictive ability of the model\nagainst the current practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:39:50 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Silva", "Ricardo", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1408.2044", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Afshin Rostamizadeh", "title": "Matrix Coherence and the Nystrom Method", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-572-579", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method is an efficient technique used to speed up large-scale\nlearning applications by generating low-rank approximations. Crucial to the\nperformance of this technique is the assumption that a matrix can be well\napproximated by working exclusively with a subset of its columns. In this work\nwe relate this assumption to the concept of matrix coherence, connecting\ncoherence to the performance of the Nystrom method. Making use of related work\nin the compressed sensing and the matrix completion literature, we derive novel\ncoherence-based bounds for the Nystrom method in the low-rank setting. We then\npresent empirical results that corroborate these theoretical bounds. Finally,\nwe present more general empirical results for the full-rank setting that\nconvincingly demonstrate the ability of matrix coherence to measure the degree\nto which information can be extracted from a subset of columns.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:40:28 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1408.2047", "submitter": "Yutian Chen", "authors": "Yutian Chen, Max Welling", "title": "Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-174-184", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncrossvalidation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:45:11 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1408.2049", "submitter": "David Duvenaud", "authors": "Ferenc Huszar, David Duvenaud", "title": "Optimally-Weighted Herding is Bayesian Quadrature", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012). This copy was withdrawn since it's a\n  duplicate of arXiv:1204.1664", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-377-386", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herding and kernel herding are deterministic methods of choosing samples\nwhich summarise a probability distribution. A related task is choosing samples\nfor estimating integrals using Bayesian quadrature. We show that the criterion\nminimised when selecting samples in kernel herding is equivalent to the\nposterior variance in Bayesian quadrature. We then show that sequential\nBayesian quadrature can be viewed as a weighted version of kernel herding which\nachieves performance superior to any other weighted herding method. We\ndemonstrate empirically a rate of convergence faster than O(1/N). Our results\nalso imply an upper bound on the empirical error of the Bayesian quadrature\nestimate.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:47:25 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 21:01:52 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Huszar", "Ferenc", ""], ["Duvenaud", "David", ""]]}, {"id": "1408.2051", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Jeff A. Bilmes", "title": "Algorithms for Approximate Minimization of the Difference Between\n  Submodular Functions, with Applications", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-407-417", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the work of Narasimhan and Bilmes [30] for minimizing set functions\nrepresentable as a dierence between submodular functions. Similar to [30], our\nnew algorithms are guaranteed to monotonically reduce the objective function at\nevery step. We empirically and theoretically show that the per-iteration cost\nof our algorithms is much less than [30], and our algorithms can be used to\nefficiently minimize a dierence between submodular functions under various\ncombinatorial constraints, a problem not previously addressed. We provide\ncomputational bounds and a hardness result on the multiplicative\ninapproximability of minimizing the dierence between submodular functions. We\nshow, however, that it is possible to give worst-case additive bounds by\nproviding a polynomial time computable lower-bound on the minima. Finally we\nshow how a number of machine learning problems can be modeled as minimizing the\ndierence between submodular functions. We experimentally show the validity of\nour algorithms by testing them on the problem of feature selection with\nsubmodular cost features.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:48:31 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1408.2054", "submitter": "David Wipf", "authors": "David Wipf", "title": "Non-Convex Rank Minimization via an Empirical Bayesian Approach", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-914-923", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications that require matrix solutions of minimal rank, the\nunderlying cost function is non-convex leading to an intractable, NP-hard\noptimization problem. Consequently, the convex nuclear norm is frequently used\nas a surrogate penalty term for matrix rank. The problem is that in many\npractical scenarios there is no longer any guarantee that we can correctly\nestimate generative low-rank matrices of interest, theoretical special cases\nnotwithstanding. Consequently, this paper proposes an alternative empirical\nBayesian procedure build upon a variational approximation that, unlike the\nnuclear norm, retains the same globally minimizing point estimate as the rank\nfunction under many useful constraints. However, locally minimizing solutions\nare largely smoothed away via marginalization, allowing the algorithm to\nsucceed when standard convex relaxations completely fail. While the proposed\nmethodology is generally applicable to a wide range of low-rank applications,\nwe focus our attention on the robust principal component analysis problem\n(RPCA), which involves estimating an unknown low-rank matrix with unknown\nsparse corruptions. Theoretical and empirical evidence are presented to show\nthat our method is potentially superior to related MAP-based approaches, for\nwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,\n2011) can be viewed as a special case.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:52:02 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Wipf", "David", ""]]}, {"id": "1408.2055", "submitter": "Amy Zhang", "authors": "Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari", "title": "Guess Who Rated This Movie: Identifying Users Through Subspace\n  Clustering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-944-953", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often the case that, within an online recommender system, multiple\nusers share a common account. Can such shared accounts be identified solely on\nthe basis of the userprovided ratings? Once a shared account is identified, can\nthe different users sharing it be identified as well? Whenever such user\nidentification is feasible, it opens the way to possible improvements in\npersonalized recommendations, but also raises privacy concerns. We develop a\nmodel for composite accounts based on unions of linear subspaces, and use\nsubspace clustering for carrying out the identification task. We show that a\nsignificant fraction of such accounts is identifiable in a reliable manner, and\nillustrate potential uses for personalized recommendation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:54:49 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Zhang", "Amy", ""], ["Fawaz", "Nadia", ""], ["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""]]}, {"id": "1408.2060", "submitter": "Jie Chen", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-152-161", "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:58:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1408.2061", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani", "title": "Warped Mixtures for Nonparametric Cluster Shapes", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-311-320", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of Gaussians fit to a single curved or heavy-tailed cluster will\nreport that the data contains many clusters. To produce more appropriate\nclusterings, we introduce a model which warps a latent mixture of Gaussians to\nproduce nonparametric cluster shapes. The possibly low-dimensional latent\nmixture model allows us to summarize the properties of the high-dimensional\nclusters (or density manifolds) describing the data. The number of manifolds,\nas well as the shape and dimension of each manifold is automatically inferred.\nWe derive a simple inference scheme for this model which analytically\nintegrates out both the mixture parameters and the warping function. We show\nthat our model is effective for density estimation, performs better than\ninfinite Gaussian mixture models at recovering the true number of clusters, and\nproduces interpretable summaries of high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:00:05 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Duvenaud", "David", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1408.2062", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Jeff A. Bilmes", "title": "The Lovasz-Bregman Divergence and connections to rank aggregation,\n  clustering, and web ranking", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-321-330", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the recently introduced theory of Lovasz-Bregman (LB) divergences\n(Iyer & Bilmes 2012) in several ways. We show that they represent a distortion\nbetween a \"score\" and an \"ordering\", thus providing a new view of rank\naggregation and order based clustering with interesting connections to web\nranking. We show how the LB divergences have a number of properties akin to\nmany permutation based metrics, and in fact have as special cases forms very\nsimilar to the Kendall-tau metric. We also show how the LB divergences subsume\na number of commonly used ranking measures in information retrieval, like NDCG\nand AUC. Unlike the traditional permutation based metrics, however, the LB\ndivergence naturally captures a notion of \"confidence\" in the orderings, thus\nproviding a new representation to applications involving aggregating scores as\nopposed to just orderings. We show how a number of recently used web ranking\nmodels are forms of Lovasz-Bregman rank aggregation and also observe that a\nnatural form of Mallow's model using the LB divergence has been used as\nconditional ranking models for the \"Learning to Rank\" problem.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:01:37 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1408.2064", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Bernhard Schoelkopf", "title": "One-Class Support Measure Machines for Group Anomaly Detection", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-449-458", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose one-class support measure machines (OCSMMs) for group anomaly\ndetection which aims at recognizing anomalous aggregate behaviors of data\npoints. The OCSMMs generalize well-known one-class support vector machines\n(OCSVMs) to a space of probability measures. By formulating the problem as\nquantile estimation on distributions, we can establish an interesting\nconnection to the OCSVMs and variable kernel density estimators (VKDEs) over\nthe input space on which the distributions are defined, bridging the gap\nbetween large-margin methods and kernel density estimators. In particular, we\nshow that various types of VKDEs can be considered as solutions to a class of\nregularization problems studied in this paper. Experiments on Sloan Digital Sky\nSurvey dataset and High Energy Particle Physics dataset demonstrate the\nbenefits of the proposed framework in real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:04:33 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Muandet", "Krikamol", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1408.2065", "submitter": "Stephane Ross", "authors": "Stephane Ross, Paul Mineiro, John Langford", "title": "Normalized Online Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-537-545", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online learning algorithms which are independent of feature\nscales, proving regret bounds dependent on the ratio of scales existent in the\ndata rather than the absolute scale. This has several useful effects: there is\nno need to pre-normalize data, the test-time and test-space complexity are\nreduced, and the algorithms are more robust.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:05:51 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Ross", "Stephane", ""], ["Mineiro", "Paul", ""], ["Langford", "John", ""]]}, {"id": "1408.2066", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani, Ha Quang Minh, Aurelie Lozano", "title": "Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear\n  Multivariate Regression and Granger Causality", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-586-595", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general matrix-valued multiple kernel learning framework for\nhigh-dimensional nonlinear multivariate regression problems. This framework\nallows a broad class of mixed norm regularizers, including those that induce\nsparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel\nHilbert Spaces. We develop a highly scalable and eigendecomposition-free\nalgorithm that orchestrates two inexact solvers for simultaneously learning\nboth the input and output components of separable matrix-valued kernels. As a\nkey application enabled by our framework, we show how high-dimensional causal\ninference tasks can be naturally cast as sparse function estimation problems,\nleading to novel nonlinear extensions of a class of Graphical Granger Causality\ntechniques. Our algorithmic developments and extensive empirical studies are\ncomplemented by theoretical analyses in terms of Rademacher generalization\nbounds.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:06:49 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Minh", "Ha Quang", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1408.2067", "submitter": "Aristide Tossou", "authors": "Aristide Tossou, Christos Dimitrakakis", "title": "Probabilistic inverse reinforcement learning in unknown environments", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-635-643", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning by demonstration from agents acting in\nunknown stochastic Markov environments or games. Our aim is to estimate agent\npreferences in order to construct improved policies for the same task that the\nagents are trying to solve. To do so, we extend previous probabilistic\napproaches for inverse reinforcement learning in known MDPs to the case of\nunknown dynamics or opponents. We do this by deriving two simplified\nprobabilistic models of the demonstrator's policy and utility. For\ntractability, we use maximum a posteriori estimation rather than full Bayesian\ninference. Under a flat prior, this results in a convex optimisation problem.\nWe find that the resulting algorithms are highly competitive against a variety\nof other methods for inverse reinforcement learning that do have knowledge of\nthe dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 06:07:52 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Tossou", "Aristide", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1408.2156", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu", "title": "Statistical guarantees for the EM algorithm: From population to\n  sample-based analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for proving rigorous guarantees on the\nperformance of the EM algorithm and a variant known as gradient EM. Our\nanalysis is divided into two parts: a treatment of these algorithms at the\npopulation level (in the limit of infinite data), followed by results that\napply to updates based on a finite set of samples. First, we characterize the\ndomain of attraction of any global maximizer of the population likelihood. This\ncharacterization is based on a novel view of the EM updates as a perturbed form\nof likelihood ascent, or in parallel, of the gradient EM updates as a perturbed\nform of standard gradient ascent. Leveraging this characterization, we then\nprovide non-asymptotic guarantees on the EM and gradient EM algorithms when\napplied to a finite set of samples. We develop consequences of our general\ntheory for three canonical examples of incomplete-data problems: mixture of\nGaussians, mixture of regressions, and linear regression with covariates\nmissing completely at random. In each case, our theory guarantees that with a\nsuitable initialization, a relatively small number of EM (or gradient EM) steps\nwill yield (with high probability) an estimate that is within statistical error\nof the MLE. We provide simulations to confirm this theoretically predicted\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 21:40:15 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1408.2539", "submitter": "Constantinos Daskalakis", "authors": "Yang Cai, Constantinos Daskalakis, Christos H. Papadimitriou", "title": "Optimum Statistical Estimation with Strategic Data Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimum mechanism for providing monetary incentives to the data\nsources of a statistical estimator such as linear regression, so that high\nquality data is provided at low cost, in the sense that the sum of payments and\nestimation error is minimized. The mechanism applies to a broad range of\nestimators, including linear and polynomial regression, kernel regression, and,\nunder some additional assumptions, ridge regression. It also generalizes to\nseveral objectives, including minimizing estimation error subject to budget\nconstraints. Besides our concrete results for regression problems, we\ncontribute a mechanism design framework through which to design and analyze\nstatistical estimators whose examples are supplied by workers with cost for\nlabeling said examples.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 20:13:15 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 16:39:54 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""], ["Papadimitriou", "Christos H.", ""]]}, {"id": "1408.2552", "submitter": "Amit Gulab Deshwar", "authors": "Amit G. Deshwar, Shankar Vembu, Quaid Morris", "title": "Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction\n  of Tumors", "comments": "Preprint of an article submitted for consideration in the Pacific\n  Symposium on Biocomputing \\c{opyright} 2015; World Scientific Publishing Co.,\n  Singapore, 2015; http://psb.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning methods, especially nonparametric Bayesian\nmethods, have become increasingly popular to infer clonal population structure\nof tumors. Here we describe the treeCRP, an extension of the Chinese restaurant\nprocess (CRP), a popular construction used in nonparametric mixture models, to\ninfer the phylogeny and genotype of major subclonal lineages represented in the\npopulation of cancer cells. We also propose new split-merge updates tailored to\nthe subclonal reconstruction problem that improve the mixing time of Markov\nchains. In comparisons with the tree-structured stick breaking prior used in\nPhyloSub, we demonstrate superior mixing and running time using the treeCRP\nwith our new split-merge procedures. We also show that given the same number of\nsamples, TSSB and treeCRP have similar ability to recover the subclonal\nstructure of a tumor.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 20:39:42 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Deshwar", "Amit G.", ""], ["Vembu", "Shankar", ""], ["Morris", "Quaid", ""]]}, {"id": "1408.2597", "submitter": "Yangyang Xu", "authors": "Yangyang Xu and Wotao Yin", "title": "Block stochastic gradient iteration for convex and nonconvex\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic gradient (SG) method can minimize an objective function\ncomposed of a large number of differentiable functions, or solve a stochastic\noptimization problem, to a moderate accuracy. The block coordinate\ndescent/update (BCD) method, on the other hand, handles problems with multiple\nblocks of variables by updating them one at a time; when the blocks of\nvariables are easier to update individually than together, BCD has a lower\nper-iteration cost. This paper introduces a method that combines the features\nof SG and BCD for problems with many components in the objective and with\nmultiple (blocks of) variables.\n  Specifically, a block stochastic gradient (BSG) method is proposed for\nsolving both convex and nonconvex programs. At each iteration, BSG approximates\nthe gradient of the differentiable part of the objective by randomly sampling a\nsmall set of data or sampling a few functions from the sum term in the\nobjective, and then, using those samples, it updates all the blocks of\nvariables in either a deterministic or a randomly shuffled order. Its\nconvergence for both convex and nonconvex cases are established in different\nsenses. In the convex case, the proposed method has the same order of\nconvergence rate as the SG method. In the nonconvex case, its convergence is\nestablished in terms of the expected violation of a first-order optimality\ncondition. The proposed method was numerically tested on problems including\nstochastic least squares and logistic regression, which are convex, as well as\nlow-rank tensor recovery and bilinear logistic regression, which are nonconvex.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 01:21:42 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 13:14:26 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 04:02:54 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Xu", "Yangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1408.2700", "submitter": "Radu Horaud P", "authors": "Antoine Deleforge, Radu Horaud, Yoav Schechner and Laurent Girin", "title": "Co-Localization of Audio Sources in Images Using Binaural Features and\n  Locally-Linear Regression", "comments": "15 pages, 8 figures", "journal-ref": "IEEE Transactions on Audio, Speech, and Language Processing 23(4),\n  718-731, April, 2015", "doi": "10.1109/TASLP.2015.2405475", "report-no": null, "categories": "cs.SD cs.MM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of localizing audio sources using binaural\nmeasurements. We propose a supervised formulation that simultaneously localizes\nmultiple sources at different locations. The approach is intrinsically\nefficient because, contrary to prior work, it relies neither on source\nseparation, nor on monaural segregation. The method starts with a training\nstage that establishes a locally-linear Gaussian regression model between the\ndirectional coordinates of all the sources and the auditory features extracted\nfrom binaural measurements. While fixed-length wide-spectrum sounds (white\nnoise) are used for training to reliably estimate the model parameters, we show\nthat the testing (localization) can be extended to variable-length\nsparse-spectrum sounds (such as speech), thus enabling a wide range of\nrealistic applications. Indeed, we demonstrate that the method can be used for\naudio-visual fusion, namely to map speech signals onto images and hence to\nspatially align the audio and visual modalities, thus enabling to discriminate\nbetween speaking and non-speaking faces. We release a novel corpus of real-room\nrecordings that allow quantitative evaluation of the co-localization method in\nthe presence of one or two sound sources. Experiments demonstrate increased\naccuracy and speed relative to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 12:08:13 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 10:35:07 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 13:45:06 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 10:00:02 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Deleforge", "Antoine", ""], ["Horaud", "Radu", ""], ["Schechner", "Yoav", ""], ["Girin", "Laurent", ""]]}, {"id": "1408.2714", "submitter": "Nguyen Viet Cuong", "authors": "Vu Dinh, Lam Si Tung Ho, Nguyen Viet Cuong, Duy Nguyen, Binh T. Nguyen", "title": "Learning From Non-iid Data: Fast Rates for the One-vs-All Multiclass\n  Plug-in Classifiers", "comments": "12th Annual Conference on Theory and Applications of Models of\n  Computation (TAMC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove new fast learning rates for the one-vs-all multiclass plug-in\nclassifiers trained either from exponentially strongly mixing data or from data\ngenerated by a converging drifting distribution. These are two typical\nscenarios where training data are not iid. The learning rates are obtained\nunder a multiclass version of Tsybakov's margin assumption, a type of low-noise\nassumption, and do not depend on the number of classes. Our results are general\nand include a previous result for binary-class plug-in classifiers with iid\ndata as a special case. In contrast to previous works for least squares SVMs\nunder the binary-class setting, our results retain the optimal learning rate in\nthe iid case.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 14:04:24 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 16:30:43 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Dinh", "Vu", ""], ["Ho", "Lam Si Tung", ""], ["Cuong", "Nguyen Viet", ""], ["Nguyen", "Duy", ""], ["Nguyen", "Binh T.", ""]]}, {"id": "1408.2764", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Shivani Agarwal", "title": "Convex Calibration Dimension for Multiclass Loss Matrices", "comments": "Accepted to JMLR, pending editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study consistency properties of surrogate loss functions for general\nmulticlass learning problems, defined by a general multiclass loss matrix. We\nextend the notion of classification calibration, which has been studied for\nbinary and multiclass 0-1 classification problems (and for certain other\nspecific learning problems), to the general multiclass setting, and derive\nnecessary and sufficient conditions for a surrogate loss to be calibrated with\nrespect to a loss matrix in this setting. We then introduce the notion of\nconvex calibration dimension of a multiclass loss matrix, which measures the\nsmallest `size' of a prediction space in which it is possible to design a\nconvex surrogate that is calibrated with respect to the loss matrix. We derive\nboth upper and lower bounds on this quantity, and use these results to analyze\nvarious loss matrices. In particular, we apply our framework to study various\nsubset ranking losses, and use the convex calibration dimension as a tool to\nshow both the existence and non-existence of various types of convex calibrated\nsurrogates for these losses. Our results strengthen recent results of Duchi et\nal. (2010) and Calauzenes et al. (2012) on the non-existence of certain types\nof convex calibrated surrogates in subset ranking. We anticipate the convex\ncalibration dimension may prove to be a useful tool in the study and design of\nsurrogate losses for general multiclass learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 16:13:50 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 18:56:15 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1408.2869", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Jacek Tabor", "title": "Cluster based RBF Kernel for Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Gaussian SVM classification we use the feature space\nprojection transforming points to normal distributions with fixed covariance\nmatrices (identity in the standard RBF and the covariance of the whole dataset\nin Mahalanobis RBF). In this paper we add additional information to Gaussian\nSVM by considering local geometry-dependent feature space projection. We\nemphasize that our approach is in fact an algorithm for a construction of the\nnew Gaussian-type kernel.\n  We show that better (compared to standard RBF and Mahalanobis RBF)\nclassification results are obtained in the simple case when the space is\npreliminary divided by k-means into two sets and points are represented as\nnormal distributions with a covariances calculated according to the dataset\npartitioning.\n  We call the constructed method C$_k$RBF, where $k$ stands for the amount of\nclusters used in k-means. We show empirically on nine datasets from UCI\nrepository that C$_2$RBF increases the stability of the grid search (measured\nas the probability of finding good parameters).\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 22:30:11 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Tabor", "Jacek", ""]]}, {"id": "1408.2923", "submitter": "Panos Toulis", "authors": "Panos Toulis and Edoardo M. Airoldi", "title": "Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients", "comments": "Annals of Statistics, 2016, forthcoming; 71 pages, 37-page main body;\n  9 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent procedures have gained popularity for parameter\nestimation from large data sets. However, their statistical properties are not\nwell understood, in theory. And in practice, avoiding numerical instability\nrequires careful tuning of key parameters. Here, we introduce implicit\nstochastic gradient descent procedures, which involve parameter updates that\nare implicitly defined. Intuitively, implicit updates shrink standard\nstochastic gradient descent updates. The amount of shrinkage depends on the\nobserved Fisher information matrix, which does not need to be explicitly\ncomputed; thus, implicit procedures increase stability without increasing the\ncomputational burden. Our theoretical analysis provides the first full\ncharacterization of the asymptotic behavior of both standard and implicit\nstochastic gradient descent-based estimators, including finite-sample error\nbounds. Importantly, analytical expressions for the variances of these\nstochastic gradient-based estimators reveal their exact loss of efficiency. We\nalso develop new algorithms to compute implicit stochastic gradient\ndescent-based estimators for generalized linear models, Cox proportional\nhazards, M-estimators, in practice, and perform extensive experiments. Our\nresults suggest that implicit stochastic gradient descent procedures are poised\nto become a workhorse for approximate inference from large data sets\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 06:47:25 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 16:43:26 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 00:27:00 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 17:17:20 GMT"}, {"version": "v5", "created": "Sun, 4 Oct 2015 21:11:17 GMT"}, {"version": "v6", "created": "Wed, 28 Sep 2016 15:29:27 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1408.3060", "submitter": "Alex Smola J", "authors": "Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola", "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 17:37:43 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Le", "Quoc Viet", ""], ["Sarlos", "Tamas", ""], ["Smola", "Alexander Johannes", ""]]}, {"id": "1408.3081", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Human Activity Learning and Segmentation using Partially Hidden\n  Discriminative Models", "comments": "HAREM 2005: Proceedings of the International Workshop on Human\n  Activity Recognition and Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and understanding the typical patterns in the daily activities and\nroutines of people from low-level sensory data is an important problem in many\napplication domains such as building smart environments, or providing\nintelligent assistance. Traditional approaches to this problem typically rely\non supervised learning and generative models such as the hidden Markov models\nand its extensions. While activity data can be readily acquired from pervasive\nsensors, e.g. in smart environments, providing manual labels to support\nsupervised training is often extremely expensive. In this paper, we propose a\nnew approach based on semi-supervised training of partially hidden\ndiscriminative models such as the conditional random field (CRF) and the\nmaximum entropy Markov model (MEMM). We show that these models allow us to\nincorporate both labeled and unlabeled data for learning, and at the same time,\nprovide us with the flexibility and accuracy of the discriminative framework.\nOur experimental results in the video surveillance domain illustrate that these\nmodels can perform better than their generative counterpart, the partially\nhidden Markov model, even when a substantial amount of labels are unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:35:49 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.3092", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Convergence rate of Bayesian tensor estimator: Optimal rate without\n  restricted strong convexity", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the statistical convergence rate of a Bayesian\nlow-rank tensor estimator. Our problem setting is the regression problem where\na tensor structure underlying the data is estimated. This problem setting\noccurs in many practical applications, such as collaborative filtering,\nmulti-task learning, and spatio-temporal data analysis. The convergence rate is\nanalyzed in terms of both in-sample and out-of-sample predictive accuracies. It\nis shown that a near optimal rate is achieved without any strong convexity of\nthe observation. Moreover, we show that the method has adaptivity to the\nunknown rank of the true tensor, that is, the near optimal rate depending on\nthe true rank is achieved even if it is not known a priori.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 19:16:29 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1408.3115", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Rong Jin, Shenghuo Zhu, Qihang Lin", "title": "On Data Preconditioning for Regularized Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study data preconditioning, a well-known and long-existing\ntechnique, for boosting the convergence of first-order methods for regularized\nloss minimization. It is well understood that the condition number of the\nproblem, i.e., the ratio of the Lipschitz constant to the strong convexity\nmodulus, has a harsh effect on the convergence of the first-order optimization\nmethods. Therefore, minimizing a small regularized loss for achieving good\ngeneralization performance, yielding an ill conditioned problem, becomes the\nbottleneck for big data problems. We provide a theory on data preconditioning\nfor regularized loss minimization. In particular, our analysis exhibits an\nappropriate data preconditioner and characterizes the conditions on the loss\nfunction and on the data under which data preconditioning can reduce the\ncondition number and therefore boost the convergence for minimizing the\nregularized loss. To make the data preconditioning practically useful, we\nendeavor to employ and analyze a random sampling approach to efficiently\ncompute the preconditioned data. The preliminary experiments validate our\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 18:44:18 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 06:29:17 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 15:23:40 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2015 15:35:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Lin", "Qihang", ""]]}, {"id": "1408.3332", "submitter": "Victor Nedelko", "authors": "Victor Nedelko", "title": "Exact and empirical estimation of misclassification probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of risk estimation in the classification problem, with\nspecific focus on finding distributions that maximize the confidence intervals\nof risk estimation. We derived simple analytic approximations for the maximum\nbias of empirical risk for histogram classifier. We carry out a detailed study\non using these analytic estimates for empirical estimation of risk.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:29:36 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Nedelko", "Victor", ""]]}, {"id": "1408.3378", "submitter": "Creighton Heaukulani", "authors": "Creighton Heaukulani, David A. Knowles, Zoubin Ghahramani", "title": "Beta diffusion trees and hierarchical feature allocations", "comments": "43 pages, 13 figures. Major revision to the proof of Thm. 2. Large\n  portions of Chs. 2 & 4 moved into the appendix. Added Fig. 4. Revisions\n  throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the beta diffusion tree, a random tree structure with a set of\nleaves that defines a collection of overlapping subsets of objects, known as a\nfeature allocation. A generative process for the tree structure is defined in\nterms of particles (representing the objects) diffusing in some continuous\nspace, analogously to the Dirichlet diffusion tree (Neal, 2003), which defines\na tree structure over partitions (i.e., non-overlapping subsets) of the\nobjects. Unlike in the Dirichlet diffusion tree, multiple copies of a particle\nmay exist and diffuse along multiple branches in the beta diffusion tree, and\nan object may therefore belong to multiple subsets of particles. We demonstrate\nhow to build a hierarchically-clustered factor analysis model with the beta\ndiffusion tree and how to perform inference over the random tree structures\nwith a Markov chain Monte Carlo algorithm. We conclude with several numerical\nexperiments on missing data problems with data sets of gene expression\nmicroarrays, international development statistics, and intranational\nsocioeconomic measurements.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 18:29:20 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 17:04:49 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Heaukulani", "Creighton", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1408.3467", "submitter": "Qianqian Xu", "authors": "Qianqian Xu and Jiechao Xiong and Xiaochun Cao and Qingming Huang and\n  Yuan Yao", "title": "Evaluating Visual Properties via Robust HodgeRank", "comments": "25 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 05:18:19 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:06:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1408.3807", "submitter": "David Barber", "authors": "David Barber", "title": "On solving Ordinary Differential Equations using Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of Gaussian Process based approaches that can be used to\nsolve non-linear Ordinary Differential Equations. We suggest an explicit\nprobabilistic solver and two implicit methods, one analogous to Picard\niteration and the other to gradient matching. All methods have greater accuracy\nthan previously suggested Gaussian Process approaches. We also suggest a\ngeneral approach that can yield error estimates from any standard ODE solver.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 09:52:06 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Barber", "David", ""]]}, {"id": "1408.4045", "submitter": "Soledad Villar", "authors": "Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar\n  Krishnaswamy, Soledad Villar, Rachel Ward", "title": "Relax, no need to round: integrality of clustering formulations", "comments": "30 pages, ITCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact recovery conditions for convex relaxations of point cloud\nclustering problems, focusing on two of the most common optimization problems\nfor unsupervised clustering: $k$-means and $k$-median clustering. Motivations\nfor focusing on convex relaxations are: (a) they come with a certificate of\noptimality, and (b) they are generic tools which are relatively parameter-free,\nnot tailored to specific assumptions over the input. More precisely, we\nconsider the distributional setting where there are $k$ clusters in\n$\\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a\nsymmetric distribution within a ball of unit radius. We ask: what is the\nminimal separation distance between cluster centers needed for convex\nrelaxations to exactly recover these $k$ clusters as the optimal integral\nsolution? For the $k$-median linear programming relaxation we show a tight\nbound: exact recovery is obtained given arbitrarily small pairwise separation\n$\\epsilon > 0$ between the balls. In other words, the pairwise center\nseparation is $\\Delta > 2+\\epsilon$. Under the same distributional model, the\n$k$-means LP relaxation fails to recover such clusters at separation as large\nas $\\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get\nexact cluster recovery at center separation $\\Delta > 2\\sqrt2(1+\\sqrt{1/m})$.\nIn contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means\nalgorithm) can fail to recover clusters in this setting; even with arbitrarily\nlarge cluster separation, k-means++ with overseeding by any constant factor\nfails with high probability at exact cluster recovery. To complement the\ntheoretical analysis, we provide an experimental study of the recovery\nguarantees for these various methods, and discuss several open problems which\nthese experiments suggest.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 15:42:16 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:37:34 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 18:07:10 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 16:11:36 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 02:11:54 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Bandeira", "Afonso S.", ""], ["Charikar", "Moses", ""], ["Krishnaswamy", "Ravishankar", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1408.4140", "submitter": "Leo Duan", "authors": "Leo L. Duan, John P. Clancy and Rhonda D. Szczesniak", "title": "BET: Bayesian Ensemble Trees for Clustering and Prediction in\n  Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel \"tree-averaging\" model that utilizes the ensemble of\nclassification and regression trees (CART). Each constituent tree is estimated\nwith a subset of similar data. We treat this grouping of subsets as Bayesian\nensemble trees (BET) and model them as an infinite mixture Dirichlet process.\nWe show that BET adapts to data heterogeneity and accurately estimates each\ncomponent. Compared with the bootstrap-aggregating approach, BET shows improved\nprediction performance with fewer trees. We develop an efficient estimating\nprocedure with improved sampling strategies in both CART and mixture models. We\ndemonstrate these advantages of BET with simulations, classification of breast\ncancer and regression of lung function measurement of cystic fibrosis patients.\n  Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity;\nMixture of Trees.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 20:12:09 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Duan", "Leo L.", ""], ["Clancy", "John P.", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1408.4622", "submitter": "Emmanuel Vazquez", "authors": "Emmanuel Vazquez and Julien Bect", "title": "A new integral loss function for Bayesian optimization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a real-valued continuous function $f$\nusing a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n\\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\nconsidering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\nfunction value observed after $n$ evaluations of $f$). This loss function puts\nemphasis on the value of the maximum, at the expense of the location of the\nmaximizer. In the special case of a one-step Bayes-optimal strategy, it leads\nto the classical Expected Improvement (EI) sampling criterion. This is a\nspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\nassociated to a certain uncertainty measure (here, the expected loss) on the\nquantity of interest is minimized at each step of the algorithm. In this\narticle, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n\\lambda)$, we propose to consider instead the integral loss function\n$\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\nthe case of a Gaussian process prior, to a new numerically tractable sampling\ncriterion that we call $\\rm EI^2$ (for Expected Integrated Expected\nImprovement). A numerical experiment illustrates that a SUR strategy based on\nthis new sampling criterion reduces the error on both the value and the\nlocation of the maximizer faster than the EI-based strategy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 12:16:54 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Vazquez", "Emmanuel", ""], ["Bect", "Julien", ""]]}, {"id": "1408.4660", "submitter": "Leo Duan", "authors": "Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak", "title": "Joint Hierarchical Gaussian Process Model with Application to Forecast\n  in Medical Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel extrapolation method is proposed for longitudinal forecasting. A\nhierarchical Gaussian process model is used to combine nonlinear population\nchange and individual memory of the past to make prediction. The prediction\nerror is minimized through the hierarchical design. The method is further\nextended to joint modeling of continuous measurements and survival events. The\nbaseline hazard, covariate and joint effects are conveniently modeled in this\nhierarchical structure. The estimation and inference are implemented in fully\nBayesian framework using the objective and shrinkage priors. In simulation\nstudies, this model shows robustness in latent estimation, correlation\ndetection and high accuracy in forecasting. The model is illustrated with\nmedical monitoring data from cystic fibrosis (CF) patients. Estimation and\nforecasts are obtained in the measurement of lung function and records of acute\nrespiratory events.\n  Keyword: Extrapolation, Joint Model, Longitudinal Model, Hierarchical\nGaussian Process, Cystic Fibrosis, Medical Monitoring\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 14:03:40 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 14:36:56 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Duan", "Leo L.", ""], ["Clancy", "John P.", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1408.4908", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael\n  Mitzenmacher", "title": "Theoretical Foundations of Equitability and the Maximal Information\n  Coefficient", "comments": "46 pages, 3 figures, 2 tables. This paper has been subsumed by\n  arXiv:1505.02213 and arXiv:1505.02212. Please cite those papers instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal information coefficient (MIC) is a tool for finding the strongest\npairwise relationships in a data set with many variables (Reshef et al., 2011).\nMIC is useful because it gives similar scores to equally noisy relationships of\ndifferent types. This property, called {\\em equitability}, is important for\nanalyzing high-dimensional data sets.\n  Here we formalize the theory behind both equitability and MIC in the language\nof estimation theory. This formalization has a number of advantages. First, it\nallows us to show that equitability is a generalization of power against\nstatistical independence. Second, it allows us to compute and discuss the\npopulation value of MIC, which we call MIC_*. In doing so we generalize and\nstrengthen the mathematical results proven in Reshef et al. (2011) and clarify\nthe relationship between MIC and mutual information. Introducing MIC_* also\nenables us to reason about the properties of MIC more abstractly: for instance,\nwe show that MIC_* is continuous and that there is a sense in which it is a\ncanonical \"smoothing\" of mutual information. We also prove an alternate,\nequivalent characterization of MIC_* that we use to state new estimators of it\nas well as an algorithm for explicitly computing it when the joint probability\ndensity function of a pair of random variables is known. Our hope is that this\npaper provides a richer theoretical foundation for MIC and equitability going\nforward.\n  This paper will be accompanied by a forthcoming companion paper that performs\nextensive empirical analysis and comparison to other methods and discusses the\npractical aspects of both equitability and the use of MIC and its related\nstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 08:17:13 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 19:08:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2015 19:58:17 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1408.4966", "submitter": "Jimmy Dubuisson", "authors": "Jimmy Dubuisson, Jean-Pierre Eckmann and Andrea Agazzi", "title": "Diffusion Fingerprints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce, test and discuss a method for classifying and clustering data\nmodeled as directed graphs. The idea is to start diffusion processes from any\nsubset of a data collection, generating corresponding distributions for\nreaching points in the network. These distributions take the form of\nhigh-dimensional numerical vectors and capture essential topological properties\nof the original dataset. We show how these diffusion vectors can be\nsuccessfully applied for getting state-of-the-art accuracies in the problem of\nextracting pathways from metabolic networks. We also provide a guideline to\nillustrate how to use our method for classification problems, and discuss\nimportant details of its implementation. In particular, we present a simple\ndimensionality reduction technique that lowers the computational cost of\nclassifying diffusion vectors, while leaving the predictive power of the\nclassification process substantially unaltered. Although the method has very\nfew parameters, the results we obtain show its flexibility and power. This\nshould make it helpful in many other contexts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 11:34:37 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 13:48:40 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Dubuisson", "Jimmy", ""], ["Eckmann", "Jean-Pierre", ""], ["Agazzi", "Andrea", ""]]}, {"id": "1408.5032", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Guille D. Canas, Lorenzo Rosasco", "title": "On the Sample Complexity of Subspace Learning", "comments": "Extendend Version of conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of algorithms in machine learning, from principal component\nanalysis (PCA), and its non-linear (kernel) extensions, to more recent spectral\nembedding and support estimation methods, rely on estimating a linear subspace\nfrom samples. In this paper we introduce a general formulation of this problem\nand derive novel learning error estimates. Our results rely on natural\nassumptions on the spectral properties of the covariance operator associated to\nthe data distribu- tion, and hold for a wide class of metrics between\nsubspaces. As special cases, we discuss sharp error estimates for the\nreconstruction properties of PCA and spectral support estimation. Key to our\nanalysis is an operator theoretic approach that has broad applicability to\nspectral learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 15:12:37 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Rudi", "Alessandro", ""], ["Canas", "Guille D.", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1408.5099", "submitter": "Cameron Musco", "authors": "Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco,\n  Richard Peng, Aaron Sidford", "title": "Uniform Sampling for Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling has become a critical tool in solving massive matrix\nproblems. For linear regression, a small, manageable set of data rows can be\nrandomly selected to approximate a tall, skinny data matrix, improving\nprocessing time significantly. For theoretical performance guarantees, each row\nmust be sampled with probability proportional to its statistical leverage\nscore. Unfortunately, leverage scores are difficult to compute.\n  A simple alternative is to sample rows uniformly at random. While this often\nworks, uniform sampling will eliminate critical row information for many\nnatural instances. We take a fresh look at uniform sampling by examining what\ninformation it does preserve. Specifically, we show that uniform sampling\nyields a matrix that, in some sense, well approximates a large fraction of the\noriginal. While this weak form of approximation is not enough for solving\nlinear regression directly, it is enough to compute a better approximation.\n  This observation leads to simple iterative row sampling algorithms for matrix\napproximation that run in input-sparsity time and preserve row structure and\nsparsity at all intermediate steps. In addition to an improved understanding of\nuniform sampling, our main proof introduces a structural result of independent\ninterest: we show that every matrix can be made to have low coherence by\nreweighting a small subset of its rows.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 18:32:00 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Cohen", "Michael B.", ""], ["Lee", "Yin Tat", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Peng", "Richard", ""], ["Sidford", "Aaron", ""]]}, {"id": "1408.5286", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi", "title": "Designing labeled graph classifiers by exploiting the R\\'enyi entropy of\n  the dissimilarity representation", "comments": "Revised version", "journal-ref": null, "doi": "10.3390/e19050216", "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing patterns as labeled graphs is becoming increasingly common in\nthe broad field of computational intelligence. Accordingly, a wide repertoire\nof pattern recognition tools, such as classifiers and knowledge discovery\nprocedures, are nowadays available and tested for various datasets of labeled\ngraphs. However, the design of effective learning procedures operating in the\nspace of labeled graphs is still a challenging problem, especially from the\ncomputational complexity viewpoint. In this paper, we present a major\nimprovement of a general-purpose classifier for graphs, which is conceived on\nan interplay between dissimilarity representation, clustering,\ninformation-theoretic techniques, and evolutionary optimization algorithms. The\nimprovement focuses on a specific key subroutine devised to compress the input\ndata. We prove different theorems which are fundamental to the setting of the\nparameters controlling such a compression operation. We demonstrate the\neffectiveness of the resulting classifier by benchmarking the developed\nvariants on well-known datasets of labeled graphs, considering as distinct\nperformance indicators the classification accuracy, computing time, and\nparsimony in terms of structural complexity of the synthesized classification\nmodels. The results show state-of-the-art standards in terms of test set\naccuracy and a considerable speed-up for what concerns the computing time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 13:03:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jan 2015 15:29:31 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2016 23:44:25 GMT"}, {"version": "v4", "created": "Fri, 11 Mar 2016 13:18:17 GMT"}, {"version": "v5", "created": "Fri, 31 Mar 2017 19:26:16 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 20:48:07 GMT"}, {"version": "v7", "created": "Thu, 20 Apr 2017 14:40:11 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Livi", "Lorenzo", ""]]}, {"id": "1408.5352", "submitter": "Han Liu", "authors": "Zhaoran Wang, Huanran Lu, Han Liu", "title": "Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time", "comments": "64 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 16:33:09 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Wang", "Zhaoran", ""], ["Lu", "Huanran", ""], ["Liu", "Han", ""]]}, {"id": "1408.5369", "submitter": "Tengyao Wang", "authors": "Tengyao Wang, Quentin Berthet, Richard J. Samworth", "title": "Statistical and computational trade-offs in estimation of sparse\n  principal components", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1369 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 5, 1896-1930", "doi": "10.1214/15-AOS1369", "report-no": "IMS-AOS-AOS1369", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sparse principal component analysis has emerged as an\nextremely popular dimension reduction technique for high-dimensional data. The\ntheoretical challenge, in the simplest case, is to estimate the leading\neigenvector of a population covariance matrix under the assumption that this\neigenvector is sparse. An impressive range of estimators have been proposed;\nsome of these are fast to compute, while others are known to achieve the\nminimax optimal rate over certain Gaussian or sub-Gaussian classes. In this\npaper, we show that, under a widely-believed assumption from computational\ncomplexity theory, there is a fundamental trade-off between statistical and\ncomputational performance in this problem. More precisely, working with new,\nlarger classes satisfying a restricted covariance concentration condition, we\nshow that there is an effective sample size regime in which no randomised\npolynomial time algorithm can achieve the minimax optimal rate. We also study\nthe theoretical performance of a (polynomial time) variant of the well-known\nsemidefinite relaxation estimator, revealing a subtle interplay between\nstatistical and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 17:42:35 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 13:16:21 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Wang", "Tengyao", ""], ["Berthet", "Quentin", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1408.5404", "submitter": "Kacper Chwialkowski", "authors": "Kacper Chwialkowski and Dino Sejdinovic and Arthur Gretton", "title": "A Wild Bootstrap for Degenerate Kernel Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wild bootstrap method for nonparametric hypothesis tests based on kernel\ndistribution embeddings is proposed. This bootstrap method is used to construct\nprovably consistent tests that apply to random processes, for which the naive\npermutation-based bootstrap fails. It applies to a large group of kernel tests\nbased on V-statistics, which are degenerate under the null hypothesis, and\nnon-degenerate elsewhere. To illustrate this approach, we construct a\ntwo-sample test, an instantaneous independence test and a multiple lag\nindependence test for time series. In experiments, the wild bootstrap gives\nstrong performance on synthetic examples, on audio data, and in performance\nbenchmarking for the Gibbs sampler.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 03:16:41 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 10:15:33 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Chwialkowski", "Kacper", ""], ["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""]]}, {"id": "1408.5427", "submitter": "Carl Meyer Dr.", "authors": "Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek", "title": "A Case Study in Text Mining: Interpreting Twitter Data From World Cup\n  Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a field of data analysis that extracts underlying\npatterns in data. One application of cluster analysis is in text-mining, the\nanalysis of large collections of text to find similarities between documents.\nWe used a collection of about 30,000 tweets extracted from Twitter just before\nthe World Cup started. A common problem with real world text data is the\npresence of linguistic noise. In our case it would be extraneous tweets that\nare unrelated to dominant themes. To combat this problem, we created an\nalgorithm that combined the DBSCAN algorithm and a consensus matrix. This way\nwe are left with the tweets that are related to those dominant themes. We then\nused cluster analysis to find those topics that the tweets describe. We\nclustered the tweets using k-means, a commonly used clustering algorithm, and\nNon-Negative Matrix Factorization (NMF) and compared the results. The two\nalgorithms gave similar results, but NMF proved to be faster and provided more\neasily interpreted results. We explored our results using two visualization\ntools, Gephi and Wordle.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 17:58:33 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Godfrey", "Daniel", ""], ["Johns", "Caley", ""], ["Meyer", "Carl", ""], ["Race", "Shaina", ""], ["Sadek", "Carol", ""]]}, {"id": "1408.5449", "submitter": "Kar-Ann Toh", "authors": "Kar-Ann Toh", "title": "Stretchy Polynomial Regression", "comments": "Article created in April and revised in August 2014. Submitted to\n  ICARCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel solution for stretchy polynomial regression\nlearning. The solution comes in primal and dual closed-forms similar to that of\nridge regression. Essentially, the proposed solution stretches the covariance\ncomputation via a power term thereby compresses or amplifies the estimation.\nOur experiments on both synthetic data and real-world data show effectiveness\nof the proposed method for compressive learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 01:23:23 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Toh", "Kar-Ann", ""]]}, {"id": "1408.5456", "submitter": "Houtao Deng", "authors": "Houtao Deng", "title": "Interpreting Tree Ensembles with inTrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensembles such as random forests and boosted trees are accurate but\ndifficult to understand, debug and deploy. In this work, we provide the inTrees\n(interpretable trees) framework that extracts, measures, prunes and selects\nrules from a tree ensemble, and calculates frequent variable interactions. An\nrule-based learner, referred to as the simplified tree ensemble learner (STEL),\ncan also be formed and used for future prediction. The inTrees framework can\napplied to both classification and regression problems, and is applicable to\nmany types of tree ensembles, e.g., random forests, regularized random forests,\nand boosted trees. We implemented the inTrees algorithms in the \"inTrees\" R\npackage.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 05:06:55 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Deng", "Houtao", ""]]}, {"id": "1408.5544", "submitter": "Daniel Pimentel", "authors": "Daniel L. Pimentel-Alarc\\'on", "title": "To lie or not to lie in a subspace", "comments": "First author mistakenly listed advisors as co-authors in his research\n  proposal. This is corrected in the current version. 59 pages, 19 figures.\n  Subspace clustering, missing data, converse of matrix completion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Give deterministic necessary and sufficient conditions to guarantee that if a\nsubspace fits certain partially observed data from a union of subspaces, it is\nbecause such data really lies in a subspace.\n  Furthermore, Give deterministic necessary and sufficient conditions to\nguarantee that if a subspace fits certain partially observed data, such\nsubspace is unique.\n  Do this by characterizing when and only when a set of incomplete vectors\nbehaves as a single but complete one.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 02:53:57 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 15:21:09 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""]]}, {"id": "1408.5634", "submitter": "R. Sean Bowman", "authors": "R. Sean Bowman and Douglas Heisterkamp and Jesse Johnson and Danielle\n  O'Donnol", "title": "An application of topological graph clustering to protein function\n  prediction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a semisupervised learning algorithm based on a topological data\nanalysis approach to assign functional categories to yeast proteins using\nsimilarity graphs. This new approach to analyzing biological networks yields\nresults that are as good as or better than state of the art existing\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 20:41:51 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Bowman", "R. Sean", ""], ["Heisterkamp", "Douglas", ""], ["Johnson", "Jesse", ""], ["O'Donnol", "Danielle", ""]]}, {"id": "1408.5661", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data science and machine learning, hierarchical parametric models, such as\nmixture models, are often used. They contain two kinds of variables: observable\nvariables, which represent the parts of the data that can be directly measured,\nand latent variables, which represent the underlying processes that generate\nthe data. Although there has been an increase in research on the estimation\naccuracy for observable variables, the theoretical analysis of estimating\nlatent variables has not been thoroughly investigated. In a previous study, we\ndetermined the accuracy of a Bayes estimation for the joint probability of the\nlatent variables in a dataset, and we proved that the Bayes method is\nasymptotically more accurate than the maximum-likelihood method. However, the\naccuracy of the Bayes estimation for a single latent variable remains unknown.\nIn the present paper, we derive the asymptotic expansions of the error\nfunctions, which are defined by the Kullback-Leibler divergence, for two types\nof single-variable estimations when the statistical regularity is satisfied.\nOur results indicate that the accuracies of the Bayes and maximum-likelihood\nmethods are asymptotically equivalent and clarify that the Bayes method is only\nadvantageous for multivariable estimations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 04:44:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:00:06 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 06:59:26 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1408.5801", "submitter": "Ryan Tibshirani", "authors": "Ryan J. Tibshirani", "title": "A General Framework for Fast Stagewise Algorithms", "comments": "56 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward stagewise regression follows a very simple strategy for constructing\na sequence of sparse regression estimates: it starts with all coefficients\nequal to zero, and iteratively updates the coefficient (by a small amount\n$\\epsilon$) of the variable that achieves the maximal absolute inner product\nwith the current residual. This procedure has an interesting connection to the\nlasso: under some conditions, it is known that the sequence of forward\nstagewise estimates exactly coincides with the lasso path, as the step size\n$\\epsilon$ goes to zero. Furthermore, essentially the same equivalence holds\noutside of least squares regression, with the minimization of a differentiable\nconvex loss function subject to an $\\ell_1$ norm constraint (the stagewise\nalgorithm now updates the coefficient corresponding to the maximal absolute\ncomponent of the gradient).\n  Even when they do not match their $\\ell_1$-constrained analogues, stagewise\nestimates provide a useful approximation, and are computationally appealing.\nTheir success in sparse modeling motivates the question: can a simple,\neffective strategy like forward stagewise be applied more broadly in other\nregularization settings, beyond the $\\ell_1$ norm and sparsity? The current\npaper is an attempt to do just this. We present a general framework for\nstagewise estimation, which yields fast algorithms for problems such as\ngroup-structured learning, matrix completion, image denoising, and more.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 15:21:24 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 15:49:17 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "1408.5810", "submitter": "Somayeh Danafar", "authors": "Somayeh Danafar and Kenji Fukumizu and Faustino Gomez", "title": "Kernel-based Information Criterion", "comments": "We modified the reference 17, and the subcaptions of Figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Kernel-based Information Criterion (KIC) for model\nselection in regression analysis. The novel kernel-based complexity measure in\nKIC efficiently computes the interdependency between parameters of the model\nusing a variable-wise variance and yields selection of better, more robust\nregressors. Experimental results show superior performance on both simulated\nand real data sets compared to Leave-One-Out Cross-Validation (LOOCV),\nkernel-based Information Complexity (ICOMP), and maximum log of marginal\nlikelihood in Gaussian Process Regression (GPR).\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 15:44:05 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 17:02:54 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Danafar", "Somayeh", ""], ["Fukumizu", "Kenji", ""], ["Gomez", "Faustino", ""]]}, {"id": "1408.6032", "submitter": "Daniele Ramazzotti", "authors": "Fabrizio Angaroni, Kevin Chen, Chiara Damiani, Giulio Caravagna, Alex\n  Graudenzi, Daniele Ramazzotti", "title": "PMCE: efficient inference of expressive models of cancer evolution with\n  high prognostic power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Driver (epi)genomic alterations underlie the positive selection\nof cancer subpopulations, which promotes drug resistance and relapse. Even\nthough substantial heterogeneity is witnessed in most cancer types, mutation\naccumulation patterns can be regularly found and can be exploited to\nreconstruct predictive models of cancer evolution. Yet, available methods\ncannot infer logical formulas connecting events to represent alternative\nevolutionary routes or convergent evolution. Results: We introduce PMCE, an\nexpressive framework that leverages mutational profiles from cross-sectional\nsequencing data to infer probabilistic graphical models of cancer evolution\nincluding arbitrary logical formulas, and which outperforms the\nstate-of-the-art in terms of accuracy and robustness to noise, on simulations.\nThe application of PMCE to 7866 samples from the TCGA database allows us to\nidentify a highly significant correlation between the predicted evolutionary\npaths and the overall survival in 7 tumor types, proving that our approach can\neffectively stratify cancer patients in reliable risk groups. Availability:\nPMCE is freely available at https://github.com/danro9685/HESBCN. The code to\nreplicate all the analyses is available at:\nhttps://github.com/BIMIB-DISCo/PMCE. Contacts: daniele.ramazzotti@unimib.it,\nalex.graudenzi@ibfm.cnr.it\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 07:04:25 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 15:03:28 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Angaroni", "Fabrizio", ""], ["Chen", "Kevin", ""], ["Damiani", "Chiara", ""], ["Caravagna", "Giulio", ""], ["Graudenzi", "Alex", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1408.6214", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "A Methodology for the Diagnostic of Aircraft Engine Based on Indicators\n  Aggregation", "comments": "Proceedings of the 14th Industrial Conference, ICDM 2014, St.\n  Petersburg : Russian Federation (2014)", "journal-ref": null, "doi": "10.1007/978-3-319-08976-8_11", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aircraft engine manufacturers collect large amount of engine related data\nduring flights. These data are used to detect anomalies in the engines in order\nto help companies optimize their maintenance costs. This article introduces and\nstudies a generic methodology that allows one to build automatic early signs of\nanomaly detection in a way that is understandable by human operators who make\nthe final maintenance decision. The main idea of the method is to generate a\nvery large number of binary indicators based on parametric anomaly scores\ndesigned by experts, complemented by simple aggregations of those scores. The\nbest indicators are selected via a classical forward scheme, leading to a much\nreduced number of indicators that are tuned to a data set. We illustrate the\ninterest of the method on simulated data which contain realistic early signs of\nanomalies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:15:21 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1408.6218", "submitter": "Joseph Salmon", "authors": "Olga Klopp (MODAL'X, CREST-INSEE), Jean Lafond (LTCI), Eric Moulines\n  (LTCI), Joseph Salmon (LTCI)", "title": "Adaptive Multinomial Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of estimating a matrix given a sample of observed entries is known\nas the \\emph{matrix completion problem}. Most works on matrix completion have\nfocused on recovering an unknown real-valued low-rank matrix from a random\nsample of its entries. Here, we investigate the case of highly quantized\nobservations when the measurements can take only a small number of values.\nThese quantized outputs are generated according to a probability distribution\nparametrized by the unknown matrix of interest. This model corresponds, for\nexample, to ratings in recommender systems or labels in multi-class\nclassification. We consider a general, non-uniform, sampling scheme and give\ntheoretical guarantees on the performance of a constrained, nuclear norm\npenalized maximum likelihood estimator. One important advantage of this\nestimator is that it does not require knowledge of the rank or an upper bound\non the nuclear norm of the unknown matrix and, thus, it is adaptive. We provide\nlower bounds showing that our estimator is minimax optimal. An efficient\nalgorithm based on lifted coordinate gradient descent is proposed to compute\nthe estimator. A limited Monte-Carlo experiment, using both simulated and real\ndata is provided to support our claims.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:19:39 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Klopp", "Olga", "", "MODAL'X, CREST-INSEE"], ["Lafond", "Jean", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"], ["Salmon", "Joseph", "", "LTCI"]]}, {"id": "1408.6618", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Falsifiable implies Learnable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates that falsifiability is fundamental to learning. We\nprove the following theorem for statistical learning and sequential prediction:\nIf a theory is falsifiable then it is learnable -- i.e. admits a strategy that\npredicts optimally. An analogous result is shown for universal induction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 03:29:06 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1408.6686", "submitter": "Junxiao Song", "authors": "Junxiao Song, Prabhu Babu, and Daniel P. Palomar", "title": "Sparse Generalized Eigenvalue Problem via Smooth Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2394443", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an $\\ell_{0}$-norm penalized formulation of the\ngeneralized eigenvalue problem (GEP), aimed at extracting the leading sparse\ngeneralized eigenvector of a matrix pair. The formulation involves maximization\nof a discontinuous nonconcave objective function over a nonconvex constraint\nset, and is therefore computationally intractable. To tackle the problem, we\nfirst approximate the $\\ell_{0}$-norm by a continuous surrogate function. Then\nan algorithm is developed via iteratively majorizing the surrogate function by\na quadratic separable function, which at each iteration reduces to a regular\ngeneralized eigenvalue problem. A preconditioned steepest ascent algorithm for\nfinding the leading generalized eigenvector is provided. A systematic way based\non smoothing is proposed to deal with the \"singularity issue\" that arises when\na quadratic function is used to majorize the nondifferentiable surrogate\nfunction. For sparse GEPs with special structure, algorithms that admit a\nclosed-form solution at every iteration are derived. Numerical experiments show\nthat the proposed algorithms match or outperform existing algorithms in terms\nof computational complexity and support recovery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 11:22:08 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 07:07:13 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Song", "Junxiao", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1408.6693", "submitter": "Tianwen Wei", "authors": "Tianwen Wei", "title": "A study of the fixed points and spurious solutions of the FastICA\n  algorithm", "comments": "Submitted to Elsevier journal of Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FastICA algorithm is one of the most popular iterative algorithms in the\ndomain of linear independent component analysis. Despite its success, it is\nobserved that FastICA occasionally yields outcomes that do not correspond to\nany true solutions (known as demixing vectors) of the ICA problem. These\noutcomes are commonly referred to as spurious solutions. Although FastICA is\namong the most extensively studied ICA algorithms, the occurrence of spurious\nsolutions are not yet completely understood by the community. In this\ncontribution, we aim at addressing this issue. In the first part of this work,\nwe are interested in the relationship between demixing vectors, local\noptimizers of the contrast function and (attractive or unattractive) fixed\npoints of FastICA algorithm. Characterizations of these sets are given, and an\ninclusion relationship is discovered. In the second part, we investigate the\npossible scenarios where spurious solutions occur. We show that when certain\nbimodal Gaussian mixtures distributions are involved, there may exist spurious\nsolutions that are attractive fixed points of FastICA. In this case, popular\nnonlinearities such as \"gauss\" or \"tanh\" tend to yield spurious solutions,\nwhereas only \"kurtosis\" may give reliable results. Some advices are given for\nthe practical choice of nonlinearity function.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 12:11:43 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Wei", "Tianwen", ""]]}, {"id": "1408.6980", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Loukia Meligkotsidou", "title": "Augmentation Schemes for Particle MCMC", "comments": null, "journal-ref": "Statistics and Computing (November 2016), 26, 1293-1306", "doi": "10.1007/s11222-015-9603-4", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle MCMC involves using a particle filter within an MCMC algorithm. For\ninference of a model which involves an unobserved stochastic process, the\nstandard implementation uses the particle filter to propose new values for the\nstochastic process, and MCMC moves to propose new values for the parameters. We\nshow how particle MCMC can be generalised beyond this. Our key idea is to\nintroduce new latent variables. We then use the MCMC moves to update the latent\nvariables, and the particle filter to propose new values for the parameters and\nstochastic process given the latent variables. A generic way of defining these\nlatent variables is to model them as pseudo-observations of the parameters or\nof the stochastic process. By choosing the amount of information these latent\nvariables have about the parameters and the stochastic process we can often\nimprove the mixing of the particle MCMC algorithm by trading off the Monte\nCarlo error of the particle filter and the mixing of the MCMC moves. We show\nthat using pseudo-observations within particle MCMC can improve its efficiency\nin certain scenarios: dealing with initialisation problems of the particle\nfilter; speeding up the mixing of particle Gibbs when there is strong\ndependence between the parameters and the stochastic process; and enabling\nfurther MCMC steps to be used within the particle filter.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 11:02:41 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Fearnhead", "Paul", ""], ["Meligkotsidou", "Loukia", ""]]}]