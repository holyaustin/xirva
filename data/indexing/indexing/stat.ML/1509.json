[{"id": "1509.00106", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh", "title": "Adaptive Smoothing Algorithms for Nonsmooth Composite Convex\n  Minimization", "comments": "This paper has 23 pages, 3 figures and 1 table", "journal-ref": null, "doi": null, "report-no": "Tech. Report. STOR-2015-a", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive smoothing algorithm based on Nesterov's smoothing\ntechnique in \\cite{Nesterov2005c} for solving \"fully\" nonsmooth composite\nconvex optimization problems. Our method combines both Nesterov's accelerated\nproximal gradient scheme and a new homotopy strategy for smoothness parameter.\nBy an appropriate choice of smoothing functions, we develop a new algorithm\nthat has the $\\mathcal{O}\\left(\\frac{1}{\\varepsilon}\\right)$-worst-case\niteration-complexity while preserves the same complexity-per-iteration as in\nNesterov's method and allows one to automatically update the smoothness\nparameter at each iteration. Then, we customize our algorithm to solve four\nspecial cases that cover various applications. We also specify our algorithm to\nsolve constrained convex optimization problems and show its convergence\nguarantee on a primal sequence of iterates. We demonstrate our algorithm\nthrough three numerical examples and compare it with other related algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 01:00:59 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2015 13:48:28 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 19:28:33 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 12:12:53 GMT"}, {"version": "v5", "created": "Sun, 3 Jul 2016 19:36:26 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Tran-Dinh", "Quoc", ""]]}, {"id": "1509.00114", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie, and Nagi Gebraeel", "title": "Multi-Sensor Slope Change Detection", "comments": "Accepted with minor revision at ANOR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 01:49:15 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 03:08:31 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "1509.00130", "submitter": "Yao Xie", "authors": "Ruiyang Song, Yao Xie, and Sebastian Pokutta", "title": "Sequential Information Guided Sensing", "comments": "Submitted for journal publication. arXiv admin note: substantial text\n  overlap with arXiv:1501.06241", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the value of information in sequential compressed sensing by\ncharacterizing the performance of sequential information guided sensing in\npractical scenarios when information is inaccurate. In particular, we assume\nthe signal distribution is parameterized through Gaussian or Gaussian mixtures\nwith estimated mean and covariance matrices, and we can measure compressively\nthrough a noisy linear projection or using one-sparse vectors, i.e., observing\none entry of the signal each time. We establish a set of performance bounds for\nthe bias and variance of the signal estimator via posterior mean, by capturing\nthe conditional entropy (which is also related to the size of the uncertainty),\nand the additional power required due to inaccurate information to reach a\ndesired precision. Based on this, we further study how to estimate covariance\nbased on direct samples or covariance sketching. Numerical examples also\ndemonstrate the superior performance of Info-Greedy Sensing algorithms compared\nwith their random and non-adaptive counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 03:59:33 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Song", "Ruiyang", ""], ["Xie", "Yao", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1509.00137", "submitter": "Yao Xie", "authors": "Yao Xie, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song", "title": "Online Supervised Subspace Tracking", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for supervised subspace tracking, when there are two\ntime series $x_t$ and $y_t$, one being the high-dimensional predictors and the\nother being the response variables and the subspace tracking needs to take into\nconsideration of both sequences. It extends the classic online subspace\ntracking work which can be viewed as tracking of $x_t$ only. Our online\nsufficient dimensionality reduction (OSDR) is a meta-algorithm that can be\napplied to various cases including linear regression, logistic regression,\nmultiple linear regression, multinomial logistic regression, support vector\nmachine, the random dot product model and the multi-scale union-of-subspace\nmodel. OSDR reduces data-dimensionality on-the-fly with low-computational\ncomplexity and it can also handle missing data and dynamic data. OSDR uses an\nalternating minimization scheme and updates the subspace via gradient descent\non the Grassmannian manifold. The subspace update can be performed efficiently\nutilizing the fact that the Grassmannian gradient with respect to the subspace\nin many settings is rank-one (or low-rank in certain cases). The optimization\nproblem for OSDR is non-convex and hard to analyze in general; we provide\nconvergence analysis of OSDR in a simple linear regression setting. The good\nperformance of OSDR compared with the conventional unsupervised subspace\ntracking are demonstrated via numerical examples on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 04:42:39 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Xie", "Yao", ""], ["Song", "Ruiyang", ""], ["Dai", "Hanjun", ""], ["Li", "Qingbin", ""], ["Song", "Le", ""]]}, {"id": "1509.00151", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang", "title": "Learning A Task-Specific Deep Architecture For Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sparse coding-based clustering methods have shown to be successful,\ntheir bottlenecks in both efficiency and scalability limit the practical usage.\nIn recent years, deep learning has been proved to be a highly effective,\nefficient and scalable feature learning tool. In this paper, we propose to\nemulate the sparse coding-based clustering pipeline in the context of deep\nlearning, leading to a carefully crafted deep model benefiting from both. A\nfeed-forward network structure, named TAGnet, is constructed based on a\ngraph-regularized sparse coding algorithm. It is then trained with\ntask-specific loss functions from end to end. We discover that connecting deep\nlearning to sparse coding benefits not only the model performance, but also its\ninitialization and interpretation. Moreover, by introducing auxiliary\nclustering tasks to the intermediate feature hierarchy, we formulate DTAGnet\nand obtain a further performance boost. Extensive experiments demonstrate that\nthe proposed model gains remarkable margins over several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:12:29 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 18:32:27 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 06:38:37 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Zhou", "Jiayu", ""], ["Wang", "Meng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1509.00153", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Qing Ling, Thomas S. Huang", "title": "Learning Deep $\\ell_0$ Encoders", "comments": "Full paper at AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its nonconvex nature, $\\ell_0$ sparse approximation is desirable in\nmany theoretical and application cases. We study the $\\ell_0$ sparse\napproximation problem with the tool of deep learning, by proposing Deep\n$\\ell_0$ Encoders. Two typical forms, the $\\ell_0$ regularized problem and the\n$M$-sparse problem, are investigated. Based on solid iterative algorithms, we\nmodel them as feed-forward neural networks, through introducing novel neurons\nand pooling functions. Enforcing such structural priors acts as an effective\nnetwork regularization. The deep encoders also enjoy faster inference, larger\nlearning capacity, and better scalability compared to conventional sparse\ncoding solutions. Furthermore, under task-driven losses, the models can be\nconveniently optimized from end to end. Numerical results demonstrate the\nimpressive performances of the proposed encoders.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:20:34 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 16:02:43 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Wang", "Zhangyang", ""], ["Ling", "Qing", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1509.00154", "submitter": "Jalil Rasekhi", "authors": "Jalil Rasekhi", "title": "Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and\n  Active Contour", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 5,6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for motion tracking of tumors in liver\nultrasound image sequences. Our algorithm has two main steps. In the first\nstep, we apply mean shift algorithm with multiple features to estimate the\ncenter of the target in each frame. Target in the first frame is defined using\nan ellipse. Edge, texture, and intensity features are extracted from the first\nframe, and then mean shift algorithm is applied to each feature separately to\nfind the center of ellipse related to that feature in the next frame. The\ncenter of ellipse will be the weighted average of these centers. By using mean\nshift actually we estimate the target movement between two consecutive frames.\nOnce the correct ellipsoid in each frame is known, in the second step we apply\nthe Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour\nmodels, in order to find the correct boundary of tumors. We sample a few points\non the boundary of active contour then translate those points based on the\ntranslation of the center of ellipsoid in two consecutive frames to determine\nthe target movement. We use these translated sample points as an initial guess\nfor active contour in the next frame. Our experimental results show that, the\nsuggested method provides a reliable performance for liver tumor tracking in\nultrasound image sequences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:21:44 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 11:22:42 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 21:09:05 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2015 23:15:38 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Rasekhi", "Jalil", ""]]}, {"id": "1509.00519", "submitter": "Yuri Burda", "authors": "Yuri Burda, Roger Grosse, Ruslan Salakhutdinov", "title": "Importance Weighted Autoencoders", "comments": "Submitted to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 22:33:13 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 19:43:20 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 05:47:05 GMT"}, {"version": "v4", "created": "Mon, 7 Nov 2016 17:29:24 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Burda", "Yuri", ""], ["Grosse", "Roger", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1509.00727", "submitter": "Anupama Nandi", "authors": "Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher", "title": "Heavy-tailed Independent Component Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is the problem of efficiently recovering\na matrix $A \\in \\mathbb{R}^{n\\times n}$ from i.i.d. observations of $X=AS$\nwhere $S \\in \\mathbb{R}^n$ is a random vector with mutually independent\ncoordinates. This problem has been intensively studied, but all existing\nefficient algorithms with provable guarantees require that the coordinates\n$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem\nwhere we do not make this assumption, about the second moment. This problem\nalso has received considerable attention in the applied literature. In the\npresent work, we first give a provably efficient algorithm that works under the\nassumption that for constant $\\gamma > 0$, each $S_i$ has finite\n$(1+\\gamma)$-moment, thus substantially weakening the moment requirement\ncondition for the ICA problem to be solvable. We then give an algorithm that\nworks under the assumption that matrix $A$ has orthogonal columns but requires\nno moment assumptions. Our techniques draw ideas from convex geometry and\nexploit standard properties of the multivariate spherical Gaussian distribution\nin a novel way.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:56:22 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Nandi", "Anupama", ""], ["Rademacher", "Luis", ""]]}, {"id": "1509.00728", "submitter": "Florian Bernard", "authors": "Johan Thunberg, Florian Bernard, Jorge Goncalves", "title": "On Transitive Consistency for Linear Invertible Transformations between\n  Euclidean Coordinate Systems", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.MA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transitive consistency is an intrinsic property for collections of linear\ninvertible transformations between Euclidean coordinate frames. In practice,\nwhen the transformations are estimated from data, this property is lacking.\nThis work addresses the problem of synchronizing transformations that are not\ntransitively consistent. Once the transformations have been synchronized, they\nsatisfy the transitive consistency condition - a transformation from frame $A$\nto frame $C$ is equal to the composite transformation of first transforming A\nto B and then transforming B to C. The coordinate frames correspond to nodes in\na graph and the transformations correspond to edges in the same graph. Two\ndirect or centralized synchronization methods are presented for different graph\ntopologies; the first one for quasi-strongly connected graphs, and the second\none for connected graphs. As an extension of the second method, an iterative\nGauss-Newton method is presented, which is later adapted to the case of affine\nand Euclidean transformations. Two distributed synchronization methods are also\npresented for orthogonal matrices, which can be seen as distributed versions of\nthe two direct or centralized methods; they are similar in nature to standard\nconsensus protocols used for distributed averaging. When the transformations\nare orthogonal matrices, a bound on the optimality gap can be computed.\nSimulations show that the gap is almost right, even for noise large in\nmagnitude. This work also contributes on a theoretical level by providing\nlinear algebraic relationships for transitively consistent transformations. One\nof the benefits of the proposed methods is their simplicity - basic linear\nalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a\nwide range of parameter settings, the methods are numerically validated.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:57:16 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Thunberg", "Johan", ""], ["Bernard", "Florian", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1509.00825", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti", "title": "A DEEP analysis of the META-DES framework for dynamic selection of\n  ensemble of classifiers", "comments": "47 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic ensemble selection (DES) techniques work by estimating the level of\ncompetence of each classifier from a pool of classifiers. Only the most\ncompetent ones are selected to classify a given test sample. Hence, the key\nissue in DES is the criterion used to estimate the level of competence of the\nclassifiers in predicting the label of a given test sample. In order to perform\na more robust ensemble selection, we proposed the META-DES framework using\nmeta-learning, where multiple criteria are encoded as meta-features and are\npassed down to a meta-classifier that is trained to estimate the competence\nlevel of a given classifier. In this technical report, we present a\nstep-by-step analysis of each phase of the framework during training and test.\nWe show how each set of meta-features is extracted as well as their impact on\nthe estimation of the competence level of the base classifier. Moreover, an\nanalysis of the impact of several factors in the system performance, such as\nthe number of classifiers in the pool, the use of different linear base\nclassifiers, as well as the size of the validation data. We show that using the\ndynamic selection of linear classifiers through the META-DES framework, we can\nsolve complex non-linear classification problems where other combination\ntechniques such as AdaBoost cannot.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 19:14:47 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 02:30:27 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1509.00980", "submitter": "Mike Ludkovski", "authors": "Ruimeng Hu and Mike Ludkovski", "title": "Sequential Design for Ranking Response Surfaces", "comments": "26 pages, 7 figures (updated several sections and figures)", "journal-ref": null, "doi": "10.1137/15M1045168", "report-no": null, "categories": "stat.ML q-fin.CP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze sequential design methods for the problem of ranking\nseveral response surfaces. Namely, given $L \\ge 2$ response surfaces over a\ncontinuous input space $\\cal X$, the aim is to efficiently find the index of\nthe minimal response across the entire $\\cal X$. The response surfaces are not\nknown and have to be noisily sampled one-at-a-time. This setting is motivated\nby stochastic control applications and requires joint experimental design both\nin space and response-index dimensions. To generate sequential design\nheuristics we investigate stepwise uncertainty reduction approaches, as well as\nsampling based on posterior classification complexity. We also make connections\nbetween our continuous-input formulation and the discrete framework of pure\nregret in multi-armed bandits. To model the response surfaces we utilize\nkriging surrogates. Several numerical examples using both synthetic data and an\nepidemics control problem are provided to illustrate our approach and the\nefficacy of respective adaptive designs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 08:27:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 22:17:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Hu", "Ruimeng", ""], ["Ludkovski", "Mike", ""]]}, {"id": "1509.01004", "submitter": "Yohei  Kondo", "authors": "Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda", "title": "Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy for sparse linear regression is to introduce\nregularization, which eliminates irrelevant features by letting the\ncorresponding weights be zeros. However, regularization often shrinks the\nestimator for relevant features, which leads to incorrect feature selection.\nMotivated by the above-mentioned issue, we propose Bayesian masking (BM), a\nsparse estimation method which imposes no regularization on the weights. The\nkey concept of BM is to introduce binary latent variables that randomly mask\nfeatures. Estimating the masking rates determines the relevance of the features\nautomatically. We derive a variational Bayesian inference algorithm that\nmaximizes the lower bound of the factorized information criterion (FIC), which\nis a recently developed asymptotic criterion for evaluating the marginal\nlog-likelihood. In addition, we propose reparametrization to accelerate the\nconvergence of the derived algorithm. Finally, we show that BM outperforms\nLasso and automatic relevance determination (ARD) in terms of the\nsparsity-shrinkage trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:35:48 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 06:07:55 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Kondo", "Yohei", ""], ["Hayashi", "Kohei", ""], ["Maeda", "Shin-ichi", ""]]}, {"id": "1509.01168", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence", "title": "Semi-described and semi-supervised learning with Gaussian processes", "comments": "Published in the proceedings for Uncertainty in Artificial\n  Intelligence (UAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propagating input uncertainty through non-linear Gaussian process (GP)\nmappings is intractable. This hinders the task of training GPs using uncertain\nand partially observed inputs. In this paper we refer to this task as\n\"semi-described learning\". We then introduce a GP framework that solves both,\nthe semi-described and the semi-supervised learning problems (where missing\nvalues occur in the outputs). Auto-regressive state space simulation is also\nrecognised as a special case of semi-described learning. To achieve our goal we\ndevelop variational methods for handling semi-described inputs in GPs, and\ncouple them with algorithms that allow for imputing the missing values while\ntreating the uncertainty in a principled, Bayesian manner. Extensive\nexperiments on simulated and real-world data study the problems of iterative\nforecasting and regression/classification with missing values. The results\nsuggest that the principled propagation of uncertainty stemming from our\nframework can significantly improve performance in these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 17:22:15 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1509.01173", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Elizaveta Levina, Ji Zhu", "title": "Community Detection in Networks with Node Features", "comments": "16 pages, 5 pages", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3153-3178", "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been proposed for community detection in networks, but most\nof them do not take into account additional information on the nodes that is\noften available in practice. In this paper, we propose a new joint community\ndetection criterion that uses both the network edge information and the node\nfeatures to detect community structures. One advantage our method has over\nexisting joint detection approaches is the flexibility of learning the impact\nof different features which may differ across communities. Another advantage is\nthe flexibility of choosing the amount of influence the feature information has\non communities. The method is asymptotically consistent under the block model\nwith additional assumptions on the feature distributions, and performs well on\nsimulated and real networks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 17:37:08 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Zhang", "Yuan", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1509.01199", "submitter": "Erika Fille Legara", "authors": "Erika Fille Legara and Christopher Monterola", "title": "Inferring Passenger Type from Commuter Eigentravel Matrices", "comments": "14 pages, 7 figures. Preprint submitted to Elsevier and is currently\n  under review. An earlier version of this work (contributed as an extended\n  abstract) has been accepted for presentation at the 2015 Conference on\n  Complex Systems in Phoenix, Arizona, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sufficient knowledge of the demographics of a commuting public is essential\nin formulating and implementing more targeted transportation policies, as\ncommuters exhibit different ways of traveling. With the advent of the Automated\nFare Collection system (AFC), probing the travel patterns of commuters has\nbecome less invasive and more accessible. Consequently, numerous transport\nstudies related to human mobility have shown that these observed patterns allow\none to pair individuals with locations and/or activities at certain times of\nthe day. However, classifying commuters using their travel signatures is yet to\nbe thoroughly examined.\n  Here, we contribute to the literature by demonstrating a procedure to\ncharacterize passenger types (Adult, Child/Student, and Senior Citizen) based\non their three-month travel patterns taken from a smart fare card system. We\nfirst establish a method to construct distinct commuter matrices, which we\nrefer to as eigentravel matrices, that capture the characteristic travel\nroutines of individuals. From the eigentravel matrices, we build classification\nmodels that predict the type of passengers traveling. Among the models\nexplored, the gradient boosting method (GBM) gives the best prediction accuracy\nat 76%, which is 84% better than the minimum model accuracy (41%) required\nvis-\\`a-vis the proportional chance criterion. In addition, we find that travel\nfeatures generated during weekdays have greater predictive power than those on\nweekends. This work should not only be useful for transport planners, but for\nmarket researchers as well. With the awareness of which commuter types are\ntraveling, ads, service announcements, and surveys, among others, can be made\nmore targeted spatiotemporally. Finally, our framework should be effective in\ncreating synthetic populations for use in real-world simulations that involve a\nmetropolitan's public transport system.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 16:10:08 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Legara", "Erika Fille", ""], ["Monterola", "Christopher", ""]]}, {"id": "1509.01228", "submitter": "Philip Graff", "authors": "Philip B Graff, Amy Y Lien, John G Baker, Takanori Sakamoto", "title": "Machine Learning Model of the Swift/BAT Trigger Algorithm for Long GRB\n  Population Studies", "comments": "16 pages, 18 figures, 5 tables, published by ApJ", "journal-ref": "ApJ, 818, 55 (2016)", "doi": "10.3847/0004-637X/818/1/55", "report-no": null, "categories": "astro-ph.HE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To draw inferences about gamma-ray burst (GRB) source populations based on\nSwift observations, it is essential to understand the detection efficiency of\nthe Swift burst alert telescope (BAT). This study considers the problem of\nmodeling the Swift/BAT triggering algorithm for long GRBs, a computationally\nexpensive procedure, and models it using machine learning algorithms. A large\nsample of simulated GRBs from Lien 2014 is used to train various models: random\nforests, boosted decision trees (with AdaBoost), support vector machines, and\nartificial neural networks. The best models have accuracies of $\\gtrsim97\\%$\n($\\lesssim 3\\%$ error), which is a significant improvement on a cut in GRB flux\nwhich has an accuracy of $89.6\\%$ ($10.4\\%$ error). These models are then used\nto measure the detection efficiency of Swift as a function of redshift $z$,\nwhich is used to perform Bayesian parameter estimation on the GRB rate\ndistribution. We find a local GRB rate density of $n_0 \\sim\n0.48^{+0.41}_{-0.23} \\ {\\rm Gpc}^{-3} {\\rm yr}^{-1}$ with power-law indices of\n$n_1 \\sim 1.7^{+0.6}_{-0.5}$ and $n_2 \\sim -5.9^{+5.7}_{-0.1}$ for GRBs above\nand below a break point of $z_1 \\sim 6.8^{+2.8}_{-3.2}$. This methodology is\nable to improve upon earlier studies by more accurately modeling Swift\ndetection and using this for fully Bayesian model fitting. The code used in\nthis is analysis is publicly available online\n(https://github.com/PBGraff/SwiftGRB_PEanalysis).\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 19:30:00 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 01:56:57 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Graff", "Philip B", ""], ["Lien", "Amy Y", ""], ["Baker", "John G", ""], ["Sakamoto", "Takanori", ""]]}, {"id": "1509.01240", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Benjamin Recht and Yoram Singer", "title": "Train faster, generalize better: Stability of stochastic gradient\n  descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that parametric models trained by a stochastic gradient method (SGM)\nwith few iterations have vanishing generalization error. We prove our results\nby arguing that SGM is algorithmically stable in the sense of Bousquet and\nElisseeff. Our analysis only employs elementary tools from convex and\ncontinuous optimization. We derive stability bounds for both convex and\nnon-convex optimization under standard Lipschitz and smoothness assumptions.\n  Applying our results to the convex case, we provide new insights for why\nmultiple epochs of stochastic gradient methods generalize well in practice. In\nthe non-convex case, we give a new interpretation of common practices in neural\nnetworks, and formally show that popular techniques for training large deep\nmodels are indeed stability-promoting. Our findings conceptually underscore the\nimportance of reducing training time beyond its obvious benefit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 19:53:40 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 17:06:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Hardt", "Moritz", ""], ["Recht", "Benjamin", ""], ["Singer", "Yoram", ""]]}, {"id": "1509.01323", "submitter": "Yi Guo", "authors": "Xia Hong, Sheng Chen, Yi Guo, Junbin Gao", "title": "l1-norm Penalized Orthogonal Forward Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm is\nproposed based on the concept of leaveone- out mean square error (LOOMSE).\nFirstly, a new l1-norm penalized cost function is defined in the constructed\northogonal space, and each orthogonal basis is associated with an individually\ntunable regularization parameter. Secondly, due to orthogonal computation, the\nLOOMSE can be analytically computed without actually splitting the data set,\nand moreover a closed form of the optimal regularization parameter in terms of\nminimal LOOMSE is derived. Thirdly, a lower bound for regularization parameters\nis proposed, which can be used for robust LOOMSE estimation by adaptively\ndetecting and removing regressors to an inactive set so that the computational\ncost of the algorithm is significantly reduced. Illustrative examples are\nincluded to demonstrate the effectiveness of this new l1-POFR approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 01:45:09 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Hong", "Xia", ""], ["Chen", "Sheng", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""]]}, {"id": "1509.01386", "submitter": "Jawwad Ahmed Dr.", "authors": "Jawwad Ahmed, Andreas Johnsson, Rerngvit Yanggratoke, John Ardelius,\n  Christofer Flinta, Rolf Stadler", "title": "Predicting SLA Violations in Real Time using Online Machine Learning", "comments": "8 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting faults and SLA violations in a timely manner is critical for\ntelecom providers, in order to avoid loss in business, revenue and reputation.\nAt the same time predicting SLA violations for user services in telecom\nenvironments is difficult, due to time-varying user demands and infrastructure\nload conditions.\n  In this paper, we propose a service-agnostic online learning approach,\nwhereby the behavior of the system is learned on the fly, in order to predict\nclient-side SLA violations. The approach uses device-level metrics, which are\ncollected in a streaming fashion on the server side.\n  Our results show that the approach can produce highly accurate predictions\n(>90% classification accuracy and < 10% false alarm rate) in scenarios where\nSLA violations are predicted for a video-on-demand service under changing load\npatterns. The paper also highlight the limitations of traditional offline\nlearning methods, which perform significantly worse in many of the considered\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 09:54:48 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Ahmed", "Jawwad", ""], ["Johnsson", "Andreas", ""], ["Yanggratoke", "Rerngvit", ""], ["Ardelius", "John", ""], ["Flinta", "Christofer", ""], ["Stadler", "Rolf", ""]]}, {"id": "1509.01404", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon", "title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization", "comments": "25 pages, 5 figures, 7 tables. Main changes: comparison with another\n  symNMF algorithm (namely, BetaSNMF), and correction of an error in the\n  convergence proof", "journal-ref": "IEEE Transactions on Signal Processing 64 (21), pp. 5571-5584,\n  2016", "doi": "10.1109/TSP.2016.2591510", "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 11:19:35 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 12:50:38 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Lei", "Qi", ""], ["Zhong", "Kai", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1509.01469", "submitter": "Ruiqi Guo", "authors": "Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski and David Simcha", "title": "Quantization based Fast Inner Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quantization based approach for fast approximate Maximum Inner\nProduct Search (MIPS). Each database vector is quantized in multiple subspaces\nvia a set of codebooks, learned directly by minimizing the inner product\nquantization error. Then, the inner product of a query to a database vector is\napproximated as the sum of inner products with the subspace quantizers.\nDifferent from recently proposed LSH approaches to MIPS, the database vectors\nand queries do not need to be augmented in a higher dimensional feature space.\nWe also provide a theoretical analysis of the proposed approach, consisting of\nthe concentration results under mild assumptions. Furthermore, if a small\nsample of example queries is given at the training time, we propose a modified\ncodebook learning procedure which further improves the accuracy. Experimental\nresults on a variety of datasets including those arising from deep neural\nnetworks show that the proposed approach significantly outperforms the existing\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 14:43:11 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["Choromanski", "Krzysztof", ""], ["Simcha", "David", ""]]}, {"id": "1509.01509", "submitter": "Radu Horaud P", "authors": "Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes and Radu\n  Horaud", "title": "EM Algorithms for Weighted-Data Clustering with Application to\n  Audio-Visual Scene Analysis", "comments": "14 pages, 4 figures, 4 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  volume 38, number 12, 2402 - 2415, 2016", "doi": "10.1109/TPAMI.2016.2522425", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering has received a lot of attention and numerous methods,\nalgorithms and software packages are available. Among these techniques,\nparametric finite-mixture models play a central role due to their interesting\nmathematical properties and to the existence of maximum-likelihood estimators\nbased on expectation-maximization (EM). In this paper we propose a new mixture\nmodel that associates a weight with each observed point. We introduce the\nweighted-data Gaussian mixture and we derive two EM algorithms. The first one\nconsiders a fixed weight for each observation. The second one treats each\nweight as a random variable following a gamma distribution. We propose a model\nselection method based on a minimum message length criterion, provide a weight\ninitialization strategy, and validate the proposed algorithms by comparing them\nwith several state of the art parametric and non-parametric clustering\ntechniques. We also demonstrate the effectiveness and robustness of the\nproposed clustering technique in the presence of heterogeneous data, namely\naudio-visual scene analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 15:51:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 11:17:13 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gebru", "Israel D.", ""], ["Alameda-Pineda", "Xavier", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "1509.01520", "submitter": "Radu Horaud P", "authors": "Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero and Radu Horaud", "title": "An On-line Variational Bayesian Model for Multi-Person Tracking from\n  Cluttered Scenes", "comments": "21 pages, 9 figures, 4 tables", "journal-ref": "Computer Vision and Image Understanding, volume 153, December\n  2016, pages 64-76", "doi": "10.1016/j.cviu.2016.07.006", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is an ubiquitous problem that appears in many applications\nsuch as remote sensing, audio processing, computer vision, human-machine\ninterfaces, human-robot interaction, etc. Although thoroughly investigated in\ncomputer vision, tracking a time-varying number of persons remains a\nchallenging open problem. In this paper, we propose an on-line variational\nBayesian model for multi-person tracking from cluttered visual observations\nprovided by person detectors. The contributions of this paper are the\nfollowings. First, we propose a variational Bayesian framework for tracking an\nunknown and varying number of persons. Second, our model results in a\nvariational expectation-maximization (VEM) algorithm with closed-form\nexpressions for the posterior distributions of the latent variables and for the\nestimation of the model parameters. Third, the proposed model exploits\nobservations from multiple detectors, and it is therefore multimodal by nature.\nFinally, we propose to embed both object-birth and object-visibility processes\nin an effort to robustly handle person appearances and disappearances over\ntime. Evaluated on classical multiple person tracking datasets, our method\nshows competitive results with respect to state-of-the-art multiple-object\ntracking models, such as the probability hypothesis density (PHD) filter among\nothers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 16:16:42 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:06:59 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 08:50:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Ba", "Sileye", ""], ["Alameda-Pineda", "Xavier", ""], ["Xompero", "Alessio", ""], ["Horaud", "Radu", ""]]}, {"id": "1509.01546", "submitter": "David Hofmeyr", "authors": "David P. Hofmeyr and Nicos G. Pavlidis and Idris A. Eckley", "title": "Minimum Spectral Connectivity Projection Pursuit", "comments": null, "journal-ref": "Statistics and Computing (2019) 29: 391", "doi": "10.1007/s11222-018-9814-6", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of determining the optimal low dimensional projection\nfor maximising the separability of a binary partition of an unlabelled dataset,\nas measured by spectral graph theory. This is achieved by finding projections\nwhich minimise the second eigenvalue of the graph Laplacian of the projected\ndata, which corresponds to a non-convex, non-smooth optimisation problem. We\nshow that the optimal univariate projection based on spectral connectivity\nconverges to the vector normal to the maximum margin hyperplane through the\ndata, as the scaling parameter is reduced to zero. This establishes a\nconnection between connectivity as measured by spectral graph theory and\nmaximal Euclidean separation. The computational cost associated with each\neigen-problem is quadratic in the number of data. To mitigate this issue, we\npropose an approximation method using microclusters with provable approximation\nerror bounds. Combining multiple binary partitions within a divisive\nhierarchical model allows us to construct clustering solutions admitting\nclusters with varying scales and lying within different subspaces. We evaluate\nthe performance of the proposed method on a large collection of benchmark\ndatasets and find that it compares favourably with existing methods for\nprojection pursuit and dimension reduction for data clustering.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 18:06:31 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 21:31:02 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:45:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Hofmeyr", "David P.", ""], ["Pavlidis", "Nicos G.", ""], ["Eckley", "Idris A.", ""]]}, {"id": "1509.01604", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Juan Kalemkerian, Pamela Llop", "title": "A nonlinear aggregation type classifier", "comments": "arXiv admin note: text overlap with arXiv:1411.2687", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonlinear aggregation type classifier for functional data\ndefined on a separable and complete metric space. The new rule is built up from\na collection of $M$ arbitrary training classifiers. If the classifiers are\nconsistent, then so is the aggregation rule. Moreover, asymptotically the\naggregation rule behaves as well as the best of the $M$ classifiers. The\nresults of a small simulation are reported both, for high dimensional and\nfunctional data, and a real data example is analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 20:58:20 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 01:44:03 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Kalemkerian", "Juan", ""], ["Llop", "Pamela", ""]]}, {"id": "1509.01631", "submitter": "David A. Knowles", "authors": "David A. Knowles", "title": "Stochastic gradient variational Bayes for gamma approximating\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While stochastic variational inference is relatively well known for scaling\ninference in Bayesian probabilistic models, related methods also offer ways to\ncircumnavigate the approximation of analytically intractable expectations. The\nkey challenge in either setting is controlling the variance of gradient\nestimates: recent work has shown that for continuous latent variables,\nparticularly multivariate Gaussians, this can be achieved by using the gradient\nof the log posterior. In this paper we apply the same idea to gamma distributed\nlatent variables given gamma variational distributions, enabling\nstraightforward \"black box\" variational inference in models where sparsity and\nnon-negativity are appropriate. We demonstrate the method on a recently\nproposed gamma process model for network data, as well as a novel sparse factor\nanalysis. We outperform generic sampling algorithms and the approach of using\nGaussian variational distributions on transformed variables.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 23:13:27 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Knowles", "David A.", ""]]}, {"id": "1509.01698", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Kamer Kaya, Figen \\\"Oztoprak, \\c{S}. \\.Ilker Birbil, A. Taylan Cemgil,\n  Umut \\c{S}im\\c{s}ekli, Nurdan Kuru, Hazal Koptagel, M. Kaan \\\"Ozt\\\"urk", "title": "HAMSI: A Parallel Incremental Optimization Algorithm Using Quadratic\n  Approximations for Solving Partially Separable Problems", "comments": "The software is available at https://github.com/spartensor/hamsi-mf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HAMSI (Hessian Approximated Multiple Subsets Iteration), which is\na provably convergent, second order incremental algorithm for solving\nlarge-scale partially separable optimization problems. The algorithm is based\non a local quadratic approximation, and hence, allows incorporating curvature\ninformation to speed-up the convergence. HAMSI is inherently parallel and it\nscales nicely with the number of processors. Combined with techniques for\neffectively utilizing modern parallel computer architectures, we illustrate\nthat the proposed method converges more rapidly than a parallel stochastic\ngradient descent when both methods are used to solve large-scale matrix\nfactorization problems. This performance gain comes only at the expense of\nusing memory that scales linearly with the total size of the optimization\nvariables. We conclude that HAMSI may be considered as a viable alternative in\nmany large scale problems, where first order methods based on variants of\nstochastic gradient descent are applicable.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 12:48:01 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 12:15:23 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 15:09:50 GMT"}, {"version": "v4", "created": "Fri, 4 Aug 2017 04:37:32 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Kaya", "Kamer", ""], ["\u00d6ztoprak", "Figen", ""], ["Birbil", "\u015e. \u0130lker", ""], ["Cemgil", "A. Taylan", ""], ["\u015eim\u015fekli", "Umut", ""], ["Kuru", "Nurdan", ""], ["Koptagel", "Hazal", ""], ["\u00d6zt\u00fcrk", "M. Kaan", ""]]}, {"id": "1509.01770", "submitter": "Kishan Wimalawarne", "authors": "Kishan Wimalawarne, Ryota Tomioka and Masashi Sugiyama", "title": "Theoretical and Experimental Analyses of Tensor-Based Regression and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically and experimentally investigate tensor-based regression and\nclassification. Our focus is regularization with various tensor norms,\nincluding the overlapped trace norm, the latent trace norm, and the scaled\nlatent trace norm. We first give dual optimization methods using the\nalternating direction method of multipliers, which is computationally efficient\nwhen the number of training samples is moderate. We then theoretically derive\nan excess risk bound for each tensor norm and clarify their behavior. Finally,\nwe perform extensive experiments using simulated and real data and demonstrate\nthe superiority of tensor-based learning methods over vector- and matrix-based\nlearning methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 05:03:27 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Wimalawarne", "Kishan", ""], ["Tomioka", "Ryota", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1509.02088", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Aky{\\i}ld{\\i}z", "title": "Matrix Factorisation with Linear Filters", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text investigates relations between two well-known family of algorithms,\nmatrix factorisations and recursive linear filters, by describing a\nprobabilistic model in which approximate inference corresponds to a matrix\nfactorisation algorithm. Using the probabilistic model, we derive a matrix\nfactorisation algorithm as a recursive linear filter. More precisely, we derive\na matrix-variate recursive linear filter in order to perform efficient\ninference in high dimensions. We also show that it is possible to interpret our\nalgorithm as a nontrivial stochastic gradient algorithm. Demonstrations and\ncomparisons on an image restoration task are given.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 15:47:39 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Aky\u0131ld\u0131z", "\u00d6mer Deniz", ""]]}, {"id": "1509.02116", "submitter": "Rong Zhu", "authors": "Rong Zhu", "title": "Poisson Subsampling Algorithms for Large Sample Linear Regression in\n  Massive Data", "comments": "This paper has been withdrawn by the author due to an improper\n  citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large sample size brings the computation bottleneck for modern data analysis.\nSubsampling is one of efficient strategies to handle this problem. In previous\nstudies, researchers make more fo- cus on subsampling with replacement (SSR)\nthan on subsampling without replacement (SSWR). In this paper we investigate a\nkind of SSWR, poisson subsampling (PSS), for fast algorithm in ordinary\nleast-square problem. We establish non-asymptotic property, i.e, the error\nbound of the correspond- ing subsample estimator, which provide a tradeoff\nbetween computation cost and approximation efficiency. Besides the\nnon-asymptotic result, we provide asymptotic consistency and normality of the\nsubsample estimator. Methodologically, we propose a two-step subsampling\nalgorithm, which is efficient with respect to a statistical objective and\nindependent on the linear model assumption.. Synthetic and real data are used\nto empirically study our proposed subsampling strategies. We argue by these\nempirical studies that, (1) our proposed two-step algorithm has obvious\nadvantage when the assumed linear model does not accurate, and (2) the PSS\nstrategy performs obviously better than SSR when the subsampling ratio\nincreases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 16:46:56 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 15:24:02 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 08:31:43 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zhu", "Rong", ""]]}, {"id": "1509.02216", "submitter": "Benjamin Nachman", "authors": "Lester Mackey, Benjamin Nachman, Ariel Schwartzman, and Conrad\n  Stansbury", "title": "Fuzzy Jets", "comments": null, "journal-ref": "JHEP 06 (2016) 010", "doi": "10.1007/JHEP06(2016)010", "report-no": null, "categories": "hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collimated streams of particles produced in high energy physics experiments\nare organized using clustering algorithms to form jets. To construct jets, the\nexperimental collaborations based at the Large Hadron Collider (LHC) primarily\nuse agglomerative hierarchical clustering schemes known as sequential\nrecombination. We propose a new class of algorithms for clustering jets that\nuse infrared and collinear safe mixture models. These new algorithms, known as\nfuzzy jets, are clustered using maximum likelihood techniques and can\ndynamically determine various properties of jets like their size. We show that\nthe fuzzy jet size adds additional information to conventional jet tagging\nvariables. Furthermore, we study the impact of pileup and show that with some\nslight modifications to the algorithm, fuzzy jets can be stable up to high\npileup interaction multiplicities.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:49:43 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Mackey", "Lester", ""], ["Nachman", "Benjamin", ""], ["Schwartzman", "Ariel", ""], ["Stansbury", "Conrad", ""]]}, {"id": "1509.02237", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Nicolas Garcia, Marco Cuturi", "title": "On Wasserstein Two Sample Testing and Related Families of Nonparametric\n  Tests", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample or homogeneity testing is a decision theoretic\nproblem that involves identifying differences between two random variables\nwithout making parametric assumptions about their underlying distributions. The\nliterature is old and rich, with a wide variety of statistics having being\nintelligently designed and analyzed, both for the unidimensional and the\nmultivariate setting. Our contribution is to tie together many of these tests,\ndrawing connections between seemingly very different statistics. In this work,\nour central object is the Wasserstein distance, as we form a chain of\nconnections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ\nplots and ROC/ODC curves, to multivariate tests involving energy statistics and\nkernel based maximum mean discrepancy. Some connections proceed through the\nconstruction of a \\textit{smoothed} Wasserstein distance, and others through\nthe pursuit of a \"distribution-free\" Wasserstein test. Some observations in\nthis chain are implicit in the literature, while others seem to have not been\nnoticed thus far. Given nonparametric two sample testing's classical and\ncontinued importance, we aim to provide useful connections for theorists and\npractitioners familiar with one subset of methods but not others.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 01:08:04 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 01:42:46 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Garcia", "Nicolas", ""], ["Cuturi", "Marco", ""]]}, {"id": "1509.02314", "submitter": "Shenjian Zhao", "authors": "Shenjian Zhao, Cong Xie, Zhihua Zhang", "title": "A Scalable and Extensible Framework for Superposition-Structured Models", "comments": null, "journal-ref": "AAAI 2016: 2372-2378", "doi": null, "report-no": null, "categories": "cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning tasks, structural models usually lead to better\ninterpretability and higher generalization performance. In recent years,\nhowever, the simple structural models such as lasso are frequently proved to be\ninsufficient. Accordingly, there has been a lot of work on\n\"superposition-structured\" models where multiple structural constraints are\nimposed. To efficiently solve these \"superposition-structured\" statistical\nmodels, we develop a framework based on a proximal Newton-type method.\nEmploying the smoothed conic dual approach with the LBFGS updating formula, we\npropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.\nEmpirical analysis on various datasets shows that our framework is potentially\npowerful, and achieves super-linear convergence rate for optimizing some\npopular \"superposition-structured\" statistical models such as the fused sparse\ngroup lasso.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:33:27 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 04:29:43 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhao", "Shenjian", ""], ["Xie", "Cong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1509.02347", "submitter": "Fabrice Rossi", "authors": "Marco Corneli (SAMM), Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Modelling time evolving interactions in networks through a non\n  stationary extension of stochastic block models", "comments": null, "journal-ref": "47\\`emes Journ\\'ees de Statistique de la SFdS, Jun 2015, Lille,\n  France. 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the stochastic block model (SBM),a probabilistic\ntool describing interactions between nodes of a network using latent clusters.\nThe SBM assumes that the networkhas a stationary structure, in which\nconnections of time varying intensity are not taken into account. In other\nwords, interactions between two groups are forced to have the same features\nduring the whole observation time. To overcome this limitation,we propose a\npartition of the whole time horizon, in which interactions are observed, and\ndevelop a non stationary extension of the SBM,allowing to simultaneously\ncluster the nodes in a network along with fixed time intervals in which the\ninteractions take place. The number of clusters (K for nodes, D for time\nintervals) as well as the class memberships are finallyobtained through\nmaximizing the complete-data integrated likelihood by means of a greedy search\napproach. After showing that the model works properly with simulated data, we\nfocus on a real data set. We thus consider the three days ACM Hypertext\nconference held in Turin,June 29th - July 1st 2009. Proximity interactions\nbetween attendees during the first day are modelled and an\ninterestingclustering of the daily hours is finally obtained, with times of\nsocial gathering (e.g. coffee breaks) recovered by the approach. Applications\nto large networks are limited due to the computational complexity of the greedy\nsearch which is dominated bythe number $K\\_{max}$ and $D\\_{max}$ of clusters\nused in the initialization. Therefore,advanced clustering tools are considered\nto reduce the number of clusters expected in the data, making the greedy search\napplicable to large networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 12:59:19 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Corneli", "Marco", "", "SAMM"], ["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1509.02348", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "On the complexity of piecewise affine system identification", "comments": "Automatica, International Federation of Automatic Control, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides results regarding the computational complexity of hybrid\nsystem identification. More precisely, we focus on the estimation of piecewise\naffine (PWA) maps from input-output data and analyze the complexity of\ncomputing a global minimizer of the error. Previous work showed that a global\nsolution could be obtained for continuous PWA maps with a worst-case complexity\nexponential in the number of data. In this paper, we show how global optimality\ncan be reached for a slightly more general class of possibly discontinuous PWA\nmaps with a complexity only polynomial in the number of data, however with an\nexponential complexity with respect to the data dimension. This result is\nobtained via an analysis of the intrinsic classification subproblem of\nassociating the data points to the different modes. In addition, we prove that\nthe problem is NP-hard, and thus that the exponential complexity in the\ndimension is a natural expectation for any exact algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 13:03:19 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1509.02357", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (Viadeo, SAMM), B\\'en\\'edicte Le Grand (CRI),\n  Fabrice Rossi (SAMM)", "title": "Empirical risk minimization is consistent with the mean absolute\n  percentage error", "comments": "in French, 47\\`emes Journ\\'ees de Statistique de la SFdS, Jun 2015,\n  Lille, France. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We show that\nfinding the best model under the MAPE is equivalent to doing weighted Mean\nAbsolute Error (MAE) regression. We also show that, under some asumptions,\nuniversal consistency of Empirical Risk Minimization remains possible using the\nMAPE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 13:17:28 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["De Myttenaere", "Arnaud", "", "Viadeo, SAMM"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1509.02438", "submitter": "Arnold Salas", "authors": "Arnold Salas and Stephen J. Roberts and Michael A. Osborne", "title": "A Variational Bayesian State-Space Approach to Online Passive-Aggressive\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Passive-Aggressive (PA) learning is a class of online margin-based\nalgorithms suitable for a wide range of real-time prediction tasks, including\nclassification and regression. PA algorithms are formulated in terms of\ndeterministic point-estimation problems governed by a set of user-defined\nhyperparameters: the approach fails to capture model/prediction uncertainty and\nmakes their performance highly sensitive to hyperparameter configurations. In\nthis paper, we introduce a novel PA learning framework for regression that\novercomes the above limitations. We contribute a Bayesian state-space\ninterpretation of PA regression, along with a novel online variational\ninference scheme, that not only produces probabilistic predictions, but also\noffers the benefit of automatic hyperparameter tuning. Experiments with various\nreal-world data sets show that our approach performs significantly better than\na more standard, linear Gaussian state-space model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 16:42:39 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Salas", "Arnold", ""], ["Roberts", "Stephen J.", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1509.02805", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously in 2014, we proposed the Nearest Descent (ND) method, capable of\ngenerating an efficient Graph, called the in-tree (IT). Due to some beautiful\nand effective features, this IT structure proves well suited for data\nclustering. Although there exist some redundant edges in IT, they usually have\nsalient features and thus it is not hard to remove them.\n  Subsequently, in order to prevent the seemingly redundant edges from\noccurring, we proposed the Nearest Neighbor Descent (NND) by adding the\n\"Neighborhood\" constraint on ND. Consequently, clusters automatically emerged,\nwithout the additional requirement of removing the redundant edges. However,\nNND proved still not perfect, since it brought in a new yet worse problem, the\n\"over-partitioning\" problem.\n  Now, in this paper, we propose a method, called the Hierarchical Nearest\nNeighbor Descent (H-NND), which overcomes the over-partitioning problem of NND\nvia using the hierarchical strategy. Specifically, H-NND uses ND to effectively\nmerge the over-segmented sub-graphs or clusters that NND produces. Like ND,\nH-NND also generates the IT structure, in which the redundant edges once again\nappear. This seemingly comes back to the situation that ND faces. However,\ncompared with ND, the redundant edges in the IT structure generated by H-NND\ngenerally become more salient, thus being much easier and more reliable to be\nidentified even by the simplest edge-removing method which takes the edge\nlength as the only measure. In other words, the IT structure constructed by\nH-NND becomes more fitted for data clustering. We prove this on several\nclustering datasets of varying shapes, dimensions and attributes. Besides,\ncompared with ND, H-NND generally takes less computation time to construct the\nIT data structure for the input data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 15:15:44 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:43:25 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:50:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1509.02857", "submitter": "Michael Katehakis", "authors": "Apostolos N. Burnetas and Odysseas Kanavetas and Michael N. Katehakis", "title": "Asymptotically Optimal Multi-Armed Bandit Policies under a Cost\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop asymptotically optimal policies for the multi armed bandit (MAB),\nproblem, under a cost constraint. This model is applicable in situations where\neach sample (or activation) from a population (bandit) incurs a known bandit\ndependent cost. Successive samples from each population are iid random\nvariables with unknown distribution. The objective is to design a feasible\npolicy for deciding from which population to sample from, so as to maximize the\nexpected sum of outcomes of $n$ total samples or equivalently to minimize the\nregret due to lack on information on sample distributions, For this problem we\nconsider the class of feasible uniformly fast (f-UF) convergent policies, that\nsatisfy the cost constraint sample-path wise. We first establish a necessary\nasymptotic lower bound for the rate of increase of the regret function of f-UF\npolicies. Then we construct a class of f-UF policies and provide conditions\nunder which they are asymptotically optimal within the class of f-UF policies,\nachieving this asymptotic lower bound. At the end we provide the explicit form\nof such policies for the case in which the unknown distributions are Normal\nwith unknown means and known variances.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 17:27:19 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 13:32:44 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 15:00:47 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Burnetas", "Apostolos N.", ""], ["Kanavetas", "Odysseas", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1509.02866", "submitter": "Kai Fan", "authors": "Kai Fan, Ziteng Wang, Jeff Beck, James Kwok and Katherine Heller", "title": "Fast Second-Order Stochastic Backpropagation for Variational Inference", "comments": "Accepted by NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a second-order (Hessian or Hessian-free) based optimization method\nfor variational inference inspired by Gaussian backpropagation, and argue that\nquasi-Newton optimization can be developed as well. This is accomplished by\ngeneralizing the gradient computation in stochastic backpropagation via a\nreparametrization trick with lower complexity. As an illustrative example, we\napply this approach to the problems of Bayesian logistic regression and\nvariational auto-encoder (VAE). Additionally, we compute bounds on the\nestimator variance of intractable expectations for the family of Lipschitz\ncontinuous function. Our method is practical, scalable and model free. We\ndemonstrate our method on several real-world datasets and provide comparisons\nwith other stochastic gradient methods to show substantial enhancement in\nconvergence rates.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 17:44:37 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 19:53:32 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Fan", "Kai", ""], ["Wang", "Ziteng", ""], ["Beck", "Jeff", ""], ["Kwok", "James", ""], ["Heller", "Katherine", ""]]}, {"id": "1509.02873", "submitter": "Fabrice Rossi", "authors": "Bienvenue Kouway\\`e (SAMM), No\\\"el Fonton, Fabrice Rossi (SAMM)", "title": "S\\'election de variables par le GLM-Lasso pour la pr\\'ediction du risque\n  palustre", "comments": "in French", "journal-ref": "47\\`emes Journ\\'ees de Statistique de la SFdS, Jun 2015, Lille,\n  France. 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose an automatic learning method for variables\nselection based on Lasso in epidemiology context. One of the aim of this\napproach is to overcome the pretreatment of experts in medicine and\nepidemiology on collected data. These pretreatment consist in recoding some\nvariables and to choose some interactions based on expertise. The approach\nproposed uses all available explanatory variables without treatment and\ngenerate automatically all interactions between them. This lead to high\ndimension. We use Lasso, one of the robust methods of variable selection in\nhigh dimension. To avoid over fitting a two levels cross-validation is used.\nBecause the target variable is account variable and the lasso estimators are\nbiased, variables selected by lasso are debiased by a GLM and used to predict\nthe distribution of the main vector of malaria which is Anopheles. Results show\nthat only few climatic and environmental variables are the mains factors\nassociated to the malaria risk exposure.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 17:59:23 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Kouway\u00e8", "Bienvenue", "", "SAMM"], ["Fonton", "No\u00ebl", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1509.02900", "submitter": "Michael Hoffman", "authors": "Beate Franke and Jean-Fran\\c{c}ois Plante and Ribana Roscher and Annie\n  Lee and Cathal Smyth and Armin Hatefi and Fuqi Chen and Einat Gil and\n  Alexander Schwing and Alessandro Selvitella and Michael M. Hoffman and Roger\n  Grosse and Dieter Hendricks and Nancy Reid", "title": "Statistical Inference, Learning and Models in Big Data", "comments": "Thematic Program on Statistical Inference, Learning, and Models for\n  Big Data, Fields Institute; 23 pages, 2 figures", "journal-ref": "Int Stat Rev 84 (2017) 371-389", "doi": "10.1111/insr.12176", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for new methods to deal with big data is a common theme in most\nscientific fields, although its definition tends to vary with the context.\nStatistical ideas are an essential part of this, and as a partial response, a\nthematic program on statistical inference, learning, and models in big data was\nheld in 2015 in Canada, under the general direction of the Canadian Statistical\nSciences Institute, with major funding from, and most activities located at,\nthe Fields Institute for Research in Mathematical Sciences. This paper gives an\noverview of the topics covered, describing challenges and strategies that seem\ncommon to many different areas of application, and including some examples of\napplications to make these challenges and strategies more concrete.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 19:33:31 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 20:26:03 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Franke", "Beate", ""], ["Plante", "Jean-Fran\u00e7ois", ""], ["Roscher", "Ribana", ""], ["Lee", "Annie", ""], ["Smyth", "Cathal", ""], ["Hatefi", "Armin", ""], ["Chen", "Fuqi", ""], ["Gil", "Einat", ""], ["Schwing", "Alexander", ""], ["Selvitella", "Alessandro", ""], ["Hoffman", "Michael M.", ""], ["Grosse", "Roger", ""], ["Hendricks", "Dieter", ""], ["Reid", "Nancy", ""]]}, {"id": "1509.02954", "submitter": "Joseph Wang", "authors": "Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama", "title": "Sensor Selection by Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn sensor trees from training data to minimize sensor acquisition costs\nduring test time. Our system adaptively selects sensors at each stage if\nnecessary to make a confident classification. We pose the problem as empirical\nrisk minimization over the choice of trees and node decision rules. We\ndecompose the problem, which is known to be intractable, into combinatorial\n(tree structures) and continuous parts (node decision rules) and propose to\nsolve them separately. Using training data we greedily solve for the\ncombinatorial tree structures and for the continuous part, which is a\nnon-convex multilinear objective function, we derive convex surrogate loss\nfunctions that are piecewise linear. The resulting problem can be cast as a\nlinear program and has the advantage of guaranteed convergence, global\noptimality, repeatability and computational efficiency. We show that our\nproposed approach outperforms the state-of-art on a number of benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 21:15:32 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Wang", "Joseph", ""], ["Trapeznikov", "Kirill", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.02957", "submitter": "Congrui Yi", "authors": "Congrui Yi and Jian Huang", "title": "Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized\n  Huber Loss Regression and Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm, semismooth Newton coordinate descent (SNCD), for the\nelastic-net penalized Huber loss regression and quantile regression in high\ndimensional settings. Unlike existing coordinate descent type algorithms, the\nSNCD updates each regression coefficient and its corresponding subgradient\nsimultaneously in each iteration. It combines the strengths of the coordinate\ndescent and the semismooth Newton algorithm, and effectively solves the\ncomputational challenges posed by dimensionality and nonsmoothness. We\nestablish the convergence properties of the algorithm. In addition, we present\nan adaptive version of the \"strong rule\" for screening predictors to gain extra\nefficiency. Through numerical experiments, we demonstrate that the proposed\nalgorithm is very efficient and scalable to ultra-high dimensions. We\nillustrate the application via a real data example.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 21:26:39 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 21:09:17 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Yi", "Congrui", ""], ["Huang", "Jian", ""]]}, {"id": "1509.02962", "submitter": "Andreas Stuhlm\\\"uller", "authors": "Andreas Stuhlm\\\"uller, Robert X.D. Hawkins, N. Siddharth, Noah D.\n  Goodman", "title": "Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical techniques for probabilistic inference require a sequence of\ndistributions that interpolate between a tractable distribution and an\nintractable distribution of interest. Usually, the sequences used are simple,\ne.g., based on geometric averages between distributions. When models are\nexpressed as probabilistic programs, the models themselves are highly\nstructured objects that can be used to derive annealing sequences that are more\nsensitive to domain structure. We propose an algorithm for transforming\nprobabilistic programs to coarse-to-fine programs which have the same marginal\ndistribution as the original programs, but generate the data at increasing\nlevels of detail, from coarse to fine. We apply this algorithm to an Ising\nmodel, its depth-from-disparity variation, and a factorial hidden Markov model.\nWe show preliminary evidence that the use of coarse-to-fine models can make\nexisting generic inference algorithms more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 21:48:22 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Stuhlm\u00fcller", "Andreas", ""], ["Hawkins", "Robert X. D.", ""], ["Siddharth", "N.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1509.02971", "submitter": "Jonathan Hunt", "authors": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas\n  Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra", "title": "Continuous control with deep reinforcement learning", "comments": "10 pages + supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 23:01:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:34:41 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:09:07 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 20:30:47 GMT"}, {"version": "v5", "created": "Mon, 29 Feb 2016 18:45:53 GMT"}, {"version": "v6", "created": "Fri, 5 Jul 2019 10:47:27 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Lillicrap", "Timothy P.", ""], ["Hunt", "Jonathan J.", ""], ["Pritzel", "Alexander", ""], ["Heess", "Nicolas", ""], ["Erez", "Tom", ""], ["Tassa", "Yuval", ""], ["Silver", "David", ""], ["Wierstra", "Daan", ""]]}, {"id": "1509.03005", "submitter": "David Balduzzi", "authors": "David Balduzzi, Muhammad Ghifary", "title": "Compatible Value Gradients for Reinforcement Learning of Continuous Deep\n  Policies", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes GProp, a deep reinforcement learning algorithm for\ncontinuous policies with compatible function approximation. The algorithm is\nbased on two innovations. Firstly, we present a temporal-difference based\nmethod for learning the gradient of the value-function. Secondly, we present\nthe deviator-actor-critic (DAC) model, which comprises three neural networks\nthat estimate the value function, its gradient, and determine the actor's\npolicy respectively. We evaluate GProp on two challenging tasks: a contextual\nbandit problem constructed from nonparametric regression datasets that is\ndesigned to probe the ability of reinforcement learning algorithms to\naccurately estimate gradients; and the octopus arm, a challenging reinforcement\nlearning benchmark. GProp is competitive with fully supervised methods on the\nbandit task and achieves the best performance to date on the octopus arm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 04:14:54 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Balduzzi", "David", ""], ["Ghifary", "Muhammad", ""]]}, {"id": "1509.03025", "submitter": "Yudong Chen", "authors": "Yudong Chen, Martin J. Wainwright", "title": "Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 06:07:52 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Chen", "Yudong", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1509.03248", "submitter": "George Trigeorgis", "authors": "George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern\n  W.Schuller", "title": "A deep matrix factorization method for learning attribute\n  representations", "comments": "Submitted to TPAMI (16-Mar-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Non-negative Matrix Factorization is a technique that learns a\nlow-dimensional representation of a dataset that lends itself to a clustering\ninterpretation. It is possible that the mapping between this new representation\nand our original data matrix contains rather complex hierarchical information\nwith implicit lower-level hidden attributes, that classical one level\nclustering methodologies can not interpret. In this work we propose a novel\nmodel, Deep Semi-NMF, that is able to learn such hidden representations that\nallow themselves to an interpretation of clustering according to different,\nunknown attributes of a given dataset. We also present a semi-supervised\nversion of the algorithm, named Deep WSF, that allows the use of (partial)\nprior information for each of the known attributes of a dataset, that allows\nthe model to be used on datasets with mixed attribute knowledge. Finally, we\nshow that our models are able to learn low-dimensional representations that are\nbetter suited for clustering, but also classification, outperforming\nSemi-Non-negative Matrix Factorization, but also other state-of-the-art\nmethodologies variants.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 17:57:03 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Trigeorgis", "George", ""], ["Bousmalis", "Konstantinos", ""], ["Zafeiriou", "Stefanos", ""], ["Schuller", "Bjoern W.", ""]]}, {"id": "1509.03281", "submitter": "Jiaming Xu", "authors": "Elchanan Mossel and Jiaming Xu", "title": "Density Evolution in the Degree-correlated Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent surge of interest in identifying the sharp recovery\nthresholds for cluster recovery under the stochastic block model. In this\npaper, we address the more refined question of how many vertices that will be\nmisclassified on average. We consider the binary form of the stochastic block\nmodel, where $n$ vertices are partitioned into two clusters with edge\nprobability $a/n$ within the first cluster, $c/n$ within the second cluster,\nand $b/n$ across clusters. Suppose that as $n \\to \\infty$, $a= b+ \\mu \\sqrt{ b}\n$, $c=b+ \\nu \\sqrt{ b} $ for two fixed constants $\\mu, \\nu$, and $b \\to \\infty$\nwith $b=n^{o(1)}$. When the cluster sizes are balanced and $\\mu \\neq \\nu$, we\nshow that the minimum fraction of misclassified vertices on average is given by\n$Q(\\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ is\nthe unique fixed point of $v= \\frac{(\\mu-\\nu)^2}{16} + \\frac{ (\\mu+\\nu)^2 }{16}\n\\mathbb{E}[ \\tanh(v+ \\sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, the\nminimum misclassified fraction on average is attained by a local algorithm,\nnamely belief propagation, in time linear in the number of edges. Our proof\ntechniques are based on connecting the cluster recovery problem to tree\nreconstruction problems, and analyzing the density evolution of belief\npropagation on trees with Gaussian approximations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 19:11:56 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 16:20:38 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Mossel", "Elchanan", ""], ["Xu", "Jiaming", ""]]}, {"id": "1509.03302", "submitter": "Matt Barnes", "authors": "Matt Barnes, Kyle Miller, Artur Dubrawski", "title": "Performance Bounds for Pairwise Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant challenge to scaling entity resolution algorithms to massive\ndatasets is understanding how performance changes after moving beyond the realm\nof small, manually labeled reference datasets. Unlike traditional machine\nlearning tasks, when an entity resolution algorithm performs well on small\nhold-out datasets, there is no guarantee this performance holds on larger\nhold-out datasets. We prove simple bounding properties between the performance\nof a match function on a small validation set and the performance of a pairwise\nentity resolution algorithm on arbitrarily sized datasets. Thus, our approach\nenables optimization of pairwise entity resolution algorithms for large\ndatasets, using a small set of labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 19:58:44 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Barnes", "Matt", ""], ["Miller", "Kyle", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1509.03381", "submitter": "Jie Ding", "authors": "Jie Ding, Mohammad Noshad, Vahid Tarokh", "title": "Learning the Number of Autoregressive Mixtures in Time Series Using the\n  Gap Statistics", "comments": "This paper has been accepted by 2015 IEEE International Conference on\n  Data Mining", "journal-ref": null, "doi": "10.1109/ICDMW.2015.209", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a proper model to characterize a time series is crucial in making\naccurate predictions. In this work we use time-varying autoregressive process\n(TVAR) to describe non-stationary time series and model it as a mixture of\nmultiple stable autoregressive (AR) processes. We introduce a new model\nselection technique based on Gap statistics to learn the appropriate number of\nAR filters needed to model a time series. We define a new distance measure\nbetween stable AR filters and draw a reference curve that is used to measure\nhow much adding a new AR filter improves the performance of the model, and then\nchoose the number of AR filters that has the maximum gap with the reference\ncurve. To that end, we propose a new method in order to generate uniform random\nstable AR filters in root domain. Numerical results are provided demonstrating\nthe performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 03:16:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ding", "Jie", ""], ["Noshad", "Mohammad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1509.03475", "submitter": "Minhyung Cho", "authors": "Minhyung Cho, Chandra Shekhar Dhir, Jaehyung Lee", "title": "Hessian-free Optimization for Learning Deep Multidimensional Recurrent\n  Neural Networks", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable\nperformance in the area of speech and handwriting recognition. The performance\nof an MDRNN is improved by further increasing its depth, and the difficulty of\nlearning the deeper network is overcome by using Hessian-free (HF)\noptimization. Given that connectionist temporal classification (CTC) is\nutilized as an objective of learning an MDRNN for sequence labeling, the\nnon-convexity of CTC poses a problem when applying HF to the network. As a\nsolution, a convex approximation of CTC is formulated and its relationship with\nthe EM algorithm and the Fisher information matrix is discussed. An MDRNN up to\na depth of 15 layers is successfully trained using HF, resulting in an improved\nperformance for sequence labeling.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 12:28:36 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 07:14:04 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Cho", "Minhyung", ""], ["Dhir", "Chandra Shekhar", ""], ["Lee", "Jaehyung", ""]]}, {"id": "1509.03808", "submitter": "Jascha Sohl-Dickstein", "authors": "Andrew B. Berger, Mayur Mudigonda, Michael R. DeWeese, Jascha\n  Sohl-Dickstein", "title": "A Markov Jump Process for More Efficient Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most sampling algorithms, including Hamiltonian Monte Carlo, transition\nrates between states correspond to the probability of making a transition in a\nsingle time step, and are constrained to be less than or equal to 1. We derive\na Hamiltonian Monte Carlo algorithm using a continuous time Markov jump\nprocess, and are thus able to escape this constraint. Transition rates in a\nMarkov jump process need only be non-negative. We demonstrate that the new\nalgorithm leads to improved mixing for several example problems, both by\nevaluating the spectral gap of the Markov operator, and by computing\nautocorrelation as a function of compute time. We release the algorithm as an\nopen source Python package.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 04:29:13 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 00:45:44 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2015 18:10:58 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Berger", "Andrew B.", ""], ["Mudigonda", "Mayur", ""], ["DeWeese", "Michael R.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1509.03917", "submitter": "Anastasios Kyrillidis", "authors": "Srinadh Bhojanapalli, Anastasios Kyrillidis, Sujay Sanghavi", "title": "Dropping Convexity for Faster Semi-definite Optimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimization of a convex function $f(X)$ over the set of\n$n\\times n$ positive semi-definite matrices, but when the problem is recast as\n$\\min_U g(U) := f(UU^\\top)$, with $U \\in \\mathbb{R}^{n \\times r}$ and $r \\leq\nn$. We study the performance of gradient descent on $g$---which we refer to as\nFactored Gradient Descent (FGD)---under standard assumptions on the original\nfunction $f$.\n  We provide a rule for selecting the step size and, with this choice, show\nthat the local convergence rate of FGD mirrors that of standard gradient\ndescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for\nsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly\nconvex. In addition, we provide a procedure to initialize FGD for (restricted)\nstrongly convex objectives and when one only has access to $f$ via a\nfirst-order oracle; for several problem instances, such proper initialization\nleads to global convergence guarantees.\n  FGD and similar procedures are widely used in practice for problems that can\nbe posed as matrix factorization. To the best of our knowledge, this is the\nfirst paper to provide precise convergence rate guarantees for general convex\nfunctions under standard convex assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 00:40:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 01:45:02 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 03:17:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1509.03935", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Markov Boundary Discovery with Ridge Regularized Linear Models", "comments": "submitted to the Journal of Causal Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge regularized linear models (RRLMs), such as ridge regression and the\nSVM, are a popular group of methods that are used in conjunction with\ncoefficient hypothesis testing to discover explanatory variables with a\nsignificant multivariate association to a response. However, many investigators\nare reluctant to draw causal interpretations of the selected variables due to\nthe incomplete knowledge of the capabilities of RRLMs in causal inference.\nUnder reasonable assumptions, we show that a modified form of RRLMs can get\nvery close to identifying a subset of the Markov boundary by providing a\nworst-case bound on the space of possible solutions. The results hold for any\nconvex loss, even when the underlying functional relationship is nonlinear, and\nthe solution is not unique. Our approach combines ideas in Markov boundary and\nsufficient dimension reduction theory. Experimental results show that the\nmodified RRLMs are competitive against state-of-the-art algorithms in\ndiscovering part of the Markov boundary from gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 02:58:58 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1509.03977", "submitter": "Pablo Escandell-Montero", "authors": "Pablo Escandell-Montero, Milena Chermisi, Jos\\'e M.\n  Mart\\'inez-Mart\\'inez, Juan G\\'omez-Sanchis, Carlo Barbieri, Emilio\n  Soria-Olivas, Flavio Mari, Joan Vila-Franc\\'es, Andrea Stopper, Emanuele\n  Gatti and Jos\\'e D. Mart\\'in-Guerrero", "title": "Optimization of anemia treatment in hemodialysis patients via\n  reinforcement learning", "comments": "17 pages, 10 figures", "journal-ref": "Artificial Intelligence in Medicine, Volume 62, Issue 1, September\n  2014, Pages 47-60", "doi": "10.1016/j.artmed.2014.07.004", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Anemia is a frequent comorbidity in hemodialysis patients that can\nbe successfully treated by administering erythropoiesis-stimulating agents\n(ESAs). ESAs dosing is currently based on clinical protocols that often do not\naccount for the high inter- and intra-individual variability in the patient's\nresponse. As a result, the hemoglobin level of some patients oscillates around\nthe target range, which is associated with multiple risks and side-effects.\nThis work proposes a methodology based on reinforcement learning (RL) to\noptimize ESA therapy.\n  Methods: RL is a data-driven approach for solving sequential decision-making\nproblems that are formulated as Markov decision processes (MDPs). Computing\noptimal drug administration strategies for chronic diseases is a sequential\ndecision-making problem in which the goal is to find the best sequence of drug\ndoses. MDPs are particularly suitable for modeling these problems due to their\nability to capture the uncertainty associated with the outcome of the treatment\nand the stochastic nature of the underlying process. The RL algorithm employed\nin the proposed methodology is fitted Q iteration, which stands out for its\nability to make an efficient use of data.\n  Results: The experiments reported here are based on a computational model\nthat describes the effect of ESAs on the hemoglobin level. The performance of\nthe proposed method is evaluated and compared with the well-known Q-learning\nalgorithm and with a standard protocol. Simulation results show that the\nperformance of Q-learning is substantially lower than FQI and the protocol.\n  Conclusion: Although prospective validation is required, promising results\ndemonstrate the potential of RL to become an alternative to current protocols.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:52:00 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Escandell-Montero", "Pablo", ""], ["Chermisi", "Milena", ""], ["Mart\u00ednez-Mart\u00ednez", "Jos\u00e9 M.", ""], ["G\u00f3mez-Sanchis", "Juan", ""], ["Barbieri", "Carlo", ""], ["Soria-Olivas", "Emilio", ""], ["Mari", "Flavio", ""], ["Vila-Franc\u00e9s", "Joan", ""], ["Stopper", "Andrea", ""], ["Gatti", "Emanuele", ""], ["Mart\u00edn-Guerrero", "Jos\u00e9 D.", ""]]}, {"id": "1509.04072", "submitter": "Manuel W\\\"uthrich", "authors": "Manuel W\\\"uthrich, Cristina Garcia Cifuentes, Sebastian Trimpe,\n  Franziska Meier, Jeannette Bohg, Jan Issac, Stefan Schaal", "title": "Robust Gaussian Filtering using a Pseudo Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sensors, such as range, sonar, radar, GPS and visual devices, produce\nmeasurements which are contaminated by outliers. This problem can be addressed\nby using fat-tailed sensor models, which account for the possibility of\noutliers. Unfortunately, all estimation algorithms belonging to the family of\nGaussian filters (such as the widely-used extended Kalman filter and unscented\nKalman filter) are inherently incompatible with such fat-tailed sensor models.\nThe contribution of this paper is to show that any Gaussian filter can be made\ncompatible with fat-tailed sensor models by applying one simple change: Instead\nof filtering with the physical measurement, we propose to filter with a pseudo\nmeasurement obtained by applying a feature function to the physical\nmeasurement. We derive such a feature function which is optimal under some\nconditions. Simulation results show that the proposed method can effectively\nhandle measurement outliers and allows for robust filtering in both linear and\nnonlinear systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 13:04:42 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 12:39:11 GMT"}, {"version": "v3", "created": "Mon, 30 May 2016 16:00:39 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["W\u00fcthrich", "Manuel", ""], ["Cifuentes", "Cristina Garcia", ""], ["Trimpe", "Sebastian", ""], ["Meier", "Franziska", ""], ["Bohg", "Jeannette", ""], ["Issac", "Jan", ""], ["Schaal", "Stefan", ""]]}, {"id": "1509.04210", "submitter": "Wei Zhang", "authors": "Suyog Gupta, Wei Zhang, Fei Wang", "title": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\n  Systematic Study", "comments": "Accepted by The IEEE International Conference on Data Mining 2016\n  (ICDM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Rudra, a parameter server based distributed computing\nframework tuned for training large-scale deep neural networks. Using variants\nof the asynchronous stochastic gradient descent algorithm we study the impact\nof synchronization protocol, stale gradient updates, minibatch size, learning\nrates, and number of learners on runtime performance and model accuracy. We\nintroduce a new learning rate modulation strategy to counter the effect of\nstale gradients and propose a new synchronization protocol that can effectively\nbound the staleness in gradients, improve runtime performance and achieve good\nmodel accuracy. Our empirical investigation reveals a principled approach for\ndistributed training of neural networks: the mini-batch size per learner should\nbe reduced as more learners are added to the system to preserve the model\naccuracy. We validate this approach using commonly-used image classification\nbenchmarks: CIFAR10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:14:52 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 01:31:18 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 21:26:38 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Gupta", "Suyog", ""], ["Zhang", "Wei", ""], ["Wang", "Fei", ""]]}, {"id": "1509.04238", "submitter": "Matt Barnes", "authors": "Matt Barnes", "title": "A Practioner's Guide to Evaluating Entity Resolution Results", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the task of identifying records belonging to the\nsame entity (e.g. individual, group) across one or multiple databases.\nIronically, it has multiple names: deduplication and record linkage, among\nothers. In this paper we survey metrics used to evaluate ER results in order to\niteratively improve performance and guarantee sufficient quality prior to\ndeployment. Some of these metrics are borrowed from multi-class classification\nand clustering domains, though some key differences exist differentiating\nentity resolution from general clustering. Menestrina et al. empirically showed\nrankings from these metrics often conflict with each other, thus our primary\nmotivation for studying them. This paper provides practitioners the basic\nknowledge to begin evaluating their entity resolution results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 18:57:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Barnes", "Matt", ""]]}, {"id": "1509.04332", "submitter": "Shahin Shahrampour", "authors": "Mohammad Amin Rahimian, Shahin Shahrampour, Ali Jadbabaie", "title": "Learning without Recall by Random Walks on Directed Graphs", "comments": "6 pages, To Appear in Conference on Decision and Control 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network of agents that aim to learn some unknown state of the\nworld using private observations and exchange of beliefs. At each time, agents\nobserve private signals generated based on the true unknown state. Each agent\nmight not be able to distinguish the true state based only on her private\nobservations. This occurs when some other states are observationally equivalent\nto the true state from the agent's perspective. To overcome this shortcoming,\nagents must communicate with each other to benefit from local observations. We\npropose a model where each agent selects one of her neighbors randomly at each\ntime. Then, she refines her opinion using her private signal and the prior of\nthat particular neighbor. The proposed rule can be thought of as a Bayesian\nagent who cannot recall the priors based on which other agents make inferences.\nThis learning without recall approach preserves some aspects of the Bayesian\ninference while being computationally tractable. By establishing a\ncorrespondence with a random walk on the network graph, we prove that under the\ndescribed protocol, agents learn the truth exponentially fast in the almost\nsure sense. The asymptotic rate is expressed as the sum of the relative\nentropies between the signal structures of every agent weighted by the\nstationary distribution of the random walk.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 21:23:50 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Rahimian", "Mohammad Amin", ""], ["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1509.04376", "submitter": "Weiyu Xu", "authors": "Bingwen Zhang, Weiyu Xu, Jian-Feng Cai and Lifeng Lai", "title": "Precise Phase Transition of Total Variation Minimization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the phase transitions of convex optimizations in recovering\nstructured signals or data is of central importance in compressed sensing,\nmachine learning and statistics. The phase transitions of many convex\noptimization signal recovery methods such as $\\ell_1$ minimization and nuclear\nnorm minimization are well understood through recent years' research. However,\nrigorously characterizing the phase transition of total variation (TV)\nminimization in recovering sparse-gradient signal is still open. In this paper,\nwe fully characterize the phase transition curve of the TV minimization. Our\nproof builds on Donoho, Johnstone and Montanari's conjectured phase transition\ncurve for the TV approximate message passing algorithm (AMP), together with the\nlinkage between the minmax Mean Square Error of a denoising problem and the\nhigh-dimensional convex geometry for TV minimization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 02:18:58 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Zhang", "Bingwen", ""], ["Xu", "Weiyu", ""], ["Cai", "Jian-Feng", ""], ["Lai", "Lifeng", ""]]}, {"id": "1509.04397", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh", "title": "Exponential Family Matrix Completion under Structural Constraints", "comments": "20 pages, 9 figures", "journal-ref": "Gunasekar, Suriya, Pradeep Ravikumar, and Joydeep Ghosh.\n  \"Exponential family matrix completion under structural constraints\".\n  Proceedings of The 31st International Conference on Machine Learning, pp.\n  1917-1925, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the matrix completion problem of recovering a structured matrix\nfrom noisy and partial measurements. Recent works have proposed tractable\nestimators with strong statistical guarantees for the case where the underlying\nmatrix is low--rank, and the measurements consist of a subset, either of the\nexact individual entries, or of the entries perturbed by additive Gaussian\nnoise, which is thus implicitly suited for thin--tailed continuous data.\nArguably, common applications of matrix completion require estimators for (a)\nheterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)\nfor heterogeneous noise models (beyond Gaussian), which capture varied\nuncertainty in the measurements, and (c) heterogeneous structural constraints\nbeyond low--rank, such as block--sparsity, or a superposition structure of\nlow--rank plus elementwise sparseness, among others. In this paper, we provide\na vastly unified framework for generalized matrix completion by considering a\nmatrix completion setting wherein the matrix entries are sampled from any\nmember of the rich family of exponential family distributions; and impose\ngeneral structural constraints on the underlying matrix, as captured by a\ngeneral regularizer $\\mathcal{R}(.)$. We propose a simple convex regularized\n$M$--estimator for the generalized framework, and provide a unified and novel\nstatistical analysis for this general class of estimators. We finally\ncorroborate our theoretical results on simulated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 04:49:57 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Ravikumar", "Pradeep", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1509.04491", "submitter": "Philip Schniter", "authors": "Evan Byrne and Philip Schniter", "title": "Sparse Multinomial Logistic Regression via Approximate Message Passing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2593691", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of multi-class linear classification and feature selection,\nwe propose approximate message passing approaches to sparse multinomial\nlogistic regression (MLR). First, we propose two algorithms based on the Hybrid\nGeneralized Approximate Message Passing (HyGAMP) framework: one finds the\nmaximum a posteriori (MAP) linear classifier and the other finds an\napproximation of the test-error-rate minimizing linear classifier. Then we\ndesign computationally simplified variants of these two algorithms. Next, we\ndetail methods to tune the hyperparameters of their assumed statistical models\nusing Stein's unbiased risk estimate (SURE) and expectation-maximization (EM),\nrespectively. Finally, using both synthetic and real-world datasets, we\ndemonstrate improved error-rate and runtime performance relative to existing\nstate-of-the-art approaches to sparse MLR.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 11:08:33 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 19:11:23 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Byrne", "Evan", ""], ["Schniter", "Philip", ""]]}, {"id": "1509.04541", "submitter": "Christopher Dance", "authors": "Christopher R. Dance and Tomi Silander", "title": "When are Kalman-filter restless bandits indexable?", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the restless bandit associated with an extremely simple scalar\nKalman filter model in discrete time. Under certain assumptions, we prove that\nthe problem is indexable in the sense that the Whittle index is a\nnon-decreasing function of the relevant belief state. In spite of the long\nhistory of this problem, this appears to be the first such proof. We use\nresults about Schur-convexity and mechanical words, which are particular binary\nstrings intimately related to palindromes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 13:33:52 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Dance", "Christopher R.", ""], ["Silander", "Tomi", ""]]}, {"id": "1509.04580", "submitter": "Badong Chen", "authors": "Badong Chen, Xi Liu, Haiquan Zhao, Jos\\'e C. Pr\\'incipe", "title": "Maximum Correntropy Kalman Filter", "comments": "11 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Kalman filter (KF) is derived under the well-known minimum mean\nsquare error (MMSE) criterion, which is optimal under Gaussian assumption.\nHowever, when the signals are non-Gaussian, especially when the system is\ndisturbed by some heavy-tailed impulsive noises, the performance of KF will\ndeteriorate seriously. To improve the robustness of KF against impulsive\nnoises, we propose in this work a new Kalman filter, called the maximum\ncorrentropy Kalman filter (MCKF), which adopts the robust maximum correntropy\ncriterion (MCC) as the optimality criterion, instead of using the MMSE. Similar\nto the traditional KF, the state mean and covariance matrix propagation\nequations are used to give prior estimations of the state and covariance matrix\nin MCKF. A novel fixed-point algorithm is then used to update the posterior\nestimations. A sufficient condition that guarantees the convergence of the\nfixed-point algorithm is given. Illustration examples are presented to\ndemonstrate the effectiveness and robustness of the new algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 14:34:20 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Chen", "Badong", ""], ["Liu", "Xi", ""], ["Zhao", "Haiquan", ""], ["Pr\u00edncipe", "Jos\u00e9 C.", ""]]}, {"id": "1509.04610", "submitter": "Jaak Simm", "authors": "Jaak Simm, Adam Arany, Pooya Zakeri, Tom Haber, J\\\"org K. Wegner,\n  Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau", "title": "Macau: Scalable Bayesian Multi-relational Factorization with Side\n  Information using MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Macau, a powerful and flexible Bayesian factorization method for\nheterogeneous data. Our model can factorize any set of entities and relations\nthat can be represented by a relational model, including tensors and also\nmultiple relations for each entity. Macau can also incorporate side\ninformation, specifically entity and relation features, which are crucial for\npredicting sparsely observed relations. Macau scales to millions of entity\ninstances, hundred millions of observations, and sparse entity features with\nmillions of dimensions. To achieve the scale up, we specially designed sampling\nprocedure for entity and relation features that relies primarily on noise\ninjection in linear regressions. We show performance and advanced features of\nMacau in a set of experiments, including challenging drug-protein activity\nprediction task.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 15:52:22 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 21:39:33 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Simm", "Jaak", ""], ["Arany", "Adam", ""], ["Zakeri", "Pooya", ""], ["Haber", "Tom", ""], ["Wegner", "J\u00f6rg K.", ""], ["Chupakhin", "Vladimir", ""], ["Ceulemans", "Hugo", ""], ["Moreau", "Yves", ""]]}, {"id": "1509.04612", "submitter": "Alan Mosca", "authors": "Alan Mosca and George D. Magoulas", "title": "Adapting Resilient Propagation for Deep Learning", "comments": "Published in the proceedings of the UK workshop on Computational\n  Intelligence 2015 (UKCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resilient Propagation (Rprop) algorithm has been very popular for\nbackpropagation training of multilayer feed-forward neural networks in various\napplications. The standard Rprop however encounters difficulties in the context\nof deep neural networks as typically happens with gradient-based learning\nalgorithms. In this paper, we propose a modification of the Rprop that combines\nstandard Rprop steps with a special drop out technique. We apply the method for\ntraining Deep Neural Networks as standalone components and in ensemble\nformulations. Results on the MNIST dataset show that the proposed modification\nalleviates standard Rprop's problems demonstrating improved learning speed and\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 15:55:29 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 11:45:48 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Mosca", "Alan", ""], ["Magoulas", "George D.", ""]]}, {"id": "1509.04632", "submitter": "Diego Hernan Diaz Martinez", "authors": "Diego Hern\\'an D\\'iaz Mart\\'inez, Facundo M\\'emoli, Washington Mio", "title": "The Shape of Data and Probability Measures", "comments": "46 pages, 12 figures, Theorems 1 and 3 have been revised, as well as\n  the results derived from them", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of multiscale covariance tensor fields (CTF)\nassociated with Euclidean random variables as a gateway to the shape of their\ndistributions. Multiscale CTFs quantify variation of the data about every point\nin the data landscape at all spatial scales, unlike the usual covariance tensor\nthat only quantifies global variation about the mean. Empirical forms of\nlocalized covariance previously have been used in data analysis and\nvisualization, but we develop a framework for the systematic treatment of\ntheoretical questions and computational models based on localized covariance.\nWe prove strong stability theorems with respect to the Wasserstein distance\nbetween probability measures, obtain consistency results, as well as estimates\nfor the rate of convergence of empirical CTFs. These results ensure that CTFs\nare robust to sampling, noise and outliers. We provide numerous illustrations\nof how CTFs let us extract shape from data and also apply CTFs to manifold\nclustering, the problem of categorizing data points according to their noisy\nmembership in a collection of possibly intersecting, smooth submanifolds of\nEuclidean space. We prove that the proposed manifold clustering method is\nstable and carry out several experiments to validate the method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:36:26 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 02:26:39 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Mart\u00ednez", "Diego Hern\u00e1n D\u00edaz", ""], ["M\u00e9moli", "Facundo", ""], ["Mio", "Washington", ""]]}, {"id": "1509.04634", "submitter": "Arno Solin", "authors": "Arno Solin, Manon Kok, Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Simo\n  S\\\"arkk\\\"a", "title": "Modeling and interpolation of the ambient magnetic field by Gaussian\n  processes", "comments": "17 pages, 12 figures, to appear in IEEE Transactions on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in the ambient magnetic field can be used as features in indoor\npositioning and navigation. By using Maxwell's equations, we derive and present\na Bayesian non-parametric probabilistic modeling approach for interpolation and\nextrapolation of the magnetic field. We model the magnetic field components\njointly by imposing a Gaussian process (GP) prior on the latent scalar\npotential of the magnetic field. By rewriting the GP model in terms of a\nHilbert space representation, we circumvent the computational pitfalls\nassociated with GP modeling and provide a computationally efficient and\nphysically justified modeling tool for the ambient magnetic field. The model\nallows for sequential updating of the estimate and time-dependent changes in\nthe magnetic field. The model is shown to work well in practice in different\napplications: we demonstrate mapping of the magnetic field both with an\ninexpensive Raspberry Pi powered robot and on foot using a standard smartphone.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:42:08 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 11:32:30 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Solin", "Arno", ""], ["Kok", "Manon", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1509.04640", "submitter": "Laurent Charlin", "authors": "Laurent Charlin and Rajesh Ranganath and James McInerney and David M.\n  Blei", "title": "Dynamic Poisson Factorization", "comments": "RecSys 2015", "journal-ref": null, "doi": "10.1145/2792838.2800174", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:57:15 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Charlin", "Laurent", ""], ["Ranganath", "Rajesh", ""], ["McInerney", "James", ""], ["Blei", "David M.", ""]]}, {"id": "1509.04681", "submitter": "Calvin McCarter", "authors": "Calvin McCarter, Seyoung Kim", "title": "Large-Scale Optimization Algorithms for Sparse Conditional Gaussian\n  Graphical Models", "comments": "11 pages, 7 figures. Appearing in Proceedings of the 19th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2016, Cadiz, Spain. JMLR: W&CP volume 41", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of scalable optimization for L1-regularized\nconditional Gaussian graphical models. Conditional Gaussian graphical models\ngeneralize the well-known Gaussian graphical models to conditional\ndistributions to model the output network influenced by conditioning input\nvariables. While highly scalable optimization methods exist for sparse Gaussian\ngraphical model estimation, state-of-the-art methods for conditional Gaussian\ngraphical models are not efficient enough and more importantly, fail due to\nmemory constraints for very large problems. In this paper, we propose a new\noptimization procedure based on a Newton method that efficiently iterates over\ntwo sub-problems, leading to drastic improvement in computation time compared\nto the previous methods. We then extend our method to scale to large problems\nunder memory constraints, using block coordinate descent to limit memory usage\nwhile achieving fast convergence. Using synthetic and genomic data, we show\nthat our methods can solve one million dimensional problems to high accuracy in\na little over a day on a single machine.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 19:03:48 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2015 21:46:34 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["McCarter", "Calvin", ""], ["Kim", "Seyoung", ""]]}, {"id": "1509.04740", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto and Martin Rosvall", "title": "Modeling sequences and temporal networks with dynamic community\n  structures", "comments": "15 Pages, 6 figures, 2 tables", "journal-ref": "Nature Communications 8, 582 (2017)", "doi": "10.1038/s41467-017-00148-9", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In evolving complex systems such as air traffic and social organizations,\ncollective effects emerge from their many components' dynamic interactions.\nWhile the dynamic interactions can be represented by temporal networks with\nnodes and links that change over time, they remain highly complex. It is\ntherefore often necessary to use methods that extract the temporal networks'\nlarge-scale dynamic community structure. However, such methods are subject to\noverfitting or suffer from effects of arbitrary, a priori imposed timescales,\nwhich should instead be extracted from data. Here we simultaneously address\nboth problems and develop a principled data-driven method that determines\nrelevant timescales and identifies patterns of dynamics that take place on\nnetworks as well as shape the networks themselves. We base our method on an\narbitrary-order Markov chain model with community structure, and develop a\nnonparametric Bayesian inference framework that identifies the simplest such\nmodel that can explain temporal interaction data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:08:52 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 11:13:02 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 10:00:57 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Peixoto", "Tiago P.", ""], ["Rosvall", "Martin", ""]]}, {"id": "1509.04752", "submitter": "Michael Riis Andersen", "authors": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "title": "Bayesian inference for spatio-temporal spike-and-slab priors", "comments": "58 pages, 17 figures", "journal-ref": "Journal of Machine Learning Research, 18(139):1-58, 2017", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 18:10:07 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 13:38:56 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""], ["Winther", "Ole", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1509.04767", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "Zero-Shot Learning via Semantic Similarity Embedding", "comments": "accepted for ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a version of the zero-shot learning problem where\nseen class source and target domain data are provided. The goal during\ntest-time is to accurately predict the class label of an unseen target domain\ninstance based on revealed source domain side information (\\eg attributes) for\nunseen classes. Our method is based on viewing each source or target data as a\nmixture of seen class proportions and we postulate that the mixture patterns\nhave to be similar if the two instances belong to the same unseen class. This\nperspective leads us to learning source/target embedding functions that map an\narbitrary source/target domain data into a same semantic space where similarity\ncan be readily measured. We develop a max-margin framework to learn these\nsimilarity functions and jointly optimize parameters by means of cross\nvalidation. Our test results are compelling, leading to significant improvement\nin terms of accuracy on most benchmark datasets for zero-shot recognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 23:18:52 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 20:26:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.04771", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Victoria Vilalta-Gil, Paul J. Rathouz, Benjamin B.\n  Lahey, David H. Zald", "title": "Mapping Heritability of Large-Scale Brain Networks with a Billion\n  Connections {\\em via} Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many human brain network studies, we do not have sufficient number (n) of\nimages relative to the number (p) of voxels due to the prohibitively expensive\ncost of scanning enough subjects. Thus, brain network models usually suffer the\nsmall-n large-p problem. Such a problem is often remedied by sparse network\nmodels, which are usually solved numerically by optimizing L1-penalties.\nUnfortunately, due to the computational bottleneck associated with optimizing\nL1-penalties, it is not practical to apply such methods to construct\nlarge-scale brain networks at the voxel-level. In this paper, we propose a new\nscalable sparse network model using cross-correlations that bypass the\ncomputational bottleneck. Our model can build sparse brain networks at the\nvoxel level with p > 25000. Instead of using a single sparse parameter that may\nnot be optimal in other studies and datasets, the computational speed gain\nenables us to analyze the collection of networks at every possible sparse\nparameter in a coherent mathematical framework via persistent homology. The\nmethod is subsequently applied in determining the extent of heritability on a\nfunctional brain network at the voxel-level for the first time using twin fMRI.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 23:54:12 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 13:28:31 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chung", "Moo K.", ""], ["Vilalta-Gil", "Victoria", ""], ["Rathouz", "Paul J.", ""], ["Lahey", "Benjamin B.", ""], ["Zald", "David H.", ""]]}, {"id": "1509.04781", "submitter": "Hong Ge", "authors": "Hong Ge, Yarin Gal, Zoubin Ghahramani", "title": "Dirichlet Fragmentation Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree structures are ubiquitous in data across many domains, and many datasets\nare naturally modelled by unobserved tree structures. In this paper, first we\nreview the theory of random fragmentation processes [Bertoin, 2006], and a\nnumber of existing methods for modelling trees, including the popular nested\nChinese restaurant process (nCRP). Then we define a general class of\nprobability distributions over trees: the Dirichlet fragmentation process (DFP)\nthrough a novel combination of the theory of Dirichlet processes and random\nfragmentation processes. This DFP presents a stick-breaking construction, and\nrelates to the nCRP in the same way the Dirichlet process relates to the\nChinese restaurant process. Furthermore, we develop a novel hierarchical\nmixture model with the DFP, and empirically compare the new model to similar\nmodels in machine learning. Experiments show the DFP mixture model to be\nconvincingly better than existing state-of-the-art approaches for hierarchical\nclustering and density modelling.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 01:07:24 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Ge", "Hong", ""], ["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1509.04783", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuting Chen, and Venkatesh Saligrama", "title": "Group Membership Prediction", "comments": "accepted for ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group membership prediction (GMP) problem involves predicting whether or\nnot a collection of instances share a certain semantic property. For instance,\nin kinship verification given a collection of images, the goal is to predict\nwhether or not they share a {\\it familial} relationship. In this context we\npropose a novel probability model and introduce latent {\\em view-specific} and\n{\\em view-shared} random variables to jointly account for the view-specific\nappearance and cross-view similarities among data instances. Our model posits\nthat data from each view is independent conditioned on the shared variables.\nThis postulate leads to a parametric probability model that decomposes group\nmembership likelihood into a tensor product of data-independent parameters and\ndata-dependent factors. We propose learning the data-independent parameters in\na discriminative way with bilinear classifiers, and test our prediction\nalgorithm on challenging visual recognition tasks such as multi-camera person\nre-identification and kinship verification. On most benchmark datasets, our\nmethod can significantly outperform the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 01:22:40 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Zhang", "Ziming", ""], ["Chen", "Yuting", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.05009", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir, Amnon Shashua", "title": "On the Expressive Power of Deep Learning: A Tensor Analysis", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, pp. 698-728, 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 19:32:54 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 16:31:49 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:07:22 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1509.05111", "submitter": "Rong Zhu", "authors": "Rong Zhu, Ping Ma, Michael W. Mahoney, Bin Yu", "title": "Optimal Subsampling Approaches for Large Sample Linear Regression", "comments": "This paper has been withdrawn by the author due to the incompleteness\n  of this draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant hurdle for analyzing large sample data is the lack of effective\nstatistical computing and inference methods. An emerging powerful approach for\nanalyzing large sample data is subsampling, by which one takes a random\nsubsample from the original full sample and uses it as a surrogate for\nsubsequent computation and estimation. In this paper, we study subsampling\nmethods under two scenarios: approximating the full sample ordinary\nleast-square (OLS) estimator and estimating the coefficients in linear\nregression. We present two algorithms, weighted estimation algorithm and\nunweighted estimation algorithm, and analyze asymptotic behaviors of their\nresulting subsample estimators under general conditions. For the weighted\nestimation algorithm, we propose a criterion for selecting the optimal sampling\nprobability by making use of the asymptotic results. On the basis of the\ncriterion, we provide two novel subsampling methods, the optimal subsampling\nand the predictor- length subsampling methods. The predictor-length subsampling\nmethod is based on the L2 norm of predictors rather than leverage scores. Its\ncomputational cost is scalable. For unweighted estimation algorithm, we show\nthat its resulting subsample estimator is not consistent to the full sample OLS\nestimator. However, it has better performance than the weighted estimation\nalgorithm for estimating the coefficients. Simulation studies and a real data\nexample are used to demonstrate the effectiveness of our proposed subsampling\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 03:25:21 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 00:17:26 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zhu", "Rong", ""], ["Ma", "Ping", ""], ["Mahoney", "Michael W.", ""], ["Yu", "Bin", ""]]}, {"id": "1509.05113", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Madeleine Udell", "title": "Revealed Preference at Scale: Learning Personalized Preferences from\n  Assortment Choices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the preferences of a heterogeneous\npopulation by observing choices from an assortment of products, ads, or other\nofferings. Our observation model takes a form common in assortment planning\napplications: each arriving customer is offered an assortment consisting of a\nsubset of all possible offerings; we observe only the assortment and the\ncustomer's single choice.\n  In this paper we propose a mixture choice model with a natural underlying\nlow-dimensional structure, and show how to estimate its parameters. In our\nmodel, the preferences of each customer or segment follow a separate parametric\nchoice model, but the underlying structure of these parameters over all the\nmodels has low dimension. We show that a nuclear-norm regularized maximum\nlikelihood estimator can learn the preferences of all customers using a number\nof observations much smaller than the number of item-customer combinations.\nThis result shows the potential for structural assumptions to speed up learning\nand improve revenues in assortment planning and customization. We provide a\nspecialized factored gradient descent algorithm and study the success of the\napproach empirically.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 03:37:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 23:40:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Kallus", "Nathan", ""], ["Udell", "Madeleine", ""]]}, {"id": "1509.05142", "submitter": "Rajiv Sambasivan", "authors": "Sourish Das, Sasanka Roy, Rajiv Sambasivan", "title": "Fast Gaussian Process Regression for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes are widely used for regression tasks. A known limitation\nin the application of Gaussian Processes to regression tasks is that the\ncomputation of the solution requires performing a matrix inversion. The\nsolution also requires the storage of a large matrix in memory. These factors\nrestrict the application of Gaussian Process regression to small and moderate\nsize data sets. We present an algorithm that combines estimates from models\ndeveloped using subsets of the data obtained in a manner similar to the\nbootstrap. The sample size is a critical parameter for this algorithm.\nGuidelines for reasonable choices of algorithm parameters, based on detailed\nexperimental study, are provided. Various techniques have been proposed to\nscale Gaussian Processes to large scale regression tasks. The most appropriate\nchoice depends on the problem context. The proposed method is most appropriate\nfor problems where an additive model works well and the response depends on a\nsmall number of features. The minimax rate of convergence for such problems is\nattractive and we can build effective models with a small subset of the data.\nThe Stochastic Variational Gaussian Process and the Sparse Gaussian Process are\nalso appropriate choices for such problems. These methods pick a subset of data\nbased on theoretical considerations. The proposed algorithm uses bagging and\nrandom sampling. Results from experiments conducted as part of this study\nindicate that the algorithm presented in this work can be as effective as these\nmethods. Model stacking can be used to combine the model developed with the\nproposed method with models from other methods for large scale regression such\nas Gradient Boosted Trees. This can yield performance gains.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 06:18:08 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 12:16:05 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 17:08:48 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 03:40:11 GMT"}, {"version": "v5", "created": "Mon, 26 Sep 2016 12:07:46 GMT"}, {"version": "v6", "created": "Sat, 19 Aug 2017 02:06:53 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Das", "Sourish", ""], ["Roy", "Sasanka", ""], ["Sambasivan", "Rajiv", ""]]}, {"id": "1509.05172", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Aviv Tamar, Remi Munos, Shie Mannor", "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance\n  Analysis", "comments": "arXiv admin note: text overlap with arXiv:1508.03411", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the off-policy evaluation problem in Markov decision processes\nwith function approximation. We propose a generalization of the recently\nintroduced \\emph{emphatic temporal differences} (ETD) algorithm\n\\citep{SuttonMW15}, which encompasses the original ETD($\\lambda$), as well as\nseveral other off-policy evaluation algorithms as special cases. We call this\nframework \\ETD, where our introduced parameter $\\beta$ controls the decay rate\nof an importance-sampling term. We study conditions under which the projected\nfixed-point equation underlying \\ETD\\ involves a contraction operator, allowing\nus to present the first asymptotic error bounds (bias) for \\ETD. Our results\nshow that the original ETD algorithm always involves a contraction operator,\nand its bias is bounded. Moreover, by controlling $\\beta$, our proposed\ngeneralization allows trading-off bias for variance reduction, thereby\nachieving a lower total error.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 09:03:35 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 07:17:55 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Hallak", "Assaf", ""], ["Tamar", "Aviv", ""], ["Munos", "Remi", ""], ["Mannor", "Shie", ""]]}, {"id": "1509.05257", "submitter": "Ernesto Diaz-Aviles", "authors": "Hoang Thanh Lam and Ernesto Diaz-Aviles and Alessandra Pascale and\n  Yiannis Gkoufas and Bei Chen", "title": "(Blue) Taxi Destination and Trip Time Prediction from Partial\n  Trajectories", "comments": "ECML/PKDD Discovery Challenge 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time estimation of destination and travel time for taxis is of great\nimportance for existing electronic dispatch systems. We present an approach\nbased on trip matching and ensemble learning, in which we leverage the patterns\nobserved in a dataset of roughly 1.7 million taxi journeys to predict the\ncorresponding final destination and travel time for ongoing taxi trips, as a\nsolution for the ECML/PKDD Discovery Challenge 2015 competition. The results of\nour empirical evaluation show that our approach is effective and very robust,\nwhich led our team -- BlueTaxi -- to the 3rd and 7th position of the final\nrankings for the trip time and destination prediction tasks, respectively.\nGiven the fact that the final rankings were computed using a very small test\nset (with only 320 trips) we believe that our approach is one of the most\nrobust solutions for the challenge based on the consistency of our good results\nacross the test sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 13:51:55 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Lam", "Hoang Thanh", ""], ["Diaz-Aviles", "Ernesto", ""], ["Pascale", "Alessandra", ""], ["Gkoufas", "Yiannis", ""], ["Chen", "Bei", ""]]}, {"id": "1509.05285", "submitter": "Ehud Strobach", "authors": "Ehud Strobach and Golan Bel", "title": "Decadal climate predictions using sequential learning algorithms", "comments": null, "journal-ref": null, "doi": "10.1175/JCLI-D-15-0648.1", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of climate models are commonly used to improve climate predictions\nand assess the uncertainties associated with them. Weighting the models\naccording to their performances holds the promise of further improving their\npredictions. Here, we use an ensemble of decadal climate predictions to\ndemonstrate the ability of sequential learning algorithms (SLAs) to reduce the\nforecast errors and reduce the uncertainties. Three different SLAs are\nconsidered, and their performances are compared with those of an equally\nweighted ensemble, a linear regression and the climatology. Predictions of four\ndifferent variables--the surface temperature, the zonal and meridional wind,\nand pressure--are considered. The spatial distributions of the performances are\npresented, and the statistical significance of the improvements achieved by the\nSLAs is tested. Based on the performances of the SLAs, we propose one to be\nhighly suitable for the improvement of decadal climate predictions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:16:05 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Strobach", "Ehud", ""], ["Bel", "Golan", ""]]}, {"id": "1509.05438", "submitter": "Qiyi Lu", "authors": "Qiyi Lu and Xingye Qiao", "title": "Sparse Fisher's Linear Discriminant Analysis for Partially Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important tool with many useful applications. Among the\nmany classification methods, Fisher's Linear Discriminant Analysis (LDA) is a\ntraditional model-based approach which makes use of the covariance information.\nHowever, in the high-dimensional, low-sample size setting, LDA cannot be\ndirectly deployed because the sample covariance is not invertible. While there\nare modern methods designed to deal with high-dimensional data, they may not\nfully use the covariance information as LDA does. Hence in some situations, it\nis still desirable to use a model-based method such as LDA for classification.\nThis article exploits the potential of LDA in more complicated data settings.\nIn many real applications, it is costly to manually place labels on\nobservations; hence it is often that only a small portion of labeled data is\navailable while a large number of observations are left without a label. It is\na great challenge to obtain good classification performance through the labeled\ndata alone, especially when the dimension is greater than the size of the\nlabeled data. In order to overcome this issue, we propose a semi-supervised\nsparse LDA classifier to take advantage of the seemingly useless unlabeled\ndata. They provide additional information which helps to boost the\nclassification performance in some situations. A direct estimation method is\nused to reconstruct LDA and achieve the sparsity; meanwhile we employ the\ndifference-convex algorithm to handle the non-convex loss function associated\nwith the unlabeled data. Theoretical properties of the proposed classifier are\nstudied. Our simulated examples help to understand when and how the information\nextracted from the unlabeled data can be useful. A real data example further\nillustrates the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 20:42:42 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Lu", "Qiyi", ""], ["Qiao", "Xingye", ""]]}, {"id": "1509.05722", "submitter": "Miguel Angel Rodriguez Marquez", "authors": "Michael Zehnder, Holger Wache, Hans-Friedrich Witschel, Danilo\n  Zanatta, Miguel Rodriguez", "title": "Energy saving in smart homes based on consumer behaviour: A case study", "comments": "To be presented on IEEE International Smart Cities Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a case study of a recommender system that can be used to\nsave energy in smart homes without lowering the comfort of the inhabitants. We\npresent an algorithm that uses consumer behavior data only and uses machine\nlearning to suggest actions for inhabitants to reduce the energy consumption of\ntheir homes. The system mines for frequent and periodic patterns in the event\ndata provided by the Digitalstrom home automation system. These patterns are\nconverted into association rules, prioritized and compared with the current\nbehavior of the inhabitants. If the system detects an opportunities to save\nenergy without decreasing the comfort level it sends a recommendation to the\nresidents.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 17:31:25 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Zehnder", "Michael", ""], ["Wache", "Holger", ""], ["Witschel", "Hans-Friedrich", ""], ["Zanatta", "Danilo", ""], ["Rodriguez", "Miguel", ""]]}, {"id": "1509.05742", "submitter": "Haohan Wang", "authors": "Haohan Wang, Madhavi K. Ganapathiraju", "title": "Evaluation of Protein-protein Interaction Predictors with Noisy\n  Partially Labeled Data Sets", "comments": "preprint version, 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein-protein interaction (PPI) prediction is an important problem in\nmachine learning and computational biology. However, there is no data set for\ntraining or evaluation purposes, where all the instances are accurately\nlabeled. Instead, what is available are instances of positive class (with\npossibly noisy labels) and no instances of negative class. The non-availability\nof negative class data is typically handled with the observation that randomly\nchosen protein-pairs have a nearly 100% chance of being negative class, as only\n1 in 1,500 protein pairs expected is expected to be an interacting pair. In\nthis paper, we focused on the problem that non-availability of accurately\nlabeled testing data sets in the domain of protein-protein interaction (PPI)\nprediction may lead to biased evaluation results. We first showed that not\nacknowledging the inherent skew in the interactome (i.e. rare occurrence of\npositive instances) leads to an over-estimated accuracy of the predictor. Then\nwe show that, with the belief that positive interactions are a rare category,\nsampling random pairs of proteins excluding known interacting proteins set as\nthe negative testing data set could lead to an under-estimated evaluation\nresult. We formalized those two problems to validate the above claim, and based\non the formalization, we proposed a balancing method to cancel out the\nover-estimation with under-estimation. Finally, our experiments validated the\ntheoretical aspects and showed that this balancing evaluation could evaluate\nthe exact performance without availability of golden standard data sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 18:45:49 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Wang", "Haohan", ""], ["Ganapathiraju", "Madhavi K.", ""]]}, {"id": "1509.05753", "submitter": "Carlo Baldassi", "authors": "Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti\n  and Riccardo Zecchina", "title": "Subdominant Dense Clusters Allow for Simple Learning and High\n  Computational Performance in Neural Networks with Discrete Synapses", "comments": "11 pages, 4 figures (main text: 5 pages, 3 figures; Supplemental\n  Material: 6 pages, 1 figure)", "journal-ref": "Physical Review Letters, 15, 128101 (2015)\n  url=http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.115.128101", "doi": "10.1103/PhysRevLett.115.128101", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that discrete synaptic weights can be efficiently used for learning\nin large scale neural systems, and lead to unanticipated computational\nperformance. We focus on the representative case of learning random patterns\nwith binary synapses in single layer networks. The standard statistical\nanalysis shows that this problem is exponentially dominated by isolated\nsolutions that are extremely hard to find algorithmically. Here, we introduce a\nnovel method that allows us to find analytical evidence for the existence of\nsubdominant and extremely dense regions of solutions. Numerical experiments\nconfirm these findings. We also show that the dense regions are surprisingly\naccessible by simple learning protocols, and that these synaptic configurations\nare robust to perturbations and generalize better than typical solutions. These\noutcomes extend to synapses with multiple states and to deeper neural\narchitectures. The large deviation measure also suggests how to design novel\nalgorithmic schemes for optimization based on local entropy maximization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 19:12:55 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Baldassi", "Carlo", ""], ["Ingrosso", "Alessandro", ""], ["Lucibello", "Carlo", ""], ["Saglietti", "Luca", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1509.05760", "submitter": "Scott Yang", "authors": "Mehryar Mohri, Scott Yang", "title": "Accelerating Optimization via Adaptive Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful general framework for designing data-dependent\noptimization algorithms, building upon and unifying recent techniques in\nadaptive regularization, optimistic gradient predictions, and problem-dependent\nrandomization. We first present a series of new regret guarantees that hold at\nany time and under very minimal assumptions, and then show how different\nrelaxations recover existing algorithms, both basic as well as more recent\nsophisticated ones. Finally, we show how combining adaptivity, optimism, and\nproblem-dependent randomization can guide the design of algorithms that benefit\nfrom more favorable guarantees than recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 19:49:54 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 20:13:42 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 16:40:18 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Mohri", "Mehryar", ""], ["Yang", "Scott", ""]]}, {"id": "1509.05789", "submitter": "Alessandro Checco", "authors": "Alessandro Checco, Giuseppe Bianchi, Doug Leith", "title": "BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 20:21:43 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 22:28:38 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 23:51:40 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Checco", "Alessandro", ""], ["Bianchi", "Giuseppe", ""], ["Leith", "Doug", ""]]}, {"id": "1509.05808", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, David Alvarez-Melis, Tommi S. Jaakkola", "title": "Word, graph and manifold embedding from Markov processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:50:38 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1509.05962", "submitter": "Rakesh Achanta", "authors": "Rakesh Achanta, Trevor Hastie", "title": "Telugu OCR Framework using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of Optical Character Recognition(OCR) for\nthe Telugu script. We present an end-to-end framework that segments the text\nimage, classifies the characters and extracts lines using a language model. The\nsegmentation is based on mathematical morphology. The classification module,\nwhich is the most challenging task of the three, is a deep convolutional neural\nnetwork. The language is modelled as a third degree markov chain at the glyph\nlevel. Telugu script is a complex alphasyllabary and the language is\nagglutinative, making the problem hard. In this paper we apply the latest\nadvances in neural networks to achieve state-of-the-art error rates. We also\nreview convolutional neural networks in great detail and expound the\nstatistical justification behind the many tricks needed to make Deep Learning\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 03:35:05 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 02:29:04 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Achanta", "Rakesh", ""], ["Hastie", "Trevor", ""]]}, {"id": "1509.06061", "submitter": "Brandon Willard", "authors": "Nicholas G. Polson, Brandon T. Willard, Massoud Heidari", "title": "A Statistical Theory of Deep Learning via Proximal Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a statistical theory and an implementation of deep\nlearning models. We show that an elegant variable splitting scheme for the\nalternating direction method of multipliers optimises a deep learning\nobjective. We allow for non-smooth non-convex regularisation penalties to\ninduce sparsity in parameter weights. We provide a link between traditional\nshallow layer statistical models such as principal component and sliced inverse\nregression and deep layer models. We also define the degrees of freedom of a\ndeep learning predictor and a predictive MSE criteria to perform model\nselection for comparing architecture designs. We focus on deep multiclass\nlogistic learning although our methods apply more generally. Our results\nsuggest an interesting and previously under-exploited relationship between deep\nlearning and proximal splitting techniques. To illustrate our methodology, we\nprovide a multi-class logit classification analysis of Fisher's Iris data where\nwe illustrate the convergence of our algorithm. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 21:39:47 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Willard", "Brandon T.", ""], ["Heidari", "Massoud", ""]]}, {"id": "1509.06088", "submitter": "Qiyi Lu", "authors": "Qiyi Lu, Xingye Qiao", "title": "Significance Analysis of High-Dimensional, Low-Sample Size Partially\n  Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering are both important topics in statistical\nlearning. A natural question herein is whether predefined classes are really\ndifferent from one another, or whether clusters are really there. Specifically,\nwe may be interested in knowing whether the two classes defined by some class\nlabels (when they are provided), or the two clusters tagged by a clustering\nalgorithm (where class labels are not provided), are from the same underlying\ndistribution. Although both are challenging questions for the high-dimensional,\nlow-sample size data, there has been some recent development for both. However,\nwhen it is costly to manually place labels on observations, it is often that\nonly a small portion of the class labels is available. In this article, we\npropose a significance analysis approach for such type of data, namely\npartially labeled data. Our method makes use of the whole data and tries to\ntest the class difference as if all the labels were observed. Compared to a\ntesting method that ignores the label information, our method provides a\ngreater power, meanwhile, maintaining the size, illustrated by a comprehensive\nsimulation study. Theoretical properties of the proposed method are studied\nwith emphasis on the high-dimensional, low-sample size setting. Our simulated\nexamples help to understand when and how the information extracted from the\nlabeled data can be effective. A real data example further illustrates the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 01:23:45 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Lu", "Qiyi", ""], ["Qiao", "Xingye", ""]]}, {"id": "1509.06290", "submitter": "Matthew Hawes", "authors": "Matthew Hawes, Lyudmila Mihaylova, Francois Septier and Simon Godsill", "title": "A Bayesian Compressed Sensing Kalman Filter for Direction of Arrival\n  Estimation", "comments": "Fusion 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look to address the problem of estimating the dynamic\ndirection of arrival (DOA) of a narrowband signal impinging on a sensor array\nfrom the far field. The initial estimate is made using a Bayesian compressive\nsensing (BCS) framework and then tracked using a Bayesian compressed sensing\nKalman filter (BCSKF). The BCS framework splits the angular region into N\npotential DOAs and enforces a belief that only a few of the DOAs will have a\nnon-zero valued signal present. A BCSKF can then be used to track the change in\nthe DOA using the same framework. There can be an issue when the DOA approaches\nthe endfire of the array. In this angular region current methods can struggle\nto accurately estimate and track changes in the DOAs. To tackle this problem,\nwe propose changing the traditional sparse belief associated with BCS to a\nbelief that the estimated signals will match the predicted signals given a\nknown DOA change. This is done by modelling the difference between the expected\nsparse received signals and the estimated sparse received signals as a Gaussian\ndistribution. Example test scenarios are provided and comparisons made with the\ntraditional BCS based estimation method. They show that an improvement in\nestimation accuracy is possible without a significant increase in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 16:29:16 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Hawes", "Matthew", ""], ["Mihaylova", "Lyudmila", ""], ["Septier", "Francois", ""], ["Godsill", "Simon", ""]]}, {"id": "1509.06449", "submitter": "Yingxiang Yang", "authors": "Yingxiang Yang, Jalal Etesami, Negar Kiyavash", "title": "Efficient Neighborhood Selection for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of neighborhood selection for Gaussian\ngraphical models. We present two heuristic algorithms: a forward-backward\ngreedy algorithm for general Gaussian graphical models based on mutual\ninformation test, and a threshold-based algorithm for walk summable Gaussian\ngraphical models. Both algorithms are shown to be structurally consistent, and\nefficient. Numerical results show that both algorithms work very well.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 02:51:30 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Yang", "Yingxiang", ""], ["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1509.06457", "submitter": "Suneel Sarswat", "authors": "Suneel Sarswat, Kandathil Mathew Abraham, Subir Kumar Ghosh", "title": "Identifying collusion groups using spectral clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an illiquid stock, traders can collude and place orders on a predetermined\nprice and quantity at a fixed schedule. This is usually done to manipulate the\nprice of the stock or to create artificial liquidity in the stock, which may\nmislead genuine investors. Here, the problem is to identify such group of\ncolluding traders. We modeled the problem instance as a graph, where each\ntrader corresponds to a vertex of the graph and trade corresponds to edges of\nthe graph. Further, we assign weights on edges depending on total volume, total\nnumber of trades, maximum change in the price and commonality between two\nvertices. Spectral clustering algorithms are used on the constructed graph to\nidentify colluding group(s). We have compared our results with simulated data\nto show the effectiveness of spectral clustering to detecting colluding groups.\nMoreover, we also have used parameters of real data to test the effectiveness\nof our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:03:18 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 12:40:52 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sarswat", "Suneel", ""], ["Abraham", "Kandathil Mathew", ""], ["Ghosh", "Subir Kumar", ""]]}, {"id": "1509.06459", "submitter": "Dustin Tran", "authors": "Dustin Tran, Panos Toulis, Edoardo M. Airoldi", "title": "Stochastic gradient descent methods for estimation with large data sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for parameter estimation in settings with large-scale data\nsets, where traditional methods are no longer tenable. Our methods rely on\nstochastic approximations, which are computationally efficient as they maintain\none iterate as a parameter estimate, and successively update that iterate based\non a single data point. When the update is based on a noisy gradient, the\nstochastic approximation is known as standard stochastic gradient descent,\nwhich has been fundamental in modern applications with large data sets.\nAdditionally, our methods are numerically stable because they employ implicit\nupdates of the iterates. Intuitively, an implicit update is a shrinked version\nof a standard one, where the shrinkage factor depends on the observed Fisher\ninformation at the corresponding data point. This shrinkage prevents numerical\ndivergence of the iterates, which can be caused either by excess noise or\noutliers. Our sgd package in R offers the most extensive and robust\nimplementation of stochastic gradient descent methods. We demonstrate that sgd\ndominates alternative software in runtime for several estimation problems with\nmassive data sets. Our applications include the wide class of generalized\nlinear models as well as M-estimation for robust regression.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:25:54 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Tran", "Dustin", ""], ["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1509.06492", "submitter": "Tiep Mai", "authors": "Tiep Mai and Simon Wilson", "title": "Modifying iterated Laplace approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, several modifications are introduced to the functional\napproximation method iterLap to reduce the approximation error, including\nstopping rule adjustment, proposal of new residual function, starting point\nselection for numerical optimisation, scaling of Hessian matrix. Illustrative\nexamples are also provided to show the trade-off between running time and\naccuracy of the original and modified methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 08:09:21 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Mai", "Tiep", ""], ["Wilson", "Simon", ""]]}, {"id": "1509.06673", "submitter": "S\\\"oren Christensen", "authors": "S\\\"oren Christensen, Albrecht Irle, and Lars Willert", "title": "Classification error in multiclass discrimination from Markov data", "comments": null, "journal-ref": null, "doi": "10.1007/s11203-015-9129-6", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a model for an on-line classification setting we consider a stochastic\nprocess $(X_{-n},Y_{-n})_{n}$, the present time-point being denoted by 0, with\nobservables $ \\ldots,X_{-n},X_{-n+1},\\ldots, X_{-1}, X_0$ from which the\npattern $Y_0$ is to be inferred. So in this classification setting, in addition\nto the present observation $X_0$ a number $l$ of preceding observations may be\nused for classification, thus taking a possible dependence structure into\naccount as it occurs e.g. in an ongoing classification of handwritten\ncharacters. We treat the question how the performance of classifiers is\nimproved by using such additional information. For our analysis, a hidden\nMarkov model is used. Letting $R_l$ denote the minimal risk of\nmisclassification using $l$ preceding observations we show that the difference\n$\\sup_k |R_l - R_{l+k}|$ decreases exponentially fast as $l$ increases. This\nsuggests that a small $l$ might already lead to a noticeable improvement. To\nfollow this point we look at the use of past observations for kernel\nclassification rules. Our practical findings in simulated hidden Markov models\nand in the classification of handwritten characters indicate that using $l=1$,\ni.e. just the last preceding observation in addition to $X_0$, can lead to a\nsubstantial reduction of the risk of misclassification. So, in the presence of\nstochastic dependencies, we advocate to use $ X_{-1},X_0$ for finding the\npattern $Y_0$ instead of only $X_0$ as one would in the independent situation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 16:32:30 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Christensen", "S\u00f6ren", ""], ["Irle", "Albrecht", ""], ["Willert", "Lars", ""]]}, {"id": "1509.06807", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Bandit Label Inference for Weakly Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of data annotated at the desired level of granularity is a\nrecurring issue in many applications. Significant amounts of effort have been\ndevoted to developing weakly supervised methods tailored to each individual\nsetting, which are often carefully designed to take advantage of the particular\nproperties of weak supervision regimes, form of available data and prior\nknowledge of the task at hand. Unfortunately, it is difficult to adapt these\nmethods to new tasks and/or forms of data, which often require different weak\nsupervision regimes or models. We present a general-purpose method that can\nsolve any weakly supervised learning problem irrespective of the weak\nsupervision regime or the model. The proposed method turns any off-the-shelf\nstrongly supervised classifier into a weakly supervised classifier and allows\nthe user to specify any arbitrary weakly supervision regime via a loss\nfunction. We apply the method to several different weak supervision regimes and\ndemonstrate competitive results compared to methods specifically engineered for\nthose settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 23:13:12 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1509.06831", "submitter": "Hao Su", "authors": "Kun Yang, Hao Su, Wing Hung Wang", "title": "Density Estimation via Discrepancy", "comments": "arXiv admin note: substantial text overlap with arXiv:1404.1425", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given i.i.d samples from some unknown continuous density on hyper-rectangle\n$[0, 1]^d$, we attempt to learn a piecewise constant function that approximates\nthis underlying density non-parametrically. Our density estimate is defined on\na binary split of $[0, 1]^d$ and built up sequentially according to discrepancy\ncriteria; the key ingredient is to control the discrepancy adaptively in each\nsub-rectangle to achieve overall bound. We prove that the estimate, even though\nsimple as it appears, preserves most of the estimation power. By exploiting its\nstructure, it can be directly applied to some important pattern recognition\ntasks such as mode seeking and density landscape exploration. We demonstrate\nits applicability through simulations and examples.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 03:20:28 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Yang", "Kun", ""], ["Su", "Hao", ""], ["Wang", "Wing Hung", ""]]}, {"id": "1509.06849", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn (1), Sejun Park (1), Michael Chertkov (2), Jinwoo Shin (1)\n  ((1) Korea Advanced Institute of Science and Technology (2) Los Alamos\n  National Laboratory)", "title": "Minimum Weight Perfect Matching via Blossom Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-product Belief Propagation (BP) is a popular message-passing algorithm\nfor computing a Maximum-A-Posteriori (MAP) assignment over a distribution\nrepresented by a Graphical Model (GM). It has been shown that BP can solve a\nnumber of combinatorial optimization problems including minimum weight\nmatching, shortest path, network flow and vertex cover under the following\ncommon assumption: the respective Linear Programming (LP) relaxation is tight,\ni.e., no integrality gap is present. However, when LP shows an integrality gap,\nno model has been known which can be solved systematically via sequential\napplications of BP. In this paper, we develop the first such algorithm, coined\nBlossom-BP, for solving the minimum weight matching problem over arbitrary\ngraphs. Each step of the sequential algorithm requires applying BP over a\nmodified graph constructed by contractions and expansions of blossoms, i.e.,\nodd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs,\nwhere n is the number of vertices in the original graph. In essence, the\nBlossom-BP offers a distributed version of the celebrated Edmonds' Blossom\nalgorithm by jumping at once over many sub-steps with a single BP. Moreover,\nour result provides an interpretation of the Edmonds' algorithm as a sequence\nof LPs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 05:49:53 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Park", "Sejun", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1509.06893", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov, Theodor Misiakiewicz", "title": "Efficient reconstruction of transmission probabilities in a spreading\n  process from partial observations", "comments": "5 pages, 9 pages of supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem of reconstruction of diffusion network and transmission\nprobabilities from the data has attracted a considerable attention in the past\nseveral years. A number of recent papers introduced efficient algorithms for\nthe estimation of spreading parameters, based on the maximization of the\nlikelihood of observed cascades, assuming that the full information for all the\nnodes in the network is available. In this work, we focus on a more realistic\nand restricted scenario, in which only a partial information on the cascades is\navailable: either the set of activation times for a limited number of nodes, or\nthe states of nodes for a subset of observation times. To tackle this problem,\nwe first introduce a framework based on the maximization of the likelihood of\nthe incomplete diffusion trace. However, we argue that the computation of this\nincomplete likelihood is a computationally hard problem, and show that a fast\nand robust reconstruction of transmission probabilities in sparse networks can\nbe achieved with a new algorithm based on recently introduced dynamic\nmessage-passing equations for the spreading processes. The suggested approach\ncan be easily generalized to a large class of discrete and continuous dynamic\nmodels, as well as to the cases of dynamically-changing networks and noisy\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 09:14:34 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Lokhov", "Andrey Y.", ""], ["Misiakiewicz", "Theodor", ""]]}, {"id": "1509.06920", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "Naresh Kumar Mallenahalli", "title": "Predicting Climate Variability over the Indian Region Using Data Mining\n  Strategies", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.ao-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper an approach based on expectation maximization (EM) clustering\nto find the climate regions and a support vector machine to build a predictive\nmodel for each of these regions is proposed. To minimize the biases in the\nestimations a ten cross fold validation is adopted both for obtaining clusters\nand building the predictive models. The EM clustering could identify all the\nzones as per the Koppen classification over Indian region. The proposed\nstrategy when employed for predicting temperature has resulted in an RMSE of\n$1.19$ in the Montane climate region and $0.89$ in the Humid Sub Tropical\nregion as compared to $2.9$ and $0.95$ respectively predicted using k-means and\nlinear regression method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 11:04:42 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Mallenahalli", "Naresh Kumar", ""]]}, {"id": "1509.06957", "submitter": "Ville Hyv\\\"onen", "authors": "Ville Hyv\\\"onen, Teemu Pitk\\\"anen, Sotiris Tasoulis, Elias\n  J\\\"a\\\"asaari, Risto Tuomainen, Liang Wang, Jukka Corander, Teemu Roos", "title": "Fast k-NN search", "comments": null, "journal-ref": "IEEE International Conference on Big Data 2016, p. 881-888", "doi": "10.1109/BigData.2016.7840682", "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient index structures for fast approximate nearest neighbor queries are\nrequired in many applications such as recommendation systems. In\nhigh-dimensional spaces, many conventional methods suffer from excessive usage\nof memory and slow response times. We propose a method where multiple random\nprojection trees are combined by a novel voting scheme. The key idea is to\nexploit the redundancy in a large number of candidate sets obtained by\nindependently generated random projections in order to reduce the number of\nexpensive exact distance evaluations. The method is straightforward to\nimplement using sparse projections which leads to a reduced memory footprint\nand fast index construction. Furthermore, it enables grouping of the required\ncomputations into big matrix multiplications, which leads to additional savings\ndue to cache effects and low-level parallelization. We demonstrate by extensive\nexperiments on a wide variety of data sets that the method is faster than\nexisting partitioning tree or hashing based approaches, making it the fastest\navailable technique on high accuracy levels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 13:10:36 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 12:54:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hyv\u00f6nen", "Ville", ""], ["Pitk\u00e4nen", "Teemu", ""], ["Tasoulis", "Sotiris", ""], ["J\u00e4\u00e4saari", "Elias", ""], ["Tuomainen", "Risto", ""], ["Wang", "Liang", ""], ["Corander", "Jukka", ""], ["Roos", "Teemu", ""]]}, {"id": "1509.07078", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Erik M. Bollt", "title": "Detecting phase transitions in collective behavior using manifold's\n  curvature", "comments": "17 pages, 9 figures, accepted in Journal of Mathematical Bioscience\n  and Engineering", "journal-ref": null, "doi": "10.3934/mbe.2017027", "report-no": null, "categories": "math.DS cs.LG cs.MA math.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a given behavior of a multi-agent system restricts the phase variable to a\ninvariant manifold, then we define a phase transition as change of physical\ncharacteristics such as speed, coordination, and structure. We define such a\nphase transition as splitting an underlying manifold into two sub-manifolds\nwith distinct dimensionalities around the singularity where the phase\ntransition physically exists. Here, we propose a method of detecting phase\ntransitions and splitting the manifold into phase transitions free\nsub-manifolds. Therein, we utilize a relationship between curvature and\nsingular value ratio of points sampled in a curve, and then extend the\nassertion into higher-dimensions using the shape operator. Then we attest that\nthe same phase transition can also be approximated by singular value ratios\ncomputed locally over the data in a neighborhood on the manifold. We validate\nthe phase transitions detection method using one particle simulation and three\nreal world examples.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:04:56 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 16:41:30 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1509.07087", "submitter": "Zhe Gan", "authors": "Zhe Gan, Chunyuan Li, Ricardo Henao, David Carlson and Lawrence Carin", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "comments": "to appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep dynamic generative models are developed to learn sequential dependencies\nin time-series data. The multi-layered model is designed by constructing a\nhierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential\nstack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden\nstate, inherited from the previous SBNs in the sequence, and is used to\nregulate its hidden bias. Scalable learning and inference algorithms are\nderived by introducing a recognition model that yields fast sampling from the\nvariational posterior. This recognition model is trained jointly with the\ngenerative model, by maximizing its variational lower bound on the\nlog-likelihood. Experimental results on bouncing balls, polyphonic music,\nmotion capture, and text streams show that the proposed approach achieves\nstate-of-the-art predictive performance, and has the capacity to synthesize\nvarious sequences.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:36:42 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Gan", "Zhe", ""], ["Li", "Chunyuan", ""], ["Henao", "Ricardo", ""], ["Carlson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1509.07093", "submitter": "Pablo Estevez Prof.", "authors": "David Nova and Pablo A. Estevez", "title": "A review of learning vector quantization classifiers", "comments": "14 pages", "journal-ref": "Neural Computing & Applications, vol. 25, pp. 511-524, 2014", "doi": "10.1007/s00521-013-1535-3", "report-no": null, "categories": "cs.LG astro-ph.IM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a review of the state of the art of Learning Vector\nQuantization (LVQ) classifiers. A taxonomy is proposed which integrates the\nmost relevant LVQ approaches to date. The main concepts associated with modern\nLVQ approaches are defined. A comparison is made among eleven LVQ classifiers\nusing one real-world and two artificial datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:46:31 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Nova", "David", ""], ["Estevez", "Pablo A.", ""]]}, {"id": "1509.07179", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang and Shyam Upadhyay and Ming-Wei Chang and Vivek Srikumar\n  and Dan Roth", "title": "IllinoisSL: A JAVA Library for Structured Prediction", "comments": "http://cogcomp.cs.illinois.edu/software/illinois-sl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IllinoisSL is a Java library for learning structured prediction models. It\nsupports structured Support Vector Machines and structured Perceptron. The\nlibrary consists of a core learning module and several applications, which can\nbe executed from command-lines. Documentation is provided to guide users. In\nComparison to other structured learning libraries, IllinoisSL is efficient,\ngeneral, and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 23:22:38 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chang", "Kai-Wei", ""], ["Upadhyay", "Shyam", ""], ["Chang", "Ming-Wei", ""], ["Srikumar", "Vivek", ""], ["Roth", "Dan", ""]]}, {"id": "1509.07344", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Julien Velcin, St\\'ephane Bonnevay and Julien Jacques", "title": "Opinion mining from twitter data using evolutionary multinomial mixture\n  models", "comments": "Submitted to the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image of an entity can be defined as a structured and dynamic representation\nwhich can be extracted from the opinions of a group of users or population.\nAutomatic extraction of such an image has certain importance in political\nscience and sociology related studies, e.g., when an extended inquiry from\nlarge-scale data is required. We study the images of two politically\nsignificant entities of France. These images are constructed by analyzing the\nopinions collected from a well known social media called Twitter. Our goal is\nto build a system which can be used to automatically extract the image of\nentities over time.\n  In this paper, we propose a novel evolutionary clustering method based on the\nparametric link among Multinomial mixture models. First we propose the\nformulation of a generalized model that establishes parametric links among the\nMultinomial distributions. Afterward, we follow a model-based clustering\napproach to explore different parametric sub-models and select the best model.\nFor the experiments, first we use synthetic temporal data. Next, we apply the\nmethod to analyze the annotated social media data. Results show that the\nproposed method is better than the state-of-the-art based on the common\nevaluation metrics. Additionally, our method can provide interpretation about\nthe temporal evolution of the clusters.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 12:40:12 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Jacques", "Julien", ""]]}, {"id": "1509.07385", "submitter": "Uri Shaham", "authors": "Uri Shaham, Alexander Cloninger, Ronald R. Coifman", "title": "Provable approximation properties for deep neural networks", "comments": "accepted for publication in Applied and Computational Harmonic\n  Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2016.04.003", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss approximation of functions using deep neural nets. Given a\nfunction $f$ on a $d$-dimensional manifold $\\Gamma \\subset \\mathbb{R}^m$, we\nconstruct a sparsely-connected depth-4 neural network and bound its error in\napproximating $f$. The size of the network depends on dimension and curvature\nof the manifold $\\Gamma$, the complexity of $f$, in terms of its wavelet\ndescription, and only weakly on the ambient dimension $m$. Essentially, our\nnetwork computes wavelet functions, which are computed from Rectified Linear\nUnits (ReLU)\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 14:20:29 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 14:31:54 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 13:46:06 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Shaham", "Uri", ""], ["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1509.07469", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar and Giuseppe Caire", "title": "Channel Vector Subspace Estimation from Low-Dimensional Projections", "comments": "5 Figures, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive MIMO is a variant of multiuser MIMO where the number of base-station\nantennas $M$ is very large (typically 100), and generally much larger than the\nnumber of spatially multiplexed data streams (typically 10). Unfortunately, the\nfront-end A/D conversion necessary to drive hundreds of antennas, with a signal\nbandwidth of the order of 10 to 100 MHz, requires very large sampling bit-rate\nand power consumption.\n  In order to reduce such implementation requirements, Hybrid Digital-Analog\narchitectures have been proposed. In particular, our work in this paper is\nmotivated by one of such schemes named Joint Spatial Division and Multiplexing\n(JSDM), where the downlink precoder (resp., uplink linear receiver) is split\ninto the product of a baseband linear projection (digital) and an RF\nreconfigurable beamforming network (analog), such that only a reduced number $m\n\\ll M$ of A/D converters and RF modulation/demodulation chains is needed. In\nJSDM, users are grouped according to the similarity of their channel dominant\nsubspaces, and these groups are separated by the analog beamforming stage,\nwhere the multiplexing gain in each group is achieved using the digital\nprecoder. Therefore, it is apparent that extracting the channel subspace\ninformation of the $M$-dim channel vectors from snapshots of $m$-dim\nprojections, with $m \\ll M$, plays a fundamental role in JSDM implementation.\n  In this paper, we develop novel efficient algorithms that require sampling\nonly $m = O(2\\sqrt{M})$ specific array elements according to a coprime sampling\nscheme, and for a given $p \\ll M$, return a $p$-dim beamformer that has a\nperformance comparable with the best p-dim beamformer that can be designed from\nthe full knowledge of the exact channel covariance matrix. We assess the\nperformance of our proposed estimators both analytically and empirically via\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 18:39:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 12:13:23 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Haghighatshoar", "Saeid", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1509.07497", "submitter": "Yi (Grace) Wang", "authors": "Yi (Grace) Wang, Guangliang Chen and Mauro Maggioni", "title": "High Dimensional Data Modeling Techniques for Detection of Chemical\n  Plumes and Anomalies in Hyperspectral Images and Movies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly review recent progress in techniques for modeling and analyzing\nhyperspectral images and movies, in particular for detecting plumes of both\nknown and unknown chemicals. For detecting chemicals of known spectrum, we\nextend the technique of using a single subspace for modeling the background to\na \"mixture of subspaces\" model to tackle more complicated background.\nFurthermore, we use partial least squares regression on a resampled training\nset to boost performance. For the detection of unknown chemicals we view the\nproblem as an anomaly detection problem, and use novel estimators with\nlow-sampled complexity for intrinsically low-dimensional data in\nhigh-dimensions that enable us to model the \"normal\" spectra and detect\nanomalies. We apply these algorithms to benchmark data sets made available by\nthe Automated Target Detection program co-funded by NSF, DTRA and NGA, and\ncompare, when applicable, to current state-of-the-art algorithms, with\nfavorable results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 19:59:46 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 20:12:08 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Yi", "", "", "Grace"], ["Wang", "", ""], ["Chen", "Guangliang", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1509.07553", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland and Junier B. Oliva and Barnab\\'as P\\'oczos and\n  Jeff Schneider", "title": "Linear-time Learning on Distributions with Approximate Kernel Embeddings", "comments": null, "journal-ref": "AAAI'16: Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence, February 2016, 2073-2079", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting machine learning problems are best posed by considering\ninstances that are distributions, or sample sets drawn from distributions.\nPrevious work devoted to machine learning tasks with distributional inputs has\ndone so through pairwise kernel evaluations between pdfs (or sample sets).\nWhile such an approach is fine for smaller datasets, the computation of an $N\n\\times N$ Gram matrix is prohibitive in large datasets. Recent scalable\nestimators that work over pdfs have done so only with kernels that use\nEuclidean metrics, like the $L_2$ distance. However, there are a myriad of\nother useful metrics available, such as total variation, Hellinger distance,\nand the Jensen-Shannon divergence. This work develops the first random features\nfor pdfs whose dot product approximates kernels using these non-Euclidean\nmetrics, allowing estimators using such kernels to scale to large datasets by\nworking in a primal space, without computing large Gram matrices. We provide an\nanalysis of the approximation error in using our proposed random features and\nshow empirically the quality of our approximation both in estimating a Gram\nmatrix and in solving learning tasks in real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 22:26:02 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:30:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Oliva", "Junier B.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Schneider", "Jeff", ""]]}, {"id": "1509.07577", "submitter": "Jorge R. Vergara", "authors": "Jorge R. Vergara, Pablo A. Est\\'evez", "title": "A Review of Feature Selection Methods Based on Mutual Information", "comments": null, "journal-ref": "Neural Computing & Applications, vol. 24 (1), pp. 175-186, 2014", "doi": "10.1007/s00521-013-1368-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a review of the state of the art of information\ntheoretic feature selection methods. The concepts of feature relevance,\nredundance and complementarity (synergy) are clearly defined, as well as Markov\nblanket. The problem of optimal feature selection is defined. A unifying\ntheoretical framework is described, which can retrofit successful heuristic\ncriteria, indicating the approximations made by each method. A number of open\nproblems in the field are presented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:36:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Vergara", "Jorge R.", ""], ["Est\u00e9vez", "Pablo A.", ""]]}, {"id": "1509.07636", "submitter": "Irene Winkler", "authors": "Irene Winkler and Danny Panknin and Daniel Bartz and Klaus-Robert\n  M\\\"uller and Stefan Haufe", "title": "Validity of time reversal for testing Granger causality", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2531628", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal interactions from observed data is a challenging problem,\nespecially in the presence of measurement noise. To alleviate the problem of\nspurious causality, Haufe et al. (2013) proposed to contrast measures of\ninformation flow obtained on the original data against the same measures\nobtained on time-reversed data. They show that this procedure, time-reversed\nGranger causality (TRGC), robustly rejects causal interpretations on mixtures\nof independent signals. While promising results have been achieved in\nsimulations, it was so far unknown whether time reversal leads to valid\nmeasures of information flow in the presence of true interaction. Here we prove\nthat, for linear finite-order autoregressive processes with unidirectional\ninformation flow, the application of time reversal for testing Granger\ncausality indeed leads to correct estimates of information flow and its\ndirectionality. Using simulations, we further show that TRGC is able to infer\ncorrect directionality with similar statistical power as the net Granger\ncausality between two variables, while being much more robust to the presence\nof measurement noise.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 08:58:24 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 19:55:53 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Winkler", "Irene", ""], ["Panknin", "Danny", ""], ["Bartz", "Daniel", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Haufe", "Stefan", ""]]}, {"id": "1509.07751", "submitter": "Josef H\\\"o\\\"ok", "authors": "Lars Josef H\\\"o\\\"ok and Erik Lindstr\\\"om", "title": "Efficient Computation of the Quasi Likelihood function for Discretely\n  Observed Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple method for nearly simultaneous computation of all\nmoments needed for quasi maximum likelihood estimation of parameters in\ndiscretely observed stochastic differential equations commonly seen in finance.\nThe method proposed in this papers is not restricted to any particular dynamics\nof the differential equation and is virtually insensitive to the sampling\ninterval. The key contribution of the paper is that computational complexity is\nsublinear in the number of observations as we compute all moments through a\nsingle operation. Furthermore, that operation can be done offline. The\nsimulations show that the method is unbiased for all practical purposes for any\nsampling design, including random sampling, and that the computational cost is\ncomparable (actually faster for moderate and large data sets) to the simple,\noften severely biased, Euler-Maruyama approximation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 15:17:37 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["H\u00f6\u00f6k", "Lars Josef", ""], ["Lindstr\u00f6m", "Erik", ""]]}, {"id": "1509.07859", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Information Limits for Recovering a Hidden Community", "comments": "v2 establishes information limits of both weak and exact recovery\n  with sharp constants for general P and Q", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering a hidden community of cardinality $K$ from\nan $n \\times n$ symmetric data matrix $A$, where for distinct indices $i,j$,\n$A_{ij} \\sim P$ if $i, j$ both belong to the community and $A_{ij} \\sim Q$\notherwise, for two known probability distributions $P$ and $Q$ depending on\n$n$. If $P={\\rm Bern}(p)$ and $Q={\\rm Bern}(q)$ with $p>q$, it reduces to the\nproblem of finding a densely-connected $K$-subgraph planted in a large\nErd\\\"os-R\\'enyi graph; if $P=\\mathcal{N}(\\mu,1)$ and $Q=\\mathcal{N}(0,1)$ with\n$\\mu>0$, it corresponds to the problem of locating a $K \\times K$ principal\nsubmatrix of elevated means in a large Gaussian random matrix. We focus on two\ntypes of asymptotic recovery guarantees as $n \\to \\infty$: (1) weak recovery:\nexpected number of classification errors is $o(K)$; (2) exact recovery:\nprobability of classifying all indices correctly converges to one. Under mild\nassumptions on $P$ and $Q$, and allowing the community size to scale\nsublinearly with $n$, we derive a set of sufficient conditions and a set of\nnecessary conditions for recovery, which are asymptotically tight with sharp\nconstants. The results hold in particular for the Gaussian case, and for the\ncase of bounded log likelihood ratio, including the Bernoulli case whenever\n$\\frac{p}{q}$ and $\\frac{1-p}{1-q}$ are bounded away from zero and infinity. An\nimportant algorithmic implication is that, whenever exact recovery is\ninformation theoretically possible, any algorithm that provides weak recovery\nwhen the community size is concentrated near $K$ can be upgraded to achieve\nexact recovery in linear additional time by a simple voting procedure.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 19:56:35 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 04:05:25 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1509.07892", "submitter": "Alex Kantchelian", "authors": "Alex Kantchelian, J. D. Tygar, Anthony D. Joseph", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "comments": "11 pages, 7 figures, Appears in Proceedings of the 33rd International\n  Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&CP\n  volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier evasion consists in finding for a given instance $x$ the nearest\ninstance $x'$ such that the classifier predictions of $x$ and $x'$ are\ndifferent. We present two novel algorithms for systematically computing\nevasions for tree ensembles such as boosted trees and random forests. Our first\nalgorithm uses a Mixed Integer Linear Program solver and finds the optimal\nevading instance under an expressive set of constraints. Our second algorithm\ntrades off optimality for speed by using symbolic prediction, a novel algorithm\nfor fast finite differences on tree ensembles. On a digit recognition task, we\ndemonstrate that both gradient boosted trees and random forests are extremely\nsusceptible to evasions. Finally, we harden a boosted tree model without loss\nof predictive accuracy by augmenting the training set of each boosting round\nwith evading instances, a technique we call adversarial boosting.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 20:57:35 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 01:09:22 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Kantchelian", "Alex", ""], ["Tygar", "J. D.", ""], ["Joseph", "Anthony D.", ""]]}, {"id": "1509.07982", "submitter": "Carel F.W. Peeters", "authors": "Anders Ellern Bilgrau, Carel F.W. Peeters, Poul Svante Eriksen, Martin\n  B{\\o}gsted, Wessel N. van Wieringen", "title": "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from\n  Multiple High-Dimensional Data Classes", "comments": "52 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research, 21(26):1--52, 2020", "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of jointly estimating multiple inverse covariance\nmatrices from high-dimensional data consisting of distinct classes. An\n$\\ell_2$-penalized maximum likelihood approach is employed. The suggested\napproach is flexible and generic, incorporating several other\n$\\ell_2$-penalized estimators as special cases. In addition, the approach\nallows specification of target matrices through which prior knowledge may be\nincorporated and which can stabilize the estimation procedure in\nhigh-dimensional settings. The result is a targeted fused ridge estimator that\nis of use when the precision matrices of the constituent classes are believed\nto chiefly share the same structure while potentially differing in a number of\nlocations of interest. It has many applications in (multi)factorial study\ndesigns. We focus on the graphical interpretation of precision matrices with\nthe proposed estimator then serving as a basis for integrative or meta-analytic\nGaussian graphical modeling. Situations are considered in which the classes are\ndefined by data sets and subtypes of diseases. The performance of the proposed\nestimator in the graphical modeling setting is assessed through extensive\nsimulation experiments. Its practical usability is illustrated by the\ndifferential network modeling of 12 large-scale gene expression data sets of\ndiffuse large B-cell lymphoma subtypes. The estimator and its related\nprocedures are incorporated into the R-package rags2ridges.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 14:08:14 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 14:06:45 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Bilgrau", "Anders Ellern", ""], ["Peeters", "Carel F. W.", ""], ["Eriksen", "Poul Svante", ""], ["B\u00f8gsted", "Martin", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "1509.08144", "submitter": "Gautier Marti", "authors": "Gautier Marti, Frank Nielsen, Philippe Donnat", "title": "Optimal Copula Transport for Clustering Multivariate Time Series", "comments": "Accepted at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new methodology for clustering multivariate time series\nleveraging optimal transport between copulas. Copulas are used to encode both\n(i) intra-dependence of a multivariate time series, and (ii) inter-dependence\nbetween two time series. Then, optimal copula transport allows us to define two\ndistances between multivariate time series: (i) one for measuring\nintra-dependence dissimilarity, (ii) another one for measuring inter-dependence\ndissimilarity based on a new multivariate dependence coefficient which is\nrobust to noise, deterministic, and which can target specified dependencies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 21:17:09 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 17:05:49 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1509.08327", "submitter": "Anastasis Georgoulas", "authors": "Anastasis Georgoulas, Jane Hillston and Guido Sanguinetti", "title": "Unbiased Bayesian Inference for Population Markov Jump Processes via\n  Random Truncations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider continuous time Markovian processes where populations of\nindividual agents interact stochastically according to kinetic rules. Despite\nthe increasing prominence of such models in fields ranging from biology to\nsmart cities, Bayesian inference for such systems remains challenging, as these\nare continuous time, discrete state systems with potentially infinite\nstate-space. Here we propose a novel efficient algorithm for joint state /\nparameter posterior sampling in population Markov Jump processes. We introduce\na class of pseudo-marginal sampling algorithms based on a random truncation\nmethod which enables a principled treatment of infinite state spaces. Extensive\nevaluation on a number of benchmark models shows that this approach achieves\nconsiderable savings compared to state of the art methods, retaining accuracy\nand fast convergence. We also present results on a synthetic biology data set\nshowing the potential for practical usefulness of our work.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:14:07 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 16:38:03 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Georgoulas", "Anastasis", ""], ["Hillston", "Jane", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1509.08329", "submitter": "Alberto N. Escalante-B.", "authors": "Alberto N. Escalante-B., Laurenz Wiskott", "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA\n  for the Design of Training Graphs", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is an unsupervised learning algorithm that\nextracts slowly varying features from a time series. Graph-based SFA (GSFA) is\na supervised extension that can solve regression problems if followed by a\npost-processing regression algorithm. A training graph specifies arbitrary\nconnections between the training samples. The connections in current graphs,\nhowever, only depend on the rank of the involved labels. Exploiting the exact\nlabel values makes further improvements in estimation accuracy possible.\n  In this article, we propose the exact label learning (ELL) method to create a\ngraph that codes the desired label explicitly, so that GSFA is able to extract\na normalized version of it directly. The ELL method is used for three tasks:\n(1) We estimate gender from artificial images of human faces (regression) and\nshow the advantage of coding additional labels, particularly skin color. (2) We\nanalyze two existing graphs for regression. (3) We extract compact\ndiscriminative features to classify traffic sign images. When the number of\noutput features is limited, a higher classification rate is obtained compared\nto a graph equivalent to nonlinear Fisher discriminant analysis. The method is\nversatile, directly supports multiple labels, and provides higher accuracy\ncompared to current graphs for the problems considered.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:19:59 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1509.08333", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu and Nikhil Rao and Inderjit S. Dhillon", "title": "High-dimensional Time Series Prediction with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time series prediction is needed in applications as diverse\nas demand forecasting and climatology. Often, such applications require methods\nthat are both highly scalable, and deal with noisy data in terms of corruptions\nor missing values. Classical time series methods usually fall short of handling\nboth these issues. In this paper, we propose to adapt matrix matrix completion\napproaches that have previously been successfully applied to large scale noisy\ndata, but which fail to adequately model high-dimensional time series due to\ntemporal dependencies. We present a novel temporal regularized matrix\nfactorization (TRMF) framework which supports data-driven temporal dependency\nlearning and enables forecasting ability to our new matrix factorization\napproach. TRMF is highly general, and subsumes many existing matrix\nfactorization approaches for time series data. We make interesting connections\nto graph regularized matrix factorization methods in the context of learning\nthe dependencies. Experiments on both real and synthetic data show that TRMF\noutperforms several existing approaches for common time series tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:37:14 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 19:58:05 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 02:50:31 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Rao", "Nikhil", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1509.08360", "submitter": "Dinesh Ramasamy", "authors": "Dinesh Ramasamy and Upamanyu Madhow", "title": "Compressive spectral embedding: sidestepping the SVD", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding based on the Singular Value Decomposition (SVD) is a\nwidely used \"preprocessing\" step in many learning tasks, typically leading to\ndimensionality reduction by projecting onto a number of dominant singular\nvectors and rescaling the coordinate axes (by a predefined function of the\nsingular value). However, the number of such vectors required to capture\nproblem structure grows with problem size, and even partial SVD computation\nbecomes a bottleneck. In this paper, we propose a low-complexity it compressive\nspectral embedding algorithm, which employs random projections and finite order\npolynomial expansions to compute approximations to SVD-based embedding. For an\nm times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)),\nand the embedding dimension is O(log(m+n)), both of which are independent of\nthe number of singular vectors whose effect we wish to capture. To the best of\nour knowledge, this is the first work to circumvent this dependence on the\nnumber of singular vectors for general SVD-based embeddings. The key to\nsidestepping the SVD is the observation that, for downstream inference tasks\nsuch as clustering and classification, we are only interested in using the\nresulting embedding to evaluate pairwise similarity metrics derived from the\neuclidean norm, rather than capturing the effect of the underlying matrix on\narbitrary vectors as a partial SVD tries to do. Our numerical results on\nnetwork datasets demonstrate the efficacy of the proposed method, and motivate\nfurther exploration of its application to large-scale inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 15:32:20 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Ramasamy", "Dinesh", ""], ["Madhow", "Upamanyu", ""]]}, {"id": "1509.08387", "submitter": "John Lipor", "authors": "John Lipor, Brandon Wong, Donald Scavia, Branko Kerkez, and Laura\n  Balzano", "title": "Distance-Penalized Active Learning Using Quantile Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sampling theory has shown that, with proper assumptions on the\nsignal class, algorithms exist to reconstruct a signal in $\\mathbb{R}^{d}$ with\nan optimal number of samples. We generalize this problem to the case of spatial\nsignals, where the sampling cost is a function of both the number of samples\ntaken and the distance traveled during estimation. This is motivated by our\nwork studying regions of low oxygen concentration in the Great Lakes. We show\nthat for one-dimensional threshold classifiers, a tradeoff between the number\nof samples taken and distance traveled can be achieved using a generalization\nof binary search, which we refer to as quantile search. We characterize both\nthe estimation error after a fixed number of samples and the distance traveled\nin the noiseless case, as well as the estimation error in the case of noisy\nmeasurements. We illustrate our results in both simulations and experiments and\nshow that our method outperforms existing algorithms in the majority of\npractical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 16:48:39 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 22:38:40 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Lipor", "John", ""], ["Wong", "Brandon", ""], ["Scavia", "Donald", ""], ["Kerkez", "Branko", ""], ["Balzano", "Laura", ""]]}, {"id": "1509.08455", "submitter": "Maximilian Karl", "authors": "Maximilian Karl, Justin Bayer, Patrick van der Smagt", "title": "Efficient Empowerment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowerment quantifies the influence an agent has on its environment. This is\nformally achieved by the maximum of the expected KL-divergence between the\ndistribution of the successor state conditioned on a specific action and a\ndistribution where the actions are marginalised out. This is a natural\ncandidate for an intrinsic reward signal in the context of reinforcement\nlearning: the agent will place itself in a situation where its action have\nmaximum stability and maximum influence on the future. The limiting factor so\nfar has been the computational complexity of the method: the only way of\ncalculation has so far been a brute force algorithm, reducing the applicability\nof the method to environments with a small set discrete states. In this work,\nwe propose to use an efficient approximation for marginalising out the actions\nin the case of continuous environments. This allows fast evaluation of\nempowerment, paving the way towards challenging environments such as real world\nrobotics. The method is presented on a pendulum swing up problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:58:31 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Karl", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1509.08535", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Barnabas Poczos, Russell Greiner", "title": "Boolean Matrix Factorization and Noisy Completion via Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.DM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix factorization and Boolean matrix completion from noisy\nobservations are desirable unsupervised data-analysis methods due to their\ninterpretability, but hard to perform due to their NP-hardness. We treat these\nproblems as maximum a posteriori inference problems in a graphical model and\npresent a message passing approach that scales linearly with the number of\nobservations and factors. Our empirical study demonstrates that message passing\nis able to recover low-rank Boolean matrices, in the boundaries of\ntheoretically possible recovery and compares favorably with state-of-the-art in\nreal-world applications, such collaborative filtering with large-scale Boolean\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 23:11:16 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 19:27:13 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 21:05:32 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""], ["Greiner", "Russell", ""]]}, {"id": "1509.08581", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Optimization over Sparse Symmetric Sets via a Nonmonotone Projected\n  Gradient Method", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a Lipschitz differentiable function\nover a class of sparse symmetric sets that has wide applications in engineering\nand science. For this problem, it is known that any accumulation point of the\nclassical projected gradient (PG) method with a constant stepsize $1/L$\nsatisfies the $L$-stationarity optimality condition that was introduced in [3].\nIn this paper we introduce a new optimality condition that is stronger than the\n$L$-stationarity optimality condition. We also propose a nonmonotone projected\ngradient (NPG) method for this problem by incorporating some support-changing\nand coordintate-swapping strategies into a projected gradient method with\nvariable stepsizes. It is shown that any accumulation point of NPG satisfies\nthe new optimality condition and moreover it is a coordinatewise stationary\npoint. Under some suitable assumptions, we further show that it is a global or\na local minimizer of the problem. Numerical experiments are conducted to\ncompare the performance of PG and NPG. The computational results demonstrate\nthat NPG has substantially better solution quality than PG, and moreover, it is\nat least comparable to, but sometimes can be much faster than PG in terms of\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 03:39:01 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 22:19:11 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 18:47:57 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1509.08582", "submitter": "Sanggyun Kim", "authors": "Sanggyun Kim, Diego Mesa, Rui Ma, Todd P. Coleman", "title": "Tractable Fully Bayesian Inference via Convex Optimization and Optimal\n  Transport Theory", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transforming samples from one continuous source\ndistribution into samples from another target distribution. We demonstrate with\noptimal transport theory that when the source distribution can be easily\nsampled from and the target distribution is log-concave, this can be tractably\nsolved with convex optimization. We show that a special case of this, when the\nsource is the prior and the target is the posterior, is Bayesian inference.\nHere, we can tractably calculate the normalization constant and draw posterior\ni.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log\nconcavity of the prior and likelihood: the same criterion for tractable\ncalculation of the maximum a posteriori point estimate. With simulated data, we\ndemonstrate how we can attain the Bayes risk in simulations. With physiologic\ndata, we demonstrate improvements over point estimation in intensive care unit\noutcome prediction and electroencephalography-based sleep staging.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 03:44:36 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Kim", "Sanggyun", ""], ["Mesa", "Diego", ""], ["Ma", "Rui", ""], ["Coleman", "Todd P.", ""]]}, {"id": "1509.08588", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Elizaveta Levina, Ji Zhu", "title": "Estimating network edge probabilities by neighborhood smoothing", "comments": "22 pages, 4 figures, 3 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of probabilities of network edges from the observed adjacency\nmatrix has important applications to predicting missing links and network\ndenoising. It has usually been addressed by estimating the graphon, a function\nthat determines the matrix of edge probabilities, but this is ill-defined\nwithout strong assumptions on the network structure. Here we propose a novel\ncomputationally efficient method, based on neighborhood smoothing to estimate\nthe expectation of the adjacency matrix directly, without making the structural\nassumptions that graphon estimation requires. The neighborhood smoothing method\nrequires little tuning, has a competitive mean-squared error rate, and\noutperforms many benchmark methods on link prediction in simulated and real\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 04:51:24 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2015 20:16:14 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 10:37:22 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhang", "Yuan", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1509.08627", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Semantics, Representations and Grammars for Deep Learning", "comments": "20 pages, many diagrams", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is currently the subject of intensive study. However,\nfundamental concepts such as representations are not formally defined --\nresearchers \"know them when they see them\" -- and there is no common language\nfor describing and analyzing algorithms. This essay proposes an abstract\nframework that identifies the essential features of current practice and may\nprovide a foundation for future developments.\n  The backbone of almost all deep learning algorithms is backpropagation, which\nis simply a gradient computation distributed over a neural network. The main\ningredients of the framework are thus, unsurprisingly: (i) game theory, to\nformalize distributed optimization; and (ii) communication protocols, to track\nthe flow of zeroth and first-order information. The framework allows natural\ndefinitions of semantics (as the meaning encoded in functions), representations\n(as functions whose semantics is chosen to optimized a criterion) and grammars\n(as communication protocols equipped with first-order convergence guarantees).\n  Much of the essay is spent discussing examples taken from the literature. The\nultimate aim is to develop a graphical language for describing the structure of\ndeep learning algorithms that backgrounds the details of the optimization\nprocedure and foregrounds how the components interact. Inspiration is taken\nfrom probabilistic graphical models and factor graphs, which capture the\nessential structural features of multivariate distributions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:14:21 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1509.08634", "submitter": "Takayuki Osogami", "authors": "Takayuki Osogami and Makoto Otsuka", "title": "Learning dynamic Boltzmann machines with spike-timing dependent\n  plasticity", "comments": "Preliminary and substantially different version of the paper appeared\n  in http://www.nature.com/articles/srep14149", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a particularly structured Boltzmann machine, which we refer to as\na dynamic Boltzmann machine (DyBM), as a stochastic model of a\nmulti-dimensional time-series. The DyBM can have infinitely many layers of\nunits but allows exact and efficient inference and learning when its parameters\nhave a proposed structure. This proposed structure is motivated by postulates\nand observations, from biological neural networks, that the synaptic weight is\nstrengthened or weakened, depending on the timing of spikes (i.e., spike-timing\ndependent plasticity or STDP). We show that the learning rule of updating the\nparameters of the DyBM in the direction of maximizing the likelihood of given\ntime-series can be interpreted as STDP with long term potentiation and long\nterm depression. The learning rule has a guarantee of convergence and can be\nperformed in a distributed matter (i.e., local in space) with limited memory\n(i.e., local in time).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:30:12 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Osogami", "Takayuki", ""], ["Otsuka", "Makoto", ""]]}, {"id": "1509.08644", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Neural-based machine translation for medical text domain. Based on\n  European Medicines Agency leaflet texts", "comments": "machine translation, statistical machine translation, neural machine\n  trasnlation, nlp, text processing, medical communication", "journal-ref": "Procedia Computer Science, 2015, 64: 2-9", "doi": "10.1016/j.procs.2015.08.456", "report-no": null, "categories": "cs.CL cs.CY cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of machine translation is rapidly evolving. Today one can find\nseveral machine translation systems on the web that provide reasonable\ntranslations, although the systems are not perfect. In some specific domains,\nthe quality may decrease. A recently proposed approach to this domain is neural\nmachine translation. It aims at building a jointly-tuned single neural network\nthat maximizes translation performance, a very different approach from\ntraditional statistical machine translation. Recently proposed neural machine\ntranslation models often belong to the encoder-decoder family in which a source\nsentence is encoded into a fixed length vector that is, in turn, decoded to\ngenerate a translation. The present research examines the effects of different\ntraining methods on a Polish-English Machine Translation system used for\nmedical data. The European Medicines Agency parallel text corpus was used as\nthe basis for training of neural and statistical network-based translation\nsystems. The main machine translation evaluation metrics have also been used in\nanalysis of the systems. A comparison and implementation of a real-time medical\ntranslator is the main focus of our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:54:48 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08660", "submitter": "Jesus Fernandez-Bes", "authors": "Jesus Fernandez-Bes, Roc\\'io Arroyo-Valles, Jer\\'onimo\n  Arenas-Garc\\'ia, Jes\\'us Cid-Sueiro", "title": "Censoring Diffusion for Harvesting WSNs", "comments": "Accepted in 2015 IEEE International Workshop on Computational\n  Advances in Multi-Sensor Adaptive Processing (CAMSAP 2015)", "journal-ref": null, "doi": "10.1109/CAMSAP.2015.7383780", "report-no": null, "categories": "cs.SY cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze energy-harvesting adaptive diffusion networks for a\ndistributed estimation problem. In order to wisely manage the available energy\nresources, we propose a scheme where a censoring algorithm is jointly applied\nover the diffusion strategy. An energy-aware variation of a diffusion algorithm\nis used, and a new way of measuring the relevance of the estimates in diffusion\nnetworks is proposed in order to apply a subsequent censoring mechanism.\nSimulation results show the potential benefit of integrating censoring schemes\nin energy-constrained diffusion networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 09:41:29 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fernandez-Bes", "Jesus", ""], ["Arroyo-Valles", "Roc\u00edo", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""], ["Cid-Sueiro", "Jes\u00fas", ""]]}, {"id": "1509.08731", "submitter": "Shakir Mohamed", "authors": "Shakir Mohamed and Danilo Jimenez Rezende", "title": "Variational Information Maximisation for Intrinsically Motivated\n  Reinforcement Learning", "comments": "Proceedings of the 29th Conference on Neural Information Processing\n  Systems (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mutual information is a core statistical quantity that has applications\nin all areas of machine learning, whether this is in training of density models\nover multiple data modalities, in maximising the efficiency of noisy\ntransmission channels, or when learning behaviour policies for exploration by\nartificial agents. Most learning algorithms that involve optimisation of the\nmutual information rely on the Blahut-Arimoto algorithm --- an enumerative\nalgorithm with exponential complexity that is not suitable for modern machine\nlearning applications. This paper provides a new approach for scalable\noptimisation of the mutual information by merging techniques from variational\ninference and deep learning. We develop our approach by focusing on the problem\nof intrinsically-motivated learning, where the mutual information forms the\ndefinition of a well-known internal drive known as empowerment. Using a\nvariational lower bound on the mutual information, combined with convolutional\nnetworks for handling visual input streams, we develop a stochastic\noptimisation algorithm that allows for scalable information maximisation and\nempowerment-based reasoning directly from pixels to actions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 13:04:03 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Mohamed", "Shakir", ""], ["Rezende", "Danilo Jimenez", ""]]}, {"id": "1509.08880", "submitter": "Dmitry Storcheus", "authors": "Mehryar Mohri, Afshin Rostamizadeh, Dmitry Storcheus", "title": "Foundations of Coupled Nonlinear Dimensionality Reduction", "comments": "12 pages, 3 figures, authors in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and analyze the learning scenario of \\emph{coupled\nnonlinear dimensionality reduction}, which combines two major steps of machine\nlearning pipeline: projection onto a manifold and subsequent supervised\nlearning. First, we present new generalization bounds for this scenario and,\nsecond, we introduce an algorithm that follows from these bounds. The\ngeneralization error bound is based on a careful analysis of the empirical\nRademacher complexity of the relevant hypothesis set. In particular, we show an\nupper bound on the Rademacher complexity that is in $\\widetilde\nO(\\sqrt{\\Lambda_{(r)}/m})$, where $m$ is the sample size and $\\Lambda_{(r)}$\nthe upper bound on the Ky-Fan $r$-norm of the associated kernel matrix. We give\nboth upper and lower bound guarantees in terms of that Ky-Fan $r$-norm, which\nstrongly justifies the definition of our hypothesis set. To the best of our\nknowledge, these are the first learning guarantees for the problem of coupled\ndimensionality reduction. Our analysis and learning guarantees further apply to\nseveral special cases, such as that of using a fixed kernel with supervised\ndimensionality reduction or that of unsupervised learning of a kernel for\ndimensionality reduction followed by a supervised learning algorithm. Based on\ntheoretical analysis, we suggest a structural risk minimization algorithm\nconsisting of the coupled fitting of a low dimensional manifold and a\nseparation function on that manifold.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:33:57 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 16:51:31 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""], ["Storcheus", "Dmitry", ""]]}, {"id": "1509.08881", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Building Subject-aligned Comparable Corpora and Mining it for Truly\n  Parallel Sentence Pairs", "comments": null, "journal-ref": "Procedia Technology, 18, Elsevier, p.126-132, 2014", "doi": "10.1016/j.protcy.2014.11.024", "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentences are a relatively scarce but extremely useful resource for\nmany applications including cross-lingual retrieval and statistical machine\ntranslation. This research explores our methodology for mining such data from\npreviously obtained comparable corpora. The task is highly practical since\nnon-parallel multilingual data exist in far greater quantities than parallel\ncorpora, but parallel sentences are a much more useful resource. Here we\npropose a web crawling method for building subject-aligned comparable corpora\nfrom Wikipedia articles. We also introduce a method for extracting truly\nparallel sentences that are filtered out from noisy or just comparable sentence\npairs. We describe our implementation of a specialized tool for this task as\nwell as training and adaption of a machine translation system that supplies our\nfilter with additional information about the similarity of comparable sentence\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:35:49 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08909", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish -English Statistical Machine Translation of Medical Texts", "comments": "New Research in Multimedia and Internet Systems, Springer. 09/2014,\n  ISSN: 1867-5662. arXiv admin note: text overlap with arXiv:1509.08874", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This new research explores the effects of various training methods on a\nPolish to English Statistical Machine Translation system for medical texts.\nVarious elements of the EMEA parallel text corpora from the OPUS project were\nused as the basis for training of phrase tables and language models and for\ndevelopment, tuning and testing of the translation system. The BLEU, NIST,\nMETEOR, RIBES and TER metrics have been used to evaluate the effects of various\nsystem and data preparations on translation results. Our experiments included\nsystems that used POS tagging, factored phrase models, hierarchical models,\nsyntactic taggers, and many different alignment methods. We also conducted a\ndeep analysis of Polish data as preparatory work for automatic data correction\nsuch as true casing and punctuation normalization phase.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 19:57:24 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08985", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee, Patrick W. Gallagher, Zhuowen Tu", "title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,\n  Gated, and Tree", "comments": "Patent disclosure, UCSD Docket No. SD2015-184, \"Forest Convolutional\n  Neural Network\", filed on March 4, 2015. UCSD Docket No. SD2016-053,\n  \"Generalizing Pooling Functions in Convolutional Neural Network\", filed on\n  Sept 23, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to improve deep neural networks by generalizing the pooling\noperations that play a central role in current architectures. We pursue a\ncareful exploration of approaches to allow pooling to learn and to adapt to\ncomplex and variable patterns. The two primary directions lie in (1) learning a\npooling function via (two strategies of) combining of max and average pooling,\nand (2) learning a pooling function in the form of a tree-structured fusion of\npooling filters that are themselves learned. In our experiments every\ngeneralized pooling operation we explore improves performance when used in\nplace of average or max pooling. We experimentally demonstrate that the\nproposed pooling operations provide a boost in invariance properties relative\nto conventional pooling and set the state of the art on several widely adopted\nbenchmark datasets; they are also easy to implement, and can be applied within\nvarious deep neural network architectures. These benefits come with only a\nlight increase in computational overhead during training and a very modest\nincrease in the number of model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:06:36 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 03:18:45 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Gallagher", "Patrick W.", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1509.08990", "submitter": "Mohammad Amin Rahimian", "authors": "Mohammad Amin Rahimian and Ali Jadbabaie", "title": "Learning without Recall: A Case for Log-Linear Learning", "comments": "in 5th IFAC Workshop on Distributed Estimation and Control in\n  Networked Systems, (NecSys 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a model of learning and belief formation in networks in which\nagents follow Bayes rule yet they do not recall their history of past\nobservations and cannot reason about how other agents' beliefs are formed. They\ndo so by making rational inferences about their observations which include a\nsequence of independent and identically distributed private signals as well as\nthe beliefs of their neighboring agents at each time. Fully rational agents\nwould successively apply Bayes rule to the entire history of observations. This\nleads to forebodingly complex inferences due to lack of knowledge about the\nglobal network structure that causes those observations. To address these\ncomplexities, we consider a Learning without Recall model, which in addition to\nproviding a tractable framework for analyzing the behavior of rational agents\nin social networks, can also provide a behavioral foundation for the variety of\nnon-Bayesian update rules in the literature. We present the implications of\nvarious choices for time-varying priors of such agents and how this choice\naffects learning and its rate.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:43:09 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Rahimian", "Mohammad Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1509.08992", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing\n  Parameter Sets", "comments": "Advances in Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference is typically intractable in high-treewidth undirected graphical\nmodels, making maximum likelihood learning a challenge. One way to overcome\nthis is to restrict parameters to a tractable set, most typically the set of\ntree-structured parameters. This paper explores an alternative notion of a\ntractable set, namely a set of \"fast-mixing parameters\" where Markov chain\nMonte Carlo (MCMC) inference can be guaranteed to quickly converge to the\nstationary distribution. While it is common in practice to approximate the\nlikelihood gradient using samples obtained from MCMC, such procedures lack\ntheoretical guarantees. This paper proves that for any exponential family with\nbounded sufficient statistics, (not just graphical models) when parameters are\nconstrained to a fast-mixing set, gradient descent with gradients approximated\nby sampling will approximate the maximum likelihood solution inside the set\nwith high-probability. When unregularized, to find a solution epsilon-accurate\nin log-likelihood requires a total amount of effort cubic in 1/epsilon,\ndisregarding logarithmic factors. When ridge-regularized, strong convexity\nallows a solution epsilon-accurate in parameter distance with effort quadratic\nin 1/epsilon. Both of these provide of a fully-polynomial time randomized\napproximation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:44:41 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 07:29:08 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1509.09002", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Convergence of Stochastic Gradient Descent for PCA", "comments": "Added analysis of the positive eigengap scenario, with new results;\n  Some minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of principal component analysis (PCA) in a streaming\nstochastic setting, where our goal is to find a direction of approximate\nmaximal variance, based on a stream of i.i.d. data points in $\\reals^d$. A\nsimple and computationally cheap algorithm for this is stochastic gradient\ndescent (SGD), which incrementally updates its estimate based on each new data\npoint. However, due to the non-convex nature of the problem, analyzing its\nperformance has been a challenge. In particular, existing guarantees rely on a\nnon-trivial eigengap assumption on the covariance matrix, which is intuitively\nunnecessary. In this paper, we provide (to the best of our knowledge) the first\neigengap-free convergence guarantees for SGD in the context of PCA. This also\npartially resolves an open problem posed in \\cite{hardt2014noisy}. Moreover,\nunder an eigengap assumption, we show that the same techniques lead to new SGD\nconvergence guarantees with better dependence on the eigengap.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 03:02:59 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 08:25:56 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1509.09011", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hiroshi Nakagawa", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial\n  Monitoring", "comments": "24 pages, to appear in NIPS2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial monitoring is a general model for sequential learning with limited\nfeedback formalized as a game between two players. In this game, the learner\nchooses an action and at the same time the opponent chooses an outcome, then\nthe learner suffers a loss and receives a feedback signal. The goal of the\nlearner is to minimize the total loss. In this paper, we study partial\nmonitoring with finite actions and stochastic outcomes. We derive a logarithmic\ndistribution-dependent regret lower bound that defines the hardness of the\nproblem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the\nmulti-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the\ndistribution-dependent regret. PM-DMED significantly outperforms\nstate-of-the-art algorithms in numerical experiments. To show the optimality of\nPM-DMED with respect to the regret bound, we slightly modify the algorithm by\nintroducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically\noptimal regret upper bound of PM-DMED-Hinge that matches the lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 04:36:40 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1509.09088", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Enhanced Bilingual Evaluation Understudy", "comments": "machine translation evaluation, enchanced bleu. in Lecture Notes on\n  Information Theory, ISSN: 2301-3788, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research extends the Bilingual Evaluation Understudy (BLEU) evaluation\ntechnique for statistical machine translation to make it more adjustable and\nrobust. We intend to adapt it to resemble human evaluation more. We perform\nexperiments to evaluate the performance of our technique against the primary\nexisting evaluation methods. We describe and show the improvements it makes\nover existing methods as well as correlation to them. When human translators\ntranslate a text, they often use synonyms, different word orders or style, and\nother similar variations. We propose an SMT evaluation technique that enhances\nthe BLEU metric to consider variations such as those.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:13:00 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09090", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Real-Time Statistical Speech Translation", "comments": "machine translation, polish english", "journal-ref": "Advances in Intelligent Systems and Computing volume 275,\n  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates the Statistical Machine Translation approaches to\ntranslate speech in real time automatically. Such systems can be used in a\npipeline with speech recognition and synthesis software in order to produce a\nreal-time voice communication system between foreigners. We obtained three main\ndata sets from spoken proceedings that represent three different types of human\nspeech. TED, Europarl, and OPUS parallel text corpora were used as the basis\nfor training of language models, for developmental tuning and testing of the\ntranslation system. We also conducted experiments involving part of speech\ntagging, compound splitting, linear language model interpolation, TrueCasing\nand morphosyntactic analysis. We evaluated the effects of variety of data\npreparations on the translation results using the BLEU, NIST, METEOR and TER\nmetrics and tried to give answer which metric is most suitable for PL-EN\nlanguage pair.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:20:27 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09097", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish - English Speech Statistical Machine Translation Systems for the\n  IWSLT 2013", "comments": "statistical machine translation. arXiv admin note: substantial text\n  overlap with arXiv:1509.08874, arXiv:1509.08909", "journal-ref": "Proceedings of the 10th International Workshop on Spoken Language\n  Translation, Heidelberg, Germany, p. 113-119, 2013", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research explores the effects of various training settings from Polish\nto English Statistical Machine Translation system for spoken language. Various\nelements of the TED parallel text corpora for the IWSLT 2013 evaluation\ncampaign were used as the basis for training of language models, and for\ndevelopment, tuning and testing of the translation system. The BLEU, NIST,\nMETEOR and TER metrics were used to evaluate the effects of data preparations\non translation results. Our experiments included systems, which use stems and\nmorphological information on Polish words. We also conducted a deep analysis of\nprovided Polish data as preparatory work for the automatic data correction and\ncleaning phase.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:35:38 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09130", "submitter": "Claire Vernade", "authors": "Claire Vernade (LTCI), Olivier Capp\\'e (LTCI)", "title": "Learning From Missing Data Using Selection Bias in Movie Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending items to users is a challenging task due to the large amount of\nmissing information. In many cases, the data solely consist of ratings or tags\nvoluntarily contributed by each user on a very limited subset of the available\nitems, so that most of the data of potential interest is actually missing.\nCurrent approaches to recommendation usually assume that the unobserved data is\nmissing at random. In this contribution, we provide statistical evidence that\nexisting movie recommendation datasets reveal a significant positive\nassociation between the rating of items and the propensity to select these\nitems. We propose a computationally efficient variational approach that makes\nit possible to exploit this selection bias so as to improve the estimation of\nratings from small populations of users. Results obtained with this approach\napplied to neighborhood-based collaborative filtering illustrate its potential\nfor improving the reliability of the recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 11:40:21 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Vernade", "Claire", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1509.09259", "submitter": "Soroosh Shafieezadeh-Abadeh", "authors": "Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, Daniel Kuhn", "title": "Distributionally Robust Logistic Regression", "comments": "Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a distributionally robust approach to logistic\nregression. We use the Wasserstein distance to construct a ball in the space of\nprobability distributions centered at the uniform distribution on the training\nsamples. If the radius of this ball is chosen judiciously, we can guarantee\nthat it contains the unknown data-generating distribution with high confidence.\nWe then formulate a distributionally robust logistic regression model that\nminimizes a worst-case expected logloss function, where the worst case is taken\nover all distributions in the Wasserstein ball. We prove that this optimization\nproblem admits a tractable reformulation and encapsulates the classical as well\nas the popular regularized logistic regression problems as special cases. We\nfurther propose a distributionally robust approach based on Wasserstein balls\nto compute upper and lower confidence bounds on the misclassification\nprobability of the resulting classifier. These bounds are given by the optimal\nvalues of two highly tractable linear programs. We validate our theoretical\nout-of-sample guarantees through simulated and empirical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 17:14:46 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 15:35:09 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2015 11:18:50 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Shafieezadeh-Abadeh", "Soroosh", ""], ["Esfahani", "Peyman Mohajerin", ""], ["Kuhn", "Daniel", ""]]}, {"id": "1509.09292", "submitter": "David Duvenaud", "authors": "David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael\n  G\\'omez-Bombarelli, Timothy Hirzel, Al\\'an Aspuru-Guzik, Ryan P. Adams", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "comments": "9 pages, 5 figures. To appear in Neural Information Processing\n  Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a convolutional neural network that operates directly on graphs.\nThese networks allow end-to-end learning of prediction pipelines whose inputs\nare graphs of arbitrary size and shape. The architecture we present generalizes\nstandard molecular feature extraction methods based on circular fingerprints.\nWe show that these data-driven features are more interpretable, and have better\npredictive performance on a variety of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 18:33:50 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 17:18:32 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Duvenaud", "David", ""], ["Maclaurin", "Dougal", ""], ["Aguilera-Iparraguirre", "Jorge", ""], ["G\u00f3mez-Bombarelli", "Rafael", ""], ["Hirzel", "Timothy", ""], ["Aspuru-Guzik", "Al\u00e1n", ""], ["Adams", "Ryan P.", ""]]}]