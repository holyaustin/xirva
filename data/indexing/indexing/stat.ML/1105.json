[{"id": "1105.0121", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Methods of Hierarchical Clustering", "comments": "21 pages, 2 figures, 1 table, 69 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey agglomerative hierarchical clustering algorithms and discuss\nefficient implementations that are available in R and other software\nenvironments. We look at hierarchical self-organizing maps, and mixture models.\nWe review grid-based clustering, focusing on hierarchical density-based\napproaches. Finally we describe a recently developed very efficient (linear\ntime) hierarchical clustering algorithm, which can also be viewed as a\nhierarchical grid-based algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2011 21:29:08 GMT"}], "update_date": "2011-05-03", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1105.0167", "submitter": "Gang Niu", "authors": "Gang Niu, Bo Dai, Makoto Yamada and Masashi Sugiyama", "title": "SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity", "comments": "The same paper has been submitted to arXiv by ICML 2012. See\n  http://arxiv.org/abs/1206.4614", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general information-theoretic approach called Seraph\n(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric\nlearning that does not rely upon the manifold assumption. Given the probability\nparameterized by a Mahalanobis distance, we maximize the entropy of that\nprobability on labeled data and minimize it on unlabeled data following entropy\nregularization, which allows the supervised and unsupervised parts to be\nintegrated in a natural and meaningful way. Furthermore, Seraph is regularized\nby encouraging a low-rank projection induced from the metric. The optimization\nof Seraph is solved efficiently and stably by an EM-like scheme with the\nanalytical E-Step and convex M-Step. Experiments demonstrate that Seraph\ncompares favorably with many well-known global and local metric learning\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2011 13:07:51 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2011 02:26:05 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 07:05:14 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Niu", "Gang", ""], ["Dai", "Bo", ""], ["Yamada", "Makoto", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1105.0363", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton (LIENS, INRIA Paris - Rocquencourt), Alexandre\n  Gramfort (LNAO, INRIA Saclay - Ile de France), Vincent Michel (LNAO, INRIA\n  Saclay - Ile de France), Guillaume Obozinski (LIENS, INRIA Paris -\n  Rocquencourt), Evelyn Eger, Francis Bach (LIENS, INRIA Paris - Rocquencourt),\n  Bertrand Thirion (LNAO, INRIA Saclay - Ile de France)", "title": "Multi-scale Mining of fMRI data with Hierarchical Structured Sparsity", "comments": "(2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse inference, or \"brain reading\", is a recent paradigm for analyzing\nfunctional magnetic resonance imaging (fMRI) data, based on pattern recognition\nand statistical learning. By predicting some cognitive variables related to\nbrain activation maps, this approach aims at decoding brain activity. Inverse\ninference takes into account the multivariate information between voxels and is\ncurrently the only way to assess how precisely some cognitive information is\nencoded by the activity of neural populations within the whole brain. However,\nit relies on a prediction function that is plagued by the curse of\ndimensionality, since there are far more features than samples, i.e., more\nvoxels than fMRI volumes. To address this problem, different methods have been\nproposed, such as, among others, univariate feature selection, feature\nagglomeration and regularization techniques. In this paper, we consider a\nsparse hierarchical structured regularization. Specifically, the penalization\nwe use is constructed from a tree that is obtained by spatially-constrained\nagglomerative clustering. This approach encodes the spatial structure of the\ndata at different scales into the regularization, which makes the overall\nprediction procedure more robust to inter-subject variability. The\nregularization used induces the selection of spatially coherent predictive\nbrain regions simultaneously at different scales. We test our algorithm on real\ndata acquired to study the mental representation of objects, and we show that\nthe proposed algorithm not only delineates meaningful brain regions but yields\nas well better prediction accuracy than reference methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2011 15:39:50 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 09:18:39 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Michel", "Vincent", "", "LNAO, INRIA\n  Saclay - Ile de France"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris -\n  Rocquencourt"], ["Eger", "Evelyn", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Thirion", "Bertrand", "", "LNAO, INRIA Saclay - Ile de France"]]}, {"id": "1105.0382", "submitter": "Raphael Pelossof", "authors": "Raphael Pelossof and Zhiliang Ying", "title": "Rapid Learning with Stochastic Focus of Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to stop the evaluation of a decision making process when\nthe result of the full evaluation is obvious. This trait is highly desirable\nfor online margin-based machine learning algorithms where a classifier\ntraditionally evaluates all the features for every example. We observe that\nsome examples are easier to classify than others, a phenomenon which is\ncharacterized by the event when most of the features agree on the class of an\nexample. By stopping the feature evaluation when encountering an easy to\nclassify example, the learning algorithm can achieve substantial gains in\ncomputation. Our method provides a natural attention mechanism for learning\nalgorithms. By modifying Pegasos, a margin-based online learning algorithm, to\ninclude our attentive method we lower the number of attributes computed from\n$n$ to an average of $O(\\sqrt{n})$ features without loss in prediction\naccuracy. We demonstrate the effectiveness of Attentive Pegasos on MNIST data.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2011 17:10:49 GMT"}], "update_date": "2011-05-03", "authors_parsed": [["Pelossof", "Raphael", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1105.0540", "submitter": "Samory Kpotufe", "authors": "Samory Kpotufe, Ulrike von Luxburg", "title": "Pruning nearest neighbor cluster trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (k-NN) graphs are widely used in machine learning and data\nmining applications, and our aim is to better understand what they reveal about\nthe cluster structure of the unknown underlying distribution of points.\nMoreover, is it possible to identify spurious structures that might arise due\nto sampling variability?\n  Our first contribution is a statistical analysis that reveals how certain\nsubgraphs of a k-NN graph form a consistent estimator of the cluster tree of\nthe underlying distribution of points. Our second and perhaps most important\ncontribution is the following finite sample guarantee. We carefully work out\nthe tradeoff between aggressive and conservative pruning and are able to\nguarantee the removal of all spurious cluster structures at all levels of the\ntree while at the same time guaranteeing the recovery of salient clusters. This\nis the first such finite sample result in the context of clustering.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 10:34:25 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 14:13:49 GMT"}], "update_date": "2011-05-06", "authors_parsed": [["Kpotufe", "Samory", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1105.0562", "submitter": "Vincent Dubourg", "authors": "V. Dubourg and F. Deheeger and B. Sudret", "title": "Metamodel-based importance sampling for structural reliability analysis", "comments": "20 pages, 7 figures, 2 tables. Preprint submitted to Probabilistic\n  Engineering Mechanics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural reliability methods aim at computing the probability of failure of\nsystems with respect to some prescribed performance functions. In modern\nengineering such functions usually resort to running an expensive-to-evaluate\ncomputational model (e.g. a finite element model). In this respect simulation\nmethods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate\nmodels such as quadratic response surfaces, polynomial chaos expansions or\nkriging (which are built from a limited number of runs of the original model)\nare then introduced as a substitute of the original model to cope with the\ncomputational cost. In practice it is almost impossible to quantify the error\nmade by this substitution though. In this paper we propose to use a kriging\nsurrogate of the performance function as a means to build a quasi-optimal\nimportance sampling density. The probability of failure is eventually obtained\nas the product of an augmented probability computed by substituting the\nmeta-model for the original performance function and a correction term which\nensures that there is no bias in the estimation even if the meta-model is not\nfully accurate. The approach is applied to analytical and finite element\nreliability problems and proves efficient up to 100 random variables.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 12:16:58 GMT"}, {"version": "v2", "created": "Sat, 7 May 2011 09:54:02 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Dubourg", "V.", ""], ["Deheeger", "F.", ""], ["Sudret", "B.", ""]]}, {"id": "1105.0725", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao", "title": "Exploiting Correlation in Sparse Signal Recovery Problems: Multiple\n  Measurement Vectors, Block Sparsity, and Time-Varying Sparsity", "comments": "Extended abstract for ICML 2011 Structured Sparsity: Learning and\n  Inference Workshop. Experiment codes can be downloaded from:\n  http://dsp.ucsd.edu/~zhilin/papers/ICMLworkshop_code.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trend in compressed sensing (CS) is to exploit structure for improved\nreconstruction performance. In the basic CS model, exploiting the clustering\nstructure among nonzero elements in the solution vector has drawn much\nattention, and many algorithms have been proposed. However, few algorithms\nexplicitly consider correlation within a cluster. Meanwhile, in the multiple\nmeasurement vector (MMV) model correlation among multiple solution vectors is\nlargely ignored. Although several recently developed algorithms consider the\nexploitation of the correlation, these algorithms need to know a priori the\ncorrelation structure, thus limiting their effectiveness in practical problems.\n  Recently, we developed a sparse Bayesian learning (SBL) algorithm, namely\nT-SBL, and its variants, which adaptively learn the correlation structure and\nexploit such correlation information to significantly improve reconstruction\nperformance. Here we establish their connections to other popular algorithms,\nsuch as the group Lasso, iterative reweighted $\\ell_1$ and $\\ell_2$ algorithms,\nand algorithms for time-varying sparsity. We also provide strategies to improve\nthese existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 02:43:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2011 23:43:22 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1105.0728", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin and Donald Goldfarb", "title": "Structured Sparsity via Alternating Direction Methods", "comments": null, "journal-ref": "Journal of Machine Learning Research 13 (2012) 1435-1468", "doi": null, "report-no": null, "categories": "math.OC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of sparse learning problems in high dimensional feature\nspace regularized by a structured sparsity-inducing norm which incorporates\nprior knowledge of the group structure of the features. Such problems often\npose a considerable challenge to optimization algorithms due to the\nnon-smoothness and non-separability of the regularization term. In this paper,\nwe focus on two commonly adopted sparsity-inducing regularization terms, the\noverlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\\infty$-norm. We\npropose a unified framework based on the augmented Lagrangian method, under\nwhich problems with both types of regularization and their variants can be\nefficiently solved. As the core building-block of this framework, we develop\nnew algorithms using an alternating partial-linearization/splitting technique,\nand we prove that the accelerated versions of these algorithms require\n$O(\\frac{1}{\\sqrt{\\epsilon}})$ iterations to obtain an $\\epsilon$-optimal\nsolution. To demonstrate the efficiency and relevance of our algorithms, we\ntest them on a collection of data sets and apply them to two real-world\nproblems to compare the relative merits of the two norms.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 03:02:19 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2011 03:29:46 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Qin", "Zhiwei", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1105.0760", "submitter": "Stevenn Volant", "authors": "Stevenn Volant, Marie-Laure Martin Magniette and St\\'ephane Robin", "title": "Variational Bayes approach for model aggregation in unsupervised\n  classification with Markovian dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary unsupervised classification problem where each\nobservation is associated with an unobserved label that we want to retrieve.\nMore precisely, we assume that there are two groups of observation: normal and\nabnormal. The `normal' observations are coming from a known distribution\nwhereas the distribution of the `abnormal' observations is unknown. Several\nmodels have been developed to fit this unknown distribution. In this paper, we\npropose an alternative based on a mixture of Gaussian distributions. The\ninference is done within a variational Bayesian framework and our aim is to\ninfer the posterior probability of belonging to the class of interest. To this\nend, it makes no sense to estimate the mixture component number since each\nmixture model provides more or less relevant information to the posterior\nprobability estimation. By computing a weighted average (named aggregated\nestimator) over the model collection, Bayesian Model Averaging (BMA) is one way\nof combining models in order to account for information provided by each model.\nThe aim is then the estimation of the weights and the posterior probability for\none specific model. In this work, we derive optimal approximations of these\nquantities from the variational theory and propose other approximations of the\nweights. To perform our method, we consider that the data are dependent\n(Markovian dependency) and hence we consider a Hidden Markov Model. A\nsimulation study is carried out to evaluate the accuracy of the estimates in\nterms of classification. We also present an application to the analysis of\npublic health surveillance systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 08:30:58 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Volant", "Stevenn", ""], ["Magniette", "Marie-Laure Martin", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1105.0828", "submitter": "Daniel Stekhoven", "authors": "Daniel J. Stekhoven and Peter B\\\"uhlmann", "title": "MissForest - nonparametric missing value imputation for mixed-type data", "comments": "Submitted to Oxford Journal's Bioinformatics on 3rd of May 2011", "journal-ref": "Bioinformatics Vol. 28 no. 1 2012, pages 112-118", "doi": "10.1093/bioinformatics/btr597", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data acquisition based on high-throughput technology is often facing\nthe problem of missing data. Algorithms commonly used in the analysis of such\nlarge-scale data often depend on a complete set. Missing value imputation\noffers a solution to this problem. However, the majority of available\nimputation methods are restricted to one type of variable only: continuous or\ncategorical. For mixed-type data the different types are usually handled\nseparately. Therefore, these methods ignore possible relations between variable\ntypes. We propose a nonparametric method which can cope with different types of\nvariables simultaneously. We compare several state of the art methods for the\nimputation of missing values. We propose and evaluate an iterative imputation\nmethod (missForest) based on a random forest. By averaging over many unpruned\nclassification or regression trees random forest intrinsically constitutes a\nmultiple imputation scheme. Using the built-in out-of-bag error estimates of\nrandom forest we are able to estimate the imputation error without the need of\na test set. Evaluation is performed on multiple data sets coming from a diverse\nselection of biological fields with artificially introduced missing values\nranging from 10% to 30%. We show that missForest can successfully handle\nmissing values, particularly in data sets including different types of\nvariables. In our comparative study missForest outperforms other methods of\nimputation especially in data settings where complex interactions and nonlinear\nrelations are suspected. The out-of-bag imputation error estimates of\nmissForest prove to be adequate in all settings. Additionally, missForest\nexhibits attractive computational efficiency and can cope with high-dimensional\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 13:53:59 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2011 14:01:38 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Stekhoven", "Daniel J.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1105.0875", "submitter": "Paramveer Dhillon", "authors": "Paramveer S. Dhillon, Dean P. Foster, Sham M. Kakade and Lyle H. Ungar", "title": "A Risk Comparison of Ordinary Least Squares vs Ridge Regression", "comments": "Appearing in JMLR 14, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the risk of ridge regression to a simple variant of ordinary least\nsquares, in which one simply projects the data onto a finite dimensional\nsubspace (as specified by a Principal Component Analysis) and then performs an\nordinary (un-regularized) least squares regression in this subspace. This note\nshows that the risk of this ordinary least squares method is within a constant\nfactor (namely 4) of the risk of ridge regression.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 17:25:52 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 16:52:11 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Dhillon", "Paramveer S.", ""], ["Foster", "Dean P.", ""], ["Kakade", "Sham M.", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1105.0972", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Eddie Xu, Kilian Q. Weinberger, Fei Sha", "title": "Rapid Feature Learning with Stacked Linear Denoisers", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate unsupervised pre-training of deep architectures as feature\ngenerators for \"shallow\" classifiers. Stacked Denoising Autoencoders (SdA),\nwhen used as feature pre-processing tools for SVM classification, can lead to\nsignificant improvements in accuracy - however, at the price of a substantial\nincrease in computational cost. In this paper we create a simple algorithm\nwhich mimics the layer by layer training of SdAs. However, in contrast to SdAs,\nour algorithm requires no training through gradient descent as the parameters\ncan be computed in closed-form. It can be implemented in less than 20 lines of\nMATLABTMand reduces the computation time from several hours to mere seconds. We\nshow that our feature transformation reliably improves the results of SVM\nclassification significantly on all our data sets - often outperforming SdAs\nand even deep neural networks in three out of four deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2011 04:02:35 GMT"}], "update_date": "2012-08-17", "authors_parsed": [["Xu", "Zhixiang Eddie", ""], ["Weinberger", "Kilian Q.", ""], ["Sha", "Fei", ""]]}, {"id": "1105.1178", "submitter": "Daniel Tarlow", "authors": "Daniel Tarlow, Inmar E. Givoni, Richard S. Zemel, Brendan J. Frey", "title": "Interpreting Graph Cuts as a Max-Product Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum a posteriori (MAP) configuration of binary variable models with\nsubmodular graph-structured energy functions can be found efficiently and\nexactly by graph cuts. Max-product belief propagation (MP) has been shown to be\nsuboptimal on this class of energy functions by a canonical counterexample\nwhere MP converges to a suboptimal fixed point (Kulesza & Pereira, 2008).\n  In this work, we show that under a particular scheduling and damping scheme,\nMP is equivalent to graph cuts, and thus optimal. We explain the apparent\ncontradiction by showing that with proper scheduling and damping, MP always\nconverges to an optimal fixed point. Thus, the canonical counterexample only\nshows the suboptimality of MP with a particular suboptimal choice of schedule\nand damping. With proper choices, MP is optimal.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2011 21:24:37 GMT"}], "update_date": "2012-02-19", "authors_parsed": [["Tarlow", "Daniel", ""], ["Givoni", "Inmar E.", ""], ["Zemel", "Richard S.", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1105.1575", "submitter": "Yuan-chin Chang yc.ivan.chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Evaluating the diagnostic powers of variables and their linear\n  combinations when the gold standard is continuous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic (ROC) curve is a very useful tool for\nanalyzing the diagnostic/classification power of instruments/classification\nschemes as long as a binary-scale gold standard is available. When the gold\nstandard is continuous and there is no confirmative threshold, ROC curve\nbecomes less useful. Hence, there are several extensions proposed for\nevaluating the diagnostic potential of variables of interest. However, due to\nthe computational difficulties of these nonparametric based extensions, they\nare not easy to be used for finding the optimal combination of variables to\nimprove the individual diagnostic power. Therefore, we propose a new measure,\nwhich extends the AUC index for identifying variables with good potential to be\nused in a diagnostic scheme. In addition, we propose a threshold gradient\ndescent based algorithm for finding the best linear combination of variables\nthat maximizes this new measure, which is applicable even when the number of\nvariables is huge. The estimate of the proposed index and its asymptotic\nproperty are studied. The performance of the proposed method is illustrated\nusing both synthesized and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 03:54:34 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1105.1758", "submitter": "Arnau Tibau Puig", "authors": "Arnau Tibau Puig and Alfred O. Hero III", "title": "Order-preserving factor analysis (OPFA)", "comments": "Technical Report - Communications and Signal Processing Laboratory,\n  University of Michigan, Ann Arbor, MI", "journal-ref": null, "doi": null, "report-no": "cspl-396", "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel factor analysis method that can be applied to the\ndiscovery of common factors shared among trajectories in multivariate time\nseries data. These factors satisfy a precedence-ordering property: certain\nfactors are recruited only after some other factors are activated.\nPrecedence-ordering arise in applications where variables are activated in a\nspecific order, which is unknown. The proposed method is based on a linear\nmodel that accounts for each factor's inherent delays and relative order. We\npresent an algorithm to fit the model in an unsupervised manner using\ntechniques from convex and non-convex optimization that enforce sparsity of the\nfactor scores and consistent precedence-order of the factor loadings. We\nillustrate the Order-Preserving Factor Analysis (OPFA) method for the problem\nof extracting precedence-ordered factors from a longitudinal (time course)\nstudy of gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 19:04:11 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Puig", "Arnau Tibau", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1105.1853", "submitter": "Ying Liu", "authors": "Ying Liu, Venkat Chandrasekaran, Animashree Anandkumar, Alan S.\n  Willsky", "title": "Feedback Message Passing for Inference in Gaussian Graphical Models", "comments": "30 pages", "journal-ref": null, "doi": "10.1109/TSP.2012.2195656", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While loopy belief propagation (LBP) performs reasonably well for inference\nin some Gaussian graphical models with cycles, its performance is\nunsatisfactory for many others. In particular for some models LBP does not\nconverge, and in general when it does converge, the computed variances are\nincorrect (except for cycle-free graphs for which belief propagation (BP) is\nnon-iterative and exact). In this paper we propose {\\em feedback message\npassing} (FMP), a message-passing algorithm that makes use of a special set of\nvertices (called a {\\em feedback vertex set} or {\\em FVS}) whose removal\nresults in a cycle-free graph. In FMP, standard BP is employed several times on\nthe cycle-free subgraph excluding the FVS while a special message-passing\nscheme is used for the nodes in the FVS. The computational complexity of exact\ninference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is\nthe total number of nodes. When the size of the FVS is very large, FMP is\nintractable. Hence we propose {\\em approximate FMP}, where a pseudo-FVS is used\ninstead of an FVS, and where inference in the non-cycle-free graph obtained by\nremoving the pseudo-FVS is carried out approximately using LBP. We show that,\nwhen approximate FMP converges, it yields exact means and variances on the\npseudo-FVS and exact means throughout the remainder of the graph. We also\nprovide theoretical results on the convergence and accuracy of approximate FMP.\nIn particular, we prove error bounds on variance computation. Based on these\ntheoretical results, we design efficient algorithms to select a pseudo-FVS of\nbounded size. The choice of the pseudo-FVS allows us to explicitly trade off\nbetween efficiency and accuracy. Experimental results show that using a\npseudo-FVS of size no larger than $\\log(n)$, this procedure converges much more\noften, more quickly, and provides more accurate results than LBP on the entire\ngraph.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 04:22:00 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Liu", "Ying", ""], ["Chandrasekaran", "Venkat", ""], ["Anandkumar", "Animashree", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1105.1951", "submitter": "Wolfgang Konen K", "authors": "Wolfgang Konen", "title": "Self-configuration from a Machine-Learning Perspective", "comments": "12 pages, 5 figures, Dagstuhl seminar 11181 \"Organic Computing -\n  Design of Self-Organizing Systems\", May 2011", "journal-ref": null, "doi": null, "report-no": "DPA-11181", "categories": "nlin.AO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of machine learning is to provide solutions which are trained by\ndata or by experience coming from the environment. Many training algorithms\nexist and some brilliant successes were achieved. But even in structured\nenvironments for machine learning (e.g. data mining or board games), most\napplications beyond the level of toy problems need careful hand-tuning or human\ningenuity (i.e. detection of interesting patterns) or both. We discuss several\naspects how self-configuration can help to alleviate these problems. One aspect\nis the self-configuration by tuning of algorithms, where recent advances have\nbeen made in the area of SPO (Sequen- tial Parameter Optimization). Another\naspect is the self-configuration by pattern detection or feature construction.\nForming multiple features (e.g. random boolean functions) and using algorithms\n(e.g. random forests) which easily digest many fea- tures can largely increase\nlearning speed. However, a full-fledged theory of feature construction is not\nyet available and forms a current barrier in machine learning. We discuss\nseveral ideas for systematic inclusion of feature construction. This may lead\nto partly self-configuring machine learning solutions which show robustness,\nflexibility, and fast learning in potentially changing environments.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 14:01:41 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2011 11:54:46 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Konen", "Wolfgang", ""]]}, {"id": "1105.2054", "submitter": "Alexander Grubb", "authors": "Alexander Grubb and J. Andrew Bagnell", "title": "Generalized Boosting Algorithms for Convex Optimization", "comments": "Extended version of paper presented at the International Conference\n  on Machine Learning, 2011. 9 pages + appendix with proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is a popular way to derive powerful learners from simpler hypothesis\nclasses. Following previous work (Mason et al., 1999; Friedman, 2000) on\ngeneral boosting frameworks, we analyze gradient-based descent algorithms for\nboosting with respect to any convex objective and introduce a new measure of\nweak learner performance into this setting which generalizes existing work. We\npresent the weak to strong learning guarantees for the existing gradient\nboosting work for strongly-smooth, strongly-convex objectives under this new\nmeasure of performance, and also demonstrate that this work fails for\nnon-smooth objectives. To address this issue, we present new algorithms which\nextend this boosting approach to arbitrary convex loss functions and give\ncorresponding weak to strong convergence results. In addition, we demonstrate\nexperimental results that support our analysis and demonstrate the need for the\nnew algorithms we present.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 21:02:58 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2012 06:33:18 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Grubb", "Alexander", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1105.2416", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin and Fran\\c{c}ois Laviolette and John Shawe-Taylor and\n  Jan Peters and Peter Auer", "title": "PAC-Bayesian Analysis of Martingales and Multiarmed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two alternative ways to apply PAC-Bayesian analysis to sequences\nof dependent random variables. The first is based on a new lemma that enables\nto bound expectations of convex functions of certain dependent random variables\nby expectations of the same functions of independent Bernoulli random\nvariables. This lemma provides an alternative tool to Hoeffding-Azuma\ninequality to bound concentration of martingale values. Our second approach is\nbased on integration of Hoeffding-Azuma inequality with PAC-Bayesian analysis.\nWe also introduce a way to apply PAC-Bayesian analysis in situation of limited\nfeedback. We combine the new tools to derive PAC-Bayesian generalization and\nregret bounds for the multiarmed bandit problem. Although our regret bound is\nnot yet as tight as state-of-the-art regret bounds based on other\nwell-established techniques, our results significantly expand the range of\npotential applications of PAC-Bayesian analysis and introduce a new analysis\ntool to reinforcement learning and many other fields, where martingales and\nlimited feedback are encountered.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2011 10:40:19 GMT"}, {"version": "v2", "created": "Thu, 19 May 2011 17:04:35 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Shawe-Taylor", "John", ""], ["Peters", "Jan", ""], ["Auer", "Peter", ""]]}, {"id": "1105.2493", "submitter": "Abdul-Saboor Sheikh", "authors": "J\\\"org L\\\"ucke and Abdul-Saboor Sheikh", "title": "Closed-form EM for Sparse Coding and its Application to Source\n  Separation", "comments": "joint first authorship", "journal-ref": "L\\\"ucke, J. and Sheikh, A.-S. Proc. LVA/ICA, LNCS pp. 213-221,\n  2012", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and discuss the first sparse coding algorithm based on closed-form\nEM updates and continuous latent variables. The underlying generative model\nconsists of a standard `spike-and-slab' prior and a Gaussian noise model.\nClosed-form solutions for E- and M-step equations are derived by generalizing\nprobabilistic PCA. The resulting EM algorithm can take all modes of a\npotentially multi-modal posterior into account. The computational cost of the\nalgorithm scales exponentially with the number of hidden dimensions. However,\nwith current computational resources, it is still possible to efficiently learn\nmodel parameters for medium-scale problems. Thus the model can be applied to\nthe typical range of source separation tasks. In numerical experiments on\nartificial data we verify likelihood maximization and show that the derived\nalgorithm recovers the sparse directions of standard sparse coding\ndistributions. On source separation benchmarks comprised of realistic data we\nshow that the algorithm is competitive with other recent methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2011 14:50:09 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 09:26:35 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2011 17:32:36 GMT"}, {"version": "v4", "created": "Mon, 26 Dec 2011 14:16:12 GMT"}, {"version": "v5", "created": "Wed, 11 Jan 2012 15:40:45 GMT"}, {"version": "v6", "created": "Fri, 2 Mar 2012 10:46:38 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["L\u00fccke", "J\u00f6rg", ""], ["Sheikh", "Abdul-Saboor", ""]]}, {"id": "1105.2952", "submitter": "Bela Frigyik", "authors": "Bela A. Frigyik and Maya R. Gupta", "title": "Bounds on the Bayes Error Given Moments", "comments": "10 pages, 2 figures, to appear in IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute lower bounds for the supremum Bayes error if the\nclass-conditional distributions must satisfy moment constraints, where the\nsupremum is with respect to the unknown class-conditional distributions. Our\napproach makes use of Curto and Fialkow's solutions for the truncated moment\nproblem. The lower bound shows that the popular Gaussian assumption is not\nrobust in this regard. We also construct an upper bound for the supremum Bayes\nerror by constraining the decision boundary to be linear.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 16:05:11 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2011 22:06:42 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2012 06:31:45 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Frigyik", "Bela A.", ""], ["Gupta", "Maya R.", ""]]}, {"id": "1105.2965", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Sergey Kirshner", "title": "Generating Similar Graphs From Spherical Features", "comments": "29 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for generating graphs similar to a given example\ngraph. Unlike standard approaches that compute features of graphs in Euclidean\nspace, our approach obtains features on a surface of a hypersphere. We then\nutilize a von Mises-Fisher distribution, an exponential family distribution on\nthe surface of a hypersphere, to define a model over possible feature values.\nWhile our approach bears similarity to a popular exponential random graph model\n(ERGM), unlike ERGMs, it does not suffer from degeneracy, a situation when a\nsignificant probability mass is placed on unrealistic graphs. We propose a\nparameter estimation approach for our model, and a procedure for drawing\nsamples from the distribution. We evaluate the performance of our approach both\non the small domain of all 8-node graphs as well as larger real-world social\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 20:23:45 GMT"}, {"version": "v2", "created": "Thu, 19 May 2011 03:26:10 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Lunga", "Dalton", ""], ["Kirshner", "Sergey", ""]]}, {"id": "1105.2978", "submitter": "Shujie Hou", "authors": "Shujie Hou and Robert C. Qiu", "title": "Spectrum Sensing for Cognitive Radio Using Kernel-Based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel method is a very powerful tool in machine learning. The trick of\nkernel has been effectively and extensively applied in many areas of machine\nlearning, such as support vector machine (SVM) and kernel principal component\nanalysis (kernel PCA). Kernel trick is to define a kernel function which relies\non the inner-product of data in the feature space without knowing these feature\nspace data. In this paper, the kernel trick will be employed to extend the\nalgorithm of spectrum sensing with leading eigenvector under the framework of\nPCA to a higher dimensional feature space. Namely, the leading eigenvector of\nthe sample covariance matrix in the feature space is used for spectrum sensing\nwithout knowing the leading eigenvector explicitly. Spectrum sensing with\nleading eigenvector under the framework of kernel PCA is proposed with the\ninner-product as a measure of similarity. A modified kernel GLRT algorithm\nbased on matched subspace model will be the first time applied to spectrum\nsensing. The experimental results on simulated sinusoidal signal show that\nspectrum sensing with kernel PCA is about 4 dB better than PCA, besides, kernel\nGLRT is also better than GLRT. The proposed algorithms are also tested on the\nmeasured DTV signal. The simulation results show that kernel methods are 4 dB\nbetter than the corresponding linear methods. The leading eigenvector of the\nsample covariance matrix learned by kernel PCA is more stable than that learned\nby PCA for different segments of DTV signal.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 23:48:37 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Hou", "Shujie", ""], ["Qiu", "Robert C.", ""]]}, {"id": "1105.3361", "submitter": "Anders Gorst-Rasmussen", "authors": "Anders Gorst-Rasmussen and Thomas H. Scheike", "title": "Independent screening for single-index hazard rate models with\n  ultra-high dimensional features", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data sets with many more features than observations, independent screening\nbased on all univariate regression models leads to a computationally convenient\nvariable selection method. Recent efforts have shown that in the case of\ngeneralized linear models, independent screening may suffice to capture all\nrelevant features with high probability, even in ultra-high dimension. It is\nunclear whether this formal sure screening property is attainable when the\nresponse is a right-censored survival time. We propose a computationally very\nefficient independent screening method for survival data which can be viewed as\nthe natural survival equivalent of correlation screening. We state conditions\nunder which the method admits the sure screening property within a general\nclass of single-index hazard rate models with ultra-high dimensional features.\nAn iterative variant is also described which combines screening with penalized\nregression in order to handle more complex feature covariance structures. The\nmethods are evaluated through simulation studies and through application to a\nreal gene expression dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2011 13:05:11 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 12:03:44 GMT"}], "update_date": "2011-08-12", "authors_parsed": [["Gorst-Rasmussen", "Anders", ""], ["Scheike", "Thomas H.", ""]]}, {"id": "1105.3422", "submitter": "Daniel Dunlavy", "authors": "Evrim Acar, Tamara G. Kolda and Daniel M. Dunlavy", "title": "All-at-once Optimization for Coupled Matrix and Tensor Factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint analysis of data from multiple sources has the potential to improve our\nunderstanding of the underlying structures in complex data sets. For instance,\nin restaurant recommendation systems, recommendations can be based on rating\nhistories of customers. In addition to rating histories, customers' social\nnetworks (e.g., Facebook friendships) and restaurant categories information\n(e.g., Thai or Italian) can also be used to make better recommendations. The\ntask of fusing data, however, is challenging since data sets can be incomplete\nand heterogeneous, i.e., data consist of both matrices, e.g., the person by\nperson social network matrix or the restaurant by category matrix, and\nhigher-order tensors, e.g., the \"ratings\" tensor of the form restaurant by meal\nby person.\n  In this paper, we are particularly interested in fusing data sets with the\ngoal of capturing their underlying latent structures. We formulate this problem\nas a coupled matrix and tensor factorization (CMTF) problem where heterogeneous\ndata sets are modeled by fitting outer-product models to higher-order tensors\nand matrices in a coupled manner. Unlike traditional approaches solving this\nproblem using alternating algorithms, we propose an all-at-once optimization\napproach called CMTF-OPT (CMTF-OPTimization), which is a gradient-based\noptimization approach for joint analysis of matrices and higher-order tensors.\nWe also extend the algorithm to handle coupled incomplete data sets. Using\nnumerical experiments, we demonstrate that the proposed all-at-once approach is\nmore accurate than the alternating least squares approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2011 16:12:19 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Acar", "Evrim", ""], ["Kolda", "Tamara G.", ""], ["Dunlavy", "Daniel M.", ""]]}, {"id": "1105.3931", "submitter": "Xueyuan Zhou", "authors": "Xueyuan Zhou and Mikhail Belkin", "title": "Behavior of Graph Laplacians on Manifolds with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In manifold learning, algorithms based on graph Laplacians constructed from\ndata have received considerable attention both in practical applications and\ntheoretical analysis. In particular, the convergence of graph Laplacians\nobtained from sampled data to certain continuous operators has become an active\nresearch topic recently. Most of the existing work has been done under the\nassumption that the data is sampled from a manifold without boundary or that\nthe functions of interests are evaluated at a point away from the boundary.\nHowever, the question of boundary behavior is of considerable practical and\ntheoretical interest. In this paper we provide an analysis of the behavior of\ngraph Laplacians at a point near or on the boundary, discuss their convergence\nrates and their implications and provide some numerical results. It turns out\nthat while points near the boundary occupy only a small part of the total\nvolume of a manifold, the behavior of graph Laplacian there has different\nscaling properties from its behavior elsewhere on the manifold, with global\neffects on the whole manifold, an observation with potentially important\nimplications for the general problem of learning on manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2011 16:54:39 GMT"}], "update_date": "2011-05-23", "authors_parsed": [["Zhou", "Xueyuan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1105.4042", "submitter": "Sebastien Gerchinovitz", "authors": "S\\'ebastien Gerchinovitz (DMA, CLASSIC), Jia Yuan Yu", "title": "Adaptive and optimal online linear regression on $\\ell^1$-balls", "comments": null, "journal-ref": "Theoretical Computer Science, Elsevier, 2014, 519, pp.4-28", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online linear regression on individual sequences.\nThe goal in this paper is for the forecaster to output sequential predictions\nwhich are, after $T$ time rounds, almost as good as the ones output by the best\nlinear predictor in a given $\\ell^1$-ball in $\\\\R^d$. We consider both the\ncases where the dimension~$d$ is small and large relative to the time horizon\n$T$. We first present regret bounds with optimal dependencies on $d$, $T$, and\non the sizes $U$, $X$ and $Y$ of the $\\ell^1$-ball, the input data and the\nobservations. The minimax regret is shown to exhibit a regime transition around\nthe point $d = \\sqrt{T} U X / (2 Y)$. Furthermore, we present efficient\nalgorithms that are adaptive, \\ie, that do not require the knowledge of $U$,\n$X$, $Y$, and $T$, but still achieve nearly optimal regret bounds.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2011 09:14:03 GMT"}, {"version": "v2", "created": "Wed, 25 May 2011 07:54:09 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2012 19:32:08 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 13:01:19 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Gerchinovitz", "S\u00e9bastien", "", "DMA, CLASSIC"], ["Yu", "Jia Yuan", ""]]}, {"id": "1105.4385", "submitter": "Ping Li", "authors": "Ping Li and Joshua Moore and Christian Konig", "title": "b-Bit Minwise Hashing for Large-Scale Linear SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to (seamlessly) integrate b-bit minwise hashing\nwith linear SVM to substantially improve the training (and testing) efficiency\nusing much smaller memory, with essentially no loss of accuracy. Theoretically,\nwe prove that the resemblance matrix, the minwise hashing matrix, and the b-bit\nminwise hashing matrix are all positive definite matrices (kernels).\nInterestingly, our proof for the positive definiteness of the b-bit minwise\nhashing kernel naturally suggests a simple strategy to integrate b-bit hashing\nwith linear SVM. Our technique is particularly useful when the data can not fit\nin memory, which is an increasingly critical issue in large-scale machine\nlearning. Our preliminary experimental results on a publicly available webspam\ndataset (350K samples and 16 million dimensions) verified the effectiveness of\nour algorithm. For example, the training time was reduced to merely a few\nseconds. In addition, our technique can be easily extended to many other linear\nand nonlinear machine learning applications such as logistic regression.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 01:56:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Li", "Ping", ""], ["Moore", "Joshua", ""], ["Konig", "Christian", ""]]}, {"id": "1105.4585", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin, Nicol\\`o Cesa-Bianchi, Fran\\c{c}ois Laviolette, Peter\n  Auer, John Shawe-Taylor, Jan Peters", "title": "PAC-Bayesian Analysis of the Exploration-Exploitation Trade-off", "comments": "On-line Trading of Exploration and Exploitation 2 - ICML-2011\n  workshop. http://explo.cs.ucl.ac.uk/workshop/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a coherent framework for integrative simultaneous analysis of the\nexploration-exploitation and model order selection trade-offs. We improve over\nour preceding results on the same subject (Seldin et al., 2011) by combining\nPAC-Bayesian analysis with Bernstein-type inequality for martingales. Such a\ncombination is also of independent interest for studies of multiple\nsimultaneously evolving martingales.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 19:10:03 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Auer", "Peter", ""], ["Shawe-Taylor", "John", ""], ["Peters", "Jan", ""]]}, {"id": "1105.4681", "submitter": "John Duchi", "authors": "John C. Duchi, Alekh Agarwal, Mikael Johansson, Michael I. Jordan", "title": "Ergodic Mirror Descent", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize stochastic subgradient descent methods to situations in which\nwe do not receive independent samples from the distribution over which we\noptimize, but instead receive samples that are coupled over time. We show that\nas long as the source of randomness is suitably ergodic---it converges quickly\nenough to a stationary distribution---the method enjoys strong convergence\nguarantees, both in expectation and with high probability. This result has\nimplications for stochastic optimization in high-dimensional spaces,\npeer-to-peer distributed optimization schemes, decision problems with dependent\ndata, and stochastic optimization problems over combinatorial spaces.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2011 05:12:06 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2012 21:38:01 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2012 06:50:42 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Duchi", "John C.", ""], ["Agarwal", "Alekh", ""], ["Johansson", "Mikael", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1105.4871", "submitter": "Sebastien Bubeck", "authors": "Jean-Yves Audibert, Sebastien Bubeck, Gabor Lugosi", "title": "Minimax Policies for Combinatorial Prediction Games", "comments": null, "journal-ref": "24th annual conference on learning theory, 2011", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the online linear optimization problem when the actions of the\nforecaster are represented by binary vectors. Our goal is to understand the\nmagnitude of the minimax regret for the worst possible set of actions. We study\nthe problem under three different assumptions for the feedback: full\ninformation, and the partial information models of the so-called \"semi-bandit\",\nand \"bandit\" problems. We consider both $L_\\infty$-, and $L_2$-type of\nrestrictions for the losses assigned by the adversary.\n  We formulate a general strategy using Bregman projections on top of a\npotential-based gradient descent, which generalizes the ones studied in the\nseries of papers Gyorgy et al. (2007), Dani et al. (2008), Abernethy et al.\n(2008), Cesa-Bianchi and Lugosi (2009), Helmbold and Warmuth (2009), Koolen et\nal. (2010), Uchiya et al. (2010), Kale et al. (2010) and Audibert and Bubeck\n(2010). We provide simple proofs that recover most of the previous results. We\npropose new upper bounds for the semi-bandit game. Moreover we derive lower\nbounds for all three feedback assumptions. With the only exception of the\nbandit game, the upper and lower bounds are tight, up to a constant factor.\nFinally, we answer a question asked by Koolen et al. (2010) by showing that the\nexponentially weighted average forecaster is suboptimal against $L_{\\infty}$\nadversaries.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2011 19:53:21 GMT"}], "update_date": "2011-05-25", "authors_parsed": [["Audibert", "Jean-Yves", ""], ["Bubeck", "Sebastien", ""], ["Lugosi", "Gabor", ""]]}, {"id": "1105.4924", "submitter": "Guangliang Chen", "authors": "William K. Allard, Guangliang Chen, and Mauro Maggioni", "title": "Multiscale Geometric Methods for Data Sets II: Geometric\n  Multi-Resolution Analysis", "comments": "Re-formatted using AMS style", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sets are often modeled as point clouds in $R^D$, for $D$ large. It is\noften assumed that the data has some interesting low-dimensional structure, for\nexample that of a $d$-dimensional manifold $M$, with $d$ much smaller than $D$.\nWhen $M$ is simply a linear subspace, one may exploit this assumption for\nencoding efficiently the data by projecting onto a dictionary of $d$ vectors in\n$R^D$ (for example found by SVD), at a cost $(n+D)d$ for $n$ data points. When\n$M$ is nonlinear, there are no \"explicit\" constructions of dictionaries that\nachieve a similar efficiency: typically one uses either random dictionaries, or\ndictionaries obtained by black-box optimization. In this paper we construct\ndata-dependent multi-scale dictionaries that aim at efficient encoding and\nmanipulating of the data. Their construction is fast, and so are the algorithms\nthat map data points to dictionary coefficients and vice versa. In addition,\ndata points are guaranteed to have a sparse representation in terms of the\ndictionary. We think of dictionaries as the analogue of wavelets, but for\napproximating point clouds rather than functions.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2011 03:13:39 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 18:41:30 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2011 03:33:33 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Allard", "William K.", ""], ["Chen", "Guangliang", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1105.5332", "submitter": "Andrej Cvetkovski", "authors": "Andrej Cvetkovski and Mark Crovella", "title": "Multidimensional Scaling in the Poincare Disk", "comments": null, "journal-ref": "Applied Mathematics & Information Sciences, 10(1):125, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling (MDS) is a class of projective algorithms\ntraditionally used in Euclidean space to produce two- or three-dimensional\nvisualizations of datasets of multidimensional points or point distances. More\nrecently however, several authors have pointed out that for certain datasets,\nhyperbolic target space may provide a better fit than Euclidean space.\n  In this paper we develop PD-MDS, a metric MDS algorithm designed specifically\nfor the Poincare disk (PD) model of the hyperbolic plane. Emphasizing the\nimportance of proceeding from first principles in spite of the availability of\nvarious black box optimizers, our construction is based on an elementary\nhyperbolic line search and reveals numerous particulars that need to be\ncarefully addressed when implementing this as well as more sophisticated\niterative optimization methods in a hyperbolic space model.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2011 16:05:23 GMT"}, {"version": "v2", "created": "Sun, 29 May 2011 06:06:30 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 09:39:02 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Cvetkovski", "Andrej", ""], ["Crovella", "Mark", ""]]}, {"id": "1105.5669", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "PAC learnability under non-atomic measures: a problem by Vidyasagar", "comments": "21 pages, 3 figures, latex 2e. A final revision of a submission to\n  Theor. Comp. Sci. Two figures have been added", "journal-ref": "Theoretical Computer Science 473 (2013), 29-45", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAC\nlearnability of a concept class $\\mathscr C$ under the family of all non-atomic\n(diffuse) measures on the domain $\\Omega$. The uniform Glivenko--Cantelli\nproperty with respect to non-atomic measures is no longer a necessary\ncondition, and consistent learnability cannot in general be expected. Our\ncriterion is stated in terms of a combinatorial parameter $\\VC({\\mathscr\nC}\\,{\\mathrm{mod}}\\,\\omega_1)$ which we call the VC dimension of $\\mathscr C$\nmodulo countable sets. The new parameter is obtained by \"thickening up\" single\npoints in the definition of VC dimension to uncountable \"clusters\".\nEquivalently, $\\VC(\\mathscr C\\modd\\omega_1)\\leq d$ if and only if every\ncountable subclass of $\\mathscr C$ has VC dimension $\\leq d$ outside a\ncountable subset of $\\Omega$. The new parameter can be also expressed as the\nclassical VC dimension of $\\mathscr C$ calculated on a suitable subset of a\ncompactification of $\\Omega$. We do not make any measurability assumptions on\n$\\mathscr C$, assuming instead the validity of Martin's Axiom (MA). Similar\nresults are obtained for function learning in terms of fat-shattering dimension\nmodulo countable sets, but, just like in the classical distribution-free case,\nthe finiteness of this parameter is sufficient but not necessary for PAC\nlearnability under non-atomic measures.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 23:31:15 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2011 20:52:37 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2012 23:52:20 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1105.6351", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini and Martin J. Wainwright", "title": "Approximation properties of certain operator-induced norms on Hilbert\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of operator-induced norms, acting as finite-dimensional\nsurrogates to the L2 norm, and study their approximation properties over\nHilbert subspaces of L2 . The class includes, as a special case, the usual\nempirical norm encountered, for example, in the context of nonparametric\nregression in reproducing kernel Hilbert spaces (RKHS). Our results have\nimplications to the analysis of M-estimators in models based on\nfinite-dimensional linear approximation of functions, and also to some related\npacking problems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 17:51:52 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Amini", "Arash A.", ""], ["Wainwright", "Martin J.", ""]]}]