[{"id": "1201.0341", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Barnabas Poczos, Andras Lorincz", "title": "Collaborative Filtering via Group-Structured Dictionary Learning", "comments": "A compressed version of the paper has been accepted for publication\n  at the 10th International Conference on Latent Variable Analysis and Source\n  Separation (LVA/ICA 2012)", "journal-ref": "International Conference on Latent Variable Analysis and Source\n  Separation (LVA/ICA), vol. 7191 of LNCS, pp. 247-254, 2012", "doi": "10.1007/978-3-642-28551-6_31", "report-no": null, "categories": "math.OC cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sparse coding and the related structured dictionary learning\nproblems are novel research areas in machine learning. In this paper we present\na new application of structured dictionary learning for collaborative filtering\nbased recommender systems. Our extensive numerical experiments demonstrate that\nthe presented technique outperforms its state-of-the-art competitors and has\nseveral advantages over approaches that do not put structured constraints on\nthe dictionary elements.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 09:05:33 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Szabo", "Zoltan", ""], ["Poczos", "Barnabas", ""], ["Lorincz", "Andras", ""]]}, {"id": "1201.0610", "submitter": "Jason J Corso", "authors": "Caiming Xiong, David Johnson, Ran Xu and Jason J. Corso", "title": "Random Forests for Metric Learning with Implicit Pairwise Position\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning makes it plausible to learn distances for complex\ndistributions of data from labeled data. However, to date, most metric learning\nmethods are based on a single Mahalanobis metric, which cannot handle\nheterogeneous data well. Those that learn multiple metrics throughout the space\nhave demonstrated superior accuracy, but at the cost of computational\nefficiency. Here, we take a new angle to the metric learning problem and learn\na single metric that is able to implicitly adapt its distance function\nthroughout the feature space. This metric adaptation is accomplished by using a\nrandom forest-based classifier to underpin the distance function and\nincorporate both absolute pairwise position and standard relative position into\nthe representation. We have implemented and tested our method against state of\nthe art global and multi-metric methods on a variety of data sets. Overall, the\nproposed method outperforms both types of methods in terms of accuracy\n(consistently ranked first) and is an order of magnitude faster than state of\nthe art multi-metric methods (16x faster in the worst case).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 11:29:17 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Xiong", "Caiming", ""], ["Johnson", "David", ""], ["Xu", "Ran", ""], ["Corso", "Jason J.", ""]]}, {"id": "1201.0794", "submitter": "John Lafferty", "authors": "John Lafferty, Han Liu, Larry Wasserman", "title": "Sparse Nonparametric Graphical Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS391 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 519-537", "doi": "10.1214/12-STS391", "report-no": "IMS-STS-STS391", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some nonparametric methods for graphical modeling. In the discrete\ncase, where the data are binary or drawn from a finite alphabet, Markov random\nfields are already essentially nonparametric, since the cliques can take only a\nfinite number of values. Continuous data are different. The Gaussian graphical\nmodel is the standard parametric model for continuous data, but it makes\ndistributional assumptions that are often unrealistic. We discuss two\napproaches to building more flexible graphical models. One allows arbitrary\ngraphs and a nonparametric extension of the Gaussian; the other uses kernel\ndensity estimation and restricts the graphs to trees and forests. Examples of\nboth methods are presented. We also discuss possible future research directions\nfor nonparametric graphical modeling.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 00:43:53 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 13:43:13 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Lafferty", "John", ""], ["Liu", "Han", ""], ["Wasserman", "Larry", ""]]}, {"id": "1201.0862", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao", "title": "Extension of SBL Algorithms for the Recovery of Block Sparse Signals\n  with Intra-Block Correlation", "comments": "Matlab codes can be downloaded at:\n  https://sites.google.com/site/researchbyzhang/bsbl, or\n  http://dsp.ucsd.edu/~zhilin/BSBL.html", "journal-ref": null, "doi": "10.1109/TSP.2013.2241055", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the recovery of block sparse signals and extend the framework in\ntwo important directions; one by exploiting signals' intra-block correlation\nand the other by generalizing signals' block structure. We propose two families\nof algorithms based on the framework of block sparse Bayesian learning (BSBL).\nOne family, directly derived from the BSBL framework, requires knowledge of the\nblock structure. Another family, derived from an expanded BSBL framework, is\nbased on a weaker assumption on the block structure, and can be used when the\nblock structure is completely unknown. Using these algorithms we show that\nexploiting intra-block correlation is very helpful in improving recovery\nperformance. These algorithms also shed light on how to modify existing\nalgorithms or design new ones to exploit such correlation and improve\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 10:01:32 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2012 11:41:31 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 10:51:34 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2012 08:27:30 GMT"}, {"version": "v5", "created": "Sun, 2 Nov 2014 05:55:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1201.0959", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (LTCI), Yves Lechevallier (INRIA Rocquencourt / INRIA\n  Sophia Antipolis)", "title": "Constrained variable clustering and the best basis problem in functional\n  data analysis", "comments": null, "journal-ref": "Classification and Multivariate Analysis for Complex Data\n  Structures 435-444 (2011)", "doi": "10.1007/978-3-642-13312-1_46", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis involves data described by regular functions rather\nthan by a finite number of real valued variables. While some robust data\nanalysis methods can be applied directly to the very high dimensional vectors\nobtained from a fine grid sampling of functional data, all methods benefit from\na prior simplification of the functions that reduces the redundancy induced by\nthe regularity. In this paper we propose to use a clustering approach that\ntargets variables rather than individual to design a piecewise constant\nrepresentation of a set of functions. The contiguity constraint induced by the\nfunctional nature of the variables allows a polynomial complexity algorithm to\ngive the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 18:39:37 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Rossi", "Fabrice", "", "LTCI"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"]]}, {"id": "1201.0963", "submitter": "Fabrice Rossi", "authors": "Alzennyr Da Silva (INRIA Rocquencourt / INRIA Sophia Antipolis), Yves\n  Lechevallier (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis), Francisco De A. T. De Carvahlo\n  (CIn)", "title": "Clustering Dynamic Web Usage Data", "comments": null, "journal-ref": "Innovative Applications in Data Mining (2009) 71-82", "doi": "10.1007/978-3-540-88045-5", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification methods are based on the assumption that data conforms to\na stationary distribution. The machine learning domain currently suffers from a\nlack of classification techniques that are able to detect the occurrence of a\nchange in the underlying data distribution. Ignoring possible changes in the\nunderlying concept, also known as concept drift, may degrade the performance of\nthe classification model. Often these changes make the model inconsistent and\nregular updatings become necessary. Taking the temporal dimension into account\nduring the analysis of Web usage data is a necessity, since the way a site is\nvisited may indeed evolve due to modifications in the structure and content of\nthe site, or even due to changes in the behavior of certain user groups. One\nsolution to this problem, proposed in this article, is to update models using\nsummaries obtained by means of an evolutionary approach based on an intelligent\nclustering approach. We carry out various clustering strategies that are\napplied on time sub-periods. To validate our approach we apply two external\nevaluation criteria which compare different partitions from the same data set.\nOur experiments show that the proposed approach is efficient to detect the\noccurrence of changes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 18:45:23 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Da Silva", "Alzennyr", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["De Carvahlo", "Francisco De A. T.", "", "CIn"]]}, {"id": "1201.1450", "submitter": "Casey Bennett", "authors": "Casey Bennett", "title": "The Interaction of Entropy-Based Discretization and Sample Size: An\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical investigation of the interaction of sample size and\ndiscretization - in this case the entropy-based method CAIM (Class-Attribute\nInterdependence Maximization) - was undertaken to evaluate the impact and\npotential bias introduced into data mining performance metrics due to variation\nin sample size as it impacts the discretization process. Of particular interest\nwas the effect of discretizing within cross-validation folds averse to outside\ndiscretization folds. Previous publications have suggested that discretizing\nexternally can bias performance results; however, a thorough review of the\nliterature found no empirical evidence to support such an assertion. This\ninvestigation involved construction of over 117,000 models on seven distinct\ndatasets from the UCI (University of California-Irvine) Machine Learning\nLibrary and multiple modeling methods across a variety of configurations of\nsample size and discretization, with each unique \"setup\" being independently\nreplicated ten times. The analysis revealed a significant optimistic bias as\nsample sizes decreased and discretization was employed. The study also revealed\nthat there may be a relationship between the interaction that produces such\nbias and the numbers and types of predictor attributes, extending the \"curse of\ndimensionality\" concept from feature selection into the discretization realm.\nDirections for further exploration are laid out, as well some general\nguidelines about the proper application of discretization in light of these\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 16:45:57 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Bennett", "Casey", ""]]}, {"id": "1201.1587", "submitter": "Houtao Deng", "authors": "Houtao Deng and George Runger", "title": "Feature Selection via Regularized Trees", "comments": "8 pages; The 2012 International Joint Conference on Neural Networks\n  (IJCNN), IEEE, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tree regularization framework, which enables many tree models to\nperform feature selection efficiently. The key idea of the regularization\nframework is to penalize selecting a new feature for splitting when its gain\n(e.g. information gain) is similar to the features used in previous splits. The\nregularization framework is applied on random forest and boosted trees here,\nand can be easily applied to other tree models. Experimental studies show that\nthe regularized trees can select high-quality feature subsets with regard to\nboth strong and weak classifiers. Because tree models can naturally deal with\ncategorical and numerical variables, missing values, different scales between\nvariables, interactions and nonlinearities etc., the tree regularization\nframework provides an effective and efficient feature selection solution for\nmany practical problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2012 21:15:32 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2012 01:12:49 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2012 06:31:53 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""]]}, {"id": "1201.1657", "submitter": "Chong Wang", "authors": "Chong Wang and David M. Blei", "title": "A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hierarchical Dirichlet process (HDP) has become an important Bayesian\nnonparametric model for grouped data, such as document collections. The HDP is\nused to construct a flexible mixed-membership model where the number of\ncomponents is determined by the data. As for most Bayesian nonparametric\nmodels, exact posterior inference is intractable---practitioners use Markov\nchain Monte Carlo (MCMC) or variational inference. Inspired by the split-merge\nMCMC algorithm for the Dirichlet process (DP) mixture model, we describe a\nnovel split-merge MCMC sampling algorithm for posterior inference in the HDP.\nWe study its properties on both synthetic data and text corpora. We find that\nsplit-merge MCMC for the HDP can provide significant improvements over\ntraditional Gibbs sampling, and we give some understanding of the data\nproperties that give rise to larger improvements.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2012 20:28:42 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Wang", "Chong", ""], ["Blei", "David M.", ""]]}, {"id": "1201.2555", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Sparse Reward Processes", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of learning problems where the agent is presented with a\nseries of tasks. Intuitively, if there is relation among those tasks, then the\ninformation gained during execution of one task has value for the execution of\nanother task. Consequently, the agent is intrinsically motivated to explore its\nenvironment beyond the degree necessary to solve the current task it has at\nhand. We develop a decision theoretic setting that generalises standard\nreinforcement learning tasks and captures this intuition. More precisely, we\nconsider a multi-stage stochastic game between a learning agent and an\nopponent. We posit that the setting is a good model for the problem of\nlife-long learning in uncertain environments, where while resources must be\nspent learning about currently important tasks, there is also the need to\nallocate effort towards learning about aspects of the world which are not\nrelevant at the moment. This is due to the fact that unpredictable future\nevents may lead to a change of priorities for the decision maker. Thus, in some\nsense, the model \"explains\" the necessity of curiosity. Apart from introducing\nthe general formalism, the paper provides algorithms. These are evaluated\nexperimentally in some exemplary domains. In addition, performance bounds are\nproven for some cases of this problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 13:08:27 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2012 12:23:38 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1201.3302", "submitter": "Tong Zhang", "authors": "Cun-Hui Zhang, Tong Zhang", "title": "A General Framework of Dual Certificate Analysis for Structured Sparse\n  Recovery Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general theoretical framework to analyze structured\nsparse recovery problems using the notation of dual certificate. Although\ncertain aspects of the dual certificate idea have already been used in some\nprevious work, due to the lack of a general and coherent theory, the analysis\nhas so far only been carried out in limited scopes for specific problems. In\nthis context the current paper makes two contributions. First, we introduce a\ngeneral definition of dual certificate, which we then use to develop a unified\ntheory of sparse recovery analysis for convex programming. Second, we present a\nclass of structured sparsity regularization called structured Lasso for which\ncalculations can be readily performed under our theoretical framework. This new\ntheory includes many seemingly loosely related previous work as special cases;\nit also implies new results that improve existing ones even for standard\nformulations such as L1 regularization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 16:10:42 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 09:32:57 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1201.3382", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow and Aaron Courville and Yoshua Bengio", "title": "Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using a factor model we call {\\em spike-and-slab\nsparse coding} (S3C) to learn features for a classification task. The S3C model\nresembles both the spike-and-slab RBM and sparse coding. Since exact inference\nin this model is intractable, we derive a structured variational inference\nprocedure and employ a variational EM training algorithm. Prior work on\napproximate inference for this model has not prioritized the ability to exploit\nparallel architectures and scale to enormous problem sizes. We present an\ninference procedure appropriate for use with GPUs which allows us to\ndramatically increase both the training set size and the amount of latent\nfactors.\n  We demonstrate that this approach improves upon the supervised learning\ncapabilities of both sparse coding and the ssRBM on the CIFAR-10 dataset. We\nevaluate our approach's potential for semi-supervised learning on subsets of\nCIFAR-10. We demonstrate state-of-the art self-taught learning performance on\nthe STL-10 dataset and use our method to win the NIPS 2011 Workshop on\nChallenges In Learning Hierarchical Models' Transfer Learning Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 22:00:07 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 22:48:52 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1201.3674", "submitter": "Allen Yang", "authors": "Dheeraj Singaraju, Ehsan Elhamifar, Roberto Tron, Allen Y. Yang, S.\n  Shankar Sastry", "title": "On the Lagrangian Biduality of Sparsity Minimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in Compressive Sensing have shown that, under certain\nconditions, the solution to an underdetermined system of linear equations with\nsparsity-based regularization can be accurately recovered by solving convex\nrelaxations of the original problem. In this work, we present a novel\nprimal-dual analysis on a class of sparsity minimization problems. We show that\nthe Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the\nsparsity minimization problems can be used to derive interesting convex\nrelaxations: the bidual of the $\\ell_0$-minimization problem is the\n$\\ell_1$-minimization problem; and the bidual of the $\\ell_{0,1}$-minimization\nproblem for enforcing group sparsity on structured data is the\n$\\ell_{1,\\infty}$-minimization problem. The analysis provides a means to\ncompute per-instance non-trivial lower bounds on the (group) sparsity of the\ndesired solutions. In a real-world application, the bidual relaxation improves\nthe performance of a sparsity-based classification framework applied to robust\nface recognition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 00:46:12 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Singaraju", "Dheeraj", ""], ["Elhamifar", "Ehsan", ""], ["Tron", "Roberto", ""], ["Yang", "Allen Y.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1201.3973", "submitter": "Babak Shahbaba", "authors": "Shiwei Lan and Babak Shahbaba", "title": "Split HMC for Gaussian Process Models", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo\n(Split HMC) method for Gaussian process model (GPM). This method is based on\nsplitting the Hamiltonian in a way that allows much of the movement around the\nstate space to be done at low computational cost. To this end, we approximate\nthe negative log density (i.e., the energy function) of the distribution of\ninterest by a quadratic function U0 for which Hamiltonian dynamics can be\nsolved analytically. The overall energy function U is then written as U0 + U1,\nwhere U1 is the approximation error. The Hamiltonian is then split into two\nparts; one part is based on U0 is handled analytically, the other part is based\non U1 for which we approximate Hamiltonian's equations by discretizing time. We\nuse simulated and real data to compare the performance of our method to the\nstandard HMC. We find that splitting the Hamiltonian for GP models could lead\nto substantial improvement (up to 10 folds) of sampling efficiency, which is\nmeasured in terms of the amount of time required for producing an independent\nsample with high acceptance probability from posterior distributions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 05:08:45 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2012 14:04:55 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Lan", "Shiwei", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1201.4002", "submitter": "Apostolos Burnetas", "authors": "Apostolos Burnetas and Odysseas Kanavetas", "title": "Adaptive Policies for Sequential Sampling under Incomplete Information\n  and a Cost Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential sampling from a finite number of\nindependent statistical populations to maximize the expected infinite horizon\naverage outcome per period, under a constraint that the expected average\nsampling cost does not exceed an upper bound. The outcome distributions are not\nknown. We construct a class of consistent adaptive policies, under which the\naverage outcome converges with probability 1 to the true value under complete\ninformation for all distributions with finite means. We also compare the rate\nof convergence for various policies in this class using simulation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 10:06:29 GMT"}], "update_date": "2012-01-20", "authors_parsed": [["Burnetas", "Apostolos", ""], ["Kanavetas", "Odysseas", ""]]}, {"id": "1201.4058", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "On the Prior and Posterior Distributions Used in Graphical Modelling", "comments": "28 pages, 6 figures", "journal-ref": "Bayesian Analysis 2013, 8(3), 505-532", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical model learning and inference are often performed using Bayesian\ntechniques. In particular, learning is usually performed in two separate steps.\nFirst, the graph structure is learned from the data; then the parameters of the\nmodel are estimated conditional on that graph structure. While the probability\ndistributions involved in this second step have been studied in depth, the ones\nused in the first step have not been explored in as much detail.\n  In this paper, we will study the prior and posterior distributions defined\nover the space of the graph structures for the purpose of learning the\nstructure of a graphical model. In particular, we will provide a\ncharacterisation of the behaviour of those distributions as a function of the\npossible edges of the graph. We will then use the properties resulting from\nthis characterisation to define measures of structural variability for both\nBayesian and Markov networks, and we will point out some of their possible\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 13:43:08 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 15:55:39 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1201.4714", "submitter": "Huyen  Do", "authors": "Huyen Do, Alexandros Kalousis, Jun Wang and Adam Woznica", "title": "A metric learning perspective of SVM: on the relation of SVM and LMNN", "comments": "To appear in AISTATS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor\nalgorithm, LMNN, are two very popular learning algorithms with quite different\nlearning biases. In this paper we bring them into a unified view and show that\nthey have a much stronger relation than what is commonly thought. We analyze\nSVMs from a metric learning perspective and cast them as a metric learning\nproblem, a view which helps us uncover the relations of the two algorithms. We\nshow that LMNN can be seen as learning a set of local SVM-like models in a\nquadratic space. Along the way and inspired by the metric-based interpretation\nof SVM s we derive a novel variant of SVMs, epsilon-SVM, to which LMNN is even\nmore similar. We give a unified view of LMNN and the different SVM variants.\nFinally we provide some preliminary experiments on a number of benchmark\ndatasets in which show that epsilon-SVM compares favorably both with respect to\nLMNN and SVM.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 13:48:33 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Do", "Huyen", ""], ["Kalousis", "Alexandros", ""], ["Wang", "Jun", ""], ["Woznica", "Adam", ""]]}, {"id": "1201.5338", "submitter": "Xiang Wang", "authors": "Xiang Wang, Buyue Qian, Ian Davidson", "title": "On Constrained Spectral Clustering and Its Applications", "comments": "Data Mining and Knowledge Discovery, 2012", "journal-ref": null, "doi": "10.1007/s10618-012-0291-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained clustering has been well-studied for algorithms such as $K$-means\nand hierarchical clustering. However, how to satisfy many constraints in these\nalgorithmic settings has been shown to be intractable. One alternative to\nencode many constraints is to use spectral clustering, which remains a\ndeveloping area. In this paper, we propose a flexible framework for constrained\nspectral clustering. In contrast to some previous efforts that implicitly\nencode Must-Link and Cannot-Link constraints by modifying the graph Laplacian\nor constraining the underlying eigenspace, we present a more natural and\nprincipled formulation, which explicitly encodes the constraints as part of a\nconstrained optimization problem. Our method offers several practical\nadvantages: it can encode the degree of belief in Must-Link and Cannot-Link\nconstraints; it guarantees to lower-bound how well the given constraints are\nsatisfied using a user-specified threshold; it can be solved deterministically\nin polynomial time through generalized eigendecomposition. Furthermore, by\ninheriting the objective function from spectral clustering and encoding the\nconstraints explicitly, much of the existing analysis of unconstrained spectral\nclustering techniques remains valid for our formulation. We validate the\neffectiveness of our approach by empirical results on both artificial and real\ndatasets. We also demonstrate an innovative use of encoding large number of\nconstraints: transfer learning via constraints.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 18:36:11 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 06:04:35 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Wang", "Xiang", ""], ["Qian", "Buyue", ""], ["Davidson", "Ian", ""]]}, {"id": "1201.5568", "submitter": "Robert B. Gramacy", "authors": "Christoforos Anagnostopoulos and Robert B. Gramacy", "title": "Dynamic trees for streaming and massive data contexts", "comments": "18 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection at a massive scale is becoming ubiquitous in a wide variety\nof settings, from vast offline databases to streaming real-time information.\nLearning algorithms deployed in such contexts must rely on single-pass\ninference, where the data history is never revisited. In streaming contexts,\nlearning must also be temporally adaptive to remain up-to-date against\nunforeseen changes in the data generating mechanism. Although rapidly growing,\nthe online Bayesian inference literature remains challenged by massive data and\ntransient, evolving data streams. Non-parametric modelling techniques can prove\nparticularly ill-suited, as the complexity of the model is allowed to increase\nwith the sample size. In this work, we take steps to overcome these challenges\nby porting standard streaming techniques, like data discarding and\ndownweighting, into a fully Bayesian framework via the use of informative\npriors and active learning heuristics. We showcase our methods by augmenting a\nmodern non-parametric modelling framework, dynamic trees, and illustrate its\nperformance on a number of practical examples. The end product is a powerful\nstreaming regression and classification tool, whose performance compares\nfavourably to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2012 16:20:05 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Anagnostopoulos", "Christoforos", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1201.6082", "submitter": "Matias Salibian-Barrera", "authors": "Yumi Kondo, Matias Salibian-Barrera, Ruben Zamar", "title": "A robust and sparse K-means clustering algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations where the interest lies in identifying clusters one might\nexpect that not all available variables carry information about these groups.\nFurthermore, data quality (e.g. outliers or missing entries) might present a\nserious and sometimes hard-to-assess problem for large and complex datasets. In\nthis paper we show that a small proportion of atypical observations might have\nserious adverse effects on the solutions found by the sparse clustering\nalgorithm of Witten and Tibshirani (2010). We propose a robustification of\ntheir sparse K-means algorithm based on the trimmed K-means algorithm of\nCuesta-Albertos et al. (1997) Our proposal is also able to handle datasets with\nmissing values. We illustrate the use of our method on microarray data for\ncancer patients where we are able to identify strong biological clusters with a\nmuch reduced number of genes. Our simulation studies show that, when there are\noutliers in the data, our robust sparse K-means algorithm performs better than\nother competing methods both in terms of the selection of features and also the\nidentified clusters. This robust sparse K-means algorithm is implemented in the\nR package RSKC which is publicly available from the CRAN repository.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2012 21:17:02 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Kondo", "Yumi", ""], ["Salibian-Barrera", "Matias", ""], ["Zamar", "Ruben", ""]]}, {"id": "1201.6530", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harish Karnick", "title": "Random Feature Maps for Dot Product Kernels", "comments": "To appear in the proceedings of the 15th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2012). This version corrects\n  a minor error with Lemma 10. Acknowledgements : Devanshu Bhimwal", "journal-ref": "Journal of Machine Learning Research, W&CP 22 (2012) 583-591", "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating non-linear kernels using feature maps has gained a lot of\ninterest in recent years due to applications in reducing training and testing\ntimes of SVM classifiers and other kernel based learning algorithms. We extend\nthis line of work and present low distortion embeddings for dot product kernels\ninto linear Euclidean spaces. We base our results on a classical result in\nharmonic analysis characterizing all dot product kernels and use it to define\nrandomized feature maps into explicit low dimensional Euclidean spaces in which\nthe native dot product provides an approximation to the dot product kernel with\nhigh confidence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 12:59:50 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 13:57:55 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2012 10:56:00 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Kar", "Purushottam", ""], ["Karnick", "Harish", ""]]}]