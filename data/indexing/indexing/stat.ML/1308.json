[{"id": "1308.0641", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen, Subhadeep Mukhopadhyay", "title": "United Statistical Algorithm, Small and Big Data: Future OF Statistician", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the role of big idea statisticians in future of Big\nData Science. We describe the `United Statistical Algorithms' framework for\ncomprehensive unification of traditional and novel statistical methods for\nmodeling Small Data and Big Data, especially mixed data (discrete, continuous).\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 23:54:44 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Parzen", "Emanuel", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1308.0642", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Emanuel Parzen", "title": "Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and\n  Application", "comments": "Major restructuring has been done", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new comprehensive approach to nonlinear time series analysis and modeling\nis developed in the present paper. We introduce novel data-specific\nmid-distribution based Legendre Polynomial (LP) like nonlinear transformations\nof the original time series Y(t) that enables us to adapt all the existing\nstationary linear Gaussian time series modeling strategy and made it applicable\nfor non-Gaussian and nonlinear processes in a robust fashion. The emphasis of\nthe present paper is on empirical time series modeling via the algorithm\nLPTime. We demonstrate the effectiveness of our theoretical framework using\ndaily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime\nalgorithm systematically discovers all the `stylized facts' of the financial\ntime series automatically all at once, which were previously noted by many\nresearchers one at a time.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 00:04:00 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2013 18:59:35 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 23:03:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:40:19 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1308.0810", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "Risk-consistency of cross-validation with lasso-type procedures", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso and related sparsity inducing algorithms have been the target of\nsubstantial theoretical and applied research. Correspondingly, many results are\nknown about their behavior for a fixed or optimally chosen tuning parameter\nspecified up to unknown constants. In practice, however, this oracle tuning\nparameter is inaccessible so one must use the data to select one. Common\nstatistical practice is to use a variant of cross-validation for this task.\nHowever, little is known about the theoretical properties of the resulting\npredictions with such data-dependent methods. We consider the high-dimensional\nsetting with random design wherein the number of predictors $p$ grows with the\nnumber of observations $n$. Under typical assumptions on the data generating\nprocess, similar to those in the literature, we recover oracle rates up to a\nlog factor when choosing the tuning parameter with cross-validation. Under\nweaker conditions, when the true model is not necessarily linear, we show that\nthe lasso remains risk consistent relative to its linear oracle. We also\ngeneralize these results to the group lasso and square-root lasso and\ninvestigate the predictive and model selection performance of cross-validation\nvia simulation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 13:51:29 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 21:48:15 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 22:43:11 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1308.0900", "submitter": "Donny Lee", "authors": "Donny Lee", "title": "Trading USDCHF filtered by Gold dynamics via HMM coupling", "comments": "Abridge version titled \"The profitable, hidden and Markovian couple\n  of Swiss and gold\" in Nov '13 issue of Futures. Read it online at\n  http://www.futuresmag.com/2013/10/21/the-profitable-hidden-and-markovian-couple-of-swi", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a USDCHF trading strategy using the dynamics of gold as a filter.\nOur strategy involves modelling both USDCHF and gold using a coupled hidden\nMarkov model (CHMM). The observations will be indicators, RSI and CCI, which\nwill be used as triggers for our trading signals. Upon decoding the model in\neach iteration, we can get the next most probable state and the next most\nprobable observation. Hopefully by taking advantage of intermarket analysis and\nthe Markov property implicit in the model, trading with these most probable\nvalues will produce profitable results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 08:16:30 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 02:13:47 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Lee", "Donny", ""]]}, {"id": "1308.1196", "submitter": "Kentaro Tanaka", "authors": "Kentaro Tanaka, Masami Miyakawa", "title": "The Group Lasso for Design of Experiments", "comments": "11 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an application of the group lasso to design of experiments. Note\nthat we are NOT trying to explain experimental design for the group lasso.\nConversely, we explain how we can use the idea of the group lasso in\nexperimental design, showing that the problem of constructing an optimal design\nmatrix can be transformed into a problem of the group lasso. In some numerical\nexamples, we show that we can obtain the orthogonal arrays as the solutions of\nthe group lasso problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 07:24:36 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 05:08:42 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Tanaka", "Kentaro", ""], ["Miyakawa", "Masami", ""]]}, {"id": "1308.1269", "submitter": "Rajen Shah", "authors": "Rajen D. Shah and Nicolai Meinshausen", "title": "On b-bit min-wise hashing for large-scale regression and classification\n  with sparse data", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale regression problems where both the number of variables, $p$, and\nthe number of observations, $n$, may be large and in the order of millions or\nmore, are becoming increasingly more common. Typically the data are sparse:\nonly a fraction of a percent of the entries in the design matrix are non-zero.\nNevertheless, often the only computationally feasible approach is to perform\ndimension reduction to obtain a new design matrix with far fewer columns and\nthen work with this compressed data.\n  $b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimension\nreduction scheme for sparse matrices which produces a set of random features\nsuch that regression on the resulting design matrix approximates a kernel\nregression with the resemblance kernel. In this work, we derive bounds on the\nprediction error of such regressions. For both linear and logistic models we\nshow that the average prediction error vanishes asymptotically as long as $q\n\\|\\beta^*\\|_2^2 /n \\rightarrow 0$, where $q$ is the average number of non-zero\nentries in each row of the design matrix and $\\beta^*$ is the coefficient of\nthe linear predictor.\n  We also show that ordinary least squares or ridge regression applied to the\nreduced data can in fact allow us fit more flexible models. We obtain\nnon-asymptotic prediction error bounds for interaction models and for models\nwhere an unknown row normalisation must be applied in order for the signal to\nbe linear in the predictors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 13:42:24 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 17:28:02 GMT"}, {"version": "v3", "created": "Sat, 30 Apr 2016 07:34:37 GMT"}, {"version": "v4", "created": "Mon, 26 Feb 2018 17:26:13 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Shah", "Rajen D.", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1308.1359", "submitter": "David Ginsbourger", "authors": "David Ginsbourger (IMSV), Olivier Roustant (- M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, DEMO-ENSMSE), Nicolas\n  Durrande", "title": "Invariances of random fields paths, with applications in Gaussian\n  Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pathwise invariances of centred random fields that can be controlled\nthrough the covariance. A result involving composition operators is obtained in\nsecond-order settings, and we show that various path properties including\nadditivity boil down to invariances of the covariance kernel. These results are\nextended to a broader class of operators in the Gaussian case, via the Lo\\`eve\nisometry. Several covariance-driven pathwise invariances are illustrated,\nincluding fields with symmetric paths, centred paths, harmonic paths, or sparse\npaths. The proposed approach delivers a number of promising results and\nperspectives in Gaussian process regression.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 17:56:52 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Ginsbourger", "David", "", "IMSV"], ["Roustant", "Olivier", "", "- M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, DEMO-ENSMSE"], ["Durrande", "Nicolas", ""]]}, {"id": "1308.1479", "submitter": "Han Liu", "authors": "Jianqing Fan, Fang Han, Han Liu", "title": "Challenges of Big Data Analysis", "comments": null, "journal-ref": "National Science Review, 1:293-324, 2014", "doi": "10.1093/nsr/nwt032", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data bring new opportunities to modern society and challenges to data\nscientists. On one hand, Big Data hold great promises for discovering subtle\npopulation patterns and heterogeneities that are not possible with small-scale\ndata. On the other hand, the massive sample size and high dimensionality of Big\nData introduce unique computational and statistical challenges, including\nscalability and storage bottleneck, noise accumulation, spurious correlation,\nincidental endogeneity, and measurement errors. These challenges are\ndistinguished and require new computational and statistical paradigm. This\narticle give overviews on the salient features of Big Data and how these\nfeatures impact on paradigm change on statistical and computational methods as\nwell as computing architectures. We also provide various new perspectives on\nthe Big Data analysis and computation. In particular, we emphasis on the\nviability of the sparsest solution in high-confidence set and point out that\nexogeneous assumptions in most statistical methods for Big Data can not be\nvalidated due to incidental endogeneity. They can lead to wrong statistical\ninferences and consequently wrong scientific conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 05:09:33 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 09:32:46 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Fan", "Jianqing", ""], ["Han", "Fang", ""], ["Liu", "Han", ""]]}, {"id": "1308.1603", "submitter": "Dietmar Volz", "authors": "Dietmar Volz", "title": "A Note on Topology Preservation in Classification, and the Construction\n  of a Universal Neuron Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI nlin.AO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It will be shown that according to theorems of K. Menger, every neuron grid\nif identified with a curve is able to preserve the adopted qualitative\nstructure of a data space. Furthermore, if this identification is made, the\nneuron grid structure can always be mapped to a subset of a universal neuron\ngrid which is constructable in three space dimensions. Conclusions will be\ndrawn for established neuron grid types as well as neural fields.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 15:29:09 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 06:14:59 GMT"}, {"version": "v3", "created": "Mon, 22 Jan 2018 12:05:48 GMT"}, {"version": "v4", "created": "Thu, 1 Feb 2018 07:45:48 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Volz", "Dietmar", ""]]}, {"id": "1308.1975", "submitter": "Zhiyong Wang", "authors": "Zhiyong Wang and Jinbo Xu", "title": "Predicting protein contact map using evolutionary and physical\n  constraints by integer programming (extended version)", "comments": "14 pages, 13 figures, 10 tables", "journal-ref": "Bioinformatics (2013) 29 (13): i266-i273", "doi": "10.1093/bioinformatics/btt211", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Protein contact map describes the pairwise spatial and functional\nrelationship of residues in a protein and contains key information for protein\n3D structure prediction. Although studied extensively, it remains very\nchallenging to predict contact map using only sequence information. Most\nexisting methods predict the contact map matrix element-by-element, ignoring\ncorrelation among contacts and physical feasibility of the whole contact map. A\ncouple of recent methods predict contact map based upon residue co-evolution,\ntaking into consideration contact correlation and enforcing a sparsity\nrestraint, but these methods require a very large number of sequence homologs\nfor the protein under consideration and the resultant contact map may be still\nphysically unfavorable.\n  Results. This paper presents a novel method PhyCMAP for contact map\nprediction, integrating both evolutionary and physical restraints by machine\nlearning and integer linear programming (ILP). The evolutionary restraints\ninclude sequence profile, residue co-evolution and context-specific statistical\npotential. The physical restraints specify more concrete relationship among\ncontacts than the sparsity restraint. As such, our method greatly reduces the\nsolution space of the contact map matrix and thus, significantly improves\nprediction accuracy. Experimental results confirm that PhyCMAP outperforms\ncurrently popular methods no matter how many sequence homologs are available\nfor the protein under consideration. PhyCMAP can predict contacts within\nminutes after PSIBLAST search for sequence homologs is done, much faster than\nthe two recent methods PSICOV and EvFold.\n  See http://raptorx.uchicago.edu for the web server.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 20:44:01 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 16:24:06 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1308.2029", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised\n  Learning", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical probabilistic models, such as Gaussian mixture models, are\nwidely used for unsupervised learning tasks. These models consist of observable\nand latent variables, which represent the observable data and the underlying\ndata-generation process, respectively. Unsupervised learning tasks, such as\ncluster analysis, are regarded as estimations of latent variables based on the\nobservable ones. The estimation of latent variables in semi-supervised\nlearning, where some labels are observed, will be more precise than that in\nunsupervised, and one of the concerns is to clarify the effect of the labeled\ndata. However, there has not been sufficient theoretical analysis of the\naccuracy of the estimation of latent variables. In a previous study, a\ndistribution-based error function was formulated, and its asymptotic form was\ncalculated for unsupervised learning with generative models. It has been shown\nthat, for the estimation of latent variables, the Bayes method is more accurate\nthan the maximum-likelihood method. The present paper reveals the asymptotic\nforms of the error function in Bayesian semi-supervised learning for both\ndiscriminative and generative models. The results show that the generative\nmodel, which uses all of the given data, performs better when the model is well\nspecified.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 04:13:09 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 05:12:56 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 01:14:19 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1308.2302", "submitter": "Antoine Deleforge", "authors": "Antoine Deleforge and Florence Forbes and Radu Horaud", "title": "High-Dimensional Regression with Gaussian Mixtures and Partially-Latent\n  Response Variables", "comments": null, "journal-ref": "Statistics and Computing, 25(5), 893-911, 2015", "doi": "10.1007/s11222-014-9461-5", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of approximating high-dimensional data\nwith a low-dimensional representation. We make the following contributions. We\npropose an inverse regression method which exchanges the roles of input and\nresponse, such that the low-dimensional variable becomes the regressor, and\nwhich is tractable. We introduce a mixture of locally-linear probabilistic\nmapping model that starts with estimating the parameters of inverse regression,\nand follows with inferring closed-form solutions for the forward parameters of\nthe high-dimensional regression problem of interest. Moreover, we introduce a\npartially-latent paradigm, such that the vector-valued response variable is\ncomposed of both observed and latent entries, thus being able to deal with data\ncontaminated by experimental artifacts that cannot be explained with noise\nmodels. The proposed probabilistic formulation could be viewed as a\nlatent-variable augmentation of regression. We devise expectation-maximization\n(EM) procedures based on a data augmentation strategy which facilitates the\nmaximum-likelihood search over the model parameters. We propose two\naugmentation schemes and we describe in detail the associated EM inference\nprocedures that may well be viewed as generalizations of a number of EM\nregression, dimension reduction, and factor analysis algorithms. The proposed\nframework is validated with both synthetic and real data. We provide\nexperimental evidence that our method outperforms several existing regression\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 10:47:25 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 15:24:50 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2013 15:15:53 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Deleforge", "Antoine", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "1308.2403", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "CDfdr: A Comparison Density Approach to Local False Discovery Rate\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efron et al. (2001) proposed empirical Bayes formulation of the frequentist\nBenjamini and Hochbergs False Discovery Rate method (Benjamini and\nHochberg,1995). This article attempts to unify the `two cultures' using\nconcepts of comparison density and distribution function. We have also shown\nhow almost all of the existing local fdr methods can be viewed as proposing\nvarious model specification for comparison density - unifies the vast\nliterature of false discovery methods under one concept and notation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 15:46:36 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1308.2410", "submitter": "Grigori Fursin", "authors": "Grigori Fursin (INRIA Saclay - Ile de France)", "title": "Collective Mind: cleaning up the research and experimentation mess in\n  computer engineering using crowdsourcing, big data and machine learning", "comments": "I started drafting this document at the beginning of the development\n  of the 3rd version of plugin-based cTuning infrastructure and repository (aka\n  Collective Mind) to systematize and crowdsource program and architecture\n  auto-tuning; (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software and hardware co-design and optimization of HPC systems has become\nintolerably complex, ad-hoc, time consuming and error prone due to enormous\nnumber of available design and optimization choices, complex interactions\nbetween all software and hardware components, and multiple strict requirements\nplaced on performance, power consumption, size, reliability and cost. We\npresent our novel long-term holistic and practical solution to this problem\nbased on customizable, plugin-based, schema-free, heterogeneous, open-source\nCollective Mind repository and infrastructure with unified web interfaces and\non-line advise system. This collaborative framework distributes analysis and\nmulti-objective off-line and on-line auto-tuning of computer systems among many\nparticipants while utilizing any available smart phone, tablet, laptop, cluster\nor data center, and continuously observing, classifying and modeling their\nrealistic behavior. Any unexpected behavior is analyzed using shared data\nmining and predictive modeling plugins or exposed to the community at\ncTuning.org for collaborative explanation, top-down complexity reduction,\nincremental problem decomposition and detection of correlating program,\narchitecture or run-time properties (features). Gradually increasing\noptimization knowledge helps to continuously improve optimization heuristics of\nany compiler, predict optimizations for new programs or suggest efficient\nrun-time (online) tuning and adaptation strategies depending on end-user\nrequirements. We decided to share all our past research artifacts including\nhundreds of codelets, numerical applications, data sets, models, universal\nexperimental analysis and auto-tuning pipelines, self-tuning machine learning\nbased meta compiler, and unified statistical analysis and machine learning\nplugins in a public repository to initiate systematic, reproducible and\ncollaborative research, development and experimentation with a new publication\nmodel where experiments and techniques are validated, ranked and improved by\nthe community.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 17:01:06 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Fursin", "Grigori", "", "INRIA Saclay - Ile de France"]]}, {"id": "1308.2655", "submitter": "Loshchilov Ilya", "authors": "Ilya Loshchilov (LIS), Marc Schoenauer (INRIA Saclay - Ile de France,\n  LRI), Mich\\`ele Sebag (LRI)", "title": "KL-based Control of the Learning Schedule for Surrogate Black-Box\n  Optimization", "comments": null, "journal-ref": "Conf\\'erence sur l'Apprentissage Automatique (2013)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the control of an ML component within the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box\noptimization. The known CMA-ES weakness is its sample complexity, the number of\nevaluations of the objective function needed to approximate the global optimum.\nThis weakness is commonly addressed through surrogate optimization, learning an\nestimate of the objective function a.k.a. surrogate model, and replacing most\nevaluations of the true objective function with the (inexpensive) evaluation of\nthe surrogate model. This paper presents a principled control of the learning\nschedule (when to relearn the surrogate model), based on the Kullback-Leibler\ndivergence of the current search distribution and the training distribution of\nthe former surrogate model. The experimental validation of the proposed\napproach shows significant performance gains on a comprehensive set of\nill-conditioned benchmark problems, compared to the best state of the art\nincluding the quasi-Newton high-precision BFGS method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 19:31:59 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2013 19:30:19 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Loshchilov", "Ilya", "", "LIS"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France,\n  LRI"], ["Sebag", "Mich\u00e8le", "", "LRI"]]}, {"id": "1308.2853", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade", "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:16:10 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Janzamin", "Majid", ""], ["Kakade", "Sham", ""]]}, {"id": "1308.2867", "submitter": "Anastasios Kyrillidis", "authors": "Quoc Tran-Dinh, Anastasios Kyrillidis and Volkan Cevher", "title": "Composite Self-Concordant Minimization", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variable metric framework for minimizing the sum of a\nself-concordant function and a possibly non-smooth convex function, endowed\nwith an easily computable proximal operator. We theoretically establish the\nconvergence of our framework without relying on the usual Lipschitz gradient\nassumption on the smooth part. An important highlight of our work is a new set\nof analytic step-size selection and correction procedures based on the\nstructure of the problem. We describe concrete algorithmic instances of our\nframework for several interesting applications and demonstrate them numerically\non both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:55:12 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 15:20:52 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Kyrillidis", "Anastasios", ""], ["Cevher", "Volkan", ""]]}, {"id": "1308.2955", "submitter": "Nicolas Verzelen", "authors": "Ery Arias-Castro (Math Dept, UCSD), Nicolas Verzelen (MISTEA)", "title": "Community Detection in Sparse Random Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a tight community in a sparse random\nnetwork. This is formalized as testing for the existence of a dense random\nsubgraph in a random graph. Under the null hypothesis, the graph is a\nrealization of an Erd\\\"os-R\\'enyi graph on $N$ vertices and with connection\nprobability $p_0$; under the alternative, there is an unknown subgraph on $n$\nvertices where the connection probability is p1 > p0. In Arias-Castro and\nVerzelen (2012), we focused on the asymptotically dense regime where p0 is\nlarge enough that np0>(n/N)^{o(1)}. We consider here the asymptotically sparse\nregime where p0 is small enough that np0<(n/N)^{c0} for some c0>0. As before,\nwe derive information theoretic lower bounds, and also establish the\nperformance of various tests. Compared to our previous work, the arguments for\nthe lower bounds are based on the same technology, but are substantially more\ntechnical in the details; also, the methods we study are different: besides a\nvariant of the scan statistic, we study other statistics such as the size of\nthe largest connected component, the number of triangles, the eigengap of the\nadjacency matrix, etc. Our detection bounds are sharp, except in the Poisson\nregime where we were not able to fully characterize the constant arising in the\nbound.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 19:39:48 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 17:07:14 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Arias-Castro", "Ery", "", "Math Dept, UCSD"], ["Verzelen", "Nicolas", "", "MISTEA"]]}, {"id": "1308.3101", "submitter": "Christian H\\\"ane", "authors": "Christopher Zach and Christian H\\\"ane", "title": "Compact Relaxations for MAP Inference in Pairwise MRFs with Piecewise\n  Linear Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label assignment problems with large state spaces are important tasks\nespecially in computer vision. Often the pairwise interaction (or smoothness\nprior) between labels assigned at adjacent nodes (or pixels) can be described\nas a function of the label difference. Exact inference in such labeling tasks\nis still difficult, and therefore approximate inference methods based on a\nlinear programming (LP) relaxation are commonly used in practice. In this work\nwe study how compact linear programs can be constructed for general piecwise\nlinear smoothness priors. The number of unknowns is O(LK) per pairwise clique\nin terms of the state space size $L$ and the number of linear segments K. This\ncompares to an O(L^2) size complexity of the standard LP relaxation if the\npiecewise linear structure is ignored. Our compact construction and the\nstandard LP relaxation are equivalent and lead to the same (approximate) label\nassignment.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 12:27:24 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:51:30 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zach", "Christopher", ""], ["H\u00e4ne", "Christian", ""]]}, {"id": "1308.3201", "submitter": "Ulrike Schneider", "authors": "Ulrike Schneider", "title": "Confidence Sets Based on Thresholding Estimators in High-Dimensional\n  Gaussian Regression Models", "comments": "Section 1 and 2 rewritten, small numerical study added, minor\n  corrections", "journal-ref": "Economet. Rev. 35 (2016), 1412-1455", "doi": "10.1080/07474938.2015.1092798", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study confidence intervals based on hard-thresholding, soft-thresholding,\nand adaptive soft-thresholding in a linear regression model where the number of\nregressors $k$ may depend on and diverge with sample size $n$. In addition to\nthe case of known error variance, we define and study versions of the\nestimators when the error variance is unknown. In the known variance case, we\nprovide an exact analysis of the coverage properties of such intervals in\nfinite samples. We show that these intervals are always larger than the\nstandard interval based on the least-squares estimator. Asymptotically, the\nintervals based on the thresholding estimators are larger even by an order of\nmagnitude when the estimators are tuned to perform consistent variable\nselection. For the unknown-variance case, we provide non-trivial lower bounds\nfor the coverage probabilities in finite samples and conduct an asymptotic\nanalysis where the results from the known-variance case can be shown to carry\nover asymptotically if the number of degrees of freedom $n-k$ tends to infinity\nfast enough in relation to the thresholding parameter.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 18:25:49 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 15:00:44 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Schneider", "Ulrike", ""]]}, {"id": "1308.3314", "submitter": "Sebastien Loustau", "authors": "Camille Brunet (LAREMA), S\\'ebastien Loustau (LAREMA)", "title": "The algorithm of noisy k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we introduce a new algorithm to deal with finite dimensional\nclustering with errors in variables. The design of this algorithm is based on\nrecent theoretical advances (see Loustau (2013a,b)) in statistical learning\nwith errors in variables. As the previous mentioned papers, the algorithm mixes\ndifferent tools from the inverse problem literature and the machine learning\ncommunity. Coarsely, it is based on a two-step procedure: (1) a deconvolution\nstep to deal with noisy inputs and (2) Newton's iterations as the popular\nk-means.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 06:15:21 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Brunet", "Camille", "", "LAREMA"], ["Loustau", "S\u00e9bastien", "", "LAREMA"]]}, {"id": "1308.3381", "submitter": "Ananl Lotsi", "authors": "Anani Lotsi and Ernst Wit", "title": "High dimensional Sparse Gaussian Graphical Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of networks reconstruction from\nheterogeneous data using a Gaussian Graphical Mixture Model (GGMM). It is well\nknown that parameter estimation in this context is challenging due to large\nnumbers of variables coupled with the degeneracy of the likelihood. We propose\nas a solution a penalized maximum likelihood technique by imposing an $l_{1}$\npenalty on the precision matrix. Our approach shrinks the parameters thereby\nresulting in better identifiability and variable selection. We use the\nExpectation Maximization (EM) algorithm which involves the graphical LASSO to\nestimate the mixing coefficients and the precision matrices. We show that under\ncertain regularity conditions the Penalized Maximum Likelihood (PML) estimates\nare consistent. We demonstrate the performance of the PML estimator through\nsimulations and we show the utility of our method for high dimensional data\nanalysis in a genomic application.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 13:17:47 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 16:57:00 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2013 13:18:05 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Lotsi", "Anani", ""], ["Wit", "Ernst", ""]]}, {"id": "1308.3383", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven, Elena Marchiori", "title": "Axioms for graph clustering quality functions", "comments": "23 pages. Full text and sources available on:\n  http://www.cs.ru.nl/~T.vanLaarhoven/graph-clustering-axioms-2014/", "journal-ref": "Journal of Machine Learning Research, 15(Jan):193-215, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties that intuitively ought to be satisfied by graph\nclustering quality functions, that is, functions that assign a score to a\nclustering of a graph. Graph clustering, also known as network community\ndetection, is often performed by optimizing such a function. Two axioms\ntailored for graph clustering quality functions are introduced, and the four\naxioms introduced in previous work on distance based clustering are\nreformulated and generalized for the graph setting. We show that modularity, a\nstandard quality function for graph clustering, does not satisfy all of these\nsix properties. This motivates the derivation of a new family of quality\nfunctions, adaptive scale modularity, which does satisfy the proposed axioms.\nAdaptive scale modularity has two parameters, which give greater flexibility in\nthe kinds of clusterings that can be found. Standard graph clustering quality\nfunctions, such as normalized cut and unnormalized cut, are obtained as special\ncases of adaptive scale modularity.\n  In general, the results of our investigation indicate that the considered\naxiomatic framework covers existing `good' quality functions for graph\nclustering, and can be used to derive an interesting new family of quality\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 13:22:24 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 16:22:29 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1308.3506", "submitter": "Kevin Waugh", "authors": "Kevin Waugh and Brian D. Ziebart and J. Andrew Bagnell", "title": "Computational Rationalization: The Inverse Equilibrium Problem", "comments": "In submission to JMLR, conference version: arXiv:1103.5254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the purposeful behavior of imperfect agents from a small number of\nobservations is a challenging task. When restricted to the single-agent\ndecision-theoretic setting, inverse optimal control techniques assume that\nobserved behavior is an approximately optimal solution to an unknown decision\nproblem. These techniques learn a utility function that explains the example\nbehavior and can then be used to accurately predict or imitate future behavior\nin similar observed or unobserved situations.\n  In this work, we consider similar tasks in competitive and cooperative\nmulti-agent domains. Here, unlike single-agent settings, a player cannot\nmyopically maximize its reward; it must speculate on how the other agents may\nact to influence the game's outcome. Employing the game-theoretic notion of\nregret and the principle of maximum entropy, we introduce a technique for\npredicting and generalizing behavior.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 20:43:47 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Waugh", "Kevin", ""], ["Ziebart", "Brian D.", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1308.3740", "submitter": "Paul McNicholas", "authors": "Mateen Shaikh, Paul D. McNicholas, M. Luiza Antonie and T. Brendan\n  Murphy", "title": "Standardizing Interestingness Measures for Association Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interestingness measures provide information that can be used to prune or\nselect association rules. A given value of an interestingness measure is often\ninterpreted relative to the overall range of the values that the\ninterestingness measure can take. However, properties of individual association\nrules restrict the values an interestingness measure can achieve. An\ninteresting measure can be standardized to take this into account, but this has\nonly been done for one interestingness measure to date, i.e., the lift.\nStandardization provides greater insight than the raw value and may even alter\nresearchers' perception of the data. We derive standardized analogues of three\ninterestingness measures and use real and simulated data to compare them to\ntheir raw versions, each other, and the standardized lift.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 23:42:05 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Shaikh", "Mateen", ""], ["McNicholas", "Paul D.", ""], ["Antonie", "M. Luiza", ""], ["Murphy", "T. Brendan", ""]]}, {"id": "1308.3779", "submitter": "Luca Martino", "authors": "L. Martino, R. Casarin, F. Leisen, D. Luengo", "title": "Adaptive Independent Sticky MCMC algorithms", "comments": "A preliminary Matlab code is provided at\n  https://www.mathworks.com/matlabcentral/fileexchange/54701-adaptive-independent-sticky-metropolis--aism--algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel class of adaptive Monte Carlo methods,\ncalled adaptive independent sticky MCMC algorithms, for efficient sampling from\na generic target probability density function (pdf). The new class of\nalgorithms employs adaptive non-parametric proposal densities which become\ncloser and closer to the target as the number of iterations increases. The\nproposal pdf is built using interpolation procedures based on a set of support\npoints which is constructed iteratively based on previously drawn samples. The\nalgorithm's efficiency is ensured by a test that controls the evolution of the\nset of support points. This extra stage controls the computational cost and the\nconvergence of the proposal density to the target. Each part of the novel\nfamily of algorithms is discussed and several examples are provided. Although\nthe novel algorithms are presented for univariate target densities, we show\nthat they can be easily extended to the multivariate context within a\nGibbs-type sampler. The ergodicity is ensured and discussed. Exhaustive\nnumerical examples illustrate the efficiency of sticky schemes, both as a\nstand-alone methods to sample from complicated one-dimensional pdfs and within\nGibbs in order to draw from multi-dimensional target distributions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2013 12:35:08 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2013 12:59:49 GMT"}, {"version": "v3", "created": "Tue, 5 Aug 2014 17:42:02 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2016 14:49:27 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Martino", "L.", ""], ["Casarin", "R.", ""], ["Leisen", "F.", ""], ["Luengo", "D.", ""]]}, {"id": "1308.3818", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Reference Distance Estimator", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical study is presented for a simple linear classifier called\nreference distance estimator (RDE), which assigns the weight of each feature j\nas P(r|j)-P(r), where r is a reference feature relevant to the target class y.\nThe analysis shows that if r performs better than random guess in predicting y\nand is conditionally independent with each feature j, the RDE will have the\nsame classification performance as that from P(y|j)-P(y), a classifier trained\nwith the gold standard y. Since the estimation of P(r|j)-P(r) does not require\nlabeled data, under the assumption above, RDE trained with a large number of\nunlabeled examples would be close to that trained with infinite labeled\nexamples. For the case the assumption does not hold, we theoretically analyze\nthe factors that influence the closeness of the RDE to the perfect one under\nthe assumption, and present an algorithm to select reference features and\ncombine multiple RDEs from different reference features using both labeled and\nunlabeled data. The experimental results on 10 text classification tasks show\nthat the semi-supervised learning method improves supervised methods using\n5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its\nperformance is even close to a classifier trained with 13 million labeled\nexamples. In addition, the bounds in the theorems provide good estimation of\nthe classification performance and can be useful for new algorithm design.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 01:08:55 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1308.3925", "submitter": "Mercedes Richards", "authors": "Elizabeth Martinez-Gomez, Mercedes T. Richards, Donald St. P. Richards", "title": "Distance Correlation Methods for Discovering Associations in Large\n  Astrophysical Databases", "comments": "11 pages, 6 figures, 4 tables; Astrophysical Journal, accepted, in\n  press", "journal-ref": null, "doi": "10.1088/0004-637X/781/1/39", "report-no": null, "categories": "astro-ph.CO math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional, large-sample astrophysical databases of galaxy clusters,\nsuch as the Chandra Deep Field South COMBO-17 database, provide measurements on\nmany variables for thousands of galaxies and a range of redshifts. Current\nunderstanding of galaxy formation and evolution rests sensitively on\nrelationships between different astrophysical variables; hence an ability to\ndetect and verify associations or correlations between variables is important\nin astrophysical research. In this paper, we apply a recently defined\nstatistical measure called the distance correlation coefficient which can be\nused to identify new associations and correlations between astrophysical\nvariables. The distance correlation coefficient applies to variables of any\ndimension; it can be used to determine smaller sets of variables that provide\nequivalent astrophysical information; it is zero only when variables are\nindependent; and it is capable of detecting nonlinear associations that are\nundetectable by the classical Pearson correlation coefficient. Hence, the\ndistance correlation coefficient provides more information than the Pearson\ncoefficient. We analyze numerous pairs of variables in the COMBO-17 database\nwith the distance correlation method and with the maximal information\ncoefficient. We show that the Pearson coefficient can be estimated with higher\naccuracy from the corresponding distance correlation coefficient than from the\nmaximal information coefficient. For given values of the Pearson coefficient,\nthe distance correlation method has a greater ability than the maximal\ninformation coefficient to resolve astrophysical data into highly concentrated\nV-shapes, which enhances classification and pattern identification. These\nresults are observed over a range of redshifts beyond the local universe and\nfor galaxies from elliptical to spiral.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 05:27:09 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 20:47:20 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Martinez-Gomez", "Elizabeth", ""], ["Richards", "Mercedes T.", ""], ["Richards", "Donald St. P.", ""]]}, {"id": "1308.4004", "submitter": "Steffen Borgwardt", "authors": "Steffen Borgwardt, Andreas Brieden and Peter Gritzmann", "title": "A balanced k-means algorithm for weighted point sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical $k$-means algorithm for partitioning $n$ points in\n$\\mathbb{R}^d$ into $k$ clusters is one of the most popular and widely spread\nclustering methods. The need to respect prescribed lower bounds on the cluster\nsizes has been observed in many scientific and business applications.\n  In this paper, we present and analyze a generalization of $k$-means that is\ncapable of handling weighted point sets and prescribed lower and upper bounds\non the cluster sizes. We call it weight-balanced $k$-means. The key difference\nto existing models lies in the ability to handle the combination of weighted\npoint sets with prescribed bounds on the cluster sizes. This imposes the need\nto perform partial membership clustering, and leads to significant differences.\n  For example, while finite termination of all $k$-means variants for\nunweighted point sets is a simple consequence of the existence of only finitely\nmany partitions of a given set of points, the situation is more involved for\nweighted point sets, as there are infinitely many partial membership\nclusterings. Using polyhedral theory, we show that the number of iterations of\nweight-balanced $k$-means is bounded above by $n^{O(dk)}$, so in particular it\nis polynomial for fixed $k$ and $d$. This is similar to the known worst-case\nupper bound for classical $k$-means for unweighted point sets and unrestricted\ncluster sizes, despite the much more general framework. We conclude with the\ndiscussion of some additional favorable properties of our method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 12:46:33 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 11:57:36 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Borgwardt", "Steffen", ""], ["Brieden", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1308.4017", "submitter": "Berdakh Abibullaev", "authors": "Berdakh Abibullaev, Jinung An, Seung-Hyun Lee and Jeon-Il Moon", "title": "A Study on Stroke Rehabilitation through Task-Oriented Control of a\n  Haptic Device via Near-Infrared Spectroscopy-Based BCI", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study in task-oriented approach to stroke\nrehabilitation by controlling a haptic device via near-infrared\nspectroscopy-based brain-computer interface (BCI). The task is to command the\nhaptic device to move in opposing directions of leftward and rightward\nmovement. Our study consists of data acquisition, signal preprocessing, and\nclassification. In data acquisition, we conduct experiments based on two\ndifferent mental tasks: one on pure motor imagery, and another on combined\nmotor imagery and action observation. The experiments were conducted in both\noffline and online modes. In the signal preprocessing, we use localization\nmethod to eliminate channels that are irrelevant to the mental task, as well as\nperform feature extraction for subsequent classification. We propose multiple\nsupport vector machine classifiers with a majority-voting scheme for improved\nclassification results. And lastly, we present test results to demonstrate the\nefficacy of our proposed approach to possible stroke rehabilitation practice.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 13:31:34 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 09:32:52 GMT"}, {"version": "v3", "created": "Mon, 14 Apr 2014 08:06:07 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Abibullaev", "Berdakh", ""], ["An", "Jinung", ""], ["Lee", "Seung-Hyun", ""], ["Moon", "Jeon-Il", ""]]}, {"id": "1308.4200", "submitter": "Erik Rodner", "authors": "Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, Kate Saenko", "title": "Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with\n  Implicit Low-rank Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images seen during test time are often not from the same distribution as\nimages used for learning. This problem, known as domain shift, occurs when\ntraining classifiers from object-centric internet image databases and trying to\napply them directly to scene understanding tasks. The consequence is often\nsevere performance degradation and is one of the major barriers for the\napplication of classifiers in real-world systems. In this paper, we show how to\nlearn transform-based domain adaptation classifiers in a scalable manner. The\nkey idea is to exploit an implicit rank constraint, originated from a\nmax-margin domain adaptation formulation, to make optimization tractable.\nExperiments show that the transformation between domains can be very\nefficiently learned from data and easily applied to new categories. This begins\nto bridge the gap between large-scale internet image collections and object\nimages captured in everyday life environments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 01:07:35 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Rodner", "Erik", ""], ["Hoffman", "Judy", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1308.4211", "submitter": "William Fithian", "authors": "William Fithian and Rahul Mazumder", "title": "Flexible Low-Rank Statistical Modeling with Side Information", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for reduced-rank modeling of matrix-valued\ndata. By applying a generalized nuclear norm penalty we can directly model\nlow-dimensional latent variables associated with rows and columns. Our\nframework flexibly incorporates row and column features, smoothing kernels, and\nother sources of side information by penalizing deviations from the row and\ncolumn models. Moreover, a large class of these models can be estimated\nscalably using convex optimization. The computational bottleneck in each case\nis one singular value decomposition per iteration of a large but easy-to-apply\nmatrix. Our framework generalizes traditional convex matrix completion and\nmulti-task learning methods as well as maximum a posteriori estimation under a\nlarge class of popular hierarchical Bayesian models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 02:33:49 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 17:38:54 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Fithian", "William", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1308.4214", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent\n  Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr\\'ed\\'eric Bastien,\n  Yoshua Bengio", "title": "Pylearn2: a machine learning research library", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pylearn2 is a machine learning research library. This does not just mean that\nit is a collection of machine learning algorithms that share a common API; it\nmeans that it has been designed for flexibility and extensibility in order to\nfacilitate research projects that involve new or unusual use cases. In this\npaper we give a brief history of the library, an overview of its basic\nphilosophy, a summary of the library's architecture, and a description of how\nthe Pylearn2 community functions socially.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 02:50:43 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Warde-Farley", "David", ""], ["Lamblin", "Pascal", ""], ["Dumoulin", "Vincent", ""], ["Mirza", "Mehdi", ""], ["Pascanu", "Razvan", ""], ["Bergstra", "James", ""], ["Bastien", "Fr\u00e9d\u00e9ric", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1308.4338", "submitter": "Leonardo Torres", "authors": "Leonardo Torres and Alejandro C. Frery", "title": "SAR Image Despeckling Algorithms using Stochastic Distances and Nonlocal\n  Means", "comments": "Accepted for publication in Workshop of Theses and Dissertations\n  (WTD) in Conference on Graphics, Patterns, and Images (SIBGRAPI 2013). This\n  paper received the first best work award in the Dissertation category at the\n  WTD-SIBGRAPI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two approaches for filter design based on stochastic\ndistances for intensity speckle reduction. A window is defined around each\npixel, overlapping samples are compared and only those which pass a\ngoodness-of-fit test are used to compute the filtered value. The tests stem\nfrom stochastic divergences within the Information Theory framework. The\ntechnique is applied to intensity Synthetic Aperture Radar (SAR) data with\nhomogeneous regions using the Gamma model. The first approach uses a\nNagao-Matsuyama-type procedure for setting the overlapping samples, and the\nsecond uses the nonlocal method. The proposals are compared with the Improved\nSigma filter and with anisotropic diffusion for speckled data (SRAD) using a\nprotocol based on Monte Carlo simulation. Among the criteria used to quantify\nthe quality of filters, we employ the equivalent number of looks, and line and\nedge preservation. Moreover, we also assessed the filters by the Universal\nImage Quality Index and by the Pearson correlation between edges. Applications\nto real images are also discussed. The proposed methods show good results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 15:58:19 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Torres", "Leonardo", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1308.4568", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Distributed Online Learning via Cooperative Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel framework for decentralized, online learning\nby many learners. At each moment of time, an instance characterized by a\ncertain context may arrive to each learner; based on the context, the learner\ncan select one of its own actions (which gives a reward and provides\ninformation) or request assistance from another learner. In the latter case,\nthe requester pays a cost and receives the reward but the provider learns the\ninformation. In our framework, learners are modeled as cooperative contextual\nbandits. Each learner seeks to maximize the expected reward from its arrivals,\nwhich involves trading off the reward received from its own actions, the\ninformation learned from its own actions, the reward received from the actions\nrequested of others and the cost paid for these actions - taking into account\nwhat it has learned about the value of assistance from each other learner. We\ndevelop distributed online learning algorithms and provide analytic bounds to\ncompare the efficiency of these with algorithms with the complete knowledge\n(oracle) benchmark (in which the expected reward of every action in every\ncontext is known by every learner). Our estimates show that regret - the loss\nincurred by the algorithm - is sublinear in time. Our theoretical framework can\nbe used in many practical applications including Big Data mining, event\ndetection in surveillance sensor networks and distributed online recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 13:28:43 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2013 14:19:28 GMT"}, {"version": "v3", "created": "Sat, 19 Apr 2014 09:40:06 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2015 14:06:27 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1308.4718", "submitter": "Radu Balan", "authors": "Radu Balan, Yang Wang", "title": "Invertibility and Robustness of Phaseless Reconstruction", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the question of reconstructing a vector in a\nfinite-dimensional real Hilbert space when only the magnitudes of the\ncoefficients of the vector under a redundant linear map are known. We analyze\nvarious Lipschitz bounds of the nonlinear analysis map and we establish\ntheoretical performance bounds of any reconstruction algorithm. We show that\nrobust and stable reconstruction requires additional redundancy than the\ncritical threshold.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 21:06:42 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Balan", "Radu", ""], ["Wang", "Yang", ""]]}, {"id": "1308.4747", "submitter": "Emily B. Fox", "authors": "Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan", "title": "Joint modeling of multiple time series via the beta process with\n  application to motion capture segmentation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS742 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note: text\n  overlap with arXiv:1111.4226", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1281-1313", "doi": "10.1214/14-AOAS742", "report-no": "IMS-AOAS-AOAS742", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to the problem of jointly\nmodeling multiple related time series. Our model discovers a latent set of\ndynamical behaviors shared among the sequences, and segments each time series\ninto regions defined by a subset of these behaviors. Using a beta process\nprior, the size of the behavior set and the sharing pattern are both inferred\nfrom data. We develop Markov chain Monte Carlo (MCMC) methods based on the\nIndian buffet process representation of the predictive distribution of the beta\nprocess. Our MCMC inference algorithm efficiently adds and removes behaviors\nvia novel split-merge moves as well as data-driven birth and death proposals,\navoiding the need to consider a truncated model. We demonstrate promising\nresults on unsupervised segmentation of human motion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 00:52:02 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 05:55:04 GMT"}, {"version": "v3", "created": "Thu, 13 Nov 2014 10:11:57 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Fox", "Emily B.", ""], ["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1308.4757", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Rujie Liu", "title": "Online and stochastic Douglas-Rachford splitting method for large scale\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online and stochastic learning has emerged as powerful tool in large scale\noptimization. In this work, we generalize the Douglas-Rachford splitting (DRs)\nmethod for minimizing composite functions to online and stochastic settings (to\nour best knowledge this is the first time DRs been generalized to sequential\nversion). We first establish an $O(1/\\sqrt{T})$ regret bound for batch DRs\nmethod. Then we proved that the online DRs splitting method enjoy an $O(1)$\nregret bound and stochastic DRs splitting has a convergence rate of\n$O(1/\\sqrt{T})$. The proof is simple and intuitive, and the results and\ntechnique can be served as a initiate for the research on the large scale\nmachine learning employ the DRs method. Numerical experiments of the proposed\nmethod demonstrate the effectiveness of the online and stochastic update rule,\nand further confirm our regret and convergence analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 03:40:41 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 06:21:25 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2013 06:50:16 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 04:30:41 GMT"}, {"version": "v5", "created": "Wed, 25 Sep 2013 08:20:54 GMT"}, {"version": "v6", "created": "Tue, 16 Aug 2016 07:05:38 GMT"}, {"version": "v7", "created": "Mon, 10 Oct 2016 08:46:25 GMT"}, {"version": "v8", "created": "Tue, 11 Oct 2016 00:52:10 GMT"}, {"version": "v9", "created": "Wed, 21 Dec 2016 07:05:13 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Shi", "Ziqiang", ""], ["Liu", "Rujie", ""]]}, {"id": "1308.4915", "submitter": "Braxton Osting", "authors": "Braxton Osting, Chris D. White, Edouard Oudet", "title": "Minimal Dirichlet energy partitions for graphs", "comments": "17 pages, 6 figures", "journal-ref": "SIAM Journal of Scientific Computing 36 (2014), no. 4, pp.\n  A1635-A1651", "doi": "10.1137/130934568", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a geometric problem, we introduce a new non-convex graph\npartitioning objective where the optimality criterion is given by the sum of\nthe Dirichlet eigenvalues of the partition components. A relaxed formulation is\nidentified and a novel rearrangement algorithm is proposed, which we show is\nstrictly decreasing and converges in a finite number of iterations to a local\nminimum of the relaxed objective function. Our method is applied to several\nclustering problems on graphs constructed from synthetic data, MNIST\nhandwritten digits, and manifold discretizations. The model has a\nsemi-supervised extension and provides a natural representative for the\nclusters as well.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 17:02:57 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 04:13:06 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Osting", "Braxton", ""], ["White", "Chris D.", ""], ["Oudet", "Edouard", ""]]}, {"id": "1308.4922", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Learning Deep Representation Without Parameter Inference for Nonlinear\n  Dimensionality Reduction", "comments": "This paper has been withdrawn by the author due to a lack of full\n  empirical evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep learning is one of the most powerful representation\nlearning techniques. Restricted Boltzman machine, sparse coding, regularized\nauto-encoders, and convolutional neural networks are pioneering building blocks\nof deep learning. In this paper, we propose a new building block -- distributed\nrandom models. The proposed method is a special full implementation of the\nproduct of experts: (i) each expert owns multiple hidden units and different\nexperts have different numbers of hidden units; (ii) the model of each expert\nis a k-center clustering, whose k-centers are only uniformly sampled examples,\nand whose output (i.e. the hidden units) is a sparse code that only the\nsimilarity values from a few nearest neighbors are reserved. The relationship\nbetween the pioneering building blocks, several notable research branches and\nthe proposed method is analyzed. Experimental results show that the proposed\ndeep model can learn better representations than deep belief networks and\nmeanwhile can train a much larger network with much less time than deep belief\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 17:15:36 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 23:35:03 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1308.5036", "submitter": "Yang Feng", "authors": "Yang Feng, Tengfei Li, Zhiliang Ying", "title": "Likelihood Adaptively Modified Penalties", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of penalty functions, adaptive to likelihood, is introduced for\nmodel selection in general regression models. It arises naturally through\nassuming certain types of prior distribution on the regression parameters. To\nstudy stability properties of the penalized maximum likelihood estimator, two\ntypes of asymptotic stability are defined. Theoretical properties, including\nthe parameter estimation consistency, model selection consistency, and\nasymptotic stability, are established under suitable regularity conditions. An\nefficient coordinate-descent algorithm is proposed. Simulation results and real\ndata analysis show that the proposed method has competitive performance in\ncomparison with existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 03:30:31 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Feng", "Yang", ""], ["Li", "Tengfei", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1308.5038", "submitter": "Ivan Selesnick", "authors": "Po-Yu Chen, Ivan W. Selesnick", "title": "Group-Sparse Signal Denoising: Non-Convex Regularization, Convex\n  Optimization", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2329274", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization with sparsity-promoting convex regularization is a\nstandard approach for estimating sparse signals in noise. In order to promote\nsparsity more strongly than convex regularization, it is also standard practice\nto employ non-convex optimization. In this paper, we take a third approach. We\nutilize a non-convex regularization term chosen such that the total cost\nfunction (consisting of data consistency and regularization terms) is convex.\nTherefore, sparsity is more strongly promoted than in the standard convex\nformulation, but without sacrificing the attractive aspects of convex\noptimization (unique minimum, robust algorithms, etc.). We use this idea to\nimprove the recently developed 'overlapping group shrinkage' (OGS) algorithm\nfor the denoising of group-sparse signals. The algorithm is applied to the\nproblem of speech enhancement with favorable results in terms of both SNR and\nperceptual quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 03:32:57 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2013 19:18:49 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Chen", "Po-Yu", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1308.5200", "submitter": "Nicolas Boumal", "authors": "Nicolas Boumal and Bamdev Mishra and P.-A. Absil and Rodolphe\n  Sepulchre", "title": "Manopt, a Matlab toolbox for optimization on manifolds", "comments": null, "journal-ref": "The Journal of Machine Learning Research, 15(1), 1455-1459 (2014)", "doi": null, "report-no": null, "categories": "cs.MS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization on manifolds is a rapidly developing branch of nonlinear\noptimization. Its focus is on problems where the smooth geometry of the search\nspace can be leveraged to design efficient numerical algorithms. In particular,\noptimization on manifolds is well-suited to deal with rank and orthogonality\nconstraints. Such structured constraints appear pervasively in machine learning\napplications, including low-rank matrix completion, sensor network\nlocalization, camera network registration, independent component analysis,\nmetric learning, dimensionality reduction and so on. The Manopt toolbox,\navailable at www.manopt.org, is a user-friendly, documented piece of software\ndedicated to simplify experimenting with state of the art Riemannian\noptimization algorithms. We aim particularly at reaching practitioners outside\nour field.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 18:35:59 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Boumal", "Nicolas", ""], ["Mishra", "Bamdev", ""], ["Absil", "P. -A.", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1308.5275", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "The Lovasz-Bregman Divergence and connections to rank aggregation,\n  clustering, and web ranking", "comments": "18 pages. A shorter version appeared in Proc. Uncertainty in\n  Artificial Intelligence (UAI)-2013, Bellevue, WA", "journal-ref": "UAI-2013", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the recently introduced theory of Lovasz-Bregman (LB) divergences\n(Iyer & Bilmes, 2012) in several ways. We show that they represent a distortion\nbetween a 'score' and an 'ordering', thus providing a new view of rank\naggregation and order based clustering with interesting connections to web\nranking. We show how the LB divergences have a number of properties akin to\nmany permutation based metrics, and in fact have as special cases forms very\nsimilar to the Kendall-$\\tau$ metric. We also show how the LB divergences\nsubsume a number of commonly used ranking measures in information retrieval,\nlike the NDCG and AUC. Unlike the traditional permutation based metrics,\nhowever, the LB divergence naturally captures a notion of \"confidence\" in the\norderings, thus providing a new representation to applications involving\naggregating scores as opposed to just orderings. We show how a number of\nrecently used web ranking models are forms of Lovasz-Bregman rank aggregation\nand also observe that a natural form of Mallow's model using the LB divergence\nhas been used as conditional ranking models for the 'Learning to Rank' problem.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 01:31:22 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1308.5465", "submitter": "Radu Balan", "authors": "Radu Balan", "title": "Stability of Phase Retrievable Frames", "comments": "13 pages, presented at SPIE 2013 conference", "journal-ref": null, "doi": "10.1117/12.2026135", "report-no": null, "categories": "math.FA cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the property of phase retrievability by redundant\nsysems of vectors under perturbations of the frame set. Specifically we show\nthat if a set $\\fc$ of $m$ vectors in the complex Hilbert space of dimension n\nallows for vector reconstruction from magnitudes of its coefficients, then\nthere is a perturbation bound $\\rho$ so that any frame set within $\\rho$ from\n$\\fc$ has the same property. In particular this proves the recent construction\nin \\cite{BH13} is stable under perturbations. By the same token we reduce the\ncritical cardinality conjectured in \\cite{BCMN13a} to proving a stability\nresult for non phase-retrievable frames.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2013 23:59:15 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Balan", "Radu", ""]]}, {"id": "1308.5546", "submitter": "Jeremy Rapin", "authors": "J\\'er\\'emy Rapin, J\\'er\\^ome Bobin, Anthony Larue and Jean-Luc Starck", "title": "Sparse and Non-Negative BSS for Noisy Data", "comments": "13 pages, 18 figures, to be published in IEEE Transactions on Signal\n  Processing", "journal-ref": "IEEE Trans. Signal Process. 61 (2013) 5620-5632", "doi": "10.1109/TSP.2013.2279358", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative blind source separation (BSS) has raised interest in various\nfields of research, as testified by the wide literature on the topic of\nnon-negative matrix factorization (NMF). In this context, it is fundamental\nthat the sources to be estimated present some diversity in order to be\nefficiently retrieved. Sparsity is known to enhance such contrast between the\nsources while producing very robust approaches, especially to noise. In this\npaper we introduce a new algorithm in order to tackle the blind separation of\nnon-negative sparse sources from noisy measurements. We first show that\nsparsity and non-negativity constraints have to be carefully applied on the\nsought-after solution. In fact, improperly constrained solutions are unlikely\nto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA\n(non-negative Generalized Morphological Component Analysis), makes use of\nproximal calculus techniques to provide properly constrained solutions. The\nperformance of nGMCA compared to other state-of-the-art algorithms is\ndemonstrated by numerical experiments encompassing a wide variety of settings,\nwith negligible parameter tuning. In particular, nGMCA is shown to provide\nrobustness to noise and performs well on synthetic mixtures of real NMR\nspectra.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 11:31:38 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Rapin", "J\u00e9r\u00e9my", ""], ["Bobin", "J\u00e9r\u00f4me", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1308.5576", "submitter": "Francesco  Palmieri A. N.", "authors": "Francesco A. N. Palmieri", "title": "A Comparison of Algorithms for Learning Hidden Variables in Normal\n  Graphs", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian factor graph reduced to normal form consists in the\ninterconnection of diverter units (or equal constraint units) and\nSingle-Input/Single-Output (SISO) blocks. In this framework localized\nadaptation rules are explicitly derived from a constrained maximum likelihood\n(ML) formulation and from a minimum KL-divergence criterion using KKT\nconditions. The learning algorithms are compared with two other updating\nequations based on a Viterbi-like and on a variational approximation\nrespectively. The performance of the various algorithm is verified on synthetic\ndata sets for various architectures. The objective of this paper is to provide\nthe programmer with explicit algorithms for rapid deployment of Bayesian graphs\nin the applications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 13:20:25 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Palmieri", "Francesco A. N.", ""]]}, {"id": "1308.5609", "submitter": "Yu Zhang", "authors": "Yu Zhang, Guoxu Zhou, Jing Jin, Xingyu Wang, Andrzej Cichocki", "title": "Frequency Recognition in SSVEP-based BCI using Multiset Canonical\n  Correlation Analysis", "comments": null, "journal-ref": "International Journal of Neural Systems, 2014, vol.24, no.2,\n  pp.1450013 (14 pages)", "doi": "10.1142/S0129065714500130", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has been one of the most popular methods\nfor frequency recognition in steady-state visual evoked potential (SSVEP)-based\nbrain-computer interfaces (BCIs). Despite its efficiency, a potential problem\nis that using pre-constructed sine-cosine waves as the required reference\nsignals in the CCA method often does not result in the optimal recognition\naccuracy due to their lack of features from the real EEG data. To address this\nproblem, this study proposes a novel method based on multiset canonical\ncorrelation analysis (MsetCCA) to optimize the reference signals used in the\nCCA method for SSVEP frequency recognition. The MsetCCA method learns multiple\nlinear transforms that implement joint spatial filtering to maximize the\noverall correlation among canonical variates, and hence extracts SSVEP common\nfeatures from multiple sets of EEG data recorded at the same stimulus\nfrequency. The optimized reference signals are formed by combination of the\ncommon features and completely based on training data. Experimental study with\nEEG data from ten healthy subjects demonstrates that the MsetCCA method\nimproves the recognition accuracy of SSVEP frequency in comparison with the CCA\nmethod and other two competing methods (multiway CCA (MwayCCA) and phase\nconstrained CCA (PCCA)), especially for a small number of channels and a short\ntime window length. The superiority indicates that the proposed MsetCCA method\nis a new promising candidate for frequency recognition in SSVEP-based BCIs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 15:11:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 08:40:37 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Zhang", "Yu", ""], ["Zhou", "Guoxu", ""], ["Jin", "Jing", ""], ["Wang", "Xingyu", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1308.5712", "submitter": "Alexander Luedtke", "authors": "Alexander Luedtke and Linh Tran", "title": "The Generalized Mean Information Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reshef & Reshef recently published a paper in which they present a method\ncalled the Maximal Information Coefficient (MIC) that can detect all forms of\nstatistical dependence between pairs of variables as sample size goes to\ninfinity. While this method has been praised by some, it has also been\ncriticized for its lack of power in finite samples. We seek to modify MIC so\nthat it has higher power in detecting associations for limited sample sizes.\nHere we present the Generalized Mean Information Coefficient (GMIC), a\ngeneralization of MIC which incorporates a tuning parameter that can be used to\nmodify the complexity of the association favored by the measure. We define GMIC\nand prove it maintains several key asymptotic properties of MIC. Its increased\npower over MIC is demonstrated using a simulation of eight different functional\nrelationships at sixty different noise levels. The results are compared to the\nPearson correlation, distance correlation, and MIC. Simulation results suggest\nthat while generally GMIC has slightly lower power than the distance\ncorrelation measure, it achieves higher power than MIC for many forms of\nunderlying association. For some functional relationships, GMIC surpasses all\nother statistics calculated. Preliminary results suggest choosing a moderate\nvalue of the tuning parameter for GMIC will yield a test that is robust across\nunderlying relationships. GMIC is a promising new method that mitigates the\npower issues suffered by MIC, at the possible expense of equitability.\nNonetheless, distance correlation was in our simulations more powerful for many\nforms of underlying relationships. At a minimum, this work motivates further\nconsideration of maximal information-based nonparametric exploration (MINE)\nmethods as statistical tests of independence.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 22:05:46 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Luedtke", "Alexander", ""], ["Tran", "Linh", ""]]}, {"id": "1308.6069", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Jin Li", "title": "Compound Poisson Processes, Latent Shrinkage Priors and Bayesian\n  Nonconvex Penalization", "comments": "Published at http://dx.doi.org/10.1214/14-BA892 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 247-274", "doi": "10.1214/14-BA892", "report-no": "VTeX-BA-BA892", "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss Bayesian nonconvex penalization for sparse learning\nproblems. We explore a nonparametric formulation for latent shrinkage\nparameters using subordinators which are one-dimensional L\\'{e}vy processes. We\nparticularly study a family of continuous compound Poisson subordinators and a\nfamily of discrete compound Poisson subordinators. We exemplify four specific\nsubordinators: Gamma, Poisson, negative binomial and squared Bessel\nsubordinators. The Laplace exponents of the subordinators are Bernstein\nfunctions, so they can be used as sparsity-inducing nonconvex penalty\nfunctions. We exploit these subordinators in regression problems, yielding a\nhierarchical model with multiple regularization parameters. We devise ECME\n(Expectation/Conditional Maximization Either) algorithms to simultaneously\nestimate regression coefficients and regularization parameters. The empirical\nevaluation of simulated data shows that our approach is feasible and effective\nin high-dimensional data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 06:05:42 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:34:53 GMT"}, {"version": "v3", "created": "Fri, 15 May 2015 06:01:46 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Zhang", "Zhihua", ""], ["Li", "Jin", ""]]}, {"id": "1308.6181", "submitter": "Jesus Cerquides", "authors": "Victor Bellon and Jesus Cerquides and Ivo Grosse", "title": "Bayesian Conditional Gaussian Network Classifiers with Applications to\n  Mass Spectra Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers based on probabilistic graphical models are very effective. In\ncontinuous domains, maximum likelihood is usually used to assess the\npredictions of those classifiers. When data is scarce, this can easily lead to\noverfitting. In any probabilistic setting, Bayesian averaging (BA) provides\ntheoretically optimal predictions and is known to be robust to overfitting. In\nthis work we introduce Bayesian Conditional Gaussian Network Classifiers, which\nefficiently perform exact Bayesian averaging over the parameters. We evaluate\nthe proposed classifiers against the maximum likelihood alternatives proposed\nso far over standard UCI datasets, concluding that performing BA improves the\nquality of the assessed probabilities (conditional log likelihood) whilst\nmaintaining the error rate.\n  Overfitting is more likely to occur in domains where the number of data items\nis small and the number of variables is large. These two conditions are met in\nthe realm of bioinformatics, where the early diagnosis of cancer from mass\nspectra is a relevant task. We provide an application of our classification\nframework to that problem, comparing it with the standard maximum likelihood\nalternative, where the improvement of quality in the assessed probabilities is\nconfirmed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 15:14:47 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Bellon", "Victor", ""], ["Cerquides", "Jesus", ""], ["Grosse", "Ivo", ""]]}, {"id": "1308.6273", "submitter": "Rong Ge", "authors": "Sanjeev Arora and Rong Ge and Ankur Moitra", "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse recovery we are given a matrix $A$ (the dictionary) and a vector of\nthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is a\ncentral notion in signal processing, statistics and machine learning. But in\napplications such as sparse coding, edge detection, compression and super\nresolution, the dictionary $A$ is unknown and has to be learned from random\nexamples of the form $Y = AX$ where $X$ is drawn from an appropriate\ndistribution --- this is the dictionary learning problem. In most settings, $A$\nis overcomplete: it has more columns than rows. This paper presents a\npolynomial-time algorithm for learning overcomplete dictionaries; the only\npreviously known algorithm with provable guarantees is the recent work of\nSpielman, Wang and Wright who gave an algorithm for the full-rank case, which\nis rarely the case in applications. Our algorithm applies to incoherent\ndictionaries which have been a central object of study since they were\nintroduced in seminal work of Donoho and Huo. In particular, a dictionary is\n$\\mu$-incoherent if each pair of columns has inner product at most $\\mu /\n\\sqrt{n}$.\n  The algorithm makes natural stochastic assumptions about the unknown sparse\nvector $X$, which can contain $k \\leq c \\min(\\sqrt{n}/\\mu \\log n, m^{1/2\n-\\eta})$ non-zero entries (for any $\\eta > 0$). This is close to the best $k$\nallowable by the best sparse recovery algorithms even if one knows the\ndictionary $A$ exactly. Moreover, both the running time and sample complexity\ndepend on $\\log 1/\\epsilon$, where $\\epsilon$ is the target accuracy, and so\nour algorithms converge very quickly to the true dictionary. Our algorithm can\nalso tolerate substantial amounts of noise provided it is incoherent with\nrespect to the dictionary (e.g., Gaussian). In the noisy setting, our running\ntime and sample complexity depend polynomially on $1/\\epsilon$, and this is\nnecessary.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 19:46:05 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 19:34:59 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 18:35:17 GMT"}, {"version": "v5", "created": "Mon, 26 May 2014 17:38:58 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1308.6315", "submitter": "Paul McNicholas", "authors": "Katherine Morris and Paul D. McNicholas", "title": "Clustering, Classification, Discriminant Analysis, and Dimension\n  Reduction via Generalized Hyperbolic Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.10.008", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for dimension reduction with clustering, classification, or\ndiscriminant analysis is introduced. This mixture model-based approach is based\non fitting generalized hyperbolic mixtures on a reduced subspace within the\nparadigm of model-based clustering, classification, or discriminant analysis. A\nreduced subspace of the data is derived by considering the extent to which\ngroup means and group covariances vary. The members of the subspace arise\nthrough linear combinations of the original data, and are ordered by importance\nvia the associated eigenvalues. The observations can be projected onto the\nsubspace, resulting in a set of variables that captures most of the clustering\ninformation available. The use of generalized hyperbolic mixtures gives a\nrobust framework capable of dealing with skewed clusters. Although dimension\nreduction is increasingly in demand across many application areas, the authors\nare most familiar with biological applications and so two of the five real data\nexamples are within that sphere. Simulated data are also used for illustration.\nThe approach introduced herein can be considered the most general such approach\navailable, and so we compare results to three special and limiting cases.\nComparisons with several well established techniques illustrate its promising\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 21:24:50 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 22:35:18 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 22:20:50 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Morris", "Katherine", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1308.6342", "submitter": "Nando de Freitas", "authors": "Yariv Dror Mizrahi, Misha Denil and Nando de Freitas", "title": "Linear and Parallel Learning of Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new embarrassingly parallel parameter learning algorithm for\nMarkov random fields with untied parameters which is efficient for a large\nclass of practical models. Our algorithm parallelizes naturally over cliques\nand, for graphs of bounded degree, its complexity is linear in the number of\ncliques. Unlike its competitors, our algorithm is fully parallel and for\nlog-linear models it is also data efficient, requiring only the local\nsufficient statistics of the data to estimate parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 01:55:37 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 15:04:57 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 15:05:06 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2014 17:59:18 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Mizrahi", "Yariv Dror", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1308.6487", "submitter": "Leonardo Torres", "authors": "Leonardo Torres and Tamer Cavalcante and Alejandro C. Frery", "title": "A New Algorithm of Speckle Filtering using Stochastic Distances", "comments": "Accepted for publication on the proceedings of the IEEE Geoscience\n  and Remote Sensing Symposium (IGARSS 2012), to be published in IEEE Press.\n  Available: http://www.igarss2012.org/Papers/viewpapers.asp?papernum=4877", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for filter design based on stochastic\ndistances and tests between distributions. A window is defined around each\npixel, overlapping samples are compared and only those which pass a\ngoodness-of-fit test are used to compute the filtered value. The technique is\napplied to intensity SAR data with homogeneous regions using the Gamma model.\nThe proposal is compared with the Lee's filter using a protocol based on Monte\nCarlo. Among the criteria used to quantify the quality of filters, we employ\nthe equivalent number of looks, line and edge preservation. Moreover, we also\nassessed the filters by the Universal Image Quality Index and the Pearson's\ncorrelation on edges regions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 14:56:01 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Torres", "Leonardo", ""], ["Cavalcante", "Tamer", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1308.6774", "submitter": "Peter Richtarik", "authors": "Rachael Tappenden and Peter Richtarik and Burak Buke", "title": "Separable Approximations and Decomposition Methods for the Augmented\n  Lagrangian", "comments": "28 pages, 6 algorithms, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study decomposition methods based on separable\napproximations for minimizing the augmented Lagrangian. In particular, we study\nand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and\nRuszczy\\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of\nRicht\\'arik and Tak\\'a\\v{c}. We show that the two methods are equivalent for\nfeasibility problems up to the selection of a single step-size parameter.\nFurthermore, we prove an improved complexity bound for PCDM under strong\nconvexity, and show that this bound is at least $8(L'/\\bar{L})(\\omega-1)^2$\ntimes better than the best known bound for DQAM, where $\\omega$ is the degree\nof partial separability and $L'$ and $\\bar{L}$ are the maximum and average of\nthe block Lipschitz constants of the gradient of the quadratic penalty\nappearing in the augmented Lagrangian.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 15:39:32 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Tappenden", "Rachael", ""], ["Richtarik", "Peter", ""], ["Buke", "Burak", ""]]}, {"id": "1308.6797", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "Online Ranking: Discrete Choice, Spearman Correlation and Other Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $V$ of $n$ objects, an online ranking system outputs at each time\nstep a full ranking of the set, observes a feedback of some form and suffers a\nloss. We study the setting in which the (adversarial) feedback is an element in\n$V$, and the loss is the position (0th, 1st, 2nd...) of the item in the\noutputted ranking. More generally, we study a setting in which the feedback is\na subset $U$ of at most $k$ elements in $V$, and the loss is the sum of the\npositions of those elements.\n  We present an algorithm of expected regret $O(n^{3/2}\\sqrt{Tk})$ over a time\nhorizon of $T$ steps with respect to the best single ranking in hindsight. This\nimproves previous algorithms and analyses either by a factor of either\n$\\Omega(\\sqrt{k})$, a factor of $\\Omega(\\sqrt{\\log n})$ or by improving running\ntime from quadratic to $O(n\\log n)$ per round. We also prove a matching lower\nbound. Our techniques also imply an improved regret bound for online rank\naggregation over the Spearman correlation measure, and to other more complex\nranking loss functions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 17:03:16 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 02:18:18 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2013 02:52:39 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2013 22:05:33 GMT"}, {"version": "v5", "created": "Mon, 14 Oct 2013 14:44:41 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Ailon", "Nir", ""]]}]