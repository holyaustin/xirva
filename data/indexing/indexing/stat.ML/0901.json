[{"id": "0901.0026", "submitter": "Alessandro Rinaldo", "authors": "Stephen E. Fienberg, Alessandro Rinaldo, Yi Zhou", "title": "On the Geometry of Discrete Exponential Families with Application to\n  Exponential Random Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of interest in statistical models for analyzing\nnetwork data, and considerable interest in the class of exponential random\ngraph (ERG) models, especially in connection with difficulties in computing\nmaximum likelihood estimates. The issues associated with these difficulties\nrelate to the broader structure of discrete exponential families. This paper\nre-examines the issues in two parts. First we consider the closure of\n$k$-dimensional exponential families of distribution with discrete base measure\nand polyhedral convex support $\\mathrm{P}$. We show that the normal fan of\n$\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving\nthe statistical and geometric properties of the corresponding extended\nexponential families. We discuss its relevance to maximum likelihood\nestimation, both from a theoretical and computational standpoint. Second, we\napply our results to the analysis of ERG models. In particular, by means of a\ndetailed example, we provide some characterization of the properties of ERG\nmodels, and, in particular, of certain behaviors of ERG models known as\ndegeneracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2008 23:09:18 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Fienberg", "Stephen E.", ""], ["Rinaldo", "Alessandro", ""], ["Zhou", "Yi", ""]]}, {"id": "0901.0135", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Wenjie Fu, Le Song", "title": "A state-space mixed membership blockmodel for dynamic network tomography", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS311 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 535-566", "doi": "10.1214/09-AOAS311", "report-no": "IMS-AOAS-AOAS311", "categories": "stat.ML q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic social or biological environment, the interactions between the\nactors can undergo large and systematic changes. In this paper we propose a\nmodel-based approach to analyze what we will refer to as the dynamic tomography\nof such time-evolving networks. Our approach offers an intuitive but powerful\ntool to infer the semantic underpinnings of each actor, such as its social\nroles or biological functions, underlying the observed network topologies. Our\nmodel builds on earlier work on a mixed membership stochastic blockmodel for\nstatic networks, and the state-space model for tracking object trajectory. It\novercomes a major limitation of many current network inference techniques,\nwhich assume that each actor plays a unique and invariant role that accounts\nfor all its interactions with other actors; instead, our method models the role\nof each actor as a time-evolving mixed membership vector that allows actors to\nbehave differently over time and carry out different roles/functions when\ninteracting with different peers, which is closer to reality. We present an\nefficient algorithm for approximate inference and learning using our model; and\nwe applied our model to analyze a social network between monks (i.e., the\nSampson's network), a dynamic email communication network between the Enron\nemployees, and a rewiring gene interaction network of fruit fly collected\nduring its full life cycle. In all cases, our model reveals interesting\npatterns of the dynamic roles of the actors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2008 21:27:33 GMT"}, {"version": "v2", "created": "Mon, 8 Nov 2010 09:05:23 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Fu", "Wenjie", ""], ["Song", "Le", ""]]}, {"id": "0901.0138", "submitter": "Amr Ahmed", "authors": "Amr Ahmed, Le Song and Eric P. Xing", "title": "Time-Varying Networks: Recovering Temporally Rewiring Genetic Networks\n  During the Life Cycle of Drosophila melanogaster", "comments": "Correcting some figure formatting errors", "journal-ref": null, "doi": null, "report-no": "Amr Ahmed, Le Song, Eric Xing (2008). Time-Varying Networks:\n  Reconstructing Temporally Rewiring Genetic Interactions During the Life Cycle\n  of Drosophila melanogaster. CMU-MLD Technical Report CMU-ML-08-118", "categories": "q-bio.MN q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the dynamic nature of biological systems, biological networks\nunderlying temporal process such as the development of {\\it Drosophila\nmelanogaster} can exhibit significant topological changes to facilitate dynamic\nregulatory functions. Thus it is essential to develop methodologies that\ncapture the temporal evolution of networks, which make it possible to study the\ndriving forces underlying dynamic rewiring of gene regulation circuity, and to\npredict future network structures. Using a new machine learning method called\nTesla, which builds on a novel temporal logistic regression technique, we\nreport the first successful genome-wide reverse-engineering of the latent\nsequence of temporally rewiring gene networks over more than 4000 genes during\nthe life cycle of \\textit{Drosophila melanogaster}, given longitudinal gene\nexpression measurements and even when a single snapshot of such measurement\nresulted from each (time-specific) network is available. Our methods offer the\nfirst glimpse of time-specific snapshots and temporal evolution patterns of\ngene networks in a living organism during its full developmental course. The\nrecovered networks with this unprecedented resolution chart the onset and\nduration of many gene interactions which are missed by typical static network\nanalysis, and are suggestive of a wide array of other temporal behaviors of the\ngene network over time not noticed before.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2008 20:42:52 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2009 23:28:12 GMT"}], "update_date": "2009-01-07", "authors_parsed": [["Ahmed", "Amr", ""], ["Song", "Le", ""], ["Xing", "Eric P.", ""]]}, {"id": "0901.0356", "submitter": "Mark Reid", "authors": "Mark D. Reid and Robert C. Williamson", "title": "Information, Divergence and Risk for Binary Experiments", "comments": "89 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify f-divergences, Bregman divergences, surrogate loss bounds (regret\nbounds), proper scoring rules, matching losses, cost curves, ROC-curves and\ninformation. We do this by systematically studying integral and variational\nrepresentations of these objects and in so doing identify their primitives\nwhich all are related to cost-sensitive binary classification. As well as\nclarifying relationships between generative and discriminative views of\nlearning, the new machinery leads to tight and more general surrogate loss\nbounds and generalised Pinsker inequalities relating f-divergences to\nvariational divergence. The new viewpoint illuminates existing algorithms: it\nprovides a new derivation of Support Vector Machines in terms of divergences\nand relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also\nsuggests new techniques for estimating f-divergences.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2009 06:37:01 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["Reid", "Mark D.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "0901.1365", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, Katrina Ligett, Larry Wasserman", "title": "Differential Privacy with Compression", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies formal utility and privacy guarantees for a simple\nmultiplicative database transformation, where the data are compressed by a\nrandom linear or affine transformation, reducing the number of data records\nsubstantially, while preserving the number of original input variables. We\nprovide an analysis framework inspired by a recent concept known as\ndifferential privacy (Dwork 06). Our goal is to show that, despite the general\ndifficulty of achieving the differential privacy guarantee, it is possible to\npublish synthetic data that are useful for a number of common statistical\nlearning applications. This includes high dimensional sparse regression (Zhou\net al. 07), principal component analysis (PCA), and other statistical measures\n(Liu et al. 06) based on the covariance of the initial data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2009 12:12:31 GMT"}], "update_date": "2009-01-13", "authors_parsed": [["Zhou", "Shuheng", ""], ["Ligett", "Katrina", ""], ["Wasserman", "Larry", ""]]}, {"id": "0901.1504", "submitter": "Bharath Sriperumbudur", "authors": "Bharath Sriperumbudur, David Torres and Gert Lanckriet", "title": "A D.C. Programming Approach to the Sparse Generalized Eigenvalue Problem", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the sparse eigenvalue problem wherein the goal is\nto obtain a sparse solution to the generalized eigenvalue problem. We achieve\nthis by constraining the cardinality of the solution to the generalized\neigenvalue problem and obtain sparse principal component analysis (PCA), sparse\ncanonical correlation analysis (CCA) and sparse Fisher discriminant analysis\n(FDA) as special cases. Unlike the $\\ell_1$-norm approximation to the\ncardinality constraint, which previous methods have used in the context of\nsparse PCA, we propose a tighter approximation that is related to the negative\nlog-likelihood of a Student's t-distribution. The problem is then framed as a\nd.c. (difference of convex functions) program and is solved as a sequence of\nconvex programs by invoking the majorization-minimization method. The resulting\nalgorithm is proved to exhibit \\emph{global convergence} behavior, i.e., for\nany random initialization, the sequence (subsequence) of iterates generated by\nthe algorithm converges to a stationary point of the d.c. program. The\nperformance of the algorithm is empirically demonstrated on both sparse PCA\n(finding few relevant genes that explain as much variance as possible in a\nhigh-dimensional gene dataset) and sparse CCA (cross-language document\nretrieval and vocabulary selection for music retrieval) applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2009 05:02:18 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2009 03:01:23 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Sriperumbudur", "Bharath", ""], ["Torres", "David", ""], ["Lanckriet", "Gert", ""]]}, {"id": "0901.2044", "submitter": "Florentina Bunea", "authors": "Florentina Bunea, Alexandre B. Tsybakov, Marten H. Wegkamp, Adrian\n  Barbu", "title": "SPADES and mixture models", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS790 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 4, 2525-2558", "doi": "10.1214/09-AOS790", "report-no": "IMS-AOS-AOS790", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sparse density estimation via $\\ell_1$ penalization\n(SPADES). We focus on estimation in high-dimensional mixture models and\nnonparametric adaptive density estimation. We show, respectively, that SPADES\ncan recover, with high probability, the unknown components of a mixture of\nprobability densities and that it yields minimax adaptive density estimates.\nThese results are based on a general sparsity oracle inequality that the SPADES\nestimates satisfy. We offer a data driven method for the choice of the tuning\nparameter used in the construction of SPADES. The method uses the generalized\nbisection method first introduced in \\citebb09. The suggested procedure\nbypasses the need for a grid search and offers substantial computational\nsavings. We complement our theoretical results with a simulation study that\nemploys this method for approximations of one and two-dimensional densities\nwith mixtures. The numerical results strongly support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2009 15:34:13 GMT"}, {"version": "v2", "created": "Thu, 21 Oct 2010 13:30:14 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["Bunea", "Florentina", ""], ["Tsybakov", "Alexandre B.", ""], ["Wegkamp", "Marten H.", ""], ["Barbu", "Adrian", ""]]}, {"id": "0901.2234", "submitter": "Nicole Kraemer", "authors": "Stefan Haufe, Guido Nolte, Klaus-Robert Mueller, Nicole Kraemer", "title": "Sparse Causal Discovery in Multivariate Time Series", "comments": "to appear in Journal of Machine Learning Research, Proceedings of the\n  NIPS'08 workshop on Causality", "journal-ref": "JMLR Workshop and Conference Proceedings 6: Causality: Objectives\n  and Assessment (NIPS 2008), 97 - 106", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to estimate causal interactions in multivariate time series.\nUsing vector autoregressive (VAR) models, these can be defined based on\nnon-vanishing coefficients belonging to respective time-lagged instances. As in\nmost cases a parsimonious causality structure is assumed, a promising approach\nto causal discovery consists in fitting VAR models with an additional\nsparsity-promoting regularization. Along this line we here propose that\nsparsity should be enforced for the subgroups of coefficients that belong to\neach pair of time series, as the absence of a causal relation requires the\ncoefficients for all time-lags to become jointly zero. Such behavior can be\nachieved by means of l1-l2-norm regularized regression, for which an efficient\nactive set solver has been proposed recently. Our method is shown to outperform\nstandard methods in recovering simulated causality graphs. The results are on\npar with a second novel approach which uses multiple statistical testing.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 11:21:33 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Haufe", "Stefan", ""], ["Nolte", "Guido", ""], ["Mueller", "Klaus-Robert", ""], ["Kraemer", "Nicole", ""]]}, {"id": "0901.2321", "submitter": "{\\L}ukasz D{\\ke}bowski", "authors": "{\\L}ukasz D\\k{e}bowski", "title": "The Redundancy of a Computable Code on a Noncomputable Distribution", "comments": "5 pages; an intro to a longer article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new definitions of universal and superuniversal computable\ncodes, which are based on a code's ability to approximate Kolmogorov complexity\nwithin the prescribed margin for all individual sequences from a given set.\nSuch sets of sequences may be singled out almost surely with respect to certain\nprobability measures. Consider a measure parameterized with a real parameter\nand put an arbitrary prior on the parameter. The Bayesian measure is the\nexpectation of the parameterized measure with respect to the prior. It appears\nthat a modified Shannon-Fano code for any computable Bayesian measure, which we\ncall the Bayesian code, is superuniversal on a set of parameterized\nmeasure-almost all sequences for prior-almost every parameter. According to\nthis result, in the typical setting of mathematical statistics no computable\ncode enjoys redundancy which is ultimately much less than that of the Bayesian\ncode. Thus we introduce another characteristic of computable codes: The\ncatch-up time is the length of data for which the code length drops below the\nKolmogorov complexity plus the prescribed margin. Some codes may have smaller\ncatch-up times than Bayesian codes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 17:37:23 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2009 09:23:10 GMT"}], "update_date": "2009-04-10", "authors_parsed": [["D\u0119bowski", "\u0141ukasz", ""]]}, {"id": "0901.2730", "submitter": "Jun Zhu", "authors": "Jun Zhu, and Eric P. Xing", "title": "Maximum Entropy Discrimination Markov Networks", "comments": "39 pages", "journal-ref": "Journal of Machine Learning Research, 10(Nov):2531-2569, 2009", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel and general framework called {\\it Maximum\nEntropy Discrimination Markov Networks} (MaxEnDNet), which integrates the\nmax-margin structured learning and Bayesian-style estimation and combines and\nextends their merits. Major innovations of this model include: 1) It\ngeneralizes the extant Markov network prediction rule based on a point\nestimator of weights to a Bayesian-style estimator that integrates over a\nlearned distribution of the weights. 2) It extends the conventional max-entropy\ndiscrimination learning of classification rule to a new structural max-entropy\ndiscrimination paradigm of learning the distribution of Markov networks. 3) It\nsubsumes the well-known and powerful Maximum Margin Markov network (M$^3$N) as\na special case, and leads to a model similar to an $L_1$-regularized M$^3$N\nthat is simultaneously primal and dual sparse, or other types of Markov network\nby plugging in different prior distributions of the weights. 4) It offers a\nsimple inference algorithm that combines existing variational inference and\nconvex-optimization based M$^3$N solvers as subroutines. 5) It offers a\nPAC-Bayesian style generalization bound. This work represents the first\nsuccessful attempt to combine Bayesian-style learning (based on generative\nmodels) with structured maximum margin learning (based on a discriminative\nmodel), and outperforms a wide array of competing methods for structured\ninput/output learning on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2009 20:07:17 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "0901.2735", "submitter": "Robert Grossman", "authors": "Robert L Grossman and Richard G Larson", "title": "State Space Realization Theorems For Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.RA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider formal series associated with events, profiles\nderived from events, and statistical models that make predictions about events.\nWe prove theorems about realizations for these formal series using the language\nand tools of Hopf algebras.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2009 20:54:05 GMT"}], "update_date": "2009-01-20", "authors_parsed": [["Grossman", "Robert L", ""], ["Larson", "Richard G", ""]]}, {"id": "0901.2962", "submitter": "Tong Zhang", "authors": "Junzhou Huang, Tong Zhang", "title": "The Benefit of Group Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a theory for group Lasso using a concept called strong\ngroup sparsity. Our result shows that group Lasso is superior to standard Lasso\nfor strongly group-sparse signals. This provides a convincing theoretical\njustification for using group sparse regularization when the underlying group\nstructure is consistent with the data. Moreover, the theory predicts some\nlimitations of the group Lasso formulation that are confirmed by simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2009 02:26:44 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2009 13:33:45 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Huang", "Junzhou", ""], ["Zhang", "Tong", ""]]}, {"id": "0901.3150", "submitter": "Sewoong Oh", "authors": "Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh", "title": "Matrix Completion from a Few Entries", "comments": "30 pages, 1 figure, journal version (v1, v2: Conference version ISIT\n  2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let M be a random (alpha n) x n matrix of rank r<<n, and assume that a\nuniformly random subset E of its entries is observed. We describe an efficient\nalgorithm that reconstructs M from |E| = O(rn) observed entries with relative\nroot mean square error RMSE <= C(rn/|E|)^0.5 . Further, if r=O(1), M can be\nreconstructed exactly from |E| = O(n log(n)) entries. These results apply\nbeyond random matrices to general low-rank incoherent matrices.\n  This settles (in the case of bounded rank) a question left open by Candes and\nRecht and improves over the guarantees for their reconstruction algorithm. The\ncomplexity of our algorithm is O(|E|r log(n)), which opens the way to its use\nfor massive data sets. In the process of proving these statements, we obtain a\ngeneralization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek\non the spectrum of sparse random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2009 21:32:57 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2009 07:00:15 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2009 03:27:35 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2009 09:26:46 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "0901.3202", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Model-Consistent Sparse Estimation through the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 08:05:19 GMT"}], "update_date": "2009-01-22", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0901.4137", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Practical Robust Estimators for the Imprecise Dirichlet Model", "comments": "22 pages, 2 figures", "journal-ref": "International Journal of Approximate Reasoning, 50:2 (2009) pages\n  231-242", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walley's Imprecise Dirichlet Model (IDM) for categorical i.i.d. data extends\nthe classical Dirichlet model to a set of priors. It overcomes several\nfundamental problems which other approaches to uncertainty suffer from. Yet, to\nbe useful in practice, one needs efficient ways for computing the\nimprecise=robust sets or intervals. The main objective of this work is to\nderive exact, conservative, and approximate, robust and credible interval\nestimates under the IDM for a large class of statistical estimators, including\nthe entropy and mutual information.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 23:05:06 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}]