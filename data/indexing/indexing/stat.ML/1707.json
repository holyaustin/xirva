[{"id": "1707.00010", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna\n  P. Gummadi, Adrian Weller", "title": "From Parity to Preference-based Notions of Fairness in Classification", "comments": "To appear in Proceedings of the 31st Conference on Neural Information\n  Processing Systems (NIPS 2017). Code available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of automated, data-driven decision making in an ever expanding\nrange of applications has raised concerns about its potential unfairness\ntowards certain social groups. In this context, a number of recent studies have\nfocused on defining, detecting, and removing unfairness from data-driven\ndecision systems. However, the existing notions of fairness, based on parity\n(equality) in treatment or outcomes for different social groups, tend to be\nquite stringent, limiting the overall decision making accuracy. In this paper,\nwe draw inspiration from the fair-division and envy-freeness literature in\neconomics and game theory and propose preference-based notions of fairness --\ngiven the choice between various sets of decision treatments or outcomes, any\ngroup of users would collectively prefer its treatment or outcomes, regardless\nof the (dis)parity as compared to the other groups. Then, we introduce\ntractable proxies to design margin-based classifiers that satisfy these\npreference-based notions of fairness. Finally, we experiment with a variety of\nsynthetic and real-world datasets and show that preference-based fairness\nallows for greater decision accuracy than parity-based fairness.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 18:01:49 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 19:00:49 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""]]}, {"id": "1707.00044", "submitter": "Yahav Bechavod", "authors": "Yahav Bechavod and Katrina Ligett", "title": "Penalizing Unfairness in Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for mitigating unfairness in learned classifiers.\nIn particular, we focus on binary classification tasks over individuals from\ntwo populations, where, as our criterion for fairness, we wish to achieve\nsimilar false positive rates in both populations, and similar false negative\nrates in both populations. As a proof of concept, we implement our approach and\nempirically evaluate its ability to achieve both fairness and accuracy, using\ndatasets from the fields of criminal risk assessment, credit, lending, and\ncollege admissions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 20:59:44 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 22:21:52 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 17:58:40 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Bechavod", "Yahav", ""], ["Ligett", "Katrina", ""]]}, {"id": "1707.00046", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova and Max G'Sell", "title": "Fairer and more accurate, but for whom?", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex statistical machine learning models are increasingly being used or\nconsidered for use in high-stakes decision-making pipelines in domains such as\nfinancial services, health care, criminal justice and human services. These\nmodels are often investigated as possible improvements over more classical\ntools such as regression models or human judgement. While the modeling approach\nmay be new, the practice of using some form of risk assessment to inform\ndecisions is not. When determining whether a new model should be adopted, it is\ntherefore essential to be able to compare the proposed model to the existing\napproach across a range of task-relevant accuracy and fairness metrics. Looking\nat overall performance metrics, however, may be misleading. Even when two\nmodels have comparable overall performance, they may nevertheless disagree in\ntheir classifications on a considerable fraction of cases. In this paper we\nintroduce a model comparison framework for automatically identifying subgroups\nin which the differences between models are most pronounced. Our primary focus\nis on identifying subgroups where the models differ in terms of\nfairness-related quantities such as racial or gender disparities. We present\nexperimental results from a recidivism prediction task and a hypothetical\nlending example.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 21:07:09 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Chouldechova", "Alexandra", ""], ["G'Sell", "Max", ""]]}, {"id": "1707.00081", "submitter": "Alexander Wong", "authors": "A. H. Karimi, M. J. Shafiee, A. Ghodsi, and A. Wong", "title": "Synthesizing Deep Neural Network Architectures using Biological Synaptic\n  Strength Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an exploratory study on synthesizing deep neural\nnetworks using biological synaptic strength distributions, and the potential\ninfluence of different distributions on modelling performance particularly for\nthe scenario associated with small data sets. Surprisingly, a CNN with\nconvolutional layer synaptic strengths drawn from biologically-inspired\ndistributions such as log-normal or correlated center-surround distributions\nperformed relatively well suggesting a possibility for designing deep neural\nnetwork architectures that do not require many data samples to learn, and can\nsidestep current training procedures while maintaining or boosting modelling\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 01:30:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Karimi", "A. H.", ""], ["Shafiee", "M. J.", ""], ["Ghodsi", "A.", ""], ["Wong", "A.", ""]]}, {"id": "1707.00102", "submitter": "Scott Powers", "authors": "Scott Powers, Junyang Qian, Kenneth Jung, Alejandro Schuler, Nigam H.\n  Shah, Trevor Hastie and Robert Tibshirani", "title": "Some methods for heterogeneous treatment effect estimation in\n  high-dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When devising a course of treatment for a patient, doctors often have little\nquantitative evidence on which to base their decisions, beyond their medical\neducation and published clinical trials. Stanford Health Care alone has\nmillions of electronic medical records (EMRs) that are only just recently being\nleveraged to inform better treatment recommendations. These data present a\nunique challenge because they are high-dimensional and observational. Our goal\nis to make personalized treatment recommendations based on the outcomes for\npast patients similar to a new patient. We propose and analyze three methods\nfor estimating heterogeneous treatment effects using observational data. Our\nmethods perform well in simulations using a wide variety of treatment effect\nfunctions, and we present results of applying the two most promising methods to\ndata from The SPRINT Data Analysis Challenge, from a large randomized trial of\na treatment for high blood pressure.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 06:41:00 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Powers", "Scott", ""], ["Qian", "Junyang", ""], ["Jung", "Kenneth", ""], ["Schuler", "Alejandro", ""], ["Shah", "Nigam H.", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1707.00117", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Lifeng Hua, Lei Li, Hang Su, Tian Wang, Ning Chen, Bo Zhang", "title": "SAM: Semantic Attribute Modulation for Language Modeling and Style\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Semantic Attribute Modulation (SAM) for language\nmodeling and style variation. The semantic attribute modulation includes\nvarious document attributes, such as titles, authors, and document categories.\nWe consider two types of attributes, (title attributes and category\nattributes), and a flexible attribute selection scheme by automatically scoring\nthem via an attribute attention mechanism. The semantic attributes are embedded\ninto the hidden semantic space as the generation inputs. With the attributes\nproperly harnessed, our proposed SAM can generate interpretable texts with\nregard to the input attributes. Qualitative analysis, including word semantic\nanalysis and attention values, shows the interpretability of SAM. On several\ntypical text datasets, we empirically demonstrate the superiority of the\nSemantic Attribute Modulated language model with different combinations of\ndocument attributes. Moreover, we present a style variation for the lyric\ngeneration using SAM, which shows a strong connection between the style\nvariation and the semantic attributes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:00:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:59:04 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 03:53:00 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Hu", "Wenbo", ""], ["Hua", "Lifeng", ""], ["Li", "Lei", ""], ["Su", "Hang", ""], ["Wang", "Tian", ""], ["Chen", "Ning", ""], ["Zhang", "Bo", ""]]}, {"id": "1707.00192", "submitter": "Yixin Fang", "authors": "Yixin Fang, Jinfeng Xu, Lei Yang", "title": "On Scalable Inference with Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications involving large dataset or online updating, stochastic\ngradient descent (SGD) provides a scalable way to compute parameter estimates\nand has gained increasing popularity due to its numerical convenience and\nmemory efficiency. While the asymptotic properties of SGD-based estimators have\nbeen established decades ago, statistical inference such as interval estimation\nremains much unexplored. The traditional resampling method such as the\nbootstrap is not computationally feasible since it requires to repeatedly draw\nindependent samples from the entire dataset. The plug-in method is not\napplicable when there are no explicit formulas for the covariance matrix of the\nestimator. In this paper, we propose a scalable inferential procedure for\nstochastic gradient descent, which, upon the arrival of each observation,\nupdates the SGD estimate as well as a large number of randomly perturbed SGD\nestimates. The proposed method is easy to implement in practice. We establish\nits theoretical properties for a general class of models that includes\ngeneralized linear models and quantile regression models as special cases. The\nfinite-sample performance and numerical utility is evaluated by simulation\nstudies and two real data applications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:54:34 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Fang", "Yixin", ""], ["Xu", "Jinfeng", ""], ["Yang", "Lei", ""]]}, {"id": "1707.00206", "submitter": "Zhiting Hu", "authors": "Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying Huang, Eric P.\n  Xing", "title": "Efficient Correlated Topic Modeling with Topic Embedding", "comments": "KDD 2017 oral. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated topic modeling has been limited to small model and problem sizes\ndue to their high computational cost and poor scaling. In this paper, we\npropose a new model which learns compact topic embeddings and captures topic\ncorrelations through the closeness between the topic vectors. Our method\nenables efficient inference in the low-dimensional embedding space, reducing\nprevious cubic or quadratic time complexity to linear w.r.t the topic size. We\nfurther speedup variational inference with a fast sampler to exploit sparsity\nof topic occurrence. Extensive experiments show that our approach is capable of\nhandling model and data scales which are several orders of magnitude larger\nthan existing correlation results, without sacrificing modeling quality by\nproviding competitive or superior performance in document classification and\nretrieval.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 21:10:15 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["He", "Junxian", ""], ["Hu", "Zhiting", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Huang", "Ying", ""], ["Xing", "Eric P.", ""]]}, {"id": "1707.00260", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, John Paisley, Qiuyang Liu", "title": "Location Dependent Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet processes (DP) are widely applied in Bayesian nonparametric\nmodeling. However, in their basic form they do not directly integrate\ndependency information among data arising from space and time. In this paper,\nwe propose location dependent Dirichlet processes (LDDP) which incorporate\nnonparametric Gaussian processes in the DP modeling framework to model such\ndependencies. We develop the LDDP in the context of mixture modeling, and\ndevelop a mean field variational inference algorithm for this mixture model.\nThe effectiveness of the proposed modeling framework is shown on an image\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 08:33:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sun", "Shiliang", ""], ["Paisley", "John", ""], ["Liu", "Qiuyang", ""]]}, {"id": "1707.00306", "submitter": "Michael Fop", "authors": "Michael Fop and Thomas Brendan Murphy", "title": "Variable Selection Methods for Model-based Clustering", "comments": null, "journal-ref": "Statistics Surveys, 12 (2018) 1-48", "doi": "10.1214/18-SS119", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 15:29:13 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 07:52:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fop", "Michael", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1707.00309", "submitter": "R Devon Hjelm", "authors": "Karan Grewal and R Devon Hjelm and Yoshua Bengio", "title": "Variance Regularizing Adversarial Learning", "comments": "Method is out of date and some results are incorrect", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for training adversarial models by replacing\nthe discriminator score with a bi-modal Gaussian distribution over the\nreal/fake indicator variables. In order to do this, we train the Gaussian\nclassifier to match the target bi-modal distribution implicitly through\nmeta-adversarial training. We hypothesize that this approach ensures a non-zero\ngradient to the generator, even in the limit of a perfect classifier. We test\nour method against standard benchmark image datasets as well as show the\nclassifier output distribution is smooth and has overlap between the real and\nfake modes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 15:36:07 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 20:21:33 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Grewal", "Karan", ""], ["Hjelm", "R Devon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.00351", "submitter": "Minyar Sassi", "authors": "Rania Mkhinini Gahar, Olfa Arfaoui, Minyar Sassi Hidri, Nejib Ben-Hadj\n  Alouane", "title": "Dimensionality reduction with missing values imputation", "comments": "6 pages, 2 figures, The first Computer science University of Tunis El\n  Manar, PhD Symposium (CUPS'17), Tunisia, May 22-25, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a new statical approach for high-dimensionality\nreduction of heterogenous data that limits the curse of dimensionality and\ndeals with missing values. To handle these latter, we propose to use the Random\nForest imputation's method. The main purpose here is to extract useful\ninformation and so reducing the search space to facilitate the data exploration\nprocess. Several illustrative numeric examples, using data coming from publicly\navailable machine learning repositories are also included. The experimental\ncomponent of the study shows the efficiency of the proposed analytical\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 20:47:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Gahar", "Rania Mkhinini", ""], ["Arfaoui", "Olfa", ""], ["Hidri", "Minyar Sassi", ""], ["Alouane", "Nejib Ben-Hadj", ""]]}, {"id": "1707.00372", "submitter": "Jong Chul Ye", "authors": "Jong Chul Ye, Yoseob Han, Eunju Cha", "title": "Deep Convolutional Framelets: A General Deep Learning Framework for\n  Inverse Problems", "comments": "This will appear in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches with various network architectures have\nachieved significant performance improvement over existing iterative\nreconstruction methods in various imaging problems. However, it is still\nunclear why these deep learning architectures work for specific inverse\nproblems. To address these issues, here we show that the long-searched-for\nmissing link is the convolution framelets for representing a signal by\nconvolving local and non-local bases. The convolution framelets was originally\ndeveloped to generalize the theory of low-rank Hankel matrix approaches for\ninverse problems, and this paper further extends the idea so that we can obtain\na deep neural network using multilayer convolution framelets with perfect\nreconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our\nanalysis also shows that the popular deep network components such as residual\nblock, redundant filter channels, and concatenated ReLU (CReLU) do indeed help\nto achieve the PR, while the pooling and unpooling layers should be augmented\nwith high-pass branches to meet the PR condition. Moreover, by changing the\nnumber of filter channels and bias, we can control the shrinkage behaviors of\nthe neural network. This discovery leads us to propose a novel theory for deep\nconvolutional framelets neural network. Using numerical experiments with\nvarious inverse problems, we demonstrated that our deep convolution framelets\nnetwork shows consistent improvement over existing deep architectures.This\ndiscovery suggests that the success of deep learning is not from a magical\npower of a black-box, but rather comes from the power of a novel signal\nrepresentation using non-local basis combined with data-driven local basis,\nwhich is indeed a natural extension of classical signal processing theory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 00:16:04 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 08:20:27 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 13:27:07 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 07:12:01 GMT"}, {"version": "v5", "created": "Thu, 25 Jan 2018 09:37:10 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Ye", "Jong Chul", ""], ["Han", "Yoseob", ""], ["Cha", "Eunju", ""]]}, {"id": "1707.00391", "submitter": "Suresh Venkatasubramanian", "authors": "Amanda Bower and Sarah N. Kitchen and Laura Niss and Martin J. Strauss\n  and Alexander Vargas and Suresh Venkatasubramanian", "title": "Fair Pipelines", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work facilitates ensuring fairness of machine learning in the real world\nby decoupling fairness considerations in compound decisions. In particular,\nthis work studies how fairness propagates through a compound decision-making\nprocesses, which we call a pipeline. Prior work in algorithmic fairness only\nfocuses on fairness with respect to one decision. However, many decision-making\nprocesses require more than one decision. For instance, hiring is at least a\ntwo stage model: deciding who to interview from the applicant pool and then\ndeciding who to hire from the interview pool. Perhaps surprisingly, we show\nthat the composition of fair components may not guarantee a fair pipeline under\na $(1+\\varepsilon)$-equal opportunity definition of fair. However, we identify\ncircumstances that do provide that guarantee. We also propose numerous\ndirections for future work on more general compound machine learning decisions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 03:53:29 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Bower", "Amanda", ""], ["Kitchen", "Sarah N.", ""], ["Niss", "Laura", ""], ["Strauss", "Martin J.", ""], ["Vargas", "Alexander", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1707.00415", "submitter": "Tao Qin Dr.", "authors": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, Tie-Yan Liu", "title": "Dual Supervised Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many supervised learning tasks are emerged in dual forms, e.g.,\nEnglish-to-French translation vs. French-to-English translation, speech\nrecognition vs. text to speech, and image classification vs. image generation.\nTwo dual tasks have intrinsic connections with each other due to the\nprobabilistic correlation between their models. This connection is, however,\nnot effectively utilized today, since people usually train the models of two\ndual tasks separately and independently. In this work, we propose training the\nmodels of two dual tasks simultaneously, and explicitly exploiting the\nprobabilistic correlation between them to regularize the training process. For\nease of reference, we call the proposed approach \\emph{dual supervised\nlearning}. We demonstrate that dual supervised learning can improve the\npractical performances of both tasks, for various applications including\nmachine translation, image processing, and sentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 06:19:31 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Xia", "Yingce", ""], ["Qin", "Tao", ""], ["Chen", "Wei", ""], ["Bian", "Jiang", ""], ["Yu", "Nenghai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1707.00424", "submitter": "Pratik Chaudhari", "authors": "Pratik Chaudhari, Carlo Baldassi, Riccardo Zecchina, Stefano Soatto,\n  Ameet Talwalkar, Adam Oberman", "title": "Parle: parallelizing stochastic gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm called Parle for parallel training of deep\nnetworks that converges 2-4x faster than a data-parallel implementation of SGD,\nwhile achieving significantly improved error rates that are nearly\nstate-of-the-art on several benchmarks including CIFAR-10 and CIFAR-100,\nwithout introducing any additional hyper-parameters. We exploit the phenomenon\nof flat minima that has been shown to lead to improved generalization error for\ndeep networks. Parle requires very infrequent communication with the parameter\nserver and instead performs more computation on each client, which makes it\nwell-suited to both single-machine, multi-GPU settings and distributed\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 07:14:56 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 04:22:49 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Chaudhari", "Pratik", ""], ["Baldassi", "Carlo", ""], ["Zecchina", "Riccardo", ""], ["Soatto", "Stefano", ""], ["Talwalkar", "Ameet", ""], ["Oberman", "Adam", ""]]}, {"id": "1707.00514", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger, Brita Roy, Carley Riley, Harlan M. Krumholz", "title": "People Mover's Distance: Class level geometry using fast pairwise data\n  adaptive transportation costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of defining a network graph on a large collection of\nclasses. Each class is comprised of a collection of data points, sampled in a\nnon i.i.d. way, from some unknown underlying distribution. The application we\nconsider in this paper is a large scale high dimensional survey of people\nliving in the US, and the question of how similar or different are the various\ncounties in which these people live. We use a co-clustering diffusion metric to\nlearn the underlying distribution of people, and build an approximate earth\nmover's distance algorithm using this data adaptive transportation cost.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 12:57:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Cloninger", "Alexander", ""], ["Roy", "Brita", ""], ["Riley", "Carley", ""], ["Krumholz", "Harlan M.", ""]]}, {"id": "1707.00524", "submitter": "Hayan Yin", "authors": "Haiyan Yin, Jianda Chen, Sinno Jialin Pan", "title": "Hashing over Predicted Future Frames for Informed Exploration of Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep reinforcement learning (RL) tasks, an efficient exploration mechanism\nshould be able to encourage an agent to take actions that lead to less frequent\nstates which may yield higher accumulative future return. However, both knowing\nabout the future and evaluating the frequentness of states are non-trivial\ntasks, especially for deep RL domains, where a state is represented by\nhigh-dimensional image frames. In this paper, we propose a novel informed\nexploration framework for deep RL, where we build the capability for an RL\nagent to predict over the future transitions and evaluate the frequentness for\nthe predicted future frames in a meaningful manner. To this end, we train a\ndeep prediction model to predict future frames given a state-action pair, and a\nconvolutional autoencoder model to hash over the seen frames. In addition, to\nutilize the counts derived from the seen frames to evaluate the frequentness\nfor the predicted frames, we tackle the challenge of matching the predicted\nfuture frames and their corresponding seen frames at the latent feature level.\nIn this way, we derive a reliable metric for evaluating the novelty of the\nfuture direction pointed by each action, and hence inform the agent to explore\nthe least frequent one.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:07:40 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 12:09:47 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Yin", "Haiyan", ""], ["Chen", "Jianda", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1707.00532", "submitter": "Muriel Lang", "authors": "Muriel Lang", "title": "Approximation of probability density functions on the Euclidean group\n  parametrized by dual quaternions", "comments": "Diploma thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception is fundamental to many robot application areas especially in\nservice robotics. Our aim is to perceive and model an unprepared kitchen\nscenario with many objects. We start with the perception of a single target\nobject. The modeling relies especially on fusing and merging of weak\ninformation from the sensors of the robot in order to localize objects. This\nrequires the representation of various probability distributions of pose in\n$S_3 \\times \\mathbb{R}^3$ as orientation and position have to be localized. In\nthis thesis I present a framework for probabilistic modeling of poses in $S_3\n\\times \\mathbb{R}^3$ that represents a large class of probability distributions\nand provides among others the operations of the fusion and the merge of\nestimates. Further it offers the propagation of uncertain information data. I\nwork out why we choose to represent the orientation part of a pose by a unit\nquaternion. The translation part is described either by a 3-dimensional vector\nor by a purely imaginary quaternion. This depends on whether we define the\nprobability density function or whether we want to represent a transformation\nwhich consists of a rotation and a translation by a dual quaternion. A basic\nprobability den- sity function over the poses is defined by a tangent point on\nthe hypersphere and a 6-dimensional Gaussian distribution. The hypersphere is\nembedded to the R4 which is representing a unit quaternions whereas the\nGaussian is defined over the product of the tangent space of the sphere and of\nthe space of translations. The projection of this Gaussian to the hypersphere\ninduces a distribution over poses in $S_3 \\times \\mathbb{R}^3$. The set of\nmixtures of projected Gaussians can approximate the probability density\nfunctions that arise in our application. Moreover it is closed under the\noperations introduced in this framework and allows for an efficient\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 11:48:46 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lang", "Muriel", ""]]}, {"id": "1707.00536", "submitter": "Peng Yang", "authors": "Peng Yang, Peilin Zhao, Xin Gao, Yong Liu", "title": "Robust Cost-Sensitive Learning for Recommendation with Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation is the task of improving customer experience through\npersonalized recommendation based on users' past feedback. In this paper, we\ninvestigate the most common scenario: the user-item (U-I) matrix of implicit\nfeedback. Even though many recommendation approaches are designed based on\nimplicit feedback, they attempt to project the U-I matrix into a low-rank\nlatent space, which is a strict restriction that rarely holds in practice. In\naddition, although misclassification costs from imbalanced classes are\nsignificantly different, few methods take the cost of classification error into\naccount. To address aforementioned issues, we propose a robust framework by\ndecomposing the U-I matrix into two components: (1) a low-rank matrix that\ncaptures the common preference, and (2) a sparse matrix that detects the\nuser-specific preference of individuals. A cost-sensitive learning model is\nembedded into the framework. Specifically, this model exploits different costs\nin the loss function for the observed and unobserved instances. We show that\nthe resulting non-smooth convex objective can be optimized efficiently by an\naccelerated projected gradient method with closed-form solutions. Morever, the\nproposed algorithm can be scaled up to large-sized datasets after a relaxation.\nThe theoretical result shows that even with a small fraction of 1's in the U-I\nmatrix $M\\in\\mathbb{R}^{n\\times m}$, the cost-sensitive error of the proposed\nmodel is upper bounded by $O(\\frac{\\alpha}{\\sqrt{mn}})$, where $\\alpha$ is a\nbias over imbalanced classes. Finally, empirical experiments are extensively\ncarried out to evaluate the effectiveness of our proposed algorithm.\nEncouraging experimental results show that our algorithm outperforms several\nstate-of-the-art algorithms on benchmark recommendation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:27:56 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 07:02:49 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Yang", "Peng", ""], ["Zhao", "Peilin", ""], ["Gao", "Xin", ""], ["Liu", "Yong", ""]]}, {"id": "1707.00558", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Bhargav Srinivasa Desikan", "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation", "comments": null, "journal-ref": "Journal of Machine Learning Research (JMLR), 18(190):1--5, 2018", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce \\texttt{pycobra}, a Python library devoted to ensemble learning\n(regression and classification) and visualisation. Its main assets are the\nimplementation of several ensemble learning algorithms, a flexible and generic\ninterface to compare and blend any existing machine learning algorithm\navailable in Python libraries (as long as a \\texttt{predict} method is given),\nand visualisation tools such as Voronoi tessellations. \\texttt{pycobra} is\nfully \\texttt{scikit-learn} compatible and is released under the MIT\nopen-source license. \\texttt{pycobra} can be downloaded from the Python Package\nIndex (PyPi) and Machine Learning Open Source Software (MLOSS). The current\nversion (along with Jupyter notebooks, extensive documentation, and continuous\nintegration tests) is available at\n\\href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra}\nand official documentation website is\n\\href{https://modal.lille.inria.fr/pycobra}{https://modal.lille.inria.fr/pycobra}.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:05:34 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 10:24:54 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 06:04:41 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Guedj", "Benjamin", ""], ["Desikan", "Bhargav Srinivasa", ""]]}, {"id": "1707.00577", "submitter": "Junhong Lin", "authors": "Junhong Lin, Lorenzo Rosasco", "title": "Generalization Properties of Doubly Stochastic Learning Algorithms", "comments": "24 pages. To appear in Journal of Complexity", "journal-ref": null, "doi": "10.1016/j.jco.2018.02.004", "report-no": null, "categories": "stat.ML cs.LG math.FA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly stochastic learning algorithms are scalable kernel methods that\nperform very well in practice. However, their generalization properties are not\nwell understood and their analysis is challenging since the corresponding\nlearning sequence may not be in the hypothesis space induced by the kernel. In\nthis paper, we provide an in-depth theoretical analysis for different variants\nof doubly stochastic learning algorithms within the setting of nonparametric\nregression in a reproducing kernel Hilbert space and considering the square\nloss. Particularly, we derive convergence results on the generalization error\nfor the studied algorithms either with or without an explicit penalty term. To\nthe best of our knowledge, the derived results for the unregularized variants\nare the first of this kind, while the results for the regularized variants\nimprove those in the literature. The novelties in our proof are a sample error\nbound that requires controlling the trace norm of a cumulative operator, and a\nrefined analysis of bounding initial error.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 14:46:05 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 21:22:12 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lin", "Junhong", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1707.00622", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang, Vaneet Aggarwal", "title": "Rank Determination for Low-Rank Data Completion", "comments": null, "journal-ref": "Journal of Machine Learning Research, 18(98), pp. 1-29, Sept 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fundamental conditions on the sampling patterns have been obtained\nfor finite completability of low-rank matrices or tensors given the\ncorresponding ranks. In this paper, we consider the scenario where the rank is\nnot given and we aim to approximate the unknown rank based on the location of\nsampled entries and some given completion. We consider a number of data models,\nincluding single-view matrix, multi-view matrix, CP tensor, tensor-train tensor\nand Tucker tensor. For each of these data models, we provide an upper bound on\nthe rank when an arbitrary low-rank completion is given. We characterize these\nbounds both deterministically, i.e., with probability one given that the\nsampling pattern satisfies certain combinatorial properties, and\nprobabilistically, i.e., with high probability given that the sampling\nprobability is above some threshold. Moreover, for both single-view matrix and\nCP tensor, we are able to show that the obtained upper bound is exactly equal\nto the unknown rank if the lowest-rank completion is given. Furthermore, we\nprovide numerical experiments for the case of single-view matrix, where we use\nnuclear norm minimization to find a low-rank completion of the sampled data and\nwe observe that in most of the cases the proposed upper bound on the rank is\nequal to the true rank.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 16:08:07 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1707.00703", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Automated Problem Identification: Regression vs Classification via\n  Evolutionary Deep Networks", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regression or classification? This is perhaps the most basic question faced\nwhen tackling a new supervised learning problem. We present an Evolutionary\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\nquestion type with high accuracy, along with a proposed deep architecture.\nTypically, a significant amount of human insight and preparation is required\nprior to executing machine learning algorithms. For example, when creating deep\nneural networks, the number of parameters must be selected in advance and\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\nof the data such as the use of a categorical cross entropy loss function.\nHumans are able to study a dataset and decide whether it represents a\nclassification or a regression problem, and consequently make decisions which\nwill be applied to the execution of the neural network. We propose the\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\nif a dataset represents a classification or a regression problem. We test API\non 16 different classification, regression and sentiment analysis datasets with\nup to 10,000 features and up to 17,000 unique target values. API achieves an\naverage accuracy of $96.3\\%$ in identifying the problem type without hardcoding\nany insights about the general characteristics of regression or classification\nproblems. For example, API successfully identifies classification problems even\nwith 1000 target values. Furthermore, the algorithm recommends which loss\nfunction to use and also recommends a neural network architecture. Our work is\ntherefore a step towards fully automated machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:00:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1707.00724", "submitter": "Daniel Brown", "authors": "Daniel S. Brown and Scott Niekum", "title": "Efficient Probabilistic Performance Bounds for Inverse Reinforcement\n  Learning", "comments": "In proceedings AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of reinforcement learning there has been recent progress towards\nsafety and high-confidence bounds on policy performance. However, to our\nknowledge, no practical methods exist for determining high-confidence policy\nperformance bounds in the inverse reinforcement learning setting---where the\ntrue reward function is unknown and only samples of expert behavior are given.\nWe propose a sampling method based on Bayesian inverse reinforcement learning\nthat uses demonstrations to determine practical high-confidence upper bounds on\nthe $\\alpha$-worst-case difference in expected return between any evaluation\npolicy and the optimal policy under the expert's unknown reward function. We\nevaluate our proposed bound on both a standard grid navigation task and a\nsimulated driving task and achieve tighter and more accurate bounds than a\nfeature count-based baseline. We also give examples of how our proposed bound\ncan be utilized to perform risk-aware policy selection and risk-aware policy\nimprovement. Because our proposed bound requires several orders of magnitude\nfewer demonstrations than existing high-confidence bounds, it is the first\npractical method that allows agents that learn from demonstration to express\nconfidence in the quality of their learned policy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:40:13 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 19:00:28 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 23:28:40 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 21:27:17 GMT"}, {"version": "v5", "created": "Sat, 23 Jun 2018 02:11:52 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Brown", "Daniel S.", ""], ["Niekum", "Scott", ""]]}, {"id": "1707.00727", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, William J. Welch, and Ruben H. Zamar", "title": "Regression Phalanxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomal et al. (2015) introduced the notion of \"phalanxes\" in the context of\nrare-class detection in two-class classification problems. A phalanx is a\nsubset of features that work well for classification tasks. In this paper, we\npropose a different class of phalanxes for application in regression settings.\nWe define a \"Regression Phalanx\" - a subset of features that work well together\nfor prediction. We propose a novel algorithm which automatically chooses\nRegression Phalanxes from high-dimensional data sets using hierarchical\nclustering and builds a prediction model for each phalanx for further\nensembling. Through extensive simulation studies and several real-life\napplications in various areas (including drug discovery, chemical analysis of\nspectra data, microarray analysis and climate projections) we show that an\nensemble of Regression Phalanxes improves prediction accuracy when combined\nwith effective prediction methods like Lasso or Random Forests.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:53:37 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Zhang", "Hongyang", ""], ["Welch", "William J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1707.00755", "submitter": "Tolga Tasdizen", "authors": "Tolga Tasdizen, Mehdi Sajjadi, Mehran Javanmardi, Nisha Ramesh", "title": "Appearance invariance in convolutional networks with neighborhood\n  similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neighborhood similarity layer (NSL) which induces appearance\ninvariance in a network when used in conjunction with convolutional layers. We\nare motivated by the observation that, even though convolutional networks have\nlow generalization error, their generalization capability does not extend to\nsamples which are not represented by the training data. For instance, while\nnovel appearances of learned concepts pose no problem for the human visual\nsystem, feedforward convolutional networks are generally not successful in such\nsituations. Motivated by the Gestalt principle of grouping with respect to\nsimilarity, the proposed NSL transforms its input feature map using the feature\nvectors at each pixel as a frame of reference, i.e. center of attention, for\nits surrounding neighborhood. This transformation is spatially varying, hence\nnot a convolution. It is differentiable; therefore, networks including the\nproposed layer can be trained in an end-to-end manner. We analyze the\ninvariance of NSL to significant changes in appearance that are not represented\nin the training data. We also demonstrate its advantages for digit recognition,\nsemantic labeling and cell detection problems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 20:53:56 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Tasdizen", "Tolga", ""], ["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Ramesh", "Nisha", ""]]}, {"id": "1707.00762", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Amartya Sanyal, Hugo Larochelle and Yoshua\n  Bengio", "title": "Multiscale sequence modeling with a learned dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of neural network sequence models. Instead of\npredicting one symbol at a time, our multi-scale model makes predictions over\nmultiple, potentially overlapping multi-symbol tokens. A variation of the\nbyte-pair encoding (BPE) compression algorithm is used to learn the dictionary\nof tokens that the model is trained with. When applied to language modelling,\nour model has the flexibility of character-level models while maintaining many\nof the performance benefits of word-level models. Our experiments show that\nthis model performs better than a regular LSTM on language modeling tasks,\nespecially for smaller models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 21:16:49 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 14:45:00 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Sanyal", "Amartya", ""], ["Larochelle", "Hugo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.00768", "submitter": "Alexander B. Jung", "authors": "Alexander B. Jung", "title": "Learning to Avoid Errors in GANs by Manipulating Input Spaces", "comments": "9 pages, 6 figures (plus 21 pages Appendix), code available at:\n  https://github.com/aleju/gan-error-avoidance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances, large scale visual artifacts are still a common\noccurrence in images generated by GANs. Previous work has focused on improving\nthe generator's capability to accurately imitate the data distribution\n$p_{data}$. In this paper, we instead explore methods that enable GANs to\nactively avoid errors by manipulating the input space. The core idea is to\napply small changes to each noise vector in order to shift them away from areas\nin the input space that tend to result in errors. We derive three different\narchitectures from that idea. The main one of these consists of a simple\nresidual module that leads to significantly less visual artifacts, while only\nslightly decreasing diversity. The module is trivial to add to existing GANs\nand costs almost zero computation and memory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 21:58:23 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Jung", "Alexander B.", ""]]}, {"id": "1707.00780", "submitter": "Chao Lan", "authors": "Chao Lan and Jun Huan", "title": "Discriminatory Transfer", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe standard transfer learning can improve prediction accuracies of\ntarget tasks at the cost of lowering their prediction fairness -- a phenomenon\nwe named discriminatory transfer. We examine prediction fairness of a standard\nhypothesis transfer algorithm and a standard multi-task learning algorithm, and\nshow they both suffer discriminatory transfer on the real-world Communities and\nCrime data set. The presented case study introduces an interaction between\nfairness and transfer learning, as an extension of existing fairness studies\nthat focus on single task learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:20:39 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 15:02:09 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 15:29:17 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 21:00:19 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lan", "Chao", ""], ["Huan", "Jun", ""]]}, {"id": "1707.00783", "submitter": "Jonathan Wells", "authors": "Jonathan R. Wells and Kai Ming Ting", "title": "A simple efficient density estimator that enables fast systematic search", "comments": "Corrected typos in the reference section and added an acknowledgement\n  on the first page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple and efficient density estimator that enables\nfast systematic search. To show its advantage over commonly used kernel density\nestimator, we apply it to outlying aspects mining. Outlying aspects mining\ndiscovers feature subsets (or subspaces) that describe how a query stand out\nfrom a given dataset. The task demands a systematic search of subspaces. We\nidentify that existing outlying aspects miners are restricted to datasets with\nsmall data size and dimensions because they employ kernel density estimator,\nwhich is computationally expensive, for subspace assessments. We show that a\nrecent outlying aspects miner can run orders of magnitude faster by simply\nreplacing its density estimator with the proposed density estimator, enabling\nit to deal with large datasets with thousands of dimensions that would\notherwise be impossible.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:42:46 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 05:23:11 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Wells", "Jonathan R.", ""], ["Ting", "Kai Ming", ""]]}, {"id": "1707.00797", "submitter": "Dilin Wang", "authors": "Qiang Liu, Dilin Wang", "title": "Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a number of new algorithms for learning deep energy models and\ndemonstrate their properties. We show that our SteinCD performs well in term of\ntest likelihood, while SteinGAN performs well in terms of generating realistic\nlooking images. Our results suggest promising directions for learning better\nmodels by combining GAN-style methods with traditional energy-based learning.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:05:52 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Liu", "Qiang", ""], ["Wang", "Dilin", ""]]}, {"id": "1707.00819", "submitter": "Sebastian Weichwald", "authors": "Paul K. Rubenstein, Sebastian Weichwald, Stephan Bongers, Joris M.\n  Mooij, Dominik Janzing, Moritz Grosse-Wentrup, Bernhard Sch\\\"olkopf", "title": "Causal Consistency of Structural Equation Models", "comments": "equal contribution between Rubenstein and Weichwald; accepted\n  manuscript", "journal-ref": "Proceedings of the Annual Conference on Uncertainty in Artificial\n  Intelligence, UAI 2017 ( http://auai.org/uai2017/proceedings/papers/11.pdf )", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems can be modelled at various levels of detail. Ideally, causal\nmodels of the same system should be consistent with one another in the sense\nthat they agree in their predictions of the effects of interventions. We\nformalise this notion of consistency in the case of Structural Equation Models\n(SEMs) by introducing exact transformations between SEMs. This provides a\ngeneral language to consider, for instance, the different levels of description\nin the following three scenarios: (a) models with large numbers of variables\nversus models in which the `irrelevant' or unobservable variables have been\nmarginalised out; (b) micro-level models versus macro-level models in which the\nmacro-variables are aggregate features of the micro-variables; (c) dynamical\ntime series models versus models of their stationary behaviour. Our analysis\nstresses the importance of well specified interventions in the causal modelling\nprocess and sheds light on the interpretation of cyclic SEMs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:05:31 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Weichwald", "Sebastian", ""], ["Bongers", "Stephan", ""], ["Mooij", "Joris M.", ""], ["Janzing", "Dominik", ""], ["Grosse-Wentrup", "Moritz", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1707.00833", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier,\n  Ulrike von Luxburg", "title": "Two-sample Hypothesis Testing for Inhomogeneous Random Graphs", "comments": "To appear in the Annals of Statistics. This 54-page version includes\n  the supplementary material (appendix to the main paper)", "journal-ref": "Ann. Statist. Volume 48, Number 4 (2020), 2208-2229", "doi": "10.1214/19-AOS1884", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of networks leads to a wide range of high dimensional inference\nproblems. In many practical applications, one needs to draw inference from one\nor few large sparse networks. The present paper studies hypothesis testing of\ngraphs in this high-dimensional regime, where the goal is to test between two\npopulations of inhomogeneous random graphs defined on the same set of $n$\nvertices. The size of each population $m$ is much smaller than $n$, and can\neven be a constant as small as 1. The critical question in this context is\nwhether the problem is solvable for small $m$.\n  We answer this question from a minimax testing perspective. Let $P,Q$ be the\npopulation adjacencies of two sparse inhomogeneous random graph models, and $d$\nbe a suitably defined distance function. Given a population of $m$ graphs from\neach model, we derive minimax separation rates for the problem of testing $P=Q$\nagainst $d(P,Q)>\\rho$. We observe that if $m$ is small, then the minimax\nseparation is too large for some popular choices of $d$, including total\nvariation distance between corresponding distributions. This implies that some\nmodels that are widely separated in $d$ cannot be distinguished for small $m$,\nand hence, the testing problem is generally not solvable in these cases.\n  We also show that if $m>1$, then the minimax separation is relatively small\nif $d$ is the Frobenius norm or operator norm distance between $P$ and $Q$. For\n$m=1$, only the latter distance provides small minimax separation. Thus, for\nthese distances, the problem is solvable for small $m$. We also present\nnear-optimal two-sample tests in both cases, where tests are adaptive with\nrespect to sparsity level of the graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 07:25:45 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 13:05:26 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 12:56:03 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 13:32:07 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Gutzeit", "Maurilio", ""], ["Carpentier", "Alexandra", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1707.01047", "submitter": "Vasilis Syrgkanis", "authors": "Robert Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis", "title": "Robust Optimization for Non-Convex Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust optimization problems, where the goal is to optimize in\nthe worst case over a class of objective functions. We develop a reduction from\nrobust improper optimization to Bayesian optimization: given an oracle that\nreturns $\\alpha$-approximate solutions for distributions over objectives, we\ncompute a distribution over solutions that is $\\alpha$-approximate in the worst\ncase. We show that de-randomizing this solution is NP-hard in general, but can\nbe done for a broad class of statistical learning tasks. We apply our results\nto robust neural network training and submodular optimization. We evaluate our\napproach experimentally on corrupted character classification, and robust\ninfluence maximization in networks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:56:42 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Chen", "Robert", ""], ["Lucier", "Brendan", ""], ["Singer", "Yaron", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1707.01069", "submitter": "Robert Bamler", "authors": "Robert Bamler and Stephan Mandt", "title": "Structured Black Box Variational Inference for Latent Time Series Models", "comments": "5 pages, 1 figure; presented at the ICML 2017 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous latent time series models are prevalent in Bayesian modeling;\nexamples include the Kalman filter, dynamic collaborative filtering, or dynamic\ntopic models. These models often benefit from structured, non mean field\nvariational approximations that capture correlations between time steps. Black\nbox variational inference with reparameterization gradients (BBVI) allows us to\nexplore a rich new class of Bayesian non-conjugate latent time series models;\nhowever, a naive application of BBVI to such structured variational models\nwould scale quadratically in the number of time steps. We describe a BBVI\nalgorithm analogous to the forward-backward algorithm which instead scales\nlinearly in time. It allows us to efficiently sample from the variational\ndistribution and estimate the gradients of the ELBO. Finally, we show results\non the recently proposed dynamic word embedding model, which was trained using\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 17:03:59 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Bamler", "Robert", ""], ["Mandt", "Stephan", ""]]}, {"id": "1707.01164", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan", "title": "Kernel Feature Selection via Conditional Covariance Minimization", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for feature selection that employs kernel-based measures\nof independence to find a subset of covariates that is maximally predictive of\nthe response. Building on past work in kernel dimension reduction, we show how\nto perform feature selection via a constrained optimization problem involving\nthe trace of the conditional covariance operator. We prove various consistency\nresults for this procedure, and also demonstrate that our method compares\nfavorably with other state-of-the-art algorithms on a variety of synthetic and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 22:00:58 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 19:34:49 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Chen", "Jianbo", ""], ["Stern", "Mitchell", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.01195", "submitter": "Thomas Miconi", "authors": "Thomas Miconi", "title": "The impossibility of \"fairness\": a generalized impossibility result for\n  decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various measures can be used to estimate bias or unfairness in a predictor.\nPrevious work has already established that some of these measures are\nincompatible with each other. Here we show that, when groups differ in\nprevalence of the predicted event, several intuitive, reasonable measures of\nfairness (probability of positive prediction given occurrence or\nnon-occurrence; probability of occurrence given prediction or non-prediction;\nand ratio of predictions over occurrences for each group) are all mutually\nexclusive: if one of them is equal among groups, the other two must differ. The\nonly exceptions are for perfect, or trivial (always-positive or\nalways-negative) predictors. As a consequence, any non-perfect, non-trivial\npredictor must necessarily be \"unfair\" under two out of three reasonable sets\nof criteria. This result readily generalizes to a wide range of well-known\nstatistical quantities (sensitivity, specificity, false positive rate,\nprecision, etc.), all of which can be divided into three mutually exclusive\ngroups. Importantly, The results applies to all predictors, whether algorithmic\nor human. We conclude with possible ways to handle this effect when assessing\nand designing prediction methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 02:02:42 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:31:06 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 21:18:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Miconi", "Thomas", ""]]}, {"id": "1707.01203", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao, Yanjun Han, Irena Fischer-Hwang and Tsachy Weissman", "title": "Estimating the Fundamental Limits is Easier than Achieving the\n  Fundamental Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show through case studies that it is easier to estimate the fundamental\nlimits of data processing than to construct explicit algorithms to achieve\nthose limits. Focusing on binary classification, data compression, and\nprediction under logarithmic loss, we show that in the finite space setting,\nwhen it is possible to construct an estimator of the limits with vanishing\nerror with $n$ samples, it may require at least $n\\ln n$ samples to construct\nan explicit algorithm to achieve the limits.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:24:01 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 20:22:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Jiao", "Jiantao", ""], ["Han", "Yanjun", ""], ["Fischer-Hwang", "Irena", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1707.01207", "submitter": "Dong Xia", "authors": "Dong Xia and Fan Zhou", "title": "The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The higher order singular value decomposition (HOSVD) of tensors is a\ngeneralization of matrix SVD. The perturbation analysis of HOSVD under random\nnoise is more delicate than its matrix counterpart. Recently, polynomial time\nalgorithms have been proposed where statistically optimal estimates of the\nsingular subspaces and the low rank tensors are attainable in the Euclidean\nnorm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and\nintroduce estimators of the singular subspaces with sharp deviation bounds in\nthe sup-norm. We also investigate a low rank tensor denoising estimator and\ndemonstrate its fast convergence rate with respect to the entry-wise errors.\nThe sup-norm perturbation bounds reveal unconventional phase transitions for\nstatistical learning applications such as the exact clustering in high\ndimensional Gaussian mixture model and the exact support recovery in sub-tensor\nlocalizations. In addition, the bounds established for HOSVD also elaborate the\none-sided sup-norm perturbation bounds for the singular subspaces of unbalanced\n(or fat) matrices.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:58:11 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 01:39:33 GMT"}, {"version": "v3", "created": "Sat, 21 Apr 2018 22:19:15 GMT"}, {"version": "v4", "created": "Thu, 3 May 2018 15:38:51 GMT"}, {"version": "v5", "created": "Tue, 1 Jan 2019 09:53:59 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Xia", "Dong", ""], ["Zhou", "Fan", ""]]}, {"id": "1707.01209", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part I: general framework", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing neural nets is an active research problem, given the large size\nof state-of-the-art nets for tasks such as object recognition, and the\ncomputational limits imposed by mobile devices. We give a general formulation\nof model compression as constrained optimization. This includes many types of\ncompression: quantization, low-rank decomposition, pruning, lossless\ncompression and others. Then, we give a general algorithm to optimize this\nnonconvex problem based on the augmented Lagrangian and alternating\noptimization. This results in a \"learning-compression\" algorithm, which\nalternates a learning step of the uncompressed model, independent of the\ncompression type, with a compression step of the model parameters, independent\nof the learning task. This simple, efficient algorithm is guaranteed to find\nthe best compressed model for the task in a local sense under standard\nassumptions.\n  We present separately in several companion papers the development of this\ngeneral framework into specific algorithms for model compression based on\nquantization, pruning and other variations, including experimental results on\ncompressing neural nets and other models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 04:36:26 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1707.01212", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy, Amit Dhurandhar, Guillermo Cecchi, and Charu\n  Aggarwal", "title": "Efficient Data Representation by Selecting Prototypes with Importance\n  Weights", "comments": "Accepted for publication in International Conference on Data Mining\n  (ICDM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototypical examples that best summarizes and compactly represents an\nunderlying complex data distribution communicate meaningful insights to humans\nin domains where simple explanations are hard to extract. In this paper we\npresent algorithms with strong theoretical guarantees to mine these data sets\nand select prototypes a.k.a. representatives that optimally describes them. Our\nwork notably generalizes the recent work by Kim et al. (2016) where in addition\nto selecting prototypes, we also associate non-negative weights which are\nindicative of their importance. This extension provides a single coherent\nframework under which both prototypes and criticisms (i.e. outliers) can be\nfound. Furthermore, our framework works for any symmetric positive definite\nkernel thus addressing one of the key open questions laid out in Kim et al.\n(2016). By establishing that our objective function enjoys a key property of\nthat of weak submodularity, we present a fast ProtoDash algorithm and also\nderive approximation guarantees for the same. We demonstrate the efficacy of\nour method on diverse domains such as retail, digit recognition (MNIST) and on\npublicly available 40 health questionnaires obtained from the Center for\nDisease Control (CDC) website maintained by the US Dept. of Health. We validate\nthe results quantitatively as well as qualitatively based on expert feedback\nand recently published scientific studies on public health, thus showcasing the\npower of our technique in providing actionability (for retail), utility (for\nMNIST) and insight (on CDC datasets) which arguably are the hallmarks of an\neffective data mining method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:17:10 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 15:12:08 GMT"}, {"version": "v3", "created": "Sat, 3 Feb 2018 10:10:45 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 05:35:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Dhurandhar", "Amit", ""], ["Cecchi", "Guillermo", ""], ["Aggarwal", "Charu", ""]]}, {"id": "1707.01217", "submitter": "Jian Shen", "authors": "Jian Shen, Yanru Qu, Weinan Zhang, Yong Yu", "title": "Wasserstein Distance Guided Representation Learning for Domain\n  Adaptation", "comments": "The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims at generalizing a high-performance learner on a target\ndomain via utilizing the knowledge distilled from a source domain which has a\ndifferent but related data distribution. One solution to domain adaptation is\nto learn domain invariant feature representations while the learned\nrepresentations should also be discriminative in prediction. To learn such\nrepresentations, domain adaptation frameworks usually include a domain\ninvariant representation learning approach to measure and reduce the domain\ndiscrepancy, as well as a discriminator for classification. Inspired by\nWasserstein GAN, in this paper we propose a novel approach to learn domain\ninvariant feature representations, namely Wasserstein Distance Guided\nRepresentation Learning (WDGRL). WDGRL utilizes a neural network, denoted by\nthe domain critic, to estimate empirical Wasserstein distance between the\nsource and target samples and optimizes the feature extractor network to\nminimize the estimated Wasserstein distance in an adversarial manner. The\ntheoretical advantages of Wasserstein distance for domain adaptation lie in its\ngradient property and promising generalization bound. Empirical studies on\ncommon sentiment and image classification adaptation datasets demonstrate that\nour proposed WDGRL outperforms the state-of-the-art domain invariant\nrepresentation learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:34:13 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 05:39:24 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 12:15:15 GMT"}, {"version": "v4", "created": "Fri, 9 Mar 2018 07:20:13 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Shen", "Jian", ""], ["Qu", "Yanru", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1707.01377", "submitter": "Edouard Ribes", "authors": "Edouard Ribes (1), Karim Touahri (2), Beno\\^it Perthame (3) ((1) IRSEM\n  (2) UPD5 (3) LJLL)", "title": "Employee turnover prediction and retention policies design: a case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates the similarities between the problems of customer\nchurn and employee turnover. An example of employee turnover prediction model\nleveraging classical machine learning techniques is developed. Model outputs\nare then discussed to design \\& test employee retention policies. This type of\nretention discussion is, to our knowledge, innovative and constitutes the main\nvalue of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:06:54 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ribes", "Edouard", ""], ["Touahri", "Karim", ""], ["Perthame", "Beno\u00eet", ""]]}, {"id": "1707.01461", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Sunita Sarawagi", "title": "Labeled Memory Networks for Online Model Adaptation", "comments": "Accepted at AAAI 2018, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting a neural network with memory that can grow without growing the\nnumber of trained parameters is a recent powerful concept with many exciting\napplications. We propose a design of memory augmented neural networks (MANNs)\ncalled Labeled Memory Networks (LMNs) suited for tasks requiring online\nadaptation in classification models. LMNs organize the memory with classes as\nthe primary key.The memory acts as a second boosted stage following a regular\nneural network thereby allowing the memory and the primary network to play\ncomplementary roles. Unlike existing MANNs that write to memory for every\ninstance and use LRU based memory replacement, LMNs write only for instances\nwith non-zero loss and use label-based memory replacement. We demonstrate\nsignificant accuracy gains on various tasks including word-modelling and\nfew-shot learning. In this paper, we establish their potential in online\nadapting a batch trained neural network to domain-relevant labeled data at\ndeployment time. We show that LMNs are better than other MANNs designed for\nmeta-learning. We also found them to be more accurate and faster than\nstate-of-the-art methods of retuning model parameters for adapting to\ndomain-specific labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 16:49:28 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 09:40:19 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 07:31:20 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Shankar", "Shiv", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1707.01473", "submitter": "Jann Spiess", "authors": "Jens Ludwig, Sendhil Mullainathan, Jann Spiess", "title": "Machine-Learning Tests for Effects on Multiple Outcomes", "comments": "Minor update: new abstract, introduction, and section on representing\n  distribution differences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present tools for applied researchers that re-purpose\noff-the-shelf methods from the computer-science field of machine learning to\ncreate a \"discovery engine\" for data from randomized controlled trials (RCTs).\nThe applied problem we seek to solve is that economists invest vast resources\ninto carrying out RCTs, including the collection of a rich set of candidate\noutcome measures. But given concerns about inference in the presence of\nmultiple testing, economists usually wind up exploring just a small subset of\nthe hypotheses that the available data could be used to test. This prevents us\nfrom extracting as much information as possible from each RCT, which in turn\nimpairs our ability to develop new theories or strengthen the design of policy\ninterventions. Our proposed solution combines the basic intuition of reverse\nregression, where the dependent variable of interest now becomes treatment\nassignment itself, with methods from machine learning that use the data\nthemselves to flexibly identify whether there is any function of the outcomes\nthat predicts (or has signal about) treatment group status. This leads to\ncorrectly-sized tests with appropriate $p$-values, which also have the\nimportant virtue of being easy to implement in practice. One open challenge\nthat remains with our work is how to meaningfully interpret the signal that\nthese methods find.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:13:08 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 18:10:56 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ludwig", "Jens", ""], ["Mullainathan", "Sendhil", ""], ["Spiess", "Jann", ""]]}, {"id": "1707.01475", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon and Maximilian Nickel", "title": "Complex and Holographic Embeddings of Knowledge Graphs: A Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of knowledge graphs have received significant attention due to\ntheir excellent performance for tasks like link prediction and entity\nresolution. In this short paper, we are providing a comparison of two\nstate-of-the-art knowledge graph embeddings for which their equivalence has\nrecently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio,\n2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly\nreview both models and discuss how their scoring functions are equivalent. We\nthen analyze the discrepancy of results reported in the original articles, and\nshow experimentally that they are likely due to the use of different loss\nfunctions. In further experiments, we evaluate the ability of both models to\nembed symmetric and antisymmetric patterns. Finally, we discuss advantages and\ndisadvantages of both models and under which conditions one would be preferable\nto the other.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:17:34 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 04:30:21 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Nickel", "Maximilian", ""]]}, {"id": "1707.01543", "submitter": "Yuting Wei", "authors": "Yuting Wei, Fanny Yang, Martin J. Wainwright", "title": "Early stopping for kernel boosting algorithms: A general analysis with\n  localized complexities", "comments": "Compared to the nips version, this version includes results for the\n  random design case and the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early stopping of iterative algorithms is a widely-used form of\nregularization in statistics, commonly used in conjunction with boosting and\nrelated gradient-type algorithms. Although consistency results have been\nestablished in some settings, such estimators are less well-understood than\ntheir analogues based on penalized regularization. In this paper, for a\nrelatively broad class of loss functions and boosting algorithms (including\nL2-boost, LogitBoost and AdaBoost, among others), we exhibit a direct\nconnection between the performance of a stopped iterate and the localized\nGaussian complexity of the associated function class. This connection allows us\nto show that local fixed point analysis of Gaussian or Rademacher complexities,\nnow standard in the analysis of penalized estimators, can be used to derive\noptimal stopping rules. We derive such stopping rules in detail for various\nkernel classes, and illustrate the correspondence of our theory with practice\nfor Sobolev kernel classes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:12:12 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 20:17:06 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wei", "Yuting", ""], ["Yang", "Fanny", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1707.01546", "submitter": "Amartya Sanyal", "authors": "Amartya Sanyal, Sanjana Garg, Asim Unmesh", "title": "Agent based simulation of the evolution of society as an alternate\n  maximization problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the evolution of human society, as a complex adaptive system,\nis a task that has been looked upon from various angles. In this paper, we\nsimulate an agent-based model with a high enough population tractably. To do\nthis, we characterize an entity called \\textit{society}, which helps us reduce\nthe complexity of each step from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n)$. We\npropose a very realistic setting, where we design a joint alternate\nmaximization step algorithm to maximize a certain \\textit{fitness} function,\nwhich we believe simulates the way societies develop. Our key contributions\ninclude (i) proposing a novel protocol for simulating the evolution of a\nsociety with cheap, non-optimal joint alternate maximization steps (ii)\nproviding a framework for carrying out experiments that adhere to this\njoint-optimization simulation framework (iii) carrying out experiments to show\nthat it makes sense empirically (iv) providing an alternate justification for\nthe use of \\textit{society} in the simulations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:19:55 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Sanyal", "Amartya", ""], ["Garg", "Sanjana", ""], ["Unmesh", "Asim", ""]]}, {"id": "1707.01591", "submitter": "Arya Farahi", "authors": "Alex Chojnacki, Chengyu Dai, Arya Farahi, Guangsha Shi, Jared Webb,\n  Daniel T. Zhang, Jacob Abernethy, Eric Schwartz", "title": "A Data Science Approach to Understanding Residential Water Contamination\n  in Flint", "comments": "Applied Data Science track paper at KDD 2017. For associated\n  promotional video, see https://www.youtube.com/watch?v=0g66ImaV8Ag", "journal-ref": null, "doi": "10.1145/3097983.3098078", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the residents of Flint learned that lead had contaminated their water\nsystem, the local government made water-testing kits available to them free of\ncharge. The city government published the results of these tests, creating a\nvaluable dataset that is key to understanding the causes and extent of the lead\ncontamination event in Flint. This is the nation's largest dataset on lead in a\nmunicipal water system.\n  In this paper, we predict the lead contamination for each household's water\nsupply, and we study several related aspects of Flint's water troubles, many of\nwhich generalize well beyond this one city. For example, we show that elevated\nlead risks can be (weakly) predicted from observable home attributes. Then we\nexplore the factors associated with elevated lead. These risk assessments were\ndeveloped in part via a crowd sourced prediction challenge at the University of\nMichigan. To inform Flint residents of these assessments, they have been\nincorporated into a web and mobile application funded by \\texttt{Google.org}.\nWe also explore questions of self-selection in the residential testing program,\nexamining which factors are linked to when and how frequently residents\nvoluntarily sample their water.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 22:12:14 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Chojnacki", "Alex", ""], ["Dai", "Chengyu", ""], ["Farahi", "Arya", ""], ["Shi", "Guangsha", ""], ["Webb", "Jared", ""], ["Zhang", "Daniel T.", ""], ["Abernethy", "Jacob", ""], ["Schwartz", "Eric", ""]]}, {"id": "1707.01596", "submitter": "Deepjyoti Deka", "authors": "Deepjyoti Deka, Saurav Talukdar, Michael Chertkov, and Murti Salapaka", "title": "Topology Estimation in Bulk Power Grids: Guarantees on Exact Recovery", "comments": "10 pages, 8 figures. A version of this paper will appear in IREP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topology of a power grid affects its dynamic operation and settlement in\nthe electricity market. Real-time topology identification can enable faster\ncontrol action following an emergency scenario like failure of a line. This\narticle discusses a graphical model framework for topology estimation in bulk\npower grids (both loopy transmission and radial distribution) using\nmeasurements of voltage collected from the grid nodes. The graphical model for\nthe probability distribution of nodal voltages in linear power flow models is\nshown to include additional edges along with the operational edges in the true\ngrid. Our proposed estimation algorithms first learn the graphical model and\nsubsequently extract the operational edges using either thresholding or a\nneighborhood counting scheme. For grid topologies containing no three-node\ncycles (two buses do not share a common neighbor), we prove that an exact\nextraction of the operational topology is theoretically guaranteed. This\nincludes a majority of distribution grids that have radial topologies. For\ngrids that include cycles of length three, we provide sufficient conditions\nthat ensure existence of algorithms for exact reconstruction. In particular,\nfor grids with constant impedance per unit length and uniform injection\ncovariances, this observation leads to conditions on geographical placement of\nthe buses. The performance of algorithms is demonstrated in test case\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 22:57:59 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 19:14:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Deka", "Deepjyoti", ""], ["Talukdar", "Saurav", ""], ["Chertkov", "Michael", ""], ["Salapaka", "Murti", ""]]}, {"id": "1707.01647", "submitter": "Alex Kim", "authors": "HyoungSeok Kim, JiHoon Kang, WooMyoung Park, SukHyun Ko, YoonHo Cho,\n  DaeSung Yu, YoungSook Song, JungWon Choi", "title": "Convergence Analysis of Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The regret bound of an optimization algorithms is one of the basic criteria\nfor evaluating the performance of the given algorithm. By inspecting the\ndifferences between the regret bounds of traditional algorithms and adaptive\none, we provide a guide for choosing an optimizer with respect to the given\ndata set and the loss function. For analysis, we assume that the loss function\nis convex and its gradient is Lipschitz continuous.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 06:10:53 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Kim", "HyoungSeok", ""], ["Kang", "JiHoon", ""], ["Park", "WooMyoung", ""], ["Ko", "SukHyun", ""], ["Cho", "YoonHo", ""], ["Yu", "DaeSung", ""], ["Song", "YoungSook", ""], ["Choi", "JungWon", ""]]}, {"id": "1707.01711", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki, Takafumi Kanamori, Aapo Hyv\\\"arinen, Gang Niu, Masashi\n  Sugiyama", "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct\n  Estimation of Density-Derivative-Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modes and ridges of the probability density function behind observed data are\nuseful geometric features. Mode-seeking clustering assigns cluster labels by\nassociating data samples with the nearest modes, and estimation of density\nridges enables us to find lower-dimensional structures hidden in data. A key\ntechnical challenge both in mode-seeking clustering and density ridge\nestimation is accurate estimation of the ratios of the first- and second-order\ndensity derivatives to the density. A naive approach takes a three-step\napproach of first estimating the data density, then computing its derivatives,\nand finally taking their ratios. However, this three-step approach can be\nunreliable because a good density estimator does not necessarily mean a good\ndensity derivative estimator, and division by the estimated density could\nsignificantly magnify the estimation error. To cope with these problems, we\npropose a novel estimator for the \\emph{density-derivative-ratios}. The\nproposed estimator does not involve density estimation, but rather\n\\emph{directly} approximates the ratios of density derivatives of any order.\nMoreover, we establish a convergence rate of the proposed estimator. Based on\nthe proposed estimator, novel methods both for mode-seeking clustering and\ndensity ridge estimation are developed, and the respective convergence rates to\nthe mode and ridge of the underlying density are also established. Finally, we\nexperimentally demonstrate that the developed methods significantly outperform\nexisting methods, particularly for relatively high-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:57:08 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 15:08:43 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 22:49:48 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Kanamori", "Takafumi", ""], ["Hyv\u00e4rinen", "Aapo", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1707.01822", "submitter": "Bowen Li", "authors": "Bowen Li", "title": "Nonparametric Marginal Analysis of Recurrent Events Data under Competing\n  Risks", "comments": "PhD Dissertation, 73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project was motivated by a dialysis study in northern Taiwan. Dialysis\npatients, after shunt implantation, may experience two types (\"acute\" or\n\"non-acute\") of shunt thrombosis, both of which may recur. We formulate the\nproblem under the framework of recurrent events data in the presence of\ncompeting risks. In particular we focus on marginal inference for the gap time\nvariable of specific type. The functions of interest are the cumulative\nincidence function and cause-specific hazard function. The major challenge of\nnonparametric inference is the problem of induced dependent censoring. We apply\nthe technique of inverse probability of censoring weighting (IPCW) to adjust\nfor the selection bias. Besides point estimation, we apply the bootstrap\nre-sampling method for further inference. Large sample properties of the\nproposed estimators are derived. Simulations are performed to examine the\nfinite-sample performances of the proposed methods. Finally we apply the\nproposed methodology to analyze the dialysis data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:49:42 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Li", "Bowen", ""]]}, {"id": "1707.01826", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Xiaolin Huang, Chen Gong, Jie Yang, Johan A.K. Suykens", "title": "Indefinite Kernel Logistic Regression with Concave-inexact-convex\n  Procedure", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2018.2851305", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In kernel methods, the kernels are often required to be positive definite,\nwhich restricts the use of many indefinite kernels. To consider those\nnon-positive definite kernels, in this paper, we aim to build an indefinite\nkernel learning framework for kernel logistic regression. The proposed\nindefinite kernel logistic regression (IKLR) model is analysed in the\nReproducing Kernel Kre\\u{\\i}n Spaces (RKKS) and then becomes non-convex. Using\nthe positive decomposition of a non-positive definite kernel, the derived IKLR\nmodel can be decomposed into the difference of two convex functions.\nAccordingly, a concave-convex procedure is introduced to solve the non-convex\noptimization problem. Since the concave-convex procedure has to solve a\nsub-problem in each iteration, we propose a concave-inexact-convex procedure\n(CCICP) algorithm with an inexact solving scheme to accelerate the solving\nprocess. Besides, we propose a stochastic variant of CCICP to efficiently\nobtain a proximal solution, which achieves the similar purpose with the inexact\nsolving scheme in CCICP. The convergence analyses of the above two variants of\nconcave-convex procedure are conducted. By doing so, our method works\neffectively not only under a deterministic setting but also under a stochastic\nsetting. Experimental results on several benchmarks suggest that the proposed\nIKLR model performs favorably against the standard (positive-definite) kernel\nlogistic regression and other competitive indefinite learning based algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:55:08 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 13:42:21 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liu", "Fanghui", ""], ["Huang", "Xiaolin", ""], ["Gong", "Chen", ""], ["Yang", "Jie", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1707.01926", "submitter": "Yaguang Li", "authors": "Yaguang Li, Rose Yu, Cyrus Shahabi and Yan Liu", "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic\n  Forecasting", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal forecasting has various applications in neuroscience, climate\nand transportation domain. Traffic forecasting is one canonical example of such\nlearning task. The task is challenging due to (1) complex spatial dependency on\nroad networks, (2) non-linear temporal dynamics with changing road conditions\nand (3) inherent difficulty of long-term forecasting. To address these\nchallenges, we propose to model the traffic flow as a diffusion process on a\ndirected graph and introduce Diffusion Convolutional Recurrent Neural Network\n(DCRNN), a deep learning framework for traffic forecasting that incorporates\nboth spatial and temporal dependency in the traffic flow. Specifically, DCRNN\ncaptures the spatial dependency using bidirectional random walks on the graph,\nand the temporal dependency using the encoder-decoder architecture with\nscheduled sampling. We evaluate the framework on two real-world large scale\nroad network traffic datasets and observe consistent improvement of 12% - 15%\nover state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:20:59 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:51:18 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:52:51 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Li", "Yaguang", ""], ["Yu", "Rose", ""], ["Shahabi", "Cyrus", ""], ["Liu", "Yan", ""]]}, {"id": "1707.01932", "submitter": "Eric Jang", "authors": "Eric Jang, Sudheendra Vijayanarasimhan, Peter Pastor, Julian Ibarz,\n  Sergey Levine", "title": "End-to-End Learning of Semantic Grasping", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of semantic robotic grasping, in which a robot picks up\nan object of a user-specified class using only monocular images. Inspired by\nthe two-stream hypothesis of visual reasoning, we present a semantic grasping\nframework that learns object detection, classification, and grasp planning in\nan end-to-end fashion. A \"ventral stream\" recognizes object class while a\n\"dorsal stream\" simultaneously interprets the geometric relationships necessary\nto execute successful grasps. We leverage the autonomous data collection\ncapabilities of robots to obtain a large self-supervised dataset for training\nthe dorsal stream, and use semi-supervised label propagation to train the\nventral stream with only a modest amount of human supervision. We\nexperimentally show that our approach improves upon grasping systems whose\ncomponents are not learned end-to-end, including a baseline method that uses\nbounding box detection. Furthermore, we show that jointly training our model\nwith auxiliary data consisting of non-semantic grasping data, as well as\nsemantically labeled images without grasp actions, has the potential to\nsubstantially improve semantic grasping performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:41:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 07:41:54 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 08:57:52 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Jang", "Eric", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Pastor", "Peter", ""], ["Ibarz", "Julian", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.01939", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Shahin Nazarian, Massoud Pedram", "title": "High-Performance FPGA Implementation of Equivariant Adaptive Separation\n  via Independence Algorithm for Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a dimensionality reduction technique\nthat can boost efficiency of machine learning models that deal with probability\ndensity functions, e.g. Bayesian neural networks. Algorithms that implement\nadaptive ICA converge slower than their nonadaptive counterparts, however, they\nare capable of tracking changes in underlying distributions of input features.\nThis intrinsically slow convergence of adaptive methods combined with existing\nhardware implementations that operate at very low clock frequencies necessitate\nfundamental improvements in both algorithm and hardware design. This paper\npresents an algorithm that allows efficient hardware implementation of ICA.\nCompared to previous work, our FPGA implementation of adaptive ICA improves\nclock frequency by at least one order of magnitude and throughput by at least\ntwo orders of magnitude. Our proposed algorithm is not limited to ICA and can\nbe used in various machine learning problems that use stochastic gradient\ndescent optimization.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:02:11 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Nazarian", "Shahin", ""], ["Pedram", "Massoud", ""]]}, {"id": "1707.01945", "submitter": "Deanna Needell", "authors": "Deanna Needell, Rayan Saab, Tina Woolf", "title": "Simple Classification using Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary, or one-bit, representations of data arise naturally in many\napplications, and are appealing in both hardware implementations and algorithm\ndesign. In this work, we study the problem of data classification from binary\ndata and propose a framework with low computation and resource costs. We\nillustrate the utility of the proposed approach through stylized and realistic\nnumerical experiments, and provide a theoretical analysis for a simple case. We\nhope that our framework and analysis will serve as a foundation for studying\nsimilar types of approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:45:29 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Needell", "Deanna", ""], ["Saab", "Rayan", ""], ["Woolf", "Tina", ""]]}, {"id": "1707.02050", "submitter": "Yasuhiko Igarashi", "authors": "Yasuhiko Igarashi, Hikaru Takenaka, Yoshinori Nakanishi-Ohno, Makoto\n  Uemura, Shiro Ikeda and Masato Okada", "title": "Exhaustive search for sparse variable selection in linear regression", "comments": "19pages, 3 figures", "journal-ref": null, "doi": "10.7566/JPSJ.87.044802", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a K-sparse exhaustive search (ES-K) method and a K-sparse\napproximate exhaustive search method (AES-K) for selecting variables in linear\nregression. With these methods, K-sparse combinations of variables are tested\nexhaustively assuming that the optimal combination of explanatory variables is\nK-sparse. By collecting the results of exhaustively computing ES-K, various\napproximate methods for selecting sparse variables can be summarized as density\nof states. With this density of states, we can compare different methods for\nselecting sparse variables such as relaxation and sampling. For large problems\nwhere the combinatorial explosion of explanatory variables is crucial, the\nAES-K method enables density of states to be effectively reconstructed by using\nthe replica-exchange Monte Carlo method and the multiple histogram method.\nApplying the ES-K and AES-K methods to type Ia supernova data, we confirmed the\nconventional understanding in astronomy when an appropriate K is given\nbeforehand. However, we found the difficulty to determine K from the data.\nUsing virtual measurement and analysis, we argue that this is caused by data\nshortage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:19:00 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Igarashi", "Yasuhiko", ""], ["Takenaka", "Hikaru", ""], ["Nakanishi-Ohno", "Yoshinori", ""], ["Uemura", "Makoto", ""], ["Ikeda", "Shiro", ""], ["Okada", "Masato", ""]]}, {"id": "1707.02158", "submitter": "Bora Edizel", "authors": "Bora Edizel, Amin Mantrach, Xiao Bai", "title": "Deep Character-Level Click-Through Rate Prediction for Sponsored Search", "comments": "SIGIR2017, 10 pages", "journal-ref": null, "doi": "10.1145/3077136.3080811", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the click-through rate of an advertisement is a critical component\nof online advertising platforms. In sponsored search, the click-through rate\nestimates the probability that a displayed advertisement is clicked by a user\nafter she submits a query to the search engine. Commercial search engines\ntypically rely on machine learning models trained with a large number of\nfeatures to make such predictions. This is inevitably requires a lot of\nengineering efforts to define, compute, and select the appropriate features. In\nthis paper, we propose two novel approaches (one working at character level and\nthe other working at word level) that use deep convolutional neural networks to\npredict the click-through rate of a query-advertisement pair. Specially, the\nproposed architectures only consider the textual content appearing in a\nquery-advertisement pair as input, and produce as output a click-through rate\nprediction. By comparing the character-level model with the word-level model,\nwe show that language representation can be learnt from scratch at character\nlevel when trained on enough data. Through extensive experiments using billions\nof query-advertisement pairs of a popular commercial search engine, we\ndemonstrate that both approaches significantly outperform a baseline model\nbuilt on well-selected text features and a state-of-the-art word2vec-based\napproach. Finally, by combining the predictions of the deep models introduced\nin this study with the prediction of the model in production of the same\ncommercial search engine, we significantly improve the accuracy and the\ncalibration of the click-through rate prediction of the production system.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 13:15:45 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Edizel", "Bora", ""], ["Mantrach", "Amin", ""], ["Bai", "Xiao", ""]]}, {"id": "1707.02293", "submitter": "Andres Masegosa R", "authors": "Andres Masegosa, Thomas D. Nielsen, Helge Langseth, Dario Ramos-Lopez,\n  Antonio Salmeron, Anders L. Madsen", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making inferences from data streams is a pervasive problem in many modern\ndata analysis applications. But it requires to address the problem of\ncontinuous model updating and adapt to changes or drifts in the underlying data\ngenerating distribution. In this paper, we approach these problems from a\nBayesian perspective covering general conjugate exponential models. Our\nproposal makes use of non-conjugate hierarchical priors to explicitly model\ntemporal changes of the model parameters. We also derive a novel variational\ninference scheme which overcomes the use of non-conjugate priors while\nmaintaining the computational efficiency of variational methods over conjugate\nmodels. The approach is validated on three real data sets over three latent\nvariable models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 09:44:15 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Masegosa", "Andres", ""], ["Nielsen", "Thomas D.", ""], ["Langseth", "Helge", ""], ["Ramos-Lopez", "Dario", ""], ["Salmeron", "Antonio", ""], ["Madsen", "Anders L.", ""]]}, {"id": "1707.02294", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Raghav Somani and Sreangsu Acharyya", "title": "A case study of Empirical Bayes in User-Movie Recommendation system", "comments": "14 pages, 3 figures, 4 subfigures", "journal-ref": null, "doi": "10.1080/23737484.2017.1392266", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we provide a formulation of empirical bayes described by\nAtchade (2011) to tune the hyperparameters of priors used in bayesian set up of\ncollaborative filter. We implement the same in MovieLens small dataset. We see\nthat it can be used to get a good initial choice for the parameters. It can\nalso be used to guess an initial choice for hyper-parameters in grid search\nprocedure even for the datasets where MCMC oscillates around the true value or\ntakes long time to converge.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:54:55 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Somani", "Raghav", ""], ["Acharyya", "Sreangsu", ""]]}, {"id": "1707.02329", "submitter": "Faris B Mismar", "authors": "Faris B. Mismar and Brian L. Evans", "title": "Deep Q-Learning for Self-Organizing Networks Fault Management and Radio\n  Performance Improvement", "comments": "(c) 2018 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2018 52nd Annual Asilomar Conference on Signals, Systems, and\n  Computers, Pacific Grove, CA, USA, 2018, pp. 1-5", "doi": "10.1109/ACSSC.2018.8645083", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to automate fault management in an outdoor cellular\nnetwork using deep reinforcement learning (RL) against wireless impairments.\nThis algorithm enables the cellular network cluster to self-heal by allowing RL\nto learn how to improve the downlink signal to interference plus noise ratio\nthrough exploration and exploitation of various alarm corrective actions. The\nmain contributions of this paper are to 1) introduce a deep RL-based fault\nhandling algorithm which self-organizing networks can implement in a polynomial\nruntime and 2) show that this fault management method can improve the radio\nlink performance in a realistic network setup. Simulation results show that our\nproposed algorithm learns an action sequence to clear alarms and improve the\nperformance in the cellular cluster better than existing algorithms, even\nagainst the randomness of the network fault occurrences and user movements.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 02:29:17 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 02:51:02 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 01:49:48 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 04:31:27 GMT"}, {"version": "v5", "created": "Wed, 7 Nov 2018 21:19:46 GMT"}, {"version": "v6", "created": "Thu, 10 Jan 2019 12:41:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Mismar", "Faris B.", ""], ["Evans", "Brian L.", ""]]}, {"id": "1707.02391", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Ravishankar Krishnaswamy, Prateek Jain", "title": "Learning Mixture of Gaussians with Streaming Data", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning a mixture of Gaussians with\nstreaming data: given a stream of $N$ points in $d$ dimensions generated by an\nunknown mixture of $k$ spherical Gaussians, the goal is to estimate the model\nparameters using a single pass over the data stream. We analyze a streaming\nversion of the popular Lloyd's heuristic and show that the algorithm estimates\nall the unknown centers of the component Gaussians accurately if they are\nsufficiently separated. Assuming each pair of centers are $C\\sigma$ distant\nwith $C=\\Omega((k\\log k)^{1/4}\\sigma)$ and where $\\sigma^2$ is the maximum\nvariance of any Gaussian component, we show that asymptotically the algorithm\nestimates the centers optimally (up to constants); our center separation\nrequirement matches the best known result for spherical Gaussians\n\\citep{vempalawang}. For finite samples, we show that a bias term based on the\ninitial estimate decreases at $O(1/{\\rm poly}(N))$ rate while variance\ndecreases at nearly optimal rate of $\\sigma^2 d/N$.\n  Our analysis requires seeding the algorithm with a good initial estimate of\nthe true cluster centers for which we provide an online PCA based clustering\nalgorithm. Indeed, the asymptotic per-step time complexity of our algorithm is\nthe optimal $d\\cdot k$ while space complexity of our algorithm is $O(dk\\log\nk)$.\n  In addition to the bias and variance terms which tend to $0$, the\nhard-thresholding based updates of streaming Lloyd's algorithm is agnostic to\nthe data distribution and hence incurs an approximation error that cannot be\navoided. However, by using a streaming version of the classical\n(soft-thresholding-based) EM method that exploits the Gaussian distribution\nexplicitly, we show that for a mixture of two Gaussians the true means can be\nestimated consistently, with estimation error decreasing at nearly optimal\nrate, and tending to $0$ for $N\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 03:35:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Krishnaswamy", "Ravishankar", ""], ["Jain", "Prateek", ""]]}, {"id": "1707.02432", "submitter": "Senthil Yogamani", "authors": "Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, Senthil Yogamani", "title": "Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and\n  Challenges", "comments": "To appear in IEEE ITSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation was seen as a challenging computer vision problem few\nyears ago. Due to recent advancements in deep learning, relatively accurate\nsolutions are now possible for its use in automated driving. In this paper, the\nsemantic segmentation problem is explored from the perspective of automated\ndriving. Most of the current semantic segmentation algorithms are designed for\ngeneric images and do not incorporate prior structure and end goal for\nautomated driving. First, the paper begins with a generic taxonomic survey of\nsemantic segmentation algorithms and then discusses how it fits in the context\nof automated driving. Second, the particular challenges of deploying it into a\nsafety system which needs high level of accuracy and robustness are listed.\nThird, different alternatives instead of using an independent semantic\nsegmentation module are explored. Finally, an empirical evaluation of various\nsemantic segmentation architectures was performed on CamVid dataset in terms of\naccuracy and speed. This paper is a preliminary shorter version of a more\ndetailed survey which is work in progress.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 12:32:51 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 14:54:40 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Siam", "Mennatullah", ""], ["Elkerdawy", "Sara", ""], ["Jagersand", "Martin", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1707.02444", "submitter": "Chulhee Yun", "authors": "Chulhee Yun, Suvrit Sra, Ali Jadbabaie", "title": "Global optimality conditions for deep neural networks", "comments": "14 pages. A camera-ready version that will appear at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the error landscape of deep linear and nonlinear neural networks\nwith the squared error loss. Minimizing the loss of a deep linear neural\nnetwork is a nonconvex problem, and despite recent progress, our understanding\nof this loss surface is still incomplete. For deep linear networks, we present\nnecessary and sufficient conditions for a critical point of the risk function\nto be a global minimum. Surprisingly, our conditions provide an efficiently\ncheckable test for global optimality, while such tests are typically\nintractable in nonconvex optimization. We further extend these results to deep\nnonlinear neural networks and prove similar sufficient conditions for global\noptimality, albeit in a more limited function space setting.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 14:04:37 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 03:37:54 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 05:26:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yun", "Chulhee", ""], ["Sra", "Suvrit", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1707.02461", "submitter": "Zachary Charles", "authors": "Zachary Charles, Amin Jalali, Rebecca Willett", "title": "Subspace Clustering with Missing and Corrupted Data", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given full or partial information about a collection of points that lie close\nto a union of several subspaces, subspace clustering refers to the process of\nclustering the points according to their subspace and identifying the\nsubspaces. One popular approach, sparse subspace clustering (SSC), represents\neach sample as a weighted combination of the other samples, with weights of\nminimal $\\ell_1$ norm, and then uses those learned weights to cluster the\nsamples. SSC is stable in settings where each sample is contaminated by a\nrelatively small amount of noise. However, when there is a significant amount\nof additive noise, or a considerable number of entries are missing, theoretical\nguarantees are scarce. In this paper, we study a robust variant of SSC and\nestablish clustering guarantees in the presence of corrupted or missing data.\nWe give explicit bounds on amount of noise and missing data that the algorithm\ncan tolerate, both in deterministic settings and in a random generative model.\nNotably, our approach provides guarantees for higher tolerance to noise and\nmissing data than existing analyses for this method. By design, the results\nhold even when we do not know the locations of the missing data; e.g., as in\npresence-only data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 16:24:11 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 18:26:24 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Charles", "Zachary", ""], ["Jalali", "Amin", ""], ["Willett", "Rebecca", ""]]}, {"id": "1707.02476", "submitter": "John Bradshaw", "authors": "John Bradshaw, Alexander G. de G. Matthews, Zoubin Ghahramani", "title": "Adversarial Examples, Uncertainty, and Transfer Testing Robustness in\n  Gaussian Process Hybrid Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have excellent representative power and are state\nof the art classifiers on many tasks. However, they often do not capture their\nown uncertainties well making them less robust in the real world as they\noverconfidently extrapolate and do not notice domain shift. Gaussian processes\n(GPs) with RBF kernels on the other hand have better calibrated uncertainties\nand do not overconfidently extrapolate far from data in their training set.\nHowever, GPs have poor representational power and do not perform as well as\nDNNs on complex domains. In this paper we show that GP hybrid deep networks,\nGPDNNs, (GPs on top of DNNs and trained end-to-end) inherit the nice properties\nof both GPs and DNNs and are much more robust to adversarial examples. When\nextrapolating to adversarial examples and testing in domain shift settings,\nGPDNNs frequently output high entropy class probabilities corresponding to\nessentially \"don't know\". GPDNNs are therefore promising as deep architectures\nthat know when they don't know.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 18:31:46 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bradshaw", "John", ""], ["Matthews", "Alexander G. de G.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.02510", "submitter": "Siddhartha Saxena", "authors": "Siddhartha Saxena, Shibhansh Dohare and Jaivardhan Kapoor", "title": "Variational Inference via Transformations on Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods often focus on the problem of efficient model\noptimization, with little emphasis on the choice of the approximating\nposterior. In this paper, we review and implement the various methods that\nenable us to develop a rich family of approximating posteriors. We show that\none particular method employing transformations on distributions results in\ndeveloping very rich and complex posterior approximation. We analyze its\nperformance on the MNIST dataset by implementing with a Variational Autoencoder\nand demonstrate its effectiveness in learning better posterior distributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 01:25:05 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Saxena", "Siddhartha", ""], ["Dohare", "Shibhansh", ""], ["Kapoor", "Jaivardhan", ""]]}, {"id": "1707.02641", "submitter": "Vincent Dorie", "authors": "Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone", "title": "Automated versus do-it-yourself methods for causal inference: Lessons\n  learned from a data analysis competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians have made great progress in creating methods that reduce our\nreliance on parametric assumptions. However this explosion in research has\nresulted in a breadth of inferential strategies that both create opportunities\nfor more reliable inference as well as complicate the choices that an applied\nresearcher has to make and defend. Relatedly, researchers advocating for new\nmethods typically compare their method to at best 2 or 3 other causal inference\nstrategies and test using simulations that may or may not be designed to\nequally tease out flaws in all the competing methods. The causal inference data\nanalysis challenge, \"Is Your SATT Where It's At?\", launched as part of the 2016\nAtlantic Causal Inference Conference, sought to make progress with respect to\nboth of these issues. The researchers creating the data testing grounds were\ndistinct from the researchers submitting methods whose efficacy would be\nevaluated. Results from 30 competitors across the two versions of the\ncompetition (black box algorithms and do-it-yourself analyses) are presented\nalong with post-hoc analyses that reveal information about the characteristics\nof causal inference strategies and settings that affect performance. The most\nconsistent conclusion was that methods that flexibly model the response surface\nperform better overall than methods that fail to do so. Finally new methods are\nproposed that combine features of several of the top-performing submitted\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 21:24:25 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 18:40:08 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 19:27:46 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 16:38:31 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 16:18:04 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Dorie", "Vincent", ""], ["Hill", "Jennifer", ""], ["Shalit", "Uri", ""], ["Scott", "Marc", ""], ["Cervone", "Dan", ""]]}, {"id": "1707.02649", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Vahid Tarokh", "title": "Nonlinear Sequential Accepts and Rejects for Identification of Top Arms\n  in Stochastic Bandits", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the M-best-arm identification problem in multi-armed bandits. A\nplayer has a limited budget to explore K arms (M<K), and once pulled, each arm\nyields a reward drawn (independently) from a fixed, unknown distribution. The\ngoal is to find the top M arms in the sense of expected reward. We develop an\nalgorithm which proceeds in rounds to deactivate arms iteratively. At each\nround, the budget is divided by a nonlinear function of remaining arms, and the\narms are pulled correspondingly. Based on a decision rule, the deactivated arm\nat each round may be accepted or rejected. The algorithm outputs the accepted\narms that should ideally be the top M arms. We characterize the decay rate of\nthe misidentification probability and establish that the nonlinear budget\nallocation proves to be useful for different problem environments (described by\nthe number of competitive arms). We provide comprehensive numerical experiments\nshowing that our algorithm outperforms the state-of-the-art using suitable\nnonlinearity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 22:32:09 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1707.02670", "submitter": "Peng Xu", "authors": "Christopher De Sa, Bryan He, Ioannis Mitliagkas, Christopher R\\'e,\n  Peng Xu", "title": "Accelerated Stochastic Power Iteration", "comments": "37 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most powerful tools in\nmachine learning. The simplest method for PCA, the power iteration, requires\n$\\mathcal O(1/\\Delta)$ full-data passes to recover the principal component of a\nmatrix with eigen-gap $\\Delta$. Lanczos, a significantly more complex method,\nachieves an accelerated rate of $\\mathcal O(1/\\sqrt{\\Delta})$ passes. Modern\napplications, however, motivate methods that only ingest a subset of available\ndata, known as the stochastic setting. In the online stochastic setting, simple\nalgorithms like Oja's iteration achieve the optimal sample complexity $\\mathcal\nO(\\sigma^2/\\Delta^2)$. Unfortunately, they are fully sequential, and also\nrequire $\\mathcal O(\\sigma^2/\\Delta^2)$ iterations, far from the $\\mathcal\nO(1/\\sqrt{\\Delta})$ rate of Lanczos. We propose a simple variant of the power\niteration with an added momentum term, that achieves both the optimal sample\nand iteration complexity. In the full-pass setting, standard analysis shows\nthat momentum achieves the accelerated rate, $\\mathcal O(1/\\sqrt{\\Delta})$. We\ndemonstrate empirically that naively applying momentum to a stochastic method,\ndoes not result in acceleration. We perform a novel, tight variance analysis\nthat reveals the \"breaking-point variance\" beyond which this acceleration does\nnot occur. By combining this insight with modern variance reduction techniques,\nwe construct stochastic PCA algorithms, for the online and offline setting,\nthat achieve an accelerated iteration complexity $\\mathcal O(1/\\sqrt{\\Delta})$.\nDue to the embarassingly parallel nature of our methods, this acceleration\ntranslates directly to wall-clock time if deployed in a parallel environment.\nOur approach is very general, and applies to many non-convex optimization\nproblems that can now be accelerated using the same technique.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 01:13:33 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["De Sa", "Christopher", ""], ["He", "Bryan", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""], ["Xu", "Peng", ""]]}, {"id": "1707.02702", "submitter": "Shuang Song", "authors": "Shuang Song, Kamalika Chaudhuri", "title": "Composition Properties of Inferential Privacy for Time-Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of mobile devices and the internet of things,\ndeveloping principled solutions for privacy in time series applications has\nbecome increasingly important. While differential privacy is the gold standard\nfor database privacy, many time series applications require a different kind of\nguarantee, and a number of recent works have used some form of inferential\nprivacy to address these situations.\n  However, a major barrier to using inferential privacy in practice is its lack\nof graceful composition -- even if the same or related sensitive data is used\nin multiple releases that are safe individually, the combined release may have\npoor privacy properties. In this paper, we study composition properties of a\nform of inferential privacy called Pufferfish when applied to time-series data.\nWe show that while general Pufferfish mechanisms may not compose gracefully, a\nspecific Pufferfish mechanism, called the Markov Quilt Mechanism, which was\nrecently introduced, has strong composition properties comparable to that of\npure differential privacy when applied to time series data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 05:50:59 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Song", "Shuang", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1707.02711", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Philipp Grohs and Helmut B\\\"olcskei", "title": "Topology Reduction in Deep Convolutional Feature Extraction Networks", "comments": "Corrected errors in arguments on spectral decay of Sobolev functions.\n  Replaced part of the decay results (Sections 5-7) by corresponding statements\n  for effectively band-limited functions", "journal-ref": "Proc. of SPIE (Wavelets and Sparsity XVII), San Diego, USA, Vol.\n  10394, pp. 1039418:1-1039418:12, Aug. 2017, (invited paper)", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) used in practice employ potentially\nhundreds of layers and $10$,$000$s of nodes. Such network sizes entail\nsignificant computational complexity due to the large number of convolutions\nthat need to be carried out; in addition, a large number of parameters needs to\nbe learned and stored. Very deep and wide CNNs may therefore not be well suited\nto applications operating under severe resource constraints as is the case,\ne.g., in low-power embedded and mobile platforms. This paper aims at\nunderstanding the impact of CNN topology, specifically depth and width, on the\nnetwork's feature extraction capabilities. We address this question for the\nclass of scattering networks that employ either Weyl-Heisenberg filters or\nwavelets, the modulus non-linearity, and no pooling. The exponential feature\nmap energy decay results in Wiatowski et al., 2017, are generalized to\n$\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized\nthrough suitable choice of the Weyl-Heisenberg prototype function or the mother\nwavelet. We then show how networks of fixed (possibly small) depth $N$ can be\ndesigned to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal's\nenergy are contained in the feature vector. Based on the notion of\noperationally significant nodes, we characterize, partly rigorously and partly\nheuristically, the topology-reducing effects of (effectively) band-limited\ninput signals, band-limited filters, and feature map symmetries. Finally, for\nnetworks based on Weyl-Heisenberg filters, we determine the prototype function\nbandwidth that minimizes---for fixed network depth $N$---the average number of\noperationally significant nodes per layer.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 06:35:48 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 08:59:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1707.02736", "submitter": "Stefan Lessmann", "authors": "Korbinian Dress, Stefan Lessmann, Hans-J\\\"org von Mettenheim", "title": "Residual Value Forecasting Using Asymmetric Cost Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leasing is a popular channel to market new cars. Pricing a leasing contract\nis complicated because the leasing rate embodies an expectation of the residual\nvalue of the car after contract expiration. To aid lessors in their pricing\ndecisions, the paper develops resale price forecasting models. A peculiarity of\nthe leasing business is that forecast errors entail different costs.\nIdentifying effective ways to address this characteristic is the main objective\nof the paper. More specifically, the paper contributes to the literature\nthrough i) consolidating and integrating previous work in forecasting with\nasymmetric cost of error functions, ii) systematically evaluating previous\napproaches and comparing them to a new approach, and iii) demonstrating that\nforecasting with asymmetric cost of error functions enhances the quality of\ndecision support in car leasing. For example, under the assumption that the\ncosts of overestimating resale prices is twice that of the opposite error,\nincorporating corresponding cost asymmetry into forecast model development\nreduces decision costs by about eight percent, compared to a standard\nforecasting model. Higher asymmetry produces even larger improvements.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:20:40 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Dress", "Korbinian", ""], ["Lessmann", "Stefan", ""], ["von Mettenheim", "Hans-J\u00f6rg", ""]]}, {"id": "1707.02757", "submitter": "Damian Straszak", "authors": "Javad B. Ebrahimi and Damian Straszak and Nisheeth K. Vishnoi", "title": "Subdeterminant Maximization via Nonconvex Relaxations and\n  Anti-concentration", "comments": "in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several fundamental problems that arise in optimization and computer science\ncan be cast as follows: Given vectors $v_1,\\ldots,v_m \\in \\mathbb{R}^d$ and a\nconstraint family ${\\cal B}\\subseteq 2^{[m]}$, find a set $S \\in \\cal{B}$ that\nmaximizes the squared volume of the simplex spanned by the vectors in $S$. A\nmotivating example is the data-summarization problem in machine learning where\none is given a collection of vectors that represent data such as documents or\nimages. The volume of a set of vectors is used as a measure of their diversity,\nand partition or matroid constraints over $[m]$ are imposed in order to ensure\nresource or fairness constraints. Recently, Nikolov and Singh presented a\nconvex program and showed how it can be used to estimate the value of the most\ndiverse set when ${\\cal B}$ corresponds to a partition matroid. This result was\nrecently extended to regular matroids in works of Straszak and Vishnoi, and\nAnari and Oveis Gharan. The question of whether these estimation algorithms can\nbe converted into the more useful approximation algorithms -- that also output\na set -- remained open.\n  The main contribution of this paper is to give the first approximation\nalgorithms for both partition and regular matroids. We present novel\nformulations for the subdeterminant maximization problem for these matroids;\nthis reduces them to the problem of finding a point that maximizes the absolute\nvalue of a nonconvex function over a Cartesian product of probability\nsimplices. The technical core of our results is a new anti-concentration\ninequality for dependent random variables that allows us to relate the optimal\nvalue of these nonconvex functions to their value at a random point. Unlike\nprior work on the constrained subdeterminant maximization problem, our proofs\ndo not rely on real-stability or convexity and could be of independent interest\nboth in algorithms and complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:04:51 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 12:13:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ebrahimi", "Javad B.", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1707.02774", "submitter": "Slava Mikhaylov", "authors": "Alexander Baturo, Niheer Dasandi, Slava J. Mikhaylov", "title": "Understanding State Preferences With Text As Data: Introducing the UN\n  General Debate Corpus", "comments": null, "journal-ref": "Research & Politics, Volume 4, Issue 2, 2017", "doi": "10.1177/2053168017712821", "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year at the United Nations, member states deliver statements during the\nGeneral Debate discussing major issues in world politics. These speeches\nprovide invaluable information on governments' perspectives and preferences on\na wide range of issues, but have largely been overlooked in the study of\ninternational politics. This paper introduces a new dataset consisting of over\n7,701 English-language country statements from 1970-2016. We demonstrate how\nthe UN General Debate Corpus (UNGDC) can be used to derive country positions on\ndifferent policy dimensions using text analytic methods. The paper provides\napplications of these estimates, demonstrating the contribution the UNGDC can\nmake to the study of international politics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:40:12 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Baturo", "Alexander", ""], ["Dasandi", "Niheer", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1707.02780", "submitter": "Fabrice Rossi", "authors": "Marco Corneli (SAMM), Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Block modelling in dynamic networks with non-homogeneous Poisson\n  processes and exact ICL", "comments": null, "journal-ref": "Social Network Analysis and Mining, Springer, 2016, 6", "doi": "10.1007/s13278-016-0368-3", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model in which interactions between nodes of a dynamic network\nare counted by non homogeneous Poisson processes. In a block modelling\nperspective, nodes belong to hidden clusters (whose number is unknown) and the\nintensity functions of the counting processes only depend on the clusters of\nnodes. In order to make inference tractable we move to discrete time by\npartitioning the entire time horizon in which interactions are observed in\nfixed-length time sub-intervals. First, we derive an exact integrated\nclassification likelihood criterion and maximize it relying on a greedy search\napproach. This allows to estimate the memberships to clusters and the number of\nclusters simultaneously. Then a maximum-likelihood estimator is developed to\nestimate non parametrically the integrated intensities. We discuss the\nover-fitting problems of the model and propose a regularized version solving\nthese issues. Experiments on real and simulated data are carried out in order\nto assess the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:44:50 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Corneli", "Marco", "", "SAMM"], ["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1707.02796", "submitter": "Zackory Erickson", "authors": "Zackory Erickson, Sonia Chernova, and Charles C. Kemp", "title": "Semi-Supervised Haptic Material Recognition for Robots using Generative\n  Adversarial Networks", "comments": "11 pages, 6 figures, 6 tables, 1st Conference on Robot Learning (CoRL\n  2017)", "journal-ref": "PMLR 78:157-166 (2017)", "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material recognition enables robots to incorporate knowledge of material\nproperties into their interactions with everyday objects. For example, material\nrecognition opens up opportunities for clearer communication with a robot, such\nas \"bring me the metal coffee mug\", and recognizing plastic versus metal is\ncrucial when using a microwave or oven. However, collecting labeled training\ndata with a robot is often more difficult than unlabeled data. We present a\nsemi-supervised learning approach for material recognition that uses generative\nadversarial networks (GANs) with haptic features such as force, temperature,\nand vibration. Our approach achieves state-of-the-art results and enables a\nrobot to estimate the material class of household objects with ~90% accuracy\nwhen 92% of the training data are unlabeled. We explore how well this approach\ncan recognize the material of new objects and we discuss challenges facing\ngeneralization. To motivate learning from unlabeled training data, we also\ncompare results against several common supervised learning classifiers. In\naddition, we have released the dataset used for this work which consists of\ntime-series haptic measurements from a robot that conducted thousands of\ninteractions with 72 household objects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 11:04:42 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 21:39:48 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Erickson", "Zackory", ""], ["Chernova", "Sonia", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1707.02914", "submitter": "Xuehang Zheng", "authors": "Xuehang Zheng, Zening Lu, Saiprasad Ravishankar, Yong Long, Jeffrey A.\n  Fessler", "title": "Low Dose CT Image Reconstruction With Learned Sparsifying Transform", "comments": "This is a revised and corrected version of the IEEE IVMSP Workshop\n  paper DOI: 10.1109/IVMSPW.2016.7528219", "journal-ref": null, "doi": "10.1109/IVMSPW.2016.7528219", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in computed tomography (CT) is to reduce X-ray dose to a\nlow or even ultra-low level while maintaining the high quality of reconstructed\nimages. We propose a new method for CT reconstruction that combines penalized\nweighted-least squares reconstruction (PWLS) with regularization based on a\nsparsifying transform (PWLS-ST) learned from a dataset of numerous CT images.\nWe adopt an alternating algorithm to optimize the PWLS-ST cost function that\nalternates between a CT image update step and a sparse coding step. We adopt a\nrelaxed linearized augmented Lagrangian method with ordered-subsets (relaxed\nOS-LALM) to accelerate the CT image update step by reducing the number of\nforward and backward projections. Numerical experiments on the XCAT phantom\nshow that for low dose levels, the proposed PWLS-ST method dramatically\nimproves the quality of reconstructed images compared to PWLS reconstruction\nwith a nonadaptive edge-preserving regularizer (PWLS-EP).\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 15:42:05 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zheng", "Xuehang", ""], ["Lu", "Zening", ""], ["Ravishankar", "Saiprasad", ""], ["Long", "Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1707.02963", "submitter": "Wei Qian", "authors": "Wei Qian, Wending Li, Yasuhiro Sogawa, Ryohei Fujimaki, Xitong Yang,\n  Ji Liu", "title": "An Interactive Greedy Approach to Group Sparsity in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity learning with known grouping structure has received considerable\nattention due to wide modern applications in high-dimensional data analysis.\nAlthough advantages of using group information have been well-studied by\nshrinkage-based approaches, benefits of group sparsity have not been\nwell-documented for greedy-type methods, which much limits our understanding\nand use of this important class of methods. In this paper, generalizing from a\npopular forward-backward greedy approach, we propose a new interactive greedy\nalgorithm for group sparsity learning and prove that the proposed greedy-type\nalgorithm attains the desired benefits of group sparsity under high dimensional\nsettings. An estimation error bound refining other existing methods and a\nguarantee for group support recovery are also established simultaneously. In\naddition, we incorporate a general M-estimation framework and introduce an\ninteractive feature to allow extra algorithm flexibility without compromise in\ntheoretical properties. The promising use of our proposal is demonstrated\nthrough numerical evaluations including a real industrial application in human\nactivity recognition at home. Supplementary materials for this article are\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 17:48:28 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 15:29:29 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 04:52:25 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 17:42:56 GMT"}, {"version": "v5", "created": "Wed, 26 Sep 2018 21:31:35 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Qian", "Wei", ""], ["Li", "Wending", ""], ["Sogawa", "Yasuhiro", ""], ["Fujimaki", "Ryohei", ""], ["Yang", "Xitong", ""], ["Liu", "Ji", ""]]}, {"id": "1707.03003", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Emmanuel Bacry, Martin Bompaire, St\\'ephane Ga\\\"iffas, Soren Poulsen", "title": "Tick: a Python library for statistical learning, with a particular\n  emphasis on time-dependent modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tick is a statistical learning library for Python~3, with a particular\nemphasis on time-dependent models, such as point processes, and tools for\ngeneralized linear models and survival analysis. The core of the library is an\noptimization module providing model computational classes, solvers and proximal\noperators for regularization. tick relies on a C++ implementation and\nstate-of-the-art optimization algorithms to provide very fast computations in a\nsingle node multi-core setting. Source code and documentation can be downloaded\nfrom https://github.com/X-DataInitiative/tick\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:18:24 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 14:13:43 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Bompaire", "Martin", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Poulsen", "Soren", ""]]}, {"id": "1707.03010", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "St\\'ephane Ga\\\"iffas, Gustaw Matulewicz", "title": "Sparse inference of the drift of a high-dimensional Ornstein-Uhlenbeck\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the observation of a high-dimensional Ornstein-Uhlenbeck (OU) process\nin continuous time, we proceed to the inference of the drift parameter under a\nrow-sparsity assumption. Towards that aim, we consider the negative\nlog-likelihood of the process, penalized by an $\\ell^1$-penalization (Lasso and\nAdaptive Lasso). We provide both non-asymptotic and asymptotic results for this\nprocedure, by means of a sharp oracle inequality, and a limit theorem in the\nlong-time asymptotics, including asymptotic consistency for variable selection.\nAs a by-product, we point out the fact that for the Ornstein-Uhlenbeck process,\none does not need an assumption of restricted eigenvalue type in order to\nderive fast rates for the Lasso, while it is well-known to be mandatory for\nlinear regression for instance. Numerical results illustrate the benefits of\nthis penalized procedure compared to standard maximum likelihood approaches\nboth on simulations and real-world financial data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:33:02 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Ga\u00efffas", "St\u00e9phane", ""], ["Matulewicz", "Gustaw", ""]]}, {"id": "1707.03017", "submitter": "Ethan Perez", "authors": "Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron\n  Courville", "title": "Learning Visual Reasoning Without Strong Priors", "comments": "Full AAAI 2018 paper is at arXiv:1709.07871. Presented at ICML 2017's\n  Machine Learning in Speech and Language Processing Workshop. Code is at\n  http://github.com/ethanjperez/film", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:49:28 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 15:02:50 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 14:56:51 GMT"}, {"version": "v4", "created": "Wed, 4 Oct 2017 20:01:27 GMT"}, {"version": "v5", "created": "Mon, 18 Dec 2017 21:37:16 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.03134", "submitter": "Gautam Ramachandra", "authors": "Gautam Ramachandra", "title": "Least Square Variational Bayesian Autoencoder with Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years Variation Autoencoders have become one of the most popular\nunsupervised learning of complicated distributions.Variational Autoencoder\n(VAE) provides more efficient reconstructive performance over a traditional\nautoencoder. Variational auto enocders make better approximaiton than MCMC. The\nVAE defines a generative process in terms of ancestral sampling through a\ncascade of hidden stochastic layers. They are a directed graphic models.\nVariational autoencoder is trained to maximise the variational lower bound.\nHere we are trying maximise the likelihood and also at the same time we are\ntrying to make a good approximation of the data. Its basically trading of the\ndata log-likelihood and the KL divergence from the true posterior. This paper\ndescribes the scenario in which we wish to find a point-estimate to the\nparameters $\\theta$ of some parametric model in which we generate each\nobservations by first sampling a local latent variable and then sampling the\nassociated observation. Here we use least square loss function with\nregularization in the the reconstruction of the image, the least square loss\nfunction was found to give better reconstructed images and had a faster\ntraining time.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 05:24:01 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Ramachandra", "Gautam", ""]]}, {"id": "1707.03141", "submitter": "Nikhil Mishra", "authors": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel", "title": "A Simple Neural Attentive Meta-Learner", "comments": "iclr 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in\nthe task. In response, recent work in meta-learning proposes training a\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\nto novel but related tasks by learning a high-level strategy that captures the\nessence of the problem it is asked to solve. However, many recent meta-learning\napproaches are extensively hand-designed, either using architectures\nspecialized to a particular application, or hard-coding algorithmic components\nthat constrain how the meta-learner solves the task. We propose a class of\nsimple and generic meta-learner architectures that use a novel combination of\ntemporal convolutions and soft attention; the former to aggregate information\nfrom past experience and the latter to pinpoint specific pieces of information.\nIn the most extensive set of meta-learning experiments to date, we evaluate the\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by significant margins.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 06:21:31 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 16:08:03 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 04:55:20 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mishra", "Nikhil", ""], ["Rohaninejad", "Mostafa", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1707.03157", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja, Krzysztof Hajto and Jacek Tabor", "title": "Efficient mixture model for clustering of sparse high dimensional binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a mixture model, SparseMix, for clustering of sparse\nhigh dimensional binary data, which connects model-based with centroid-based\nclustering. Every group is described by a representative and a probability\ndistribution modeling dispersion from this representative. In contrast to\nclassical mixture models based on EM algorithm, SparseMix:\n  -is especially designed for the processing of sparse data,\n  -can be efficiently realized by an on-line Hartigan optimization algorithm,\n  -is able to automatically reduce unnecessary clusters.\n  We perform extensive experimental studies on various types of data, which\nconfirm that SparseMix builds partitions with higher compatibility with\nreference grouping than related methods. Moreover, constructed representatives\noften better reveal the internal structure of data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 07:48:57 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["\u015amieja", "Marek", ""], ["Hajto", "Krzysztof", ""], ["Tabor", "Jacek", ""]]}, {"id": "1707.03190", "submitter": "Yuanyuan Liu", "authors": "Yuanyuan Liu, Fanhua Shang, James Cheng", "title": "Accelerated Variance Reduced Stochastic ADMM", "comments": "16 pages, 5 figures, Appears in Proceedings of the 31th AAAI\n  Conference on Artificial Intelligence (AAAI), San Francisco, California, USA,\n  pp. 2287--2293, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many variance reduced stochastic alternating direction method of\nmultipliers (ADMM) methods (e.g.\\ SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have made\nexciting progress such as linear convergence rates for strongly convex\nproblems. However, the best known convergence rate for general convex problems\nis O(1/T) as opposed to O(1/T^2) of accelerated batch algorithms, where $T$ is\nthe number of iterations. Thus, there still remains a gap in convergence rates\nbetween existing stochastic ADMM and batch algorithms. To bridge this gap, we\nintroduce the momentum acceleration trick for batch optimization into the\nstochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an\naccelerated (ASVRG-ADMM) method. Then we design two different momentum term\nupdate rules for strongly convex and general convex cases. We prove that\nASVRG-ADMM converges linearly for strongly convex problems. Besides having a\nlow per-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM\nimproves the convergence rate on general convex problems from O(1/T) to\nO(1/T^2). Our experimental results show the effectiveness of ASVRG-ADMM.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:29:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Liu", "Yuanyuan", ""], ["Shang", "Fanhua", ""], ["Cheng", "James", ""]]}, {"id": "1707.03194", "submitter": "Jalal Fadili", "authors": "Jalal Fadili, J\\'er\\^ome Malick and Gabriel Peyr\\'e", "title": "Sensitivity Analysis for Mirror-Stratifiable Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:35:14 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 09:35:30 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 08:25:18 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Fadili", "Jalal", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1707.03269", "submitter": "Faris B Mismar", "authors": "Faris B. Mismar and Brian L. Evans", "title": "Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small\n  Cells", "comments": "(c) 2018 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2018 52nd Asilomar Conference on Signals, Systems, and Computers,\n  Pacific Grove, CA, USA, 2018, pp. 1-5", "doi": "10.1109/ACSSC.2018.8645168", "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning (RL) based closed loop power control\nalgorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an\nindoor environment served by small cells. The main contributions of our paper\nare to 1) use RL to solve performance tuning problems in an indoor cellular\nnetwork for voice bearers and 2) show that our derived lower bound loss in\neffective signal to interference plus noise ratio due to neighboring cell\nfailure is sufficient for VoLTE power control purposes in practical cellular\nnetworks. In our simulation, the proposed RL-based power control algorithm\nsignificantly improves both voice retainability and mean opinion score compared\nto current industry standards. The improvement is due to maintaining an\neffective downlink signal to interference plus noise ratio against adverse\nnetwork operational issues and faults.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 02:30:01 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 23:43:46 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 21:17:35 GMT"}, {"version": "v4", "created": "Mon, 5 Mar 2018 14:44:43 GMT"}, {"version": "v5", "created": "Mon, 30 Jul 2018 22:03:53 GMT"}, {"version": "v6", "created": "Wed, 7 Nov 2018 21:16:08 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Mismar", "Faris B.", ""], ["Evans", "Brian L.", ""]]}, {"id": "1707.03321", "submitter": "Stanislas Chambon", "authors": "Stanislas Chambon, Mathieu Galtier, Pierrick Arnal, Gilles Wainrib and\n  Alexandre Gramfort", "title": "A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep stage classification constitutes an important preliminary exam in the\ndiagnosis of sleep disorders. It is traditionally performed by a sleep expert\nwho assigns to each 30s of signal a sleep stage, based on the visual inspection\nof signals such as electroencephalograms (EEG), electrooculograms (EOG),\nelectrocardiograms (ECG) and electromyograms (EMG). We introduce here the first\ndeep learning approach for sleep stage classification that learns end-to-end\nwithout computing spectrograms or extracting hand-crafted features, that\nexploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,\nEMG and EOG), and that can exploit the temporal context of each 30s window of\ndata. For each modality the first layer learns linear spatial filters that\nexploit the array of sensors to increase the signal-to-noise ratio, and the\nlast layer feeds the learnt representation to a softmax classifier. Our model\nis compared to alternative automatic approaches based on convolutional networks\nor decisions trees. Results obtained on 61 publicly available PSG records with\nup to 20 EEG channels demonstrate that our network architecture yields\nstate-of-the-art performance. Our study reveals a number of insights on the\nspatio-temporal distribution of the signal of interest: a good trade-off for\noptimal classification performance measured with balanced accuracy is to use 6\nEEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one\nminute of data before and after each data segment offers the strongest\nimprovement when a limited number of channels is available. As sleep experts,\nour system exploits the multivariate and multimodal nature of PSG signals in\norder to deliver state-of-the-art classification performance with a small\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:29:36 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:37:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Chambon", "Stanislas", ""], ["Galtier", "Mathieu", ""], ["Arnal", "Pierrick", ""], ["Wainrib", "Gilles", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1707.03324", "submitter": "Zhiqiang Zhou", "authors": "Guanghui Lan and Zhiqiang Zhou", "title": "Dynamic Stochastic Approximation for Multi-stage Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multi-stage stochastic optimization problems with\nconvex objectives and conic constraints at each stage. We present a new\nstochastic first-order method, namely the dynamic stochastic approximation\n(DSA) algorithm, for solving these types of stochastic optimization problems.\nWe show that DSA can achieve an optimal ${\\cal O}(1/\\epsilon^4)$ rate of\nconvergence in terms of the total number of required scenarios when applied to\na three-stage stochastic optimization problem. We further show that this rate\nof convergence can be improved to ${\\cal O}(1/\\epsilon^2)$ when the objective\nfunction is strongly convex. We also discuss variants of DSA for solving more\ngeneral multi-stage stochastic optimization problems with the number of stages\n$T > 3$. The developed DSA algorithms only need to go through the scenario tree\nonce in order to compute an $\\epsilon$-solution of the multi-stage stochastic\noptimization problem. As a result, the memory required by DSA only grows\nlinearly with respect to the number of stages. To the best of our knowledge,\nthis is the first time that stochastic approximation type methods are\ngeneralized for multi-stage stochastic optimization with $T \\ge 3$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 15:29:55 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 22:30:13 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Zhiqiang", ""]]}, {"id": "1707.03334", "submitter": "Jun Sakuma", "authors": "Jun Sakuma, Tatsuya Osame", "title": "Recommendation with k-anonymized Ratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are widely used to predict personalized preferences of\ngoods or services using users' past activities, such as item ratings or\npurchase histories. If collections of such personal activities were made\npublicly available, they could be used to personalize a diverse range of\nservices, including targeted advertisement or recommendations. However, there\nwould be an accompanying risk of privacy violations. The pioneering work of\nNarayanan et al.\\ demonstrated that even if the identifiers are eliminated, the\npublic release of user ratings can allow for the identification of users by\nthose who have only a small amount of data on the users' past ratings.\n  In this paper, we assume the following setting. A collector collects user\nratings, then anonymizes and distributes them. A recommender constructs a\nrecommender system based on the anonymized ratings provided by the collector.\nBased on this setting, we exhaustively list the models of recommender systems\nthat use anonymized ratings. For each model, we then present an item-based\ncollaborative filtering algorithm for making recommendations based on\nanonymized ratings. Our experimental results show that an item-based\ncollaborative filtering based on anonymized ratings can perform better than\ncollaborative filterings based on 5--10 non-anonymized ratings. This surprising\nresult indicates that, in some settings, privacy protection does not\nnecessarily reduce the usefulness of recommendations. From the experimental\nanalysis of this counterintuitive result, we observed that the sparsity of the\nratings can be reduced by anonymization and the variance of the prediction can\nbe reduced if $k$, the anonymization parameter, is appropriately tuned. In this\nway, the predictive performance of recommendations based on anonymized ratings\ncan be improved in some settings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 07:21:25 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Sakuma", "Jun", ""], ["Osame", "Tatsuya", ""]]}, {"id": "1707.03340", "submitter": "Bao Wang", "authors": "Bao Wang, Duo Zhang, Duanhao Zhang, P.Jeffery Brantingham, Andrea L.\n  Bertozzi", "title": "Deep Learning for Real Time Crime Forecasting", "comments": "4 pages, 6 figures, NOLTA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real time crime prediction is a fundamental issue for public safety,\nbut remains a challenging problem for the scientific community. Crime\noccurrences depend on many complex factors. Compared to many predictable\nevents, crime is sparse. At different spatio-temporal scales, crime\ndistributions display dramatically different patterns. These distributions are\nof very low regularity in both space and time. In this work, we adapt the\nstate-of-the-art deep learning spatio-temporal predictor, ST-ResNet [Zhang et\nal, AAAI, 2017], to collectively predict crime distribution over the Los\nAngeles area. Our models are two staged. First, we preprocess the raw crime\ndata. This includes regularization in both space and time to enhance\npredictable signals. Second, we adapt hierarchical structures of residual\nconvolutional units to train multi-factor crime prediction models. Experiments\nover a half year period in Los Angeles reveal highly accurate predictive power\nof our models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 17:36:53 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Wang", "Bao", ""], ["Zhang", "Duo", ""], ["Zhang", "Duanhao", ""], ["Brantingham", "P. Jeffery", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1707.03360", "submitter": "Haozhe Shan", "authors": "Haozhe Shan, Peggy Mason", "title": "Unsupervised identification of rat behavioral motifs across timescales", "comments": "9 pages, 6 figures", "journal-ref": "NeurIPS 2020 Learning Meaningful Representations of Life (LMRL)\n  workshop", "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Behaviors of several laboratory animals can be modeled as sequences of\nstereotyped behaviors, or behavioral motifs. However, identifying such motifs\nis a challenging problem. Behaviors have a multi-scale structure: the animal\ncan be simultaneously performing a small-scale motif and a large-scale one\n(e.g. \\textit{chewing} and \\textit{feeding}). Motifs are compositional: a\nlarge-scale motif is a chain of smaller-scale ones, folded in (some behavioral)\nspace in a specific manner. We demonstrate an approach which captures these\nstructures, using rat locomotor data as an example. From the same dataset, we\nused a preprocessing procedure to create different versions, each describing\nmotifs of a different scale. We then trained several Hidden Markov Models\n(HMMs) in parallel, one for each dataset version. This approach essentially\nforced each HMM to learn motifs on a different scale, allowing us to capture\nbehavioral structures lost in previous approaches. By comparing HMMs with\nmodels representing different null hypotheses, we found that rat locomotion was\ncomposed of distinct motifs from second scale to minute scale. We found that\ntransitions between motifs were modulated by rats' location in the environment,\nleading to non-Markovian transitions. To test the ethological relevance of\nmotifs we discovered, we compared their usage between rats with differences in\na high-level trait, prosociality. We found that these rats had distinct motif\nrepertoires, suggesting that motif usage statistics can be used to infer\ninternal states of rats. Our method is therefore an efficient way to discover\nmulti-scale, compositional structures in animal behaviors. It may also be\napplied as a sensitive assay for internal states.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 16:55:48 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 16:13:48 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 14:51:26 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Shan", "Haozhe", ""], ["Mason", "Peggy", ""]]}, {"id": "1707.03372", "submitter": "Stephen Mussmann", "authors": "Stephen Mussmann, Daniel Levy, Stefano Ermon", "title": "Fast Amortized Inference and Learning in Log-linear Models with Randomly\n  Perturbed Nearest Neighbor Search", "comments": "In UAI proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in log-linear models scales linearly in the size of output space in\nthe worst-case. This is often a bottleneck in natural language processing and\ncomputer vision tasks when the output space is feasibly enumerable but very\nlarge. We propose a method to perform inference in log-linear models with\nsublinear amortized cost. Our idea hinges on using Gumbel random variable\nperturbations and a pre-computed Maximum Inner Product Search data structure to\naccess the most-likely elements in sublinear amortized time. Our method yields\nprovable runtime and accuracy guarantees. Further, we present empirical\nexperiments on ImageNet and Word Embeddings showing significant speedups for\nsampling, inference, and learning in log-linear models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:23:10 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Mussmann", "Stephen", ""], ["Levy", "Daniel", ""], ["Ermon", "Stefano", ""]]}, {"id": "1707.03383", "submitter": "Christopher Beckham", "authors": "Christopher Beckham and Christopher Pal", "title": "A step towards procedural terrain generation with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural terrain generation for video games has been traditionally been\ndone with smartly designed but handcrafted algorithms that generate heightmaps.\nWe propose a first step toward the learning and synthesis of these using recent\nadvances in deep generative modelling with openly available satellite imagery\nfrom NASA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:44:20 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Beckham", "Christopher", ""], ["Pal", "Christopher", ""]]}, {"id": "1707.03384", "submitter": "Sebastian D\\\"orner", "authors": "Sebastian D\\\"orner, Sebastian Cammerer, Jakob Hoydis, Stephan ten\n  Brink", "title": "Deep Learning-Based Communication Over the Air", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2017.2784180", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning of communications systems is a fascinating novel concept\nthat has so far only been validated by simulations for block-based\ntransmissions. It allows learning of transmitter and receiver implementations\nas deep neural networks (NNs) that are optimized for an arbitrary\ndifferentiable end-to-end performance metric, e.g., block error rate (BLER). In\nthis paper, we demonstrate that over-the-air transmissions are possible: We\nbuild, train, and run a complete communications system solely composed of NNs\nusing unsynchronized off-the-shelf software-defined radios (SDRs) and\nopen-source deep learning (DL) software libraries. We extend the existing ideas\ntowards continuous data transmission which eases their current restriction to\nshort block lengths but also entails the issue of receiver synchronization. We\novercome this problem by introducing a frame synchronization module based on\nanother NN. A comparison of the BLER performance of the \"learned\" system with\nthat of a practical baseline shows competitive performance close to 1 dB, even\nwithout extensive hyperparameter tuning. We identify several practical\nchallenges of training such a system over actual channels, in particular the\nmissing channel gradient, and propose a two-step learning procedure based on\nthe idea of transfer learning that circumvents this issue.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:47:23 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["D\u00f6rner", "Sebastian", ""], ["Cammerer", "Sebastian", ""], ["Hoydis", "Jakob", ""], ["Brink", "Stephan ten", ""]]}, {"id": "1707.03386", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Gautam Dasarathy, Richard G. Baraniuk", "title": "DeepCodec: Adaptive Sensing and Recovery via Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a novel computational sensing framework for sensing\nand recovering structured signals. When trained on a set of representative\nsignals, our framework learns to take undersampled measurements and recover\nsignals from them using a deep convolutional neural network. In other words, it\nlearns a transformation from the original signals to a near-optimal number of\nundersampled measurements and the inverse transformation from measurements to\nsignals. This is in contrast to traditional compressive sensing (CS) systems\nthat use random linear measurements and convex optimization or iterative\nalgorithms for signal recovery. We compare our new framework with\n$\\ell_1$-minimization from the phase transition point of view and demonstrate\nthat it outperforms $\\ell_1$-minimization in the regions of phase transition\nplot where $\\ell_1$-minimization cannot recover the exact solution. In\naddition, we experimentally demonstrate how learning measurements enhances the\noverall recovery performance, speeds up training of recovery framework, and\nleads to having fewer parameters to learn.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:49:20 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Mousavi", "Ali", ""], ["Dasarathy", "Gautam", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1707.03389", "submitter": "Irina Higgins", "authors": "Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P\n  Burgess, Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis,\n  Alexander Lerchner", "title": "SCAN: Learning Hierarchical Compositional Visual Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly infinite diversity of the natural world arises from a\nrelatively small set of coherent rules, such as the laws of physics or\nchemistry. We conjecture that these rules give rise to regularities that can be\ndiscovered through primarily unsupervised experiences and represented as\nabstract concepts. If such representations are compositional and hierarchical,\nthey can be recombined into an exponentially large set of new concepts. This\npaper describes SCAN (Symbol-Concept Association Network), a new framework for\nlearning such abstractions in the visual domain. SCAN learns concepts through\nfast symbol association, grounding them in disentangled visual primitives that\nare discovered in an unsupervised manner. Unlike state of the art multimodal\ngenerative model baselines, our approach requires very few pairings between\nsymbols and images and makes no assumptions about the form of symbol\nrepresentations. Once trained, SCAN is capable of multimodal bi-directional\ninference, generating a diverse set of image samples from symbolic descriptions\nand vice versa. It also allows for traversal and manipulation of the implicit\nhierarchy of visual concepts through symbolic instructions and learnt logical\nrecombination operations. Such manipulations enable SCAN to break away from its\ntraining data distribution and imagine novel visual concepts through\nsymbolically instructed recombination of previously learnt concepts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:58:45 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:46:31 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 17:25:01 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Higgins", "Irina", ""], ["Sonnerat", "Nicolas", ""], ["Matthey", "Loic", ""], ["Pal", "Arka", ""], ["Burgess", "Christopher P", ""], ["Bosnjak", "Matko", ""], ["Shanahan", "Murray", ""], ["Botvinick", "Matthew", ""], ["Hassabis", "Demis", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1707.03426", "submitter": "Niloofar Yousefi", "authors": "Niloofar Yousefi, Cong Li, Mansooreh Mollaghasemi, Georgios\n  Anagnostopoulos and Michael Georgiopoulos", "title": "Multi-Task Learning Using Neighborhood Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new and effective algorithm for learning kernels in a\nMulti-Task Learning (MTL) setting. Although, we consider a MTL scenario here,\nour approach can be easily applied to standard single task learning, as well.\nAs shown by our empirical results, our algorithm consistently outperforms the\ntraditional kernel learning algorithms such as uniform combination solution,\nconvex combinations of base kernels as well as some kernel alignment-based\nmodels, which have been proven to give promising results in the past. We\npresent a Rademacher complexity bound based on which a new Multi-Task Multiple\nKernel Learning (MT-MKL) model is derived. In particular, we propose a Support\nVector Machine-regularized model in which, for each task, an optimal kernel is\nlearned based on a neighborhood-defining kernel that is not restricted to be\npositive semi-definite. Comparative experimental results are showcased that\nunderline the merits of our neighborhood-defining framework in both\nclassification and regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 18:43:41 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Yousefi", "Niloofar", ""], ["Li", "Cong", ""], ["Mollaghasemi", "Mansooreh", ""], ["Anagnostopoulos", "Georgios", ""], ["Georgiopoulos", "Michael", ""]]}, {"id": "1707.03450", "submitter": "Cristobal Silva", "authors": "Iv\\'an Castro, Crist\\'obal Silva, Felipe Tobar", "title": "Initialising Kernel Adaptive Filters via Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic framework for both (i) determining the initial\nsettings of kernel adaptive filters (KAFs) and (ii) constructing fully-adaptive\nKAFs whereby in addition to weights and dictionaries, kernel parameters are\nlearnt sequentially. This is achieved by formulating the estimator as a\nprobabilistic model and defining dedicated prior distributions over the kernel\nparameters, weights and dictionary, enforcing desired properties such as\nsparsity. The model can then be trained using a subset of data to initialise\nstandard KAFs or updated sequentially each time a new observation becomes\navailable. Due to the nonlinear/non-Gaussian properties of the model, learning\nand inference is achieved using gradient-based maximum-a-posteriori\noptimisation and Markov chain Monte Carlo methods, and can be confidently used\nto compute predictions. The proposed framework was validated on nonlinear time\nseries of both synthetic and real-world nature, where it outperformed standard\nKAFs in terms of mean square error and the sparsity of the learnt dictionaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 20:03:31 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Castro", "Iv\u00e1n", ""], ["Silva", "Crist\u00f3bal", ""], ["Tobar", "Felipe", ""]]}, {"id": "1707.03490", "submitter": "Slava Mikhaylov", "authors": "Stefano Gurciullo and Slava Mikhaylov", "title": "Detecting Policy Preferences and Dynamics in the UN General Debate with\n  Neural Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreign policy analysis has been struggling to find ways to measure policy\npreferences and paradigm shifts in international political systems. This paper\npresents a novel, potential solution to this challenge, through the application\nof a neural word embedding (Word2vec) model on a dataset featuring speeches by\nheads of state or government in the United Nations General Debate. The paper\nprovides three key contributions based on the output of the Word2vec model.\nFirst, it presents a set of policy attention indices, synthesizing the semantic\nproximity of political speeches to specific policy themes. Second, it\nintroduces country-specific semantic centrality indices, based on topological\nanalyses of countries' semantic positions with respect to each other. Third, it\ntests the hypothesis that there exists a statistical relation between the\nsemantic content of political speeches and UN voting behavior, falsifying it\nand suggesting that political speeches contain information of different nature\nthen the one behind voting outcomes. The paper concludes with a discussion of\nthe practical use of its results and consequences for foreign policy analysis,\npublic accountability, and transparency.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:16:20 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Gurciullo", "Stefano", ""], ["Mikhaylov", "Slava", ""]]}, {"id": "1707.03494", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy, Akhilesh Gotmare, Martin Jaggi", "title": "Unsupervised robust nonparametric learning of hidden community\n  properties", "comments": "Experiments with new types of adversaries added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning of fundamental properties of communities in large noisy\nnetworks, in the prototypical situation where the nodes or users are split into\ntwo classes according to a binary property, e.g., according to their opinions\nor preferences on a topic. For learning these properties, we propose a\nnonparametric, unsupervised, and scalable graph scan procedure that is, in\naddition, robust against a class of powerful adversaries. In our setup, one of\nthe communities can fall under the influence of a knowledgeable adversarial\nleader, who knows the full network structure, has unlimited computational\nresources and can completely foresee our planned actions on the network. We\nprove strong consistency of our results in this setup with minimal assumptions.\nIn particular, the learning procedure estimates the baseline activity of normal\nusers asymptotically correctly with probability 1; the only assumption being\nthe existence of a single implicit community of asymptotically negligible\nlogarithmic size. We provide experiments on real and synthetic data to\nillustrate the performance of our method, including examples with adversaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:28:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 16:22:55 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Gotmare", "Akhilesh", ""], ["Jaggi", "Martin", ""]]}, {"id": "1707.03530", "submitter": "Bradley Price", "authors": "Bradley S. Price and Ben Sherwood", "title": "A Cluster Elastic Net for Multivariate Regression", "comments": "37 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating coefficients in multivariate regression\nwhen there is a clustering structure to the response variables. The proposed\nmethod includes a fusion penalty, to shrink the difference in fitted values\nfrom responses in the same cluster, and an L1 penalty for simultaneous variable\nselection and estimation. The method can be used when the grouping structure of\nthe response variables is known or unknown. When the clustering structure is\nunknown the method will simultaneously estimate the clusters of the response\nand the regression coefficients. Theoretical results are presented for the\npenalized least squares case, including asymptotic results allowing for p >> n.\nWe extend our method to the setting where the responses are binomial variables.\nWe propose a coordinate descent algorithm for both the normal and binomial\nlikelihood, which can easily be extended to other generalized linear model\n(GLM) settings. Simulations and data examples from business operations and\ngenomics are presented to show the merits of both the least squares and\nbinomial methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 03:50:38 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 14:23:10 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Price", "Bradley S.", ""], ["Sherwood", "Ben", ""]]}, {"id": "1707.03538", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Faicel Chamroukhi", "title": "An Introduction to the Practical and Theoretical Aspects of\n  Mixture-of-Experts Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture-of-experts (MoE) models are a powerful paradigm for modeling of data\narising from complex data generating processes (DGPs). In this article, we\ndemonstrate how different MoE models can be constructed to approximate the\nunderlying DGPs of arbitrary types of data. Due to the probabilistic nature of\nMoE models, we propose the maximum quasi-likelihood (MQL) estimator as a method\nfor estimating MoE model parameters from data, and we provide conditions under\nwhich MQL estimators are consistent and asymptotically normal. The blockwise\nminorization-maximizatoin (blockwise-MM) algorithm framework is proposed as an\nall-purpose method for constructing algorithms for obtaining MQL estimators. An\nexample derivation of a blockwise-MM algorithm is provided. We then present a\nmethod for constructing information criteria for estimating the number of\ncomponents in MoE models and provide justification for the classic Bayesian\ninformation criterion (BIC). We explain how MoE models can be used to conduct\nclassification, clustering, and regression and we illustrate these applications\nvia a pair of worked examples.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 04:44:14 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Chamroukhi", "Faicel", ""]]}, {"id": "1707.03663", "submitter": "Niladri Chatterji", "authors": "Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett and Michael I.\n  Jordan", "title": "Underdamped Langevin MCMC: A non-asymptotic analysis", "comments": "23 pages; Correction to Corollary 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the underdamped Langevin diffusion when the log of the target\ndistribution is smooth and strongly concave. We present a MCMC algorithm based\non its discretization and show that it achieves $\\varepsilon$ error (in\n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a\nsignificant improvement over the best known rate for overdamped Langevin MCMC,\nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same\nsmoothness/concavity assumptions.\n  The underdamped Langevin MCMC scheme can be viewed as a version of\nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped\nLangevin MCMC methods in a number of application areas. We provide quantitative\nrates that support this empirical wisdom.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 12:08:55 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 04:23:36 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 20:55:26 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 20:06:25 GMT"}, {"version": "v5", "created": "Mon, 1 Jan 2018 23:24:29 GMT"}, {"version": "v6", "created": "Tue, 16 Jan 2018 19:28:32 GMT"}, {"version": "v7", "created": "Fri, 26 Jan 2018 21:56:51 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cheng", "Xiang", ""], ["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.03815", "submitter": "Aleksandar Bojchevski", "authors": "Aleksandar Bojchevski, Stephan G\\\"unnemann", "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via\n  Ranking", "comments": "Updated: ICLR 2018 camera-ready version", "journal-ref": "International Conference on Learning Representations, ICLR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:54:04 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 17:04:01 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 13:32:40 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 10:20:09 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bojchevski", "Aleksandar", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1707.03821", "submitter": "David Tolpin", "authors": "Michael Dymshits, Ben Myara, David Tolpin", "title": "Process Monitoring on Sequences of System Call Count Vectors", "comments": "5 pages, 4 figures, ICCST 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for efficient monitoring of processes running on\nhosts in a corporate network. The methodology is based on collecting streams of\nsystem calls produced by all or selected processes on the hosts, and sending\nthem over the network to a monitoring server, where machine learning algorithms\nare used to identify changes in process behavior due to malicious activity,\nhardware failures, or software errors. The methodology uses a sequence of\nsystem call count vectors as the data format which can handle large and varying\nvolumes of data.\n  Unlike previous approaches, the methodology introduced in this paper is\nsuitable for distributed collection and processing of data in large corporate\nnetworks. We evaluate the methodology both in a laboratory setting on a\nreal-life setup and provide statistics characterizing performance and accuracy\nof the methodology.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 13:14:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Dymshits", "Michael", ""], ["Myara", "Ben", ""], ["Tolpin", "David", ""]]}, {"id": "1707.03854", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Greg Valiant, James Zou", "title": "Estimating the unseen from multiple populations", "comments": "13 pages, 3 figures, appearing at the International Conference on\n  Machine Learning 2017 (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from a distribution, how many new elements should we expect to\nfind if we continue sampling this distribution? This is an important and\nactively studied problem, with many applications ranging from unseen species\nestimation to genomics. We generalize this extrapolation and related unseen\nestimation problems to the multiple population setting, where population $j$\nhas an unknown distribution $D_j$ from which we observe $n_j$ samples. We\nderive an optimal estimator for the total number of elements we expect to find\namong new samples across the populations. Surprisingly, we prove that our\nestimator's accuracy is independent of the number of populations. We also\ndevelop an efficient optimization algorithm to solve the more general problem\nof estimating multi-population frequency distributions. We validate our methods\nand theory through extensive experiments. Finally, on a real dataset of human\ngenomes across multiple ancestries, we demonstrate how our approach for unseen\nestimation can enable cohort designs that can discover interesting mutations\nwith greater efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:26:19 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Valiant", "Greg", ""], ["Zou", "James", ""]]}, {"id": "1707.03858", "submitter": "Netanel Raviv", "authors": "Netanel Raviv, Itzhak Tamo, Rashish Tandon, Alexandros G. Dimakis", "title": "Gradient Coding from Cyclic MDS Codes and Expander Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient coding is a technique for straggler mitigation in distributed\nlearning. In this paper we design novel gradient codes using tools from\nclassical coding theory, namely, cyclic MDS codes, which compare favorably with\nexisting solutions, both in the applicable range of parameters and in the\ncomplexity of the involved algorithms. Second, we introduce an approximate\nvariant of the gradient coding problem, in which we settle for approximate\ngradient computation instead of the exact one. This approach enables graceful\ndegradation, i.e., the $\\ell_2$ error of the approximate gradient is a\ndecreasing function of the number of stragglers. Our main result is that\nnormalized adjacency matrices of expander graphs yield excellent approximate\ngradient codes, which enable significantly less computation compared to exact\ngradient coding, and guarantee faster convergence than trivial solutions under\nstandard assumptions. We experimentally test our approach on Amazon EC2, and\nshow that the generalization error of approximate gradient coding is very close\nto the full gradient while requiring significantly less computation from the\nworkers.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:38:54 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 15:40:52 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 17:35:14 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Raviv", "Netanel", ""], ["Tamo", "Itzhak", ""], ["Tandon", "Rashish", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1707.03897", "submitter": "Chavent Marie", "authors": "Marie Chavent and Vanessa Kuentz-Simonet and Amaury Labenne and\n  J\\'er\\^ome Saracco", "title": "ClustGeo: an R package for hierarchical clustering with spatial\n  constraints", "comments": null, "journal-ref": null, "doi": "10.1007/s00180-018-0791-1", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Ward-like hierarchical clustering algorithm\nincluding spatial/geographical constraints. Two dissimilarity matrices $D_0$\nand $D_1$ are inputted, along with a mixing parameter $\\alpha \\in [0,1]$. The\ndissimilarities can be non-Euclidean and the weights of the observations can be\nnon-uniform. The first matrix gives the dissimilarities in the \"feature space\"\nand the second matrix gives the dissimilarities in the \"constraint space\". The\ncriterion minimized at each stage is a convex combination of the homogeneity\ncriterion calculated with $D_0$ and the homogeneity criterion calculated with\n$D_1$. The idea is then to determine a value of $\\alpha$ which increases the\nspatial contiguity without deteriorating too much the quality of the solution\nbased on the variables of interest i.e. those of the feature space. This\nprocedure is illustrated on a real dataset using the R package ClustGeo.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:29:59 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 10:50:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Chavent", "Marie", ""], ["Kuentz-Simonet", "Vanessa", ""], ["Labenne", "Amaury", ""], ["Saracco", "J\u00e9r\u00f4me", ""]]}, {"id": "1707.03905", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Artem Papanov", "title": "Influence of Resampling on Accuracy of Imbalanced Classification", "comments": "5 pages, 2 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE9875, 2015", "doi": "10.1117/12.2228523", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world binary classification tasks (e.g. detection of certain\nobjects from images), an available dataset is imbalanced, i.e., it has much\nless representatives of a one class (a minor class), than of another.\nGenerally, accurate prediction of the minor class is crucial but it's hard to\nachieve since there is not much information about the minor class. One approach\nto deal with this problem is to preliminarily resample the dataset, i.e., add\nnew elements to the dataset or remove existing ones. Resampling can be done in\nvarious ways which raises the problem of choosing the most appropriate one. In\nthis paper we experimentally investigate impact of resampling on classification\naccuracy, compare resampling methods and highlight key points and difficulties\nof resampling.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:55:22 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""]]}, {"id": "1707.03909", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Dmitry Smolyakov", "title": "Model Selection for Anomaly Detection", "comments": "6 pages, 3 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE 9875, 2015", "doi": "10.1117/12.2228794", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection based on one-class classification algorithms is broadly\nused in many applied domains like image processing (e.g. detection of whether a\npatient is \"cancerous\" or \"healthy\" from mammography image), network intrusion\ndetection, etc. Performance of an anomaly detection algorithm crucially depends\non a kernel, used to measure similarity in a feature space. The standard\napproaches (e.g. cross-validation) for kernel selection, used in two-class\nclassification problems, can not be used directly due to the specific nature of\na data (absence of a second, abnormal, class data). In this paper we generalize\nseveral kernel selection methods from binary-class case to the case of\none-class classification and perform extensive comparison of these approaches\nusing both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:03:36 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Smolyakov", "Dmitry", ""]]}, {"id": "1707.03916", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Alexey Zaytsev", "title": "Large Scale Variable Fidelity Surrogate Modeling", "comments": "21 pages, 4 figures, Ann Math Artif Intell (2017)", "journal-ref": null, "doi": "10.1007/s10472-017-9545-y", "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineers widely use Gaussian process regression framework to construct\nsurrogate models aimed to replace computationally expensive physical models\nwhile exploring design space. Thanks to Gaussian process properties we can use\nboth samples generated by a high fidelity function (an expensive and accurate\nrepresentation of a physical phenomenon) and a low fidelity function (a cheap\nand coarse approximation of the same physical phenomenon) while constructing a\nsurrogate model. However, if samples sizes are more than few thousands of\npoints, computational costs of the Gaussian process regression become\nprohibitive both in case of learning and in case of prediction calculation. We\npropose two approaches to circumvent this computational burden: one approach is\nbased on the Nystr\\\"om approximation of sample covariance matrices and another\nis based on an intelligent usage of a blackbox that can evaluate a~low fidelity\nfunction on the fly at any point of a design space. We examine performance of\nthe proposed approaches using a number of artificial and real problems,\nincluding engineering optimization of a rotating disk shape.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:21:32 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Zaytsev", "Alexey", ""]]}, {"id": "1707.04025", "submitter": "Marco Loog", "authors": "Marco Loog, Jesse H. Krijthe, Are C. Jensen", "title": "On Measuring and Quantifying Performance: Error Rates, Surrogate Loss,\n  and an Example in SSL", "comments": null, "journal-ref": "In Handbook of Pattern Recognition and Computer Vision (pp. 53-68)\n  (2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various approaches to learning, notably in domain adaptation, active\nlearning, learning under covariate shift, semi-supervised learning, learning\nwith concept drift, and the like, one often wants to compare a baseline\nclassifier to one or more advanced (or at least different) strategies. In this\nchapter, we basically argue that if such classifiers, in their respective\ntraining phases, optimize a so-called surrogate loss that it may also be\nvaluable to compare the behavior of this loss on the test set, next to the\nregular classification error rate. It can provide us with an additional view on\nthe classifiers' relative performances that error rates cannot capture. As an\nexample, limited but convincing empirical results demonstrates that we may be\nable to find semi-supervised learning strategies that can guarantee performance\nimprovements with increasing numbers of unlabeled data in terms of\nlog-likelihood. In contrast, the latter may be impossible to guarantee for the\nclassification error rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 08:29:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Loog", "Marco", ""], ["Krijthe", "Jesse H.", ""], ["Jensen", "Are C.", ""]]}, {"id": "1707.04035", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, Aurelio\n  Uncini", "title": "Kafnets: kernel-based non-parametric activation functions for neural\n  networks", "comments": "Preprint submitted to Neural Networks (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are generally built by interleaving (adaptable) linear layers\nwith (fixed) nonlinear activation functions. To increase their flexibility,\nseveral authors have proposed methods for adapting the activation functions\nthemselves, endowing them with varying degrees of flexibility. None of these\napproaches, however, have gained wide acceptance in practice, and research in\nthis topic remains open. In this paper, we introduce a novel family of flexible\nactivation functions that are based on an inexpensive kernel expansion at every\nneuron. Leveraging over several properties of kernel-based models, we propose\nmultiple variations for designing and initializing these kernel activation\nfunctions (KAFs), including a multidimensional scheme allowing to nonlinearly\ncombine information from different paths in the network. The resulting KAFs can\napproximate any mapping defined over a subset of the real line, either convex\nor nonconvex. Furthermore, they are smooth over their entire domain, linear in\ntheir parameters, and they can be regularized using any known scheme, including\nthe use of $\\ell_1$ penalties to enforce sparseness. To the best of our\nknowledge, no other known model satisfies all these properties simultaneously.\nIn addition, we provide a relatively complete overview on alternative\ntechniques for adapting the activation functions, which is currently lacking in\nthe literature. A large set of experiments validates our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:22:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 11:33:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Totaro", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1707.04067", "submitter": "Snehasis Banerjee", "authors": "Snehasis Banerjee, Tanushyam Chattopadhyay, Arpan Pal, Utpal Garain", "title": "Automation of Feature Engineering for IoT Analytics", "comments": "AIoTAS Workshop, ISCA 2017. To be published in ACM SIGBED Review 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for automation of interpretable feature\nselection for Internet Of Things Analytics (IoTA) using machine learning (ML)\ntechniques. Authors have conducted a survey over different people involved in\ndifferent IoTA based application development tasks. The survey reveals that\nfeature selection is the most time consuming and niche skill demanding part of\nthe entire workflow. This paper shows how feature selection is successfully\nautomated without sacrificing the decision making accuracy and thereby reducing\nthe project completion time and cost of hiring expensive resources. Several\npattern recognition principles and state of art (SoA) ML techniques are\nfollowed to design the overall approach for the proposed automation. Three data\nsets are considered to establish the proof-of-concept. Experimental results\nshow that the proposed automation is able to reduce the time for feature\nselection to $2$ days instead of $4-6$ months which would have been required in\nabsence of the automation. This reduction in time is achieved without any\nsacrifice in the accuracy of the decision making process. Proposed method is\nalso compared against Multi Layer Perceptron (MLP) model as most of the state\nof the art works on IoTA uses MLP based Deep Learning. Moreover the feature\nselection method is compared against SoA feature reduction technique namely\nPrincipal Component Analysis (PCA) and its variants. The results obtained show\nthat the proposed method is effective.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 11:14:54 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Banerjee", "Snehasis", ""], ["Chattopadhyay", "Tanushyam", ""], ["Pal", "Arpan", ""], ["Garain", "Utpal", ""]]}, {"id": "1707.04114", "submitter": "Simon Lee Dettmer", "authors": "Simon Lee Dettmer and Johannes Berg", "title": "Inferring the parameters of a Markov process from snapshots of the\n  steady state", "comments": "12 pages, 8 figures", "journal-ref": "J. Stat. Mech. (2018) 023403", "doi": "10.1088/1742-5468/aaa8ea", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn math.PR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to infer the parameters of an ergodic Markov process from samples\ntaken independently from the steady state. Our focus is on non-equilibrium\nprocesses, where the steady state is not described by the Boltzmann measure,\nbut is generally unknown and hard to compute, which prevents the application of\nestablished equilibrium inference methods. We propose a quantity we call\npropagator likelihood, which takes on the role of the likelihood in equilibrium\nprocesses. This propagator likelihood is based on fictitious transitions\nbetween those configurations of the system which occur in the samples. The\npropagator likelihood can be derived by minimising the relative entropy between\nthe empirical distribution and a distribution generated by propagating the\nempirical distribution forward in time. Maximising the propagator likelihood\nleads to an efficient reconstruction of the parameters of the underlying model\nin different systems, both with discrete configurations and with continuous\nconfigurations. We apply the method to non-equilibrium models from statistical\nphysics and theoretical biology, including the asymmetric simple exclusion\nprocess (ASEP), the kinetic Ising model, and replicator dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:31:20 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 06:05:47 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 20:51:56 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Dettmer", "Simon Lee", ""], ["Berg", "Johannes", ""]]}, {"id": "1707.04131", "submitter": "Jonas Rauber", "authors": "Jonas Rauber, Wieland Brendel, Matthias Bethge", "title": "Foolbox: A Python toolbox to benchmark the robustness of machine\n  learning models", "comments": "Code and examples available at https://github.com/bethgelab/foolbox\n  and documentation available at http://foolbox.readthedocs.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even todays most advanced machine learning models are easily fooled by almost\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\ngenerate such adversarial perturbations and to quantify and compare the\nrobustness of machine learning models. It is build around the idea that the\nmost comparable robustness measure is the minimum perturbation needed to craft\nan adversarial example. To this end, Foolbox provides reference implementations\nof most published adversarial attack methods alongside some new ones, all of\nwhich perform internal hyperparameter tuning to find the minimum adversarial\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\ndifferent adversarial criteria such as targeted misclassification and top-k\nmisclassification as well as different distance measures. The code is licensed\nunder the MIT license and is openly available at\nhttps://github.com/bethgelab/foolbox . The most up-to-date documentation can be\nfound at http://foolbox.readthedocs.io .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:59:15 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:22:01 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:10:10 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Rauber", "Jonas", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1707.04175", "submitter": "Razvan Pascanu", "authors": "Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan,\n  James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu", "title": "Distral: Robust Multitask Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep reinforcement learning algorithms are data inefficient in complex\nand rich environments, limiting their applicability to many scenarios. One\ndirection for improving data efficiency is multitask learning with shared\nneural network parameters, where efficiency may be improved through transfer\nacross related tasks. In practice, however, this is not usually observed,\nbecause gradients from different tasks can interfere negatively, making\nlearning unstable and sometimes even less data efficient. Another issue is the\ndifferent reward schemes between tasks, which can easily lead to one task\ndominating the learning of a shared model. We propose a new approach for joint\ntraining of multiple tasks, which we refer to as Distral (Distill & transfer\nlearning). Instead of sharing parameters between the different workers, we\npropose to share a \"distilled\" policy that captures common behaviour across\ntasks. Each worker is trained to solve its own task while constrained to stay\nclose to the shared policy, while the shared policy is trained by distillation\nto be the centroid of all task policies. Both aspects of the learning process\nare derived by optimizing a joint objective function. We show that our approach\nsupports efficient transfer on complex 3D environments, outperforming several\nrelated methods. Moreover, the proposed learning process is more robust and\nmore stable---attributes that are critical in deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 15:24:20 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Teh", "Yee Whye", ""], ["Bapst", "Victor", ""], ["Czarnecki", "Wojciech Marian", ""], ["Quan", "John", ""], ["Kirkpatrick", "James", ""], ["Hadsell", "Raia", ""], ["Heess", "Nicolas", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1707.04191", "submitter": "Nikitas Rontsis", "authors": "Nikitas Rontsis, Michael A. Osborne, Paul J. Goulart", "title": "Distributionally Ambiguous Optimization Techniques for Batch Bayesian\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, theoretically-grounded, acquisition function for Batch\nBayesian optimization informed by insights from distributionally ambiguous\noptimization. Our acquisition function is a lower bound on the well-known\nExpected Improvement function, which requires evaluation of a Gaussian\nExpectation over a multivariate piecewise affine function. Our bound is\ncomputed instead by evaluating the best-case expectation over all probability\ndistributions consistent with the same mean and variance as the original\nGaussian distribution. Unlike alternative approaches, including Expected\nImprovement, our proposed acquisition function avoids multi-dimensional\nintegrations entirely, and can be computed exactly - even on large batch sizes\n- as the solution of a tractable convex optimization problem. Our suggested\nacquisition function can also be optimized efficiently, since first and second\nderivative information can be calculated inexpensively as by-products of the\nacquisition function calculation itself. We derive various novel theorems that\nground our work theoretically and we demonstrate superior performance via\nsimple motivating examples, benchmark functions and real-world problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:04:14 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 11:23:32 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 12:33:38 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 16:08:40 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Rontsis", "Nikitas", ""], ["Osborne", "Michael A.", ""], ["Goulart", "Paul J.", ""]]}, {"id": "1707.04218", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Learning Features from Co-occurrences: A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a word by its co-occurrences with other words in context is an\neffective way to capture the meaning of the word. However, the theory behind\nremains a challenge. In this work, taking the example of a word classification\ntask, we give a theoretical analysis of the approaches that represent a word X\nby a function f(P(C|X)), where C is a context feature, P(C|X) is the\nconditional probability estimated from a text corpus, and the function f maps\nthe co-occurrence measure to a prediction score. We investigate the impact of\ncontext feature C and the function f. We also explain the reasons why using the\nco-occurrences with multiple context features may be better than just using a\nsingle one. In addition, some of the results shed light on the theory of\nfeature learning and machine learning in general.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:46:50 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1707.04236", "submitter": "Felipe Tobar", "authors": "Felipe Tobar", "title": "Improving Sparsity in Kernel Adaptive Filters Using a Unit-Norm\n  Dictionary", "comments": "Accepted at the IEEE Digital Signal Processing conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel adaptive filters, a class of adaptive nonlinear time-series models,\nare known by their ability to learn expressive autoregressive patterns from\nsequential data. However, for trivial monotonic signals, they struggle to\nperform accurate predictions and at the same time keep computational complexity\nwithin desired boundaries. This is because new observations are incorporated to\nthe dictionary when they are far from what the algorithm has seen in the past.\nWe propose a novel approach to kernel adaptive filtering that compares new\nobservations against dictionary samples in terms of their unit-norm\n(normalised) versions, meaning that new observations that look like previous\nsamples but have a different magnitude are not added to the dictionary. We\nachieve this by proposing the unit-norm Gaussian kernel and define a\nsparsification criterion for this novel kernel. This new methodology is\nvalidated on two real-world datasets against standard KAF in terms of the\nnormalised mean square error and the dictionary size.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:37:46 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Tobar", "Felipe", ""]]}, {"id": "1707.04314", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A.\n  Osborne, Frank Wood", "title": "Bayesian Optimization for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first general purpose framework for marginal maximum a\nposteriori estimation of probabilistic program variables. By using a series of\ncode transformations, the evidence of any probabilistic program, and therefore\nof any graphical model, can be optimized with respect to an arbitrary subset of\nits sampled variables. To carry out this optimization, we develop the first\nBayesian optimization package to directly exploit the source code of its\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\noptimization, and implicit constraint satisfaction; delivering significant\nperformance improvements over prominent existing packages. We present\napplications of our method to a number of tasks including engineering design\nand parameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:49:29 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Rainforth", "Tom", ""], ["Le", "Tuan Anh", ""], ["van de Meent", "Jan-Willem", ""], ["Osborne", "Michael A.", ""], ["Wood", "Frank", ""]]}, {"id": "1707.04319", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Yerlan Idelbayev", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part II: quantization", "comments": "33 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deep neural net compression by quantization: given\na large, reference net, we want to quantize its real-valued weights using a\ncodebook with $K$ entries so that the training loss of the quantized net is\nminimal. The codebook can be optimally learned jointly with the net, or fixed,\nas for binarization or ternarization approaches. Previous work has quantized\nthe weights of the reference net, or incorporated rounding operations in the\nbackpropagation algorithm, but this has no guarantee of converging to a\nloss-optimal, quantized net. We describe a new approach based on the recently\nproposed framework of model compression as constrained optimization\n\\citep{Carreir17a}. This results in a simple iterative \"learning-compression\"\nalgorithm, which alternates a step that learns a net of continuous weights with\na step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed\nto converge to local optimum of the loss for quantized nets. We develop\nalgorithms for an adaptive codebook or a (partially) fixed codebook. The latter\nincludes binarization, ternarization, powers-of-two and other important\nparticular cases. We show experimentally that we can achieve much higher\ncompression rates than previous quantization work (even using just 1 bit per\nweight) with negligible loss degradation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:58:40 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Idelbayev", "Yerlan", ""]]}, {"id": "1707.04327", "submitter": "Adnan Darwiche", "authors": "Adnan Darwiche", "title": "Human-Level Intelligence or Animal-Like Abilities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision systems of the eagle and the snake outperform everything that we\ncan make in the laboratory, but snakes and eagles cannot build an eyeglass or a\ntelescope or a microscope. (Judea Pearl)\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 21:17:14 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Darwiche", "Adnan", ""]]}, {"id": "1707.04347", "submitter": "Lin Chen", "authors": "Lin Chen, Moran Feldman, Amin Karbasi", "title": "Weakly Submodular Maximization Beyond Cardinality Constraints: Does\n  Randomization Help Greedy?", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions are a broad class of set functions, which naturally\narise in diverse areas. Many algorithms have been suggested for the\nmaximization of these functions. Unfortunately, once the function deviates from\nsubmodularity, the known algorithms may perform arbitrarily poorly. Amending\nthis issue, by obtaining approximation results for set functions generalizing\nsubmodular functions, has been the focus of recent works.\n  One such class, known as weakly submodular functions, has received a lot of\nattention. A key result proved by Das and Kempe (2011) showed that the\napproximation ratio of the greedy algorithm for weakly submodular maximization\nsubject to a cardinality constraint degrades smoothly with the distance from\nsubmodularity. However, no results have been obtained for maximization subject\nto constraints beyond cardinality. In particular, it is not known whether the\ngreedy algorithm achieves any non-trivial approximation ratio for such\nconstraints.\n  In this paper, we prove that a randomized version of the greedy algorithm\n(previously used by Buchbinder et al. (2014) for a different problem) achieves\nan approximation ratio of $(1 + 1/\\gamma)^{-2}$ for the maximization of a\nweakly submodular function subject to a general matroid constraint, where\n$\\gamma$ is a parameter measuring the distance of the function from\nsubmodularity. Moreover, we also experimentally compare the performance of this\nversion of the greedy algorithm on real world problems against natural\nbenchmarks, and show that the algorithm we study performs well also in\npractice. To the best of our knowledge, this is the first algorithm with a\nnon-trivial approximation guarantee for maximizing a weakly submodular function\nsubject to a constraint other than the simple cardinality constraint. In\nparticular, it is the first algorithm with such a guarantee for the important\nand broad class of matroid constraints.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 22:48:43 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Chen", "Lin", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "1707.04368", "submitter": "Md Ashad Alam PhD", "authors": "Md. Ashad Alam, Hui-Yi Lin, Vince Calhoun and Yu-Ping Wang", "title": "Kernel Method for Detecting Higher Order Interactions in multi-view\n  Data: An Application to Imaging, Genetics, and Epigenetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we tested the interaction effect of multimodal datasets using\na novel method called the kernel method for detecting higher order interactions\namong biologically relevant mulit-view data. Using a semiparametric method on a\nreproducing kernel Hilbert space (RKHS), we used a standard mixed-effects\nlinear model and derived a score-based variance component statistic that tests\nfor higher order interactions between multi-view data. The proposed method\noffers an intangible framework for the identification of higher order\ninteraction effects (e.g., three way interaction) between genetics, brain\nimaging, and epigenetic data. Extensive numerical simulation studies were first\nconducted to evaluate the performance of this method. Finally, this method was\nevaluated using data from the Mind Clinical Imaging Consortium (MCIC) including\nsingle nucleotide polymorphism (SNP) data, functional magnetic resonance\nimaging (fMRI) scans, and deoxyribonucleic acid (DNA) methylation data,\nrespectfully, in schizophrenia patients and healthy controls. We treated each\ngene-derived SNPs, region of interest (ROI) and gene-derived DNA methylation as\na single testing unit, which are combined into triplets for evaluation. In\naddition, cardiovascular disease risk factors such as age, gender, and body\nmass index were assessed as covariates on hippocampal volume and compared\nbetween triplets. Our method identified $13$-triplets ($p$-values $\\leq 0.001$)\nthat included $6$ gene-derived SNPs, $10$ ROIs, and $6$ gene-derived DNA\nmethylations that correlated with changes in hippocampal volume, suggesting\nthat these triplets may be important in explaining schizophrenia-related\nneurodegeneration. With strong evidence ($p$-values $\\leq 0.000001$), the\ntriplet ({\\bf MAGI2, CRBLCrus1.L, FBXO28}) has the potential to distinguish\nschizophrenia patients from the healthy control variations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 02:13:12 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Alam", "Md. Ashad", ""], ["Lin", "Hui-Yi", ""], ["Calhoun", "Vince", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1707.04385", "submitter": "Richard Nock", "authors": "Richard Nock and Zac Cranko and Aditya Krishna Menon and Lizhen Qu and\n  Robert C. Williamson", "title": "f-GANs in an Information Geometric Nutshell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowozin \\textit{et al} showed last year how to extend the GAN\n\\textit{principle} to all $f$-divergences. The approach is elegant but falls\nshort of a full description of the supervised game, and says little about the\nkey player, the generator: for example, what does the generator actually\nconverge to if solving the GAN game means convergence in some space of\nparameters? How does that provide hints on the generator's design and compare\nto the flourishing but almost exclusively experimental literature on the\nsubject?\n  In this paper, we unveil a broad class of distributions for which such\nconvergence happens --- namely, deformed exponential families, a wide superset\nof exponential families --- and show tight connections with the three other key\nGAN parameters: loss, game and architecture. In particular, we show that\ncurrent deep architectures are able to factorize a very large number of such\ndensities using an especially compact design, hence displaying the power of\ndeep architectures and their concinnity in the $f$-GAN game. This result holds\ngiven a sufficient condition on \\textit{activation functions} --- which turns\nout to be satisfied by popular choices. The key to our results is a variational\ngeneralization of an old theorem that relates the KL divergence between regular\nexponential families and divergences between their natural parameters. We\ncomplete this picture with additional results and experimental insights on how\nthese results may be used to ground further improvements of GAN architectures,\nvia (i) a principled design of the activation functions in the generator and\n(ii) an explicit integration of proper composite losses' link function in the\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 05:07:52 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Nock", "Richard", ""], ["Cranko", "Zac", ""], ["Menon", "Aditya Krishna", ""], ["Qu", "Lizhen", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1707.04476", "submitter": "Johannes Buchner", "authors": "Johannes Buchner", "title": "Collaborative Nested Sampling: Big Data vs. complex physical models", "comments": "Resubmitted to PASP Focus on Machine Intelligence in Astronomy and\n  Astrophysics after first referee report. Figure 6 demonstrates the scaling\n  for Collaborative MultiNest, PolyChord and RadFriends implementations. Figure\n  10 application to MUSE IFU data. Implementation at\n  https://github.com/JohannesBuchner/massivedatans", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The data torrent unleashed by current and upcoming astronomical surveys\ndemands scalable analysis methods. Many machine learning approaches scale well,\nbut separating the instrument measurement from the physical effects of\ninterest, dealing with variable errors, and deriving parameter uncertainties is\noften an after-thought. Classic forward-folding analyses with Markov Chain\nMonte Carlo or Nested Sampling enable parameter estimation and model\ncomparison, even for complex and slow-to-evaluate physical models. However,\nthese approaches require independent runs for each data set, implying an\nunfeasible number of model evaluations in the Big Data regime. Here I present a\nnew algorithm, collaborative nested sampling, for deriving parameter\nprobability distributions for each observation. Importantly, the number of\nphysical model evaluations scales sub-linearly with the number of data sets,\nand no assumptions about homogeneous errors, Gaussianity, the form of the model\nor heterogeneity/completeness of the observations need to be made.\nCollaborative nested sampling has immediate application in speeding up analyses\nof large surveys, integral-field-unit observations, and Monte Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:07:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 23:08:39 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 16:10:24 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 12:59:00 GMT"}, {"version": "v5", "created": "Tue, 2 Oct 2018 20:27:38 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Buchner", "Johannes", ""]]}, {"id": "1707.04588", "submitter": "Ga\\\"etan Hadjeres", "authors": "Ga\\\"etan Hadjeres and Frank Nielsen and Fran\\c{c}ois Pachet", "title": "GLSR-VAE: Geodesic Latent Space Regularization for Variational\n  AutoEncoder Architectures", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VAEs (Variational AutoEncoders) have proved to be powerful in the context of\ndensity modeling and have been used in a variety of contexts for creative\npurposes. In many settings, the data we model possesses continuous attributes\nthat we would like to take into account at generation time. We propose in this\npaper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational\nAutoEncoder architecture and its generalizations which allows a fine control on\nthe embedding of the data into the latent space. When augmenting the VAE loss\nwith this regularization, changes in the learned latent space reflects changes\nof the attributes of the data. This deeper understanding of the VAE latent\nspace structure offers the possibility to modulate the attributes of the\ngenerated data in a continuous way. We demonstrate its efficiency on a\nmonophonic music generation task where we manage to generate variations of\ndiscrete sequences in an intended and playful way.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:28:25 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Hadjeres", "Ga\u00ebtan", ""], ["Nielsen", "Frank", ""], ["Pachet", "Fran\u00e7ois", ""]]}, {"id": "1707.04638", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik and Jure Leskovec", "title": "Predicting multicellular function through multi-layer tissue networks", "comments": "In Proceedings of the 25th International Conference on Intelligent\n  Systems for Molecular Biology (ISMB), 2017", "journal-ref": "Bioinformatics 2017, 33 (14): i190-i198", "doi": "10.1093/bioinformatics/btx252", "report-no": null, "categories": "cs.LG cs.SI q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Understanding functions of proteins in specific human tissues is\nessential for insights into disease diagnostics and therapeutics, yet\nprediction of tissue-specific cellular function remains a critical challenge\nfor biomedicine.\n  Results: Here we present OhmNet, a hierarchy-aware unsupervised node feature\nlearning approach for multi-layer networks. We build a multi-layer network,\nwhere each layer represents molecular interactions in a different human tissue.\nOhmNet then automatically learns a mapping of proteins, represented as nodes,\nto a neural embedding based low-dimensional space of features. OhmNet\nencourages sharing of similar features among proteins with similar network\nneighborhoods and among proteins activated in similar tissues. The algorithm\ngeneralizes prior work, which generally ignores relationships between tissues,\nby modeling tissue organization with a rich multiscale tissue hierarchy. We use\nOhmNet to study multicellular function in a multi-layer protein interaction\nnetwork of 107 human tissues. In 48 tissues with known tissue-specific cellular\nfunctions, OhmNet provides more accurate predictions of cellular function than\nalternative approaches, and also generates more accurate hypotheses about\ntissue-specific protein actions. We show that taking into account the tissue\nhierarchy leads to improved predictive power. Remarkably, we also demonstrate\nthat it is possible to leverage the tissue hierarchy in order to effectively\ntransfer cellular functions to a functionally uncharacterized tissue. Overall,\nOhmNet moves from flat networks to multiscale models able to predict a range of\nphenotypes spanning cellular subsystems\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 21:03:53 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zitnik", "Marinka", ""], ["Leskovec", "Jure", ""]]}, {"id": "1707.04659", "submitter": "Hao Wu", "authors": "Hao Wu and Frank No\\'e", "title": "Variational approach for learning Markov processes from time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference, prediction and control of complex dynamical systems from time\nseries is important in many areas, including financial markets, power grid\nmanagement, climate and weather modeling, or molecular dynamics. The analysis\nof such highly nonlinear dynamical systems is facilitated by the fact that we\ncan often find a (generally nonlinear) transformation of the system coordinates\nto features in which the dynamics can be excellently approximated by a linear\nMarkovian model. Moreover, the large number of system variables often change\ncollectively on large time- and length-scales, facilitating a low-dimensional\nanalysis in feature space. In this paper, we introduce a variational approach\nfor Markov processes (VAMP) that allows us to find optimal feature mappings and\noptimal Markovian models of the dynamics from given time series data. The key\ninsight is that the best linear model can be obtained from the top singular\ncomponents of the Koopman operator. This leads to the definition of a family of\nscore functions called VAMP-r which can be calculated from data, and can be\nemployed to optimize a Markovian model. In addition, based on the relationship\nbetween the variational scores and approximation errors of Koopman operators,\nwe propose a new VAMP-E score, which can be applied to cross-validation for\nhyper-parameter optimization and model selection in VAMP. VAMP is valid for\nboth reversible and nonreversible processes and for stationary and\nnon-stationary processes or realizations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 23:07:32 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 21:20:38 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 01:52:07 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Wu", "Hao", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1707.04673", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Learning linear structural equation models in polynomial time and sample\n  complexity", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning structural equation models (SEMs) from data is a\nfundamental problem in causal inference. We develop a new algorithm --- which\nis computationally and statistically efficient and works in the\nhigh-dimensional regime --- for learning linear SEMs from purely observational\ndata with arbitrary noise distribution. We consider three aspects of the\nproblem: identifiability, computational efficiency, and statistical efficiency.\nWe show that when data is generated from a linear SEM over $p$ nodes and\nmaximum degree $d$, our algorithm recovers the directed acyclic graph (DAG)\nstructure of the SEM under an identifiability condition that is more general\nthan those considered in the literature, and without faithfulness assumptions.\nIn the population setting, our algorithm recovers the DAG structure in\n$\\mathcal{O}(p(d^2 + \\log p))$ operations. In the finite sample setting, if the\nestimated precision matrix is sparse, our algorithm has a smoothed complexity\nof $\\widetilde{\\mathcal{O}}(p^3 + pd^7)$, while if the estimated precision\nmatrix is dense, our algorithm has a smoothed complexity of\n$\\widetilde{\\mathcal{O}}(p^5)$. For sub-Gaussian noise, we show that our\nalgorithm has a sample complexity of $\\mathcal{O}(\\frac{d^8}{\\varepsilon^2}\n\\log (\\frac{p}{\\sqrt{\\delta}}))$ to achieve $\\varepsilon$ element-wise additive\nerror with respect to the true autoregression matrix with probability at most\n$1 - \\delta$, while for noise with bounded $(4m)$-th moment, with $m$ being a\npositive integer, our algorithm has a sample complexity of\n$\\mathcal{O}(\\frac{d^8}{\\varepsilon^2} (\\frac{p^2}{\\delta})^{1/m})$.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 01:10:57 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1707.04692", "submitter": "Hossein Sangrody", "authors": "Hossein Sangrody, Morteza Sarailoo, Ning Zhou, Ahmad Shokrollahi,\n  Elham Foruzan", "title": "On the Performance of Forecasting Models in the Presence of Input\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the unprecedented penetration of renewable distributed energy\nresources (DERs), the necessity of an efficient energy forecasting model is\nmore demanding than before. Generally, forecasting models are trained using\nobserved weather data while the trained models are applied for energy\nforecasting using forecasted weather data. In this study, the performance of\nseveral commonly used forecasting methods in the presence of weather predictors\nwith uncertainty is assessed and compared. Accordingly, both observed and\nforecasted weather data are collected, then the influential predictors for\nsolar PV generation forecasting model are selected using several measures.\nUsing observed and forecasted weather data, an analysis on the uncertainty of\nweather variables is represented by MAE and bootstrapping. The energy\nforecasting model is trained using observed weather data, and finally, the\nperformance of several commonly used forecasting methods in solar energy\nforecasting is simulated and compared for a real case study.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 06:16:21 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Sangrody", "Hossein", ""], ["Sarailoo", "Morteza", ""], ["Zhou", "Ning", ""], ["Shokrollahi", "Ahmad", ""], ["Foruzan", "Elham", ""]]}, {"id": "1707.04926", "submitter": "Adel Javanmard", "authors": "Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee", "title": "Theoretical insights into the optimization landscape of\n  over-parameterized shallow neural networks", "comments": "Section 3 on numerical experiments is added. Theorems 2.1 and 2.2 are\n  improved to apply to almost all input data (not just Gaussian inputs).\n  Related work section is expanded. The paper is accepted for publication in\n  IEEE transaction on Information Theory (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of learning a shallow artificial neural\nnetwork that best fits a training data set. We study this problem in the\nover-parameterized regime where the number of observations are fewer than the\nnumber of parameters in the model. We show that with quadratic activations the\noptimization landscape of training such shallow neural networks has certain\nfavorable characteristics that allow globally optimal models to be found\nefficiently using a variety of local search heuristics. This result holds for\nan arbitrary training data of input/output pairs. For differentiable activation\nfunctions we also show that gradient descent, when suitably initialized,\nconverges at a linear rate to a globally optimal model. This result focuses on\na realizable model where the inputs are chosen i.i.d. from a Gaussian\ndistribution and the labels are generated according to planted weight\ncoefficients.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 18:13:51 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 00:11:02 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""], ["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1707.04958", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Cristhian Potes, Minnan Xu-Wilson, Junzi Dong, Asif\n  Rahman, Hiep Nguyen, David Moromisato", "title": "An Ensemble Boosting Model for Predicting Transfer to the Pediatric\n  Intensive Care Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on the problem of predicting the transfer of pediatric\npatients from the general ward of a hospital to the pediatric intensive care\nunit. Using data collected over 5.5 years from the electronic health records of\ntwo medical facilities, we develop classifiers based on adaptive boosting and\ngradient tree boosting. We further combine these learned classifiers into an\nensemble model and compare its performance to a modified pediatric early\nwarning score (PEWS) baseline that relies on expert defined guidelines. To\ngauge model generalizability, we perform an inter-facility evaluation where we\ntrain our algorithm on data from one facility and perform evaluation on a\nhidden test dataset from a separate facility. We show that improvements are\nwitnessed over the PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80\nvs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 23:01:35 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rubin", "Jonathan", ""], ["Potes", "Cristhian", ""], ["Xu-Wilson", "Minnan", ""], ["Dong", "Junzi", ""], ["Rahman", "Asif", ""], ["Nguyen", "Hiep", ""], ["Moromisato", "David", ""]]}, {"id": "1707.05010", "submitter": "Phuoc Nguyen", "authors": "Phuoc Nguyen, Truyen Tran, Svetha Venkatesh", "title": "Deep Learning to Attend to Risk in ICU", "comments": "Accepted IJCAI17-KDH workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling physiological time-series in ICU is of high clinical importance.\nHowever, data collected within ICU are irregular in time and often contain\nmissing measurements. Since absence of a measure would signify its lack of\nimportance, the missingness is indeed informative and might reflect the\ndecision making by the clinician. Here we propose a deep learning architecture\nthat can effectively handle these challenges for predicting ICU mortality\noutcomes. The model is based on Long Short-Term Memory, and has layered\nattention mechanisms. At the sensing layer, the model decides whether to\nobserve and incorporate parts of the current measurements. At the reasoning\nlayer, evidences across time steps are weighted and combined. The model is\nevaluated on the PhysioNet 2012 dataset showing competitive and interpretable\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:23:20 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1707.05101", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa", "title": "On consistency of optimal pricing algorithms in repeated posted-price\n  auctions with strategic buyer", "comments": "25 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study revenue optimization learning algorithms for repeated posted-price\nauctions where a seller interacts with a single strategic buyer that holds a\nfixed private valuation for a good and seeks to maximize his cumulative\ndiscounted surplus. For this setting, first, we propose a novel algorithm that\nnever decreases offered prices and has a tight strategic regret bound in\n$\\Theta(\\log\\log T)$ under some mild assumptions on the buyer surplus\ndiscounting. This result closes the open research question on the existence of\na no-regret horizon-independent weakly consistent pricing. The proposed\nalgorithm is inspired by our observation that a double decrease of offered\nprices in a weakly consistent algorithm is enough to cause a linear regret.\nThis motivates us to construct a novel transformation that maps a\nright-consistent algorithm to a weakly consistent one that never decreases\noffered prices.\n  Second, we outperform the previously known strategic regret upper bound of\nthe algorithm PRRFES, where the improvement is achieved by means of a finer\nconstant factor $C$ of the principal term $C\\log\\log T$ in this upper bound.\nFinally, we generalize results on strategic regret previously known for\ngeometric discounting of the buyer's surplus to discounting of other types,\nnamely: the optimality of the pricing PRRFES to the case of geometrically\nconcave decreasing discounting; and linear lower bound on the strategic regret\nof a wide range of horizon-independent weakly consistent algorithms to the case\nof arbitrary discounts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 11:29:14 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 01:15:57 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Drutsa", "Alexey", ""]]}, {"id": "1707.05147", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Jes Frellsen, Pietro Li\\'o", "title": "Comparative Study of Inference Methods for Bayesian Nonnegative Matrix\n  Factorisation", "comments": "European Conference on Machine Learning and Principles and Practice\n  of Knowledge Discovery in Databases (ECML PKDD 2017). The final publication\n  will be available at link.springer.com. arXiv admin note: text overlap with\n  arXiv:1610.08127", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the trade-offs of different inference approaches for\nBayesian matrix factorisation methods, which are commonly used for predicting\nmissing values, and for finding patterns in the data. In particular, we\nconsider Bayesian nonnegative variants of matrix factorisation and\ntri-factorisation, and compare non-probabilistic inference, Gibbs sampling,\nvariational Bayesian inference, and a maximum-a-posteriori approach. The\nvariational approach is new for the Bayesian nonnegative models. We compare\ntheir convergence, and robustness to noise and sparsity of the data, on both\nsynthetic and real-world datasets. Furthermore, we extend the models with the\nBayesian automatic relevance determination prior, allowing the models to\nperform automatic model selection, and demonstrate its efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:24:55 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Brouwer", "Thomas", ""], ["Frellsen", "Jes", ""], ["Li\u00f3", "Pietro", ""]]}, {"id": "1707.05167", "submitter": "Tomasz Kacprzak", "authors": "Jorit Schmelzle, Aurelien Lucchi, Tomasz Kacprzak, Adam Amara, Raphael\n  Sgier, Alexandre R\\'efr\\'egier, and Thomas Hofmann", "title": "Cosmological model discrimination with Deep Learning", "comments": "21 pages, 9 figures, prepared for submission to JCAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the potential of Deep Learning methods for measurements of\ncosmological parameters from density fields, focusing on the extraction of\nnon-Gaussian information. We consider weak lensing mass maps as our dataset. We\naim for our method to be able to distinguish between five models, which were\nchosen to lie along the $\\sigma_8$ - $\\Omega_m$ degeneracy, and have nearly the\nsame two-point statistics. We design and implement a Deep Convolutional Neural\nNetwork (DCNN) which learns the relation between five cosmological models and\nthe mass maps they generate. We develop a new training strategy which ensures\nthe good performance of the network for high levels of noise. We compare the\nperformance of this approach to commonly used non-Gaussian statistics, namely\nthe skewness and kurtosis of the convergence maps. We find that our\nimplementation of DCNN outperforms the skewness and kurtosis statistics,\nespecially for high noise levels. The network maintains the mean discrimination\nefficiency greater than $85\\%$ even for noise levels corresponding to ground\nbased lensing observations, while the other statistics perform worse in this\nsetting, achieving efficiency less than $70\\%$. This demonstrates the ability\nof CNN-based methods to efficiently break the $\\sigma_8$ - $\\Omega_m$\ndegeneracy with weak lensing mass maps alone. We discuss the potential of this\nmethod to be applied to the analysis of real weak lensing data and other\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 13:58:18 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 08:52:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Schmelzle", "Jorit", ""], ["Lucchi", "Aurelien", ""], ["Kacprzak", "Tomasz", ""], ["Amara", "Adam", ""], ["Sgier", "Raphael", ""], ["R\u00e9fr\u00e9gier", "Alexandre", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1707.05316", "submitter": "Cory Merkel", "authors": "Cory Merkel", "title": "Current-mode Memristor Crossbars for Neuromemristive Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by advantages of current-mode design, this brief contribution\nexplores the implementation of weight matrices in neuromemristive systems via\ncurrent-mode memristor crossbar circuits. After deriving theoretical results\nfor the range and distribution of weights in the current-mode design, it is\nshown that any weight matrix based on voltage-mode crossbars can be mapped to a\ncurrent-mode crossbar if the voltage-mode weights are carefully bounded. Then,\na modified gradient descent rule is derived for the current-mode design that\ncan be used to perform backpropagation training. Behavioral simulations on the\nMNIST dataset indicate that both voltage and current-mode designs are able to\nachieve similar accuracy and have similar defect tolerance. However, analysis\nof trained weight distributions reveals that current-mode and voltage-mode\ndesigns may use different feature representations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 13:10:13 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Merkel", "Cory", ""]]}, {"id": "1707.05342", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "An optimal unrestricted learning procedure", "comments": "This version contains a different presentation of the same results,\n  written from a more CS perspective (using the notion of sample complexity\n  rather than the accuracy/confidence trade-off for a fixed sample size)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning problems involving arbitrary classes of functions $F$,\ndistributions $X$ and targets $Y$. Because proper learning procedures, i.e.,\nprocedures that are only allowed to select functions in $F$, tend to perform\npoorly unless the problem satisfies some additional structural property (e.g.,\nthat $F$ is convex), we consider unrestricted learning procedures that are free\nto choose functions outside the given class.\n  We present a new unrestricted procedure that is optimal in a very strong\nsense: the required sample complexity is essentially the best one can hope for,\nand the estimate holds for (almost) any problem, including heavy-tailed\nsituations. Moreover, the sample complexity coincides with the what one would\nexpect if $F$ were convex, even when $F$ is not. And if $F$ is convex, the\nprocedure turns out to be proper. Thus, the unrestricted procedure is actually\noptimal in both realms, for convex classes as a proper procedure and for\narbitrary classes as an unrestricted procedure.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 18:05:59 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 21:42:51 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 11:13:42 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1707.05373", "submitter": "Moustapha Cisse", "authors": "Moustapha Cisse, Yossi Adi, Natalia Neverova and Joseph Keshet", "title": "Houdini: Fooling Deep Structured Prediction Models", "comments": "12 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples is a critical step for evaluating and\nimproving the robustness of learning machines. So far, most existing methods\nonly work for classification and are not designed to alter the true performance\nmeasure of the problem at hand. We introduce a novel flexible approach named\nHoudini for generating adversarial examples specifically tailored for the final\nperformance measure of the task considered, be it combinatorial and\nnon-decomposable. We successfully apply Houdini to a range of applications such\nas speech recognition, pose estimation and semantic segmentation. In all cases,\nthe attacks based on Houdini achieve higher success rate than those based on\nthe traditional surrogates used to train the models while using a less\nperceptible adversarial perturbation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:11:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cisse", "Moustapha", ""], ["Adi", "Yossi", ""], ["Neverova", "Natalia", ""], ["Keshet", "Joseph", ""]]}, {"id": "1707.05420", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu", "title": "Cooperative Hierarchical Dirichlet Processes: Superposition vs.\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cooperative hierarchical structure is a common and significant data\nstructure observed in, or adopted by, many research areas, such as: text mining\n(author-paper-word) and multi-label classification (label-instance-feature).\nRenowned Bayesian approaches for cooperative hierarchical structure modeling\nare mostly based on topic models. However, these approaches suffer from a\nserious issue in that the number of hidden topics/factors needs to be fixed in\nadvance and an inappropriate number may lead to overfitting or underfitting.\nOne elegant way to resolve this issue is Bayesian nonparametric learning, but\nexisting work in this area still cannot be applied to cooperative hierarchical\nstructure modeling.\n  In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP)\nto fill this gap. Each node in a cooperative hierarchical structure is assigned\na Dirichlet process to model its weights on the infinite hidden factors/topics.\nTogether with measure inheritance from hierarchical Dirichlet process, two\nkinds of measure cooperation, i.e., superposition and maximization, are defined\nto capture the many-to-many relationships in the cooperative hierarchical\nstructure. Furthermore, two constructive representations for CHDP, i.e.,\nstick-breaking and international restaurant process, are designed to facilitate\nthe model inference. Experiments on synthetic and real-world data with\ncooperative hierarchical structures demonstrate the properties and the ability\nof CHDP for cooperative hierarchical structure modeling and its potential for\npractical application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 00:42:10 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1707.05470", "submitter": "Zi Yin", "authors": "Zi Yin, Keng-hao Chang, Ruofei Zhang", "title": "DeepProbe: Information Directed Sequence Understanding and Chatbot\n  Design via Recurrent Neural Networks", "comments": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2017", "journal-ref": null, "doi": "10.1145/3097983.3098148", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction and user intention identification are central topics\nin modern query understanding and recommendation systems. In this paper, we\npropose DeepProbe, a generic information-directed interaction framework which\nis built around an attention-based sequence to sequence (seq2seq) recurrent\nneural network. DeepProbe can rephrase, evaluate, and even actively ask\nquestions, leveraging the generative ability and likelihood estimation made\npossible by seq2seq models. DeepProbe makes decisions based on a derived\nuncertainty (entropy) measure conditioned on user inputs, possibly with\nmultiple rounds of interactions. Three applications, namely a rewritter, a\nrelevance scorer and a chatbot for ad recommendation, were built around\nDeepProbe, with the first two serving as precursory building blocks for the\nthird. We first use the seq2seq model in DeepProbe to rewrite a user query into\none of standard query form, which is submitted to an ordinary recommendation\nsystem. Secondly, we evaluate DeepProbe's seq2seq model-based relevance\nscoring. Finally, we build a chatbot prototype capable of making active user\ninteractions, which can ask questions that maximize information gain, allowing\nfor a more efficient user intention idenfication process. We evaluate first two\napplications by 1) comparing with baselines by BLEU and AUC, and 2) human judge\nevaluation. Both demonstrate significant improvements compared with current\nstate-of-the-art systems, proving their values as useful tools on their own,\nand at the same time laying a good foundation for the ongoing chatbot\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:12:09 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 17:05:00 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Yin", "Zi", ""], ["Chang", "Keng-hao", ""], ["Zhang", "Ruofei", ""]]}, {"id": "1707.05497", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Ilias Diakonikolas, Ronitt Rubinfeld", "title": "Differentially Private Identity and Closeness Testing of Discrete\n  Distributions", "comments": "Submitted, May 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problems of identity and closeness testing over a discrete\npopulation from random samples. Our goal is to develop efficient testers while\nguaranteeing Differential Privacy to the individuals of the population. We\ndescribe an approach that yields sample-efficient differentially private\ntesters for these problems. Our theoretical results show that there exist\nprivate identity and closeness testers that are nearly as sample-efficient as\ntheir non-private counterparts. We perform an experimental evaluation of our\nalgorithms on synthetic data. Our experiments illustrate that our private\ntesters achieve small type I and type II errors with sample size sublinear in\nthe domain size of the underlying distributions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:51:31 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Diakonikolas", "Ilias", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1707.05499", "submitter": "Anirban Laha", "authors": "Disha Shrivastava, Saneem Ahmed CG, Anirban Laha, Karthik\n  Sankaranarayanan", "title": "A Machine Learning Approach for Evaluating Creative Artifacts", "comments": "Accepted at SIGKDD Workshop on Machine Learning for Creativity\n  (ML4Creativity), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been done in understanding human creativity and defining\nmeasures to evaluate creativity. This is necessary mainly for the reason of\nhaving an objective and automatic way of quantifying creative artifacts. In\nthis work, we propose a regression-based learning framework which takes into\naccount quantitatively the essential criteria for creativity like novelty,\ninfluence, value and unexpectedness. As it is often the case with most creative\ndomains, there is no clear ground truth available for creativity. Our proposed\nlearning framework is applicable to all creative domains; yet we evaluate it on\na dataset of movies created from IMDb and Rotten Tomatoes due to availability\nof audience and critic scores, which can be used as proxy ground truth labels\nfor creativity. We report promising results and observations from our\nexperiments in the following ways : 1) Correlation of creative criteria with\ncritic scores, 2) Improvement in movie rating prediction with inclusion of\nvarious creative criteria, and 3) Identification of creative movies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:59:45 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Shrivastava", "Disha", ""], ["CG", "Saneem Ahmed", ""], ["Laha", "Anirban", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "1707.05532", "submitter": "Florian Wenzel", "authors": "Florian Wenzel, Theo Galy-Fajou, Matthaeus Deutsch, Marius Kloft", "title": "Bayesian Nonlinear Support Vector Machines for Big Data", "comments": "accepted as conference paper at ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast inference method for Bayesian nonlinear support vector\nmachines that leverages stochastic variational inference and inducing points.\nOur experiments show that the proposed method is faster than competing Bayesian\napproaches and scales easily to millions of data points. It provides additional\nfeatures over frequentist competitors such as accurate predictive uncertainty\nestimates and automatic hyperparameter search.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:16:50 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Wenzel", "Florian", ""], ["Galy-Fajou", "Theo", ""], ["Deutsch", "Matthaeus", ""], ["Kloft", "Marius", ""]]}, {"id": "1707.05533", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "Global optimization for low-dimensional switching linear regression and\n  bounded-error estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides global optimization algorithms for two particularly\ndifficult nonconvex problems raised by hybrid system identification: switching\nlinear regression and bounded-error estimation. While most works focus on local\noptimization heuristics without global optimality guarantees or with guarantees\nvalid only under restrictive conditions, the proposed approach always yields a\nsolution with a certificate of global optimality. This approach relies on a\nbranch-and-bound strategy for which we devise lower bounds that can be\nefficiently computed. In order to obtain scalable algorithms with respect to\nthe number of data, we directly optimize the model parameters in a continuous\noptimization setting without involving integer variables. Numerical experiments\nshow that the proposed algorithms offer a higher accuracy than convex\nrelaxations with a reasonable computational burden for hybrid system\nidentification. In addition, we discuss how bounded-error estimation is related\nto robust estimation in the presence of outliers and exact recovery under\nsparse noise, for which we also obtain promising numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:18:22 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 13:43:20 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 12:49:54 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1707.05534", "submitter": "Erik Bodin", "authors": "Erik Bodin, Neill D. F. Campbell, Carl Henrik Ek", "title": "Latent Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Latent Gaussian Process Regression which is a latent variable\nextension allowing modelling of non-stationary multi-modal processes using GPs.\nThe approach is built on extending the input space of a regression problem with\na latent variable that is used to modulate the covariance function over the\ntraining data. We show how our approach can be used to model multi-modal and\nnon-stationary processes. We exemplify the approach on a set of synthetic data\nand provide results on real data from motion capture and geostatistics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:19:20 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 18:18:03 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bodin", "Erik", ""], ["Campbell", "Neill D. F.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1707.05562", "submitter": "Jordan Burgess", "authors": "Jordan Burgess, James Robert Lloyd, Zoubin Ghahramani", "title": "One-Shot Learning in Discriminative Neural Networks", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of one-shot learning of visual categories. In this paper\nwe explore a Bayesian procedure for updating a pretrained convnet to classify a\nnovel image category for which data is limited. We decompose this convnet into\na fixed feature extractor and softmax classifier. We assume that the target\nweights for the new task come from the same distribution as the pretrained\nsoftmax weights, which we model as a multivariate Gaussian. By using this as a\nprior for the new weights, we demonstrate competitive performance with\nstate-of-the-art methods whilst also being consistent with 'normal' methods for\ntraining deep networks on large data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 11:17:22 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Burgess", "Jordan", ""], ["Lloyd", "James Robert", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.05587", "submitter": "Hermina Petric Maretic", "authors": "Hermina Petric Maretic, Dorina Thanou, Pascal Frossard", "title": "Graph learning under sparsity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph signals offer a very generic and natural representation for data that\nlives on networks or irregular structures. The actual data structure is however\noften unknown a priori but can sometimes be estimated from the knowledge of the\napplication domain. If this is not possible, the data structure has to be\ninferred from the mere signal observations. This is exactly the problem that we\naddress in this paper, under the assumption that the graph signals can be\nrepresented as a sparse linear combination of a few atoms of a structured graph\ndictionary. The dictionary is constructed on polynomials of the graph\nLaplacian, which can sparsely represent a general class of graph signals\ncomposed of localized patterns on the graph. We formulate a graph learning\nproblem, whose solution provides an ideal fit between the signal observations\nand the sparse graph signal model. As the problem is non-convex, we propose to\nsolve it by alternating between a signal sparse coding and a graph update step.\nWe provide experimental results that outline the good graph recovery\nperformance of our method, which generally compares favourably to other recent\nnetwork inference algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 12:31:53 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Maretic", "Hermina Petric", ""], ["Thanou", "Dorina", ""], ["Frossard", "Pascal", ""]]}, {"id": "1707.05609", "submitter": "Saverio Salzo", "authors": "Saverio Salzo, Johan A.K. Suykens and Lorenzo Rosasco", "title": "Solving $\\ell^p\\!$-norm regularization with tensor kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss how a suitable family of tensor kernels can be used\nto efficiently solve nonparametric extensions of $\\ell^p$ regularized learning\nmethods. Our main contribution is proposing a fast dual algorithm, and showing\nthat it allows to solve the problem efficiently. Our results contrast recent\nfindings suggesting kernel methods cannot be extended beyond Hilbert setting.\nNumerical experiments confirm the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:48:22 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 16:02:05 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Salzo", "Saverio", ""], ["Suykens", "Johan A. K.", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1707.05697", "submitter": "Cong Shen", "authors": "Fei Liang, Cong Shen, Feng Wu", "title": "An Iterative BP-CNN Architecture for Channel Decoding", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2794062", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in deep learning, we propose a novel iterative\nBP-CNN architecture for channel decoding under correlated noise. This\narchitecture concatenates a trained convolutional neural network (CNN) with a\nstandard belief-propagation (BP) decoder. The standard BP decoder is used to\nestimate the coded bits, followed by a CNN to remove the estimation errors of\nthe BP decoder and obtain a more accurate estimation of the channel noise.\nIterating between BP and CNN will gradually improve the decoding SNR and hence\nresult in better decoding performance. To train a well-behaved CNN model, we\ndefine a new loss function which involves not only the accuracy of the noise\nestimation but also the normality test for the estimation errors, i.e., to\nmeasure how likely the estimation errors follow a Gaussian distribution. The\nintroduction of the normality test to the CNN training shapes the residual\nnoise distribution and further reduces the BER of the iterative decoding,\ncompared to using the standard quadratic loss function. We carry out extensive\nexperiments to analyze and verify the proposed framework. The iterative BP-CNN\ndecoder has better BER performance with lower complexity, is suitable for\nparallel implementation, does not rely on any specific channel model or\nencoding method, and is robust against training mismatches. All of these\nfeatures make it a good candidate for decoding modern channel codes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:41:49 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Liang", "Fei", ""], ["Shen", "Cong", ""], ["Wu", "Feng", ""]]}, {"id": "1707.05712", "submitter": "Emilie Morvant", "authors": "Pascal Germain (MODAL), Amaury Habrard (LHC), Fran\\c{c}ois Laviolette,\n  Emilie Morvant (LHC)", "title": "PAC-Bayes and Domain Adaptation", "comments": "Neurocomputing, Elsevier, 2019. arXiv admin note: substantial text\n  overlap with arXiv:1503.06944", "journal-ref": null, "doi": "10.1016/j.neucom.2019.10.105", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two main contributions in PAC-Bayesian theory for domain\nadaptation where the objective is to learn, from a source distribution, a\nwell-performing majority vote on a different, but related, target distribution.\nFirstly, we propose an improvement of the previous approach we proposed in\nGermain et al. (2013), which relies on a novel distribution pseudodistance\nbased on a disagreement averaging, allowing us to derive a new tighter domain\nadaptation bound for the target risk. While this bound stands in the spirit of\ncommon domain adaptation works, we derive a second bound (introduced in Germain\net al., 2016) that brings a new perspective on domain adaptation by deriving an\nupper bound on the target risk where the distributions' divergence-expressed as\na ratio-controls the trade-off between a source error measure and the target\nvoters' disagreement. We discuss and compare both results, from which we obtain\nPAC-Bayesian generalization bounds. Furthermore, from the PAC-Bayesian\nspecialization to linear classifiers, we infer two learning algorithms, and we\nevaluate them on real data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 11:34:53 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 12:56:33 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 14:24:45 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Germain", "Pascal", "", "MODAL"], ["Habrard", "Amaury", "", "LHC"], ["Laviolette", "Fran\u00e7ois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "1707.05729", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin, Michael McCourt, Kevin Tee", "title": "Robust Bayesian Optimization with Student-t Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has recently attracted the attention of the automatic\nmachine learning community for its excellent results in hyperparameter tuning.\nBO is characterized by the sample efficiency with which it can optimize\nexpensive black-box functions. The efficiency is achieved in a similar fashion\nto the learning to learn methods: surrogate models (typically in the form of\nGaussian processes) learn the target function and perform intelligent sampling.\nThis surrogate model can be applied even in the presence of noise; however, as\nwith most regression methods, it is very sensitive to outlier data. This can\nresult in erroneous predictions and, in the case of BO, biased and inefficient\nexploration. In this work, we present a GP model that is robust to outliers\nwhich uses a Student-t likelihood to segregate outliers and robustly conduct\nBayesian optimization. We present numerical results evaluating the proposed\nmethod in both artificial functions and real problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:22:07 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Martinez-Cantin", "Ruben", ""], ["McCourt", "Michael", ""], ["Tee", "Kevin", ""]]}, {"id": "1707.05776", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam", "title": "Optimizing the Latent Space of Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have achieved remarkable results in\nthe task of generating realistic natural images. In most successful\napplications, GAN models share two common aspects: solving a challenging saddle\npoint optimization problem, interpreted as an adversarial game between a\ngenerator and a discriminator functions; and parameterizing the generator and\nthe discriminator as deep convolutional neural networks. The goal of this paper\nis to disentangle the contribution of these two factors to the success of GANs.\nIn particular, we introduce Generative Latent Optimization (GLO), a framework\nto train deep convolutional generators using simple reconstruction losses.\nThroughout a variety of experiments, we show that GLO enjoys many of the\ndesirable properties of GANs: synthesizing visually-appealing samples,\ninterpolating meaningfully between samples, and performing linear arithmetic\nwith noise vectors; all of this without the adversarial optimization scheme.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:58:34 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:19:44 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Lopez-Paz", "David", ""], ["Szlam", "Arthur", ""]]}, {"id": "1707.05807", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas and Lester Mackey", "title": "Improving Gibbs Sampler Scan Quality with DoGS", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pairwise influence matrix of Dobrushin has long been used as an\nanalytical tool to bound the rate of convergence of Gibbs sampling. In this\nwork, we use Dobrushin influence as the basis of a practical tool to certify\nand efficiently improve the quality of a discrete Gibbs sampler. Our\nDobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection\norders for a given sampling budget and variable subset of interest, explicit\nbounds on total variation distance to stationarity, and certifiable\nimprovements over the standard systematic and uniform random scan Gibbs\nsamplers. In our experiments with joint image segmentation and object\nrecognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising\nmodel inference, DoGS consistently deliver higher-quality inferences with\nsignificantly smaller sampling budgets than standard Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Mackey", "Lester", ""]]}, {"id": "1707.05840", "submitter": "Randall Balestriero", "authors": "Randall Balestriero", "title": "Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal\n  Basis Learning", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in the problem of learning an over-complete\nbasis and a methodology such that the reconstruction or inverse problem does\nnot need optimization. We analyze the optimality of the presented approaches,\ntheir link to popular already known techniques s.a. Artificial Neural\nNetworks,k-means or Oja's learning rule. Finally, we will see that one approach\nto reach the optimal dictionary is a factorial and hierarchical approach. The\nderived approach lead to a formulation of a Deep Oja Network. We present\nresults on different tasks and present the resulting very efficient learning\nalgorithm which brings a new vision on the training of deep nets. Finally, the\ntheoretical work shows that deep frameworks are one way to efficiently have\nover-complete (combinatorially large) dictionary yet allowing easy\nreconstruction. We thus present the Deep Residual Oja Network (DRON). We\ndemonstrate that a recursive deep approach working on the residuals allow\nexponential decrease of the error w.r.t. the depth.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:56:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Balestriero", "Randall", ""]]}, {"id": "1707.05841", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Herve Glotin", "title": "Linear Time Complexity Deep Fourier Scattering Network and Extension to\n  Nonlinear Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a scalable version of a state-of-the-art\ndeterministic time-invariant feature extraction approach based on consecutive\nchanges of basis and nonlinearities, namely, the scattering network. The first\nfocus of the paper is to extend the scattering network to allow the use of\nhigher order nonlinearities as well as extracting nonlinear and Fourier based\nstatistics leading to the required invariants of any inherently structured\ninput. In order to reach fast convolutions and to leverage the intrinsic\nstructure of wavelets, we derive our complete model in the Fourier domain. In\naddition of providing fast computations, we are now able to exploit sparse\nmatrices due to extremely high sparsity well localized in the Fourier domain.\nAs a result, we are able to reach a true linear time complexity with inputs in\nthe Fourier domain allowing fast and energy efficient solutions to machine\nlearning tasks. Validation of the features and computational results will be\npresented through the use of these invariant coefficients to perform\nclassification on audio recordings of bird songs captured in multiple different\nsoundscapes. In the end, the applicability of the presented solutions to deep\nartificial neural networks is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:04:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Balestriero", "Randall", ""], ["Glotin", "Herve", ""]]}, {"id": "1707.05861", "submitter": "Cheng Ju", "authors": "Cheng Ju, Joshua Schwab, Mark J. van der Laan", "title": "On Adaptive Propensity Score Truncation in Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The positivity assumption, or the experimental treatment assignment (ETA)\nassumption, is important for identifiability in causal inference. Even if the\npositivity assumption holds, practical violations of this assumption may\njeopardize the finite sample performance of the causal estimator. One of the\nconsequences of practical violations of the positivity assumption is extreme\nvalues in the estimated propensity score (PS). A common practice to address\nthis issue is truncating the PS estimate when constructing PS-based estimators.\nIn this study, we propose a novel adaptive truncation method,\nPositivity-C-TMLE, based on the collaborative targeted maximum likelihood\nestimation (C-TMLE) methodology. We demonstrate the outstanding performance of\nour novel approach in a variety of simulations by comparing it with other\ncommonly studied estimators. Results show that by adaptively truncating the\nestimated PS with a more targeted objective function, the Positivity-C-TMLE\nestimator achieves the best performance for both point estimation and\nconfidence interval coverage among all estimators considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Ju", "Cheng", ""], ["Schwab", "Joshua", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1707.05909", "submitter": "Felipe Tobar", "authors": "Felipe Tobar, Gonzalo Rios, Tom\\'as Valdivia, Pablo Guerrero", "title": "Recovering Latent Signals from a Mixture of Measurements using a\n  Gaussian Process Prior", "comments": "Published on IEEE Signal Processing Letters on Dec. 2016", "journal-ref": "IEEE Signal Processing Letters, vol. 24, no. 2, pp. 231-235, Feb.\n  2017", "doi": "10.1109/LSP.2016.2637312", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sensing applications, sensors cannot always measure the latent quantity of\ninterest at the required resolution, sometimes they can only acquire a blurred\nversion of it due the sensor's transfer function. To recover latent signals\nwhen only noisy mixed measurements of the signal are available, we propose the\nGaussian process mixture of measurements (GPMM), which models the latent signal\nas a Gaussian process (GP) and allows us to perform Bayesian inference on such\nsignal conditional to a set of noisy mixture of measurements. We describe how\nto train GPMM, that is, to find the hyperparameters of the GP and the mixing\nweights, and how to perform inference on the latent signal under GPMM;\nadditionally, we identify the solution to the underdetermined linear system\nresulting from a sensing application as a particular case of GPMM. The proposed\nmodel is validated in the recovery of three signals: a smooth synthetic signal,\na real-world heart-rate time series and a step function, where GPMM\noutperformed the standard GP in terms of estimation error, uncertainty\nrepresentation and recovery of the spectral content of the latent signal.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 00:56:11 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Tobar", "Felipe", ""], ["Rios", "Gonzalo", ""], ["Valdivia", "Tom\u00e1s", ""], ["Guerrero", "Pablo", ""]]}, {"id": "1707.05922", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Zoubin Ghahramani", "title": "Improving Output Uncertainty Estimation and Generalization in Deep\n  Learning via Neural Network Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method that combines neural networks and Gaussian\nprocesses. The proposed method can estimate the uncertainty of outputs and\nflexibly adjust target functions where training data exist, which are\nadvantages of Gaussian processes. The proposed method can also achieve high\ngeneralization performance for unseen input configurations, which is an\nadvantage of neural networks. With the proposed method, neural networks are\nused for the mean functions of Gaussian processes. We present a scalable\nstochastic inference procedure, where sparse Gaussian processes are inferred by\nstochastic variational inference, and the parameters of neural networks and\nkernels are estimated by stochastic gradient descent methods, simultaneously.\nWe use two real-world spatio-temporal data sets to demonstrate experimentally\nthat the proposed method achieves better uncertainty estimation and\ngeneralization performance than neural networks and Gaussian processes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 02:29:49 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.05947", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Liwei Wang, Xiyu Zhai, Kai Zheng", "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical\n  Viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm-dependent generalization error bounds are central to statistical\nlearning theory. A learning algorithm may use a large hypothesis space, but the\nlimited number of iterations controls its model capacity and generalization\nerror. The impacts of stochastic gradient methods on generalization error for\nnon-convex learning problems not only have important theoretical consequences,\nbut are also critical to generalization errors of deep learning.\n  In this paper, we study the generalization errors of Stochastic Gradient\nLangevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed\nwith non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian\nresults respectively. The stability-based theory obtains a bound of\n$O\\left(\\frac{1}{n}L\\sqrt{\\beta T_k}\\right)$, where $L$ is uniform Lipschitz\nparameter, $\\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.\nFor PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate,\nthe contribution of each step is shown with an exponentially decaying factor by\nimposing $\\ell^2$ regularization, and the uniform Lipschitz constant is also\nreplaced by actual norms of gradients along trajectory. Our bounds have no\nimplicit dependence on dimensions, norms or other capacity measures of\nparameter, which elegantly characterizes the phenomenon of \"Fast Training\nGuarantees Generalization\" in non-convex settings. This is the first\nalgorithm-dependent result with reasonable dependence on aggregated step sizes\nfor non-convex learning, and has important implications to statistical learning\naspects of stochastic gradient methods in complicated models such as deep\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 06:17:57 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mou", "Wenlong", ""], ["Wang", "Liwei", ""], ["Zhai", "Xiyu", ""], ["Zheng", "Kai", ""]]}, {"id": "1707.05961", "submitter": "Olivier Colliot", "authors": "Emilie Gerardin, Ga\\\"el Ch\\'etelat, Marie Chupin, R\\'emi Cuingnet,\n  B\\'eatrice Desgranges, Ho-Sung Kim, Marc Niethammer, Bruno Dubois, St\\'ephane\n  Leh\\'ericy, Line Garnero, Francis Eustache, Olivier Colliot", "title": "Multidimensional classification of hippocampal shape features\n  discriminates Alzheimer's disease and mild cognitive impairment from normal\n  aging", "comments": "Data used in the preparation of this article were obtained from the\n  Alzheimer's Disease Neuroimaging Initiative (ADNI) database", "journal-ref": "NeuroImage, 47 (4), pp.1476-86, 2009", "doi": "10.1016/j.neuroimage.2009.05.036", "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method to automatically discriminate between patients with\nAlzheimer's disease (AD) or mild cognitive impairment (MCI) and elderly\ncontrols, based on multidimensional classification of hippocampal shape\nfeatures. This approach uses spherical harmonics (SPHARM) coefficients to model\nthe shape of the hippocampi, which are segmented from magnetic resonance images\n(MRI) using a fully automatic method that we previously developed. SPHARM\ncoefficients are used as features in a classification procedure based on\nsupport vector machines (SVM). The most relevant features for classification\nare selected using a bagging strategy. We evaluate the accuracy of our method\nin a group of 23 patients with AD (10 males, 13 females, age $\\pm$\nstandard-deviation (SD) = 73 $\\pm$ 6 years, mini-mental score (MMS) = 24.4\n$\\pm$ 2.8), 23 patients with amnestic MCI (10 males, 13 females, age $\\pm$ SD =\n74 $\\pm$ 8 years, MMS = 27.3 $\\pm$ 1.4) and 25 elderly healthy controls (13\nmales, 12 females, age $\\pm$ SD = 64 $\\pm$ 8 years), using leave-one-out\ncross-validation. For AD vs controls, we obtain a correct classification rate\nof 94%, a sensitivity of 96%, and a specificity of 92%. For MCI vs controls, we\nobtain a classification rate of 83%, a sensitivity of 83%, and a specificity of\n84%. This accuracy is superior to that of hippocampal volumetry and is\ncomparable to recently published SVM-based whole-brain classification methods,\nwhich relied on a different strategy. This new method may become a useful tool\nto assist in the diagnosis of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 07:33:02 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Gerardin", "Emilie", ""], ["Ch\u00e9telat", "Ga\u00ebl", ""], ["Chupin", "Marie", ""], ["Cuingnet", "R\u00e9mi", ""], ["Desgranges", "B\u00e9atrice", ""], ["Kim", "Ho-Sung", ""], ["Niethammer", "Marc", ""], ["Dubois", "Bruno", ""], ["Leh\u00e9ricy", "St\u00e9phane", ""], ["Garnero", "Line", ""], ["Eustache", "Francis", ""], ["Colliot", "Olivier", ""]]}, {"id": "1707.06017", "submitter": "Shervine Amidi", "authors": "Afshine Amidi, Shervine Amidi, Dimitrios Vlachakis, Vasileios\n  Megalooikonomou, Nikos Paragios and Evangelia I. Zacharaki", "title": "EnzyNet: enzyme classification using 3D convolutional neural networks on\n  spatial representation", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the past decade, with the significant progress of computational power\nas well as ever-rising data availability, deep learning techniques became\nincreasingly popular due to their excellent performance on computer vision\nproblems. The size of the Protein Data Bank has increased more than 15 fold\nsince 1999, which enabled the expansion of models that aim at predicting\nenzymatic function via their amino acid composition. Amino acid sequence\nhowever is less conserved in nature than protein structure and therefore\nconsidered a less reliable predictor of protein function. This paper presents\nEnzyNet, a novel 3D-convolutional neural networks classifier that predicts the\nEnzyme Commission number of enzymes based only on their voxel-based spatial\nstructure. The spatial distribution of biochemical properties was also examined\nas complementary information. The 2-layer architecture was investigated on a\nlarge dataset of 63,558 enzymes from the Protein Data Bank and achieved an\naccuracy of 78.4% by exploiting only the binary representation of the protein\nshape. Code and datasets are available at https://github.com/shervinea/enzynet.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:59:29 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Amidi", "Afshine", ""], ["Amidi", "Shervine", ""], ["Vlachakis", "Dimitrios", ""], ["Megalooikonomou", "Vasileios", ""], ["Paragios", "Nikos", ""], ["Zacharaki", "Evangelia I.", ""]]}, {"id": "1707.06145", "submitter": "Xiang Li", "authors": "Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek,\n  Jieping Ye, James Thrall, Quanzheng Li", "title": "Self-paced Convolutional Neural Network for Computer Aided Detection in\n  Medical Imaging Analysis", "comments": "accepted by 8th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-67389-9_25", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue characterization has long been an important component of Computer\nAided Diagnosis (CAD) systems for automatic lesion detection and further\nclinical planning. Motivated by the superior performance of deep learning\nmethods on various computer vision problems, there has been increasing work\napplying deep learning to medical image analysis. However, the development of a\nrobust and reliable deep learning model for computer-aided diagnosis is still\nhighly challenging due to the combination of the high heterogeneity in the\nmedical images and the relative lack of training samples. Specifically,\nannotation and labeling of the medical images is much more expensive and\ntime-consuming than other applications and often involves manual labor from\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\nlearning framework utilizing a convolutional neural network (CNN) to classify\nComputed Tomography (CT) image patches. The key contribution of this approach\nis that we augment the size of training samples by refining the unlabeled\ninstances with a self-paced learning CNN. By implementing the framework on high\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\nthe experimental result, showing that the self-pace boosted network\nconsistently outperformed the original network even with very scarce manual\nlabels. The performance gain indicates that applications with limited training\nsamples such as medical image analysis can benefit from using the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:15:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Xiang", ""], ["Zhong", "Aoxiao", ""], ["Lin", "Ming", ""], ["Guo", "Ning", ""], ["Sun", "Mu", ""], ["Sitek", "Arkadiusz", ""], ["Ye", "Jieping", ""], ["Thrall", "James", ""], ["Li", "Quanzheng", ""]]}, {"id": "1707.06170", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,\n  Sebastien Racani\\`ere, David Reichert, Th\\'eophane Weber, Daan Wierstra,\n  Peter Battaglia", "title": "Learning model-based planning from scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional wisdom holds that model-based planning is a powerful approach to\nsequential decision-making. It is often very challenging in practice, however,\nbecause while a model can be used to evaluate a plan, it does not prescribe how\nto construct a plan. Here we introduce the \"Imagination-based Planner\", the\nfirst model-based, sequential decision-making agent that can learn to\nconstruct, evaluate, and execute plans. Before any action, it can perform a\nvariable number of imagination steps, which involve proposing an imagined\naction and evaluating it with its model-based imagination. All imagined actions\nand outcomes are aggregated, iteratively, into a \"plan context\" which\nconditions future real and imagined actions. The agent can even decide how to\nimagine: testing out alternative imagined actions, chaining sequences of\nactions together, or building a more complex \"imagination tree\" by navigating\nflexibly among the previously imagined states using a learned policy. And our\nagent can learn to plan economically, jointly optimizing for external rewards\nand computational costs associated with using its imagination. We show that our\narchitecture can learn to solve a challenging continuous control problem, and\nalso learn elaborate planning strategies in a discrete maze-solving task. Our\nwork opens a new direction toward learning the components of a model-based\nplanning system and how to use them.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:52:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Pascanu", "Razvan", ""], ["Li", "Yujia", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Buesing", "Lars", ""], ["Racani\u00e8re", "Sebastien", ""], ["Reichert", "David", ""], ["Weber", "Th\u00e9ophane", ""], ["Wierstra", "Daan", ""], ["Battaglia", "Peter", ""]]}, {"id": "1707.06194", "submitter": "Cassio P. de Campos", "authors": "Cassio P. de Campos, Mauro Scanagatta, Giorgio Corani, Marco Zaffalon", "title": "Entropy-based Pruning for Learning Bayesian Networks using BIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decomposable score-based structure learning of Bayesian networks,\nexisting approaches first compute a collection of candidate parent sets for\neach variable and then optimize over this collection by choosing one parent set\nfor each variable without creating directed cycles while maximizing the total\nscore. We target the task of constructing the collection of candidate parent\nsets when the score of choice is the Bayesian Information Criterion (BIC). We\nprovide new non-trivial results that can be used to prune the search space of\ncandidate parent sets of each node. We analyze how these new results relate to\nprevious ideas in the literature both theoretically and empirically. We show in\nexperiments with UCI data sets that gains can be significant. Since the new\npruning rules are easy to implement and have low computational costs, they can\nbe promptly integrated into all state-of-the-art methods for structure learning\nof Bayesian networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:03:14 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["de Campos", "Cassio P.", ""], ["Scanagatta", "Mauro", ""], ["Corani", "Giorgio", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1707.06197", "submitter": "Liu Weiyi", "authors": "Weiyi Liu and Pin-Yu Chen and Hal Cooper and Min Hwan Oh and Sailung\n  Yeung and Toyotaro Suzumura", "title": "Can GAN Learn Topological Features of a Graph?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is first-line research expanding GANs into graph topology\nanalysis. By leveraging the hierarchical connectivity structure of a graph, we\nhave demonstrated that generative adversarial networks (GANs) can successfully\ncapture topological features of any arbitrary graph, and rank edge sets by\ndifferent stages according to their contribution to topology reconstruction.\nMoreover, in addition to acting as an indicator of graph reconstruction, we\nfind that these stages can also preserve important topological features in a\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:06:21 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Liu", "Weiyi", ""], ["Chen", "Pin-Yu", ""], ["Cooper", "Hal", ""], ["Oh", "Min Hwan", ""], ["Yeung", "Sailung", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.06203", "submitter": "Th\\'eophane  Weber", "authors": "Th\\'eophane Weber, S\\'ebastien Racani\\`ere, David P. Reichert, Lars\n  Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom\\`enech Badia,\n  Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia,\n  Demis Hassabis, David Silver, Daan Wierstra", "title": "Imagination-Augmented Agents for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:12:56 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 17:26:18 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Weber", "Th\u00e9ophane", ""], ["Racani\u00e8re", "S\u00e9bastien", ""], ["Reichert", "David P.", ""], ["Buesing", "Lars", ""], ["Guez", "Arthur", ""], ["Rezende", "Danilo Jimenez", ""], ["Badia", "Adria Puigdom\u00e8nech", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Li", "Yujia", ""], ["Pascanu", "Razvan", ""], ["Battaglia", "Peter", ""], ["Hassabis", "Demis", ""], ["Silver", "David", ""], ["Wierstra", "Daan", ""]]}, {"id": "1707.06209", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Nelson F. Liu, Matt Gardner", "title": "Crowdsourcing Multiple Choice Science Questions", "comments": "accepted for the Workshop on Noisy User-generated Text (W-NUT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for obtaining high-quality, domain-targeted\nmultiple choice questions from crowd workers. Generating these questions can be\ndifficult without trading away originality, relevance or diversity in the\nanswer options. Our method addresses these problems by leveraging a large\ncorpus of domain-specific text and a small set of existing questions. It\nproduces model suggestions for document selection and answer distractor choice\nwhich aid the human question generation process. With this method we have\nassembled SciQ, a dataset of 13.7K multiple choice science exam questions\n(Dataset available at http://allenai.org/data.html). We demonstrate that the\nmethod produces in-domain questions by providing an analysis of this new\ndataset and by showing that humans cannot distinguish the crowdsourced\nquestions from original questions. When using SciQ as additional training data\nto existing questions, we observe accuracy improvements on real science exams.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:28:46 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Welbl", "Johannes", ""], ["Liu", "Nelson F.", ""], ["Gardner", "Matt", ""]]}, {"id": "1707.06213", "submitter": "Matthew Thorpe", "authors": "Dejan Slep\\v{c}ev and Matthew Thorpe", "title": "Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a family of regression problems in a semi-supervised setting.\nThe task is to assign real-valued labels to a set of $n$ sample points,\nprovided a small training subset of $N$ labeled points. A goal of\nsemi-supervised learning is to take advantage of the (geometric) structure\nprovided by the large number of unlabeled data when assigning labels. We\nconsider random geometric graphs, with connection radius $\\epsilon(n)$, to\nrepresent the geometry of the data set. Functionals which model the task reward\nthe regularity of the estimator function and impose or reward the agreement\nwith the training data. Here we consider the discrete $p$-Laplacian\nregularization.\n  We investigate asymptotic behavior when the number of unlabeled points\nincreases, while the number of training points remains fixed. We uncover a\ndelicate interplay between the regularizing nature of the functionals\nconsidered and the nonlocality inherent to the graph constructions. We\nrigorously obtain almost optimal ranges on the scaling of $\\epsilon(n)$ for the\nasymptotic consistency to hold. We prove that the minimizers of the discrete\nfunctionals in random setting converge uniformly to the desired continuum\nlimit. Furthermore we discover that for the standard model used there is a\nrestrictive upper bound on how quickly $\\epsilon(n)$ must converge to zero as\n$n \\to \\infty$. We introduce a new model which is as simple as the original\nmodel, but overcomes this restriction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:31:14 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 18:32:44 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Slep\u010dev", "Dejan", ""], ["Thorpe", "Matthew", ""]]}, {"id": "1707.06217", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Cheng Mao, Vidya Muthukumar, Martin J. Wainwright,\n  Thomas A. Courtade", "title": "Worst-case vs Average-case Design for Estimation from Fixed Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison data arises in many domains, including tournament\nrankings, web search, and preference elicitation. Given noisy comparisons of a\nfixed subset of pairs of items, we study the problem of estimating the\nunderlying comparison probabilities under the assumption of strong stochastic\ntransitivity (SST). We also consider the noisy sorting subclass of the SST\nmodel. We show that when the assignment of items to the topology is arbitrary,\nthese permutation-based models, unlike their parametric counterparts, do not\nadmit consistent estimation for most comparison topologies used in practice. We\nthen demonstrate that consistent estimation is possible when the assignment of\nitems to the topology is randomized, thus establishing a dichotomy between\nworst-case and average-case designs. We propose two estimators in the\naverage-case setting and analyze their risk, showing that it depends on the\ncomparison topology only through the degree sequence of the topology. The rates\nachieved by these estimators are shown to be optimal for a large class of\ngraphs. Our results are corroborated by simulations on multiple comparison\ntopologies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:47:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Mao", "Cheng", ""], ["Muthukumar", "Vidya", ""], ["Wainwright", "Martin J.", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "1707.06219", "submitter": "Walid Krichene", "authors": "Walid Krichene and Peter L. Bartlett", "title": "Acceleration and Averaging in Stochastic Mirror Descent Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and study a general family of (continuous-time) stochastic\ndynamics for accelerated first-order minimization of smooth convex functions.\nBuilding on an averaging formulation of accelerated mirror descent, we propose\na stochastic variant in which the gradient is contaminated by noise, and study\nthe resulting stochastic differential equation. We prove a bound on the rate of\nchange of an energy function associated with the problem, then use it to derive\nestimates of convergence rates of the function values, (a.s. and in\nexpectation) both for persistent and asymptotically vanishing noise. We discuss\nthe interaction between the parameters of the dynamics (learning rate and\naveraging weights) and the covariation of the noise process, and show, in\nparticular, how the asymptotic rate of covariation affects the choice of\nparameters and, ultimately, the convergence rate.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:50:53 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Krichene", "Walid", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1707.06261", "submitter": "Heinrich Jiang", "authors": "Heinrich Jiang", "title": "Non-Asymptotic Uniform Rates of Consistency for k-NN Regression", "comments": "In Proceedings of 33rd AAAI Conference on Artificial Intelligence\n  (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive high-probability finite-sample uniform rates of consistency for\n$k$-NN regression that are optimal up to logarithmic factors under mild\nassumptions. We moreover show that $k$-NN regression adapts to an unknown lower\nintrinsic dimension automatically. We then apply the $k$-NN regression rates to\nestablish new results about estimating the level sets and global maxima of a\nfunction from noisy observations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 18:56:04 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 00:19:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jiang", "Heinrich", ""]]}, {"id": "1707.06299", "submitter": "Stefan Ultes", "authors": "Stefan Ultes, Pawe{\\l} Budzianowski, I\\~nigo Casanueva, Nikola\n  Mrk\\v{s}i\\'c, Lina Rojas-Barahona, Pei-Hao Su, Tsung-Hsien Wen, Milica\n  Ga\\v{s}i\\'c and Steve Young", "title": "Reward-Balancing for Statistical Spoken Dialogue Systems using\n  Multi-objective Reinforcement Learning", "comments": "Accepted at SIGDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is widely used for dialogue policy optimization where\nthe reward function often consists of more than one component, e.g., the\ndialogue success and the dialogue length. In this work, we propose a structured\nmethod for finding a good balance between these components by searching for the\noptimal reward component weighting. To render this search feasible, we use\nmulti-objective reinforcement learning to significantly reduce the number of\ntraining dialogues required. We apply our proposed method to find optimized\ncomponent weights for six domains and compare them to a default baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 21:21:03 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ultes", "Stefan", ""], ["Budzianowski", "Pawe\u0142", ""], ["Casanueva", "I\u00f1igo", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Rojas-Barahona", "Lina", ""], ["Su", "Pei-Hao", ""], ["Wen", "Tsung-Hsien", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Young", "Steve", ""]]}, {"id": "1707.06315", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Marco Morucci, M. Usaid Awan, Yameng Liu, Sudeepa Roy,\n  Cynthia Rudin, Alexander Volfovsky", "title": "FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal\n  Inference", "comments": null, "journal-ref": "Journal of Machine Learning Research, 22 (2021) 1-41", "doi": null, "report-no": null, "categories": "stat.ML cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in causal inference is that of matching, where treatment\nunits need to be matched to control units based on covariate information. In\nthis work, we propose a method that computes high quality almost-exact matches\nfor high-dimensional categorical datasets. This method, called FLAME (Fast\nLarge-scale Almost Matching Exactly), learns a distance metric for matching\nusing a hold-out training data set. In order to perform matching efficiently\nfor large datasets, FLAME leverages techniques that are natural for query\nprocessing in the area of database management, and two implementations of FLAME\nare provided: the first uses SQL queries and the second uses bit-vector\ntechniques. The algorithm starts by constructing matches of the highest quality\n(exact matches on all covariates), and successively eliminates variables in\norder to match exactly on as many variables as possible, while still\nmaintaining interpretable high-quality matches and balance between treatment\nand control groups. We leverage these high quality matches to estimate\nconditional average treatment effects (CATEs). Our experiments show that FLAME\nscales to huge datasets with millions of observations where existing\nstate-of-the-art methods fail, and that it achieves significantly better\nperformance than other matching methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:35:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 19:58:11 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 04:48:46 GMT"}, {"version": "v4", "created": "Thu, 31 Jan 2019 06:22:07 GMT"}, {"version": "v5", "created": "Sat, 22 Jun 2019 01:17:35 GMT"}, {"version": "v6", "created": "Wed, 16 Oct 2019 22:16:39 GMT"}, {"version": "v7", "created": "Sat, 21 Dec 2019 01:33:09 GMT"}, {"version": "v8", "created": "Fri, 11 Sep 2020 19:24:33 GMT"}, {"version": "v9", "created": "Thu, 4 Feb 2021 11:47:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Tianyu", ""], ["Morucci", "Marco", ""], ["Awan", "M. Usaid", ""], ["Liu", "Yameng", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1707.06338", "submitter": "Florian H\\\"ase", "authors": "Florian H\\\"ase and Christoph Kreisbeck and Al\\'an Aspuru-Guzik", "title": "Machine Learning for Quantum Dynamics: Deep Learning of Excitation\n  Energy Transfer Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between the structure of light-harvesting\nsystems and their excitation energy transfer properties is of fundamental\nimportance in many applications including the development of next generation\nphotovoltaics. Natural light harvesting in photosynthesis shows remarkable\nexcitation energy transfer properties, which suggests that pigment-protein\ncomplexes could serve as blueprints for the design of nature inspired devices.\nMechanistic insights into energy transport dynamics can be gained by leveraging\nnumerically involved propagation schemes such as the hierarchical equations of\nmotion (HEOM). Solving these equations, however, is computationally costly due\nto the adverse scaling with the number of pigments. Therefore virtual\nhigh-throughput screening, which has become a powerful tool in material\ndiscovery, is less readily applicable for the search of novel excitonic\ndevices. We propose the use of artificial neural networks to bypass the\ncomputational limitations of established techniques for exploring the\nstructure-dynamics relation in excitonic systems. Once trained, our neural\nnetworks reduce computational costs by several orders of magnitudes. Our\npredicted transfer times and transfer efficiencies exhibit similar or even\nhigher accuracies than frequently used approximate methods such as secular\nRedfield theory\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 02:00:58 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["H\u00e4se", "Florian", ""], ["Kreisbeck", "Christoph", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1707.06366", "submitter": "Michael Brand", "authors": "Michael Brand", "title": "RKL: a general, invariant Bayes solution for Neyman-Scott", "comments": "15 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neyman-Scott is a classic example of an estimation problem with a\npartially-consistent posterior, for which standard estimation methods tend to\nproduce inconsistent results. Past attempts to create consistent estimators for\nNeyman-Scott have led to ad-hoc solutions, to estimators that do not satisfy\nrepresentation invariance, to restrictions over the choice of prior and more.\nWe present a simple construction for a general-purpose Bayes estimator,\ninvariant to representation, which satisfies consistency on Neyman-Scott over\nany non-degenerate prior. We argue that the good attributes of the estimator\nare due to its intrinsic properties, and generalise beyond Neyman-Scott as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:59:59 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Brand", "Michael", ""]]}, {"id": "1707.06386", "submitter": "Alain Durmus", "authors": "Aymeric Dieuleveut (SIERRA, LIENS), Alain Durmus (CMLA), Francis Bach\n  (SIERRA)", "title": "Bridging the Gap between Constant Step Size Stochastic Gradient Descent\n  and Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of an objective function given access to\nunbiased estimates of its gradient through stochastic gradient descent (SGD)\nwith constant step-size. While the detailed analysis was only performed for\nquadratic functions, we provide an explicit asymptotic expansion of the moments\nof the averaged SGD iterates that outlines the dependence on initial\nconditions, the effect of noise and the step-size, as well as the lack of\nconvergence in the general (non-quadratic) case. For this analysis, we bring\ntools from Markov chain theory into the analysis of stochastic gradient. We\nthen show that Richardson-Romberg extrapolation may be used to get closer to\nthe global optimum and we show empirical improvements of the new extrapolation\nscheme.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 06:31:22 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 09:51:46 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Dieuleveut", "Aymeric", "", "SIERRA, LIENS"], ["Durmus", "Alain", "", "CMLA"], ["Bach", "Francis", "", "SIERRA"]]}, {"id": "1707.06409", "submitter": "Eustache Diemert", "authors": "Eustache Diemert, Julien Meynet, Pierre Galland, Damien Lefortier", "title": "Attribution Modeling Increases Efficiency of Bidding in Display\n  Advertising", "comments": "The first two authors contributed equally to this paper, and should\n  be regarded as co-first authors. Accepted at AdKDD TargetAd workshop at\n  KDD'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting click and conversion probabilities when bidding on ad exchanges is\nat the core of the programmatic advertising industry. Two separated lines of\nprevious works respectively address i) the prediction of user conversion\nprobability and ii) the attribution of these conversions to advertising events\n(such as clicks) after the fact. We argue that attribution modeling improves\nthe efficiency of the bidding policy in the context of performance advertising.\nFirstly we explain the inefficiency of the standard bidding policy with respect\nto attribution. Secondly we learn and utilize an attribution model in the\nbidder itself and show how it modifies the average bid after a click. Finally\nwe produce evidence of the effectiveness of the proposed method on both offline\nand online experiments with data spanning several weeks of real traffic from\nCriteo, a leader in performance advertising.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 08:13:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 07:53:20 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Diemert", "Eustache", ""], ["Meynet", "Julien", ""], ["Galland", "Pierre", ""], ["Lefortier", "Damien", ""]]}, {"id": "1707.06422", "submitter": "Joris Mooij", "authors": "Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers,\n  Philip Versteeg, Joris M. Mooij", "title": "Domain Adaptation by Using Causal Inference to Predict Invariant\n  Conditional Distributions", "comments": "Camera-ready version, to be published in the proceedings of Neural\n  Information Processing Systems 2018 (NIPS*2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal common to domain adaptation and causal inference is to make\naccurate predictions when the distributions for the source (or training)\ndomain(s) and target (or test) domain(s) differ. In many cases, these different\ndistributions can be modeled as different contexts of a single underlying\nsystem, in which each distribution corresponds to a different perturbation of\nthe system, or in causal terms, an intervention. We focus on a class of such\ncausal domain adaptation problems, where data for one or more source domains\nare given, and the task is to predict the distribution of a certain target\nvariable from measurements of other variables in one or more target domains. We\npropose an approach for solving these problems that exploits causal inference\nand does not rely on prior knowledge of the causal graph, the type of\ninterventions or the intervention targets. We demonstrate our approach by\nevaluating a possible implementation on simulated and real world data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 09:23:31 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 15:37:54 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 11:00:40 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Magliacane", "Sara", ""], ["van Ommen", "Thijs", ""], ["Claassen", "Tom", ""], ["Bongers", "Stephan", ""], ["Versteeg", "Philip", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1707.06468", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa, R\\'emi Leblond, Simon Lacoste-Julien", "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite\n  Optimization", "comments": "Appears in Advances in Neural Information Processing Systems 30 (NIPS\n  2017), 28 pages", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to their simplicity and excellent performance, parallel asynchronous\nvariants of stochastic gradient descent have become popular methods to solve a\nwide range of large-scale optimization problems on multi-core architectures.\nYet, despite their practical success, support for nonsmooth objectives is still\nlacking, making them unsuitable for many problems of interest in machine\nlearning, such as the Lasso, group Lasso or empirical risk minimization with\nconvex constraints.\n  In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse\nmethod inspired by SAGA, a variance reduced incremental gradient algorithm. The\nproposed method is easy to implement and significantly outperforms the state of\nthe art on several nonsmooth, large-scale problems. We prove that our method\nachieves a theoretical linear speedup with respect to the sequential version\nunder assumptions on the sparsity of gradients and block-separability of the\nproximal term. Empirical benchmarks on a multi-core architecture illustrate\npractical speedups of up to 12x on a 20-core machine.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:14:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 05:24:08 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 16:49:49 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pedregosa", "Fabian", ""], ["Leblond", "R\u00e9mi", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1707.06476", "submitter": "Elie Wolfe", "authors": "Miguel Navascues and Elie Wolfe", "title": "The Inflation Technique Completely Solves the Causal Compatibility\n  Problem", "comments": "Updated to match forthcoming journal publication as closely as\n  possible. Some content removed for brevity. Expanded citations. Most\n  footnotes moved into the main text. Significant changes to subsection 4.1,\n  where we corrected an error in the example of second order inflation not\n  converging, and added an converse example where second order inflation\n  outperforms other techniques", "journal-ref": null, "doi": "10.1515/jci-2018-0008", "report-no": null, "categories": "quant-ph math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal compatibility question asks whether a given causal structure graph\n-- possibly involving latent variables -- constitutes a genuinely plausible\ncausal explanation for a given probability distribution over the graph's\nobserved variables. Algorithms predicated on merely necessary constraints for\ncausal compatibility typically suffer from false negatives, i.e. they admit\nincompatible distributions as apparently compatible with the given graph. In\n[arXiv:1609.00672], one of us introduced the inflation technique for\nformulating useful relaxations of the causal compatibility problem in terms of\nlinear programming. In this work, we develop a formal hierarchy of such causal\ncompatibility relaxations. We prove that inflation is asymptotically tight,\ni.e., that the hierarchy converges to a zero-error test for causal\ncompatibility. In this sense, the inflation technique fulfills a longstanding\ndesideratum in the field of causal inference. We quantify the rate of\nconvergence by showing that any distribution which passes the $n^{th}$-order\ninflation test must be $O\\left(n^{-1/2}\\right)$-close in Euclidean norm to some\ndistribution genuinely compatible with the given causal structure. Furthermore,\nwe show that for many causal structures, the (unrelaxed) causal compatibility\nproblem is faithfully formulated already by either the first or second order\ninflation test.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:38:06 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:37:07 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 19:06:47 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Navascues", "Miguel", ""], ["Wolfe", "Elie", ""]]}, {"id": "1707.06480", "submitter": "Zhenisbek Assylbekov", "authors": "Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov and\n  Jonathan N. Washington", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware\n  Ones", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syllabification does not seem to improve word-level RNN language modeling\nquality when compared to character-based segmentation. However, our best\nsyllable-aware language model, achieving performance comparable to the\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\n1.2-2.2 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:46:09 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""], ["Myrzakhmetov", "Bagdat", ""], ["Washington", "Jonathan N.", ""]]}, {"id": "1707.06487", "submitter": "Yunfei Ye", "authors": "Yunfei Ye", "title": "A Nonlinear Kernel Support Matrix Machine for Matrix Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s13042-018-0896-4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems of supervised tensor learning (STL), real world data such as\nface images or MRI scans are naturally represented as matrices, which are also\ncalled as second order tensors. Most existing classifiers based on tensor\nrepresentation, such as support tensor machine (STM) need to solve iteratively\nwhich occupy much time and may suffer from local minima. In this paper, we\npresent a kernel support matrix machine (KSMM) to perform supervised learning\nwhen data are represented as matrices. KSMM is a general framework for the\nconstruction of matrix-based hyperplane to exploit structural information. We\nanalyze a unifying optimization problem for which we propose an asymptotically\nconvergent algorithm. Theoretical analysis for the generalization bounds is\nderived based on Rademacher complexity with respect to a probability\ndistribution. We demonstrate the merits of the proposed method by exhaustive\nexperiments on both simulation study and a number of real-word datasets from a\nvariety of application domains.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 13:00:04 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 13:12:27 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Ye", "Yunfei", ""]]}, {"id": "1707.06541", "submitter": "Jian Wu", "authors": "Jian Wu and Peter I. Frazier", "title": "Discretization-free Knowledge Gradient Methods for Bayesian Optimization", "comments": "This paper, which combines and extends two conference papers\n  (arXiv:1703.04389, arXiv:1606.04414), has been withdrawn by the authors\n  because it was submitted prematurely before proper attribution could be\n  provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Bayesian ranking and selection (R&S) problems with\ncorrelated prior beliefs and continuous domains, i.e. Bayesian optimization\n(BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely\nstudied for discrete R&S problems, which sample the one-step Bayes-optimal\npoint. When used over continuous domains, previous work on the knowledge\ngradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely\non a discretized finite approximation. However, the discretization introduces\nerror and scales poorly as the dimension of domain grows. In this paper, we\ndevelop a fast discretization-free knowledge gradient method for Bayesian\noptimization. Our method is not restricted to the fully sequential setting, but\nuseful in all settings where knowledge gradient can be used over continuous\ndomains. We show how our method can be generalized to handle (i) batch of\npoints suggestion (parallel knowledge gradient); (ii) the setting where\nderivative information is available in the optimization process\n(derivative-enabled knowledge gradient). In numerical experiments, we\ndemonstrate that the discretization-free knowledge gradient method finds global\noptima significantly faster than previous Bayesian optimization algorithms on\nboth synthetic test functions and real-world applications, especially when\nfunction evaluations are noisy; and derivative-enabled knowledge gradient can\nfurther improve the performances, even outperforming the gradient-based\noptimizer such as BFGS when derivative information is available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:28:05 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:31:35 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Wu", "Jian", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1707.06573", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova and Gen Li", "title": "Structural Learning and Integrative Decomposition of Multi-View Data", "comments": null, "journal-ref": "Biometrics 2019, Vol. 75, No. 4, 1121-1132", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of the multi-view data (data on the same samples\nfrom multiple sources) has led to strong interest in models based on low-rank\nmatrix factorizations. These models represent each data view via shared and\nindividual components, and have been successfully applied for exploratory\ndimension reduction, association analysis between the views, and further\nlearning tasks such as consensus clustering. Despite these advances, there\nremain significant challenges in modeling partially-shared components, and\nidentifying the number of components of each type\n(shared/partially-shared/individual). In this work, we formulate a novel linked\ncomponent model that directly incorporates partially-shared structures. We call\nthis model SLIDE for Structural Learning and Integrative DEcomposition of\nmulti-view data. We prove the existence of SLIDE decomposition and explicitly\ncharacterize the identifiability conditions. The proposed model fitting and\nselection techniques allow for joint identification of the number of components\nof each type, in contrast to existing sequential approaches. In our empirical\nstudies, SLIDE demonstrates excellent performance in both signal estimation and\ncomponent selection. We further illustrate the methodology on the breast cancer\ndata from The Cancer Genome Atlas repository.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:30:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Li", "Gen", ""]]}, {"id": "1707.06611", "submitter": "Chaopeng Shen", "authors": "Kuai Fang, Chaopeng Shen, Daniel Kifer, Xiao Yang", "title": "Prolongation of SMAP to Spatio-temporally Seamless Coverage of\n  Continental US Using a Deep Learning Neural Network", "comments": null, "journal-ref": "Geophysical Research Letters, 2017", "doi": "10.1002/2017GL075619", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Soil Moisture Active Passive (SMAP) mission has delivered valuable\nsensing of surface soil moisture since 2015. However, it has a short time span\nand irregular revisit schedule. Utilizing a state-of-the-art time-series deep\nlearning neural network, Long Short-Term Memory (LSTM), we created a system\nthat predicts SMAP level-3 soil moisture data with atmospheric forcing,\nmodel-simulated moisture, and static physiographic attributes as inputs. The\nsystem removes most of the bias with model simulations and improves predicted\nmoisture climatology, achieving small test root-mean-squared error (<0.035) and\nhigh correlation coefficient >0.87 for over 75\\% of Continental United States,\nincluding the forested Southeast. As the first application of LSTM in\nhydrology, we show the proposed network avoids overfitting and is robust for\nboth temporal and spatial extrapolation tests. LSTM generalizes well across\nregions with distinct climates and physiography. With high fidelity to SMAP,\nLSTM shows great potential for hindcasting, data assimilation, and weather\nforecasting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:06:47 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 16:22:13 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 18:38:40 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Fang", "Kuai", ""], ["Shen", "Chaopeng", ""], ["Kifer", "Daniel", ""], ["Yang", "Xiao", ""]]}, {"id": "1707.06618", "submitter": "Quanquan Gu", "authors": "Pan Xu and Jinghui Chen and Difan Zou and Quanquan Gu", "title": "Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex\n  Optimization", "comments": "29 pages, 1 figure, 1 table. In NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework to analyze the global convergence of Langevin\ndynamics based algorithms for nonconvex finite-sum optimization with $n$\ncomponent functions. At the core of our analysis is a direct analysis of the\nergodicity of the numerical approximations to Langevin dynamics, which leads to\nfaster convergence rates. Specifically, we show that gradient Langevin dynamics\n(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost\nminimizer within $\\tilde O\\big(nd/(\\lambda\\epsilon) \\big)$ and $\\tilde\nO\\big(d^7/(\\lambda^5\\epsilon^5) \\big)$ stochastic gradient evaluations\nrespectively, where $d$ is the problem dimension, and $\\lambda$ is the spectral\ngap of the Markov chain generated by GLD. Both results improve upon the best\nknown gradient complexity results (Raginsky et al., 2017). Furthermore, for the\nfirst time we prove the global convergence guarantee for variance reduced\nstochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within\n$\\tilde O\\big(\\sqrt{n}d^5/(\\lambda^4\\epsilon^{5/2})\\big)$ stochastic gradient\nevaluations, which outperforms the gradient complexities of GLD and SGLD in a\nwide regime. Our theoretical analyses shed some light on using Langevin\ndynamics based algorithms for nonconvex optimization with provable guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:18:11 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 18:08:04 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 02:57:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xu", "Pan", ""], ["Chen", "Jinghui", ""], ["Zou", "Difan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1707.06626", "submitter": "Yihao Feng", "authors": "Yihao Feng, Dilin Wang, Qiang Liu", "title": "Learning to Draw Samples with Amortized Stein Variational Gradient\n  Descent", "comments": "Accepted by UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient direction (Liu & Wang, 2016)\nthat maximally decreases the KL divergence with the target distribution. Our\nmethod works for any target distribution specified by their unnormalized\ndensity function, and can train any black-box architectures that are\ndifferentiable in terms of the parameters we want to adapt. We demonstrate our\nmethod with a number of applications, including variational autoencoder (VAE)\nwith expressive encoders to model complex latent space structures, and\nhyper-parameter learning of MCMC samplers that allows Bayesian inference to\nadaptively improve itself when seeing more data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:35:52 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 21:27:44 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Feng", "Yihao", ""], ["Wang", "Dilin", ""], ["Liu", "Qiang", ""]]}, {"id": "1707.06682", "submitter": "Regina Meszl\\'enyi", "authors": "Regina Meszl\\'enyi, Krisztian Buza and Zolt\\'an Vidny\\'anszky", "title": "Resting state fMRI functional connectivity-based classification using a\n  convolutional neural network architecture", "comments": "25 pages, 4 figures, 1 table, plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have become increasingly popular in the field of\nresting state fMRI (functional magnetic resonance imaging) network based\nclassification. However, the application of convolutional networks has been\nproposed only very recently and has remained largely unexplored. In this paper\nwe describe a convolutional neural network architecture for functional\nconnectome classification called connectome-convolutional neural network\n(CCNN). Our results on simulated datasets and a publicly available dataset for\namnestic mild cognitive impairment classification demonstrate that our CCNN\nmodel can efficiently distinguish between subject groups. We also show that the\nconnectome-convolutional network is capable to combine information from diverse\nfunctional connectivity metrics and that models using a combination of\ndifferent connectivity descriptors are able to outperform classifiers using\nonly one metric. From this flexibility follows that our proposed CCNN model can\nbe easily adapted to a wide range of connectome based classification or\nregression tasks, by varying which connectivity descriptor combinations are\nused to train the network.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:12:58 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Meszl\u00e9nyi", "Regina", ""], ["Buza", "Krisztian", ""], ["Vidny\u00e1nszky", "Zolt\u00e1n", ""]]}, {"id": "1707.06719", "submitter": "Aleksandr Savchenkov", "authors": "Aleksandr Savchenkov, Andrew Davis, Xuan Zhao", "title": "Generalized Convolutional Neural Networks for Point Cloud Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of cheap RGB-D cameras, stereo cameras, and LIDAR devices\nhas given the computer vision community 3D information that conventional RGB\ncameras cannot provide. This data is often stored as a point cloud. In this\npaper, we present a novel method to apply the concept of convolutional neural\nnetworks to this type of data. By creating a mapping of nearest neighbors in a\ndataset, and individually applying weights to spatial relationships between\npoints, we achieve an architecture that works directly with point clouds, but\nclosely resembles a convolutional neural net in both design and behavior. Such\na method bypasses the need for extensive feature engineering, while proving to\nbe computationally efficient and requiring few parameters.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 23:12:11 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 02:09:03 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Savchenkov", "Aleksandr", ""], ["Davis", "Andrew", ""], ["Zhao", "Xuan", ""]]}, {"id": "1707.06742", "submitter": "Patrice Simard", "authors": "Patrice Y. Simard, Saleema Amershi, David M. Chickering, Alicia\n  Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo Ramos, Jina Suh,\n  Johan Verwey, Mo Wang, and John Wernsing", "title": "Machine Teaching: A New Paradigm for Building Machine Learning Systems", "comments": "Also available at: http://aka.ms/machineteachingpaper", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-26", "categories": "cs.LG cs.AI cs.HC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current processes for building machine learning systems require\npractitioners with deep knowledge of machine learning. This significantly\nlimits the number of machine learning systems that can be created and has led\nto a mismatch between the demand for machine learning systems and the ability\nfor organizations to build them. We believe that in order to meet this growing\ndemand for machine learning systems we must significantly increase the number\nof individuals that can teach machines. We postulate that we can achieve this\ngoal by making the process of teaching machines easy, fast and above all,\nuniversally accessible.\n  While machine learning focuses on creating new algorithms and improving the\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\nfollows and extends principles of software engineering and programming\nlanguages. We put a strong emphasis on the teacher and the teacher's\ninteraction with data, as well as crucial components such as techniques and\ndesign principles of interaction and visualization.\n  In this paper, we present our position regarding the discipline of machine\nteaching and articulate fundamental machine teaching principles. We also\ndescribe how, by decoupling knowledge about machine learning algorithms from\nthe process of teaching, we can accelerate innovation and empower millions of\nnew uses for machine learning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 02:37:04 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 05:45:05 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 00:16:49 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Simard", "Patrice Y.", ""], ["Amershi", "Saleema", ""], ["Chickering", "David M.", ""], ["Pelton", "Alicia Edelman", ""], ["Ghorashi", "Soroush", ""], ["Meek", "Christopher", ""], ["Ramos", "Gonzalo", ""], ["Suh", "Jina", ""], ["Verwey", "Johan", ""], ["Wang", "Mo", ""], ["Wernsing", "John", ""]]}, {"id": "1707.06756", "submitter": "Clayton Morrison", "authors": "Colin Reimer Dawson, Chaofan Huang, Clayton T. Morrison", "title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "comments": "16 pages, 4 figures, accepted to ICML 2017, includes supplemental\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generalization of the Hierarchical Dirichlet Process Hidden\nMarkov Model (HDP-HMM) which is able to encode prior information that state\ntransitions are more likely between \"nearby\" states. This is accomplished by\ndefining a similarity function on the state space and scaling transition\nprobabilities by pair-wise similarities, thereby inducing correlations among\nthe transition distributions. We present an augmented data representation of\nthe model as a Markov Jump Process in which: (1) some jump attempts fail, and\n(2) the probability of success is proportional to the similarity between the\nsource and destination states. This augmentation restores conditional conjugacy\nand admits a simple Gibbs sampler. We evaluate the model and inference method\non a speaker diarization task and a \"harmonic parsing\" task using four-part\nchorale data, as well as on several synthetic datasets, achieving favorable\ncomparisons to existing models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 04:39:10 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Dawson", "Colin Reimer", ""], ["Huang", "Chaofan", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1707.06757", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Randy Paffenroth, Erik M. Bollt", "title": "A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics", "comments": "13 pages, 7 figures, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dimensionality reduction methods are adept at revealing hidden\nunderlying manifolds arising from high-dimensional data and thereby producing a\nlow-dimensional representation. However, the smoothness of the manifolds\nproduced by classic techniques over sparse and noisy data is not guaranteed. In\nfact, the embedding generated using such data may distort the geometry of the\nmanifold and thereby produce an unfaithful embedding. Herein, we propose a\nframework for nonlinear dimensionality reduction that generates a manifold in\nterms of smooth geodesics that is designed to treat problems in which manifold\nmeasurements are either sparse or corrupted by noise. Our method generates a\nnetwork structure for given high-dimensional data using a nearest neighbors\nsearch and then produces piecewise linear shortest paths that are defined as\ngeodesics. Then, we fit points in each geodesic by a smoothing spline to\nemphasize the smoothness. The robustness of this approach for sparse and noisy\ndatasets is demonstrated by the implementation of the method on synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 05:04:07 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 17:38:33 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1707.06792", "submitter": "Felix Rios", "authors": "Tatjana Pavlenko and Felix Leopoldo Rios", "title": "Graphical posterior predictive classifier: Bayesian model averaging with\n  particle Gibbs", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a multi-class graphical Bayesian predictive\nclassifier that incorporates the uncertainty in the model selection into the\nstandard Bayesian formalism. For each class, the dependence structure\nunderlying the observed features is represented by a set of decomposable\nGaussian graphical models. Emphasis is then placed on the Bayesian model\naveraging which takes full account of the class-specific model uncertainty by\naveraging over the posterior graph model probabilities. An explicit evaluation\nof the model probabilities is well known to be infeasible. To address this\nissue, we consider the particle Gibbs strategy of Olsson et al. (2018b) for\nposterior sampling from decomposable graphical models which utilizes the\nChristmas tree algorithm of Olsson et al. (2018a) as proposal kernel. We also\nderive a strong hyper Markov law which we call the hyper normal Wishart law\nthat allow to perform the resultant Bayesian calculations locally. The proposed\npredictive graphical classifier reveals superior performance compared to the\nordinary Bayesian predictive rule that does not account for the model\nuncertainty, as well as to a number of out-of-the-box classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 08:19:25 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 08:37:26 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 11:30:54 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2018 21:01:48 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Pavlenko", "Tatjana", ""], ["Rios", "Felix Leopoldo", ""]]}, {"id": "1707.06887", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Will Dabney, R\\'emi Munos", "title": "A Distributional Perspective on Reinforcement Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue for the fundamental importance of the value\ndistribution: the distribution of the random return received by a reinforcement\nlearning agent. This is in contrast to the common approach to reinforcement\nlearning which models the expectation of this return, or value. Although there\nis an established body of literature studying the value distribution, thus far\nit has always been used for a specific purpose such as implementing risk-aware\nbehaviour. We begin with theoretical results in both the policy evaluation and\ncontrol settings, exposing a significant distributional instability in the\nlatter. We then use the distributional perspective to design a new algorithm\nwhich applies Bellman's equation to the learning of approximate value\ndistributions. We evaluate our algorithm using the suite of games from the\nArcade Learning Environment. We obtain both state-of-the-art results and\nanecdotal evidence demonstrating the importance of the value distribution in\napproximate reinforcement learning. Finally, we combine theoretical and\nempirical evidence to highlight the ways in which the value distribution\nimpacts learning in the approximate setting.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 13:21:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Dabney", "Will", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1707.06903", "submitter": "Chu Wang", "authors": "Chu Wang, Iraj Saniee, William S. Kennedy, Chris A. White", "title": "A New Family of Near-metrics for Universal Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of near-metrics based on local graph diffusion to capture\nsimilarity for a wide class of data sets. These quasi-metametrics, as their\nnames suggest, dispense with one or two standard axioms of metric spaces,\nspecifically distinguishability and symmetry, so that similarity between data\npoints of arbitrary type and form could be measured broadly and effectively.\nThe proposed near-metric family includes the forward k-step diffusion and its\nreverse, typically on the graph consisting of data objects and their features.\nBy construction, this family of near-metrics is particularly appropriate for\ncategorical data, continuous data, and vector representations of images and\ntext extracted via deep learning approaches. We conduct extensive experiments\nto evaluate the performance of this family of similarity measures and compare\nand contrast with traditional measures of similarity used for each specific\napplication and with the ground truth when available. We show that for\nstructured data including categorical and continuous data, the near-metrics\ncorresponding to normalized forward k-step diffusion (k small) work as one of\nthe best performing similarity measures; for vector representations of text and\nimages including those extracted from deep learning, the near-metrics derived\nfrom normalized and reverse k-step graph diffusion (k very small) exhibit\noutstanding ability to distinguish data points from different classes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:02:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 02:36:52 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 08:34:25 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Wang", "Chu", ""], ["Saniee", "Iraj", ""], ["Kennedy", "William S.", ""], ["White", "Chris A.", ""]]}, {"id": "1707.06923", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Multi-kernel learning of deep convolutional features for action\n  recognition", "comments": "ICCV 2017 Workshop on Video and Language Understanding: MovieQA and\n  the Large Scale Movie Description Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image understanding using deep convolutional network has reached human-level\nperformance, yet a closely related problem of video understanding especially,\naction recognition has not reached the requisite level of maturity. We combine\nmulti-kernels based support-vector-machines (SVM) with a multi-stream deep\nconvolutional neural network to achieve close to state-of-the-art performance\non a 51-class activity recognition problem (HMDB-51 dataset); this specific\ndataset has proved to be particularly challenging for deep neural networks due\nto the heterogeneity in camera viewpoints, video quality, etc. The resulting\narchitecture is named pillar networks as each (very) deep neural network acts\nas a pillar for the hierarchical classifiers. In addition, we illustrate that\nhand-crafted features such as improved dense trajectories (iDT) and Multi-skip\nFeature Stacking (MIFS), as additional pillars, can further supplement the\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:45:48 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:56:45 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1707.06962", "submitter": "Seongah Jeong", "authors": "Seongah Jeong, Xiang Li, Jiarui Yang, Quanzheng Li, Vahid Tarokh", "title": "Dictionary Learning and Sparse Coding-based Denoising for\n  High-Resolution Task Functional Connectivity MRI Analysis", "comments": "8 pages, 3 figures, MLMI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel denoising framework for task functional Magnetic Resonance\nImaging (tfMRI) data to delineate the high-resolution spatial pattern of the\nbrain functional connectivity via dictionary learning and sparse coding (DLSC).\nIn order to address the limitations of the unsupervised DLSC-based fMRI\nstudies, we utilize the prior knowledge of task paradigm in the learning step\nto train a data-driven dictionary and to model the sparse representation. We\napply the proposed DLSC-based method to Human Connectome Project (HCP) motor\ntfMRI dataset. Studies on the functional connectivity of cerebrocerebellar\ncircuits in somatomotor networks show that the DLSC-based denoising framework\ncan significantly improve the prominent connectivity patterns, in comparison to\nthe temporal non-local means (tNLM)-based denoising method as well as the case\nwithout denoising, which is consistent and neuroscientifically meaningful\nwithin motor area. The promising results show that the proposed method can\nprovide an important foundation for the high-resolution functional connectivity\nanalysis, and provide a better approach for fMRI preprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:20:04 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Jeong", "Seongah", ""], ["Li", "Xiang", ""], ["Yang", "Jiarui", ""], ["Li", "Quanzheng", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1707.07012", "submitter": "Quoc Le", "authors": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "title": "Learning Transferable Architectures for Scalable Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 18:10:26 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 01:37:56 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 07:48:01 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 05:12:21 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zoph", "Barret", ""], ["Vasudevan", "Vijay", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1707.07113", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Joeri Hermans, Kyle Cranmer", "title": "Adversarial Variational Optimization of Non-Differentiable Simulators", "comments": "v4: Final version published at AISTATS 2019; v5: Fixed typo in Eqn 13", "journal-ref": "PMLR 89:1438-1447, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex computer simulators are increasingly used across fields of science as\ngenerative models tying parameters of an underlying theory to experimental\nobservations. Inference in this setup is often difficult, as simulators rarely\nadmit a tractable density or likelihood function. We introduce Adversarial\nVariational Optimization (AVO), a likelihood-free inference algorithm for\nfitting a non-differentiable generative model incorporating ideas from\ngenerative adversarial networks, variational optimization and empirical Bayes.\nWe adapt the training procedure of generative adversarial networks by replacing\nthe differentiable generative network with a domain-specific simulator. We\nsolve the resulting non-differentiable minimax problem by minimizing\nvariational upper bounds of the two adversarial objectives. Effectively, the\nprocedure results in learning a proposal distribution over simulator\nparameters, such that the JS divergence between the marginal distribution of\nthe synthetic data and the empirical distribution of observed data is\nminimized. We evaluate and compare the method with simulators producing both\ndiscrete and continuous data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 07:05:38 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:37:57 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 12:40:38 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 06:18:46 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2020 07:11:49 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Louppe", "Gilles", ""], ["Hermans", "Joeri", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1707.07124", "submitter": "Imanol Perez Arribas", "authors": "Imanol Perez Arribas, Kate Saunders, Guy Goodwin and Terry Lyons", "title": "A signature-based machine learning model for bipolar disorder and\n  borderline personality disorder", "comments": null, "journal-ref": null, "doi": "10.1038/s41398-018-0334-0", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile technologies offer opportunities for higher resolution monitoring of\nhealth conditions. This opportunity seems of particular promise in psychiatry\nwhere diagnoses often rely on retrospective and subjective recall of mood\nstates. However, getting actionable information from these rather complex time\nseries is challenging, and at present the implications for clinical care are\nlargely hypothetical. This research demonstrates that, with well chosen cohorts\n(of bipolar disorder, borderline personality disorder, and control) and modern\nmethods, it is possible to objectively learn to identify distinctive behaviour\nover short periods (20 reports) that effectively separate the cohorts.\nParticipants with bipolar disorder or borderline personality disorder and\nhealthy volunteers completed daily mood ratings using a bespoke smartphone app\nfor up to a year. A signature-based machine learning model was used to classify\nparticipants on the basis of the interrelationship between the different mood\nitems assessed and to predict subsequent mood. The signature methodology was\nsignificantly superior to earlier statistical approaches applied to this data\nin distinguishing the participant three groups, clearly placing 75% into their\noriginal groups on the basis of their reports. Subsequent mood ratings were\ncorrectly predicted with greater than 70% accuracy in all groups. Prediction of\nmood was most accurate in healthy volunteers (89-98%) compared to bipolar\ndisorder (82-90%) and borderline personality disorder (70-78%).\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 08:47:21 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 15:51:35 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Arribas", "Imanol Perez", ""], ["Saunders", "Kate", ""], ["Goodwin", "Guy", ""], ["Lyons", "Terry", ""]]}, {"id": "1707.07196", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis and Georgios B. Giannakis", "title": "Sketched Subspace Clustering", "comments": "P. A. Traganitis and G. B. Giannakis, \"Sketched Subspace Clustering,\"\n  IEEE Transactions on Signal Processing, vol. 66, to appear 2018", "journal-ref": null, "doi": "10.1109/TSP.2017.2781649", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immense amount of daily generated and communicated data presents unique\nchallenges in their processing. Clustering, the grouping of data without the\npresence of ground-truth labels, is an important tool for drawing inferences\nfrom data. Subspace clustering (SC) is a relatively recent method that is able\nto successfully classify nonlinearly separable data in a multitude of settings.\nIn spite of their high clustering accuracy, SC methods incur prohibitively high\ncomputational complexity when processing large volumes of high-dimensional\ndata. Inspired by random sketching approaches for dimensionality reduction, the\npresent paper introduces a randomized scheme for SC, termed Sketch-SC, tailored\nfor large volumes of high-dimensional data. Sketch-SC accelerates the\ncomputationally heavy parts of state-of-the-art SC approaches by compressing\nthe data matrix across both dimensions using random projections, thus enabling\nfast and accurate large-scale SC. Performance analysis as well as extensive\nnumerical tests on real data corroborate the potential of Sketch-SC and its\ncompetitive performance relative to state-of-the-art scalable SC approaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 17:13:26 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 16:58:26 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1707.07240", "submitter": "Bin Wang", "authors": "Bin Wang and Zhijian Ou", "title": "Language modeling with Neural trans-dimensional random fields", "comments": "6 pages, 2 figures and 3 tables, accepted to ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trans-dimensional random field language models (TRF LMs) have recently been\nintroduced, where sentences are modeled as a collection of random fields. The\nTRF approach has been shown to have the advantages of being computationally\nmore efficient in inference than LSTM LMs with close performance and being able\nto flexibly integrating rich features. In this paper we propose neural TRFs,\nbeyond of the previous discrete TRFs that only use linear potentials with\ndiscrete features. The idea is to use nonlinear potentials with continuous\nfeatures, implemented by neural networks (NNs), in the TRF framework. Neural\nTRFs combine the advantages of both NNs and TRFs. The benefits of word\nembedding, nonlinear feature learning and larger context modeling are inherited\nfrom the use of NNs. At the same time, the strength of efficient inference by\navoiding expensive softmax is preserved. A number of technical contributions,\nincluding employing deep convolutional neural networks (CNNs) to define the\npotentials and incorporating the joint stochastic approximation (JSA) strategy\nin the training algorithm, are developed in this work, which enable us to\nsuccessfully train neural TRF LMs. Various LMs are evaluated in terms of speech\nrecognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The\nresults show that neural TRF LMs not only improve over discrete TRF LMs, but\nalso perform slightly better than LSTM LMs with only one fifth of parameters\nand 16x faster inference efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 03:06:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 01:25:18 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 08:28:42 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""]]}, {"id": "1707.07269", "submitter": "Damien Garreau", "authors": "Damien Garreau, Wittawat Jitkrittum, Motonobu Kanagawa", "title": "Large sample analysis of the median heuristic", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In kernel methods, the median heuristic has been widely used as a way of\nsetting the bandwidth of RBF kernels. While its empirical performances make it\na safe choice under many circumstances, there is little theoretical\nunderstanding of why this is the case. Our aim in this paper is to advance our\nunderstanding of the median heuristic by focusing on the setting of kernel\ntwo-sample test. We collect new findings that may be of interest for both\ntheoreticians and practitioners. In theory, we provide a convergence analysis\nthat shows the asymptotic normality of the bandwidth chosen by the median\nheuristic in the setting of kernel two-sample test. Systematic empirical\ninvestigations are also conducted in simple settings, comparing the\nperformances based on the bandwidths chosen by the median heuristic and those\nby the maximization of test power.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 09:32:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 13:25:51 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 09:48:51 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Garreau", "Damien", ""], ["Jitkrittum", "Wittawat", ""], ["Kanagawa", "Motonobu", ""]]}, {"id": "1707.07287", "submitter": "Pavel Gurevich", "authors": "Pavel Gurevich, Hannes Stuke", "title": "Pairing an arbitrary regressor with an artificial neural network\n  estimating aleatoric uncertainty", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a general approach to quantification of different forms of\naleatoric uncertainty in regression tasks performed by artificial neural\nnetworks. It is based on the simultaneous training of two neural networks with\na joint loss function and a specific hyperparameter $\\lambda>0$ that allows for\nautomatically detecting noisy and clean regions in the input space and\ncontrolling their {\\em relative contribution} to the loss and its gradients.\nAfter the model has been trained, one of the networks performs predictions and\nthe other quantifies the uncertainty of these predictions by estimating the\nlocally averaged loss of the first one. Unlike in many classical uncertainty\nquantification methods, we do not assume any a priori knowledge of the ground\ntruth probability distribution, neither do we, in general, maximize the\nlikelihood of a chosen parametric family of distributions. We analyze the\nlearning process and the influence of clean and noisy regions of the input\nspace on the loss surface, depending on $\\lambda$. In particular, we show that\nsmall values of $\\lambda$ increase the relative contribution of clean regions\nto the loss and its gradients. This explains why choosing small $\\lambda$\nallows for better predictions compared with neural networks without uncertainty\ncounterparts and those based on classical likelihood maximization. Finally, we\ndemonstrate that one can naturally form ensembles of pairs of our networks and\nthus capture both aleatoric and epistemic uncertainty and avoid overfitting.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 12:07:58 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 13:54:40 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 07:07:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gurevich", "Pavel", ""], ["Stuke", "Hannes", ""]]}, {"id": "1707.07341", "submitter": "Michael Hughes", "authors": "Michael C. Hughes and Leah Weiner and Gabriel Hope and Thomas H. McCoy\n  Jr. and Roy H. Perlis and Erik B. Sudderth and Finale Doshi-Velez", "title": "Prediction-Constrained Training for Semi-Supervised Mixture and Topic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervisory signals have the potential to make low-dimensional data\nrepresentations, like those learned by mixture and topic models, more\ninterpretable and useful. We propose a framework for training latent variable\nmodels that explicitly balances two goals: recovery of faithful generative\nexplanations of high-dimensional data, and accurate prediction of associated\nsemantic labels. Existing approaches fail to achieve these goals due to an\nincomplete treatment of a fundamental asymmetry: the intended application is\nalways predicting labels from data, not data from labels. Our\nprediction-constrained objective for training generative models coherently\nintegrates loss-based supervisory signals while enabling effective\nsemi-supervised learning from partially labeled data. We derive learning\nalgorithms for semi-supervised mixture and topic models using stochastic\ngradient descent with automatic differentiation. We demonstrate improved\nprediction quality compared to several previous supervised topic models,\nachieving predictions competitive with high-dimensional logistic regression on\ntext sentiment analysis and electronic health records tasks while\nsimultaneously learning interpretable topics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 20:19:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hughes", "Michael C.", ""], ["Weiner", "Leah", ""], ["Hope", "Gabriel", ""], ["McCoy", "Thomas H.", "Jr."], ["Perlis", "Roy H.", ""], ["Sudderth", "Erik B.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1707.07409", "submitter": "Rajiv Sambasivan", "authors": "Rajiv Sambasivan, Sourish Das", "title": "Big Data Regression Using Tree Based Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling regression to large datasets is a common problem in many application\nareas. We propose a two step approach to scaling regression to large datasets.\nUsing a regression tree (CART) to segment the large dataset constitutes the\nfirst step of this approach. The second step of this approach is to develop a\nsuitable regression model for each segment. Since segment sizes are not very\nlarge, we have the ability to apply sophisticated regression techniques if\nrequired. A nice feature of this two step approach is that it can yield models\nthat have good explanatory power as well as good predictive performance.\nEnsemble methods like Gradient Boosted Trees can offer excellent predictive\nperformance but may not provide interpretable models. In the experiments\nreported in this study, we found that the predictive performance of the\nproposed approach matched the predictive performance of Gradient Boosted Trees.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 05:33:37 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 01:55:12 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""]]}, {"id": "1707.07425", "submitter": "Stefan Lessmann", "authors": "Norman Hiob and Stefan Lessmann", "title": "Health Analytics: a systematic review of approaches to detect phenotype\n  cohorts using electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a systematic review of state-of-the-art approaches to\nidentify patient cohorts using electronic health records. It gives a\ncomprehensive overview of the most commonly de-tected phenotypes and its\nunderlying data sets. Special attention is given to preprocessing of in-put\ndata and the different modeling approaches. The literature review confirms\nnatural language processing to be a promising approach for electronic\nphenotyping. However, accessibility and lack of natural language process\nstandards for medical texts remain a challenge. Future research should develop\nsuch standards and further investigate which machine learning approaches are\nbest suited to which type of medical data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 07:19:57 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hiob", "Norman", ""], ["Lessmann", "Stefan", ""]]}, {"id": "1707.07443", "submitter": "Cem Tekin", "authors": "A. \\\"Omer Sar{\\i}ta\\c{c} and Cem Tekin", "title": "Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms:\n  A Case with Bounded Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the combinatorial multi-armed bandit problem (CMAB)\nwith probabilistically triggered arms (PTAs). Under the assumption that the arm\ntriggering probabilities (ATPs) are positive for all arms, we prove that a\nclass of upper confidence bound (UCB) policies, named Combinatorial UCB with\nexploration rate $\\kappa$ (CUCB-$\\kappa$), and Combinatorial Thompson Sampling\n(CTS), which estimates the expected states of the arms via Thompson sampling,\nachieve bounded regret. In addition, we prove that CUCB-$0$ and CTS incur\n$O(\\sqrt{T})$ gap-independent regret. These results improve the results in\nprevious works, which show $O(\\log T)$ gap-dependent and $O(\\sqrt{T\\log T})$\ngap-independent regrets, respectively, under no assumptions on the ATPs. Then,\nwe numerically evaluate the performance of CUCB-$\\kappa$ and CTS in a\nreal-world movie recommendation problem, where the actions correspond to\nrecommending a set of movies, the arms correspond to the edges between the\nmovies and the users, and the goal is to maximize the total number of users\nthat are attracted by at least one movie. Our numerical results complement our\ntheoretical findings on bounded regret. Apart from this problem, our results\nalso directly apply to the online influence maximization (OIM) problem studied\nin numerous prior works.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 09:01:46 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Sar\u0131ta\u00e7", "A. \u00d6mer", ""], ["Tekin", "Cem", ""]]}, {"id": "1707.07493", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman, Julia Kiseleva, Maarten de Rijke", "title": "Modeling Label Ambiguity for Neural List-Wise Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  List-wise learning to rank methods are considered to be the state-of-the-art.\nOne of the major problems with these methods is that the ambiguous nature of\nrelevance labels in learning to rank data is ignored. Ambiguity of relevance\nlabels refers to the phenomenon that multiple documents may be assigned the\nsame relevance label for a given query, so that no preference order should be\nlearned for those documents. In this paper we propose a novel sampling\ntechnique for computing a list-wise loss that can take into account this\nambiguity. We show the effectiveness of the proposed method by training a\n3-layer deep neural network. We compare our new loss function to two strong\nbaselines: ListNet and ListMLE. We show that our method generalizes better and\nsignificantly outperforms other methods on the validation and test sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 11:28:21 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Jagerman", "Rolf", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1707.07498", "submitter": "Tomasz Kacprzak", "authors": "Tomasz Kacprzak, J\\\"org Herbel, Adam Amara, and Alexandre\n  R\\'efr\\'egier", "title": "Accelerating Approximate Bayesian Computation with Quantile Regression:\n  Application to Cosmological Redshift Distributions", "comments": "10 pages, 5 figures, prepared for submission to JCAP", "journal-ref": null, "doi": "10.1088/1475-7516/2018/02/042", "report-no": null, "categories": "astro-ph.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a method to obtain a posterior\ndistribution without a likelihood function, using simulations and a set of\ndistance metrics. For that reason, it has recently been gaining popularity as\nan analysis tool in cosmology and astrophysics. Its drawback, however, is a\nslow convergence rate. We propose a novel method, which we call qABC, to\naccelerate ABC with Quantile Regression. In this method, we create a model of\nquantiles of distance measure as a function of input parameters. This model is\ntrained on a small number of simulations and estimates which regions of the\nprior space are likely to be accepted into the posterior. Other regions are\nthen immediately rejected. This procedure is then repeated as more simulations\nare available. We apply it to the practical problem of estimation of redshift\ndistribution of cosmological samples, using forward modelling developed in\nprevious work. The qABC method converges to nearly same posterior as the basic\nABC. It uses, however, only 20\\% of the number of simulations compared to basic\nABC, achieving a fivefold gain in execution time for our problem. For other\nproblems the acceleration rate may vary; it depends on how close the prior is\nto the final posterior. We discuss possible improvements and extensions to this\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 11:47:56 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 16:55:23 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Kacprzak", "Tomasz", ""], ["Herbel", "J\u00f6rg", ""], ["Amara", "Adam", ""], ["R\u00e9fr\u00e9gier", "Alexandre", ""]]}, {"id": "1707.07539", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Ming Yan, Chendi Huang, Jiechao Xiong, Qingming Huang,\n  Yuan Yao", "title": "Exploring Outliers in Crowdsourced Ranking for QoE", "comments": "accepted by ACM Multimedia 2017 (Oral presentation). arXiv admin\n  note: text overlap with arXiv:1407.7636", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a crucial part of robust evaluation for crowdsourceable\nassessment of Quality of Experience (QoE) and has attracted much attention in\nrecent years. In this paper, we propose some simple and fast algorithms for\noutlier detection and robust QoE evaluation based on the nonconvex optimization\nprinciple. Several iterative procedures are designed with or without knowing\nthe number of outliers in samples. Theoretical analysis is given to show that\nsuch procedures can reach statistically good estimates under mild conditions.\nFinally, experimental results with simulated and real-world crowdsourcing\ndatasets show that the proposed algorithms could produce similar performance to\nHuber-LASSO approach in robust ranking, yet with nearly 8 or 90 times speed-up,\nwithout or with a prior knowledge on the sparsity size of outliers,\nrespectively. Therefore the proposed methodology provides us a set of helpful\ntools for robust QoE evaluation with crowdsourcing data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 10:34:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Xu", "Qianqian", ""], ["Yan", "Ming", ""], ["Huang", "Chendi", ""], ["Xiong", "Jiechao", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1707.07576", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Kai Puolam\\\"aki, Antti Ukkonen", "title": "Interpreting Classifiers through Attribute Interactions in Datasets", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the novel ASTRID method for investigating which\nattribute interactions classifiers exploit when making predictions. Attribute\ninteractions in classification tasks mean that two or more attributes together\nprovide stronger evidence for a particular class label. Knowledge of such\ninteractions makes models more interpretable by revealing associations between\nattributes. This has applications, e.g., in pharmacovigilance to identify\ninteractions between drugs or in bioinformatics to investigate associations\nbetween single nucleotide polymorphisms. We also show how the found attribute\npartitioning is related to a factorisation of the data generating distribution\nand empirically demonstrate the utility of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:34:28 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Henelius", "Andreas", ""], ["Puolam\u00e4ki", "Kai", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1707.07620", "submitter": "Saptarshi Das", "authors": "Shre Kumar Chatterjee, Saptarshi Das, Koushik Maharatna, Elisa Masi,\n  Luisa Santopolo, Ilaria Colzi, Stefano Mancuso and Andrea Vitaletti", "title": "Comparison of Decision Tree Based Classification Strategies to Detect\n  External Chemical Stimuli from Raw and Filtered Plant Electrical Response", "comments": null, "journal-ref": "Sensors and Actuators B: Chemical, vol. 249, pp. 278-295, Oct.\n  2017", "doi": "10.1016/j.snb.2017.04.071", "report-no": null, "categories": "physics.bio-ph cs.LG physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants monitor their surrounding environment and control their physiological\nfunctions by producing an electrical response. We recorded electrical signals\nfrom different plants by exposing them to Sodium Chloride (NaCl), Ozone (O3)\nand Sulfuric Acid (H2SO4) under laboratory conditions. After applying\npre-processing techniques such as filtering and drift removal, we extracted few\nstatistical features from the acquired plant electrical signals. Using these\nfeatures, combined with different classification algorithms, we used a decision\ntree based multi-class classification strategy to identify the three different\nexternal chemical stimuli. We here present our exploration to obtain the\noptimum set of ranked feature and classifier combination that can separate a\nparticular chemical stimulus from the incoming stream of plant electrical\nsignals. The paper also reports an exhaustive comparison of similar feature\nbased classification using the filtered and the raw plant signals, containing\nthe high frequency stochastic part and also the low frequency trends present in\nit, as two different cases for feature extraction. The work, presented in this\npaper opens up new possibilities for using plant electrical signals to monitor\nand detect other environmental stimuli apart from NaCl, O3 and H2SO4 in future.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 19:00:14 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Chatterjee", "Shre Kumar", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Masi", "Elisa", ""], ["Santopolo", "Luisa", ""], ["Colzi", "Ilaria", ""], ["Mancuso", "Stefano", ""], ["Vitaletti", "Andrea", ""]]}, {"id": "1707.07637", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "Copy the dynamics using a learning machine", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to generally construct a dynamical system to simulate a black\nsystem without recovering the equations of motion of the latter? Here we show\nthat this goal can be approached by a learning machine. Trained by a set of\ninput-output responses or a segment of time series of a black system, a\nlearning machine can be served as a copy system to mimic the dynamics of\nvarious black systems. It can not only behave as the black system at the\nparameter set that the training data are made, but also recur the evolution\nhistory of the black system. As a result, the learning machine provides an\neffective way for prediction, and enables one to probe the global dynamics of a\nblack system. These findings have significance for practical systems whose\nequations of motion cannot be approached accurately. Examples of copying the\ndynamics of an artificial neural network, the Lorenz system, and a variable\nstar are given. Our idea paves a possible way towards copy a living brain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 16:35:23 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1707.07657", "submitter": "Ehsan Sadrfaridpour", "authors": "E. Sadrfaridpour, T. Razzaghi, I. Safro", "title": "Engineering fast multilevel support vector machines", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of solving nonlinear support vector machine\n(SVM) is prohibitive on large-scale data. In particular, this issue becomes\nvery sensitive when the data represents additional difficulties such as highly\nimbalanced class sizes. Typically, nonlinear kernels produce significantly\nhigher classification quality to linear kernels but introduce extra kernel and\nmodel parameters which requires computationally expensive fitting. This\nincreases the quality but also reduces the performance dramatically. We\nintroduce a generalized fast multilevel framework for regular and weighted SVM\nand discuss several versions of its algorithmic components that lead to a good\ntrade-off between quality and time. Our framework is implemented using PETSc\nwhich allows an easy integration with scientific computing tasks. The\nexperimental results demonstrate significant speed up compared to the\nstate-of-the-art nonlinear SVM libraries.\n  Reproducibility: our source code, documentation and parameters are available\nat https:// github.com/esadr/mlsvm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 17:32:37 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 00:41:30 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 02:37:58 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sadrfaridpour", "E.", ""], ["Razzaghi", "T.", ""], ["Safro", "I.", ""]]}, {"id": "1707.07708", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang", "title": "Per-instance Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a refinement of differential privacy --- per instance\ndifferential privacy (pDP), which captures the privacy of a specific individual\nwith respect to a fixed data set. We show that this is a strict generalization\nof the standard DP and inherits all its desirable properties, e.g.,\ncomposition, invariance to side information and closedness to postprocessing,\nexcept that they all hold for every instance separately. When the data is drawn\nfrom a distribution, we show that per-instance DP implies generalization.\nMoreover, we provide explicit calculations of the per-instance DP for the\noutput perturbation on a class of smooth learning problems. The result reveals\nan interesting and intuitive fact that an individual has stronger privacy if\nhe/she has small \"leverage score\" with respect to the data set and if he/she\ncan be predicted more accurately using the leave-one-out data set. Our\nsimulation shows several orders-of-magnitude more favorable privacy and utility\ntrade-off when we consider the privacy of only the users in the data set. In a\ncase study on differentially private linear regression, provide a novel\nanalysis of the One-Posterior-Sample (OPS) estimator and show that when the\ndata set is well-conditioned it provides $(\\epsilon,\\delta)$-pDP for any target\nindividuals and matches the exact lower bound up to a\n$1+\\tilde{O}(n^{-1}\\epsilon^{-2})$ multiplicative factor. We also demonstrate\nhow we can use a \"pDP to DP conversion\" step to design AdaOPS which uses\nadaptive regularization to achieve the same results with\n$(\\epsilon,\\delta)$-DP.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 18:35:31 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 01:39:20 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 11:15:26 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 07:11:31 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wang", "Yu-Xiang", ""]]}, {"id": "1707.07716", "submitter": "Jiasen Yang", "authors": "Jiasen Yang, Bruno Ribeiro, Jennifer Neville", "title": "Stochastic Gradient Descent for Relational Logistic Regression via\n  Partial Network Crawls", "comments": "7 pages, 3 figures, Proceedings of the Seventh International Workshop\n  on Statistical Relational AI (StarAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in statistical relational learning has produced a number of methods\nfor learning relational models from large-scale network data. While these\nmethods have been successfully applied in various domains, they have been\ndeveloped under the unrealistic assumption of full data access. In practice,\nhowever, the data are often collected by crawling the network, due to\nproprietary access, limited resources, and privacy concerns. Recently, we\nshowed that the parameter estimates for relational Bayes classifiers computed\nfrom network samples collected by existing network crawlers can be quite\ninaccurate, and developed a crawl-aware estimation method for such models\n(Yang, Ribeiro, and Neville, 2017). In this work, we extend the methodology to\nlearning relational logistic regression models via stochastic gradient descent\nfrom partial network crawls, and show that the proposed method yields accurate\nparameter estimates and confidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 19:32:16 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 01:12:29 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Yang", "Jiasen", ""], ["Ribeiro", "Bruno", ""], ["Neville", "Jennifer", ""]]}, {"id": "1707.07785", "submitter": "Seyed Mehran Kazemi", "authors": "Seyed Mehran Kazemi, Bahare Fatemi, Alexandra Kim, Zilun Peng, Moumita\n  Roy Tora, Xing Zeng, Matthew Dirks, David Poole", "title": "Comparing Aggregators for Relational Probabilistic Models", "comments": "8 pages, Accepted at Statistical Relational AI (StarAI) workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Relational probabilistic models have the challenge of aggregation, where one\nvariable depends on a population of other variables. Consider the problem of\npredicting gender from movie ratings; this is challenging because the number of\nmovies per user and users per movie can vary greatly. Surprisingly, aggregation\nis not well understood. In this paper, we show that existing relational models\n(implicitly or explicitly) either use simple numerical aggregators that lose\ngreat amounts of information, or correspond to naive Bayes, logistic\nregression, or noisy-OR that suffer from overconfidence. We propose new simple\naggregators and simple modifications of existing models that empirically\noutperform the existing ones. The intuition we provide on different (existing\nor new) models and their shortcomings plus our empirical findings promise to\nform the foundation for future representations.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 01:32:20 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Kazemi", "Seyed Mehran", ""], ["Fatemi", "Bahare", ""], ["Kim", "Alexandra", ""], ["Peng", "Zilun", ""], ["Tora", "Moumita Roy", ""], ["Zeng", "Xing", ""], ["Dirks", "Matthew", ""], ["Poole", "David", ""]]}, {"id": "1707.07821", "submitter": "Heng Wang", "authors": "Shujian Yu, Zubin Abraham, Heng Wang, Mohak Shah, Yantao Wei and\n  Jos\\'e C. Pr\\'incipe", "title": "Concept Drift Detection and Adaptation with Hierarchical Hypothesis\n  Testing", "comments": "Manuscript accepted by the Journal of The Franklin Institute. A short\n  version of this manuscript, titled \"Concept Drift Detection with Hierarchical\n  Hypothesis Test\", was presented at the 2017 SIAM International Conference on\n  Data Mining (SDM) https://epubs.siam.org/doi/10.1137/1.9781611974973.86", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental issue for statistical classification models in a streaming\nenvironment is that the joint distribution between predictor and response\nvariables changes over time (a phenomenon also known as concept drifts), such\nthat their classification performance deteriorates dramatically. In this paper,\nwe first present a hierarchical hypothesis testing (HHT) framework that can\ndetect and also adapt to various concept drift types (e.g., recurrent or\nirregular, gradual or abrupt), even in the presence of imbalanced data labels.\nA novel concept drift detector, namely Hierarchical Linear Four Rates (HLFR),\nis implemented under the HHT framework thereafter. By substituting a\nwidely-acknowledged retraining scheme with an adaptive training strategy, we\nfurther demonstrate that the concept drift adaptation capability of HLFR can be\nsignificantly boosted. The theoretical analysis on the Type-I and Type-II\nerrors of HLFR is also performed. Experiments on both simulated and real-world\ndatasets illustrate that our methods outperform state-of-the-art methods in\nterms of detection precision, detection delay as well as the adaptability\nacross different concept drift types.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:05:27 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 01:11:13 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 01:11:08 GMT"}, {"version": "v4", "created": "Wed, 29 Aug 2018 22:28:04 GMT"}, {"version": "v5", "created": "Mon, 17 Sep 2018 07:20:37 GMT"}, {"version": "v6", "created": "Fri, 8 Feb 2019 18:54:26 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Yu", "Shujian", ""], ["Abraham", "Zubin", ""], ["Wang", "Heng", ""], ["Shah", "Mohak", ""], ["Wei", "Yantao", ""], ["Pr\u00edncipe", "Jos\u00e9 C.", ""]]}, {"id": "1707.07831", "submitter": "Zhun Sun", "authors": "Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "Linear Discriminant Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method for training of GANs for unsupervised and class\nconditional generation of images, called Linear Discriminant GAN (LD-GAN). The\ndiscriminator of an LD-GAN is trained to maximize the linear separability\nbetween distributions of hidden representations of generated and targeted\nsamples, while the generator is updated based on the decision hyper-planes\ncomputed by performing LDA over the hidden representations. LD-GAN provides a\nconcrete metric of separation capacity for the discriminator, and we\nexperimentally show that it is possible to stabilize the training of LD-GAN\nsimply by calibrating the update frequencies between generators and\ndiscriminators in the unsupervised case, without employment of normalization\nmethods and constraints on weights. In the class conditional generation tasks,\nthe proposed method shows improved training stability together with better\ngeneralization performance compared to WGAN that employs an auxiliary\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:33:49 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1707.07885", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Wind models and cross-site interpolation for the refugee reception\n  islands in Greece", "comments": "23 figures, 3 tables, 17 references", "journal-ref": null, "doi": null, "report-no": "HG/DA.0725.01v1", "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, the wind data series from five locations in Aegean Sea\nislands, the most active `hotspots' in terms of refugee influx during the\nOct/2015 - Jan/2016 period, are investigated. The analysis of the\nthree-per-site data series includes standard statistical analysis and\nparametric distributions, auto-correlation analysis, cross-correlation analysis\nbetween the sites, as well as various ARMA models for estimating the\nfeasibility and accuracy of such spatio-temporal linear regressors for\npredictive analytics. Strong correlations are detected across specific sites\nand appropriately trained ARMA(7,5) models achieve 1-day look-ahead error\n(RMSE) of less than 1.9 km/h on average wind speed. The results show that such\ndata-driven statistical approaches are extremely useful in identifying\nunexpected and sometimes counter-intuitive associations between the available\nspatial data nodes, which is very important when designing corresponding models\nfor short-term forecasting of sea condition, especially average wave height and\ndirection, which is in fact what defines the associated weather risk of\ncrossing these passages in refugee influx patterns.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:48:44 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1707.07938", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "Error Bounds for Piecewise Smooth and Switching Regression", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice,after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with regression problems, in which the nonsmooth target is\nassumed to switch between different operating modes. Specifically, piecewise\nsmooth (PWS) regression considers target functions switching deterministically\nvia a partition of the input space, while switching regression considers\narbitrary switching laws. The paper derives generalization error bounds in\nthese two settings by following the approach based on Rademacher complexities.\nFor PWS regression, our derivation involves a chaining argument and a\ndecomposition of the covering numbers of PWS classes in terms of the ones of\ntheir component functions and the capacity of the classifier partitioning the\ninput space. This yields error bounds with a radical dependency on the number\nof modes. For switching regression, the decomposition can be performed directly\nat the level of the Rademacher complexities, which yields bounds with a linear\ndependency on the number of modes. By using once more chaining and a\ndecomposition at the level of covering numbers, we show how to recover a\nradical dependency. Examples of applications are given in particular for PWS\nand swichting regression with linear and kernel-based component functions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 12:06:22 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 14:09:26 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1707.07976", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang", "title": "Scaled Nuclear Norm Minimization for Low-Rank Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the nuclear norm of a matrix has been shown to be very efficient\nin reconstructing a low-rank sampled matrix. Furthermore, minimizing the sum of\nnuclear norms of matricizations of a tensor has been shown to be very efficient\nin recovering a low-Tucker-rank sampled tensor. In this paper, we propose to\nrecover a low-TT-rank sampled tensor by minimizing a weighted sum of nuclear\nnorms of unfoldings of the tensor. We provide numerical results to show that\nour proposed method requires significantly less number of samples to recover to\nthe original tensor in comparison with simply minimizing the sum of nuclear\nnorms since the structure of the unfoldings in the TT tensor model is\nfundamentally different from that of matricizations in the Tucker tensor model.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 13:16:43 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1707.08005", "submitter": "Dacheng Tao", "authors": "Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, Dacheng Tao", "title": "Towards Evolutional Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing convolutional neural networks (CNNs) is essential for\ntransferring the success of CNNs to a wide variety of applications to mobile\ndevices. In contrast to directly recognizing subtle weights or filters as\nredundant in a given CNN, this paper presents an evolutionary method to\nautomatically eliminate redundant convolution filters. We represent each\ncompressed network as a binary individual of specific fitness. Then, the\npopulation is upgraded at each evolutionary iteration using genetic operations.\nAs a result, an extremely compact CNN is generated using the fittest\nindividual. In this approach, either large or small convolution filters can be\nredundant, and filters in the compressed network are more distinct. In\naddition, since the number of filters in each convolutional layer is reduced,\nthe number of filter channels and the size of feature maps are also decreased,\nnaturally improving both the compression and speed-up ratios. Experiments on\nbenchmark deep CNN models suggest the superiority of the proposed algorithm\nover the state-of-the-art compression methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:02:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Qiu", "Jiayan", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1707.08015", "submitter": "Benjamin Bullough", "authors": "Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, Joseph\n  R. Zipkin", "title": "Predicting Exploitation of Disclosed Software Vulnerabilities Using\n  Open-source Data", "comments": null, "journal-ref": "In Proceedings of the 3rd ACM on International Workshop on\n  Security And Privacy Analytics (IWSPA 2017). ACM, New York, NY, USA, 45-53", "doi": "10.1145/3041008.3041009", "report-no": null, "categories": "cs.CR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, thousands of software vulnerabilities are discovered and reported\nto the public. Unpatched known vulnerabilities are a significant security risk.\nIt is imperative that software vendors quickly provide patches once\nvulnerabilities are known and users quickly install those patches as soon as\nthey are available. However, most vulnerabilities are never actually exploited.\nSince writing, testing, and installing software patches can involve\nconsiderable resources, it would be desirable to prioritize the remediation of\nvulnerabilities that are likely to be exploited. Several published research\nstudies have reported moderate success in applying machine learning techniques\nto the task of predicting whether a vulnerability will be exploited. These\napproaches typically use features derived from vulnerability databases (such as\nthe summary text describing the vulnerability) or social media posts that\nmention the vulnerability by name. However, these prior studies share multiple\nmethodological shortcomings that inflate predictive power of these approaches.\nWe replicate key portions of the prior work, compare their approaches, and show\nhow selection of training and test data critically affect the estimated\nperformance of predictive models. The results of this study point to important\nmethodological considerations that should be taken into account so that results\nreflect real-world utility.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:40:18 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Bullough", "Benjamin L.", ""], ["Yanchenko", "Anna K.", ""], ["Smith", "Christopher L.", ""], ["Zipkin", "Joseph R.", ""]]}, {"id": "1707.08040", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma and Piyush Rai", "title": "A Simple Exponential Family Framework for Zero-Shot Learning", "comments": "Accepted in ECML-PKDD 2017, 16 Pages: Code and Data are available:\n  https://github.com/vkverma01/Zero-Shot/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:28:22 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 06:50:51 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 05:37:04 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""]]}, {"id": "1707.08077", "submitter": "Josef Faller", "authors": "Josef Faller, Linbi Hong, Jennifer Cummings and Paul Sajda", "title": "A comparison of single-trial EEG classification and EEG-informed fMRI\n  across three MR compatible EEG recording systems", "comments": "1 Page, IEEE EMBS Conference 2017, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously recorded electroencephalography (EEG) and functional magnetic\nresonance imaging (fMRI) can be used to non-invasively measure the\nspatiotemporal dynamics of the human brain. One challenge is dealing with the\nartifacts that each modality introduces into the other when the two are\nrecorded concurrently, for example the ballistocardiogram (BCG). We conducted a\npreliminary comparison of three different MR compatible EEG recording systems\nand assessed their performance in terms of single-trial classification of the\nEEG when simultaneously collecting fMRI. We found tradeoffs across all three\nsystems, for example varied ease of setup and improved classification accuracy\nwith reference electrodes (REF) but not for pulse artifact subtraction (PAS) or\nreference layer adaptive filtering (RLAF).\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:34:42 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Faller", "Josef", ""], ["Hong", "Linbi", ""], ["Cummings", "Jennifer", ""], ["Sajda", "Paul", ""]]}, {"id": "1707.08092", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Mark Rudelson", "title": "Restricted Eigenvalue from Stable Rank with Applications to Sparse\n  Linear Regression", "comments": "27 pages, Updated paper with stronger results, Corrected Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional settings, where the data dimension ($d$) far exceeds the\nnumber of observations ($n$), are common in many statistical and machine\nlearning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are\nvery popular for sparse recovery in these settings. Restricted Eigenvalue (RE)\ncondition is among the weakest, and hence the most general, condition in\nliterature imposed on the Gram matrix that guarantees nice statistical\nproperties for the Lasso estimator. It is natural to ask: what families of\nmatrices satisfy the RE condition? Following a line of work in this area, we\nconstruct a new broad ensemble of dependent random design matrices that have an\nexplicit RE bound. Our construction starts with a fixed (deterministic) matrix\n$X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and\nwe show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where\n$\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high\nprobability, satisfies the RE condition. This construction allows incorporating\na fixed matrix that has an easily {\\em verifiable} condition into the design\nprocess, and allows for generation of {\\em compressed} design matrices that\nhave a lower storage requirement than a standard design matrix. We give two\napplications of this construction to sparse linear regression problems,\nincluding one to a compressed sparse regression setting where the regression\nalgorithm only has access to a compressed representation of a fixed design\nmatrix $X$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:15:18 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 03:15:26 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 15:00:12 GMT"}, {"version": "v4", "created": "Sat, 17 Feb 2018 23:44:07 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""]]}, {"id": "1707.08167", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Sebastien Rouault", "title": "On The Robustness of a Neural Network", "comments": "36th IEEE International Symposium on Reliable Distributed Systems 26\n  - 29 September 2017. Hong Kong, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 19:22:55 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:18:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "Sebastien", ""]]}, {"id": "1707.08212", "submitter": "Ilker Yildirim", "authors": "Ilker Yildirim, Tobias Gerstenberg, Basil Saeed, Marc Toussaint, Josh\n  Tenenbaum", "title": "Physical problem solving: Joint planning with symbolic, geometric, and\n  dynamic constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new task that investigates how people interact\nwith and make judgments about towers of blocks. In Experiment~1, participants\nin the lab solved a series of problems in which they had to re-configure three\nblocks from an initial to a final configuration. We recorded whether they used\none hand or two hands to do so. In Experiment~2, we asked participants online\nto judge whether they think the person in the lab used one or two hands. The\nresults revealed a close correspondence between participants' actions in the\nlab, and the mental simulations of participants online. To explain\nparticipants' actions and mental simulations, we develop a model that plans\nover a symbolic representation of the situation, executes the plan using a\ngeometric solver, and checks the plan's feasibility by taking into account the\nphysical constraints of the scene. Our model explains participants' actions and\njudgments to a high degree of quantitative accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:44:18 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Yildirim", "Ilker", ""], ["Gerstenberg", "Tobias", ""], ["Saeed", "Basil", ""], ["Toussaint", "Marc", ""], ["Tenenbaum", "Josh", ""]]}, {"id": "1707.08213", "submitter": "Lee Richardson", "authors": "Lee F. Richardson, William F. Eddy", "title": "The 2D Tree Sliding Window Discrete Fourier Transform", "comments": "15 pages, 4 figures, submitted to ACM TOMS", "journal-ref": "ACM Transactions on Mathematical Software, Vol. 45, No. 1, Article\n  12. Publication date: February 2019", "doi": "10.1145/3264426", "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the 2D Sliding Window Discrete Fourier\nTransform (SWDFT). Our algorithm avoids repeating calculations in overlapping\nwindows by storing them in a tree data-structure based on the ideas of the\nCooley- Tukey Fast Fourier Transform (FFT). For an $N_0 \\times N_1$ array and\n$n_0 \\times n_1$ windows, our algorithm takes $O(N_0 N_1 n_0 n_1)$ operations.\nWe provide a C implementation of our algorithm for the Radix-2 case, compare\nours with existing algorithms, and show how our algorithm easily extends to\nhigher dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:48:46 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:27:27 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Richardson", "Lee F.", ""], ["Eddy", "William F.", ""]]}, {"id": "1707.08238", "submitter": "Jieming Mao", "authors": "Xi Chen, Yuanzhi Li, Jieming Mao", "title": "A Nearly Instance Optimal Algorithm for Top-k Ranking under the\n  Multinomial Logit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the active learning problem of top-$k$ ranking from multi-wise\ncomparisons under the popular multinomial logit model. Our goal is to identify\nthe top-$k$ items with high probability by adaptively querying sets for\ncomparisons and observing the noisy output of the most preferred item from each\ncomparison. To achieve this goal, we design a new active ranking algorithm\nwithout using any information about the underlying items' preference scores. We\nalso establish a matching lower bound on the sample complexity even when the\nset of preference scores is given to the algorithm. These two results together\nshow that the proposed algorithm is nearly instance optimal (similar to\ninstance optimal [FLN03], but up to polylog factors). Our work extends the\nexisting literature on rank aggregation in three directions. First, instead of\nstudying a static problem with fixed data, we investigate the top-$k$ ranking\nproblem in an active learning setting. Second, we show our algorithm is nearly\ninstance optimal, which is a much stronger theoretical guarantee. Finally, we\nextend the pairwise comparison to the multi-wise comparison, which has not been\nfully explored in ranking literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 22:03:21 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 14:44:22 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Xi", ""], ["Li", "Yuanzhi", ""], ["Mao", "Jieming", ""]]}, {"id": "1707.08309", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee", "title": "Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities", "comments": "PhD thesis, Mar 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 07:41:27 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Mukherjee", "Subhabrata", ""]]}, {"id": "1707.08316", "submitter": "Raksha Kumaraswamy", "authors": "Lei Le, Raksha Kumaraswamy, Martha White", "title": "Learning Sparse Representations in Reinforcement Learning with Sparse\n  Coding", "comments": "6(+1) pages, 2 figures, International Joint Conference on Artificial\n  Intelligence 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 08:23:04 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Le", "Lei", ""], ["Kumaraswamy", "Raksha", ""], ["White", "Martha", ""]]}, {"id": "1707.08325", "submitter": "Qing-Yuan Jiang", "authors": "Qing-Yuan Jiang, Wu-Jun Li", "title": "Asymmetric Deep Supervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 09:07:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Jiang", "Qing-Yuan", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1707.08352", "submitter": "Isabel Valera", "authors": "Isabel Valera, Melanie F. Pradier and Zoubin Ghahramani", "title": "General Latent Feature Modeling for Data Exploration Tasks", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general Bayesian non- parametric latent feature model\nsuitable to per- form automatic exploratory analysis of heterogeneous datasets,\nwhere the attributes describing each object can be either discrete, continuous\nor mixed variables. The proposed model presents several important properties.\nFirst, it accounts for heterogeneous data while can be inferred in linear time\nwith respect to the number of objects and attributes. Second, its Bayesian\nnonparametric nature allows us to automatically infer the model complexity from\nthe data, i.e., the number of features necessary to capture the latent\nstructure in the data. Third, the latent features in the model are\nbinary-valued variables, easing the interpretability of the obtained latent\nfeatures in data exploration tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:07:52 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Valera", "Isabel", ""], ["Pradier", "Melanie F.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.08381", "submitter": "Junqiu Wu", "authors": "Ke Liu (1), Xiangyan Sun (3), Jun Ma (3), Zhenyu Zhou (3), Qilin Dong\n  (4), Shengwen Peng (3), Junqiu Wu (3), Suocheng Tan (3), G\\\"unter Blobel (2),\n  and Jie Fan (1) ((1) Accutar Biotechnology, (2) Laboratory of Cell Biology,\n  Howard Hughes Medical Institute, The Rockefeller University (3) Accutar\n  Biotechnology (Shanghai), (4) Fudan University)", "title": "Prediction of amino acid side chain conformation using a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network based architecture was constructed to predict amino\nacid side chain conformation with unprecedented accuracy. Amino acid side chain\nconformation prediction is essential for protein homology modeling and protein\ndesign. Current widely-adopted methods use physics-based energy functions to\nevaluate side chain conformation. Here, using a deep neural network\narchitecture without physics-based assumptions, we have demonstrated that side\nchain conformation prediction accuracy can be improved by more than 25%,\nespecially for aromatic residues compared with current standard methods. More\nstrikingly, the prediction method presented here is robust enough to identify\nindividual conformational outliers from high resolution structures in a protein\ndata bank without providing its structural factors. We envisage that our amino\nacid side chain predictor could be used as a quality check step for future\nprotein structure model validation and many other potential applications such\nas side chain assignment in Cryo-electron microscopy, crystallography model\nauto-building, protein folding and small molecule ligand docking.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:22:57 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Liu", "Ke", ""], ["Sun", "Xiangyan", ""], ["Ma", "Jun", ""], ["Zhou", "Zhenyu", ""], ["Dong", "Qilin", ""], ["Peng", "Shengwen", ""], ["Wu", "Junqiu", ""], ["Tan", "Suocheng", ""], ["Blobel", "G\u00fcnter", ""], ["Fan", "Jie", ""]]}, {"id": "1707.08384", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (1), S\\'everine Demeyer (1), Nicolas Fischer (1), Julien\n  Bect (2), Emmanuel Vazquez (3) ((1) LNE, (2) L2S, (3) GdR MASCOT-NUM)", "title": "Sequential design of experiments to estimate a probability of exceeding\n  a threshold in a multi-fidelity stochastic simulator", "comments": "61th World Statistics Congress of the International Statistical\n  Institute (ISI 2017), Jul 2017, Marrakech, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a stochastic numerical simulator to assess the\nimpact of some factors on a phenomenon. The simulator is seen as a black box\nwith inputs and outputs. The quality of a simulation, hereafter referred to as\nfidelity, is assumed to be tunable by means of an additional input of the\nsimulator (e.g., a mesh size parameter): high-fidelity simulations provide more\naccurate results, but are time-consuming. Using a limited computation-time\nbudget, we want to estimate, for any value of the physical inputs, the\nprobability that a certain scalar output of the simulator will exceed a given\ncritical threshold at the highest fidelity level. The problem is addressed in a\nBayesian framework, using a Gaussian process model of the multi-fidelity\nsimulator. We consider a Bayesian estimator of the probability, together with\nan associated measure of uncertainty, and propose a new multi-fidelity\nsequential design strategy, called Maximum Speed of Uncertainty Reduction\n(MSUR), to select the value of physical inputs and the fidelity level of new\nsimulations. The MSUR strategy is tested on an example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:35:58 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Bect", "Julien", "", "L2S"], ["Vazquez", "Emmanuel", "", "GdR MASCOT-NUM"]]}, {"id": "1707.08438", "submitter": "Samuel Li", "authors": "Samuel Li", "title": "Context-Independent Polyphonic Piano Onset Transcription with an\n  Infinite Training Dataset", "comments": "Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the recent approaches to polyphonic piano note onset transcription\nrequire training a machine learning model on a large piano database. However,\nsuch approaches are limited by dataset availability; additional training data\nis difficult to produce, and proposed systems often perform poorly on novel\nrecording conditions. We propose a method to quickly synthesize arbitrary\nquantities of training data, avoiding the need for curating large datasets.\nVarious aspects of piano note dynamics - including nonlinearity of note\nsignatures with velocity, different articulations, temporal clustering of\nonsets, and nonlinear note partial interference - are modeled to match the\ncharacteristics of real pianos. Our method also avoids the disentanglement\nproblem, a recently noted issue affecting machine-learning based approaches. We\ntrain a feed-forward neural network with two hidden layers on our generated\ntraining data and achieve both good transcription performance on the large MAPS\npiano dataset and excellent generalization qualities.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 13:46:33 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Li", "Samuel", ""]]}, {"id": "1707.08475", "submitter": "Irina Higgins", "authors": "Irina Higgins, Arka Pal, Andrei A. Rusu, Loic Matthey, Christopher P\n  Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, Alexander\n  Lerchner", "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important open problem in deep reinforcement learning\n(RL). In many scenarios of interest data is hard to obtain, so agents may learn\na source policy in a setting where data is readily available, with the hope\nthat it generalises well to the target domain. We propose a new multi-stage RL\nagent, DARLA (DisentAngled Representation Learning Agent), which learns to see\nbefore learning to act. DARLA's vision is based on learning a disentangled\nrepresentation of the observed environment. Once DARLA can see, it is able to\nacquire source policies that are robust to many domain shifts - even with no\naccess to the target domain. DARLA significantly outperforms conventional\nbaselines in zero-shot domain adaptation scenarios, an effect that holds across\na variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms\n(DQN, A3C and EC).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 14:50:51 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 16:51:02 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Higgins", "Irina", ""], ["Pal", "Arka", ""], ["Rusu", "Andrei A.", ""], ["Matthey", "Loic", ""], ["Burgess", "Christopher P", ""], ["Pritzel", "Alexander", ""], ["Botvinick", "Matthew", ""], ["Blundell", "Charles", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1707.08493", "submitter": "Brian Kulis", "authors": "Trevor Campbell, Brian Kulis, and Jonathan How", "title": "Dynamic Clustering Algorithms via Small-Variance Analysis of Markov\n  Chain Mixture Models", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametrics are a class of probabilistic models in which the\nmodel size is inferred from data. A recently developed methodology in this\nfield is small-variance asymptotic analysis, a mathematical technique for\nderiving learning algorithms that capture much of the flexibility of Bayesian\nnonparametric inference algorithms, but are simpler to implement and less\ncomputationally expensive. Past work on small-variance analysis of Bayesian\nnonparametric inference algorithms has exclusively considered batch models\ntrained on a single, static dataset, which are incapable of capturing time\nevolution in the latent structure of the data. This work presents a\nsmall-variance analysis of the maximum a posteriori filtering problem for a\ntemporally varying mixture model with a Markov dependence structure, which\ncaptures temporally evolving clusters within a dataset. Two clustering\nalgorithms result from the analysis: D-Means, an iterative clustering algorithm\nfor linearly separable, spherical clusters; and SD-Means, a spectral clustering\nalgorithm derived from a kernelized, relaxed version of the clustering problem.\nEmpirical results from experiments demonstrate the advantages of using D-Means\nand SD-Means over contemporary clustering algorithms, in terms of both\ncomputational cost and clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 15:25:06 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Campbell", "Trevor", ""], ["Kulis", "Brian", ""], ["How", "Jonathan", ""]]}, {"id": "1707.08551", "submitter": "Hao Dong", "authors": "Hao Dong, Akara Supratak, Luo Mai, Fangde Liu, Axel Oehmichen, Simiao\n  Yu, Yike Guo", "title": "TensorLayer: A Versatile Library for Efficient Deep Learning Development", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": "10.1145/3123266.3129391", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled major advances in the fields of computer vision,\nnatural language processing, and multimedia among many others. Developing a\ndeep learning system is arduous and complex, as it involves constructing neural\nnetwork architectures, managing training/trained models, tuning optimization\nprocess, preprocessing and organizing data, etc. TensorLayer is a versatile\nPython library that aims at helping researchers and engineers efficiently\ndevelop deep learning systems. It offers rich abstractions for neural networks,\nmodel and data management, and parallel workflow mechanism. While boosting\nefficiency, TensorLayer maintains both performance and scalability. TensorLayer\nwas released in September 2016 on GitHub, and has helped people from academia\nand industry develop real-world applications of deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:29:49 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 10:26:34 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 14:48:16 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dong", "Hao", ""], ["Supratak", "Akara", ""], ["Mai", "Luo", ""], ["Liu", "Fangde", ""], ["Oehmichen", "Axel", ""], ["Yu", "Simiao", ""], ["Guo", "Yike", ""]]}, {"id": "1707.08552", "submitter": "Albert Berahas", "authors": "Albert S. Berahas and Martin Tak\\'a\\v{c}", "title": "A Robust Multi-Batch L-BFGS Method for Machine Learning", "comments": "50 pages, 33 figures. Extension of NIPS 2016 paper: arXiv:1605.06049", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an implementation of the L-BFGS method designed to deal\nwith two adversarial situations. The first occurs in distributed computing\nenvironments where some of the computational nodes devoted to the evaluation of\nthe function and gradient are unable to return results on time. A similar\nchallenge occurs in a multi-batch approach in which the data points used to\ncompute function and gradients are purposely changed at each iteration to\naccelerate the learning process. Difficulties arise because L-BFGS employs\ngradient differences to update the Hessian approximations, and when these\ngradients are computed using different data points the updating process can be\nunstable. This paper shows how to perform stable quasi-Newton updating in the\nmulti-batch setting, studies the convergence properties for both convex and\nnonconvex functions, and illustrates the behavior of the algorithm in a\ndistributed computing platform on binary classification logistic regression and\nneural network training problems that arise in machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:33:43 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 22:30:25 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 17:18:19 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Berahas", "Albert S.", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1707.08561", "submitter": "Andrea Rocchetto", "authors": "Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo,\n  Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig", "title": "Quantum machine learning: a classical perspective", "comments": "v3 33 pages; typos corrected and references added", "journal-ref": "Proc. R. Soc. A, vol. 474, no. 2209, p. 20170551. The Royal\n  Society, 2018", "doi": "10.1098/rspa.2017.0551", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increased computational power and data availability, as well as\nalgorithmic advances, have led machine learning techniques to impressive\nresults in regression, classification, data-generation and reinforcement\nlearning tasks. Despite these successes, the proximity to the physical limits\nof chip fabrication alongside the increasing size of datasets are motivating a\ngrowing number of researchers to explore the possibility of harnessing the\npower of quantum computation to speed-up classical machine learning algorithms.\nHere we review the literature in quantum machine learning and discuss\nperspectives for a mixed readership of classical machine learning and quantum\ncomputation experts. Particular emphasis will be placed on clarifying the\nlimitations of quantum algorithms, how they compare with their best classical\ncounterparts and why quantum resources are expected to provide advantages for\nlearning problems. Learning in the presence of noise and certain\ncomputationally hard problems in machine learning are identified as promising\ndirections for the field. Practical questions, like how to upload classical\ndata into quantum form, will also be addressed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:48:25 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 15:48:01 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 19:20:03 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Herbster", "Mark", ""], ["Ialongo", "Alessandro Davide", ""], ["Pontil", "Massimiliano", ""], ["Rocchetto", "Andrea", ""], ["Severini", "Simone", ""], ["Wossnig", "Leonard", ""]]}, {"id": "1707.08600", "submitter": "Patrick Komiske", "authors": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Matthew D.\n  Schwartz", "title": "Pileup Mitigation with Machine Learning (PUMML)", "comments": "20 pages, 8 figures, 2 tables. Updated to JHEP version", "journal-ref": "JHEP 12 (2017) 051", "doi": "10.1007/JHEP12(2017)051", "report-no": "MIT-CTP 4924", "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pileup involves the contamination of the energy distribution arising from the\nprimary collision of interest (leading vertex) by radiation from soft\ncollisions (pileup). We develop a new technique for removing this contamination\nusing machine learning and convolutional neural networks. The network takes as\ninput the energy distribution of charged leading vertex particles, charged\npileup particles, and all neutral particles and outputs the energy distribution\nof particles coming from leading vertex alone. The PUMML algorithm performs\nremarkably well at eliminating pileup distortion on a wide range of simple and\ncomplex jet observables. We test the robustness of the algorithm in a number of\nways and discuss how the network can be trained directly on data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 18:27:43 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 17:30:20 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 21:14:44 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Komiske", "Patrick T.", ""], ["Metodiev", "Eric M.", ""], ["Nachman", "Benjamin", ""], ["Schwartz", "Matthew D.", ""]]}, {"id": "1707.08616", "submitter": "Mark Riedl", "authors": "Brent Harrison, Upol Ehsan, Mark O. Riedl", "title": "Guiding Reinforcement Learning Exploration Using Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation, specifically the use of encoder-decoder networks,\nto learn associations between natural language behavior descriptions and\nstate-action information. We then use this learned model to guide agent\nexploration using a modified version of policy shaping to make it more\neffective at learning in unseen environments. We evaluate this technique using\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\nevaluation shows that our modified policy shaping algorithm improves over a\nQ-learning agent as well as a baseline version of policy shaping.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 19:23:54 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 02:06:26 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Harrison", "Brent", ""], ["Ehsan", "Upol", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1707.08712", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil and Sheetal Kalyani", "title": "Signal and Noise Statistics Oblivious Sparse Reconstruction using\n  OMP/OLS", "comments": "14 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal matching pursuit (OMP) and orthogonal least squares (OLS) are\nwidely used for sparse signal reconstruction in under-determined linear\nregression problems. The performance of these compressed sensing (CS)\nalgorithms depends crucially on the \\textit{a priori} knowledge of either the\nsparsity of the signal ($k_0$) or noise variance ($\\sigma^2$). Both $k_0$ and\n$\\sigma^2$ are unknown in general and extremely difficult to estimate in under\ndetermined models. This limits the application of OMP and OLS in many practical\nsituations. In this article, we develop two computationally efficient\nframeworks namely TF-IGP and RRT-IGP for using OMP and OLS even when $k_0$ and\n$\\sigma^2$ are unavailable. Both TF-IGP and RRT-IGP are analytically shown to\naccomplish successful sparse recovery under the same set of restricted isometry\nconditions on the design matrix required for OMP/OLS with \\textit{a priori}\nknowledge of $k_0$ and $\\sigma^2$. Numerical simulations also indicate a highly\ncompetitive performance of TF-IGP and RRT-IGP in comparison to OMP/OLS with\n\\textit{a priori} knowledge of $k_0$ and $\\sigma^2$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 05:47:06 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1707.08820", "submitter": "Mastane Achab", "authors": "Mastane Achab, Stephan Cl\\'emen\\c{c}on, Aur\\'elien Garivier, Anne\n  Sabourin, Claire Vernade", "title": "Max K-armed bandit: On the ExtremeHunter algorithm and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the study of the max K-armed bandit problem, which\nconsists in sequentially allocating resources in order to detect extreme\nvalues. Our contribution is twofold. We first significantly refine the analysis\nof the ExtremeHunter algorithm carried out in Carpentier and Valko (2014), and\nnext propose an alternative approach, showing that, remarkably, Extreme Bandits\ncan be reduced to a classical version of the bandit problem to a certain\nextent. Beyond the formal analysis, these two approaches are compared through\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:26:55 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Achab", "Mastane", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""], ["Garivier", "Aur\u00e9lien", ""], ["Sabourin", "Anne", ""], ["Vernade", "Claire", ""]]}, {"id": "1707.09049", "submitter": "Yuan Zhao", "authors": "Yuan Zhao and Il Memming Park", "title": "Variational online learning of neural dynamics", "comments": "accepted by Frontiers in Computational Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New technologies for recording the activity of large neural populations\nduring complex behavior provide exciting opportunities for investigating the\nneural computations that underlie perception, cognition, and decision-making.\nNonlinear state space models provide an interpretable signal processing\nframework by combining an intuitive dynamical system with a probabilistic\nobservation model, which can provide insights into neural dynamics, neural\ncomputation, and development of neural prosthetics and treatment through\nfeedback control. It brings the challenge of learning both latent neural state\nand the underlying dynamical system because neither is known for neural systems\na priori. We developed a flexible online learning framework for latent\nnonlinear state dynamics and filtered latent states. Using the stochastic\ngradient variational Bayes approach, our method jointly optimizes the\nparameters of the nonlinear dynamical system, the observation model, and the\nblack-box recognition model. Unlike previous approaches, our framework can\nincorporate non-trivial distributions of observation noise and has constant\ntime and space complexity. These features make our approach amenable to\nreal-time applications and the potential to automate analysis and experimental\ndesign in ways that testably track and modify behavior using stimuli designed\nto influence learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 21:15:30 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 15:56:20 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 15:06:52 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 20:57:28 GMT"}, {"version": "v5", "created": "Mon, 29 Jun 2020 14:37:40 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhao", "Yuan", ""], ["Park", "Il Memming", ""]]}, {"id": "1707.09050", "submitter": "Julia Kreutzer", "authors": "Artem Sokolov, Julia Kreutzer, Kellen Sunderland, Pavel Danchenko,\n  Witold Szymaniak, Hagen F\\\"urstenau, Stefan Riezler", "title": "A Shared Task on Bandit Learning for Machine Translation", "comments": "Conference on Machine Translation (WMT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and describe the results of a novel shared task on bandit\nlearning for machine translation. The task was organized jointly by Amazon and\nHeidelberg University for the first time at the Second Conference on Machine\nTranslation (WMT 2017). The goal of the task is to encourage research on\nlearning machine translation from weak user feedback instead of human\nreferences or post-edits. On each of a sequence of rounds, a machine\ntranslation system is required to propose a translation for an input, and\nreceives a real-valued estimate of the quality of the proposed translation for\nlearning. This paper describes the shared task's learning and evaluation setup,\nusing services hosted on Amazon Web Services (AWS), the data and evaluation\nmetrics, and the results of various machine translation architectures and\nlearning protocols.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 21:16:41 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Sokolov", "Artem", ""], ["Kreutzer", "Julia", ""], ["Sunderland", "Kellen", ""], ["Danchenko", "Pavel", ""], ["Szymaniak", "Witold", ""], ["F\u00fcrstenau", "Hagen", ""], ["Riezler", "Stefan", ""]]}, {"id": "1707.09114", "submitter": "Junwei Lu", "authors": "Junwei Lu, Matey Neykov and Han Liu", "title": "Adaptive Inferential Method for Monotone Graph Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of undirected graphical model inference. In many\napplications, instead of perfectly recovering the unknown graph structure, a\nmore realistic goal is to infer some graph invariants (e.g., the maximum\ndegree, the number of connected subgraphs, the number of isolated nodes). In\nthis paper, we propose a new inferential framework for testing nested multiple\nhypotheses and constructing confidence intervals of the unknown graph\ninvariants under undirected graphical models. Compared to perfect graph\nrecovery, our methods require significantly weaker conditions. This paper makes\ntwo major contributions: (i) Methodologically, for testing nested multiple\nhypotheses, we propose a skip-down algorithm on the whole family of monotone\ngraph invariants (The invariants which are non-decreasing under addition of\nedges). We further show that the same skip-down algorithm also provides valid\nconfidence intervals for the targeted graph invariants. (ii) Theoretically, we\nprove that the length of the obtained confidence intervals are optimal and\nadaptive to the unknown signal strength. We also prove generic lower bounds for\nthe confidence interval length for various invariants. Numerical results on\nboth synthetic simulations and a brain imaging dataset are provided to\nillustrate the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:08:44 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lu", "Junwei", ""], ["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1707.09118", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence, Artem Sokolov, Stefan Riezler", "title": "Counterfactual Learning from Bandit Feedback under Deterministic\n  Logging: A Case Study in Statistical Machine Translation", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2017, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:32:47 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 13:22:23 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 13:44:31 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Lawrence", "Carolin", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1707.09157", "submitter": "Francis Bach", "authors": "Francis Bach (SIERRA)", "title": "Efficient Algorithms for Non-convex Isotonic Regression through\n  Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of submodular functions subject to ordering\nconstraints. We show that this optimization problem can be cast as a convex\noptimization problem on a space of uni-dimensional measures, with ordering\nconstraints corresponding to first-order stochastic dominance. We propose new\ndiscretization schemes that lead to simple and efficient algorithms based on\nzero-th, first, or higher order oracles; these algorithms also lead to\nimprovements without isotonic constraints. Finally, our experiments show that\nnon-convex loss functions can be much more robust to outliers for isotonic\nregression, while still leading to an efficient optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 09:02:52 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Bach", "Francis", "", "SIERRA"]]}, {"id": "1707.09161", "submitter": "Ramji Venkataramanan", "authors": "Pavan Srinath and Ramji Venkataramanan", "title": "Empirical Bayes Estimators for High-Dimensional Sparse Vectors", "comments": "35 pages, to appear in Information and Inference: A Journal of the\n  IMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a high-dimensional sparse vector\n$\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ from an observation in i.i.d. Gaussian\nnoise is considered. The performance is measured using squared-error loss. An\nempirical Bayes shrinkage estimator, derived using a Bernoulli-Gaussian prior,\nis analyzed and compared with the well-known soft-thresholding estimator. We\nobtain concentration inequalities for the Stein's unbiased risk estimate and\nthe loss function of both estimators. The results show that for large $n$, both\nthe risk estimate and the loss function concentrate on deterministic values\nclose to the true risk.\n  Depending on the underlying $\\boldsymbol{\\theta}$, either the proposed\nempirical Bayes (eBayes) estimator or soft-thresholding may have smaller loss.\nWe consider a hybrid estimator that attempts to pick the better of the\nsoft-thresholding estimator and the eBayes estimator by comparing their risk\nestimates. It is shown that: i) the loss of the hybrid estimator concentrates\non the minimum of the losses of the two competing estimators, and ii) the risk\nof the hybrid estimator is within order $\\frac{1}{\\sqrt{n}}$ of the minimum of\nthe two risks. Simulation results are provided to support the theoretical\nresults. Finally, we use the eBayes and hybrid estimators as denoisers in the\napproximate message passing (AMP) algorithm for compressed sensing, and show\nthat their performance is superior to the soft-thresholding denoiser in a wide\nrange of settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 09:17:12 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 07:54:54 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 14:30:05 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Srinath", "Pavan", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1707.09219", "submitter": "Isabeau Pr\\'emont-Schwarz", "authors": "Isabeau Pr\\'emont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti\n  Rasmus, Rinu Boney, Harri Valpola", "title": "Recurrent Ladder Networks", "comments": "9 pages, 9 figures, 7-page appendix, fixed fig 9 (c)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent extension of the Ladder networks whose structure is\nmotivated by the inference required in hierarchical latent variable models. We\ndemonstrate that the recurrent Ladder is able to handle a wide variety of\ncomplex learning tasks that benefit from iterative inference and temporal\nmodeling. The architecture shows close-to-optimal results on temporal modeling\nof video data, competitive results on music modeling, and improved perceptual\ngrouping based on higher order abstractions, such as stochastic textures and\nmotion cues. We present results for fully supervised, semi-supervised, and\nunsupervised tasks. The results suggest that the proposed architecture and\nprinciples are powerful tools for learning a hierarchy of abstractions,\nlearning iterative inference and handling temporal information.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 13:19:11 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 15:14:19 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 13:43:12 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 06:43:47 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Pr\u00e9mont-Schwarz", "Isabeau", ""], ["Ilin", "Alexander", ""], ["Hao", "Tele Hotloo", ""], ["Rasmus", "Antti", ""], ["Boney", "Rinu", ""], ["Valpola", "Harri", ""]]}, {"id": "1707.09241", "submitter": "Yannic Kilcher", "authors": "Yannic Kilcher, Aur\\'elien Lucchi, Thomas Hofmann", "title": "Generator Reversal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training generative models with deep neural\nnetworks as generators, i.e. to map latent codes to data points. Whereas the\ndominant paradigm combines simple priors over codes with complex deterministic\nmodels, we propose instead to use more flexible code distributions. These\ndistributions are estimated non-parametrically by reversing the generator map\nduring training. The benefits include: more powerful generative models, better\nmodeling of latent structure and explicit control of the degree of\ngeneralization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 14:07:56 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Kilcher", "Yannic", ""], ["Lucchi", "Aur\u00e9lien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1707.09285", "submitter": "Zachary Boyd", "authors": "Zachary Boyd, Egil Bae, Xue-Cheng Tai, and Andrea L. Bertozzi", "title": "Simplified Energy Landscape for Modularity Using Total Variation", "comments": "25 pages, 3 figures, 3 tables, submitted to SIAM J. App. Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks capture pairwise interactions between entities and are frequently\nused in applications such as social networks, food networks, and protein\ninteraction networks, to name a few. Communities, cohesive groups of nodes,\noften form in these applications, and identifying them gives insight into the\noverall organization of the network. One common quality function used to\nidentify community structure is modularity. In Hu et al. [SIAM J. App. Math.,\n73(6), 2013], it was shown that modularity optimization is equivalent to\nminimizing a particular nonconvex total variation (TV) based functional over a\ndiscrete domain. They solve this problem, assuming the number of communities is\nknown, using a Merriman, Bence, Osher (MBO) scheme.\n  We show that modularity optimization is equivalent to minimizing a convex\nTV-based functional over a discrete domain, again, assuming the number of\ncommunities is known. Furthermore, we show that modularity has no convex\nrelaxation satisfying certain natural conditions. We therefore, find a\nmanageable non-convex approximation using a Ginzburg Landau functional, which\nprovably converges to the correct energy in the limit of a certain parameter.\nWe then derive an MBO algorithm with fewer hand-tuned parameters than in Hu et\nal. and which is 7 times faster at solving the associated diffusion equation\ndue to the fact that the underlying discretization is unconditionally stable.\nOur numerical tests include a hyperspectral video whose associated graph has\n2.9x10^7 edges, which is roughly 37 times larger than was handled in the paper\nof Hu et al.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 15:39:03 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 19:28:26 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 20:32:47 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Boyd", "Zachary", ""], ["Bae", "Egil", ""], ["Tai", "Xue-Cheng", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1707.09350", "submitter": "Michael Schaub", "authors": "Marco Avella-Medina and Francesca Parise and Michael T. Schaub and\n  Santiago Segarra", "title": "Centrality measures for graphons: Accounting for uncertainty in networks", "comments": "Authors ordered alphabetically, all authors contributed equally. 21\n  pages, 7 figures", "journal-ref": "IEEE Transactions on Network Science and Engineering, 2020, vol.\n  7, no. 1, pp. 520-537", "doi": "10.1109/TNSE.2018.2884235", "report-no": null, "categories": "cs.SI cs.SY math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As relational datasets modeled as graphs keep increasing in size and their\ndata-acquisition is permeated by uncertainty, graph-based analysis techniques\ncan become computationally and conceptually challenging. In particular, node\ncentrality measures rely on the assumption that the graph is perfectly known --\na premise not necessarily fulfilled for large, uncertain networks. Accordingly,\ncentrality measures may fail to faithfully extract the importance of nodes in\nthe presence of uncertainty. To mitigate these problems, we suggest a\nstatistical approach based on graphon theory: we introduce formal definitions\nof centrality measures for graphons and establish their connections to\nclassical graph centrality measures. A key advantage of this approach is that\ncentrality measures defined at the modeling level of graphons are inherently\nrobust to stochastic variations of specific graph realizations. Using the\ntheory of linear integral operators, we define degree, eigenvector, Katz and\nPageRank centrality functions for graphons and establish concentration\ninequalities demonstrating that graphon centrality functions arise naturally as\nlimits of their counterparts defined on sequences of graphs of increasing size.\nThe same concentration inequalities also provide high-probability bounds\nbetween the graphon centrality functions and the centrality measures on any\nsampled graph, thereby establishing a measure of uncertainty of the measured\ncentrality score. The same concentration inequalities also provide\nhigh-probability bounds between the graphon centrality functions and the\ncentrality measures on any sampled graph, thereby establishing a measure of\nuncertainty of the measured centrality score.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 17:47:00 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 14:58:27 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 18:55:14 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 23:34:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Avella-Medina", "Marco", ""], ["Parise", "Francesca", ""], ["Schaub", "Michael T.", ""], ["Segarra", "Santiago", ""]]}, {"id": "1707.09430", "submitter": "Christian Albert Hammerschmidt", "authors": "Christian A. Hammerschmidt, Radu State, Sicco Verwer", "title": "Human in the Loop: Interactive Passive Automata Learning via\n  Evidence-Driven State-Merging Algorithms", "comments": "4 pages, presented at the Human in the Loop workshop at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive version of an evidence-driven state-merging (EDSM)\nalgorithm for learning variants of finite state automata. Learning these\nautomata often amounts to recovering or reverse engineering the model\ngenerating the data despite noisy, incomplete, or imperfectly sampled data\nsources rather than optimizing a purely numeric target function. Domain\nexpertise and human knowledge about the target domain can guide this process,\nand typically is captured in parameter settings. Often, domain expertise is\nsubconscious and not expressed explicitly. Directly interacting with the\nlearning algorithm makes it easier to utilize this knowledge effectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 22:19:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hammerschmidt", "Christian A.", ""], ["State", "Radu", ""], ["Verwer", "Sicco", ""]]}, {"id": "1707.09457", "submitter": "Kai-Wei Chang", "authors": "Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and\n  Kai-Wei Chang", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints", "comments": "11 pages, published in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 03:38:32 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zhao", "Jieyu", ""], ["Wang", "Tianlu", ""], ["Yatskar", "Mark", ""], ["Ordonez", "Vicente", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1707.09520", "submitter": "Kyle Helfrich", "authors": "Kyle Helfrich, Devin Willmott, Qiang Ye", "title": "Orthogonal Recurrent Neural Networks with Scaled Cayley Transform", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are designed to handle sequential data but\nsuffer from vanishing or exploding gradients. Recent work on Unitary Recurrent\nNeural Networks (uRNNs) have been used to address this issue and in some cases,\nexceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose\na simpler and novel update scheme to maintain orthogonal recurrent weight\nmatrices without using complex valued matrices. This is done by parametrizing\nwith a skew-symmetric matrix using the Cayley transform. Such a parametrization\nis unable to represent matrices with negative one eigenvalues, but this\nlimitation is overcome by scaling the recurrent weight matrix by a diagonal\nmatrix consisting of ones and negative ones. The proposed training scheme\ninvolves a straightforward gradient calculation and update step. In several\nexperiments, the proposed scaled Cayley orthogonal recurrent neural network\n(scoRNN) achieves superior results with fewer trainable parameters than other\nunitary RNNs.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 14:37:48 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 21:30:51 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 14:51:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Helfrich", "Kyle", ""], ["Willmott", "Devin", ""], ["Ye", "Qiang", ""]]}, {"id": "1707.09548", "submitter": "Guillaume Revillon", "authors": "Guillaume Revillon, Ali Mohammad-Djafari and Cyrille Enderli", "title": "A generalized multivariate Student-t mixture model for Bayesian\n  classification and clustering of radar waveforms", "comments": "Submitted on 06/30/17 to the Journal of Selected Topics in Signal\n  Processing Special Issue on Machine Learning for Cognition in Radio\n  Communications and Radar (ML-CR2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a generalized multivariate Student-t mixture model is\ndeveloped for classification and clustering of Low Probability of Intercept\nradar waveforms. A Low Probability of Intercept radar signal is characterized\nby a pulse compression waveform which is either frequency-modulated or\nphase-modulated. The proposed model can classify and cluster different\nmodulation types such as linear frequency modulation, non linear frequency\nmodulation, polyphase Barker, polyphase P1, P2, P3, P4, Frank and Zadoff codes.\nThe classification method focuses on the introduction of a new prior\ndistribution for the model hyper-parameters that gives us the possibility to\nhandle sensitivity of mixture models to initialization and to allow a less\nrestrictive modeling of data. Inference is processed through a Variational\nBayes method and a Bayesian treatment is adopted for model learning, supervised\nclassification and clustering. Moreover, the novel prior distribution is not a\nwell-known probability distribution and both deterministic and stochastic\nmethods are employed to estimate its expectations. Some numerical experiments\nshow that the proposed method is less sensitive to initialization and provides\nmore accurate results than the previous state of the art mixture models.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 18:12:52 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Revillon", "Guillaume", ""], ["Mohammad-Djafari", "Ali", ""], ["Enderli", "Cyrille", ""]]}, {"id": "1707.09561", "submitter": "Jelena Bradic", "authors": "Jue Hou, Jelena Bradic, Ronghui Xu", "title": "Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:53:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:03:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1707.09562", "submitter": "Hantian Zhang", "authors": "Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:59:18 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 16:36:55 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 11:13:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Liu", "Yu", ""], ["Zhang", "Hantian", ""], ["Zeng", "Luyuan", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "1707.09641", "submitter": "Benjamin Lengerich", "authors": "Benjamin J. Lengerich, Sandeep Konam, Eric P. Xing, Stephanie\n  Rosenthal, Manuela Veloso", "title": "Towards Visual Explanations for Convolutional Neural Networks via Input\n  Resampling", "comments": "Presented at ICML 2017 Workshop on Visualization for Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive power of neural networks often costs model interpretability.\nSeveral techniques have been developed for explaining model outputs in terms of\ninput features; however, it is difficult to translate such interpretations into\nactionable insight. Here, we propose a framework to analyze predictions in\nterms of the model's internal features by inspecting information flow through\nthe network. Given a trained network and a test image, we select neurons by two\nmetrics, both measured over a set of images created by perturbations to the\ninput image: (1) magnitude of the correlation between the neuron activation and\nthe network output and (2) precision of the neuron activation. We show that the\nformer metric selects neurons that exert large influence over the network\noutput while the latter metric selects neurons that activate on generalizable\nfeatures. By comparing the sets of neurons selected by these two metrics, our\nframework suggests a way to investigate the internal attention mechanisms of\nconvolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 17:12:20 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 14:02:23 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Lengerich", "Benjamin J.", ""], ["Konam", "Sandeep", ""], ["Xing", "Eric P.", ""], ["Rosenthal", "Stephanie", ""], ["Veloso", "Manuela", ""]]}, {"id": "1707.09688", "submitter": "Satoshi Hara", "authors": "Satoshi Hara, Takayuki Katsuki, Hiroki Yanagisawa, Masaaki Imaizumi,\n  Takafumi Ono, Ryo Okamoto, Shigeki Takeuchi", "title": "Consistent Nonparametric Different-Feature Selection via the Sparsest\n  $k$-Subgraph Problem", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample feature selection is the problem of finding features that describe\na difference between two probability distributions, which is a ubiquitous\nproblem in both scientific and engineering studies. However, existing methods\nhave limited applicability because of their restrictive assumptions on data\ndistributoins or computational difficulty. In this paper, we resolve these\ndifficulties by formulating the problem as a sparsest $k$-subgraph problem. The\nproposed method is nonparametric and does not assume any specific parametric\nmodels on the data distributions. We show that the proposed method is\ncomputationally efficient and does not require any extra computation for model\nselection. Moreover, we prove that the proposed method provides a consistent\nestimator of features under mild conditions. Our experimental results show that\nthe proposed method outperforms the current method with regard to both accuracy\nand computation time.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 00:53:59 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 01:03:57 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Hara", "Satoshi", ""], ["Katsuki", "Takayuki", ""], ["Yanagisawa", "Hiroki", ""], ["Imaizumi", "Masaaki", ""], ["Ono", "Takafumi", ""], ["Okamoto", "Ryo", ""], ["Takeuchi", "Shigeki", ""]]}, {"id": "1707.09724", "submitter": "Dacheng Tao", "authors": "Xiyu Yu, Tongliang Liu, Mingming Gong, Kun Zhang, Kayhan\n  Batmanghelich, and Dacheng Tao", "title": "Transfer Learning with Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning aims to improve learning in target domain by borrowing\nknowledge from a related but different source domain. To reduce the\ndistribution shift between source and target domains, recent methods have\nfocused on exploring invariant representations that have similar distributions\nacross domains. However, when learning this invariant knowledge, existing\nmethods assume that the labels in source domain are uncontaminated, while in\nreality, we often have access to source data with noisy labels. In this paper,\nwe first show how label noise adversely affect the learning of invariant\nrepresentations and the correcting of label shift in various transfer learning\nscenarios. To reduce the adverse effects, we propose a novel Denoising\nConditional Invariant Component (DCIC) framework, which provably ensures (1)\nextracting invariant representations given examples with noisy labels in source\ndomain and unlabeled examples in target domain; (2) estimating the label\ndistribution in target domain with no bias. Experimental results on both\nsynthetic and real-world data verify the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 05:26:41 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 01:56:30 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Yu", "Xiyu", ""], ["Liu", "Tongliang", ""], ["Gong", "Mingming", ""], ["Zhang", "Kun", ""], ["Batmanghelich", "Kayhan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1707.09727", "submitter": "Vishnu Raj", "authors": "Vishnu Raj and Sheetal Kalyani", "title": "Taming Non-stationary Bandits: A Bayesian Approach", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi armed bandit problem in non-stationary environments.\nBased on the Bayesian method, we propose a variant of Thompson Sampling which\ncan be used in both rested and restless bandit scenarios. Applying discounting\nto the parameters of prior distribution, we describe a way to systematically\nreduce the effect of past observations. Further, we derive the exact expression\nfor the probability of picking sub-optimal arms. By increasing the exploitative\nvalue of Bayes' samples, we also provide an optimistic version of the\nalgorithm. Extensive empirical analysis is conducted under various scenarios to\nvalidate the utility of proposed algorithms. A comparison study with various\nstate-of-the-arm algorithms is also included.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 05:46:39 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Raj", "Vishnu", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1707.09752", "submitter": "Peter Rousseeuw", "authors": "Peter J. Rousseeuw and Mia Hubert", "title": "Anomaly Detection by Robust Statistics", "comments": "To appear in WIREs Data Mining and Knowledge Discovery", "journal-ref": "WIREs Data Mining and Knowledge Discovery, 2018, widm.1236", "doi": "10.1002/widm.1236", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real data often contain anomalous cases, also known as outliers. These may\nspoil the resulting analysis but they may also contain valuable information. In\neither case, the ability to detect such anomalies is essential. A useful tool\nfor this purpose is robust statistics, which aims to detect the outliers by\nfirst fitting the majority of the data and then flagging data points that\ndeviate from it. We present an overview of several robust methods and the\nresulting graphical outlier detection tools. We discuss robust procedures for\nunivariate, low-dimensional, and high-dimensional data, such as estimating\nlocation and scatter, linear regression, principal component analysis,\nclassification, clustering, and functional data analysis. Also the challenging\nnew topic of cellwise outliers is introduced.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 08:12:16 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 09:44:43 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "1707.09770", "submitter": "Elie Amani", "authors": "Elie Amani (LISSI, F'SATI), Karim Djouani (LISSI, F'SATI), Anish\n  Kurien (F'SATI), Jean-R\\'emi De Boer, Willy Vigneau, Lionel Ries (CNES)", "title": "GPS Multipath Detection in the Frequency Domain", "comments": "2016 European Navigation Conference (ENC 2016), May 2016, Helsinki,\n  Finland. Proceedings of the 2016 European Navigation Conference (ENC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipath is among the major sources of errors in precise positioning using\nGPS and continues to be extensively studied. Two Fast Fourier Transform\n(FFT)-based detectors are presented in this paper as GPS multipath detection\ntechniques. The detectors are formulated as binary hypothesis tests under the\nassumption that the multipath exists for a sufficient time frame that allows\nits detection based on the quadrature arm of the coherent Early-minus-Late\ndiscriminator (Q EmL) for a scalar tracking loop (STL) or on the quadrature (Q\nEmL) and/or in-phase arm (I EmL) for a vector tracking loop (VTL), using an\nobservation window of N samples. Performance analysis of the suggested\ndetectors is done on multipath signal data acquired from the multipath\nenvironment simulator developed by the German Aerospace Centre (DLR) as well as\non multipath data from real GPS signals. Application of the detection tests to\ncorrelator outputs of scalar and vector tracking loops shows that they may be\nused to exclude multipath contaminated satellites from the navigation solution.\nThese detection techniques can be extended to other Global Navigation Satellite\nSystems (GNSS) such as GLONASS, Galileo and Beidou.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 08:58:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Amani", "Elie", "", "LISSI, F'SATI"], ["Djouani", "Karim", "", "LISSI, F'SATI"], ["Kurien", "Anish", "", "F'SATI"], ["De Boer", "Jean-R\u00e9mi", "", "CNES"], ["Vigneau", "Willy", "", "CNES"], ["Ries", "Lionel", "", "CNES"]]}, {"id": "1707.09837", "submitter": "Artur Lugmayr", "authors": "Irina Kuznetsova, Yuliya V Karpievitch, Aleksandra Filipovska, Artur\n  Lugmayr, Andreas Holzinger", "title": "Review of Machine Learning Algorithms in Differential Expression\n  Analysis", "comments": null, "journal-ref": "Proc. of the 9th Workshop on Semantic Ambient Media Experiences\n  (SAME'2016/2), Visualisation - Emerging Media - and User-Experience, Int.\n  Series on Information Systems and Management in Creative eMedia (CreMedia),\n  No. 2016/2, 2016", "doi": null, "report-no": "CreMedia/2016/02/01/02", "categories": "stat.ML cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biological research machine learning algorithms are part of nearly every\nanalytical process. They are used to identify new insights into biological\nphenomena, interpret data, provide molecular diagnosis for diseases and develop\npersonalized medicine that will enable future treatments of diseases. In this\npaper we (1) illustrate the importance of machine learning in the analysis of\nlarge scale sequencing data, (2) present an illustrative standardized workflow\nof the analysis process, (3) perform a Differential Expression (DE) analysis of\na publicly available RNA sequencing (RNASeq) data set to demonstrate the\ncapabilities of various algorithms at each step of the workflow, and (4) show a\nmachine learning solution in improving the computing time, storage\nrequirements, and minimize utilization of computer memory in analyses of\nRNA-Seq datasets. The source code of the analysis pipeline and associated\nscripts are presented in the paper appendix to allow replication of\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 14:56:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kuznetsova", "Irina", ""], ["Karpievitch", "Yuliya V", ""], ["Filipovska", "Aleksandra", ""], ["Lugmayr", "Artur", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1707.09861", "submitter": "Nils Reimers", "authors": "Nils Reimers and Iryna Gurevych", "title": "Reporting Score Distributions Makes a Difference: Performance Study of\n  LSTM-networks for Sequence Tagging", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we show that reporting a single performance score is\ninsufficient to compare non-deterministic approaches. We demonstrate for common\nsequence tagging tasks that the seed value for the random number generator can\nresult in statistically significant (p < 10^-4) differences for\nstate-of-the-art systems. For two recent systems for NER, we observe an\nabsolute difference of one percentage point F1-score depending on the selected\nseed value, making these systems perceived either as state-of-the-art or\nmediocre. Instead of publishing and reporting single performance scores, we\npropose to compare score distributions based on multiple executions. Based on\nthe evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we\npresent network architectures that produce both superior performance as well as\nare more stable with respect to the remaining hyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:25:24 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1707.09938", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Jaejun Yoo, and Jong Chul Ye", "title": "Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet\n  Residual Network", "comments": "This will appear in IEEE Transaction on Medical Imaging, a special\n  issue of Machine Learning for Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally expensive. To address this problem, we recently proposed a\ndeep convolutional neural network (CNN) for low-dose X-ray CT and won the\nsecond place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the\ntexture were not fully recovered. To address this problem, here we propose a\nnovel framelet-based denoising algorithm using wavelet residual network which\nsynergistically combines the expressive power of deep learning and the\nperformance guarantee from the framelet-based denoising algorithms. The new\nalgorithms were inspired by the recent interpretation of the deep convolutional\nneural network (CNN) as a cascaded convolution framelet signal representation.\nExtensive experimental results confirm that the proposed networks have\nsignificantly improved performance and preserves the detail texture of the\noriginal images.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:17:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:10:04 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:46:15 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kang", "Eunhee", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1707.09971", "submitter": "Cong Ma", "authors": "Yuxin Chen, Jianqing Fan, Cong Ma, Kaizheng Wang", "title": "Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking", "comments": "Add discussions on the setting of the general condition number", "journal-ref": "Annals of Statististics, Volume 47, Number 4 (2019), 2204-2235", "doi": "10.1214/18-AOS1745", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of top-$K$ ranking from pairwise\ncomparisons. Given a collection of $n$ items and a few pairwise comparisons\nacross them, one wishes to identify the set of $K$ items that receive the\nhighest ranks. To tackle this problem, we adopt the logistic parametric model\n--- the Bradley-Terry-Luce model, where each item is assigned a latent\npreference score, and where the outcome of each pairwise comparison depends\nsolely on the relative scores of the two items involved. Recent works have made\nsignificant progress towards characterizing the performance (e.g. the mean\nsquare error for estimating the scores) of several classical methods, including\nthe spectral method and the maximum likelihood estimator (MLE). However, where\nthey stand regarding top-$K$ ranking remains unsettled.\n  We demonstrate that under a natural random sampling model, the spectral\nmethod alone, or the regularized MLE alone, is minimax optimal in terms of the\nsample complexity --- the number of paired comparisons needed to ensure exact\ntop-$K$ identification, for the fixed dynamic range regime. This is\naccomplished via optimal control of the entrywise error of the score estimates.\nWe complement our theoretical studies by numerical experiments, confirming that\nboth methods yield low entrywise errors for estimating the underlying scores.\nOur theory is established via a novel leave-one-out trick, which proves\neffective for analyzing both iterative and non-iterative procedures. Along the\nway, we derive an elementary eigenvector perturbation bound for probability\ntransition matrices, which parallels the Davis-Kahan $\\sin\\Theta$ theorem for\nsymmetric matrices. This also allows us to close the gap between the $\\ell_2$\nerror upper bound for the spectral method and the minimax lower limit.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:33:15 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 03:20:10 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:35:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Wang", "Kaizheng", ""]]}]