[{"id": "1001.0036", "submitter": "Cosma Rohilla Shalizi", "authors": "Robert Haslinger, Kristina Lisa Klinkner, and Cosma Rohilla Shalizi", "title": "The Computational Structure of Spike Trains", "comments": "Somewhat different format from journal version but same content", "journal-ref": "Neural Computation, vol. 22 (2010), pp. 121--157", "doi": "10.1162/neco.2009.12-07-678", "report-no": null, "categories": "q-bio.NC cs.IT math.IT nlin.AO physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons perform computations, and convey the results of those computations\nthrough the statistical structure of their output spike trains. Here we present\na practical method, grounded in the information-theoretic analysis of\nprediction, for inferring a minimal representation of that structure and for\ncharacterizing its complexity. Starting from spike trains, our approach finds\ntheir causal state models (CSMs), the minimal hidden Markov models or\nstochastic automata capable of generating statistically identical time series.\nWe then use these CSMs to objectively quantify both the generalizable structure\nand the idiosyncratic randomness of the spike train. Specifically, we show that\nthe expected algorithmic information content (the information needed to\ndescribe the spike train exactly) can be split into three parts describing (1)\nthe time-invariant structure (complexity) of the minimal spike-generating\nprocess, which describes the spike train statistically; (2) the randomness\n(internal entropy rate) of the minimal spike-generating process; and (3) a\nresidual pure noise term not described by the minimal spike-generating process.\nWe use CSMs to approximate each of these quantities. The CSMs are inferred\nnonparametrically from the data, making only mild regularity assumptions, via\nthe causal state splitting reconstruction algorithm. The methods presented here\ncomplement more traditional spike train analyses by describing not only spiking\nprobability and spike train entropy, but also the complexity of a spike train's\nstructure. We demonstrate our approach using both simulated spike trains and\nexperimental data recorded in rat barrel cortex during vibrissa stimulation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 22:12:20 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Haslinger", "Robert", ""], ["Klinkner", "Kristina Lisa", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1001.0160", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, Hanna M. Wallach, Zoubin Ghahramani", "title": "Learning the Structure of Deep Sparse Graphical Models", "comments": "20 pages, 6 figures, AISTATS 2010, Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep belief networks are a powerful way to model complex probability\ndistributions. However, learning the structure of a belief network,\nparticularly one with hidden units, is difficult. The Indian buffet process has\nbeen used as a nonparametric Bayesian prior on the directed structure of a\nbelief network with a single infinitely wide hidden layer. In this paper, we\nintroduce the cascading Indian buffet process (CIBP), which provides a\nnonparametric prior on the structure of a layered, directed belief network that\nis unbounded in both depth and width, yet allows tractable inference. We use\nthe CIBP prior with the nonlinear Gaussian belief network so each unit can\nadditionally vary its behavior between discrete and continuous representations.\nWe provide Markov chain Monte Carlo algorithms for inference in these belief\nnetworks and explore the structures learned on several image data sets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2009 19:47:34 GMT"}, {"version": "v2", "created": "Thu, 19 Aug 2010 19:56:20 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Wallach", "Hanna M.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1001.0175", "submitter": "Iain Murray", "authors": "Iain Murray, Ryan Prescott Adams, David J.C. MacKay", "title": "Elliptical slice sampling", "comments": "8 pages, 6 figures, appearing in AISTATS 2010 (JMLR: W&CP volume 6).\n  Differences from first submission: some minor edits in response to feedback.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many probabilistic models introduce strong dependencies between variables\nusing a latent multivariate Gaussian distribution or a Gaussian process. We\npresent a new Markov chain Monte Carlo algorithm for performing inference in\nmodels with multivariate Gaussian priors. Its key properties are: 1) it has\nsimple, generic code applicable to many models, 2) it has no free parameters,\n3) it works well for a variety of Gaussian process based models. These\nproperties make our method ideal for use while model building, removing the\nneed to spend time deriving and tuning updates for more complex algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2009 19:41:31 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2010 13:05:31 GMT"}], "update_date": "2010-03-22", "authors_parsed": [["Murray", "Iain", ""], ["Adams", "Ryan Prescott", ""], ["MacKay", "David J. C.", ""]]}, {"id": "1001.0279", "submitter": "Raghunandan Hulikal Keshavan", "authors": "Raghunandan H. Keshavan, Andrea Montanari", "title": "Regularization for Matrix Completion", "comments": "5 pages, 3 figures, Conference Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a low rank matrix from noisy\nobservations of a subset of its entries. This task has applications in\nstatistical learning, computer vision, and signal processing. In these\ncontexts, \"noise\" generically refers to any contribution to the data that is\nnot captured by the low-rank model. In most applications, the noise level is\nlarge compared to the underlying signal and it is important to avoid\noverfitting. In order to tackle this problem, we define a regularized cost\nfunction well suited for spectral reconstruction methods. Within a random noise\nmodel, and in the large system limit, we prove that the resulting accuracy\nundergoes a phase transition depending on the noise level and on the fraction\nof observed entries. The cost function can be minimized using OPTSPACE (a\nmanifold gradient descent algorithm). Numerical simulations show that this\napproach is competitive with state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2010 04:14:50 GMT"}], "update_date": "2010-01-05", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""]]}, {"id": "1001.0597", "submitter": "XuanLong Nguyen", "authors": "XuanLong Nguyen", "title": "Inference of global clusters from locally distributed data", "comments": "27 pages, 12 figures", "journal-ref": "Published in Bayesian Analysis, 5(4), 817--846, 2010", "doi": null, "report-no": "Technical report 504, Department of Statistics, University of\n  Michigan", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of analyzing the heterogeneity of clustering\ndistributions for multiple groups of observed data, each of which is indexed by\na covariate value, and inferring global clusters arising from observations\naggregated over the covariate domain. We propose a novel Bayesian nonparametric\nmethod reposing on the formalism of spatial modeling and a nested hierarchy of\nDirichlet processes. We provide an analysis of the model properties, relating\nand contrasting the notions of local and global clusters. We also provide an\nefficient inference algorithm, and demonstrate the utility of our method in\nseveral data examples, including the problem of object tracking and a global\nclustering analysis of functional data where the functional identity\ninformation is not available.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 22:47:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jan 2011 15:42:15 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Nguyen", "XuanLong", ""]]}, {"id": "1001.1323", "submitter": "Gilad Lerman Dr", "authors": "Ery Arias-Castro, Guangliang Chen and Gilad Lerman", "title": "Spectral clustering based on local linear approximations", "comments": null, "journal-ref": "Electronic Journal of Statistics, Vol. 5 (2011), pages 1537-1587", "doi": "10.1214/11-EJS651", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of clustering, we assume a generative model where each cluster\nis the result of sampling points in the neighborhood of an embedded smooth\nsurface; the sample may be contaminated with outliers, which are modeled as\npoints sampled in space away from the clusters. We consider a prototype for a\nhigher-order spectral clustering method based on the residual from a local\nlinear approximation. We obtain theoretical guarantees for this algorithm and\nshow that, in terms of both separation and robustness to outliers, it\noutperforms the standard spectral clustering algorithm (based on pairwise\ndistances) of Ng, Jordan and Weiss (NIPS '01). The optimal choice for some of\nthe tuning parameters depends on the dimension and thickness of the clusters.\nWe provide estimators that come close enough for our theoretical purposes. We\nalso discuss the cases of clusters of mixed dimensions and of clusters that are\ngenerated from smoother surfaces. In our experiments, this algorithm is shown\nto outperform pairwise spectral clustering on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2010 17:15:54 GMT"}, {"version": "v2", "created": "Fri, 10 Sep 2010 14:54:06 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2011 04:26:11 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Chen", "Guangliang", ""], ["Lerman", "Gilad", ""]]}, {"id": "1001.1557", "submitter": "John Lafferty", "authors": "Han Liu, Min Xu, Haijie Gu, Anupam Gupta, John Lafferty, Larry\n  Wasserman", "title": "Forest Density Estimation", "comments": "Extended version of earlier paper titled \"Tree density estimation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph estimation and density estimation in high dimensions, using a\nfamily of density estimators based on forest structured undirected graphical\nmodels. For density estimation, we do not assume the true distribution\ncorresponds to a forest; rather, we form kernel density estimates of the\nbivariate and univariate marginals, and apply Kruskal's algorithm to estimate\nthe optimal forest on held out data. We prove an oracle inequality on the\nexcess risk of the resulting estimator relative to the risk of the best forest.\nFor graph estimation, we consider the problem of estimating forests with\nrestricted tree sizes. We prove that finding a maximum weight spanning forest\nwith restricted tree size is NP-hard, and develop an approximation algorithm\nfor this problem. Viewing the tree size as a complexity parameter, we then\nselect a forest using data splitting, and prove bounds on excess risk and\nstructure selection consistency of the procedure. Experiments with simulated\ndata and microarray data indicate that the methods are a practical alternative\nto Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2010 20:39:25 GMT"}, {"version": "v2", "created": "Wed, 20 Oct 2010 16:00:03 GMT"}], "update_date": "2010-10-21", "authors_parsed": [["Liu", "Han", ""], ["Xu", "Min", ""], ["Gu", "Haijie", ""], ["Gupta", "Anupam", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "1001.1841", "submitter": "Ansgar Steland", "authors": "Ansgar Steland, Ewaryst Rafalowicz", "title": "A Binary Control Chart to Detect Small Jumps", "comments": null, "journal-ref": "Statistics 2009, 43 (3), 295-311", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic N p chart gives a signal if the number of successes in a sequence\nof inde- pendent binary variables exceeds a control limit. Motivated by\nengineering applications in industrial image processing and, to some extent,\nfinancial statistics, we study a simple modification of this chart, which uses\nonly the most recent observations. Our aim is to construct a control chart for\ndetecting a shift of an unknown size, allowing for an unknown distribution of\nthe error terms. Simulation studies indicate that the proposed chart is su-\nperior in terms of out-of-control average run length, when one is interest in\nthe detection of very small shifts. We provide a (functional) central limit\ntheorem under a change-point model with local alternatives which explains that\nunexpected and interesting behavior. Since real observations are often not\nindependent, the question arises whether these re- sults still hold true for\nthe dependent case. Indeed, our asymptotic results work under the fairly\ngeneral condition that the observations form a martingale difference array.\nThis enlarges the applicability of our results considerably, firstly, to a\nlarge class time series models, and, secondly, to locally dependent image data,\nas we demonstrate by an example.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2010 10:43:50 GMT"}], "update_date": "2010-01-13", "authors_parsed": [["Steland", "Ansgar", ""], ["Rafalowicz", "Ewaryst", ""]]}, {"id": "1001.2615", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki", "title": "Sparsity-accuracy trade-off in MKL", "comments": "8pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically investigate the best trade-off between sparse and\nuniformly-weighted multiple kernel learning (MKL) using the elastic-net\nregularization on real and simulated datasets. We find that the best trade-off\nparameter depends not only on the sparsity of the true kernel-weight spectrum\nbut also on the linear dependence among kernels and the number of samples.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 05:53:23 GMT"}], "update_date": "2010-01-18", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1001.2753", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "P.S. Koutsourelakis and Elias Bilionis", "title": "Scalable Bayesian reduced-order models for high-dimensional multiscale\n  dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing mathematical descriptions can accurately account for phenomena\nat microscopic scales (e.g. molecular dynamics), these are often\nhigh-dimensional, stochastic and their applicability over macroscopic time\nscales of physical interest is computationally infeasible or impractical. In\ncomplex systems, with limited physical insight on the coherent behavior of\ntheir constituents, the only available information is data obtained from\nsimulations of the trajectories of huge numbers of degrees of freedom over\nmicroscopic time scales. This paper discusses a Bayesian approach to deriving\nprobabilistic coarse-grained models that simultaneously address the problems of\nidentifying appropriate reduced coordinates and the effective dynamics in this\nlower-dimensional representation. At the core of the models proposed lie\nsimple, low-dimensional dynamical systems which serve as the building blocks of\nthe global model. These approximate the latent, generating sources and\nparameterize the reduced-order dynamics. We discuss parallelizable, online\ninference and learning algorithms that employ Sequential Monte Carlo samplers\nand scale linearly with the dimensionality of the observed dynamics. We propose\na Bayesian adaptive time-integration scheme that utilizes probabilistic\npredictive estimates and enables rigorous concurrent s imulation over\nmacroscopic time scales. The data-driven perspective advocated assimilates\ncomputational and experimental data and thus can materialize data-model fusion.\nIt can deal with applications that lack a mathematical description and where\nonly observational data is available. Furthermore, it makes non-intrusive use\nof existing computational models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 19:37:59 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2010 02:48:04 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Koutsourelakis", "P. S.", ""], ["Bilionis", "Elias", ""]]}, {"id": "1001.2813", "submitter": "Anthony Di Franco", "authors": "Anthony Di Franco", "title": "A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence\n  Prediction and Planning", "comments": "Submitted to MDPI Algorithms Special Issue \"Algorithmic Complexity in\n  Physics & Embedded Artificial Intelligences\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to address the question of whether we can in\nprinciple design rational decision-making agents or artificial intelligences\nembedded in computable physics such that their decisions are optimal in\nreasonable mathematical senses. Recent developments in rare event probability\nestimation, recursive bayesian inference, neural networks, and probabilistic\nplanning are sufficient to explicitly approximate reinforcement learners of the\nAIXI style with non-trivial model classes (here, the class of resource-bounded\nTuring machines). Consideration of the effects of resource limitations in a\nconcrete implementation leads to insights about possible architectures for\nlearning systems using optimal decision makers as components.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 01:10:17 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Di Franco", "Anthony", ""]]}, {"id": "1001.3109", "submitter": "Anne-Claire Haury", "authors": "Anne-Claire Haury (CBIO), Laurent Jacob (CBIO), Jean-Philippe Vert\n  (CBIO)", "title": "Increasing stability and interpretability of gene expression signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation : Molecular signatures for diagnosis or prognosis estimated from\nlarge-scale gene expression data often lack robustness and stability, rendering\ntheir biological interpretation challenging. Increasing the signature's\ninterpretability and stability across perturbations of a given dataset and, if\npossible, across datasets, is urgently needed to ease the discovery of\nimportant biological processes and, eventually, new drug targets. Results : We\npropose a new method to construct signatures with increased stability and\neasier interpretability. The method uses a gene network as side interpretation\nand enforces a large connectivity among the genes in the signature, leading to\nsignatures typically made of genes clustered in a few subnetworks. It combines\nthe recently proposed graph Lasso procedure with a stability selection\nprocedure. We evaluate its relevance for the estimation of a prognostic\nsignature in breast cancer, and highlight in particular the increase in\ninterpretability and stability of the signature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 19:41:43 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Haury", "Anne-Claire", "", "CBIO"], ["Jacob", "Laurent", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1001.3355", "submitter": "Charles Sutton", "authors": "Charles Sutton, Michael I. Jordan", "title": "Bayesian inference for queueing networks and modeling of internet\n  services", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS392 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 254-282", "doi": "10.1214/10-AOAS392", "report-no": "IMS-AOAS-AOAS392", "categories": "stat.ML cs.NI cs.PF stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet services, such as those at Google, Yahoo!, and Amazon, handle\nbillions of requests per day on clusters of thousands of computers. Because\nthese services operate under strict performance requirements, a statistical\nunderstanding of their performance is of great practical interest. Such\nservices are modeled by networks of queues, where each queue models one of the\ncomputers in the system. A key challenge is that the data are incomplete,\nbecause recording detailed information about every request to a heavily used\nsystem can require unacceptable overhead. In this paper we develop a Bayesian\nperspective on queueing models in which the arrival and departure times that\nare not observed are treated as latent variables. Underlying this viewpoint is\nthe observation that a queueing model defines a deterministic transformation\nbetween the data and a set of independent variables called the service times.\nWith this viewpoint in hand, we sample from the posterior distribution over\nmissing data and model parameters using Markov chain Monte Carlo. We evaluate\nour framework on data from a benchmark Web application. We also present a\nsimple technique for selection among nested queueing models. We are unaware of\nany previous work that considers inference in networks of queues in the\npresence of missing data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 16:27:13 GMT"}, {"version": "v2", "created": "Wed, 4 Aug 2010 15:34:17 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2011 06:03:52 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Sutton", "Charles", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1001.4019", "submitter": "Mu Zhu", "authors": "Xiao Tang and Mu Zhu", "title": "Classifying Network Data with Deep Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by a growing interest in analyzing network data, we study the\nproblem of node classification on graphs, focusing on approaches based on\nkernel machines. Conventionally, kernel machines are linear classifiers in the\nimplicit feature space. We argue that linear classification in the feature\nspace of kernels commonly used for graphs is often not enough to produce good\nresults. When this is the case, one naturally considers nonlinear classifiers\nin the feature space. We show that repeating this process produces something we\ncall \"deep kernel machines.\" We provide some examples where deep kernel\nmachines can make a big difference in classification performance, and point out\nsome connections to various recent literature on deep architectures in\nartificial intelligence and machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2010 15:20:11 GMT"}], "update_date": "2010-01-25", "authors_parsed": [["Tang", "Xiao", ""], ["Zhu", "Mu", ""]]}, {"id": "1001.5311", "submitter": "Jarvis Haupt", "authors": "Jarvis Haupt, Rui Castro, and Robert Nowak", "title": "Distilled Sensing: Adaptive Sampling for Sparse Detection and Estimation", "comments": "23 pages, 2 figures. Revision includes minor clarifications, along\n  with more illustrative experimental results (cf. Figure 2)", "journal-ref": null, "doi": null, "report-no": "Rice University ECE Technical Report TREE1001", "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sampling results in dramatic improvements in the recovery of sparse\nsignals in white Gaussian noise. A sequential adaptive sampling-and-refinement\nprocedure called Distilled Sensing (DS) is proposed and analyzed. DS is a form\nof multi-stage experimental design and testing. Because of the adaptive nature\nof the data collection, DS can detect and localize far weaker signals than\npossible from non-adaptive measurements. In particular, reliable detection and\nlocalization (support estimation) using non-adaptive samples is possible only\nif the signal amplitudes grow logarithmically with the problem dimension. Here\nit is shown that using adaptive sampling, reliable detection is possible\nprovided the amplitude exceeds a constant, and localization is possible when\nthe amplitude exceeds any arbitrarily slowly growing function of the dimension.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2010 20:09:54 GMT"}, {"version": "v2", "created": "Thu, 27 May 2010 21:41:10 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Haupt", "Jarvis", ""], ["Castro", "Rui", ""], ["Nowak", "Robert", ""]]}]