[{"id": "1610.00040", "submitter": "Hao-Jun Shi", "authors": "Hao-Jun Michael Shi, Shenyinying Tu, Yangyang Xu, Wotao Yin", "title": "A Primer on Coordinate Descent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA CAM Report 16-67", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph presents a class of algorithms called coordinate descent\nalgorithms for mathematicians, statisticians, and engineers outside the field\nof optimization. This particular class of algorithms has recently gained\npopularity due to their effectiveness in solving large-scale optimization\nproblems in machine learning, compressed sensing, image processing, and\ncomputational statistics. Coordinate descent algorithms solve optimization\nproblems by successively minimizing along each coordinate or coordinate\nhyperplane, which is ideal for parallelized and distributed computing. Avoiding\ndetailed technicalities and proofs, this monograph gives relevant theory and\nexamples for practitioners to effectively apply coordinate descent to modern\nproblems in data science and engineering.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 21:55:55 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 20:38:20 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Shi", "Hao-Jun Michael", ""], ["Tu", "Shenyinying", ""], ["Xu", "Yangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1610.00064", "submitter": "Christopher Morris", "authors": "Christopher Morris, Nils M. Kriege, Kristian Kersting, Petra Mutzel", "title": "Faster Kernels for Graphs with Continuous Attributes via Hashing", "comments": "IEEE ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art kernels for graphs with discrete labels scale well to\ngraphs with thousands of nodes, the few existing kernels for graphs with\ncontinuous attributes, unfortunately, do not scale well. To overcome this\nlimitation, we present hash graph kernels, a general framework to derive\nkernels for graphs with continuous attributes from discrete ones. The idea is\nto iteratively turn continuous attributes into discrete labels using randomized\nhash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman\nsubtree kernel and for the shortest-path kernel. The resulting novel graph\nkernels are shown to be, both, able to handle graphs with continuous attributes\nand scalable to large graphs and data sets. This is supported by our\ntheoretical analysis and demonstrated by an extensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 00:43:19 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Morris", "Christopher", ""], ["Kriege", "Nils M.", ""], ["Kersting", "Kristian", ""], ["Mutzel", "Petra", ""]]}, {"id": "1610.00163", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Duo Wang, Nicholas D. Lane and Pietro Li\\`o", "title": "X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets", "comments": "To appear in the 7th IEEE Symposium Series on Computational\n  Intelligence (IEEE SSCI 2016), 8 pages, 6 figures. Minor revisions, in\n  response to reviewers' comments", "journal-ref": null, "doi": "10.1109/SSCI.2016.7849978", "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose cross-modal convolutional neural networks (X-CNNs),\na novel biologically inspired type of CNN architectures, treating gradient\ndescent-specialised CNNs as individual units of processing in a larger-scale\nnetwork topology, while allowing for unconstrained information flow and/or\nweight sharing between analogous hidden layers of the network---thus\ngeneralising the already well-established concept of neural network ensembles\n(where information typically may flow only between the output layers of the\nindividual networks). The constituent networks are individually designed to\nlearn the output function on their own subset of the input data, after which\ncross-connections between them are introduced after each pooling operation to\nperiodically allow for information exchange between them. This injection of\nknowledge into a model (by prior partition of the input data through domain\nknowledge or unsupervised methods) is expected to yield greatest returns in\nsparse data environments, which are typically less suitable for training CNNs.\nFor evaluation purposes, we have compared a standard four-layer CNN as well as\na sophisticated FitNet4 architecture against their cross-modal variants on the\nCIFAR-10 and CIFAR-100 datasets with differing percentages of the training data\nbeing removed, and find that at lower levels of data availability, the X-CNNs\nsignificantly outperform their baselines (typically providing a 2--6% benefit,\ndepending on the dataset size and whether data augmentation is used), while\nstill maintaining an edge on all of the full dataset tests.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 18:01:35 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 14:51:36 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Wang", "Duo", ""], ["Lane", "Nicholas D.", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1610.00168", "submitter": "Berk Ustun", "authors": "Berk Ustun, Cynthia Rudin", "title": "Learning Optimized Risk Scores", "comments": null, "journal-ref": "Journal of Machine Learning Research 2019. Volume 20. Issue 150.\n  Pages 1-75", "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk scores are simple classification models that let users make quick risk\npredictions by adding and subtracting a few small numbers. These models are\nwidely used in medicine and criminal justice, but are difficult to learn from\ndata because they need to be calibrated, sparse, use small integer\ncoefficients, and obey application-specific operational constraints. In this\npaper, we present a new machine learning approach to learn risk scores. We\nformulate the risk score problem as a mixed integer nonlinear program, and\npresent a cutting plane algorithm for non-convex settings to efficiently\nrecover its optimal solution. We improve our algorithm with specialized\ntechniques to generate feasible solutions, narrow the optimality gap, and\nreduce data-related computation. Our approach can fit risk scores in a way that\nscales linearly in the number of samples, provides a certificate of optimality,\nand obeys real-world constraints without parameter tuning or post-processing.\nWe benchmark the performance benefits of this approach through an extensive set\nof numerical experiments, comparing to risk scores built using heuristic\napproaches. We also discuss its practical benefits through a real-world\napplication where we build a customized risk score for ICU seizure prediction\nin collaboration with the Massachusetts General Hospital.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 18:40:08 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 15:06:56 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 02:27:05 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 17:18:35 GMT"}, {"version": "v5", "created": "Tue, 17 Sep 2019 01:58:54 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1610.00189", "submitter": "Dale Jennings", "authors": "D. Jennings, J. N. Corcoran", "title": "A Birth and Death Process for Bayesian Network Structure Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks (BNs) are graphical models that are useful for representing\nhigh-dimensional probability distributions. There has been a great deal of\ninterest in recent years in the NP-hard problem of learning the structure of a\nBN from observed data. Typically, one assigns a score to various structures and\nthe search becomes an optimization problem that can be approached with either\ndeterministic or stochastic methods. In this paper, we walk through the space\nof graphs by modeling the appearance and disappearance of edges as a birth and\ndeath process and compare our novel approach to the popular Metropolis-Hastings\nsearch strategy. We give empirical evidence that the birth and death process\nhas superior mixing properties.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 20:57:13 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Jennings", "D.", ""], ["Corcoran", "J. N.", ""]]}, {"id": "1610.00197", "submitter": "John Dabiri", "authors": "Kristy L. Schlueter-Kuck and John O. Dabiri", "title": "Coherent structure coloring: identification of coherent structures from\n  sparse data using graph theory", "comments": "In press at Journal of Fluid Mechanics. Software package available at\n  http://dabirilab.com/software/", "journal-ref": null, "doi": "10.1017/jfm.2016.755", "report-no": null, "categories": "physics.flu-dyn math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a frame-invariant method for detecting coherent structures from\nLagrangian flow trajectories that can be sparse in number, as is the case in\nmany fluid mechanics applications of practical interest. The method, based on\nprinciples used in graph coloring and spectral graph drawing algorithms,\nexamines a measure of the kinematic dissimilarity of all pairs of fluid\ntrajectories, either measured experimentally, e.g. using particle tracking\nvelocimetry; or numerically, by advecting fluid particles in the Eulerian\nvelocity field. Coherence is assigned to groups of particles whose kinematics\nremain similar throughout the time interval for which trajectory data is\navailable, regardless of their physical proximity to one another. Through the\nuse of several analytical and experimental validation cases, this algorithm is\nshown to robustly detect coherent structures using significantly less flow data\nthan is required by existing spectral graph theory methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 21:56:56 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 20:33:48 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Schlueter-Kuck", "Kristy L.", ""], ["Dabiri", "John O.", ""]]}, {"id": "1610.00199", "submitter": "Laura Balzano", "authors": "Dejiao Zhang, Laura Balzano", "title": "Convergence of a Grassmannian Gradient Descent Algorithm for Subspace\n  Estimation From Undersampled Data", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace learning and matrix factorization problems have great many\napplications in science and engineering, and efficient algorithms are critical\nas dataset sizes continue to grow. Many relevant problem formulations are\nnon-convex, and in a variety of contexts it has been observed that solving the\nnon-convex problem directly is not only efficient but reliably accurate. We\ndiscuss convergence theory for a particular method: first order incremental\ngradient descent constrained to the Grassmannian. The output of the algorithm\nis an orthonormal basis for a $d$-dimensional subspace spanned by an input\nstreaming data matrix. We study two sampling cases: where each data vector of\nthe streaming matrix is fully sampled, or where it is undersampled by a\nsampling matrix $A_t\\in \\mathbb{R}^{m\\times n}$ with $m\\ll n$. Our results\ncover two cases, where $A_t$ is Gaussian or a subset of rows of the identity\nmatrix. We propose an adaptive stepsize scheme that depends only on the sampled\ndata and algorithm outputs. We prove that with fully sampled data, the stepsize\nscheme maximizes the improvement of our convergence metric at each iteration,\nand this method converges from any random initialization to the true subspace,\ndespite the non-convex formulation and orthogonality constraints. For the case\nof undersampled data, we establish monotonic expected improvement on the\ndefined convergence metric for each iteration with high probability.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 22:19:02 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 19:17:34 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""]]}, {"id": "1610.00207", "submitter": "Johannes Lederer", "authors": "Wei Li and Johannes Lederer", "title": "Tuning parameter calibration for $\\ell_1$-regularized logistic\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a standard approach to understanding and modeling\nhigh-dimensional classification data, but the corresponding statistical methods\nhinge on tuning parameters that are difficult to calibrate. In particular,\nexisting calibration schemes in the logistic regression framework lack any\nfinite sample guarantees. In this paper, we introduce a novel calibration\nscheme for $\\ell_1$-penalized logistic regression. It is based on simple tests\nalong the tuning parameter path and is equipped with optimal guarantees for\nfeature selection. It is also amenable to easy and efficient implementations,\nand it rivals or outmatches existing methods in simulations and real data\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 23:53:04 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 14:27:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Li", "Wei", ""], ["Lederer", "Johannes", ""]]}, {"id": "1610.00243", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Nir Ailon", "title": "Deep unsupervised learning through spatial contrasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 08:42:59 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 15:38:31 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Ailon", "Nir", ""]]}, {"id": "1610.00246", "submitter": "Abbas Hosseini", "authors": "Seyed Abbas Hosseini, Ali Khodadadi, Soheil Arabzade and Hamid R.\n  Rabiee", "title": "HNP3: A Hierarchical Nonparametric Point Process for Modeling Content\n  Diffusion over Social Media", "comments": "Accepted in IEEE International Conference on Data Mining (ICDM) 2016,\n  Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework for modeling temporal events with\ncomplex longitudinal dependency that are generated by dependent sources. This\nframework takes advantage of multidimensional point processes for modeling time\nof events. The intensity function of the proposed process is a mixture of\nintensities, and its complexity grows with the complexity of temporal patterns\nof data. Moreover, it utilizes a hierarchical dependent nonparametric approach\nto model marks of events. These capabilities allow the proposed model to adapt\nits temporal and topical complexity according to the complexity of data, which\nmakes it a suitable candidate for real world scenarios. An online inference\nalgorithm is also proposed that makes the framework applicable to a vast range\nof applications. The framework is applied to a real world application, modeling\nthe diffusion of contents over networks. Extensive experiments reveal the\neffectiveness of the proposed framework in comparison with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 09:03:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Hosseini", "Seyed Abbas", ""], ["Khodadadi", "Ali", ""], ["Arabzade", "Soheil", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1610.00270", "submitter": "Atilla Ozgur", "authors": "Atilla Ozgur, Hamit Erdem, Fatih Nar", "title": "Sparsity-driven weighted ensemble classifier", "comments": "Last version updated according to journal version but not edited by\n  journal", "journal-ref": "International Journal of Computational Intelligence Systems, 2018", "doi": "10.2991/ijcis.11.1.73", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, a novel sparsity-driven weighted ensemble classifier (SDWEC)\nthat improves classification accuracy and minimizes the number of classifiers\nis proposed. Using pre-trained classifiers, an ensemble in which base\nclassifiers votes according to assigned weights is formed. These assigned\nweights directly affect classifier accuracy. In the proposed method, ensemble\nweights finding problem is modeled as a cost function with the following terms:\n(a) a data fidelity term aiming to decrease misclassification rate, (b) a\nsparsity term aiming to decrease the number of classifiers, and (c) a\nnon-negativity constraint on the weights of the classifiers. As the proposed\ncost function is non-convex thus hard to solve, convex relaxation techniques\nand novel approximations are employed to obtain a numerically efficient\nsolution. Sparsity term of cost function allows trade-off between accuracy and\ntesting time when needed. The efficiency of SDWEC was tested on 11 datasets and\ncompared with the state-of-the art classifier ensemble methods. The results\nshow that SDWEC provides better or similar accuracy levels using fewer\nclassifiers and reduces testing time for ensemble.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 12:33:48 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 11:16:46 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 19:25:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ozgur", "Atilla", ""], ["Erdem", "Hamit", ""], ["Nar", "Fatih", ""]]}, {"id": "1610.00279", "submitter": "Andrey Makarenko", "authors": "A.V. Makarenko", "title": "Deep Learning Algorithms for Signal Recognition in Long Perimeter\n  Monitoring Distributed Fiber Optic Sensors", "comments": "11 pages, 7 figures, 2 tables. Slightly extended preprint of paper\n  accepted for IEEE MLSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show an approach to build deep learning algorithms for\nrecognizing signals in distributed fiber optic monitoring and security systems\nfor long perimeters. Synthesizing such detection algorithms poses a non-trivial\nresearch and development challenge, because these systems face stringent error\n(type I and II) requirements and operate in difficult signal-jamming\nenvironments, with intensive signal-like jamming and a variety of changing\npossible signal portraits of possible recognized events. To address these\nissues, we have developed a twolevel event detection architecture, where the\nprimary classifier is based on an ensemble of deep convolutional networks, can\nrecognize 7 classes of signals and receives time-space data frames as input.\nUsing real-life data, we have shown that the applied methods result in\nefficient and robust multiclass detection algorithms that have a high degree of\nadaptability.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 13:46:47 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Makarenko", "A. V.", ""]]}, {"id": "1610.00345", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "Density Estimation with Distribution Element Trees", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-017-9751-9", "report-no": null, "categories": "stat.ME cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of probability densities based on available data is a central\ntask in many statistical applications. Especially in the case of large\nensembles with many samples or high-dimensional sample spaces, computationally\nefficient methods are needed. We propose a new method that is based on a\ndecomposition of the unknown distribution in terms of so-called distribution\nelements (DEs). These elements enable an adaptive and hierarchical\ndiscretization of the sample space with small or large elements in regions with\nsmoothly or highly variable densities, respectively. The novel refinement\nstrategy that we propose is based on statistical goodness-of-fit and pair-wise\n(as an approximation to mutual) independence tests that evaluate the local\napproximation of the distribution in terms of DEs. The capabilities of our new\nmethod are inspected based on several examples of different dimensionality and\nsuccessfully compared with other state-of-the-art density estimators.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 20:07:28 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 07:08:16 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1610.00362", "submitter": "Baosen Zhang", "authors": "Pan Li and Baosen Zhang", "title": "An Optimal Treatment Assignment Strategy to Evaluate Demand Response\n  Effect", "comments": "A shorter version appeared in Proceedings of the 2016 Allerton\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand response is designed to motivate electricity customers to modify their\nloads at critical time periods. The accurate estimation of impact of demand\nresponse signals to customers' consumption is central to any successful\nprogram. In practice, learning these response is nontrivial because operators\ncan only send a limited number of signals. In addition, customer behavior also\ndepends on a large number of exogenous covariates. These two features lead to a\nhigh dimensional inference problem with limited number of observations. In this\npaper, we formulate this problem by using a multivariate linear model and adopt\nan experimental design approach to estimate the impact of demand response\nsignals. We show that randomized assignment, which is widely used to estimate\nthe average treatment effect, is not efficient in reducing the variance of the\nestimator when a large number of covariates is present. In contrast, we present\na tractable algorithm that strategically assigns demand response signals to\ncustomers. This algorithm achieves the optimal reduction in estimation\nvariance, independent of the number of covariates. The results are validated\nfrom simulations on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 22:48:10 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 15:11:40 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Li", "Pan", ""], ["Zhang", "Baosen", ""]]}, {"id": "1610.00366", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "Funneled Bayesian Optimization for Design, Tuning and Control of\n  Autonomous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has become a fundamental global optimization algorithm\nin many problems where sample efficiency is of paramount importance. Recently,\nthere has been proposed a large number of new applications in fields such as\nrobotics, machine learning, experimental design, simulation, etc. In this\npaper, we focus on several problems that appear in robotics and autonomous\nsystems: algorithm tuning, automatic control and intelligent design. All those\nproblems can be mapped to global optimization problems. However, they become\nhard optimization problems. Bayesian optimization internally uses a\nprobabilistic surrogate model (e.g.: Gaussian process) to learn from the\nprocess and reduce the number of samples required. In order to generalize to\nunknown functions in a black-box fashion, the common assumption is that the\nunderlying function can be modeled with a stationary process. Nonstationary\nGaussian process regression cannot generalize easily and it typically requires\nprior knowledge of the function. Some works have designed techniques to\ngeneralize Bayesian optimization to nonstationary functions in an indirect way,\nbut using techniques originally designed for regression, where the objective is\nto improve the quality of the surrogate model everywhere. Instead optimization\nshould focus on improving the surrogate model near the optimum. In this paper,\nwe present a novel kernel function specially designed for Bayesian\noptimization, that allows nonstationary behavior of the surrogate model in an\nadaptive local region. In our experiments, we found that this new kernel\nresults in an improved local search (exploitation), without penalizing the\nglobal search (exploration). We provide results in well-known benchmarks and\nreal applications. The new method outperforms the state of the art in Bayesian\noptimization both in stationary and nonstationary problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 23:13:45 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:48:07 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1610.00470", "submitter": "Giulio Bottegal", "authors": "Giulio Bottegal, H\\r{a}kan Hjalmarsson, Gianluigi Pillonetto", "title": "A new kernel-based approach to system identification with quantized\n  output data", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method for linear system identification\nwith quantized output data. We model the impulse response as a zero-mean\nGaussian process whose covariance (kernel) is given by the recently proposed\nstable spline kernel, which encodes information on regularity and exponential\nstability. This serves as a starting point to cast our system identification\nproblem into a Bayesian framework. We employ Markov Chain Monte Carlo methods\nto provide an estimate of the system. In particular, we design two methods\nbased on the so-called Gibbs sampler that allow also to estimate the kernel\nhyperparameters by marginal likelihood maximization via the\nexpectation-maximization method. Numerical simulations show the effectiveness\nof the proposed scheme, as compared to the state-of-the-art kernel-based\nmethods when these are employed in system identification with quantized data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 09:58:40 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 08:18:51 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Bottegal", "Giulio", ""], ["Hjalmarsson", "H\u00e5kan", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1610.00494", "submitter": "Ivan Y. Tyukin", "authors": "Alexander N. Gorban, Ilya Romanenko, Richard Burton, Ivan Y. Tyukin", "title": "One-Trial Correction of Legacy AI Systems and Stochastic Separation\n  Theorems", "comments": null, "journal-ref": "Information Sciences, 484, 237-254, 2019", "doi": "10.1016/j.ins.2019.02.001", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of efficient \"on the fly\" tuning of existing, or {\\it\nlegacy}, Artificial Intelligence (AI) systems. The legacy AI systems are\nallowed to be of arbitrary class, albeit the data they are using for computing\ninterim or final decision responses should posses an underlying structure of a\nhigh-dimensional topological real vector space. The tuning method that we\npropose enables dealing with errors without the need to re-train the system.\nInstead of re-training a simple cascade of perceptron nodes is added to the\nlegacy system. The added cascade modulates the AI legacy system's decisions. If\napplied repeatedly, the process results in a network of modulating rules\n\"dressing up\" and improving performance of existing AI systems. Mathematical\nrationale behind the method is based on the fundamental property of measure\nconcentration in high dimensional spaces. The method is illustrated with an\nexample of fine-tuning a deep convolutional network that has been pre-trained\nto detect pedestrians in images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 11:15:12 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 18:52:25 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 15:24:27 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 09:14:55 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gorban", "Alexander N.", ""], ["Romanenko", "Ilya", ""], ["Burton", "Richard", ""], ["Tyukin", "Ivan Y.", ""]]}, {"id": "1610.00520", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Semi-supervised Learning with Sparse Autoencoders in Phone\n  Classification", "comments": "5 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the application of a semi-supervised learning method to improve\nthe performance of acoustic modelling for automatic speech recognition based on\ndeep neural net- works. As opposed to unsupervised initialisation followed by\nsupervised fine tuning, our method takes advantage of both unlabelled and\nlabelled data simultaneously through mini- batch stochastic gradient descent.\nWe tested the method with varying proportions of labelled vs unlabelled\nobservations in frame-based phoneme classification on the TIMIT database. Our\nexperiments show that the method outperforms standard supervised training for\nan equal amount of labelled data and provides competitive error rates compared\nto state-of-the-art graph-based semi-supervised learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 12:52:26 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1610.00667", "submitter": "Xin Gao Dr.", "authors": "Xin Gao and Raymond J. Carroll", "title": "Data Integration with High Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of data integration. Consider determining which genes\naffect a disease. The genes, which we call predictor objects, can be measured\nin different experiments on the same individual. We address the question of\nfinding which genes are predictors of disease by any of the experiments. Our\nformulation is more general. In a given data set, there are a fixed number of\nresponses for each individual, which may include a mix of discrete, binary and\ncontinuous variables. There is also a class of predictor objects, which may\ndiffer within a subject depending on how the predictor object is measured,\ni.e., depend on the experiment. The goal is to select which predictor objects\naffect any of the responses, where the number of such informative predictor\nobjects or features tends to infinity as sample size increases. There are\nmarginal likelihoods for each way the predictor object is measured, i.e., for\neach experiment. We specify a pseudolikelihood combining the marginal\nlikelihoods, and propose a pseudolikelihood information criterion. Under\nregularity conditions, we establish selection consistency for the\npseudolikelihood information criterion with unbounded true model size, which\nincludes a Bayesian information criterion with appropriate penalty term as a\nspecial case. Simulations indicate that data integration improves upon,\nsometimes dramatically, using only one of the data sources.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 18:40:25 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Gao", "Xin", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1610.00732", "submitter": "Yao Xie", "authors": "Yao Xie and Lee Seversky", "title": "Sequential Low-Rank Change Detection", "comments": "Presented at Allerton Conference, 2016. Partially supported by a AFRI\n  Visiting Faculty Fellowship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting emergence of a low-rank signal from high-dimensional data is an\nimportant problem arising from many applications such as camera surveillance\nand swarm monitoring using sensors. We consider a procedure based on the\nlargest eigenvalue of the sample covariance matrix over a sliding window to\ndetect the change. To achieve dimensionality reduction, we present a\nsketching-based approach for rank change detection using the low-dimensional\nlinear sketches of the original high-dimensional observations. The premise is\nthat when the sketching matrix is a random Gaussian matrix, and the dimension\nof the sketching vector is sufficiently large, the rank of sample covariance\nmatrix for these sketches equals the rank of the original sample covariance\nmatrix with high probability. Hence, we may be able to detect the low-rank\nchange using sample covariance matrices of the sketches without having to\nrecover the original covariance matrix. We character the performance of the\nlargest eigenvalue statistic in terms of the false-alarm-rate and the expected\ndetection delay, and present an efficient online implementation via subspace\ntracking.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:29:37 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:44:49 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Xie", "Yao", ""], ["Seversky", "Lee", ""]]}, {"id": "1610.00768", "submitter": "Yash Sharma", "authors": "Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow,\n  Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko\n  Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang,\n  Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi\n  Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, Rujun\n  Long, and Patrick McDaniel", "title": "Technical Report on the CleverHans v2.1.0 Adversarial Examples Library", "comments": "Technical report for https://github.com/tensorflow/cleverhans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CleverHans is a software library that provides standardized reference\nimplementations of adversarial example construction techniques and adversarial\ntraining. The library may be used to develop more robust machine learning\nmodels and to provide standardized benchmarks of models' performance in the\nadversarial setting. Benchmarks constructed without a standardized\nimplementation of adversarial example construction are not comparable to each\nother, because a good result may indicate a robust model or it may merely\nindicate a weak implementation of the adversarial example construction\nprocedure.\n  This technical report is structured as follows. Section 1 provides an\noverview of adversarial examples in machine learning and of the CleverHans\nsoftware. Section 2 presents the core functionalities of the library: namely\nthe attacks based on adversarial examples and defenses to improve the\nrobustness of machine learning models to these attacks. Section 3 describes how\nto report benchmark results using the library. Section 4 describes the\nversioning system.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 22:04:07 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 13:54:04 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 10:47:15 GMT"}, {"version": "v4", "created": "Thu, 5 Oct 2017 17:27:32 GMT"}, {"version": "v5", "created": "Thu, 17 May 2018 22:51:41 GMT"}, {"version": "v6", "created": "Wed, 27 Jun 2018 21:06:06 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Papernot", "Nicolas", ""], ["Faghri", "Fartash", ""], ["Carlini", "Nicholas", ""], ["Goodfellow", "Ian", ""], ["Feinman", "Reuben", ""], ["Kurakin", "Alexey", ""], ["Xie", "Cihang", ""], ["Sharma", "Yash", ""], ["Brown", "Tom", ""], ["Roy", "Aurko", ""], ["Matyasko", "Alexander", ""], ["Behzadan", "Vahid", ""], ["Hambardzumyan", "Karen", ""], ["Zhang", "Zhishuai", ""], ["Juang", "Yi-Lin", ""], ["Li", "Zhi", ""], ["Sheatsley", "Ryan", ""], ["Garg", "Abhibhav", ""], ["Uesato", "Jonathan", ""], ["Gierke", "Willi", ""], ["Dong", "Yinpeng", ""], ["Berthelot", "David", ""], ["Hendricks", "Paul", ""], ["Rauber", "Jonas", ""], ["Long", "Rujun", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1610.00843", "submitter": "Avik Ray", "authors": "Avik Ray, Joe Neeman, Sujay Sanghavi, Sanjay Shakkottai", "title": "The Search Problem in Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning the parameters of a {\\em single} component\nof a mixture model, for the case when we are given {\\em side information} about\nthat component, we call this the \"search problem\" in mixture models. We would\nlike to solve this with computational and sample complexity lower than solving\nthe overall original problem, where one learns parameters of all components.\n  Our main contributions are the development of a simple but general model for\nthe notion of side information, and a corresponding simple matrix-based\nalgorithm for solving the search problem in this general setting. We then\nspecialize this model and algorithm to four common scenarios: Gaussian mixture\nmodels, LDA topic models, subspace clustering, and mixed linear regression. For\neach one of these we show that if (and only if) the side information is\ninformative, we obtain parameter estimates with greater accuracy, and also\nimproved computation complexity than existing moment based mixture model\nalgorithms (e.g. tensor methods). We also illustrate several natural ways one\ncan obtain such side information, for specific problem instances. Our\nexperiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the\npracticality of our algorithms showing significant improvement in runtime and\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 05:01:18 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 21:49:02 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ray", "Avik", ""], ["Neeman", "Joe", ""], ["Sanghavi", "Sujay", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1610.00844", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan A. Rossi, Theodore L. Willke, Rong Zhou", "title": "Revisiting Role Discovery in Networks: From Node to Edge Roles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work in network analysis has focused on modeling the\nmixed-memberships of node roles in the graph, but not the roles of edges. We\nintroduce the edge role discovery problem and present a generalizable framework\nfor learning and extracting edge roles from arbitrary graphs automatically.\nFurthermore, while existing node-centric role models have mainly focused on\nsimple degree and egonet features, this work also explores graphlet features\nfor role discovery. In addition, we also develop an approach for automatically\nlearning and extracting important and useful edge features from an arbitrary\ngraph. The experimental results demonstrate the utility of edge roles for\nnetwork analysis tasks on a variety of graphs from various problem domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 05:04:30 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 22:10:31 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Willke", "Theodore L.", ""], ["Zhou", "Rong", ""]]}, {"id": "1610.00907", "submitter": "Stefan Bauer", "authors": "Benjamin Fischer, Nico Gorbach, Stefan Bauer, Yatao Bian, Joachim M.\n  Buhmann", "title": "Model Selection for Gaussian Process Regression by Approximation Set\n  Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are powerful, yet analytically tractable models for\nsupervised learning. A Gaussian process is characterized by a mean function and\na covariance function (kernel), which are determined by a model selection\ncriterion. The functions to be compared do not just differ in their\nparametrization but in their fundamental structure. It is often not clear which\nfunction structure to choose, for instance to decide between a squared\nexponential and a rational quadratic kernel. Based on the principle of\napproximation set coding, we develop a framework for model selection to rank\nkernels for Gaussian process regression. In our experiments approximation set\ncoding shows promise to become a model selection criterion competitive with\nmaximum evidence (also called marginal likelihood) and leave-one-out\ncross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 09:20:08 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Fischer", "Benjamin", ""], ["Gorbach", "Nico", ""], ["Bauer", "Stefan", ""], ["Bian", "Yatao", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1610.00960", "submitter": "Julien Mairal", "authors": "Hongzhou Lin (Thoth, CSAIL), Julien Mairal (Thoth), Zaid Harchaoui", "title": "An Inexact Variable Metric Proximal Point Algorithm for Generic\n  Quasi-Newton Acceleration", "comments": "to appear in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inexact variable-metric proximal point algorithm to accelerate\ngradient-based optimization algorithms. The proposed scheme, called QNing can\nbe notably applied to incremental first-order methods such as the stochastic\nvariance-reduced gradient descent algorithm (SVRG) and other randomized\nincremental optimization algorithms. QNing is also compatible with composite\nobjectives, meaning that it has the ability to provide exactly sparse solutions\nwhen the objective involves a sparsity-inducing regularization. When combined\nwith limited-memory BFGS rules, QNing is particularly effective to solve\nhigh-dimensional optimization problems, while enjoying a worst-case linear\nconvergence rate for strongly convex problems. We present experimental results\nwhere QNing gives significant improvements over competing methods for training\nmachine learning methods on large samples and in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 12:51:20 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 12:11:17 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 14:00:52 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 13:24:37 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Lin", "Hongzhou", "", "Thoth, CSAIL"], ["Mairal", "Julien", "", "Thoth"], ["Harchaoui", "Zaid", ""]]}, {"id": "1610.00970", "submitter": "Alberto Bietti", "authors": "Alberto Bietti, Julien Mairal", "title": "Stochastic Optimization with Variance Reduction for Infinite Datasets\n  with Finite-Sum Structure", "comments": "Advances in Neural Information Processing Systems (NIPS), Dec 2017,\n  Long Beach, CA, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimization algorithms with variance reduction have proven\nsuccessful for minimizing large finite sums of functions. Unfortunately, these\ntechniques are unable to deal with stochastic perturbations of input data,\ninduced for example by data augmentation. In such cases, the objective is no\nlonger a finite sum, and the main candidate for optimization is the stochastic\ngradient descent method (SGD). In this paper, we introduce a variance reduction\napproach for these settings when the objective is composite and strongly\nconvex. The convergence rate outperforms SGD with a typically much smaller\nconstant factor, which depends on the variance of gradient estimates only due\nto perturbations on a single example.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 13:08:42 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 12:22:56 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 10:21:02 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 13:45:38 GMT"}, {"version": "v5", "created": "Thu, 1 Jun 2017 10:30:01 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 13:48:57 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bietti", "Alberto", ""], ["Mairal", "Julien", ""]]}, {"id": "1610.01000", "submitter": "Lucie Montuelle", "authors": "Aur\\'elie Fischer (UPD7), Lucie Montuelle (UPD7), Mathilde Mougeot\n  (UPD7), Dominique Picard (UPD7)", "title": "Statistical learning for wind power : a modeling and stability study\n  towards forecasting", "comments": null, "journal-ref": "Wind Energy, Wiley, 2017, 20 (12), pp.2037 - 2047", "doi": "10.1002/we.2139", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on wind power modeling using machine learning techniques. We show on\nreal data provided by the wind energy company Ma{\\\"i}a Eolis, that parametric\nmodels, even following closely the physical equation relating wind production\nto wind speed are outperformed by intelligent learning algorithms. In\nparticular, the CART-Bagging algorithm gives very stable and promising results.\nBesides, as a step towards forecast, we quantify the impact of using\ndeteriorated wind measures on the performances. We show also on this\napplication that the default methodology to select a subset of predictors\nprovided in the standard random forest package can be refined, especially when\nthere exists among the predictors one variable which has a major impact.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:03:24 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 10:05:35 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Fischer", "Aur\u00e9lie", "", "UPD7"], ["Montuelle", "Lucie", "", "UPD7"], ["Mougeot", "Mathilde", "", "UPD7"], ["Picard", "Dominique", "", "UPD7"]]}, {"id": "1610.01101", "submitter": "Damek Davis", "authors": "Aleksandr Aravkin and Damek Davis", "title": "A SMART Stochastic Algorithm for Nonconvex Optimization with\n  Applications to Robust Machine Learning", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how to transform any optimization problem that arises\nfrom fitting a machine learning model into one that (1) detects and removes\ncontaminated data from the training set while (2) simultaneously fitting the\ntrimmed model on the uncontaminated data that remains. To solve the resulting\nnonconvex optimization problem, we introduce a fast stochastic\nproximal-gradient algorithm that incorporates prior knowledge through nonsmooth\nregularization. For datasets of size $n$, our approach requires\n$O(n^{2/3}/\\varepsilon)$ gradient evaluations to reach $\\varepsilon$-accuracy\nand, when a certain error bound holds, the complexity improves to $O(\\kappa\nn^{2/3}\\log(1/\\varepsilon))$. These rates are $n^{1/3}$ times better than those\nachieved by typical, full gradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 17:24:43 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 15:24:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Aravkin", "Aleksandr", ""], ["Davis", "Damek", ""]]}, {"id": "1610.01132", "submitter": "Tengyu Ma", "authors": "Elad Hazan, Tengyu Ma", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised\n  Learning", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a novel formal theoretical framework for unsupervised learning with\ntwo distinctive characteristics. First, it does not assume any generative model\nand based on a worst-case performance metric. Second, it is comparative, namely\nperformance is measured with respect to a given hypothesis class. This allows\nto avoid known computational hardness results and improper algorithms based on\nconvex relaxations. We show how several families of unsupervised learning\nmodels, which were previously only analyzed under probabilistic assumptions and\nare otherwise provably intractable, can be efficiently learned in our framework\nby convex optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 19:22:44 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 00:30:02 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 20:59:01 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Hazan", "Elad", ""], ["Ma", "Tengyu", ""]]}, {"id": "1610.01234", "submitter": "Eric Bax", "authors": "Eric Bax and Farshad Kooti", "title": "Ensemble Validation: Selectivity has a Price, but Variety is Free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose some classifiers are selected from a set of hypothesis classifiers to\nform an equally-weighted ensemble that selects a member classifier at random\nfor each input example. Then the ensemble has an error bound consisting of the\naverage error bound for the member classifiers, a term for selectivity that\nvaries from zero (if all hypothesis classifiers are selected) to a standard\nuniform error bound (if only a single classifier is selected), and small\nconstants. There is no penalty for using a richer hypothesis set if the same\nfraction of the hypothesis classifiers are selected for the ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 23:44:20 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 19:48:13 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 22:13:52 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Bax", "Eric", ""], ["Kooti", "Farshad", ""]]}, {"id": "1610.01256", "submitter": "Kush Varshney", "authors": "Kush R. Varshney and Homa Alemzadeh", "title": "On the Safety of Machine Learning: Cyber-Physical Systems, Decision\n  Sciences, and Data Products", "comments": "Big Data, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms increasingly influence our decisions and interact\nwith us in all parts of our daily lives. Therefore, just as we consider the\nsafety of power plants, highways, and a variety of other engineered\nsocio-technical systems, we must also take into account the safety of systems\ninvolving machine learning. Heretofore, the definition of safety has not been\nformalized in a machine learning context. In this paper, we do so by defining\nmachine learning safety in terms of risk, epistemic uncertainty, and the harm\nincurred by unwanted outcomes. We then use this definition to examine safety in\nall sorts of applications in cyber-physical systems, decision sciences, and\ndata products. We find that the foundational principle of modern statistical\nmachine learning, empirical risk minimization, is not always a sufficient\nobjective. Finally, we discuss how four different categories of strategies for\nachieving safety in engineering, including inherently safe design, safety\nreserves, safe fail, and procedural safeguards can be mapped to a machine\nlearning context. We then discuss example techniques that can be adopted in\neach category, such as considering interpretability and causality of predictive\nmodels, objective functions beyond expected prediction accuracy, human\ninvolvement for labeling difficult or rare examples, and user experience design\nof software and open data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 02:16:48 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 15:18:12 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Varshney", "Kush R.", ""], ["Alemzadeh", "Homa", ""]]}, {"id": "1610.01271", "submitter": "Stefan Wager", "authors": "Susan Athey, Julie Tibshirani and Stefan Wager", "title": "Generalized Random Forests", "comments": "Forthcoming in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose generalized random forests, a method for non-parametric\nstatistical estimation based on random forests (Breiman, 2001) that can be used\nto fit any quantity of interest identified as the solution to a set of local\nmoment equations. Following the literature on local maximum likelihood\nestimation, our method considers a weighted set of nearby training examples;\nhowever, instead of using classical kernel weighting functions that are prone\nto a strong curse of dimensionality, we use an adaptive weighting function\nderived from a forest designed to express heterogeneity in the specified\nquantity of interest. We propose a flexible, computationally efficient\nalgorithm for growing generalized random forests, develop a large sample theory\nfor our method showing that our estimates are consistent and asymptotically\nGaussian, and provide an estimator for their asymptotic variance that enables\nvalid confidence intervals. We use our approach to develop new methods for\nthree statistical tasks: non-parametric quantile regression, conditional\naverage partial effect estimation, and heterogeneous treatment effect\nestimation via instrumental variables. A software implementation, grf for R and\nC++, is available from CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 04:42:13 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 02:56:28 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 01:30:47 GMT"}, {"version": "v4", "created": "Thu, 5 Apr 2018 17:51:23 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Athey", "Susan", ""], ["Tibshirani", "Julie", ""], ["Wager", "Stefan", ""]]}, {"id": "1610.01417", "submitter": "Igor Colin", "authors": "Igor Colin, Christophe Dupuy", "title": "Decentralized Topic Modelling with Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving networks can be modelled as decentralized networks (e.g.,\nsensors, connected objects, smartphones), where communication between nodes of\nthe network is not controlled by an all-knowing, central node. For this type of\nnetworks, the main issue is to gather/learn global information on the network\n(e.g., by optimizing a global cost function) while keeping the (sensitive)\ninformation at each node. In this work, we focus on text information that\nagents do not want to share (e.g., text messages, emails, confidential\nreports). We use recent advances on decentralized optimization and topic models\nto infer topics from a graph with limited communication. We propose a method to\nadapt latent Dirichlet allocation (LDA) model to decentralized optimization and\nshow on synthetic data that we still recover similar parameters and similar\nperformance at each node than with stochastic methods accessing to the whole\ninformation in the graph.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 13:45:53 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Colin", "Igor", ""], ["Dupuy", "Christophe", ""]]}, {"id": "1610.01424", "submitter": "Erika Helgeson", "authors": "Erika S. Helgeson, Eric Bair", "title": "Non-Parametric Cluster Significance Testing with Reference to a Unimodal\n  Null Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is an unsupervised learning strategy that can be employed to\nidentify subgroups of observations in data sets of unknown structure. This\nstrategy is particularly useful for analyzing high-dimensional data such as\nmicroarray gene expression data. Many clustering methods are available, but it\nis challenging to determine if the identified clusters represent distinct\nsubgroups. We propose a novel strategy to investigate the significance of\nidentified clusters by comparing the within- cluster sum of squares from the\noriginal data to that produced by clustering an appropriate unimodal null\ndistribution. The null distribution we present for this problem uses kernel\ndensity estimation and thus does not require that the data follow any\nparticular distribution. We find that our method can accurately test for the\npresence of clustering even when the number of features is high.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:01:57 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 00:18:17 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Helgeson", "Erika S.", ""], ["Bair", "Eric", ""]]}, {"id": "1610.01492", "submitter": "Jiali Mei Jiali Mei", "authors": "Jiali Mei, Yohann De Castro, Yannig Goude, Georges H\\'ebrail", "title": "Recovering Multiple Nonnegative Time Series From a Few Temporal\n  Aggregates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by electricity consumption metering, we extend existing nonnegative\nmatrix factorization (NMF) algorithms to use linear measurements as\nobservations, instead of matrix entries. The objective is to estimate multiple\ntime series at a fine temporal scale from temporal aggregates measured on each\nindividual series. Furthermore, our algorithm is extended to take into account\nindividual autocorrelation to provide better estimation, using a recent convex\nrelaxation of quadratically constrained quadratic program. Extensive\nexperiments on synthetic and real-world electricity consumption datasets\nillustrate the effectiveness of our matrix recovery algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 15:58:18 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Mei", "Jiali", ""], ["De Castro", "Yohann", ""], ["Goude", "Yannig", ""], ["H\u00e9brail", "Georges", ""]]}, {"id": "1610.01549", "submitter": "Anthony Caterini", "authors": "Anthony Caterini and Dong Eui Chang", "title": "A Novel Representation of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become very popular for prediction in many\nareas. Their strength is in representation with a high number of parameters\nthat are commonly learned via gradient descent or similar optimization methods.\nHowever, the representation is non-standardized, and the gradient calculation\nmethods are often performed using component-based approaches that break\nparameters down into scalar units, instead of considering the parameters as\nwhole entities. In this work, these problems are addressed. Standard notation\nis used to represent DNNs in a compact framework. Gradients of DNN loss\nfunctions are calculated directly over the inner product space on which the\nparameters are defined. This framework is general and is applied to two common\nnetwork types: the Multilayer Perceptron and the Deep Autoencoder.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 18:06:44 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 21:31:13 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Caterini", "Anthony", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1610.01633", "submitter": "Alexandra Piryatinska Dr", "authors": "Boris Darkhovsky, Alexandra Piryatinska, Alexander Kaplan", "title": "Binary classification of multi-channel EEG records based on the\n  $\\epsilon$-complexity of continuous vector functions", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A methodology for binary classification of EEG records which correspond to\ndifferent mental states is proposed. This model-free methodology is based on\nour theory of the $\\epsilon$-complexity of continuous functions which is\nextended here (see Appendix) to the case of vector functions. This extension\npermits us to handle multichannel EEG recordings. The essence of the\nmethodology is to use the $\\epsilon$-complexity coefficients as features to\nclassify (using well known classifiers) different types of vector functions\nrepresenting EEG-records corresponding to different types of mental states. We\napply our methodology to the problem of classification of multichannel\nEEG-records related to a group of healthy adolescents and a group of\nadolescents with schizophrenia. We found that our methodology permits accurate\nclassification of the data in the four-dimensional feather space of the\n$\\epsilon$-complexity coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:24:17 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Darkhovsky", "Boris", ""], ["Piryatinska", "Alexandra", ""], ["Kaplan", "Alexander", ""]]}, {"id": "1610.01642", "submitter": "Bharath Ramsundar", "authors": "Bharath Ramsundar and Vijay S. Pande", "title": "Learning Protein Dynamics with Metastable Switching Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a machine learning approach for extracting fine-grained\nrepresentations of protein evolution from molecular dynamics datasets.\nMetastable switching linear dynamical systems extend standard switching models\nwith a physically-inspired stability constraint. This constraint enables the\nlearning of nuanced representations of protein dynamics that closely match\nphysical reality. We derive an EM algorithm for learning, where the E-step\nextends the forward-backward algorithm for HMMs and the M-step requires the\nsolution of large biconvex optimization problems. We construct an approximate\nsemidefinite program solver based on the Frank-Wolfe algorithm and use it to\nsolve the M-step. We apply our EM algorithm to learn accurate dynamics from\nlarge simulation datasets for the opioid peptide met-enkephalin and the\nproto-oncogene Src-kinase. Our learned models demonstrate significant\nimprovements in temporal coherence over HMMs and standard switching models for\nmet-enkephalin, and sample transition paths (possibly useful in rational drug\ndesign) for Src-kinase.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:52:48 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ramsundar", "Bharath", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1610.01644", "submitter": "Guillaume Alain", "authors": "Guillaume Alain and Yoshua Bengio", "title": "Understanding intermediate layers using linear classifier probes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models have a reputation for being black boxes. We propose to\nmonitor the features at every layer of a model and measure how suitable they\nare for classification. We use linear classifiers, which we refer to as\n\"probes\", trained entirely independently of the model itself.\n  This helps us better understand the roles and dynamics of the intermediate\nlayers. We demonstrate how this can be used to develop a better intuition about\nmodels and to diagnose potential problems.\n  We apply this technique to the popular models Inception v3 and Resnet-50.\nAmong other things, we observe experimentally that the linear separability of\nfeatures increase monotonically along the depth of the model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:59:01 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 02:33:57 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 18:47:19 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 23:40:00 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Alain", "Guillaume", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1610.01675", "submitter": "Michael Lash", "authors": "Michael T. Lash, Qihang Lin, W. Nick Street, Jennifer G. Robinson,\n  Jeffrey Ohlmann", "title": "Generalized Inverse Classification", "comments": "Accepted to SDM 2017. Full paper + supplemental material", "journal-ref": null, "doi": "10.1137/1.9781611974973.19", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse classification is the process of perturbing an instance in a\nmeaningful way such that it is more likely to conform to a specific class.\nHistorical methods that address such a problem are often framed to leverage\nonly a single classifier, or specific set of classifiers. These works are often\naccompanied by naive assumptions. In this work we propose generalized inverse\nclassification (GIC), which avoids restricting the classification model that\ncan be used. We incorporate this formulation into a refined framework in which\nGIC takes place. Under this framework, GIC operates on features that are\nimmediately actionable. Each change incurs an individual cost, either linear or\nnon-linear. Such changes are subjected to occur within a specified level of\ncumulative change (budget). Furthermore, our framework incorporates the\nestimation of features that change as a consequence of direct actions taken\n(indirectly changeable features). To solve such a problem, we propose three\nreal-valued heuristic-based methods and two sensitivity analysis-based\ncomparison methods, each of which is evaluated on two freely available\nreal-world datasets. Our results demonstrate the validity and benefits of our\nformulation, framework, and methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 22:28:01 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 17:38:58 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Lash", "Michael T.", ""], ["Lin", "Qihang", ""], ["Street", "W. Nick", ""], ["Robinson", "Jennifer G.", ""], ["Ohlmann", "Jeffrey", ""]]}, {"id": "1610.01683", "submitter": "Orestis Tsinalis", "authors": "Orestis Tsinalis, Paul M. Matthews, Yike Guo, Stefanos Zafeiriou", "title": "Automatic Sleep Stage Scoring with Single-Channel EEG Using\n  Convolutional Neural Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We used convolutional neural networks (CNNs) for automatic sleep stage\nscoring based on single-channel electroencephalography (EEG) to learn\ntask-specific filters for classification without using prior domain knowledge.\nWe used an openly available dataset from 20 healthy young adults for evaluation\nand applied 20-fold cross-validation. We used class-balanced random sampling\nwithin the stochastic gradient descent (SGD) optimization of the CNN to avoid\nskewed performance in favor of the most represented sleep stages. We achieved\nhigh mean F1-score (81%, range 79-83%), mean accuracy across individual sleep\nstages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all\nsubjects. By analyzing and visualizing the filters that our CNN learns, we\nfound that rules learned by the filters correspond to sleep scoring criteria in\nthe American Academy of Sleep Medicine (AASM) manual that human experts follow.\nOur method's performance is balanced across classes and our results are\ncomparable to state-of-the-art methods with hand-engineered features. We show\nthat, without using prior domain knowledge, a CNN can automatically learn to\ndistinguish among different normal sleep stages.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:13:55 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Tsinalis", "Orestis", ""], ["Matthews", "Paul M.", ""], ["Guo", "Yike", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1610.01687", "submitter": "Zifan Li", "authors": "Zifan Li, Ambuj Tewari", "title": "Sampled Fictitious Play is Hannan Consistent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fictitious play is a simple and widely studied adaptive heuristic for playing\nrepeated games. It is well known that fictitious play fails to be Hannan\nconsistent. Several variants of fictitious play including regret matching,\ngeneralized regret matching and smooth fictitious play, are known to be Hannan\nconsistent. In this note, we consider sampled fictitious play: at each round,\nthe player samples past times and plays the best response to previous moves of\nother players at the sampled time points. We show that sampled fictitious play,\nusing Bernoulli sampling, is Hannan consistent. Unlike several existing Hannan\nconsistency proofs that rely on concentration of measure results, ours instead\nuses anti-concentration results from Littlewood-Offord theory.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:41:23 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:52:46 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1610.01698", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Alan A. Stocker", "title": "Human Decision-Making under Limited Time", "comments": "9 pages, 4 figures, NIPS Advances in Neural Information Processing\n  Systems 29, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective expected utility theory assumes that decision-makers possess\nunlimited computational resources to reason about their choices; however,\nvirtually all decisions in everyday life are made under resource constraints -\ni.e. decision-makers are bounded in their rationality. Here we experimentally\ntested the predictions made by a formalization of bounded rationality based on\nideas from statistical mechanics and information-theory. We systematically\ntested human subjects in their ability to solve combinatorial puzzles under\ndifferent time limitations. We found that our bounded-rational model accounts\nwell for the data. The decomposition of the fitted model parameter into the\nsubjects' expected utility function and resource parameter provide interesting\ninsight into the subjects' information capacity limits. Our results confirm\nthat humans gradually fall back on their learned prior choice patterns when\nconfronted with increasing resource limitations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 00:40:14 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Stocker", "Alan A.", ""]]}, {"id": "1610.01712", "submitter": "Sourangshu Bhattacharya", "authors": "Asis Roy, Sourangshu Bhattacharya, Kalyan Guin", "title": "A Methodology for Customizing Clinical Tests for Esophageal Cancer based\n  on Patient Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests for Esophageal cancer can be expensive, uncomfortable and can have side\neffects. For many patients, we can predict non-existence of disease with 100%\ncertainty, just using demographics, lifestyle, and medical history information.\nOur objective is to devise a general methodology for customizing tests using\nuser preferences so that expensive or uncomfortable tests can be avoided. We\npropose to use classifiers trained from electronic health records (EHR) for\nselection of tests. The key idea is to design classifiers with 100% false\nnormal rates, possibly at the cost higher false abnormals. We compare Naive\nBayes classification (NB), Random Forests (RF), Support Vector Machines (SVM)\nand Logistic Regression (LR), and find kernel Logistic regression to be most\nsuitable for the task. We propose an algorithm for finding the best probability\nthreshold for kernel LR, based on test set accuracy. Using the proposed\nalgorithm, we describe schemes for selecting tests, which appear as features in\nthe automatic classification algorithm, using preferences on costs and\ndiscomfort of the users. We test our methodology with EHRs collected for more\nthan 3000 patients, as a part of project carried out by a reputed hospital in\nMumbai, India. Kernel SVM and kernel LR with a polynomial kernel of degree 3,\nyields an accuracy of 99.8% and sensitivity 100%, without the MP features, i.e.\nusing only clinical tests. We demonstrate our test selection algorithm using\ntwo case studies, one using cost of clinical tests, and other using\n\"discomfort\" values for clinical tests. We compute the test sets corresponding\nto the lowest false abnormals for each criterion described above, using\nexhaustive enumeration of 15 clinical tests. The sets turn out to different,\nsubstantiating our claim that one can customize test sets based on user\npreferences.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 01:56:00 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Roy", "Asis", ""], ["Bhattacharya", "Sourangshu", ""], ["Guin", "Kalyan", ""]]}, {"id": "1610.01766", "submitter": "Badong Chen", "authors": "Siyuan Peng, Badong Chen, Lei Sun, Zhiping Lin, and Wee Ser", "title": "Constrained Maximum Correntropy Adaptive Filtering", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained adaptive filtering algorithms inculding constrained least mean\nsquare (CLMS), constrained affine projection (CAP) and constrained recursive\nleast squares (CRLS) have been extensively studied in many applications. Most\nexisting constrained adaptive filtering algorithms are developed under mean\nsquare error (MSE) criterion, which is an ideal optimality criterion under\nGaussian noises. This assumption however fails to model the behavior of\nnon-Gaussian noises found in practice. Motivated by the robustness and\nsimplicity of maximum correntropy criterion (MCC) in non-Gaussian impulsive\nnoises, this paper proposes a new adaptive filtering algorithm called\nconstrained maximum correntropy criterion (CMCC). Specifically, CMCC\nincorporates a linear constraint into a MCC filter to solve a constrained\noptimization problem explicitly. The proposed adaptive filtering algorithm is\neasy to implement and has low computational complexity, and in terms of\nconvergence accuracy (say lower mean square deviation) and stability, can\nsignificantly outperform those MSE based constrained adaptive algorithms in\npresence of heavy-tailed impulsive noises. Additionally, the mean square\nconvergence behaviors are studied under energy conservation relation, and a\nsufficient condition to ensure the mean square convergence and the steady-state\nmean square deviation (MSD) of the proposed algorithm are obtained. Simulation\nresults confirm the theoretical predictions under both Gaussian and non-\nGaussian noises, and demonstrate the excellent performance of the novel\nalgorithm by comparing it with other conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 07:56:11 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 03:37:38 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Peng", "Siyuan", ""], ["Chen", "Badong", ""], ["Sun", "Lei", ""], ["Lin", "Zhiping", ""], ["Ser", "Wee", ""]]}, {"id": "1610.01857", "submitter": "Abd AlRahman AlMomani", "authors": "Abd AlRahman AlMomani and Erik M. Bollt", "title": "Go With the Flow, on Jupiter and Snow. Coherence From Model-Free Video\n  Data without Trajectories", "comments": "26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewing a data set such as the clouds of Jupiter, coherence is readily\napparent to human observers, especially the Great Red Spot, but also other\ngreat storms and persistent structures. There are now many different\ndefinitions and perspectives mathematically describing coherent structures, but\nwe will take an image processing perspective here. We describe an image\nprocessing perspective inference of coherent sets from a fluidic system\ndirectly from image data, without attempting to first model underlying flow\nfields, related to a concept in image processing called motion tracking. In\ncontrast to standard spectral methods for image processing which are generally\nrelated to a symmetric affinity matrix, leading to standard spectral graph\ntheory, we need a not symmetric affinity which arises naturally from the\nunderlying arrow of time. We develop an anisotropic, directed diffusion\noperator corresponding to flow on a directed graph, from a directed affinity\nmatrix developed with coherence in mind, and corresponding spectral graph\ntheory from the graph Laplacian. Our methodology is not offered as more\naccurate than other traditional methods of finding coherent sets, but rather\nour approach works with alternative kinds of data sets, in the absence of\nvector field. Our examples will include partitioning the weather and cloud\nstructures of Jupiter, and a local to Potsdam, N.Y. lake-effect snow event on\nEarth, as well as the benchmark test double-gyre system.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 19:41:18 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 21:06:07 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 18:44:37 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["AlMomani", "Abd AlRahman", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1610.01945", "submitter": "David Pfau", "authors": "David Pfau and Oriol Vinyals", "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods", "comments": "Added comments on inverse reinforcement learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both generative adversarial networks (GAN) in unsupervised learning and\nactor-critic methods in reinforcement learning (RL) have gained a reputation\nfor being difficult to optimize. Practitioners in both fields have amassed a\nlarge number of strategies to mitigate these instabilities and improve\ntraining. Here we show that GANs can be viewed as actor-critic methods in an\nenvironment where the actor cannot affect the reward. We review the strategies\nfor stabilizing training for each class of models, both those that generalize\nbetween the two and those that are particular to that model. We also review a\nnumber of extensions to GANs and RL algorithms with even more complicated\ninformation flow. We hope that by highlighting this formal connection we will\nencourage both GAN and RL communities to develop general, scalable, and stable\nalgorithms for multilevel optimization with deep networks, and to draw\ninspiration across communities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:00:54 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:15:48 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Pfau", "David", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1610.01959", "submitter": "Panos P. Markopoulos", "authors": "Panos P. Markopoulos, Sandipan Kundu, Shubham Chamadia, Dimitris A.\n  Pados", "title": "Efficient L1-Norm Principal-Component Analysis via Bit Flipping", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2708023", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a\nreal-valued data matrix $\\mathbf X \\in \\mathbb R^{D \\times N}$ ($N$ data\nsamples of $D$ dimensions) can be exactly calculated with cost\n$\\mathcal{O}(2^{NK})$ or, when advantageous, $\\mathcal{O}(N^{dK - K + 1})$\nwhere $d=\\mathrm{rank}(\\mathbf X)$, $K<d$ [1],[2]. In applications where\n$\\mathbf X$ is large (e.g., \"big\" data of large $N$ and/or \"heavy\" data of\nlarge $d$), these costs are prohibitive. In this work, we present a novel\nsuboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\\mathbf X$\nof cost $\\mathcal O(ND \\mathrm{min} \\{ N,D\\} + N^2(K^4 + dK^2) + dNK^3)$, which\nis comparable to that of standard (L2-norm) PC analysis. Our theoretical and\nexperimental studies show that the proposed algorithm calculates the exact\noptimal L1-PCs with high frequency and achieves higher value in the L1-PC\noptimization metric than any known alternative algorithm of comparable\ncomputational cost. The superiority of the calculated L1-PCs over standard\nL2-PCs (singular vectors) in characterizing potentially faulty\ndata/measurements is demonstrated with experiments on data dimensionality\nreduction and disease diagnosis from genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:20:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Kundu", "Sandipan", ""], ["Chamadia", "Shubham", ""], ["Pados", "Dimitris A.", ""]]}, {"id": "1610.01989", "submitter": "Sakyasingha Dasgupta", "authors": "Sakyasingha Dasgupta and Takayuki Yoshizumi and Takayuki Osogami", "title": "Regularized Dynamic Boltzmann Machine with Delay Pruning for\n  Unsupervised Learning of Temporal Sequences", "comments": "6 pages, 5 figures, accepted full paper (oral presentation) at ICPR\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Delay Pruning, a simple yet powerful technique to regularize\ndynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a\nparticularly structured Boltzmann machine, as a generative model of a\nmulti-dimensional time-series. This Boltzmann machine can have infinitely many\nlayers of units but allows exact inference and learning based on its\nbiologically motivated structure. DyBM uses the idea of conduction delays in\nthe form of fixed length first-in first-out (FIFO) queues, with a neuron\nconnected to another via this FIFO queue, and spikes from a pre-synaptic neuron\ntravel along the queue to the post-synaptic neuron with a constant period of\ndelay. Here, we present Delay Pruning as a mechanism to prune the lengths of\nthe FIFO queues (making them zero) by setting some delay lengths to one with a\nfixed probability, and finally selecting the best performing model with fixed\ndelays. The uniqueness of structure and a non-sampling based learning rule in\nDyBM, make the application of previously proposed regularization techniques\nlike Dropout or DropConnect difficult, leading to poor generalization. First,\nwe evaluate the performance of Delay Pruning to let DyBM learn a\nmultidimensional temporal sequence generated by a Markov chain. Finally, we\nshow the effectiveness of delay pruning in learning high dimensional sequences\nusing the moving MNIST dataset, and compare it with Dropout and DropConnect\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 10:04:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Dasgupta", "Sakyasingha", ""], ["Yoshizumi", "Takayuki", ""], ["Osogami", "Takayuki", ""]]}, {"id": "1610.02122", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Significance testing in non-sparse high-dimensional linear models", "comments": "43 pages", "journal-ref": null, "doi": "10.1214/18-EJS1443", "report-no": null, "categories": "stat.ME math.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional linear models, the sparsity assumption is typically made,\nstating that most of the parameters are equal to zero. Under the sparsity\nassumption, estimation and, recently, inference have been well studied.\nHowever, in practice, sparsity assumption is not checkable and more importantly\nis often violated; a large number of covariates might be expected to be\nassociated with the response, indicating that possibly all, rather than just a\nfew, parameters are non-zero. A natural example is a genome-wide gene\nexpression profiling, where all genes are believed to affect a common disease\nmarker. We show that existing inferential methods are sensitive to the sparsity\nassumption, and may, in turn, result in the severe lack of control of Type-I\nerror. In this article, we propose a new inferential method, named CorrT, which\nis robust to model misspecification such as heteroscedasticity and lack of\nsparsity. CorrT is shown to have Type I error approaching the nominal level for\n\\textit{any} models and Type II error approaching zero for sparse and many\ndense models.\n  In fact, CorrT is also shown to be optimal in a variety of frameworks:\nsparse, non-sparse and hybrid models where sparse and dense signals are mixed.\nNumerical experiments show a favorable performance of the CorrT test compared\nto the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 02:17:00 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 01:35:02 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 06:08:42 GMT"}, {"version": "v4", "created": "Mon, 28 May 2018 01:31:25 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.02143", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Aryan Mokhtari, Xin Wang, Alejandro Ribeiro, and Georgios\n  B. Giannakis", "title": "Stochastic Averaging for Constrained Optimization with Application to\n  Online Resource Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2679690", "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to resource allocation for nowadays stochastic networks\nare challenged to meet fast convergence and tolerable delay requirements. The\npresent paper leverages online learning advances to facilitate stochastic\nresource allocation tasks. By recognizing the central role of Lagrange\nmultipliers, the underlying constrained optimization problem is formulated as a\nmachine learning task involving both training and operational modes, with the\ngoal of learning the sought multipliers in a fast and efficient manner. To this\nend, an order-optimal offline learning approach is developed first for batch\ntraining, and it is then generalized to the online setting with a procedure\ntermed learn-and-adapt. The novel resource allocation protocol permeates\nbenefits of stochastic approximation and statistical learning to obtain\nlow-complexity online updates with learning errors close to the statistical\naccuracy limits, while still preserving adaptation performance, which in the\nstochastic network optimization context guarantees queue stability. Analysis\nand simulated tests demonstrate that the proposed data-driven approach improves\nthe delay and convergence performance of existing resource allocation schemes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 05:11:23 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 06:31:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chen", "Tianyi", ""], ["Mokhtari", "Aryan", ""], ["Wang", "Xin", ""], ["Ribeiro", "Alejandro", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1610.02209", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a and Carlos Escolano", "title": "Morphology Generation for Statistical Machine Translation using Deep\n  Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphology in unbalanced languages remains a big challenge in the context of\nmachine translation. In this paper, we propose to de-couple machine translation\nfrom morphology generation in order to better deal with the problem. We\ninvestigate the morphology simplification with a reasonable trade-off between\nexpected gain and generation complexity. For the Chinese-Spanish task, optimum\nmorphological simplification is in gender and number. For this purpose, we\ndesign a new classification architecture which, compared to other standard\nmachine learning techniques, obtains the best results. This proposed\nneural-based architecture consists of several layers: an embedding, a\nconvolutional followed by a recurrent neural network and, finally, ends with\nsigmoid and softmax layers. We obtain classification results over 98% accuracy\nin gender classification, over 93% in number classification, and an overall\ntranslation improvement of 0.7 METEOR.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 09:59:13 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 15:15:40 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Escolano", "Carlos", ""]]}, {"id": "1610.02276", "submitter": "Ravi Kiran Raman", "authors": "Ravi Kiran Raman and Lav Varshney", "title": "Universal Clustering via Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider unsupervised clustering of objects drawn from a discrete set,\nthrough the use of human intelligence available in crowdsourcing platforms.\nThis paper defines and studies the problem of universal clustering using\nresponses of crowd workers, without knowledge of worker reliability or task\ndifficulty. We model stochastic worker response distributions by incorporating\ntraits of memory for similar objects and traits of distance among differing\nobjects. We are particularly interested in two limiting worker\ntypes---temporary workers who retain no memory of responses and long-term\nworkers with memory. We first define clustering algorithms for these limiting\ncases and then integrate them into an algorithm for the unified worker model.\nWe prove asymptotic consistency of the algorithms and establish sufficient\nconditions on the sample complexity of the algorithm. Converse arguments\nestablish necessary conditions on sample complexity, proving that the defined\nalgorithms are asymptotically order-optimal in cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 12:03:42 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Raman", "Ravi Kiran", ""], ["Varshney", "Lav", ""]]}, {"id": "1610.02287", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz and Michalis K. Titsias and David M. Blei", "title": "The Generalized Reparameterization Gradient", "comments": "16 pages, 15 figures, NIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reparameterization gradient has become a widely used method to obtain\nMonte Carlo gradients to optimize the variational objective. However, this\ntechnique does not easily apply to commonly used distributions such as beta or\ngamma without further approximations, and most practical applications of the\nreparameterization gradient fit Gaussian distributions. In this paper, we\nintroduce the generalized reparameterization gradient, a method that extends\nthe reparameterization gradient to a wider class of variational distributions.\nGeneralized reparameterizations use invertible transformations of the latent\nvariables which lead to transformed distributions that weakly depend on the\nvariational parameters. This results in new Monte Carlo gradients that combine\nreparameterization gradients and score function gradients. We demonstrate our\napproach on variational inference for two complex probabilistic models. The\ngeneralized reparameterization is effective: even a single sample from the\nvariational distribution is enough to obtain a low-variance gradient.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 13:55:35 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 19:10:51 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 18:23:25 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Titsias", "Michalis K.", ""], ["Blei", "David M.", ""]]}, {"id": "1610.02372", "submitter": "Adelchi Azzalini", "authors": "Adelchi Azzalini", "title": "Combining local and global smoothing in multivariate density estimation", "comments": "Two figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric estimation of a multivariate density estimation is tackled via\na method which combines traditional local smoothing with a form of global\nsmoothing but without imposing a rigid structure. Simulation work delivers\nencouraging indications on the effectiveness of the method. An application to\ndensity-based clustering illustrates a possible usage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 18:54:58 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Azzalini", "Adelchi", ""]]}, {"id": "1610.02490", "submitter": "Vineet Abhishek", "authors": "Vineet Abhishek and Shie Mannor", "title": "A nonparametric sequential test for online randomized experiments", "comments": "WWW '17 Companion Proceedings of the 26th International Conference on\n  World Wide Web Companion Pages 610-616", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric sequential test that aims to address two practical\nproblems pertinent to online randomized experiments: (i) how to do a hypothesis\ntest for complex metrics; (ii) how to prevent type $1$ error inflation under\ncontinuous monitoring. The proposed test does not require knowledge of the\nunderlying probability distribution generating the data. We use the bootstrap\nto estimate the likelihood for blocks of data followed by mixture sequential\nprobability ratio test. We validate this procedure on data from a major online\ne-commerce website. We show that the proposed test controls type $1$ error at\nany time, has good power, is robust to misspecification in the distribution\ngenerating the data, and allows quick inference in online randomized\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 06:13:50 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 07:09:25 GMT"}, {"version": "v3", "created": "Sun, 13 Nov 2016 20:08:38 GMT"}, {"version": "v4", "created": "Mon, 26 Jun 2017 20:34:33 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Abhishek", "Vineet", ""], ["Mannor", "Shie", ""]]}, {"id": "1610.02496", "submitter": "Kaiwei Li", "authors": "Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu", "title": "SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete\ncount data such as text and images. Applications require LDA to handle both\nlarge datasets and a large number of topics. Though distributed CPU systems\nhave been used, GPU-based systems have emerged as a promising alternative\nbecause of the high computational power and memory bandwidth of GPUs. However,\nexisting GPU-based LDA systems cannot support a large number of topics because\nthey use algorithms on dense data structures whose time and space complexity is\nlinear to the number of topics. In this paper, we propose SaberLDA, a GPU-based\nLDA system that implements a sparsity-aware algorithm to achieve sublinear time\ncomplexity and scales well to learn a large number of topics. To address the\nchallenges introduced by sparsity, we propose a novel data layout, a new\nwarp-based sampling kernel, and an efficient sparse count matrix updating\nalgorithm that improves locality, makes efficient utilization of GPU warps, and\nreduces memory consumption. Experiments show that SaberLDA can learn from\nbillions-token-scale data with up to 10,000 topics, which is almost two orders\nof magnitude larger than that of the previous GPU-based systems. With a single\nGPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of\ntokens in a few hours, which is only achievable with clusters with tens of\nmachines before.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 07:57:00 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 12:39:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Li", "Kaiwei", ""], ["Chen", "Jianfei", ""], ["Chen", "Wenguang", ""], ["Zhu", "Jun", ""]]}, {"id": "1610.02501", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, Wenyu Liu", "title": "Revisiting Multiple Instance Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.08.026", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently neural networks and multiple instance learning are both attractive\ntopics in Artificial Intelligence related research fields. Deep neural networks\nhave achieved great success in supervised learning problems, and multiple\ninstance learning as a typical weakly-supervised learning method is effective\nfor many applications in computer vision, biometrics, nature language\nprocessing, etc. In this paper, we revisit the problem of solving multiple\ninstance learning problems using neural networks. Neural networks are appealing\nfor solving multiple instance learning problem. The multiple instance neural\nnetworks perform multiple instance learning in an end-to-end way, which take a\nbag with various number of instances as input and directly output bag label.\nAll of the parameters in a multiple instance network are able to be optimized\nvia back-propagation. We propose a new multiple instance neural network to\nlearn bag representations, which is different from the existing multiple\ninstance neural networks that focus on estimating instance label. In addition,\nrecent tricks developed in deep learning have been studied in multiple instance\nnetworks, we find deep supervision is effective for boosting bag classification\naccuracy. In the experiments, the proposed multiple instance networks achieve\nstate-of-the-art or competitive performance on several MIL benchmarks.\nMoreover, it is extremely fast for both testing and training, e.g., it takes\nonly 0.0003 second to predict a bag and a few seconds to train on a MIL\ndatasets on a moderate CPU.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 08:57:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wang", "Xinggang", ""], ["Yan", "Yongluan", ""], ["Tang", "Peng", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""]]}, {"id": "1610.02581", "submitter": "Hongseok Namkoong", "authors": "John Duchi and Hongseok Namkoong", "title": "Variance-based regularization with convex objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach to risk minimization and stochastic optimization that\nprovides a convex surrogate for variance, allowing near-optimal and\ncomputationally efficient trading between approximation and estimation error.\nOur approach builds off of techniques for distributionally robust optimization\nand Owen's empirical likelihood, and we provide a number of finite-sample and\nasymptotic results characterizing the theoretical performance of the estimator.\nIn particular, we show that our procedure comes with certificates of\noptimality, achieving (in some scenarios) faster rates of convergence than\nempirical risk minimization by virtue of automatically balancing bias and\nvariance. We give corroborating empirical evidence showing that in practice,\nthe estimator indeed trades between variance and absolute performance on a\ntraining sample, improving out-of-sample (test) performance over standard\nempirical risk minimization for a number of classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 20:52:13 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 22:31:57 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 18:50:19 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Duchi", "John", ""], ["Namkoong", "Hongseok", ""]]}, {"id": "1610.02588", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shao Tang", "title": "Iterative proportional scaling revisited: a modern optimization\n  perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the classic iterative proportional scaling (IPS) from a\nmodern optimization perspective. In contrast to the criticisms made in the\nliterature, we show that based on a coordinate descent characterization, IPS\ncan be slightly modified to deliver coefficient estimates, and from a\nmajorization-minimization standpoint, IPS can be extended to handle log-affine\nmodels with features not necessarily binary-valued or nonnegative. Furthermore,\nsome state-of-the-art optimization techniques such as block-wise computation,\nrandomization and momentum-based acceleration can be employed to provide more\nscalable IPS algorithms, as well as some regularized variants of IPS for\nconcurrent feature selection.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 22:06:58 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 19:30:01 GMT"}, {"version": "v3", "created": "Sat, 5 May 2018 00:47:30 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 23:46:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["She", "Yiyuan", ""], ["Tang", "Shao", ""]]}, {"id": "1610.02590", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shao Tang, and Qiaoya Zhang", "title": "Indirect Gaussian Graph Learning beyond Gaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to capture dependency graph structures from real data\nwhich may not be Gaussian. Starting from marginal loss functions not\nnecessarily derived from probability distributions, we utilize an additive\nover-parametrization with shrinkage to incorporate variable dependencies into\nthe criterion. An iterative Gaussian graph learning algorithm is proposed with\nease in implementation. Statistical analysis shows that the estimators achieve\nsatisfactory accuracy with the error measured in terms of a proper Bregman\ndivergence. Real-life examples in different settings are given to demonstrate\nthe efficacy of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 22:22:37 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 23:33:56 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 03:17:33 GMT"}, {"version": "v4", "created": "Sat, 30 Nov 2019 22:21:07 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["She", "Yiyuan", ""], ["Tang", "Shao", ""], ["Zhang", "Qiaoya", ""]]}, {"id": "1610.02649", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Ali Reihanian, Daoqiang Zhang, Behrouz\n  Minaei-Bidgoli", "title": "A new selection strategy for selective cluster ensemble based on\n  Diversity and Independency", "comments": "Accepted in Engineering Applications of Artificial Intelligence\n  (EAAI) Journal", "journal-ref": null, "doi": "10.1016/j.engappai.2016.10.005", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research introduces a new strategy in cluster ensemble selection by\nusing Independency and Diversity metrics. In recent years, Diversity and\nQuality, which are two metrics in evaluation procedure, have been used for\nselecting basic clustering results in the cluster ensemble selection. Although\nquality can improve the final results in cluster ensemble, it cannot control\nthe procedures of generating basic results, which causes a gap in prediction of\nthe generated basic results' accuracy. Instead of quality, this paper\nintroduces Independency as a supplementary method to be used in conjunction\nwith Diversity. Therefore, this paper uses a heuristic metric, which is based\non the procedure of converting code to graph in Software Testing, in order to\ncalculate the Independency of two basic clustering algorithms. Moreover, a new\nmodeling language, which we called as \"Clustering Algorithms Independency\nLanguage\" (CAIL), is introduced in order to generate graphs which depict\nIndependency of algorithms. Also, Uniformity, which is a new similarity metric,\nhas been introduced for evaluating the diversity of basic results. As a\ncredential, our experimental results on varied different standard data sets\nshow that the proposed framework improves the accuracy of final results\ndramatically in comparison with other cluster ensemble methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 09:28:01 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Reihanian", "Ali", ""], ["Zhang", "Daoqiang", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1610.02703", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Nonparametric Bayesian inference of the microcanonical stochastic block\n  model", "comments": "24 pages, 9 figures, 1 table. Code is freely available as part of\n  graph-tool at https://graph-tool.skewed.de . See also the HOWTO at\n  https://graph-tool.skewed.de/static/doc/demos/inference/inference.html .\n  Minor typos fixed in most recent version", "journal-ref": "Phys. Rev. E 95, 012317 (2017)", "doi": "10.1103/PhysRevE.95.012317", "report-no": null, "categories": "physics.data-an physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A principled approach to characterize the hidden structure of networks is to\nformulate generative models, and then infer their parameters from data. When\nthe desired structure is composed of modules or \"communities\", a suitable\nchoice for this task is the stochastic block model (SBM), where nodes are\ndivided into groups, and the placement of edges is conditioned on the group\nmemberships. Here, we present a nonparametric Bayesian method to infer the\nmodular structure of empirical networks, including the number of modules and\ntheir hierarchical organization. We focus on a microcanonical variant of the\nSBM, where the structure is imposed via hard constraints, i.e. the generated\nnetworks are not allowed to violate the patterns imposed by the model. We show\nhow this simple model variation allows simultaneously for two important\nimprovements over more traditional inference approaches: 1. Deeper Bayesian\nhierarchies, with noninformative priors replaced by sequences of priors and\nhyperpriors, that not only remove limitations that seriously degrade the\ninference on large networks, but also reveal structures at multiple scales; 2.\nA very efficient inference algorithm that scales well not only for networks\nwith a large number of nodes and edges, but also with an unlimited number of\nmodules. We show also how this approach can be used to sample modular\nhierarchies from the posterior distribution, as well as to perform model\nselection. We discuss and analyze the differences between sampling from the\nposterior and simply finding the single parameter estimate that maximizes it.\nFurthermore, we expose a direct equivalence between our microcanonical approach\nand alternative derivations based on the canonical SBM.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 18:07:07 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 09:44:18 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 04:23:54 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 16:53:03 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1610.02746", "submitter": "Lei Wang", "authors": "Li Huang and Lei Wang", "title": "Accelerate Monte Carlo Simulations with Restricted Boltzmann Machines", "comments": null, "journal-ref": "Phys. Rev. B 95, 035105 (2017)", "doi": "10.1103/PhysRevB.95.035105", "report-no": null, "categories": "physics.comp-ph cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their exceptional flexibility and popularity, the Monte Carlo methods\noften suffer from slow mixing times for challenging statistical physics\nproblems. We present a general strategy to overcome this difficulty by adopting\nideas and techniques from the machine learning community. We fit the\nunnormalized probability of the physical model to a feedforward neural network\nand reinterpret the architecture as a restricted Boltzmann machine. Then,\nexploiting its feature detection ability, we utilize the restricted Boltzmann\nmachine for efficient Monte Carlo updates and to speed up the simulation of the\noriginal physical system. We implement these ideas for the Falicov-Kimball\nmodel and demonstrate improved acceptance ratio and autocorrelation time near\nthe phase transition point.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 01:01:32 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 14:27:17 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Huang", "Li", ""], ["Wang", "Lei", ""]]}, {"id": "1610.02757", "submitter": "Maxime Voisin", "authors": "Maxime Voisin, Leo Dreyfus-Schmidt, Pierre Gutierrez, Samuel Ronsin\n  and Marc Beillevaire", "title": "Dataiku's Solution to SPHERE's Activity Recognition Challenge", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our team won the second prize of the Safe Aging with SPHERE Challenge\norganized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of\nthe competition was to recognize activities performed by humans, using sensor\ndata. This paper presents our solution. It is based on a rich pre-processing\nand state of the art machine learning methods. From the raw train data, we\ngenerate a synthetic train set with the same statistical characteristics as the\ntest set. We then perform feature engineering. The machine learning modeling\npart is based on stacking weak learners through a grid searched XGBoost\nalgorithm. Finally, we use post-processing to smooth our predictions over time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 02:52:21 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Voisin", "Maxime", ""], ["Dreyfus-Schmidt", "Leo", ""], ["Gutierrez", "Pierre", ""], ["Ronsin", "Samuel", ""], ["Beillevaire", "Marc", ""]]}, {"id": "1610.02758", "submitter": "Feihu Huang", "authors": "Feihu Huang, Songcan Chen and Zhaosong Lu", "title": "Stochastic Alternating Direction Method of Multipliers with Variance\n  Reduction for Nonconvex Optimization", "comments": "34 pages, 4 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we study the stochastic alternating direction method of\nmultipliers (ADMM) for the nonconvex optimizations, and propose three classes\nof the nonconvex stochastic ADMM with variance reduction, based on different\nreduced variance stochastic gradients. Specifically, the first class called the\nnonconvex stochastic variance reduced gradient ADMM (SVRG-ADMM), uses a\nmulti-stage scheme to progressively reduce the variance of stochastic\ngradients. The second is the nonconvex stochastic average gradient ADMM\n(SAG-ADMM), which additionally uses the old gradients estimated in the previous\niteration. The third called SAGA-ADMM is an extension of the SAG-ADMM method.\nMoreover, under some mild conditions, we establish the iteration complexity\nbound of $O(1/\\epsilon)$ of the proposed methods to obtain an\n$\\epsilon$-stationary solution of the nonconvex optimizations. In particular,\nwe provide a general framework to analyze the iteration complexity of these\nnonconvex stochastic ADMM methods with variance reduction. Finally, some\nnumerical experiments demonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 02:54:43 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 02:14:41 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 03:33:21 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 12:45:34 GMT"}, {"version": "v5", "created": "Wed, 26 Jul 2017 07:55:35 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Huang", "Feihu", ""], ["Chen", "Songcan", ""], ["Lu", "Zhaosong", ""]]}, {"id": "1610.02807", "submitter": "Qian Wan", "authors": "Qian Wan, Huiping Duan, Jun Fang, Hongbin Li", "title": "Robust Bayesian Compressed sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust compressed sensing whose objective is to\nrecover a high-dimensional sparse signal from compressed measurements corrupted\nby outliers. A new sparse Bayesian learning method is developed for robust\ncompressed sensing. The basic idea of the proposed method is to identify and\nremove the outliers from sparse signal recovery. To automatically identify the\noutliers, we employ a set of binary indicator hyperparameters to indicate which\nobservations are outliers. These indicator hyperparameters are treated as\nrandom variables and assigned a beta process prior such that their values are\nconfined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed\non the sparse signal to promote sparsity. Based on this hierarchical prior\nmodel, we develop a variational Bayesian method to estimate the indicator\nhyperparameters as well as the sparse signal. Simulation results show that the\nproposed method achieves a substantial performance improvement over existing\nrobust compressed sensing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 08:53:01 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 08:13:25 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Wan", "Qian", ""], ["Duan", "Huiping", ""], ["Fang", "Jun", ""], ["Li", "Hongbin", ""]]}, {"id": "1610.02918", "submitter": "Thibault Lesieur", "authors": "Thibault Lesieur, Caterina De Bacco, Jess Banks, Florent Krzakala,\n  Cris Moore and Lenka Zdeborov\\'a", "title": "Phase transitions and optimal algorithms in high-dimensional Gaussian\n  mixture clustering", "comments": "8 pages, 3 figures, conference", "journal-ref": "2016 54th Annual Allerton Conference on Communication, Control,\n  and Computing (Allerton), Pages: 601 - 608", "doi": "10.1109/ALLERTON.2016.7852287", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Gaussian mixture clustering in the\nhigh-dimensional limit where the data consists of $m$ points in $n$ dimensions,\n$n,m \\rightarrow \\infty$ and $\\alpha = m/n$ stays finite. Using exact but\nnon-rigorous methods from statistical physics, we determine the critical value\nof $\\alpha$ and the distance between the clusters at which it becomes\ninformation-theoretically possible to reconstruct the membership into clusters\nbetter than chance. We also determine the accuracy achievable by the\nBayes-optimal estimation algorithm. In particular, we find that when the number\nof clusters is sufficiently large, $r > 4 + 2 \\sqrt{\\alpha}$, there is a gap\nbetween the threshold for information-theoretically optimal performance and the\nthreshold at which known algorithms succeed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 13:57:00 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Lesieur", "Thibault", ""], ["De Bacco", "Caterina", ""], ["Banks", "Jess", ""], ["Krzakala", "Florent", ""], ["Moore", "Cris", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1610.02920", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, Yutaka\n  Matsuo", "title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective", "comments": "Add contents especially theoretical things for ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 14:02:30 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 15:22:28 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Sato", "Issei", ""], ["Suzuki", "Masahiro", ""], ["Nakayama", "Kotaro", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1610.02962", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas and C\\'edric Herzet", "title": "Low-Rank Dynamic Mode Decomposition: Optimal Solution in Polynomial-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the linear approximation of high-dimensional dynamical\nsystems using low-rank dynamic mode decomposition (DMD). Searching this\napproximation in a data-driven approach is formalised as attempting to solve a\nlow-rank constrained optimisation problem. This problem is non-convex and\nstate-of-the-art algorithms are all sub-optimal. This paper shows that there\nexists a closed-form solution, which is computed in polynomial time, and\ncharacterises the l2-norm of the optimal approximation error. The paper also\nproposes low-complexity algorithms building reduced models from this optimal\nsolution, based on singular value decomposition or eigen value decomposition.\nThe algorithms are evaluated by numerical simulations using synthetic and\nphysical data benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 15:29:12 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 16:19:42 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 12:15:56 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 14:36:29 GMT"}, {"version": "v5", "created": "Thu, 17 May 2018 13:12:09 GMT"}, {"version": "v6", "created": "Fri, 21 Dec 2018 10:58:47 GMT"}, {"version": "v7", "created": "Fri, 21 Feb 2020 14:03:19 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1610.02967", "submitter": "Soeren Laue", "authors": "Joachim Giesen and S\\\"oren Laue", "title": "Distributed Convex Optimization with Many Convex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of solving convex optimization problems with many\nconvex constraints in a distributed setting. Our approach is based on an\nextension of the alternating direction method of multipliers (ADMM) that\nrecently gained a lot of attention in the Big Data context. Although it has\nbeen invented decades ago, ADMM so far can be applied only to unconstrained\nproblems and problems with linear equality or inequality constraints. Our\nextension can handle arbitrary inequality constraints directly. It combines the\nability of ADMM to solve convex optimization problems in a distributed setting\nwith the ability of the Augmented Lagrangian method to solve constrained\noptimization problems, and as we show, it inherits the convergence guarantees\nof ADMM and the Augmented Lagrangian method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:59:46 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 10:52:50 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Giesen", "Joachim", ""], ["Laue", "S\u00f6ren", ""]]}, {"id": "1610.02987", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Linear Hypothesis Testing in Dense High-Dimensional Linear Models", "comments": "42 pages, 8 figures", "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2017.1356319", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for testing linear hypothesis in high-dimensional\nlinear models. The proposed test does not impose any restriction on the size of\nthe model, i.e. model sparsity or the loading vector representing the\nhypothesis. Providing asymptotically valid methods for testing general linear\nfunctions of the regression parameters in high-dimensions is extremely\nchallenging -- especially without making restrictive or unverifiable\nassumptions on the number of non-zero elements. We propose to test the moment\nconditions related to the newly designed restructured regression, where the\ninputs are transformed and augmented features. These new features incorporate\nthe structure of the null hypothesis directly. The test statistics are\nconstructed in such a way that lack of sparsity in the original model parameter\ndoes not present a problem for the theoretical justification of our procedures.\nWe establish asymptotically exact control on Type I error without imposing any\nsparsity assumptions on model parameter or the vector representing the linear\nhypothesis. Our method is also shown to achieve certain optimality in detecting\ndeviations from the null hypothesis. We demonstrate the favorable finite-sample\nperformance of the proposed methods, via a number of numerical and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 16:30:27 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 01:10:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.03035", "submitter": "William Chan", "authors": "William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly", "title": "Latent Sequence Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes\nsequences with variable lengthed output units as a function of both the input\nsequence and the output sequence. We present a training algorithm which samples\nvalid extensions and an approximate decoding algorithm. We experiment with the\nWall Street Journal speech recognition task. Our LSD model achieves 12.9% WER\ncompared to a character baseline of 14.8% WER. When combined with a\nconvolutional network on the encoder, we achieve 9.6% WER.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:16:08 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 20:11:21 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 17:23:55 GMT"}, {"version": "v4", "created": "Wed, 30 Nov 2016 19:14:17 GMT"}, {"version": "v5", "created": "Thu, 19 Jan 2017 22:23:44 GMT"}, {"version": "v6", "created": "Tue, 7 Feb 2017 15:52:27 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chan", "William", ""], ["Zhang", "Yu", ""], ["Le", "Quoc", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1610.03045", "submitter": "Jialei Wang", "authors": "Jialei Wang, Jason D. Lee, Mehrdad Mahdavi, Mladen Kolar, Nathan\n  Srebro", "title": "Sketching Meets Random Projection in the Dual: A Provable Recovery\n  Algorithm for Big and High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching techniques have become popular for scaling up machine learning\nalgorithms by reducing the sample size or dimensionality of massive data sets,\nwhile still maintaining the statistical power of big data. In this paper, we\nstudy sketching from an optimization point of view: we first show that the\niterative Hessian sketch is an optimization process with preconditioning, and\ndevelop accelerated iterative Hessian sketch via the searching the conjugate\ndirection; we then establish primal-dual connections between the Hessian sketch\nand dual random projection, and apply the preconditioned conjugate gradient\napproach on the dual problem, which leads to the accelerated iterative dual\nrandom projection methods. Finally to tackle the challenges from both large\nsample size and high-dimensionality, we propose the primal-dual sketch, which\niteratively sketches the primal and dual formulations. We show that using a\nlogarithmic number of calls to solvers of small scale problem, primal-dual\nsketch is able to recover the optimum of the original problem up to arbitrary\nprecision. The proposed algorithms are validated via extensive experiments on\nsynthetic and real data sets which complements our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:48:34 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wang", "Jialei", ""], ["Lee", "Jason D.", ""], ["Mahdavi", "Mehrdad", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1610.03113", "submitter": "J\\\"org L\\\"ucke", "authors": "J\\\"org L\\\"ucke", "title": "Truncated Variational Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a novel variational expectation maximization approach based on\ntruncated posterior distributions. Truncated distributions are proportional to\nexact posteriors within subsets of a discrete state space and equal zero\notherwise. The treatment of the distributions' subsets as variational\nparameters distinguishes the approach from previous variational approaches. The\nspecific structure of truncated distributions allows for deriving novel and\nmathematically grounded results, which in turn can be used to formulate novel\nefficient algorithms to optimize the parameters of probabilistic generative\nmodels. Most centrally, we find the variational lower bounds that correspond to\ntruncated distributions to be given by very concise and efficiently computable\nexpressions, while update equations for model parameters remain in their\nstandard form. Based on these findings, we show how efficient and easily\napplicable meta-algorithms can be formulated that guarantee a monotonic\nincrease of the variational bound. Example applications of the here derived\nframework provide novel theoretical results and learning procedures for latent\nvariable models as well as mixture models. Furthermore, we show that truncated\nvariation EM naturally interpolates between standard EM with full posteriors\nand EM based on the maximum a-posteriori state (MAP). The approach can,\ntherefore, be regarded as a generalization of the popular `hard EM' approach\ntowards a similarly efficient method which can capture more of the true\nposterior structure.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 22:11:43 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 22:12:09 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 11:12:07 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1610.03263", "submitter": "Patrick Bl\\\"obaum", "authors": "Patrick Bl\\\"obaum, Takashi Washio, Shohei Shimizu", "title": "Error Asymmetry in Causal and Anticausal Regression", "comments": null, "journal-ref": "Behaviormetrika, 2017, 10.1007/s41237-017-0022-z", "doi": "10.1007/s41237-017-0022-z", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally difficult to make any statements about the expected\nprediction error in an univariate setting without further knowledge about how\nthe data were generated. Recent work showed that knowledge about the real\nunderlying causal structure of a data generation process has implications for\nvarious machine learning settings. Assuming an additive noise and an\nindependence between data generating mechanism and its input, we draw a novel\nconnection between the intrinsic causal relationship of two variables and the\nexpected prediction error. We formulate the theorem that the expected error of\nthe true data generating function as prediction model is generally smaller when\nthe effect is predicted from its cause and, on the contrary, greater when the\ncause is predicted from its effect. The theorem implies an asymmetry in the\nerror depending on the prediction direction. This is further corroborated with\nempirical evaluations in artificial and real-world data sets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 10:15:15 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 12:25:44 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Bl\u00f6baum", "Patrick", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1610.03276", "submitter": "Manuel Morante", "authors": "Manuel Morante Moreno, Yannis Kopsinis, Eleftherios Kofidis, Christos\n  Chatzichristos and Sergios Theodoridis", "title": "Assisted Dictionary Learning for fMRI Data Analysis", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting information from functional magnetic resonance (fMRI) images has\nbeen a major area of research for more than two decades. The goal of this work\nis to present a new method for the analysis of fMRI data sets, that is capable\nto incorporate a priori available information, via an efficient optimization\nframework. Tests on synthetic data sets demonstrate significant performance\ngains over existing methods of this kind.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 11:06:28 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Moreno", "Manuel Morante", ""], ["Kopsinis", "Yannis", ""], ["Kofidis", "Eleftherios", ""], ["Chatzichristos", "Christos", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1610.03295", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua", "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a multi-agent setting where the host vehicle must apply\nsophisticated negotiation skills with other road users when overtaking, giving\nway, merging, taking left and right turns and while pushing ahead in\nunstructured urban roadways. Since there are many possible scenarios, manually\ntackling all possible cases will likely yield a too simplistic policy.\nMoreover, one must balance between unexpected behavior of other\ndrivers/pedestrians and at the same time not to be too defensive so that normal\ntraffic flow is maintained.\n  In this paper we apply deep reinforcement learning to the problem of forming\nlong term driving strategies. We note that there are two major challenges that\nmake autonomous driving different from other robotic tasks. First, is the\nnecessity for ensuring functional safety - something that machine learning has\ndifficulty with given that performance is optimized at the level of an\nexpectation over many instances. Second, the Markov Decision Process model\noften used in robotics is problematic in our case because of unpredictable\nbehavior of other agents in this multi-agent scenario. We make three\ncontributions in our work. First, we show how policy gradient iterations can be\nused without Markovian assumptions. Second, we decompose the problem into a\ncomposition of a Policy for Desires (which is to be learned) and trajectory\nplanning with hard constraints (which is not learned). The goal of Desires is\nto enable comfort of driving, while hard constraints guarantees the safety of\ndriving. Third, we introduce a hierarchical temporal abstraction we call an\n\"Option Graph\" with a gating mechanism that significantly reduces the effective\nhorizon and thereby reducing the variance of the gradient estimation even\nfurther.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 12:09:03 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shammah", "Shaked", ""], ["Shashua", "Amnon", ""]]}, {"id": "1610.03378", "submitter": "Alvaro Sanchez-Gonzalez", "authors": "A. Sanchez-Gonzalez, P. Micaelli, C. Olivier, T. R. Barillot, M.\n  Ilchen, A. A. Lutman, A. Marinelli, T. Maxwell, A. Achner, M. Ag{\\aa}ker, N.\n  Berrah, C. Bostedt, J. Buck, P. H. Bucksbaum, S. Carron Montero, B. Cooper,\n  J. P. Cryan, M. Dong, R. Feifel, L. J. Frasinski, H. Fukuzawa, A. Galler, G.\n  Hartmann, N. Hartmann, W. Helml, A. S. Johnson, A. Knie, A. O. Lindahl, J.\n  Liu, K. Motomura, M. Mucke, C. O'Grady, J-E. Rubensson, E. R. Simpson, R. J.\n  Squibb, C. S{\\aa}the, K. Ueda, M. Vacher, D. J. Walke, V. Zhaunerchyk, R. N.\n  Coffee and J. P. Marangos", "title": "Machine learning applied to single-shot x-ray diagnostics in an XFEL", "comments": "12 pages, 8 figures", "journal-ref": "Nature Communications 8, 15461 (2017)", "doi": "10.1038/ncomms15461", "report-no": null, "categories": "physics.data-an physics.acc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray free-electron lasers (XFELs) are the only sources currently able to\nproduce bright few-fs pulses with tunable photon energies from 100 eV to more\nthan 10 keV. Due to the stochastic SASE operating principles and other\ntechnical issues the output pulses are subject to large fluctuations, making it\nnecessary to characterize the x-ray pulses on every shot for data sorting\npurposes. We present a technique that applies machine learning tools to predict\nx-ray pulse properties using simple electron beam and x-ray parameters as\ninput. Using this technique at the Linac Coherent Light Source (LCLS), we\nreport mean errors below 0.3 eV for the prediction of the photon energy at 530\neV and below 1.6 fs for the prediction of the delay between two x-ray pulses.\nWe also demonstrate spectral shape prediction with a mean agreement of 97%.\nThis approach could potentially be used at the next generation of\nhigh-repetition-rate XFELs to provide accurate knowledge of complex x-ray\npulses at the full repetition rate.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:53:33 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Sanchez-Gonzalez", "A.", ""], ["Micaelli", "P.", ""], ["Olivier", "C.", ""], ["Barillot", "T. R.", ""], ["Ilchen", "M.", ""], ["Lutman", "A. A.", ""], ["Marinelli", "A.", ""], ["Maxwell", "T.", ""], ["Achner", "A.", ""], ["Ag\u00e5ker", "M.", ""], ["Berrah", "N.", ""], ["Bostedt", "C.", ""], ["Buck", "J.", ""], ["Bucksbaum", "P. H.", ""], ["Montero", "S. Carron", ""], ["Cooper", "B.", ""], ["Cryan", "J. P.", ""], ["Dong", "M.", ""], ["Feifel", "R.", ""], ["Frasinski", "L. J.", ""], ["Fukuzawa", "H.", ""], ["Galler", "A.", ""], ["Hartmann", "G.", ""], ["Hartmann", "N.", ""], ["Helml", "W.", ""], ["Johnson", "A. S.", ""], ["Knie", "A.", ""], ["Lindahl", "A. O.", ""], ["Liu", "J.", ""], ["Motomura", "K.", ""], ["Mucke", "M.", ""], ["O'Grady", "C.", ""], ["Rubensson", "J-E.", ""], ["Simpson", "E. R.", ""], ["Squibb", "R. J.", ""], ["S\u00e5the", "C.", ""], ["Ueda", "K.", ""], ["Vacher", "M.", ""], ["Walke", "D. J.", ""], ["Zhaunerchyk", "V.", ""], ["Coffee", "R. N.", ""], ["Marangos", "J. P.", ""]]}, {"id": "1610.03414", "submitter": "Jason Sakellariou", "authors": "Jason Sakellariou, Francesca Tria, Vittorio Loreto, Fran\\c{c}ois\n  Pachet", "title": "Maximum entropy models capture melodic styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Maximum Entropy model able to capture the statistics of\nmelodies in music. The model can be used to generate new melodies that emulate\nthe style of the musical corpus which was used to train it. Instead of using\nthe $n-$body interactions of $(n-1)-$order Markov models, traditionally used in\nautomatic music generation, we use a $k-$nearest neighbour model with pairwise\ninteractions only. In that way, we keep the number of parameters low and avoid\nover-fitting problems typical of Markov models. We show that long-range musical\nphrases don't need to be explicitly enforced using high-order Markov\ninteractions, but can instead emerge from multiple, competing, pairwise\ninteractions. We validate our Maximum Entropy model by contrasting how much the\ngenerated sequences capture the style of the original corpus without\nplagiarizing it. To this end we use a data-compression approach to discriminate\nthe levels of borrowing and innovation featured by the artificial sequences.\nThe results show that our modelling scheme outperforms both fixed-order and\nvariable-order Markov models. This shows that, despite being based only on\npairwise interactions, this Maximum Entropy scheme opens the possibility to\ngenerate musically sensible alterations of the original phrases, providing a\nway to generate innovation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 16:23:45 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sakellariou", "Jason", ""], ["Tria", "Francesca", ""], ["Loreto", "Vittorio", ""], ["Pachet", "Fran\u00e7ois", ""]]}, {"id": "1610.03425", "submitter": "Hongseok Namkoong", "authors": "John Duchi, Peter Glynn, Hongseok Namkoong", "title": "Statistics of Robust Optimization: A Generalized Empirical Likelihood\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical inference and distributionally robust solution methods\nfor stochastic optimization problems, focusing on confidence intervals for\noptimal values and solutions that achieve exact coverage asymptotically. We\ndevelop a generalized empirical likelihood framework---based on distributional\nuncertainty sets constructed from nonparametric $f$-divergence balls---for\nHadamard differentiable functionals, and in particular, stochastic optimization\nproblems. As consequences of this theory, we provide a principled method for\nchoosing the size of distributional uncertainty regions to provide one- and\ntwo-sided confidence intervals that achieve exact coverage. We also give an\nasymptotic expansion for our distributionally robust formulation, showing how\nrobustification regularizes problems by their variance. Finally, we show that\noptimizers of the distributionally robust formulations we study enjoy\n(essentially) the same consistency properties as those in classical sample\naverage approximations. Our general approach applies to quickly mixing\nstationary sequences, including geometrically ergodic Harris recurrent Markov\nchains.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 17:01:15 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 22:48:01 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 10:03:03 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Duchi", "John", ""], ["Glynn", "Peter", ""], ["Namkoong", "Hongseok", ""]]}, {"id": "1610.03483", "submitter": "Balaji Lakshminarayanan", "authors": "Shakir Mohamed and Balaji Lakshminarayanan", "title": "Learning in Implicit Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 19:59:39 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 15:11:20 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 17:44:52 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 05:47:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1610.03595", "submitter": "David Hong", "authors": "David Hong, Laura Balzano, Jeffrey A. Fessler", "title": "Towards a Theoretical Analysis of PCA for Heteroscedastic Data", "comments": "Presented at 54th Annual Allerton Conference on Communication,\n  Control, and Computing (Allerton)", "journal-ref": null, "doi": "10.1109/ALLERTON.2016.7852272", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a method for estimating a subspace\ngiven noisy samples. It is useful in a variety of problems ranging from\ndimensionality reduction to anomaly detection and the visualization of high\ndimensional data. PCA performs well in the presence of moderate noise and even\nwith missing data, but is also sensitive to outliers. PCA is also known to have\na phase transition when noise is independent and identically distributed;\nrecovery of the subspace sharply declines at a threshold noise variance.\nEffective use of PCA requires a rigorous understanding of these behaviors. This\npaper provides a step towards an analysis of PCA for samples with\nheteroscedastic noise, that is, samples that have non-uniform noise variances\nand so are no longer identically distributed. In particular, we provide a\nsimple asymptotic prediction of the recovery of a one-dimensional subspace from\nnoisy heteroscedastic samples. The prediction enables: a) easy and efficient\ncalculation of the asymptotic performance, and b) qualitative reasoning to\nunderstand how PCA is impacted by heteroscedasticity (such as outliers).\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 04:13:03 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hong", "David", ""], ["Balzano", "Laura", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1610.03713", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Optimistic Semi-supervised Least Squares Classification", "comments": "6 pages, 6 figures. International Conference on Pattern Recognition\n  (ICPR) 2016, Cancun, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of semi-supervised learning is to improve supervised classifiers by\nusing additional unlabeled training examples. In this work we study a simple\nself-learning approach to semi-supervised learning applied to the least squares\nclassifier. We show that a soft-label and a hard-label variant of self-learning\ncan be derived by applying block coordinate descent to two related but slightly\ndifferent objective functions. The resulting soft-label approach is related to\nan idea about dealing with missing data that dates back to the 1930s. We show\nthat the soft-label variant typically outperforms the hard-label variant on\nbenchmark datasets and partially explain this behaviour by studying the\nrelative difficulty of finding good local minima for the corresponding\nobjective functions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 13:52:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1610.03724", "submitter": "Lucas Fievet", "authors": "Lucas Fievet and Didier Sornette", "title": "Decision trees unearth return sign correlation in the S&P 500", "comments": "32 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technical trading rules and linear regressive models are often used by\npractitioners to find trends in financial data. However, these models are\nunsuited to find non-linearly separable patterns. We propose a decision tree\nforecasting model that has the flexibility to capture arbitrary patterns. To\nillustrate, we construct a binary Markov process with a deterministic component\nthat cannot be predicted with an autoregressive process. A simulation study\nconfirms the robustness of the trees and limitation of the autoregressive\nmodel. Finally, adjusting for multiple testing, we show that some tree based\nstrategies achieve trading performance significant at the 99% confidence level\non the S&P 500 over the past 20 years. The best strategy breaks even with the\nbuy-and-hold strategy at 21 bps in transaction costs per round trip. A\nfour-factor regression analysis shows significant intercept and correlation\nwith the market. The return anomalies are strongest during the bursts of the\ndotcom bubble, financial crisis, and European debt crisis. The correlation of\nthe return signs during these periods confirms the theoretical model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:22:05 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 17:16:19 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fievet", "Lucas", ""], ["Sornette", "Didier", ""]]}, {"id": "1610.03725", "submitter": "Makoto Yamada", "authors": "Makoto Yamada and Yuta Umezu and Kenji Fukumizu and Ichiro Takeuchi", "title": "Post Selection Inference with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel kernel based post selection inference (PSI) algorithm,\nwhich can not only handle non-linearity in data but also structured output such\nas multi-dimensional and multi-label outputs. Specifically, we develop a PSI\nalgorithm for independence measures, and propose the Hilbert-Schmidt\nIndependence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the\nproposed algorithm is that it can handle non-linearity and/or structured data\nthrough kernels. Namely, the proposed algorithm can be used for wider range of\napplications including nonlinear multi-class classification and multi-variate\nregressions, while existing PSI algorithms cannot handle them. Through\nsynthetic experiments, we show that the proposed approach can find a set of\nstatistically significant features for both regression and classification\nproblems. Moreover, we apply the hsicInf algorithm to a real-world data, and\nshow that hsicInf can successfully identify important features.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:23:09 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 00:34:11 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Yamada", "Makoto", ""], ["Umezu", "Yuta", ""], ["Fukumizu", "Kenji", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1610.03738", "submitter": "Daniel Wesierski", "authors": "Daniel Wesierski", "title": "Exploring the Entire Regularization Path for the Asymmetric Cost Linear\n  Support Vector Machine", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for exploring the entire regularization path of\nasymmetric-cost linear support vector machines. Empirical evidence suggests the\npredictive power of support vector machines depends on the regularization\nparameters of the training algorithms. The algorithms exploring the entire\nregularization paths have been proposed for single-cost support vector machines\nthereby providing the complete knowledge on the behavior of the trained model\nover the hyperparameter space. Considering the problem in two-dimensional\nhyperparameter space though enables our algorithm to maintain greater\nflexibility in dealing with special cases and sheds light on problems\nencountered by algorithms building the paths in one-dimensional spaces. We\ndemonstrate two-dimensional regularization paths for linear support vector\nmachines that we train on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:57:10 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Wesierski", "Daniel", ""]]}, {"id": "1610.03761", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Babak Taati", "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble\n  of Autoencoders", "comments": "25 pages, 6 figures, 4 Tables", "journal-ref": "Expert Systems with Applications, Volume 87, 30 November 2017,\n  Pages 280-290", "doi": "10.1016/j.eswa.2017.06.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fall is an abnormal activity that occurs rarely, so it is hard to collect\nreal data for falls. It is, therefore, difficult to use supervised learning\nmethods to automatically detect falls. Another challenge in using machine\nlearning methods to automatically detect falls is the choice of engineered\nfeatures. In this paper, we propose to use an ensemble of autoencoders to\nextract features from different channels of wearable sensor data trained only\non normal activities. We show that the traditional approach of choosing a\nthreshold as the maximum of the reconstruction error on the training normal\ndata is not the right way to identify unseen falls. We propose two methods for\nautomatic tightening of reconstruction error from only the normal activities\nfor better identification of unseen falls. We present our results on two\nactivity recognition datasets and show the efficacy of our proposed method\nagainst traditional autoencoder models and two standard one-class\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:55:06 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:00:04 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 20:51:59 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Taati", "Babak", ""]]}, {"id": "1610.03774", "submitter": "Rahul Kidambi", "authors": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli,\n  Aaron Sidford", "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression:\n  mini-batching, averaging, and model misspecification", "comments": "39 pages. Published in the Journal of Machine Learning Research\n  (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work characterizes the benefits of averaging schemes widely used in\nconjunction with stochastic gradient descent (SGD). In particular, this work\nprovides a sharp analysis of: (1) mini-batching, a method of averaging many\nsamples of a stochastic gradient to both reduce the variance of the stochastic\ngradient estimate and for parallelizing SGD and (2) tail-averaging, a method\ninvolving averaging the final few iterates of SGD to decrease the variance in\nSGD's final iterate. This work presents non-asymptotic excess risk bounds for\nthese schemes for the stochastic approximation problem of least squares\nregression.\n  Furthermore, this work establishes a precise problem-dependent extent to\nwhich mini-batch SGD yields provable near-linear parallelization speedups over\nSGD with batch size one. This allows for understanding learning rate versus\nbatch size tradeoffs for the final iterate of an SGD method. These results are\nthen utilized in providing a highly parallelizable SGD method that obtains the\nminimax risk with nearly the same number of serial updates as batch gradient\ndescent, improving significantly over existing SGD methods. A non-asymptotic\nanalysis of communication efficient parallelization schemes such as\nmodel-averaging/parameter mixing methods is then provided.\n  Finally, this work sheds light on some fundamental differences in SGD's\nbehavior when dealing with agnostic noise in the (non-realizable) least squares\nregression problem. In particular, the work shows that the stepsizes that\nensure minimax risk for the agnostic case must be a function of the noise\nproperties.\n  This paper builds on the operator view of analyzing SGD methods, introduced\nby Defossez and Bach (2015), followed by developing a novel analysis in\nbounding these operators to characterize the excess risk. These techniques are\nof broader interest in analyzing computational aspects of stochastic\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:30:11 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:09:30 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 16:20:07 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 17:50:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jain", "Prateek", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1610.03899", "submitter": "Michael Rabadi", "authors": "Michael Rabadi", "title": "Generalization bound for kernel similarity learning", "comments": "9 pages", "journal-ref": "In NIPS 2016 Brains and Bits Workshop. Barcelona, Spain", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity learning has received a large amount of interest and is an\nimportant tool for many scientific and industrial applications. In this\nframework, we wish to infer the distance (similarity) between points with\nrespect to an arbitrary distance function $d$. Here, we formulate the problem\nas a regression from a feature space $\\mathcal{X}$ to an arbitrary vector space\n$\\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then\ngive Rademacher complexity bounds on the generalization error. We find that\nwith high probability, the complexity is bounded by the maximum of the radius\nof $\\mathcal{X}$ and the radius of $\\mathcal{Y}$.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 23:35:45 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 21:39:17 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Rabadi", "Michael", ""]]}, {"id": "1610.03927", "submitter": "Yen-Chi Chen", "authors": "Yunhua Xiang, Yen-Chi Chen", "title": "Statistical Inference Using Mean Shift Denoising", "comments": "19 page, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how the mean shift algorithm can be used to denoise a\ndataset. We introduce a new framework to analyze the mean shift algorithm as a\ndenoising approach by viewing the algorithm as an operator on a distribution\nfunction. We investigate how the mean shift algorithm changes the distribution\nand show that data points shifted by the mean shift concentrate around high\ndensity regions of the underlying density function. By using the mean shift as\na denoising method, we enhance the performance of several clustering\ntechniques, improve the power of two-sample tests, and obtain a new method for\nanomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 03:36:55 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Xiang", "Yunhua", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "1610.03934", "submitter": "Krupakar Hans", "authors": "Hans Krupakar, Keerthika Rajvel, Bharathi B, Angel Deborah S,\n  Vallidevi Krishnamurthy", "title": "A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder", "comments": "(8 pages, 7 figures, IEEE Digital Xplore paper)", "journal-ref": "2016 International Conference on Information Communication and\n  Embedded Systems (ICICES), Chennai, 2016, pp. 1-9", "doi": "10.1109/ICICES.2016.7518940", "report-no": null, "categories": "cs.CL cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech Translation has always been about giving source text or audio input\nand waiting for system to give translated output in desired form. In this\npaper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice\near-piece translation device. We introduce and survey the recent advances made\nin the field of Speech Engineering, to employ in the ADD, particularly focusing\non the three major processing steps of Recognition, Translation and Synthesis.\nWe tackle the problem of machine understanding of natural language by designing\na recognition unit for source audio to text, a translation unit for source\nlanguage text to target language text, and a synthesis unit for target language\ntext to target language speech. Speech from the surroundings will be recorded\nby the recognition unit present on the ear-piece and translation will start as\nsoon as one sentence is successfully read. This way, we hope to give translated\noutput as and when input is being read. The recognition unit will use Hidden\nMarkov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory\ncells, and the synthesis unit, HMM based speech synthesis system HTS. This\nsystem will initially be built as an English to Tamil translation device.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 04:10:58 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Krupakar", "Hans", ""], ["Rajvel", "Keerthika", ""], ["B", "Bharathi", ""], ["S", "Angel Deborah", ""], ["Krishnamurthy", "Vallidevi", ""]]}, {"id": "1610.03988", "submitter": "Chin-Cheng Hsu", "authors": "Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang", "title": "Dictionary Update for NMF-based Voice Conversion Using an\n  Encoder-Decoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a dictionary update method for Nonnegative Matrix\nFactorization (NMF) with high dimensional data in a spectral conversion (SC)\ntask. Voice conversion has been widely studied due to its potential\napplications such as personalized speech synthesis and speech enhancement.\nExemplar-based NMF (ENMF) emerges as an effective and probably the simplest\nchoice among all techniques for SC, as long as a source-target parallel speech\ncorpus is given. ENMF-based SC systems usually need a large amount of bases\n(exemplars) to ensure the quality of the converted speech. However, a small and\neffective dictionary is desirable but hard to obtain via dictionary update, in\nparticular when high-dimensional features such as STRAIGHT spectra are used.\nTherefore, we propose a dictionary update framework for NMF by means of an\nencoder-decoder reformulation. Regarding NMF as an encoder-decoder network\nmakes it possible to exploit the whole parallel corpus more effectively and\nefficiently when applied to SC. Our experiments demonstrate significant gains\nof the proposed system with small dictionaries over conventional ENMF-based\nsystems with dictionaries of same or much larger size.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:18:53 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hsu", "Chin-Cheng", ""], ["Hwang", "Hsin-Te", ""], ["Wu", "Yi-Chiao", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1610.03995", "submitter": "Adrian  Calma", "authors": "Tobias Reitmaier and Adrian Calma and Bernhard Sick", "title": "Semi-Supervised Active Learning for Support Vector Machines: A Novel\n  Approach that Exploits Structure Information in Data", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our today's information society more and more data emerges, e.g.~in social\nnetworks, technical applications, or business applications. Companies try to\ncommercialize these data using data mining or machine learning methods. For\nthis purpose, the data are categorized or classified, but often at high\n(monetary or temporal) costs. An effective approach to reduce these costs is to\napply any kind of active learning (AL) methods, as AL controls the training\nprocess of a classifier by specific querying individual data points (samples),\nwhich are then labeled (e.g., provided with class memberships) by a domain\nexpert. However, an analysis of current AL research shows that AL still has\nsome shortcomings. In particular, the structure information given by the\nspatial pattern of the (un)labeled data in the input space of a classification\nmodel (e.g.,~cluster information), is used in an insufficient way. In addition,\nmany existing AL techniques pay too little attention to their practical\napplicability. To meet these challenges, this article presents several\ntechniques that together build a new approach for combining AL and\nsemi-supervised learning (SSL) for support vector machines (SVM) in\nclassification tasks. Structure information is captured by means of\nprobabilistic models that are iteratively improved at runtime when label\ninformation becomes available. The probabilistic models are considered in a\nselection strategy based on distance, density, diversity, and distribution (4DS\nstrategy) information for AL and in a kernel function (Responsibility Weighted\nMahalanobis kernel) for SVM. The approach fuses generative and discriminative\nmodeling techniques. With 20 benchmark data sets and with the MNIST data set it\nis shown that our new solution yields significantly better results than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:36:56 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 09:43:12 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Reitmaier", "Tobias", ""], ["Calma", "Adrian", ""], ["Sick", "Bernhard", ""]]}, {"id": "1610.04019", "submitter": "Chin-Cheng Hsu", "authors": "Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao and Hsin-Min Wang", "title": "Voice Conversion from Non-parallel Corpora Using Variational\n  Auto-encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 10:52:25 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hsu", "Chin-Cheng", ""], ["Hwang", "Hsin-Te", ""], ["Wu", "Yi-Chiao", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1610.04079", "submitter": "Albert Vilamala", "authors": "Albert Vilamala, Kristoffer Hougaard Madsen and Lars Kai Hansen", "title": "Towards end-to-end optimisation of functional image analysis pipelines", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of neurocognitive tasks requiring accurate localisation of activity\noften rely on functional Magnetic Resonance Imaging, a widely adopted technique\nthat makes use of a pipeline of data processing modules, each involving a\nvariety of parameters. These parameters are frequently set according to the\nlocal goal of each specific module, not accounting for the rest of the\npipeline. Given recent success of neural network research in many different\ndomains, we propose to convert the whole data pipeline into a deep neural\nnetwork, where the parameters involved are jointly optimised by the network to\nbest serve a common global goal. As a proof of concept, we develop a module\nable to adaptively apply the most suitable spatial smoothing to every brain\nvolume for each specific neuroimaging task, and we validate its results in a\nstandard brain decoding experiment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 13:57:55 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Vilamala", "Albert", ""], ["Madsen", "Kristoffer Hougaard", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1610.04167", "submitter": "Or Sharir", "authors": "Or Sharir, Ronen Tamari, Nadav Cohen and Amnon Shashua", "title": "Tensorial Mixture Models", "comments": "A git repository for reproducing our experiments is available at:\n  https://github.com/HUJI-Deep/Generative-ConvACs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casting neural networks in generative frameworks is a highly sought-after\nendeavor these days. Contemporary methods, such as Generative Adversarial\nNetworks, capture some of the generative capabilities, but not all. In\nparticular, they lack the ability of tractable marginalization, and thus are\nnot suitable for many tasks. Other methods, based on arithmetic circuits and\nsum-product networks, do allow tractable marginalization, but their performance\nis challenged by the need to learn the structure of a circuit. Building on the\ntractability of arithmetic circuits, we leverage concepts from tensor analysis,\nand derive a family of generative models we call Tensorial Mixture Models\n(TMMs). TMMs assume a simple convolutional network structure, and in addition,\nlend themselves to theoretical analyses that allow comprehensive understanding\nof the relation between their structure and their expressive properties. We\nthus obtain a generative model that is tractable on one hand, and on the other\nhand, allows effective representation of rich distributions in an easily\ncontrolled manner. These two capabilities are brought together in the task of\nclassification under missing data, where TMMs deliver state of the art\naccuracies with seamless implementation and design.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 16:43:32 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 12:13:45 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 07:50:12 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 09:08:31 GMT"}, {"version": "v5", "created": "Sun, 25 Mar 2018 09:42:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sharir", "Or", ""], ["Tamari", "Ronen", ""], ["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1610.04181", "submitter": "Uri Shaham", "authors": "Uri Shaham, Kelly P. Stanton, Jun Zhao, Huamin Li, Khadir Raddassi,\n  Ruth Montgomery, Yuval Kluger", "title": "Removal of Batch Effects using Distribution-Matching Residual Networks", "comments": "fixed typo", "journal-ref": null, "doi": "10.1093/bioinformatics/btx196", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sources of variability in experimentally derived data include measurement\nerror in addition to the physical phenomena of interest. This measurement error\nis a combination of systematic components, originating from the measuring\ninstrument, and random measurement errors. Several novel biological\ntechnologies, such as mass cytometry and single-cell RNA-seq, are plagued with\nsystematic errors that may severely affect statistical analysis if the data is\nnot properly calibrated. We propose a novel deep learning approach for removing\nsystematic batch effects. Our method is based on a residual network, trained to\nminimize the Maximum Mean Discrepancy (MMD) between the multivariate\ndistributions of two replicates, measured in different batches. We apply our\nmethod to mass cytometry and single-cell RNA-seq datasets, and demonstrate that\nit effectively attenuates batch effects.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 17:14:33 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 22:02:57 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 22:10:28 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 03:20:42 GMT"}, {"version": "v5", "created": "Fri, 23 Dec 2016 18:19:04 GMT"}, {"version": "v6", "created": "Mon, 8 Jan 2018 22:51:40 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Shaham", "Uri", ""], ["Stanton", "Kelly P.", ""], ["Zhao", "Jun", ""], ["Li", "Huamin", ""], ["Raddassi", "Khadir", ""], ["Montgomery", "Ruth", ""], ["Kluger", "Yuval", ""]]}, {"id": "1610.04210", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Justin Romberg", "title": "Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex\n  Relaxation", "comments": "Accepted in AISTATS 2017. Extended the discussion of related work and\n  added a few more references. Clarified some of the statements and notations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible convex relaxation for the phase retrieval problem that\noperates in the natural domain of the signal. Therefore, we avoid the\nprohibitive computational cost associated with \"lifting\" and semidefinite\nprogramming (SDP) in methods such as PhaseLift and compete with recently\ndeveloped non-convex techniques for phase retrieval. We relax the quadratic\nequations for phaseless measurements to inequality constraints each of which\nrepresenting a symmetric \"slab\". Through a simple convex program, our proposed\nestimator finds an extreme point of the intersection of these slabs that is\nbest aligned with a given anchor vector. We characterize geometric conditions\nthat certify success of the proposed estimator. Furthermore, using classic\nresults in statistical learning theory, we show that for random measurements\nthe geometric certificates hold with high probability at an optimal sample\ncomplexity. Phase transition of our estimator is evaluated through simulations.\nOur numerical experiments also suggest that the proposed method can solve phase\nretrieval problems with coded diffraction measurements as well.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 19:35:28 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 16:20:57 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Bahmani", "Sohail", ""], ["Romberg", "Justin", ""]]}, {"id": "1610.04211", "submitter": "Julien Perez", "authors": "Julien Perez and Fei Liu", "title": "Gated End-to-End Memory Networks", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading using differentiable reasoning models has recently shown\nremarkable progress. In this context, End-to-End trainable Memory Networks,\nMemN2N, have demonstrated promising performance on simple natural language\nbased reasoning tasks such as factual reasoning and basic deduction. However,\nother tasks, namely multi-fact question-answering, positional reasoning or\ndialog related tasks, remain challenging particularly due to the necessity of\nmore complex interactions between the memory and controller modules composing\nthis family of models. In this paper, we introduce a novel end-to-end memory\naccess regulation mechanism inspired by the current progress on the connection\nshort-cutting principle in the field of computer vision. Concretely, we develop\na Gated End-to-End trainable Memory Network architecture, GMemN2N. From the\nmachine learning perspective, this new capability is learned in an end-to-end\nfashion without the use of any additional supervision signal which is, as far\nas our knowledge goes, the first of its kind. Our experiments show significant\nimprovements on the most challenging tasks in the 20 bAbI dataset, without the\nuse of any domain knowledge. Then, we show improvements on the dialog bAbI\ntasks including the real human-bot conversion-based Dialog State Tracking\nChallenge (DSTC-2) dataset. On these two datasets, our model sets the new state\nof the art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 19:38:03 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 15:09:29 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Perez", "Julien", ""], ["Liu", "Fei", ""]]}, {"id": "1610.04336", "submitter": "Michael Brand", "authors": "Michael Brand", "title": "MML is not consistent for Neyman-Scott", "comments": "16 pages, 0 tables, 0 figures", "journal-ref": null, "doi": "10.1109/TIT.2019.2943464", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict Minimum Message Length (SMML) is an information-theoretic statistical\ninference method widely cited (but only with informal arguments) as providing\nestimations that are consistent for general estimation problems. It is,\nhowever, almost invariably intractable to compute, for which reason only\napproximations of it (known as MML algorithms) are ever used in practice. Using\nnovel techniques that allow for the first time direct, non-approximated\nanalysis of SMML solutions, we investigate the Neyman-Scott estimation problem,\nan oft-cited showcase for the consistency of MML, and show that even with a\nnatural choice of prior neither SMML nor its popular approximations are\nconsistent for it, thereby providing a counterexample to the general claim.\nThis is the first known explicit construction of an SMML solution for a\nnatural, high-dimensional problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 06:07:45 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 12:04:50 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 15:25:59 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 13:20:52 GMT"}, {"version": "v5", "created": "Mon, 6 Jan 2020 05:43:12 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Brand", "Michael", ""]]}, {"id": "1610.04345", "submitter": "Julien Perez", "authors": "Fei Liu and Julien Perez and Scott Nowson", "title": "A Language-independent and Compositional Model for Personality Trait\n  Recognition from Short Texts", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been used to recognize author personality traits from text,\ntypically combining linguistic feature engineering with shallow learning\nmodels, e.g. linear regression or Support Vector Machines. This work uses\ndeep-learning-based models and atomic features of text, the characters, to\nbuild hierarchical, vectorial word and sentence representations for trait\ninference. This method, applied to a corpus of tweets, shows state-of-the-art\nperformance across five traits and three languages (English, Spanish and\nItalian) compared with prior work in author profiling. The results, supported\nby preliminary visualisation work, are encouraging for the ability to detect\ncomplex human traits.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 07:14:44 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Liu", "Fei", ""], ["Perez", "Julien", ""], ["Nowson", "Scott", ""]]}, {"id": "1610.04351", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano", "title": "Semi-supervised Graph Embedding Approach to Dynamic Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple discrete time semi-supervised graph embedding approach to\nlink prediction in dynamic networks. The learned embedding reflects information\nfrom both the temporal and cross-sectional network structures, which is\nperformed by defining the loss function as a weighted sum of the supervised\nloss from past dynamics and the unsupervised loss of predicting the\nneighborhood context in the current network. Our model is also capable of\nlearning different embeddings for both formation and dissolution dynamics.\nThese key aspects contributes to the predictive performance of our model and we\nprovide experiments with three real--world dynamic networks showing that our\nmethod is comparable to state of the art methods in link formation prediction\nand outperforms state of the art baseline methods in link dissolution\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 07:44:33 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Hisano", "Ryohei", ""]]}, {"id": "1610.04371", "submitter": "Jean-Stephane Bailly", "authors": "Ibrahim Fayad (UMR TETIS), Nicolas Baghdadi (UMR TETIS), St\\'ephane\n  Guitet (INRA, UMR AMAP), Jean-St\\'ephane Bailly (LISAH), Bruno H\\'erault\n  (ECOFOG), Val\\'ery Gond, Mahmoud Hajj, Dinh Ho Tong Minh (UMR TETIS)", "title": "Aboveground biomass mapping in French Guiana by combining remote\n  sensing, forest inventories and environmental data", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation, Elsevier, 2016, 52, pp.502 - 514", "doi": "10.1016/j.jag.2016.07.015", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping forest aboveground biomass (AGB) has become an important task,\nparticularly for the reporting of carbon stocks and changes. AGB can be mapped\nusing synthetic aperture radar data (SAR) or passive optical data. However,\nthese data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and\n\\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical\nforests. Studies have mapped the rough variations in AGB by combining optical\nand environmental data at regional and global scales. Nevertheless, these maps\ncannot represent local variations in AGB in tropical forests. In this paper, we\nhypothesize that the problem of misrepresenting local variations in AGB and AGB\nestimation with good precision occurs because of both methodological limits\n(signal saturation or dilution bias) and a lack of adequate calibration data in\nthis range of AGB values. We test this hypothesis by developing a calibrated\nregression model to predict variations in high AGB values (mean\n\\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for\nspatial extrapolation with data from the optical geoscience laser altimeter\nsystem (GLAS), forest inventories, radar, optics, and environmental variables\nfor spatial inter-and extrapolation. Given their higher point count, GLAS data\nallow a wider coverage of AGB values. We find that the metrics from GLAS\nfootprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3\nMg/ha) with no bias for high values. First, predictive models, including\nremote-sensing, environmental variables and spatial correlation functions,\nallow us to obtain \"wall-to-wall\" AGB maps over French Guiana with an RMSE for\nthe in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We\nconclude that a calibrated regression model based on GLAS with dependent\nenvironmental data can produce good AGB predictions even for high AGB values if\nthe calibration data fit the AGB range. We also demonstrate that small temporal\nand spatial mismatches between field data and GLAS footprints are not a problem\nfor regional and global calibrated regression models because field data aim to\npredict large and deep tendencies in AGB variations from environmental\ngradients and do not aim to represent high but stochastic and temporally\nlimited variations from forest dynamics. Thus, we advocate including a greater\nvariety of data, even if less precise and shifted, to better represent high AGB\nvalues in global models and to improve the fitting of these models for high\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 08:53:48 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Fayad", "Ibrahim", "", "UMR TETIS"], ["Baghdadi", "Nicolas", "", "UMR TETIS"], ["Guitet", "St\u00e9phane", "", "INRA, UMR AMAP"], ["Bailly", "Jean-St\u00e9phane", "", "LISAH"], ["H\u00e9rault", "Bruno", "", "ECOFOG"], ["Gond", "Val\u00e9ry", "", "UMR TETIS"], ["Hajj", "Mahmoud", "", "UMR TETIS"], ["Minh", "Dinh Ho Tong", "", "UMR TETIS"]]}, {"id": "1610.04386", "submitter": "Kurt Cutajar", "authors": "Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, Maurizio Filippone", "title": "Random Feature Expansions for Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composition of multiple Gaussian Processes as a Deep Gaussian Process\n(DGP) enables a deep probabilistic nonparametric approach to flexibly tackle\ncomplex machine learning problems with sound quantification of uncertainty.\nExisting inference approaches for DGP models have limited scalability and are\nnotoriously cumbersome to construct. In this work, we introduce a novel\nformulation of DGPs based on random feature expansions that we train using\nstochastic variational inference. This yields a practical learning framework\nwhich significantly advances the state-of-the-art in inference for DGPs, and\nenables accurate quantification of uncertainty. We extensively showcase the\nscalability and performance of our proposal on several datasets with up to 8\nmillion observations, and various DGP architectures with up to 30 hidden\nlayers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 09:56:17 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:06:21 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Cutajar", "Kurt", ""], ["Bonilla", "Edwin V.", ""], ["Michiardi", "Pietro", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1610.04420", "submitter": "Ievgen Redko", "authors": "Ievgen Redko, Amaury Habrard and Marc Sebban", "title": "Theoretical Analysis of Domain Adaptation with Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is an important and emerging field of machine learning\nthat tackles the problem occurring when the distributions of training (source\ndomain) and test (target domain) data are similar but different. Current\ntheoretical results show that the efficiency of DA algorithms depends on their\ncapacity of minimizing the divergence between source and target probability\ndistributions. In this paper, we provide a theoretical study on the advantages\nthat concepts borrowed from optimal transportation theory can bring to DA. In\nparticular, we show that the Wasserstein metric can be used as a divergence\nmeasure between distributions to obtain generalization guarantees for three\ndifferent learning settings: (i) classic DA with unsupervised target data (ii)\nDA combining source and target labeled data, (iii) multiple source DA. Based on\nthe obtained results, we provide some insights showing when this analysis can\nbe tighter than other existing frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 11:59:28 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 11:04:38 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 07:59:50 GMT"}, {"version": "v4", "created": "Fri, 28 Jul 2017 21:49:07 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Redko", "Ievgen", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""]]}, {"id": "1610.04460", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and David Schultz", "title": "On the Existence of a Sample Mean in Dynamic Time Warping Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of sample mean in dynamic time warping (DTW) spaces has been\nsuccessfully applied to improve pattern recognition systems and generalize\ncentroid-based clustering algorithms. Its existence has neither been proved nor\nchallenged. This article presents sufficient conditions for existence of a\nsample mean in DTW spaces. The proposed result justifies prior work on\napproximate mean algorithms, sets the stage for constructing exact mean\nalgorithms, and is a first step towards a statistical theory of DTW spaces.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 13:42:47 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 13:00:08 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 08:57:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Schultz", "David", ""]]}, {"id": "1610.04490", "submitter": "Casper Kaae S{\\o}nderby", "authors": "Casper Kaae S{\\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi,\n  Ferenc Husz\\'ar", "title": "Amortised MAP Inference for Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) is an underdetermined inverse problem, where a\nlarge number of plausible high-resolution images can explain the same\ndownsampled image. Most current single image SR methods use empirical risk\nminimisation, often with a pixel-wise mean squared error (MSE) loss. However,\nthe outputs from such methods tend to be blurry, over-smoothed and generally\nappear implausible. A more desirable approach would employ Maximum a Posteriori\n(MAP) inference, preferring solutions that always have a high probability under\nthe image prior, and thus appear more plausible. Direct MAP estimation for SR\nis non-trivial, as it requires us to build a model for the image prior from\nsamples. Furthermore, MAP inference is often performed via optimisation-based\niterative algorithms which don't compare well with the efficiency of\nneural-network-based alternatives. Here we introduce new methods for amortised\nMAP inference whereby we calculate the MAP estimate directly using a\nconvolutional neural network. We first introduce a novel neural network\narchitecture that performs a projection to the affine subspace of valid SR\nsolutions ensuring that the high resolution output of the network is always\nconsistent with the low resolution input. We show that, using this\narchitecture, the amortised MAP inference problem reduces to minimising the\ncross-entropy between two distributions, similar to training generative models.\nWe propose three methods to solve this optimisation problem: (1) Generative\nAdversarial Networks (GAN) (2) denoiser-guided SR which backpropagates\ngradient-estimates from denoising to train the network, and (3) a baseline\nmethod using a maximum-likelihood-trained image prior. Our experiments show\nthat the GAN based approach performs best on real image data. Lastly, we\nestablish a connection between GANs and amortised variational inference as in\ne.g. variational autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 14:56:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 13:08:24 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["S\u00f8nderby", "Casper Kaae", ""], ["Caballero", "Jose", ""], ["Theis", "Lucas", ""], ["Shi", "Wenzhe", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1610.04491", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Csaba Szepesvari", "title": "The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear\n  Bandits", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic linear bandits are a natural and simple generalisation of\nfinite-armed bandits with numerous practical applications. Current approaches\nfocus on generalising existing techniques for finite-armed bandits, notably the\noptimism principle and Thompson sampling. While prior work has mostly been in\nthe worst-case setting, we analyse the asymptotic instance-dependent regret and\nshow matching upper and lower bounds on what is achievable. Surprisingly, our\nresults show that no algorithm based on optimism or Thompson sampling will ever\nachieve the optimal rate, and indeed, can be arbitrarily far from optimal, even\nin very simple cases. This is a disturbing result because these techniques are\nstandard tools that are widely used for sequential optimisation. For example,\nfor generalised linear bandits and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:58:44 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Lattimore", "Tor", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1610.04574", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues", "title": "Generalization Error of Invariant Classifiers", "comments": "Accepted to AISTATS. This version has updated references", "journal-ref": "Conference on Artificial Intelligence and Statistics (AISTATS),\n  2017, pp. 1094-1103", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:40:52 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 11:32:03 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 18:58:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1610.04576", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Chris Ding", "title": "Kernel Alignment Inspired Linear Discriminant Analysis", "comments": "Joint European Conference on Machine Learning and Knowledge Discovery\n  in Databases, ECML PKDD, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel alignment measures the degree of similarity between two kernels. In\nthis paper, inspired from kernel alignment, we propose a new Linear\nDiscriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first\ndefine two kernels, data kernel and class indicator kernel. The problem is to\nfind a subspace to maximize the alignment between subspace-transformed data\nkernel and class indicator kernel. Surprisingly, the kernel alignment induced\nkaLDA objective function is very similar to classical LDA and can be expressed\nusing between-class and total scatter matrices. This can be extended to\nmulti-label data. We use a Stiefel-manifold gradient descent algorithm to solve\nthis problem. We perform experiments on 8 single-label and 6 multi-label data\nsets. Results show that kaLDA has very good performance on many single-label\nand multi-label problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:48:03 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Zheng", "Shuai", ""], ["Ding", "Chris", ""]]}, {"id": "1610.04578", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun, Francesco Orabona, Rebecca Willett, Stephen Wright", "title": "Improved Strongly Adaptive Online Learning using Coin Betting", "comments": "fixed a few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new parameter-free online learning algorithm for\nchanging environments. In comparing against algorithms with the same time\ncomplexity as ours, we obtain a strongly adaptive regret bound that is a factor\nof at least $\\sqrt{\\log(T)}$ better, where $T$ is the time horizon. Empirical\nresults show that our algorithm outperforms state-of-the-art methods in\nlearning with expert advice and metric learning scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:51:25 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 20:46:06 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 15:26:04 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Orabona", "Francesco", ""], ["Willett", "Rebecca", ""], ["Wright", "Stephen", ""]]}, {"id": "1610.04580", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Two-sample testing in non-sparse high-dimensional linear models", "comments": "55 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing high-dimensional models, sparsity of the model parameter is a\ncommon but often undesirable assumption. In this paper, we study the following\ntwo-sample testing problem: given two samples generated by two high-dimensional\nlinear models, we aim to test whether the regression coefficients of the two\nlinear models are identical. We propose a framework named TIERS (short for\nTestIng Equality of Regression Slopes), which solves the two-sample testing\nproblem without making any assumptions on the sparsity of the regression\nparameters. TIERS builds a new model by convolving the two samples in such a\nway that the original hypothesis translates into a new moment condition. A\nself-normalization construction is then developed to form a moment test. We\nprovide rigorous theory for the developed framework. Under very weak conditions\nof the feature covariance, we show that the accuracy of the proposed test in\ncontrolling Type I errors is robust both to the lack of sparsity in the\nfeatures and to the heavy tails in the error distribution, even when the sample\nsize is much smaller than the feature dimension. Moreover, we discuss minimax\noptimality and efficiency properties of the proposed test. Simulation analysis\ndemonstrates excellent finite-sample performance of our test. In deriving the\ntest, we also develop tools that are of independent interest. The test is built\nupon a novel estimator, called Auto-aDaptive Dantzig Selector (ADDS), which not\nonly automatically chooses an appropriate scale of the error term but also\nincorporates prior information. To effectively approximate the critical value\nof the test statistic, we develop a novel high-dimensional plug-in approach\nthat complements the recent advances in Gaussian approximation theory.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:51:34 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.04583", "submitter": "Alexander Wein", "authors": "Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, Ankur Moitra", "title": "Message-passing algorithms for synchronization problems over compact\n  groups", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": "10.1002/cpa.21750", "report-no": null, "categories": "cs.IT cs.CV cs.DS math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various alignment problems arising in cryo-electron microscopy, community\ndetection, time synchronization, computer vision, and other fields fall into a\ncommon framework of synchronization problems over compact groups such as Z/L,\nU(1), or SO(3). The goal of such problems is to estimate an unknown vector of\ngroup elements given noisy relative observations. We present an efficient\niterative algorithm to solve a large class of these problems, allowing for any\ncompact group, with measurements on multiple 'frequency channels' (Fourier\nmodes, or more generally, irreducible representations of the group). Our\nalgorithm is a highly efficient iterative method following the blueprint of\napproximate message passing (AMP), which has recently arisen as a central\ntechnique for inference problems such as structured low-rank estimation and\ncompressed sensing. We augment the standard ideas of AMP with ideas from\nrepresentation theory so that the algorithm can work with distributions over\ncompact groups. Using standard but non-rigorous methods from statistical\nphysics we analyze the behavior of our algorithm on a Gaussian noise model,\nidentifying phases where the problem is easy, (computationally) hard, and\n(statistically) impossible. In particular, such evidence predicts that our\nalgorithm is information-theoretically optimal in many cases, and that the\nremaining cases show evidence of statistical-to-computational gaps.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:05:32 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""], ["Moitra", "Ankur", ""]]}, {"id": "1610.04599", "submitter": "Yao Xie", "authors": "Shuang Li, Yao Xie, and Le Song", "title": "Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection,\n  and Extreme Bandits", "comments": "Submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distribution-free approach, the data-driven threshold\nmachine (DTM), for a fundamental problem at the core of many learning tasks:\nchoose a threshold for a given pre-specified level that bounds the tail\nprobability of the maximum of a (possibly dependent but stationary) random\nsequence. We do not assume data distribution, but rather relying on the\nasymptotic distribution of extremal values, and reduce the problem to estimate\nthree parameters of the extreme value distributions and the extremal index. We\nspecially take care of data dependence via estimating extremal index since in\nmany settings, such as scan statistics, change-point detection, and extreme\nbandits, where dependence in the sequence of statistics can be significant. Key\nfeatures of our DTM also include robustness and the computational efficiency,\nand it only requires one sample path to form a reliable estimate of the\nthreshold, in contrast to the Monte Carlo sampling approach which requires\ndrawing a large number of sample paths. We demonstrate the good performance of\nDTM via numerical examples in various dependent settings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:43:16 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Song", "Le", ""]]}, {"id": "1610.04658", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Anna Choromanska and David Sontag", "title": "Simultaneous Learning of Trees and Representations for Extreme\n  Classification and Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-class classification where the predictor has a hierarchical\nstructure that allows for a very large number of labels both at train and test\ntime. The predictive power of such models can heavily depend on the structure\nof the tree, and although past work showed how to learn the tree structure, it\nexpected that the feature vectors remained static. We provide a novel algorithm\nto simultaneously perform representation learning for the input data and\nlearning of the hierarchi- cal predictor. Our approach optimizes an objec- tive\nfunction which favors balanced and easily- separable multi-way node partitions.\nWe theoret- ically analyze this objective, showing that it gives rise to a\nboosting style property and a bound on classification error. We next show how\nto extend the algorithm to conditional density estimation. We empirically\nvalidate both variants of the al- gorithm on text classification and language\nmod- eling, respectively, and show that they compare favorably to common\nbaselines in terms of accu- racy and running time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 22:03:15 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 20:33:14 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Choromanska", "Anna", ""], ["Sontag", "David", ""]]}, {"id": "1610.04751", "submitter": "Shuchin Aeron", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Unsupervised clustering under the Union of Polyhedral Cones (UOPC) model", "comments": null, "journal-ref": "Pattern Recognition Letters Volume 100, 1 December 2017, Pages\n  104-109", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider clustering data that is assumed to come from one\nof finitely many pointed convex polyhedral cones. This model is referred to as\nthe Union of Polyhedral Cones (UOPC) model. Similar to the Union of Subspaces\n(UOS) model where each data from each subspace is generated from a (unknown)\nbasis, in the UOPC model each data from each cone is assumed to be generated\nfrom a finite number of (unknown) \\emph{extreme rays}.To cluster data under\nthis model, we consider several algorithms - (a) Sparse Subspace Clustering by\nNon-negative constraints Lasso (NCL), (b) Least squares approximation (LSA),\nand (c) K-nearest neighbor (KNN) algorithm to arrive at affinity between data\npoints. Spectral Clustering (SC) is then applied on the resulting affinity\nmatrix to cluster data into different polyhedral cones. We show that on an\naverage KNN outperforms both NCL and LSA and for this algorithm we provide the\ndeterministic conditions for correct clustering. For an affinity measure\nbetween the cones it is shown that as long as the cones are not very coherent\nand as long as the density of data within each cone exceeds a threshold, KNN\nleads to accurate clustering. Finally, simulation results on real datasets\n(MNIST and YaleFace datasets) depict that the proposed algorithm works well on\nreal data indicating the utility of the UOPC model and the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 16:04:49 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1610.04782", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Zoltan Szabo, Arthur Gretton", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "comments": "8 pages of main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new computationally efficient dependence measure, and an adaptive\nstatistical test of independence, are proposed. The dependence measure is the\ndifference between analytic embeddings of the joint distribution and the\nproduct of the marginals, evaluated at a finite set of locations (features).\nThese features are chosen so as to maximize a lower bound on the test power,\nresulting in a test that is data-efficient, and that runs in linear time (with\nrespect to the sample size n). The optimized features can be interpreted as\nevidence to reject the null hypothesis, indicating regions in the joint domain\nwhere the joint distribution and the product of the marginals differ most.\nConsistency of the independence test is established, for an appropriate choice\nof features. In real-world benchmarks, independence tests using the optimized\nfeatures perform comparably to the state-of-the-art quadratic-time HSIC test,\nand outperform competing O(n) and O(n log n) tests.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:19:48 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Szabo", "Zoltan", ""], ["Gretton", "Arthur", ""]]}, {"id": "1610.04798", "submitter": "Quanquan Gu", "authors": "Lu Tian and Quanquan Gu", "title": "Communication-efficient Distributed Sparse Linear Discriminant Analysis", "comments": "29 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a communication-efficient distributed estimation method for sparse\nlinear discriminant analysis (LDA) in the high dimensional regime. Our method\ndistributes the data of size $N$ into $m$ machines, and estimates a local\nsparse LDA estimator on each machine using the data subset of size $N/m$. After\nthe distributed estimation, our method aggregates the debiased local estimators\nfrom $m$ machines, and sparsifies the aggregated estimator. We show that the\naggregated estimator attains the same statistical rate as the centralized\nestimation method, as long as the number of machines $m$ is chosen\nappropriately. Moreover, we prove that our method can attain the model\nselection consistency under a milder condition than the centralized method.\nExperiments on both synthetic and real datasets corroborate our theory.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 23:39:45 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Tian", "Lu", ""], ["Gu", "Quanquan", ""]]}, {"id": "1610.04804", "submitter": "Zhen Han", "authors": "Zhen Han and Alyson Wilson", "title": "Dynamic Stacked Generalization for Node Classification on Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 00:47:21 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Han", "Zhen", ""], ["Wilson", "Alyson", ""]]}, {"id": "1610.04811", "submitter": "Dong Xia", "authors": "Dong Xia", "title": "Estimation of low rank density matrices by Pauli measurements", "comments": "in Electronic Journal of Statistics, 2017", "journal-ref": null, "doi": "10.1214/16-EJS1222", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density matrices are positively semi-definite Hermitian matrices with unit\ntrace that describe the states of quantum systems. Many quantum systems of\nphysical interest can be represented as high-dimensional low rank density\nmatrices. A popular problem in {\\it quantum state tomography} (QST) is to\nestimate the unknown low rank density matrix of a quantum system by conducting\nPauli measurements. Our main contribution is twofold. First, we establish the\nminimax lower bounds in Schatten $p$-norms with $1\\leq p\\leq +\\infty$ for low\nrank density matrices estimation by Pauli measurements. In our previous paper,\nthese minimax lower bounds are proved under the trace regression model with\nGaussian noise and the noise is assumed to have common variance. In this paper,\nwe prove these bounds under the Binomial observation model which meets the\nactual model in QST.\n  Second, we study the Dantzig estimator (DE) for estimating the unknown low\nrank density matrix under the Binomial observation model by using Pauli\nmeasurements. In our previous papers, we studied the least squares estimator\nand the projection estimator, where we proved the optimal convergence rates for\nthe least squares estimator in Schatten $p$-norms with $1\\leq p\\leq 2$ and,\nunder a stronger condition, the optimal convergence rates for the projection\nestimator in Schatten $p$-norms with $1\\leq p\\leq +\\infty$. In this paper, we\nshow that the results of these two distinct estimators can be simultaneously\nobtained by the Dantzig estimator. Moreover, better convergence rates in\nSchatten norm distances can be proved for Dantzig estimator under conditions\nweaker than those needed in previous papers. When the objective function of DE\nis replaced by the negative von Neumann entropy, we obtain sharp convergence\nrate in Kullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 02:28:08 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 04:16:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Xia", "Dong", ""]]}, {"id": "1610.04929", "submitter": "Li Wang", "authors": "Li Wang", "title": "Probabilistic Dimensionality Reduction via Structure Learning", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic dimensionality reduction framework that can\nnaturally integrate the generative model and the locality information of data.\nBased on this framework, we present a new model, which is able to learn a\nsmooth skeleton of embedding points in a low-dimensional space from\nhigh-dimensional noisy data. The formulation of the new model can be\nequivalently interpreted as two coupled learning problem, i.e., structure\nlearning and the learning of projection matrix. This interpretation motivates\nthe learning of the embedding points that can directly form an explicit graph\nstructure. We develop a new method to learn the embedding points that form a\nspanning tree, which is further extended to obtain a discriminative and compact\nfeature representation for clustering problems. Unlike traditional clustering\nmethods, we assume that centers of clusters should be close to each other if\nthey are connected in a learned graph, and other cluster centers should be\ndistant. This can greatly facilitate data visualization and scientific\ndiscovery in downstream analysis. Extensive experiments are performed that\ndemonstrate that the proposed framework is able to obtain discriminative\nfeature representations, and correctly recover the intrinsic structures of\nvarious real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 23:37:26 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Wang", "Li", ""]]}, {"id": "1610.05083", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, and Barbara Hammer", "title": "Efficient Metric Learning for the Analysis of Motion Data", "comments": "23 pages, 8 figures, DSAA 2015 conference", "journal-ref": null, "doi": "10.1109/DSAA.2015.7344819", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate metric learning in the context of dynamic time warping (DTW),\nthe by far most popular dissimilarity measure used for the comparison and\nanalysis of motion capture data. While metric learning enables a\nproblem-adapted representation of data, the majority of methods has been\nproposed for vectorial data only. In this contribution, we extend the popular\nprinciple offered by the large margin nearest neighbors learner (LMNN) to DTW\nby treating the resulting component-wise dissimilarity values as features. We\ndemonstrate that this principle greatly enhances the classification accuracy in\nseveral benchmarks. Further, we show that recent auxiliary concepts such as\nmetric regularization can be transferred from the vectorial case to\ncomponent-wise DTW in a similar way. We illustrate that metric regularization\nconstitutes a crucial prerequisite for the interpretation of the resulting\nrelevance profiles.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 12:47:20 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 09:30:40 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 15:29:07 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Hosseini", "Babak", ""], ["Hammer", "Barbara", ""]]}, {"id": "1610.05108", "submitter": "Gian-Andrea Thanei", "authors": "Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah", "title": "The xyz algorithm for fast interaction search in high-dimensional data", "comments": null, "journal-ref": "JMLR. Journal of Machine Learning Research. The xyz algorithm for\n  fast interaction search in high-dimensional data. Gian-Andrea Thanei, Nicolai\n  Meinshausen, Rajen D. Shah. 19.37.1.42. 2018", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing regression on a dataset with $p$ variables, it is often of\ninterest to go beyond using main linear effects and include interactions as\nproducts between individual variables. For small-scale problems, these\ninteractions can be computed explicitly but this leads to a computational\ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be\nprohibitive if $p$ is very large. We introduce a new randomised algorithm that\nis able to discover interactions with high probability and under mild\nconditions has a runtime that is subquadratic in $p$. We show that strong\ninteractions can be discovered in almost linear time, whilst finding weaker\ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 < \\alpha < 2$\ndepending on their strength. The underlying idea is to transform interaction\nsearch into a closestpair problem which can be solved efficiently in\nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in\nthe language R. We demonstrate its efficiency for application to genome-wide\nassociation studies, where more than $10^{11}$ interactions can be screened in\nunder $280$ seconds with a single-core $1.2$ GHz CPU.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 13:42:22 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 06:29:04 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 23:02:36 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 20:25:04 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Thanei", "Gian-Andrea", ""], ["Meinshausen", "Nicolai", ""], ["Shah", "Rajen D.", ""]]}, {"id": "1610.05129", "submitter": "Wen Sun", "authors": "Wen Sun, Debadeepta Dey, and Ashish Kapoor", "title": "Risk-Aware Algorithms for Adversarial Contextual Bandits", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider adversarial contextual bandits with risk\nconstraints. At each round, nature prepares a context, a cost for each arm, and\nadditionally a risk for each arm. The learner leverages the context to pull an\narm and then receives the corresponding cost and risk associated with the\npulled arm. In addition to minimizing the cumulative cost, the learner also\nneeds to satisfy long-term risk constraints -- the average of the cumulative\nrisk from all pulled arms should not be larger than a pre-defined threshold. To\naddress this problem, we first study the full information setting where in each\nround the learner receives an adversarial convex loss and a convex constraint.\nWe develop a meta algorithm leveraging online mirror descent for the full\ninformation setting and extend it to contextual bandit with risk constraints\nsetting using expert advice. Our algorithms can achieve near-optimal regret in\nterms of minimizing the total cost, while successfully maintaining a sublinear\ngrowth of cumulative risk constraint violation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:14:43 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sun", "Wen", ""], ["Dey", "Debadeepta", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1610.05160", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "The Peaking Phenomenon in Semi-supervised Learning", "comments": "11 pages, 5 figures. S+SSPR 2016, M\\'erida, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the supervised least squares classifier, when the number of training\nobjects is smaller than the dimensionality of the data, adding more data to the\ntraining set may first increase the error rate before decreasing it. This,\npossibly counterintuitive, phenomenon is known as peaking. In this work, we\nobserve that a similar but more pronounced version of this phenomenon also\noccurs in the semi-supervised setting, where instead of labeled objects,\nunlabeled objects are added to the training set. We explain why the learning\ncurve has a more steep incline and a more gradual decline in this setting\nthrough simulation studies and by applying an approximation of the learning\ncurve based on the work by Raudys & Duin.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 15:22:43 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1610.05163", "submitter": "Mu Niu", "authors": "Mu Niu, Zhenwen Dai, Neil Lawrence, Kolja Becker", "title": "Spatio-temporal Gaussian processes modeling of dynamical systems in\n  systems biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative modeling of post-transcriptional regulation process is a\nchallenging problem in systems biology. A mechanical model of the regulatory\nprocess needs to be able to describe the available spatio-temporal protein\nconcentration and mRNA expression data and recover the continuous\nspatio-temporal fields. Rigorous methods are required to identify model\nparameters. A promising approach to deal with these difficulties is proposed\nusing Gaussian process as a prior distribution over the latent function of\nprotein concentration and mRNA expression. In this study, we consider a partial\ndifferential equation mechanical model with differential operators and latent\nfunction. Since the operators at stake are linear, the information from the\nphysical model can be encoded into the kernel function. Hybrid Monte Carlo\nmethods are employed to carry out Bayesian inference of the partial\ndifferential equation parameters and Gaussian process kernel parameters. The\nspatio-temporal field of protein concentration and mRNA expression are\nreconstructed without explicitly solving the partial differential equation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 15:25:56 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Niu", "Mu", ""], ["Dai", "Zhenwen", ""], ["Lawrence", "Neil", ""], ["Becker", "Kolja", ""]]}, {"id": "1610.05202", "submitter": "Aur\\'elien Bellet", "authors": "Paul Vanhaesebrouck, Aur\\'elien Bellet, Marc Tommasi", "title": "Decentralized Collaborative Learning of Personalized Models over\n  Networks", "comments": "To appear in the Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set of learning agents in a collaborative peer-to-peer network,\nwhere each agent learns a personalized model according to its own learning\nobjective. The question addressed in this paper is: how can agents improve upon\ntheir locally trained model by communicating with other agents that have\nsimilar objectives? We introduce and analyze two asynchronous gossip algorithms\nrunning in a fully decentralized manner. Our first approach, inspired from\nlabel propagation, aims to smooth pre-trained local models over the network\nwhile accounting for the confidence that each agent has in its initial model.\nIn our second approach, agents jointly learn and propagate their model by\nmaking iterative updates based on both their local dataset and the behavior of\ntheir neighbors. To optimize this challenging objective, our decentralized\nalgorithm is based on ADMM.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 16:51:49 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 18:32:17 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Vanhaesebrouck", "Paul", ""], ["Bellet", "Aur\u00e9lien", ""], ["Tommasi", "Marc", ""]]}, {"id": "1610.05214", "submitter": "Soledad Villar", "authors": "Soledad Villar and Afonso S. Bandeira and Andrew J. Blumberg and\n  Rachel Ward", "title": "A polynomial-time relaxation of the Gromov-Hausdorff distance", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gromov-Hausdorff distance provides a metric on the set of isometry\nclasses of compact metric spaces. Unfortunately, computing this metric directly\nis believed to be computationally intractable. Motivated by applications in\nshape matching and point-cloud comparison, we study a semidefinite programming\nrelaxation of the Gromov-Hausdorff metric. This relaxation can be computed in\npolynomial time, and somewhat surprisingly is itself a pseudometric. We\ndescribe the induced topology on the set of compact metric spaces. Finally, we\ndemonstrate the numerical performance of various algorithms for computing the\nrelaxed distance and apply these algorithms to several relevant data sets. In\nparticular we propose a greedy algorithm for finding the best correspondence\nbetween finite metric spaces that can handle hundreds of points.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 17:18:45 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 20:40:13 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Villar", "Soledad", ""], ["Bandeira", "Afonso S.", ""], ["Blumberg", "Andrew J.", ""], ["Ward", "Rachel", ""]]}, {"id": "1610.05246", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "BET on Independence", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1537921", "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric dependence detection. Many existing\nmethods may suffer severe power loss due to non-uniform consistency, which we\nillustrate with a paradox. To avoid such power loss, we approach the\nnonparametric test of independence through the new framework of binary\nexpansion statistics (BEStat) and binary expansion testing (BET), which examine\ndependence through a novel binary expansion filtration approximation of the\ncopula. Through a Hadamard transform, we find that the symmetry statistics in\nthe filtration are complete sufficient statistics for dependence. These\nstatistics are also uncorrelated under the null. By utilizing symmetry\nstatistics, the BET avoids the problem of non-uniform consistency and improves\nupon a wide class of commonly used methods (a) by achieving the minimax rate in\nsample size requirement for reliable power and (b) by providing clear\ninterpretations of global relationships upon rejection of independence. The\nbinary expansion approach also connects the symmetry statistics with the\ncurrent computing system to facilitate efficient bitwise implementation. We\nillustrate the BET with a study of the distribution of stars in the night sky\nand with an exploratory data analysis of the TCGA breast cancer data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:19:49 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 03:26:00 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 07:09:37 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 02:08:08 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 15:57:14 GMT"}, {"version": "v6", "created": "Sun, 13 May 2018 02:25:46 GMT"}, {"version": "v7", "created": "Mon, 15 Apr 2019 20:39:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1610.05247", "submitter": "Qiang Liu", "authors": "Qiang Liu and Jason D. Lee", "title": "Black-box Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is widely used in machine learning and statistics, but\nits power is limited by the restriction of using simple proposals for which the\nimportance weights can be tractably calculated. We address this problem by\nstudying black-box importance sampling methods that calculate importance\nweights for samples generated from any unknown proposal or black-box mechanism.\nOur method allows us to use better and richer proposals to solve difficult\nproblems, and (somewhat counter-intuitively) also has the additional benefit of\nimproving the estimation accuracy beyond typical importance sampling. Both\ntheoretical and empirical analyses are provided.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:24:30 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Liu", "Qiang", ""], ["Lee", "Jason D.", ""]]}, {"id": "1610.05261", "submitter": "Michael Schober", "authors": "Michael Schober, Simo S\\\"arkk\\\"a, Philipp Hennig", "title": "A probabilistic model for the numerical solution of initial value\n  problems", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many numerical methods, solvers for initial value problems (IVPs) on\nordinary differential equations estimate an analytically intractable quantity,\nusing the results of tractable computations as inputs. This structure is\nclosely connected to the notion of inference on latent variables in statistics.\nWe describe a class of algorithms that formulate the solution to an IVP as\ninference on a latent path that is a draw from a Gaussian process probability\nmeasure (or equivalently, the solution of a linear stochastic differential\nequation). We then show that certain members of this class are connected\nprecisely to generalized linear methods for ODEs, a number of Runge--Kutta\nmethods, and Nordsieck methods. This probabilistic formulation of classic\nmethods is valuable in two ways: analytically, it highlights implicit prior\nassumptions favoring certain approximate solutions to the IVP over others, and\ngives a precise meaning to the old observation that these methods act like\nfilters. Practically, it endows the classic solvers with `docking points' for\nnotions of uncertainty and prior information about the initial value, the value\nof the ODE itself, and the solution of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:50:35 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 17:10:05 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 21:06:48 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Schober", "Michael", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Hennig", "Philipp", ""]]}, {"id": "1610.05275", "submitter": "Quanquan Gu", "authors": "Lingxiao Wang and Xiao Zhang and Quanquan Gu", "title": "A Unified Computational and Statistical Framework for Nonconvex Low-Rank\n  Matrix Estimation", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework for estimating low-rank matrices through\nnonconvex optimization based on gradient descent algorithm. Our framework is\nquite general and can be applied to both noisy and noiseless observations. In\nthe general case with noisy observations, we show that our algorithm is\nguaranteed to linearly converge to the unknown low-rank matrix up to minimax\noptimal statistical error, provided an appropriate initial estimator. While in\nthe generic noiseless setting, our algorithm converges to the unknown low-rank\nmatrix at a linear rate and enables exact recovery with optimal sample\ncomplexity. In addition, we develop a new initialization algorithm to provide a\ndesired initial estimator, which outperforms existing initialization algorithms\nfor nonconvex low-rank matrix estimation. We illustrate the superiority of our\nframework through three examples: matrix regression, matrix completion, and\none-bit matrix completion. We also corroborate our theory through extensive\nexperiments on synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 19:16:39 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Wang", "Lingxiao", ""], ["Zhang", "Xiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1610.05350", "submitter": "Andrea Montanari", "authors": "Zhou Fan and Andrea Montanari", "title": "How Well Do Local Algorithms Solve Semidefinite Programs?", "comments": "48 pages, 1 pdf figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several probabilistic models from high-dimensional statistics and machine\nlearning reveal an intriguing --and yet poorly understood-- dichotomy. Either\nsimple local algorithms succeed in estimating the object of interest, or even\nsophisticated semi-definite programming (SDP) relaxations fail.\n  In order to explore this phenomenon, we study a classical SDP relaxation of\nthe minimum graph bisection problem, when applied to Erd\\H{o}s-Renyi random\ngraphs with bounded average degree $d>1$, and obtain several types of results.\nFirst, we use a dual witness construction (using the so-called non-backtracking\nmatrix of the graph) to upper bound the SDP value. Second, we prove that a\nsimple local algorithm approximately solves the SDP to within a factor\n$2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at\nmost $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree.\n  We then analyze a more sophisticated local algorithm, which aggregates\ninformation according to the harmonic measure on the limiting Galton-Watson\n(GW) tree. The resulting lower bound is expressed in terms of the conductance\nof the GW tree and matches surprisingly well the empirically determined SDP\nvalues on large-scale Erd\\H{o}s-Renyi graphs.\n  We finally consider the planted partition model. In this case, purely local\nalgorithms are known to fail, but they do succeed if a small amount of side\ninformation is available. Our results imply quantitative bounds on the\nthreshold for partial recovery using SDP in this model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 20:45:11 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Fan", "Zhou", ""], ["Montanari", "Andrea", ""]]}, {"id": "1610.05392", "submitter": "Edwin Bonilla", "authors": "Karl Krauth and Edwin V. Bonilla and Kurt Cutajar and Maurizio\n  Filippone", "title": "AutoGP: Exploring the Capabilities and Limitations of Gaussian Process\n  Models", "comments": "Edited results on RECTANGLES-IMAGE and related comments; minor\n  additional edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capabilities and limitations of Gaussian process models by\njointly exploring three complementary directions: (i) scalable and\nstatistically efficient inference; (ii) flexible kernels; and (iii) objective\nfunctions for hyperparameter learning alternative to the marginal likelihood.\nOur approach outperforms all previously reported GP methods on the standard\nMNIST dataset; performs comparatively to previous kernel-based methods using\nthe RECTANGLES-IMAGE dataset; and breaks the 1% error-rate barrier in GP models\nusing the MNIST8M dataset, showing along the way the scalability of our method\nat unprecedented scale for GP models (8 million observations) in classification\nproblems. Overall, our approach represents a significant breakthrough in kernel\nmethods and GP models, bridging the gap between deep learning approaches and\nkernel machines.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 01:09:19 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 05:45:32 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 00:48:36 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Krauth", "Karl", ""], ["Bonilla", "Edwin V.", ""], ["Cutajar", "Kurt", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1610.05400", "submitter": "Arvind Saibaba", "authors": "Eric Chi, Liuiyi Hu, Arvind K. Saibaba, Arvind U. K. Rao", "title": "Going off the Grid: Iterative Model Selection for Biclustered Matrix\n  Completion", "comments": "42 pages, 7 figures. Supplementary material\n  (https://github.com/echi/IMS/blob/master/BMC_Supplement_JCGS.pdf) and codes\n  available (https://github.com/echi/IMS)", "journal-ref": "Journal of Computational and Graphical Statistics, 28(1):36--47,\n  2019", "doi": "10.1080/10618600.2018.1482763", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of performing matrix completion with side information\non row-by-row and column-by-column similarities. We build upon recent proposals\nfor matrix estimation with smoothness constraints with respect to row and\ncolumn graphs. We present a novel iterative procedure for directly minimizing\nan information criterion in order to select an appropriate amount row and\ncolumn smoothing, namely perform model selection. We also discuss how to\nexploit the special structure of the problem to scale up the estimation and\nmodel selection procedure via the Hutchinson estimator. We present simulation\nresults and an application to predicting associations in imaging-genomics\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 01:50:52 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:57:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chi", "Eric", ""], ["Hu", "Liuiyi", ""], ["Saibaba", "Arvind K.", ""], ["Rao", "Arvind U. K.", ""]]}, {"id": "1610.05448", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Generalization error minimization: a new approach to model evaluation\n  and selection with an application to penalized regression", "comments": "The theoretical generalization and extension of arXiv:1606.00142 and\n  arXiv:1609.03344", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST q-fin.EC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study model evaluation and model selection from the perspective of\ngeneralization ability (GA): the ability of a model to predict outcomes in new\nsamples from the same population. We believe that GA is one way formally to\naddress concerns about the external validity of a model. The GA of a model\nestimated on a sample can be measured by its empirical out-of-sample errors,\ncalled the generalization errors (GE). We derive upper bounds for the GE, which\ndepend on sample sizes, model complexity and the distribution of the loss\nfunction. The upper bounds can be used to evaluate the GA of a model, ex ante.\nWe propose using generalization error minimization (GEM) as a framework for\nmodel selection. Using GEM, we are able to unify a big class of penalized\nregression estimators, including lasso, ridge and bridge, under the same set of\nassumptions. We establish finite-sample and asymptotic properties (including\n$\\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \\geqslant p$\nand the $n < p$ cases. We also derive the $\\mathcal{L}_2$-distance between the\npenalized and corresponding unpenalized regression estimates. In practice, GEM\ncan be implemented by validation or cross-validation. We show that the GE\nbounds can be used for selecting the optimal number of folds in $K$-fold\ncross-validation. We propose a variant of $R^2$, the $GR^2$, as a measure of\nGA, which considers both both in-sample and out-of-sample goodness of fit.\nSimulations are used to demonstrate our key results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 06:26:47 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1610.05507", "submitter": "Arda Aytekin", "authors": "Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson", "title": "Analysis and Implementation of an Asynchronous Optimization Algorithm\n  for the Parameter Server", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an asynchronous incremental aggregated gradient algorithm\nand its implementation in a parameter server framework for solving regularized\noptimization problems. The algorithm can handle both general convex (possibly\nnon-smooth) regularizers and general convex constraints. When the empirical\ndata loss is strongly convex, we establish linear convergence rate, give\nexplicit expressions for step-size choices that guarantee convergence to the\noptimum, and bound the associated convergence factors. The expressions have an\nexplicit dependence on the degree of asynchrony and recover classical results\nunder synchronous operation. Simulations and implementations on commercial\ncompute clouds validate our findings.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 09:48:51 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Aytekin", "Arda", ""], ["Feyzmahdavian", "Hamid Reza", ""], ["Johansson", "Mikael", ""]]}, {"id": "1610.05604", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Madeleine Udell", "title": "Dynamic Assortment Personalization in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of dynamic assortment personalization with large,\nheterogeneous populations and wide arrays of products, and demonstrate the\nimportance of structural priors for effective, efficient large-scale\npersonalization. Assortment personalization is the problem of choosing, for\neach individual (type), a best assortment of products, ads, or other offerings\n(items) so as to maximize revenue. This problem is central to revenue\nmanagement in e-commerce and online advertising where both items and types can\nnumber in the millions.\n  We formulate the dynamic assortment personalization problem as a\ndiscrete-contextual bandit with $m$ contexts (types) and exponentially many\narms (assortments of the $n$ items). We assume that each type's preferences\nfollow a simple parametric model with $n$ parameters. In all, there are $mn$\nparameters, and existing literature suggests that order optimal regret scales\nas $mn$. However, the data required to estimate so many parameters is orders of\nmagnitude larger than the data available in most revenue management\napplications; and the optimal regret under these models is unacceptably high.\n  In this paper, we impose a natural structure on the problem -- a small latent\ndimension, or low rank. In the static setting, we show that this model can be\nefficiently learned from surprisingly few interactions, using a time- and\nmemory-efficient optimization algorithm that converges globally whenever the\nmodel is learnable. In the dynamic setting, we show that structure-aware\ndynamic assortment personalization can have regret that is an order of\nmagnitude smaller than structure-ignorant approaches. We validate our\ntheoretical results empirically.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 13:32:36 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 15:42:42 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 14:46:44 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kallus", "Nathan", ""], ["Udell", "Madeleine", ""]]}, {"id": "1610.05672", "submitter": "Colin Wei", "authors": "Colin Wei and Iain Murray", "title": "Markov Chain Truncation for Doubly-Intractable Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing partition functions, the normalizing constants of probability\ndistributions, is often hard. Variants of importance sampling give unbiased\nestimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z\nare harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo\nsampling of \"doubly-intractable\" distributions, such as the parameter posterior\nfor Markov Random Fields or Exponential Random Graphs. We demonstrate how to\nconstruct unbiased estimates for 1/Z given access to black-box importance\nsampling estimators for Z. We adapt recent work on random series truncation and\nMarkov chain coupling, producing estimators with lower variance and a higher\npercentage of positive estimates than before. Our debiasing algorithms are\nsimple to implement, and have some theoretical and empirical advantages over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:14:52 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 22:21:42 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wei", "Colin", ""], ["Murray", "Iain", ""]]}, {"id": "1610.05683", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Francisco J. R. Ruiz and Scott W. Linderman\n  and David M. Blei", "title": "Reparameterization Gradients through Acceptance-Rejection Sampling\n  Algorithms", "comments": "An error in the von Mises distribution reparameterization in Table 2\n  has been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference using the reparameterization trick has enabled\nlarge-scale approximate Bayesian inference in complex probabilistic models,\nleveraging stochastic optimization to sidestep intractable expectations. The\nreparameterization trick is applicable when we can simulate a random variable\nby applying a differentiable deterministic function on an auxiliary random\nvariable whose distribution is fixed. For many distributions of interest (such\nas the gamma or Dirichlet), simulation of random variables relies on\nacceptance-rejection sampling. The discontinuity introduced by the\naccept-reject step means that standard reparameterization tricks are not\napplicable. We propose a new method that lets us leverage reparameterization\ngradients even when variables are outputs of a acceptance-rejection sampling\nalgorithm. Our approach enables reparameterization on a larger class of\nvariational distributions. In several studies of real and synthetic data, we\nshow that the variance of the estimator of the gradient is significantly lower\nthan other state-of-the-art methods. This leads to faster convergence of\nstochastic gradient variational inference.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 15:55:08 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 14:16:52 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 15:01:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Ruiz", "Francisco J. R.", ""], ["Linderman", "Scott W.", ""], ["Blei", "David M.", ""]]}, {"id": "1610.05735", "submitter": "Daniel Ritchie", "authors": "Daniel Ritchie, Paul Horsfall, Noah D. Goodman", "title": "Deep Amortized Inference for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages (PPLs) are a powerful modeling tool, able\nto represent any computable probability distribution. Unfortunately,\nprobabilistic program inference is often intractable, and existing PPLs mostly\nrely on expensive, approximate sampling-based methods. To alleviate this\nproblem, one could try to learn from past inferences, so that future inferences\nrun faster. This strategy is known as amortized inference; it has recently been\napplied to Bayesian networks and deep generative models. This paper proposes a\nsystem for amortized inference in PPLs. In our system, amortization comes in\nthe form of a parameterized guide program. Guide programs have similar\nstructure to the original program, but can have richer data flow, including\nneural network components. These networks can be optimized so that the guide\napproximately samples from the posterior distribution defined by the original\nprogram. We present a flexible interface for defining guide programs and a\nstochastic gradient-based scheme for optimizing guide parameters, as well as\nsome preliminary results on automatically deriving guide programs. We explore\nin detail the common machine learning pattern in which a 'local' model is\nspecified by 'global' random values and used to generate independent observed\ndata points; this gives rise to amortized local inference supporting global\nmodel learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 18:35:09 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Ritchie", "Daniel", ""], ["Horsfall", "Paul", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1610.05755", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot, Mart\\'in Abadi, \\'Ulfar Erlingsson, Ian Goodfellow,\n  Kunal Talwar", "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private\n  Training Data", "comments": "Accepted to ICLR 17 as an oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some machine learning applications involve training data that is sensitive,\nsuch as the medical histories of patients in a clinical trial. A model may\ninadvertently and implicitly store some of its training data; careful analysis\nof the model may therefore reveal sensitive information.\n  To address this problem, we demonstrate a generally applicable approach to\nproviding strong privacy guarantees for training data: Private Aggregation of\nTeacher Ensembles (PATE). The approach combines, in a black-box fashion,\nmultiple models trained with disjoint datasets, such as records from different\nsubsets of users. Because they rely directly on sensitive data, these models\nare not published, but instead used as \"teachers\" for a \"student\" model. The\nstudent learns to predict an output chosen by noisy voting among all of the\nteachers, and cannot directly access an individual teacher or the underlying\ndata or parameters. The student's privacy properties can be understood both\nintuitively (since no single teacher and thus no single dataset dictates the\nstudent's training) and formally, in terms of differential privacy. These\nproperties hold even if an adversary can not only query the student but also\ninspect its internal workings.\n  Compared with previous work, the approach imposes only weak assumptions on\nhow teachers are trained: it applies to any model, including non-convex models\nlike DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and\nSVHN thanks to an improved privacy analysis and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 19:37:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 13:18:56 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 00:18:03 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 18:56:43 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Papernot", "Nicolas", ""], ["Abadi", "Mart\u00edn", ""], ["Erlingsson", "\u00dalfar", ""], ["Goodfellow", "Ian", ""], ["Talwar", "Kunal", ""]]}, {"id": "1610.05756", "submitter": "Teague Henry", "authors": "Teague Henry, David Banks, Christine Chai, Derek Owens-Oas", "title": "Modeling community structure and topics in dynamic text networks", "comments": "Accepted at Journal of Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen great progress in both dynamic network modeling and\ntopic modeling. This paper draws upon both areas to create a Bayesian method\nthat allows topic discovery to inform the latent network model and the network\nstructure to facilitate topic identification. We apply this method to the 467\ntop political blogs of 2012. Our results find complex community structure\nwithin this set of blogs, where community membership depends strongly upon the\nset of topics in which the blogger is interested.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 19:39:44 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 15:06:13 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Henry", "Teague", ""], ["Banks", "David", ""], ["Chai", "Christine", ""], ["Owens-Oas", "Derek", ""]]}, {"id": "1610.05773", "submitter": "Manuel Gomez Rodriguez", "authors": "Ali Zarezade and Utkarsh Upadhyay and Hamid Rabiee and Manuel Gomez\n  Rodriguez", "title": "RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks", "comments": "To appear at the 10th ACM International Conference on Web Search and\n  Data Mining (WSDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users in social networks whose posts stay at the top of their followers'{}\nfeeds the longest time are more likely to be noticed. Can we design an online\nalgorithm to help them decide when to post to stay at the top? In this paper,\nwe address this question as a novel optimal control problem for jump stochastic\ndifferential equations. For a wide variety of feed dynamics, we show that the\noptimal broadcasting intensity for any user is surprisingly simple -- it is\ngiven by the position of her most recent post on each of her follower's feeds.\nAs a consequence, we are able to develop a simple and highly efficient online\nalgorithm, RedQueen, to sample the optimal times for the user to post.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently make a user's posts more visible over time,\nis robust to volume changes on her followers' feeds, and significantly\noutperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:00:05 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Zarezade", "Ali", ""], ["Upadhyay", "Utkarsh", ""], ["Rabiee", "Hamid", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1610.05775", "submitter": "Manuel Gomez Rodriguez", "authors": "Charalampos Mavroforakis and Isabel Valera and Manuel Gomez Rodriguez", "title": "Modeling the Dynamics of Online Learning Activity", "comments": "Python implementation of the proposed HDHP is available at\n  https://github.com/Networks-Learning/hdhp.py", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are increasingly relying on the Web and social media to find solutions\nto their problems in a wide range of domains. In this online setting, closely\nrelated problems often lead to the same characteristic learning pattern, in\nwhich people sharing these problems visit related pieces of information,\nperform almost identical queries or, more generally, take a series of similar\nactions. In this paper, we introduce a novel modeling framework for clustering\ncontinuous-time grouped streaming data, the hierarchical Dirichlet Hawkes\nprocess (HDHP), which allows us to automatically uncover a wide variety of\nlearning patterns from detailed traces of learning activity. Our model allows\nfor efficient inference, scaling to millions of actions taken by thousands of\nusers. Experiments on real data gathered from Stack Overflow reveal that our\nframework can recover meaningful learning patterns in terms of both content and\ntemporal dynamics, as well as accurately track users' interests and goals over\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:00:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mavroforakis", "Charalampos", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1610.05792", "submitter": "Soham De", "authors": "Soham De, Abhay Yadav, David Jacobs and Tom Goldstein", "title": "Big Batch SGD: Automated Inference using Adaptive Batch Sizes", "comments": "A preliminary version of this paper appears in AISTATS 2017\n  (International Conference on Artificial Intelligence and Statistics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical stochastic gradient methods for optimization rely on noisy gradient\napproximations that become progressively less accurate as iterates approach a\nsolution. The large noise and small signal in the resulting gradients makes it\ndifficult to use them for adaptive stepsize selection and automatic stopping.\nWe propose alternative \"big batch\" SGD schemes that adaptively grow the batch\nsize over time to maintain a nearly constant signal-to-noise ratio in the\ngradient approximation. The resulting methods have similar convergence rates to\nclassical SGD, and do not require convexity of the objective. The high fidelity\ngradients enable automated learning rate selection and do not require stepsize\ndecay. Big batch methods are thus easily automated and can run with little or\nno oversight.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:24:10 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 23:37:30 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 01:10:28 GMT"}, {"version": "v4", "created": "Thu, 6 Apr 2017 21:48:28 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Yadav", "Abhay", ""], ["Jacobs", "David", ""], ["Goldstein", "Tom", ""]]}, {"id": "1610.05820", "submitter": "Reza Shokri", "authors": "Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov", "title": "Membership Inference Attacks against Machine Learning Models", "comments": "In the proceedings of the IEEE Symposium on Security and Privacy,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantitatively investigate how machine learning models leak information\nabout the individual data records on which they were trained. We focus on the\nbasic membership inference attack: given a data record and black-box access to\na model, determine if the record was in the model's training dataset. To\nperform membership inference against a target model, we make adversarial use of\nmachine learning and train our own inference model to recognize differences in\nthe target model's predictions on the inputs that it trained on versus the\ninputs that it did not train on.\n  We empirically evaluate our inference techniques on classification models\ntrained by commercial \"machine learning as a service\" providers such as Google\nand Amazon. Using realistic datasets and classification tasks, including a\nhospital discharge dataset whose membership is sensitive from the privacy\nperspective, we show that these models can be vulnerable to membership\ninference attacks. We then investigate the factors that influence this leakage\nand evaluate mitigation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:38:33 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 22:17:07 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Shokri", "Reza", ""], ["Stronati", "Marco", ""], ["Song", "Congzheng", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1610.05872", "submitter": "Sergey Stavisky", "authors": "David Sussillo, Sergey D. Stavisky, Jonathan C. Kao, Stephen I. Ryu,\n  Krishna V. Shenoy", "title": "Making brain-machine interfaces robust to future neural variability", "comments": "D.S., S.D.S., and J.C.K. contributed equally to this work", "journal-ref": "Nature Communications. 7:13749 (2016)", "doi": "10.1038/ncomms13749", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major hurdle to clinical translation of brain-machine interfaces (BMIs) is\nthat current decoders, which are trained from a small quantity of recent data,\nbecome ineffective when neural recording conditions subsequently change. We\ntested whether a decoder could be made more robust to future neural variability\nby training it to handle a variety of recording conditions sampled from months\nof previously collected data as well as synthetic training data perturbations.\nWe developed a new multiplicative recurrent neural network BMI decoder that\nsuccessfully learned a large variety of neural-to- kinematic mappings and\nbecame more robust with larger training datasets. When tested with a non-human\nprimate preclinical BMI model, this decoder was robust under conditions that\ndisabled a state-of-the-art Kalman filter based decoder. These results validate\na new BMI strategy in which accumulated data history is effectively harnessed,\nand may facilitate reliable daily BMI use by reducing decoder retraining\ndowntime.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 05:32:32 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Sussillo", "David", ""], ["Stavisky", "Sergey D.", ""], ["Kao", "Jonathan C.", ""], ["Ryu", "Stephen I.", ""], ["Shenoy", "Krishna V.", ""]]}, {"id": "1610.05925", "submitter": "Christophe Dupuy", "authors": "Christophe Dupuy (SIERRA), Francis Bach (SIERRA, LIENS)", "title": "Learning Determinantal Point Processes in Sublinear Time", "comments": "Under review for AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of determinantal point processes (DPPs) which can be\nmanipulated for inference and parameter learning in potentially sublinear time\nin the number of items. This class, based on a specific low-rank factorization\nof the marginal kernel, is particularly suited to a subclass of continuous DPPs\nand DPPs defined on exponentially many items. We apply this new class to\nmodelling text documents as sampling a DPP of sentences, and propose a\nconditional maximum likelihood formulation to model topic proportions, which is\nmade possible with no approximation for our class of DPPs. We present an\napplication to document summarization with a DPP on $2^{500}$ items.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 09:18:10 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Dupuy", "Christophe", "", "SIERRA"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1610.05950", "submitter": "Adam Scibior", "authors": "Carl-Johann Simon-Gabriel, Adam \\'Scibior, Ilya Tolstikhin, and\n  Bernhard Sch\\\"olkopf", "title": "Consistent Kernel Mean Estimation for Functions of Random Variables", "comments": "17 pages including appendix", "journal-ref": "NIPS 2016 Proceedings (p. 1732-1740)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a theoretical foundation for non-parametric estimation of\nfunctions of random variables using kernel mean embeddings. We show that for\nany continuous function $f$, consistent estimators of the mean embedding of a\nrandom variable $X$ lead to consistent estimators of the mean embedding of\n$f(X)$. For Mat\\'ern kernels and sufficiently smooth functions we also provide\nrates of convergence. Our results extend to functions of multiple random\nvariables. If the variables are dependent, we require an estimator of the mean\nembedding of their joint distribution as a starting point; if they are\nindependent, it is sufficient to have separate estimators of the mean\nembeddings of their marginal distributions. In either case, our results cover\nboth mean embeddings based on i.i.d. samples as well as \"reduced set\"\nexpansions in terms of dependent expansion points. The latter serves as a\njustification for using such expansions to limit memory resources when applying\nthe approach as a basis for probabilistic programming.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:23:55 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["\u015acibior", "Adam", ""], ["Tolstikhin", "Ilya", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1610.05956", "submitter": "Xiurui Geng", "authors": "Xiurui Geng and Hairong Tang", "title": "Clustering by connection center evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of cluster centers generally depends on the scale that we\nuse to analyze the data to be clustered. Inappropriate scale usually leads to\nunreasonable cluster centers and thus unreasonable results. In this study, we\nfirst consider the similarity of elements in the data as the connectivity of\nnodes in an undirected graph, then present the concept of a connection center\nand regard it as the cluster center of the data. Based on this definition, the\ndetermination of cluster centers and the assignment of class are very simple,\nnatural and effective. One more crucial finding is that the cluster centers of\ndifferent scales can be obtained easily by the different powers of a similarity\nmatrix and the change of power from small to large leads to the dynamic\nevolution of cluster centers from local (microscopic) to global (microscopic).\nFurther, in this process of evolution, the number of categories changes\ndiscontinuously, which means that the presented method can automatically skip\nthe unreasonable number of clusters, suggest appropriate observation scales and\nprovide corresponding cluster results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:52:07 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Geng", "Xiurui", ""], ["Tang", "Hairong", ""]]}, {"id": "1610.06072", "submitter": "Tom Bosc", "authors": "Tom Bosc", "title": "Learning to Learn Neural Networks", "comments": "presented at \"Reasoning, Attention, Memory\" workshop, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning consists in learning learning algorithms. We use a Long Short\nTerm Memory (LSTM) based network to learn to compute on-line updates of the\nparameters of another neural network. These parameters are stored in the cell\nstate of the LSTM. Our framework allows to compare learned algorithms to\nhand-made algorithms within the traditional train and test methodology. In an\nexperiment, we learn a learning algorithm for a one-hidden layer Multi-Layer\nPerceptron (MLP) on non-linearly separable datasets. The learned algorithm is\nable to update parameters of both layers and generalise well on similar\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:46:30 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Bosc", "Tom", ""]]}, {"id": "1610.06145", "submitter": "Fan Zhang", "authors": "Fan Zhang, Chuangqi Wang, Andrew Trapp, Patrick Flaherty", "title": "A global optimization algorithm for sparse mixed membership matrix\n  factorization", "comments": "19 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed membership factorization is a popular approach for analyzing data sets\nthat have within-sample heterogeneity. In recent years, several algorithms have\nbeen developed for mixed membership matrix factorization, but they only\nguarantee estimates from a local optimum. Here, we derive a global optimization\n(GOP) algorithm that provides a guaranteed $\\epsilon$-global optimum for a\nsparse mixed membership matrix factorization problem. We test the algorithm on\nsimulated data and find the algorithm always bounds the global optimum across\nrandom initializations and explores multiple modes efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 18:39:41 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 22:12:17 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Zhang", "Fan", ""], ["Wang", "Chuangqi", ""], ["Trapp", "Andrew", ""], ["Flaherty", "Patrick", ""]]}, {"id": "1610.06194", "submitter": "Michael Minyi Zhang", "authors": "Michael Minyi Zhang, Henry Lam, Lizhen Lin", "title": "Robust and Parallel Bayesian Model Selection", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 127, 2018, Pages\n  229-247, ISSN 0167-9473", "doi": "10.1016/j.csda.2018.05.016", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective and accurate model selection is an important problem in modern data\nanalysis. One of the major challenges is the computational burden required to\nhandle large data sets that cannot be stored or processed on one machine.\nAnother challenge one may encounter is the presence of outliers and\ncontaminations that damage the inference quality. The parallel \"divide and\nconquer\" model selection strategy divides the observations of the full data set\ninto roughly equal subsets and perform inference and model selection\nindependently on each subset. After local subset inference, this method\naggregates the posterior model probabilities or other model/variable selection\ncriteria to obtain a final model by using the notion of geometric median. This\napproach leads to improved concentration in finding the \"correct\" model and\nmodel parameters and also is provably robust to outliers and data\ncontamination.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 20:09:51 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 06:16:26 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 12:56:52 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Michael Minyi", ""], ["Lam", "Henry", ""], ["Lin", "Lizhen", ""]]}, {"id": "1610.06235", "submitter": "Yuri Levin-Schwartz", "authors": "Zois Boukouvalas, Yuri Levin-Schwartz, and Tulay Adali", "title": "Enhancing ICA Performance by Exploiting Sparsity: Application to FMRI\n  Analysis", "comments": "Conference Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Independent component analysis (ICA) is a powerful method for blind source\nseparation based on the assumption that sources are statistically independent.\nThough ICA has proven useful and has been employed in many applications,\ncomplete statistical independence can be too restrictive an assumption in\npractice. Additionally, important prior information about the data, such as\nsparsity, is usually available. Sparsity is a natural property of the data, a\nform of diversity, which, if incorporated into the ICA model, can relax the\nindependence assumption, resulting in an improvement in the overall separation\nperformance. In this work, we propose a new variant of ICA by entropy bound\nminimization (ICA-EBM)-a flexible, yet parameter-free algorithm-through the\ndirect exploitation of sparsity. Using this new SparseICA-EBM algorithm, we\nstudy the synergy of independence and sparsity through simulations on synthetic\nas well as functional magnetic resonance imaging (fMRI)-like data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 21:53:07 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Boukouvalas", "Zois", ""], ["Levin-Schwartz", "Yuri", ""], ["Adali", "Tulay", ""]]}, {"id": "1610.06258", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin\n  Ionescu", "title": "Using Fast Weights to Attend to the Recent Past", "comments": "Added [Schmidhuber 1993] citation to the last paragraph of the\n  introduction. Fixed typo appendix A.1 uniform initialization to 1/\\sqrt{H}", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, research on artificial neural networks was largely restricted\nto systems with only two types of variable: Neural activities that represent\nthe current or recent input and weights that learn to capture regularities\namong inputs, outputs and payoffs. There is no good reason for this\nrestriction. Synapses have dynamics at many different time-scales and this\nsuggests that artificial neural networks might benefit from variables that\nchange slower than activities but much faster than the standard weights. These\n\"fast weights\" can be used to store temporary memories of the recent past and\nthey provide a neurally plausible way of implementing the type of attention to\nthe past that has recently proved very helpful in sequence-to-sequence models.\nBy using fast weights we can avoid the need to store copies of neural activity\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 01:03:20 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 19:53:07 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 00:14:01 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ba", "Jimmy", ""], ["Hinton", "Geoffrey", ""], ["Mnih", "Volodymyr", ""], ["Leibo", "Joel Z.", ""], ["Ionescu", "Catalin", ""]]}, {"id": "1610.06434", "submitter": "Ievgen Redko", "authors": "Ievgen Redko, Youn\\`es Bennani", "title": "Kernel Alignment for Unsupervised Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a human being to extrapolate previously gained knowledge to\nother domains inspired a new family of methods in machine learning called\ntransfer learning. Transfer learning is often based on the assumption that\nobjects in both target and source domains share some common feature and/or data\nspace. In this paper, we propose a simple and intuitive approach that minimizes\niteratively the distance between source and target task distributions by\noptimizing the kernel target alignment (KTA). We show that this procedure is\nsuitable for transfer learning by relating it to Hilbert-Schmidt Independence\nCriterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run\nour method on benchmark computer vision data sets and show that it can\noutperform some state-of-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:37:46 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Redko", "Ievgen", ""], ["Bennani", "Youn\u00e8s", ""]]}, {"id": "1610.06447", "submitter": "Nicolas Papadakis", "authors": "Arnaud Dessein and Nicolas Papadakis and Jean-Luc Rouas", "title": "Regularized Optimal Transport and the Rot Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified framework for smooth convex regularization of\ndiscrete optimal transport problems. In this context, the regularized optimal\ntransport turns out to be equivalent to a matrix nearness problem with respect\nto Bregman divergences. Our framework thus naturally generalizes a previously\nproposed regularization based on the Boltzmann-Shannon entropy related to the\nKullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We\ncall the regularized optimal transport distance the rot mover's distance in\nreference to the classical earth mover's distance. We develop two generic\nschemes that we respectively call the alternate scaling algorithm and the\nnon-negative alternate scaling algorithm, to compute efficiently the\nregularized optimal plans depending on whether the domain of the regularizer\nlies within the non-negative orthant or not. These schemes are based on\nDykstra's algorithm with alternate Bregman projections, and further exploit the\nNewton-Raphson method when applied to separable divergences. We enhance the\nseparable case with a sparse extension to deal with high data dimensions. We\nalso instantiate our proposed framework and discuss the inherent specificities\nfor well-known regularizers and statistical divergences in the machine learning\nand information geometry communities. Finally, we demonstrate the merits of our\nmethods with experiments using synthetic data to illustrate the effect of\ndifferent regularizers and penalties on the solutions, as well as real-world\ndata for a pattern recognition application to audio scene classification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:49:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 11:05:10 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 11:47:37 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 20:01:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Dessein", "Arnaud", ""], ["Papadakis", "Nicolas", ""], ["Rouas", "Jean-Luc", ""]]}, {"id": "1610.06453", "submitter": "Ye Ye", "authors": "Stephanie Allen, David Madras, Ye Ye, Greg Zanotti", "title": "Change-point Detection Methods for Body-Worn Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body-worn video (BWV) cameras are increasingly utilized by police departments\nto provide a record of police-public interactions. However, large-scale BWV\ndeployment produces terabytes of data per week, necessitating the development\nof effective computational methods to identify salient changes in video. In\nwork carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel\ntwo-stage framework for video change-point detection. First, we employ\nstate-of-the-art machine learning methods including convolutional neural\nnetworks and support vector machines for scene classification. We then develop\nand compare change-point detection algorithms utilizing mean squared-error\nminimization, forecasting methods, hidden Markov models, and maximum likelihood\nestimation to identify noteworthy changes. We test our framework on detection\nof vehicle exits and entrances in a BWV data set provided by the Los Angeles\nPolice Department and achieve over 90% recall and nearly 70% precision --\ndemonstrating robustness to rapid scene changes, extreme luminance differences,\nand frequent camera occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:11:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Allen", "Stephanie", ""], ["Madras", "David", ""], ["Ye", "Ye", ""], ["Zanotti", "Greg", ""]]}, {"id": "1610.06454", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Reasoning with Memory Augmented Neural Networks for Language\n  Comprehension", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is an important cognitive process that supports human\nreasoning. In this paper, we introduce a computational hypothesis testing\napproach based on memory augmented neural networks. Our approach involves a\nhypothesis testing loop that reconsiders and progressively refines a previously\nformed hypothesis in order to generate new hypotheses to test. We apply the\nproposed approach to language comprehension task by using Neural Semantic\nEncoders (NSE). Our NSE models achieve the state-of-the-art results showing an\nabsolute improvement of 1.2% to 2.6% accuracy over previous results obtained by\nsingle and ensemble systems on standard machine comprehension benchmarks such\nas the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:17:04 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:06:17 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1610.06461", "submitter": "Abbas Kazemipour", "authors": "Abbas Kazemipour, Ji Liu, Patrick Kanold, Min Wu, Behtash Babadi", "title": "Efficient Estimation of Compressible State-Space Models with Application\n  to Calcium Signal Deconvolution", "comments": "2016 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Dec. 7-9, 2016, Washington D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT math.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider linear state-space models with compressible\ninnovations and convergent transition matrices in order to model\nspatiotemporally sparse transient events. We perform parameter and state\nestimation using a dynamic compressed sensing framework and develop an\nefficient solution consisting of two nested Expectation-Maximization (EM)\nalgorithms. Under suitable sparsity assumptions on the innovations, we prove\nrecovery guarantees and derive confidence bounds for the state estimates. We\nprovide simulation studies as well as application to spike deconvolution from\ncalcium imaging data which verify our theoretical results and show significant\nimprovement over existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:37:53 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Liu", "Ji", ""], ["Kanold", "Patrick", ""], ["Wu", "Min", ""], ["Babadi", "Behtash", ""]]}, {"id": "1610.06462", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael Gutmann, Aki Vehtari, Pekka Marttinen", "title": "Gaussian process modeling in approximate Bayesian computation to\n  estimate horizontal gene transfer in bacteria", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) can be used for model fitting when the\nlikelihood function is intractable but simulating from the model is feasible.\nHowever, even a single evaluation of a complex model may take several hours,\nlimiting the number of model evaluations available. Modelling the discrepancy\nbetween the simulated and observed data using a Gaussian process (GP) can be\nused to reduce the number of model evaluations required by ABC, but the\nsensitivity of this approach to a specific GP formulation has not yet been\nthoroughly investigated. We begin with a comprehensive empirical evaluation of\nusing GPs in ABC, including various transformations of the discrepancies and\ntwo novel GP formulations. Our results indicate the choice of GP may\nsignificantly affect the accuracy of the estimated posterior distribution.\nSelection of an appropriate GP model is thus important. We formulate expected\nutility to measure the accuracy of classifying discrepancies below or above the\nABC threshold, and show that it can be used to automate the GP model selection\nstep. Finally, based on the understanding gained with toy examples, we fit a\npopulation genetic model for bacteria, providing insight into horizontal gene\ntransfer events within the population and from external origins.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:39:15 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:57:22 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 12:24:27 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1610.06525", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Matthias Grossglauser", "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how users navigate in a network is of high interest in many\napplications. We consider a setting where only aggregate node-level traffic is\nobserved and tackle the task of learning edge transition probabilities. We cast\nit as a preference learning problem, and we study a model where choices follow\nLuce's axiom. In this case, the $O(n)$ marginal counts of node visits are a\nsufficient statistic for the $O(n^2)$ transition probabilities. We show how to\nmake the inference problem well-posed regardless of the network's structure,\nand we present ChoiceRank, an iterative algorithm that scales to networks that\ncontains billions of nodes and edges. We apply the model to two clickstream\ndatasets and show that it successfully recovers the transition probabilities\nusing only the network structure and marginal (node-level) traffic data.\nFinally, we also consider an application to mobility networks and apply the\nmodel to one year of rides on New York City's bicycle-sharing system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 18:19:07 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 15:14:54 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1610.06545", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Maxime Oquab", "title": "Revisiting Classifier Two-Sample Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of two-sample tests is to assess whether two samples, $S_P \\sim P^n$\nand $S_Q \\sim Q^m$, are drawn from the same distribution. Perhaps intriguingly,\none relatively unexplored method to build two-sample tests is the use of binary\nclassifiers. In particular, construct a dataset by pairing the $n$ examples in\n$S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a\nnegative label. If the null hypothesis \"$P = Q$\" is true, then the\nclassification accuracy of a binary classifier on a held-out subset of this\ndataset should remain near chance-level. As we will show, such Classifier\nTwo-Sample Tests (C2ST) learn a suitable representation of the data on the fly,\nreturn test statistics in interpretable units, have a simple null distribution,\nand their predictive uncertainty allow to interpret where $P$ and $Q$ differ.\nThe goal of this paper is to establish the properties, performance, and uses of\nC2ST. First, we analyze their main theoretical properties. Second, we compare\ntheir performance against a variety of state-of-the-art alternatives. Third, we\npropose their use to evaluate the sample quality of generative models with\nintractable likelihoods, such as Generative Adversarial Networks (GANs).\nFourth, we showcase the novel application of GANs together with C2ST for causal\ndiscovery.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:16:10 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 17:13:30 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 14:51:43 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 17:18:29 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Lopez-Paz", "David", ""], ["Oquab", "Maxime", ""]]}, {"id": "1610.06551", "submitter": "Yanning Shen", "authors": "Yanning Shen, Brian Baingana, Georgios B. Giannakis", "title": "Nonlinear Structural Vector Autoregressive Models for Inferring\n  Effective Brain Network Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models (SEMs) and vector autoregressive models (VARMs)\nare two broad families of approaches that have been shown useful in effective\nbrain connectivity studies. While VARMs postulate that a given region of\ninterest in the brain is directionally connected to another one by virtue of\ntime-lagged influences, SEMs assert that causal dependencies arise due to\ncontemporaneous effects, and may even be adopted when nodal measurements are\nnot necessarily multivariate time series. To unify these complementary\nperspectives, linear structural vector autoregressive models (SVARMs) that\nleverage both contemporaneous and time-lagged nodal data have recently been put\nforth. Albeit simple and tractable, linear SVARMs are quite limited since they\nare incapable of modeling nonlinear dependencies between neuronal time series.\nTo this end, the overarching goal of the present paper is to considerably\nbroaden the span of linear SVARMs by capturing nonlinearities through kernels,\nwhich have recently emerged as a powerful nonlinear modeling framework in\ncanonical machine learning tasks, e.g., regression, classification, and\ndimensionality reduction. The merits of kernel-based methods are extended here\nto the task of learning the effective brain connectivity, and an efficient\nregularized estimator is put forth to leverage the edge sparsity inherent to\nreal-world complex networks. Judicious kernel choice from a preselected\ndictionary of kernels is also addressed using a data-driven approach. Extensive\nnumerical tests on ECoG data captured through a study on epileptic seizures\ndemonstrate that it is possible to unveil previously unknown causal links\nbetween brain regions of interest.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:37:46 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Shen", "Yanning", ""], ["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1610.06603", "submitter": "Wei Hu", "authors": "Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, Pinyan Lu", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "comments": "Published in Neural Information Processing Systems (NIPS) 2016. New\n  in this version: a minor bug fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic combinatorial multi-armed bandit\n(CMAB) framework that allows a general nonlinear reward function, whose\nexpected value may not depend only on the means of the input random variables\nbut possibly on the entire distributions of these variables. Our framework\nenables a much larger class of reward functions such as the $\\max()$ function\nand nonlinear utility functions. Existing techniques relying on accurate\nestimations of the means of random variables, such as the upper confidence\nbound (UCB) technique, do not work directly on these functions. We propose a\nnew algorithm called stochastically dominant confidence bound (SDCB), which\nestimates the distributions of underlying random variables and their\nstochastically dominant confidence bounds. We prove that SDCB can achieve\n$O(\\log{T})$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$\ndistribution-independent regret, where $T$ is the time horizon. We apply our\nresults to the $K$-MAX problem and expected utility maximization problems. In\nparticular, for $K$-MAX, we provide the first polynomial-time approximation\nscheme (PTAS) for its offline problem, and give the first $\\tilde{O}(\\sqrt T)$\nbound on the $(1-\\epsilon)$-approximation regret of its online problem, for any\n$\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 20:54:41 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 18:18:05 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 04:49:22 GMT"}, {"version": "v4", "created": "Fri, 20 Jul 2018 17:38:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chen", "Wei", ""], ["Hu", "Wei", ""], ["Li", "Fu", ""], ["Li", "Jian", ""], ["Liu", "Yu", ""], ["Lu", "Pinyan", ""]]}, {"id": "1610.06656", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Srinadh Bhojanapalli, Sujay Sanghavi, Alexandros G.\n  Dimakis", "title": "Single Pass PCA of Matrix Products", "comments": "24 pages, 4 figures, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new algorithm for computing a low rank\napproximation of the product $A^TB$ by taking only a single pass of the two\nmatrices $A$ and $B$. The straightforward way to do this is to (a) first sketch\n$A$ and $B$ individually, and then (b) find the top components using PCA on the\nsketch. Our algorithm in contrast retains additional summary information about\n$A,B$ (e.g. row and column norms etc.) and uses this additional information to\nobtain an improved approximation from the sketches. Our main analytical result\nestablishes a comparable spectral norm guarantee to existing two-pass methods;\nin addition we also provide results from an Apache Spark implementation that\nshows better computational and statistical performance on real-world and\nsynthetic evaluation datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 02:45:46 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 13:58:24 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Wu", "Shanshan", ""], ["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1610.06664", "submitter": "Changyou Chen", "authors": "Changyou Chen and Nan Ding and Chunyuan Li and Yizhe Zhang and\n  Lawrence Carin", "title": "Stochastic Gradient MCMC with Stale Gradients", "comments": "NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient MCMC (SG-MCMC) has played an important role in\nlarge-scale Bayesian learning, with well-developed theoretical convergence\nproperties. In such applications of SG-MCMC, it is becoming increasingly\npopular to employ distributed systems, where stochastic gradients are computed\nbased on some outdated parameters, yielding what are termed stale gradients.\nWhile stale gradients could be directly used in SG-MCMC, their impact on\nconvergence properties has not been well studied. In this paper we develop\ntheory to show that while the bias and MSE of an SG-MCMC algorithm depend on\nthe staleness of stochastic gradients, its estimation variance (relative to the\nexpected estimate, based on a prescribed number of samples) is independent of\nit. In a simple Bayesian distributed system with SG-MCMC, where stale gradients\nare computed asynchronously by a set of workers, our theory indicates a linear\nspeedup on the decrease of estimation variance w.r.t. the number of workers.\nExperiments on synthetic data and deep neural networks validate our theory,\ndemonstrating the effectiveness and scalability of SG-MCMC with stale\ngradients.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:18:11 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Chen", "Changyou", ""], ["Ding", "Nan", ""], ["Li", "Chunyuan", ""], ["Zhang", "Yizhe", ""], ["Carin", "Lawrence", ""]]}, {"id": "1610.06665", "submitter": "Changyou Chen", "authors": "Changyou Chen and Nan Ding and Lawrence Carin", "title": "On the Convergence of Stochastic Gradient MCMC Algorithms with\n  High-Order Integrators", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Bayesian learning with large-scale data have witnessed\nemergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic\ngradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC\n(SGHMC), and the stochastic gradient thermostat. While finite-time convergence\nproperties of the SGLD with a 1st-order Euler integrator have recently been\nstudied, corresponding theory for general SG-MCMCs has not been explored. In\nthis paper we consider general SG-MCMCs with high-order integrators, and\ndevelop theory to analyze finite-time convergence properties and their\nasymptotic invariant measures. Our theoretical results show faster convergence\nrates and more accurate invariant measures for SG-MCMCs with higher-order\nintegrators. For example, with the proposed efficient 2nd-order symmetric\nsplitting integrator, the {\\em mean square error} (MSE) of the posterior\naverage for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$\niterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler\nintegrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs\nare also developed, with the same convergence rates as their fixed-step-size\ncounterparts for a specific decreasing sequence. Experiments on both synthetic\nand real datasets verify our theory, and show advantages of the proposed method\nin two large-scale real applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:28:15 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Chen", "Changyou", ""], ["Ding", "Nan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1610.06700", "submitter": "Hao Tang", "authors": "Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu", "title": "End-to-End Training Approaches for Discriminative Segmental Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on discriminative segmental models has shown that they can\nachieve competitive speech recognition performance, using features based on\ndeep neural frame classifiers. However, segmental models can be more\nchallenging to train than standard frame-based approaches. While some segmental\nmodels have been successfully trained end to end, there is a lack of\nunderstanding of their training under different settings and with different\nlosses.\n  We investigate a model class based on recent successful approaches,\nconsisting of a linear model that combines segmental features based on an LSTM\nframe classifier. Similarly to hybrid HMM-neural network models, segmental\nmodels of this class can be trained in two stages (frame classifier training\nfollowed by linear segmental model weight training), end to end (joint training\nof both frame classifier and linear weights), or with end-to-end fine-tuning\nafter two-stage training.\n  We study segmental models trained end to end with hinge loss, log loss,\nlatent hinge loss, and marginal log loss. We consider several losses for the\ncase where training alignments are available as well as where they are not.\n  We find that in general, marginal log loss provides the most consistent\nstrong performance without requiring ground-truth alignments. We also find that\ntraining with dropout is very important in obtaining good performance with\nend-to-end training. Finally, the best results are typically obtained by a\ncombination of two-stage training and fine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 08:45:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Weiran", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1610.06731", "submitter": "Evgeny Burnaev", "authors": "Alexey Zaytsev and Evgeny Burnaev", "title": "Minimax Error of Interpolation and Optimal Design of Experiments for\n  Variable Fidelity Data", "comments": "25 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering problems often involve data sources of variable fidelity with\ndifferent costs of obtaining an observation. In particular, one can use both a\ncheap low fidelity function (e.g. a computational experiment with a CFD code)\nand an expensive high fidelity function (e.g. a wind tunnel experiment) to\ngenerate a data sample in order to construct a regression model of a high\nfidelity function. The key question in this setting is how the sizes of the\nhigh and low fidelity data samples should be selected in order to stay within a\ngiven computational budget and maximize accuracy of the regression model prior\nto committing resources on data acquisition.\n  In this paper we obtain minimax interpolation errors for single and variable\nfidelity scenarios for a multivariate Gaussian process regression. Evaluation\nof the minimax errors allows us to identify cases when the variable fidelity\ndata provides better interpolation accuracy than the exclusively high fidelity\ndata for the same computational budget.\n  These results allow us to calculate the optimal shares of variable fidelity\ndata samples under the given computational budget constraint. Real and\nsynthetic data experiments suggest that using the obtained optimal shares often\noutperforms natural heuristics in terms of the regression accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 10:24:08 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:02:57 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 10:58:19 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Zaytsev", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1610.06761", "submitter": "Erik Rodner", "authors": "Erik Rodner, Bj\\\"orn Barz, Yanira Guanche, Milan Flach, Miguel\n  Mahecha, Paul Bodesheim, Markus Reichstein, Joachim Denzler", "title": "Maximally Divergent Intervals for Anomaly Detection", "comments": "ICML Workshop on Anomaly Detection", "journal-ref": null, "doi": "10.17871/BACI_ICML2016_Rodner", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for batch anomaly detection in multivariate time\nseries. Our methods are based on maximizing the Kullback-Leibler divergence\nbetween the data distribution within and outside an interval of the time\nseries. An empirical analysis shows the benefits of our algorithms compared to\nmethods that treat each time step independently from each other without\noptimizing with respect to all possible intervals.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:30:30 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rodner", "Erik", ""], ["Barz", "Bj\u00f6rn", ""], ["Guanche", "Yanira", ""], ["Flach", "Milan", ""], ["Mahecha", "Miguel", ""], ["Bodesheim", "Paul", ""], ["Reichstein", "Markus", ""], ["Denzler", "Joachim", ""]]}, {"id": "1610.06773", "submitter": "Frank Noe", "authors": "Hao Wu, Feliks N\\\"uske, Fabian Paul, Stefan Klus, Peter Koltai and\n  Frank No\\'e", "title": "Variational Koopman models: slow collective variables and molecular\n  kinetics from short off-equilibrium simulations", "comments": null, "journal-ref": null, "doi": "10.1063/1.4979344", "report-no": null, "categories": "stat.ML physics.bio-ph physics.chem-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov state models (MSMs) and Master equation models are popular approaches\nto approximate molecular kinetics, equilibria, metastable states, and reaction\ncoordinates in terms of a state space discretization usually obtained by\nclustering. Recently, a powerful generalization of MSMs has been introduced,\nthe variational approach (VA) of molecular kinetics and its special case the\ntime-lagged independent component analysis (TICA), which allow us to\napproximate slow collective variables and molecular kinetics by linear\ncombinations of smooth basis functions or order parameters. While it is known\nhow to estimate MSMs from trajectories whose starting points are not sampled\nfrom an equilibrium ensemble, this has not yet been the case for TICA and the\nVA. Previous estimates from short trajectories, have been strongly biased and\nthus not variationally optimal. Here, we employ Koopman operator theory and\nideas from dynamic mode decomposition (DMD) to extend the VA and TICA to\nnon-equilibrium data. The main insight is that the VA and TICA provide a\ncoefficient matrix that we call Koopman model, as it approximates the\nunderlying dynamical (Koopman) operator in conjunction with the basis set used.\nThis Koopman model can be used to compute a stationary vector to reweight the\ndata to equilibrium. From such a Koopman-reweighted sample, equilibrium\nexpectation values and variationally optimal reversible Koopman models can be\nconstructed even with short simulations. The Koopman model can be used to\npropagate densities, and its eigenvalue decomposition provide estimates of\nrelaxation timescales and slow collective variables for dimension reduction.\nKoopman models are generalizations of Markov state models, TICA and the linear\nVA and allow molecular kinetics to be described without a cluster\ndiscretization.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 16:15:09 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 16:46:32 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wu", "Hao", ""], ["N\u00fcske", "Feliks", ""], ["Paul", "Fabian", ""], ["Klus", "Stefan", ""], ["Koltai", "Peter", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1610.06806", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser. M. Narabadi and Alfred O. Hero", "title": "Robust training on approximated minimal-entropy set", "comments": "13 pages; Accepted in Transaction on Signal Processing, 2016. arXiv\n  admin note: text overlap with arXiv:1507.04540", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to learn a robust large-margin\nbinary classifier when corrupt measurements, called anomalies, caused by sensor\nfailure might be present in the training set. The goal is to minimize the\ngeneralization error of the classifier on non-corrupted measurements while\ncontrolling the false alarm rate associated with anomalous samples. By\nincorporating a non-parametric regularizer based on an empirical entropy\nestimator, we propose a Geometric-Entropy-Minimization regularized Maximum\nEntropy Discrimination (GEM-MED) method to learn to classify and detect\nanomalies in a joint manner. We demonstrate using simulated data and a real\nmultimodal data set. Our GEM-MED method can yield improved performance over\nprevious robust classification methods in terms of both classification accuracy\nand anomaly detection rate.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 14:38:38 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Xie", "Tianpei", ""], ["Narabadi", "Nasser. M.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1610.06811", "submitter": "Carlos M. Ala\\'iz", "authors": "Carlos M. Ala\\'iz, Micha\\\"el Fanuel, Johan A. K. Suykens", "title": "Convex Formulation for Kernel PCA and its Use in Semi-Supervised\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 99, 1-7\n  (2017)", "doi": "10.1109/TNNLS.2017.2709838", "report-no": "ESAT-SISTA: 16-166", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Kernel PCA is reinterpreted as the solution to a convex\noptimization problem. Actually, there is a constrained convex problem for each\nprincipal component, so that the constraints guarantee that the principal\ncomponent is indeed a solution, and not a mere saddle point. Although these\ninsights do not imply any algorithmic improvement, they can be used to further\nunderstand the method, formulate possible extensions and properly address them.\nAs an example, a new convex optimization problem for semi-supervised\nclassification is proposed, which seems particularly well-suited whenever the\nnumber of known labels is small. Our formulation resembles a Least Squares SVM\nproblem with a regularization parameter multiplied by a negative sign, combined\nwith a variational principle for Kernel PCA. Our primal optimization principle\nfor semi-supervised learning is solved in terms of the Lagrange multipliers.\nNumerical experiments in several classification tasks illustrate the\nperformance of the proposed model in problems with only a few labeled data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 14:55:48 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Ala\u00edz", "Carlos M.", ""], ["Fanuel", "Micha\u00ebl", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1610.06848", "submitter": "Daniel Seita", "authors": "Daniel Seita, Xinlei Pan, Haoyu Chen, John Canny", "title": "An Efficient Minibatch Acceptance Test for Metropolis-Hastings", "comments": "Final version for UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Metropolis-Hastings method for large datasets that uses\nsmall expected-size minibatches of data. Previous work on reducing the cost of\nMetropolis-Hastings tests yield variable data consumed per sample, with only\nconstant factor reductions versus using the full dataset for each sample. Here\nwe present a method that can be tuned to provide arbitrarily small batch sizes,\nby adjusting either proposal step size or temperature. Our test uses the\nnoise-tolerant Barker acceptance test with a novel additive correction\nvariable. The resulting test has similar cost to a normal SGD update. Our\nexperiments demonstrate several order-of-magnitude speedups over previous work.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 00:19:25 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 21:08:51 GMT"}, {"version": "v3", "created": "Sun, 9 Jul 2017 16:36:03 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Seita", "Daniel", ""], ["Pan", "Xinlei", ""], ["Chen", "Haoyu", ""], ["Canny", "John", ""]]}, {"id": "1610.06902", "submitter": "Christian Weiss", "authors": "Christian Weiss and Abdelhak M. Zoubir", "title": "Dictionary Learning Strategies for Compressed Fiber Sensing Using a\n  Probabilistic Sparse Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse estimation and dictionary learning framework for\ncompressed fiber sensing based on a probabilistic hierarchical sparse model. To\nhandle severe dictionary coherence, selective shrinkage is achieved using a\nWeibull prior, which can be related to non-convex optimization with $p$-norm\nconstraints for $0 < p < 1$. In addition, we leverage the specific dictionary\nstructure to promote collective shrinkage based on a local similarity model.\nThis is incorporated in form of a kernel function in the joint prior density of\nthe sparse coefficients, thereby establishing a Markov random field-relation.\nApproximate inference is accomplished using a hybrid technique that combines\nHamilton Monte Carlo and Gibbs sampling. To estimate the dictionary parameter,\nwe pursue two strategies, relying on either a deterministic or a probabilistic\nmodel for the dictionary parameter. In the first strategy, the parameter is\nestimated based on alternating estimation. In the second strategy, it is\njointly estimated along with the sparse coefficients. The performance is\nevaluated in comparison to an existing method in various scenarios using\nsimulations and experimental data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:27:48 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Weiss", "Christian", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1610.06940", "submitter": "Xiaowei Huang", "authors": "Xiaowei Huang and Marta Kwiatkowska and Sen Wang and Min Wu", "title": "Safety Verification of Deep Neural Networks", "comments": "To appear as invited paper at CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 20:16:16 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 16:05:08 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 10:16:50 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Huang", "Xiaowei", ""], ["Kwiatkowska", "Marta", ""], ["Wang", "Sen", ""], ["Wu", "Min", ""]]}, {"id": "1610.06949", "submitter": "Stefan Bauer", "authors": "Nico S. Gorbach, Stefan Bauer, Joachim M. Buhmann", "title": "Mean-Field Variational Inference for Gradient Matching with Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient matching with Gaussian processes is a promising tool for learning\nparameters of ordinary differential equations (ODE's). The essence of gradient\nmatching is to model the prior over state variables as a Gaussian process which\nimplies that the joint distribution given the ODE's and GP kernels is also\nGaussian distributed. The state-derivatives are integrated out analytically\nsince they are modelled as latent variables. However, the state variables\nthemselves are also latent variables because they are contaminated by noise.\nPrevious work sampled the state variables since integrating them out is\n\\textit{not} analytically tractable. In this paper we use mean-field\napproximation to establish tight variational lower bounds that decouple state\nvariables and are therefore, in contrast to the integral over state variables,\nanalytically tractable and even concave for a restricted family of ODE's,\nincluding nonlinear and periodic ODE's. Such variational lower bounds\nfacilitate \"hill climbing\" to determine the maximum a posteriori estimate of\nODE parameters. An additional advantage of our approach over sampling methods\nis the determination of a proxy to the intractable posterior distribution over\nstate variables given observations and the ODE's.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 20:50:47 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Gorbach", "Nico S.", ""], ["Bauer", "Stefan", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1610.06972", "submitter": "Himabindu Lakkaraju", "authors": "Himabindu Lakkaraju, Cynthia Rudin", "title": "Learning Cost-Effective Treatment Regimes using Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision makers, such as doctors and judges, make crucial decisions such as\nrecommending treatments to patients, and granting bails to defendants on a\ndaily basis. Such decisions typically involve weighting the potential benefits\nof taking an action against the costs involved. In this work, we aim to\nautomate this task of learning \\emph{cost-effective, interpretable and\nactionable treatment regimes}. We formulate this as a problem of learning a\ndecision list -- a sequence of if-then-else rules -- which maps characteristics\nof subjects (eg., diagnostic test results of patients) to treatments. We\npropose a novel objective to construct a decision list which maximizes outcomes\nfor the population, and minimizes overall costs. We model the problem of\nlearning such a list as a Markov Decision Process (MDP) and employ a variant of\nthe Upper Confidence Bound for Trees (UCT) strategy which leverages customized\nchecks for pruning the search space effectively. Experimental results on real\nworld observational data capturing judicial bail decisions and treatment\nrecommendations for asthma patients demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 23:17:03 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Lakkaraju", "Himabindu", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1610.07104", "submitter": "Zois Boukouvalas", "authors": "Zois Boukouvalas, Rami Mowakeaa, Geng-Shen Fu, Tulay Adali", "title": "Independent Component Analysis by Entropy Maximization with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is the most popular method for blind\nsource separation (BSS) with a diverse set of applications, such as biomedical\nsignal processing, video and image analysis, and communications. Maximum\nlikelihood (ML), an optimal theoretical framework for ICA, requires knowledge\nof the true underlying probability density function (PDF) of the latent\nsources, which, in many applications, is unknown. ICA algorithms cast in the ML\nframework often deviate from its theoretical optimality properties due to poor\nestimation of the source PDF. Therefore, accurate estimation of source PDFs is\ncritical in order to avoid model mismatch and poor ICA performance. In this\npaper, we propose a new and efficient ICA algorithm based on entropy\nmaximization with kernels, (ICA-EMK), which uses both global and local\nmeasuring functions as constraints to dynamically estimate the PDF of the\nsources with reasonable complexity. In addition, the new algorithm performs\noptimization with respect to each of the cost function gradient directions\nseparately, enabling parallel implementations on multi-core computers. We\ndemonstrate the superior performance of ICA-EMK over competing ICA algorithms\nusing simulated as well as real-world data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 23:08:01 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Boukouvalas", "Zois", ""], ["Mowakeaa", "Rami", ""], ["Fu", "Geng-Shen", ""], ["Adali", "Tulay", ""]]}, {"id": "1610.07108", "submitter": "Samet Oymak", "authors": "Samet Oymak, Mahdi Soltanolkotabi", "title": "Fast and Reliable Parameter Estimation from Nonlinear Observations", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of recovering a structured but unknown\nparameter ${\\bf{\\theta}}^*$ from $n$ nonlinear observations of the form\n$y_i=f(\\langle {\\bf{x}}_i,{\\bf{\\theta}}^*\\rangle)$ for $i=1,2,\\ldots,n$. We\ndevelop a framework for characterizing time-data tradeoffs for a variety of\nparameter estimation algorithms when the nonlinear function $f$ is unknown.\nThis framework includes many popular heuristics such as projected/proximal\ngradient descent and stochastic schemes. For example, we show that a projected\ngradient descent scheme converges at a linear rate to a reliable solution with\na near minimal number of samples. We provide a sharp characterization of the\nconvergence rate of such algorithms as a function of sample size, amount of\na-prior knowledge available about the parameter and a measure of the\nnonlinearity of the function $f$. These results provide a precise understanding\nof the various tradeoffs involved between statistical and computational\nresources as well as a-prior side information available for such nonlinear\nparameter estimation problems.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 00:36:12 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Oymak", "Samet", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1610.07116", "submitter": "Bowei Yan", "authors": "Bowei Yan, Oluwasanmi Koyejo, Kai Zhong, Pradeep Ravikumar", "title": "Online Classification with Complex Metrics", "comments": "An error was found in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework and analysis of consistent binary classification for\ncomplex and non-decomposable performance metrics such as the F-measure and the\nJaccard measure. The proposed framework is general, as it applies to both batch\nand online learning, and to both linear and non-linear models. Our work follows\nrecent results showing that the Bayes optimal classifier for many complex\nmetrics is given by a thresholding of the conditional probability of the\npositive class. This manuscript extends this thresholding characterization --\nshowing that the utility is strictly locally quasi-concave with respect to the\nthreshold for a wide range of models and performance metrics. This, in turn,\nmotivates simple normalized gradient ascent updates for threshold estimation.\nWe present a finite-sample regret analysis for the resulting procedure. In\nparticular, the risk for the batch case converges to the Bayes risk at the same\nrate as that of the underlying conditional probability estimation, and the risk\nof proposed online algorithm converges at a rate that depends on the\nconditional probability estimation risk. For instance, in the special case\nwhere the conditional probability model is logistic regression, our procedure\nachieves $O(\\frac{1}{\\sqrt{n}})$ sample complexity, both for batch and online\ntraining. Empirical evaluation shows that the proposed algorithms out-perform\nalternatives in practice, with comparable or better prediction performance and\nreduced run time for various metrics and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 02:56:03 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 17:55:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yan", "Bowei", ""], ["Koyejo", "Oluwasanmi", ""], ["Zhong", "Kai", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1610.07119", "submitter": "Yi Tay", "authors": "Minh C. Phan, Yi Tay, Tuan-Anh Nguyen Pham", "title": "Cross Device Matching for Online Advertising with Neural Feature\n  Ensembles : First Place Solution at CIKM Cup 2016", "comments": "4 pages Competition Report for CIKM Cup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the 1st place winning approach for the CIKM Cup 2016 Challenge.\nIn this paper, we provide an approach to reasonably identify same users across\nmultiple devices based on browsing logs. Our approach regards a candidate\nranking problem as pairwise classification and utilizes an unsupervised neural\nfeature ensemble approach to learn latent features of users. Combined with\ntraditional hand crafted features, each user pair feature is fed into a\nsupervised classifier in order to perform pairwise classification. Lastly, we\npropose supervised and unsupervised inference techniques.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 03:25:05 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 03:33:03 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Phan", "Minh C.", ""], ["Tay", "Yi", ""], ["Pham", "Tuan-Anh Nguyen", ""]]}, {"id": "1610.07161", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel,\n  Karlheinz Meier", "title": "Stochastic inference with spiking neurons in the high-conductance state", "comments": null, "journal-ref": "Phys. Rev. E 94, 042312 (2016)", "doi": "10.1103/PhysRevE.94.042312", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly variable dynamics of neocortical circuits observed in vivo have\nbeen hypothesized to represent a signature of ongoing stochastic inference but\nstand in apparent contrast to the deterministic response of neurons measured in\nvitro. Based on a propagation of the membrane autocorrelation across spike\nbursts, we provide an analytical derivation of the neural activation function\nthat holds for a large parameter space, including the high-conductance state.\nOn this basis, we show how an ensemble of leaky integrate-and-fire neurons with\nconductance-based synapses embedded in a spiking environment can attain the\ncorrect firing statistics for sampling from a well-defined target distribution.\nFor recurrent networks, we examine convergence toward stationarity in computer\nsimulations and demonstrate sample-based Bayesian inference in a mixed\ngraphical model. This points to a new computational role of high-conductance\nstates and establishes a rigorous link between deterministic neuron models and\nfunctional stochastic dynamics on the network level.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 12:27:05 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Bill", "Johannes", ""], ["Bytschok", "Ilja", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1610.07187", "submitter": "Adam Gonczarek", "authors": "Adam Gonczarek, Jakub M. Tomczak, Szymon Zar\\k{e}ba, Joanna Kaczmar,\n  Piotr D\\k{a}browski, Micha{\\l} J. Walczak", "title": "Learning Deep Architectures for Interaction Prediction in\n  Structure-based Virtual Screening", "comments": "Workshop on Machine Learning in Computational Biology. 30th\n  Conference on Neural Information Processing Systems (NIPS 2016), Barcelona,\n  Spain Extended version published in Computers in Biology and Medicine and\n  available online:\n  http://www.sciencedirect.com/science/article/pii/S0010482517302974", "journal-ref": null, "doi": "10.1016/j.compbiomed.2017.09.007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning architecture for structure-based virtual\nscreening that generates fixed-sized fingerprints of proteins and small\nmolecules by applying learnable atom convolution and softmax operations to each\ncompound separately. These fingerprints are further transformed non-linearly,\ntheir inner-product is calculated and used to predict the binding potential.\nMoreover, we show that widely used benchmark datasets may be insufficient for\ntesting structure-based virtual screening methods that utilize machine\nlearning. Therefore, we introduce a new benchmark dataset, which we constructed\nbased on DUD-E and PDBBind databases.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 15:51:46 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 14:14:51 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 09:52:06 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Gonczarek", "Adam", ""], ["Tomczak", "Jakub M.", ""], ["Zar\u0119ba", "Szymon", ""], ["Kaczmar", "Joanna", ""], ["D\u0105browski", "Piotr", ""], ["Walczak", "Micha\u0142 J.", ""]]}, {"id": "1610.07193", "submitter": "Benjamin Guedj", "authors": "Pierre Alquier and Benjamin Guedj", "title": "Simpler PAC-Bayesian Bounds for Hostile Data", "comments": "18 pages", "journal-ref": "Machine Learning (2018), vol. 107 (5), 887--902", "doi": "10.1007/s10994-017-5690-0", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PAC-Bayesian learning bounds are of the utmost interest to the learning\ncommunity. Their role is to connect the generalization ability of an\naggregation distribution $\\rho$ to its empirical risk and to its\nKullback-Leibler divergence with respect to some prior distribution $\\pi$.\nUnfortunately, most of the available bounds typically rely on heavy assumptions\nsuch as boundedness and independence of the observations. This paper aims at\nrelaxing these constraints and provides PAC-Bayesian learning bounds that hold\nfor dependent, heavy-tailed observations (hereafter referred to as\n\\emph{hostile data}). In these bounds the Kullack-Leibler divergence is\nreplaced with a general version of Csisz\\'ar's $f$-divergence. We prove a\ngeneral PAC-Bayesian bound, and show how to use it in various hostile settings.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 16:20:46 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 05:54:26 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Alquier", "Pierre", ""], ["Guedj", "Benjamin", ""]]}, {"id": "1610.07216", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Samaneh Ebrahimi and Kamran Paynabar", "title": "Inertial Regularization and Selection (IRS): Sequential Regression in\n  High-Dimension and Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new sequential regression modeling approach for\ndata streams. Data streams are commonly found around us, e.g in a retail\nenterprise sales data is continuously collected every day. A demand forecasting\nmodel is an important outcome from the data that needs to be continuously\nupdated with the new incoming data. The main challenge in such modeling arises\nwhen there is a) high dimensional and sparsity, b) need for an adaptive use of\nprior knowledge, and/or c) structural changes in the system. The proposed\napproach addresses these challenges by incorporating an adaptive L1-penalty and\ninertia terms in the loss function, and thus called Inertial Regularization and\nSelection (IRS). The former term performs model selection to handle the first\nchallenge while the latter is shown to address the last two challenges. A\nrecursive estimation algorithm is developed, and shown to outperform the\ncommonly used state-space models, such as Kalman Filters, in experimental\nstudies and real data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 18:43:44 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 03:28:27 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Ranjan", "Chitta", ""], ["Ebrahimi", "Samaneh", ""], ["Paynabar", "Kamran", ""]]}, {"id": "1610.07262", "submitter": "Alexandre Pich\\'e", "authors": "Alexandre Pich\\'e, Russell Steele, Ian Shrier, Stephanie Long", "title": "Bayesian Nonparametric Modeling of Heterogeneous Groups of Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets containing large samples of time-to-event data arising from several\nsmall heterogeneous groups are commonly encountered in statistics. This\npresents problems as they cannot be pooled directly due to their heterogeneity\nor analyzed individually because of their small sample size. Bayesian\nnonparametric modelling approaches can be used to model such datasets given\ntheir ability to flexibly share information across groups. In this paper, we\nwill compare three popular Bayesian nonparametric methods for modelling the\nsurvival functions of heterogeneous groups. Specifically, we will first compare\nthe modelling accuracy of the Dirichlet process, the hierarchical Dirichlet\nprocess, and the nested Dirichlet process on simulated datasets of different\nsizes, where group survival curves differ in shape or in expectation. We, then,\nwill compare the models on a real-world injury dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 01:57:59 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 04:35:42 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Pich\u00e9", "Alexandre", ""], ["Steele", "Russell", ""], ["Shrier", "Ian", ""], ["Long", "Stephanie", ""]]}, {"id": "1610.07379", "submitter": "Jonathan Scarlett", "authors": "Ilija Bogunovic and Jonathan Scarlett and Andreas Krause and Volkan\n  Cevher", "title": "Truncated Variance Reduction: A Unified Approach to Bayesian\n  Optimization and Level-Set Estimation", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm, truncated variance reduction (TruVaR), that\ntreats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian\nprocesses in a unified fashion. The algorithm greedily shrinks a sum of\ntruncated variances within a set of potential maximizers (BO) or unclassified\npoints (LSE), which is updated based on confidence bounds. TruVaR is effective\nin several important settings that are typically non-trivial to incorporate\ninto myopic algorithms, including pointwise costs and heteroscedastic noise. We\nprovide a general theoretical guarantee for TruVaR covering these aspects, and\nuse it to recover and strengthen existing results on BO and LSE. Moreover, we\nprovide a new result for a setting where one can select from a number of noise\nlevels having associated costs. We demonstrate the effectiveness of the\nalgorithm on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:18:00 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Scarlett", "Jonathan", ""], ["Krause", "Andreas", ""], ["Cevher", "Volkan", ""]]}, {"id": "1610.07407", "submitter": "Simon Bussy", "authors": "Simon Bussy, Agathe Guilloux, St\\'ephane Ga\\\"iffas, Anne-Sophie Jannot", "title": "C-mix: a high dimensional mixture model for censored durations, with\n  applications to genetic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mixture model for censored durations (C-mix), and develop\nmaximum likelihood inference for the joint estimation of the time distributions\nand latent regression parameters of the model. We consider a high-dimensional\nsetting, with datasets containing a large number of biomedical covariates. We\ntherefore penalize the negative log-likelihood by the Elastic-Net, which leads\nto a sparse parameterization of the model. Inference is achieved using an\nefficient Quasi-Newton Expectation Maximization (QNEM) algorithm, for which we\nprovide convergence properties. We then propose a score by assessing the\npatients risk of early adverse event. The statistical performance of the method\nis examined on an extensive Monte Carlo simulation study, and finally\nillustrated on three genetic datasets with high-dimensional covariates. We show\nthat our approach outperforms the state-of-the-art, namely both the CURE and\nCox proportional hazards models for this task, both in terms of C-index and\nAUC(t).\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 13:40:26 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 07:38:29 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 08:56:05 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 07:50:26 GMT"}, {"version": "v5", "created": "Sat, 25 Nov 2017 17:42:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Bussy", "Simon", ""], ["Guilloux", "Agathe", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Jannot", "Anne-Sophie", ""]]}, {"id": "1610.07448", "submitter": "Simone Scardapane", "authors": "Simone Scardapane and Paolo Di Lorenzo", "title": "A Framework for Parallel and Distributed Training of Neural Networks", "comments": "Published on Neural Networks (Elsevier), in press", "journal-ref": null, "doi": "10.1016/j.neunet.2017.04.004", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to develop a general framework for training neural\nnetworks (NNs) in a distributed environment, where training data is partitioned\nover a set of agents that communicate with each other through a sparse,\npossibly time-varying, connectivity pattern. In such distributed scenario, the\ntraining problem can be formulated as the (regularized) optimization of a\nnon-convex social cost function, given by the sum of local (non-convex) costs,\nwhere each agent contributes with a single error term defined with respect to\nits local dataset. To devise a flexible and efficient solution, we customize a\nrecently proposed framework for non-convex optimization over networks, which\nhinges on a (primal) convexification-decomposition technique to handle\nnon-convexity, and a dynamic consensus procedure to diffuse information among\nthe agents. Several typical choices for the training criterion (e.g., squared\nloss, cross entropy, etc.) and regularization (e.g., $\\ell_2$ norm, sparsity\ninducing penalties, etc.) are included in the framework and explored along the\npaper. Convergence to a stationary solution of the social non-convex problem is\nguaranteed under mild assumptions. Additionally, we show a principled way\nallowing each agent to exploit a possible multi-core architecture (e.g., a\nlocal cloud) in order to parallelize its local optimization step, resulting in\nstrategies that are both distributed (across the agents) and parallel (inside\neach agent) in nature. A comprehensive set of experimental results validate the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:58:56 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 11:00:58 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 08:55:19 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Scardapane", "Simone", ""], ["Di Lorenzo", "Paolo", ""]]}, {"id": "1610.07472", "submitter": "Behzad Tabibian", "authors": "Behzad Tabibian, Isabel Valera, Mehrdad Farajtabar, Le Song, Bernhard\n  Sch\\\"olkopf, Manuel Gomez-Rodriguez", "title": "Distilling Information Reliability and Source Trustworthiness from\n  Digital Traces", "comments": "Accepted at 26th World Wide Web conference (WWW-17)", "journal-ref": null, "doi": "10.1145/3038912.3052672", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online knowledge repositories typically rely on their users or dedicated\neditors to evaluate the reliability of their content. These evaluations can be\nviewed as noisy measurements of both information reliability and information\nsource trustworthiness. Can we leverage these noisy evaluations, often biased,\nto distill a robust, unbiased and interpretable measure of both notions?\n  In this paper, we argue that the temporal traces left by these noisy\nevaluations give cues on the reliability of the information and the\ntrustworthiness of the sources. Then, we propose a temporal point process\nmodeling framework that links these temporal traces to robust, unbiased and\ninterpretable notions of information reliability and source trustworthiness.\nFurthermore, we develop an efficient convex optimization procedure to learn the\nparameters of the model from historical traces. Experiments on real-world data\ngathered from Wikipedia and Stack Overflow show that our modeling framework\naccurately predicts evaluation events, provides an interpretable measure of\ninformation reliability and source trustworthiness, and yields interesting\ninsights about real-world events.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:13:56 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 20:39:19 GMT"}, {"version": "v3", "created": "Sun, 2 Apr 2017 13:32:25 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tabibian", "Behzad", ""], ["Valera", "Isabel", ""], ["Farajtabar", "Mehrdad", ""], ["Song", "Le", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1610.07487", "submitter": "Nicole M\\\"ucke", "authors": "Gilles Blanchard and Nicole M\\\"ucke", "title": "Parallelizing Spectral Algorithms for Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed learning approach in supervised learning for a\nlarge class of spectral regularization methods in an RKHS framework. The data\nset of size n is partitioned into $m=O(n^\\alpha)$ disjoint subsets. On each\nsubset, some spectral regularization method (belonging to a large class,\nincluding in particular Kernel Ridge Regression, $L^2$-boosting and spectral\ncut-off) is applied. The regression function $f$ is then estimated via simple\naveraging, leading to a substantial reduction in computation time. We show that\nminimax optimal rates of convergence are preserved if m grows sufficiently\nslowly (corresponding to an upper bound for $\\alpha$) as $n \\to \\infty$,\ndepending on the smoothness assumptions on $f$ and the intrinsic\ndimensionality. In spirit, our approach is classical.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:50:43 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 08:07:03 GMT"}, {"version": "v3", "created": "Sun, 22 Jan 2017 21:13:22 GMT"}, {"version": "v4", "created": "Wed, 9 Aug 2017 11:30:39 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Blanchard", "Gilles", ""], ["M\u00fccke", "Nicole", ""]]}, {"id": "1610.07519", "submitter": "Emilie Chouzenoux", "authors": "Yosra Marnissi, Yuling Zheng, Emilie Chouzenoux, Jean-Christophe\n  Pesquet", "title": "A Variational Bayesian Approach for Image Restoration. Application to\n  Image Deblurring with Poisson-Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology is investigated for signal recovery in the\npresence of non-Gaussian noise. In contrast with regularized minimization\napproaches often adopted in the literature, in our algorithm the regularization\nparameter is reliably estimated from the observations. As the posterior density\nof the unknown parameters is analytically intractable, the estimation problem\nis derived in a variational Bayesian framework where the goal is to provide a\ngood approximation to the posterior distribution in order to compute posterior\nmean estimates. Moreover, a majorization technique is employed to circumvent\nthe difficulties raised by the intricate forms of the non-Gaussian likelihood\nand of the prior density. We demonstrate the potential of the proposed approach\nthrough comparisons with state-of-the-art techniques that are specifically\ntailored to signal recovery in the presence of mixed Poisson-Gaussian noise.\nResults show that the proposed approach is efficient and achieves performance\ncomparable with other methods where the regularization parameter is manually\ntuned from the ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:10:11 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 15:20:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Marnissi", "Yosra", ""], ["Zheng", "Yuling", ""], ["Chouzenoux", "Emilie", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1610.07524", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova", "title": "Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments", "comments": "FATML 2016 conference paper. A long version of the paper available on\n  the author's website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:23:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chouldechova", "Alexandra", ""]]}, {"id": "1610.07569", "submitter": "Jiaqi Mu Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "Geometry of Polysemy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representations of words have heralded a transformational approach to\nclassical problems in NLP; the most popular example is word2vec. However, a\nsingle vector does not suffice to model the polysemous nature of many\n(frequent) words, i.e., words with multiple meanings. In this paper, we propose\na three-fold approach for unsupervised polysemy modeling: (a) context\nrepresentations, (b) sense induction and disambiguation and (c) lexeme (as a\nword and sense pair) representations. A key feature of our work is the finding\nthat a sentence containing a target word is well represented by a low rank\nsubspace, instead of a point in a vector space. We then show that the subspaces\nassociated with a particular sense of the target word tend to intersect over a\nline (one-dimensional subspace), which we use to disambiguate senses using a\nclustering algorithm that harnesses the Grassmannian geometry of the\nrepresentations. The disambiguation algorithm, which we call $K$-Grassmeans,\nleads to a procedure to label the different senses of the target word in the\ncorpus -- yielding lexeme vector representations, all in an unsupervised manner\nstarting from a large (Wikipedia) corpus in English. Apart from several\nprototypical target (word,sense) examples and a host of empirical studies to\nintuit and justify the various geometric representations, we validate our\nalgorithms on standard sense induction and disambiguation datasets and present\nnew state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:35:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1610.07647", "submitter": "Mark Neumann", "authors": "Mark Neumann, Pontus Stenetorp, Sebastian Riedel", "title": "Learning to Reason With Adaptive Computation", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop inference is necessary for machine learning systems to successfully\nsolve tasks such as Recognising Textual Entailment and Machine Reading. In this\nwork, we demonstrate the effectiveness of adaptive computation for learning the\nnumber of inference steps required for examples of different complexity and\nthat learning the correct number of inference steps is difficult. We introduce\nthe first model involving Adaptive Computation Time which provides a small\nperformance benefit on top of a similar model without an adaptive component as\nwell as enabling considerable insight into the reasoning process of the model.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:48:04 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 19:35:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Neumann", "Mark", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1610.07650", "submitter": "Yining Wang", "authors": "Yining Wang, Yu-Xiang Wang and Aarti Singh", "title": "A Theoretical Analysis of Noisy Sparse Subspace Clustering on\n  Dimensionality-Reduced Data", "comments": "40 pages, 2 figures. A shorter version of this paper titled \"A\n  Deterministic Analysis of Noisy Sparse Subspace Clustering on\n  Dimensionality-Reduced Data\" with partial results appeared at Proceedings of\n  the 32nd International Conference on Machine Learning (ICML) held at Lille,\n  France in 2015", "journal-ref": "IEEE Transactions on Information Theory, 65(2):685-706, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the problem of partitioning unlabeled data points into\na number of clusters so that data points within one cluster lie approximately\non a low-dimensional linear subspace. In many practical scenarios, the\ndimensionality of data points to be clustered are compressed due to constraints\nof measurement, computation or privacy. In this paper, we study the theoretical\nproperties of a popular subspace clustering algorithm named sparse subspace\nclustering (SSC) and establish formal success conditions of SSC on\ndimensionality-reduced data. Our analysis applies to the most general fully\ndeterministic model where both underlying subspaces and data points within each\nsubspace are deterministically positioned, and also a wide range of\ndimensionality reduction techniques (e.g., Gaussian random projection, uniform\nsubsampling, sketching) that fall into a subspace embedding framework (Meng &\nMahoney, 2013; Avron et al., 2014). Finally, we apply our analysis to a\ndifferentially private SSC algorithm and established both privacy and utility\nguarantees of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:54:07 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Yu-Xiang", ""], ["Singh", "Aarti", ""]]}, {"id": "1610.07677", "submitter": "Edward Yu", "authors": "Edward Yu, Parth Parekh", "title": "A Bayesian Ensemble for Unsupervised Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for unsupervised anomaly detection suffer from the fact that the data\nis unlabeled, making it difficult to assess the optimality of detection\nalgorithms. Ensemble learning has shown exceptional results in classification\nand clustering problems, but has not seen as much research in the context of\noutlier detection. Existing methods focus on combining output scores of\nindividual detectors, but this leads to outputs that are not easily\ninterpretable. In this paper, we introduce a theoretical foundation for\ncombining individual detectors with Bayesian classifier combination. Not only\nare posterior distributions easily interpreted as the probability distribution\nof anomalies, but bias, variance, and individual error rates of detectors are\nall easily obtained. Performance on real-world datasets shows high accuracy\nacross varied types of time series data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 23:07:16 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Yu", "Edward", ""], ["Parekh", "Parth", ""]]}, {"id": "1610.07703", "submitter": "Ilya Safro", "authors": "Chris Gropp, Alexander Herzog, Ilya Safro, Paul W. Wilson, Amy W. Apon", "title": "Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet\n  Allocation (CLDA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling, a method for extracting the underlying themes from a\ncollection of documents, is an increasingly important component of the design\nof intelligent systems enabling the sense-making of highly dynamic and diverse\nstreams of text data. Traditional methods such as Dynamic Topic Modeling (DTM)\ndo not lend themselves well to direct parallelization because of dependencies\nfrom one time step to another. In this paper, we introduce and empirically\nanalyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting\ndynamic latent topics from a collection of documents. Our approach is based on\ndata decomposition in which the data is partitioned into segments, followed by\ntopic modeling on the individual segments. The resulting local models are then\ncombined into a global solution using clustering. The decomposition and\nresulting parallelization leads to very fast runtime even on very large\ndatasets. Our approach furthermore provides insight into how the composition of\ntopics changes over time and can also be applied using other data partitioning\nstrategies over any discrete features of the data, such as geographic features\nor classes of users. In this paper CLDA is applied successfully to seventeen\nyears of NIPS conference papers (2,484 documents and 3,280,697 words),\nseventeen years of computer science journal abstracts (533,560 documents and\n32,551,540 words), and to forty years of the PubMed corpus (4,025,978 documents\nand 273,853,980 words).\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 01:50:24 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 04:06:39 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 14:37:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Gropp", "Chris", ""], ["Herzog", "Alexander", ""], ["Safro", "Ilya", ""], ["Wilson", "Paul W.", ""], ["Apon", "Amy W.", ""]]}, {"id": "1610.07733", "submitter": "Yoshiyuki Kabashima", "authors": "Yoshiyuki Kabashima, Tomoyuki Obuchi, Makoto Uemura", "title": "Approximate cross-validation formula for Bayesian linear regression", "comments": "5 pages, 2 figures, invited paper for Allerton2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is a technique for evaluating the ability of\nstatistical models/learning systems based on a given data set. Despite its wide\napplicability, the rather heavy computational cost can prevent its use as the\nsystem size grows. To resolve this difficulty in the case of Bayesian linear\nregression, we develop a formula for evaluating the leave-one-out CV error\napproximately without actually performing CV. The usefulness of the developed\nformula is tested by statistical mechanical analysis for a synthetic model.\nThis is confirmed by application to a real-world supernova data set as well.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 05:10:49 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Kabashima", "Yoshiyuki", ""], ["Obuchi", "Tomoyuki", ""], ["Uemura", "Makoto", ""]]}, {"id": "1610.07748", "submitter": "Nikolay Doudchenko", "authors": "Nikolay Doudchenko and Guido W. Imbens", "title": "Balancing, Regression, Difference-In-Differences and Synthetic Control\n  Methods: A Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH), see also\nAbadie and Gardeazabal [2003], Abadie et al. [2014], develop the synthetic\ncontrol procedure for estimating the effect of a treatment, in the presence of\na single treated unit and a number of control units, with pre-treatment\noutcomes observed for all units. The method constructs a set of weights such\nthat selected covariates and pre-treatment outcomes of the treated unit are\napproximately matched by a weighted average of control units (the synthetic\ncontrol). The weights are restricted to be nonnegative and sum to one, which is\nimportant because it allows the procedure to obtain unique weights even when\nthe number of lagged outcomes is modest relative to the number of control\nunits, a common setting in applications. In the current paper we propose a\ngeneralization that allows the weights to be negative, and their sum to differ\nfrom one, and that allows for a permanent additive difference between the\ntreated unit and the controls, similar to difference-in-difference procedures.\nThe weights directly minimize the distance between the lagged outcomes for the\ntreated and the control units, using regularization methods to deal with a\npotentially large number of possible control units.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:00:36 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 02:55:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Doudchenko", "Nikolay", ""], ["Imbens", "Guido W.", ""]]}, {"id": "1610.07797", "submitter": "Simon Lacoste-Julien", "authors": "Gauthier Gidel, Tony Jebara and Simon Lacoste-Julien", "title": "Frank-Wolfe Algorithms for Saddle Point Problems", "comments": "Appears in: Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017). 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained\nsmooth convex-concave saddle point (SP) problems. Remarkably, the method only\nrequires access to linear minimization oracles. Leveraging recent advances in\nFW optimization, we provide the first proof of convergence of a FW-type saddle\npoint solver over polytopes, thereby partially answering a 30 year-old\nconjecture. We also survey other convergence results and highlight gaps in the\ntheoretical underpinnings of FW-style algorithms. Motivating applications\nwithout known efficient alternatives are explored through structured prediction\nwith combinatorial penalties as well as games over matching polytopes involving\nan exponential number of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 09:14:40 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 10:34:48 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 21:34:24 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gidel", "Gauthier", ""], ["Jebara", "Tony", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1610.07830", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa", "title": "On the convergence rate of the three operator splitting scheme", "comments": "Fixed typo in Lemma 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three operator splitting scheme was recently proposed by [Davis and Yin,\n2015] as a method to optimize composite objective functions with one convex\nsmooth term and two convex (possibly non-smooth) terms for which we have access\nto their proximity operator. In this short note we provide an alternative proof\nfor the sublinear rate of convergence of this method.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 11:25:32 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 08:35:52 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 14:14:43 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 10:33:36 GMT"}, {"version": "v5", "created": "Fri, 25 Jun 2021 19:50:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Pedregosa", "Fabian", ""]]}, {"id": "1610.07857", "submitter": "Yevgeniy Bodyanskiy", "authors": "Yevgeniy Bodyanskiy, Olena Vynokurova, Volodymyr Savvo, Tatiana\n  Tverdokhlib, Pavlo Mulesa", "title": "Hybrid clustering-classification neural network in the medical\n  diagnostics of reactive arthritis", "comments": null, "journal-ref": "International Journal of Intelligent Systems and Applications,\n  2016, Vol. 8, No. 8, pp.1-9", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hybrid clustering-classification neural network is proposed. This network\nallows increasing a quality of information processing under the condition of\noverlapping classes due to the rational choice of a learning rate parameter and\nintroducing a special procedure of fuzzy reasoning in the clustering process,\nwhich occurs both with an external learning signal (supervised) and without the\none (unsupervised). As similarity measure neighborhood function or membership\none, cosine structures are used, which allow to provide a high flexibility due\nto self-learning-learning process and to provide some new useful properties.\nMany realized experiments have confirmed the efficiency of proposed hybrid\nclustering-classification neural network; also, this network was used for\nsolving diagnostics task of reactive arthritis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 09:11:53 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bodyanskiy", "Yevgeniy", ""], ["Vynokurova", "Olena", ""], ["Savvo", "Volodymyr", ""], ["Tverdokhlib", "Tatiana", ""], ["Mulesa", "Pavlo", ""]]}, {"id": "1610.07921", "submitter": "Yangbo He", "authors": "Yangbo He and Bin Yu", "title": "Formulas for Counting the Sizes of Markov Equivalence Classes of\n  Directed Acyclic Graphs", "comments": "21 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sizes of Markov equivalence classes of directed acyclic graphs play\nimportant roles in measuring the uncertainty and complexity in causal learning.\nA Markov equivalence class can be represented by an essential graph and its\nundirected subgraphs determine the size of the class. In this paper, we develop\na method to derive the formulas for counting the sizes of Markov equivalence\nclasses. We first introduce a new concept of core graph. The size of a Markov\nequivalence class of interest is a polynomial of the number of vertices given\nits core graph. Then, we discuss the recursive and explicit formula of the\npolynomial, and provide an algorithm to derive the size formula via symbolic\ncomputation for any given core graph. The proposed size formula derivation\nsheds light on the relationships between the size of a Markov equivalence class\nand its representation graph, and makes size counting efficient, even when the\nessential graphs contain non-sparse undirected subgraphs.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 16:28:33 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["He", "Yangbo", ""], ["Yu", "Bin", ""]]}, {"id": "1610.08035", "submitter": "Alexander Grigorievskiy", "authors": "Alexander Grigorievskiy, Neil Lawrence, Simo S\\\"arkk\\\"a", "title": "Parallelizable sparse inverse formulation Gaussian processes (SpInGP)", "comments": "Presented at Machine Learning in Signal Processing (MLSP2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parallelizable sparse inverse formulation Gaussian process\n(SpInGP) for temporal models. It uses a sparse precision GP formulation and\nsparse matrix routines to speed up the computations. Due to the state-space\nformulation used in the algorithm, the time complexity of the basic SpInGP is\nlinear, and because all the computations are parallelizable, the parallel form\nof the algorithm is sublinear in the number of data points. We provide example\nalgorithms to implement the sparse matrix routines and experimentally test the\nmethod using both simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 19:39:35 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 05:22:56 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 20:59:24 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 03:14:28 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Grigorievskiy", "Alexander", ""], ["Lawrence", "Neil", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1610.08074", "submitter": "Alexander Grigorievskiy", "authors": "Alexander Grigorievskiy, Juha Karhunen", "title": "Gaussian Process Kernels for Popular State-Space Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate a link between state- space models and Gaussian\nProcesses (GP) for time series modeling and forecasting. In particular, several\nwidely used state- space models are transformed into continuous time form and\ncorresponding Gaussian Process kernels are derived. Experimen- tal results\ndemonstrate that the derived GP kernels are correct and appropriate for\nGaussian Process Regression. An experiment with a real world dataset shows that\nthe modeling is identical with state-space models and with the proposed GP\nkernels. The considered connection allows the researchers to look at their\nmodels from a different angle and facilitate sharing ideas between these two\ndifferent modeling approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:09:42 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Grigorievskiy", "Alexander", ""], ["Karhunen", "Juha", ""]]}, {"id": "1610.08077", "submitter": "James Johndrow", "authors": "Kristian Lum and James Johndrow", "title": "A statistical framework for fair predictive algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing human judgment with\ncomputer models in high stakes settings-- such as sentencing, hiring, policing,\ncollege admissions, and parole decisions-- is the perceived \"neutrality\" of\ncomputers. It is argued that because computer models do not hold personal\nprejudice, the predictions they produce will be equally free from prejudice.\nThere is growing recognition that employing algorithms does not remove the\npotential for bias, and can even amplify it, since training data were\ninevitably generated by a process that is itself biased. In this paper, we\nprovide a probabilistic definition of algorithmic bias. We propose a method to\nremove bias from predictive models by removing all information regarding\nprotected variables from the permitted training data. Unlike previous work in\nthis area, our framework is general enough to accommodate arbitrary data types,\ne.g. binary, continuous, etc. Motivated by models currently in use in the\ncriminal justice system that inform decisions on pre-trial release and\nparoling, we apply our proposed method to a dataset on the criminal histories\nof individuals at the time of sentencing to produce \"race-neutral\" predictions\nof re-arrest. In the process, we demonstrate that the most common approach to\ncreating \"race-neutral\" models-- omitting race as a covariate-- still results\nin racially disparate predictions. We then demonstrate that the application of\nour proposed method to these data removes racial disparities from predictions\nwith minimal impact on predictive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:18:24 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Lum", "Kristian", ""], ["Johndrow", "James", ""]]}, {"id": "1610.08087", "submitter": "Minh Ha Quang", "authors": "Minh Ha Quang", "title": "Infinite-dimensional Log-Determinant divergences II: Alpha-Beta\n  divergences", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a parametrized family of divergences, namely Alpha-Beta\nLog- Determinant (Log-Det) divergences, between positive definite unitized\ntrace class operators on a Hilbert space. This is a generalization of the\nAlpha-Beta Log-Determinant divergences between symmetric, positive definite\nmatrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det\ndivergences is highly general and contains many divergences as special cases,\nincluding the recently formulated infinite dimensional affine-invariant\nRiemannian distance and the infinite-dimensional Alpha Log-Det divergences\nbetween positive definite unitized trace class operators. In particular, it\nincludes a parametrized family of metrics between positive definite trace class\noperators, with the affine-invariant Riemannian distance and the square root of\nthe symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det\ndivergences between covariance operators on a Reproducing Kernel Hilbert Space\n(RKHS), we obtain closed form formulas via the corresponding Gram matrices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 17:58:58 GMT"}, {"version": "v2", "created": "Sat, 14 Jan 2017 18:36:16 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Quang", "Minh Ha", ""]]}, {"id": "1610.08123", "submitter": "Paroma Varma", "authors": "Paroma Varma, Bryan He, Dan Iter, Peng Xu, Rose Yu, Christopher De Sa,\n  Christopher R\\'e", "title": "Socratic Learning: Augmenting Generative Models to Incorporate Latent\n  Subsets in Training Data", "comments": "4 figures; 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in training discriminative models like neural networks is\nobtaining enough labeled training data. Recent approaches use generative models\nto combine weak supervision sources, like user-defined heuristics or knowledge\nbases, to label training data. Prior work has explored learning accuracies for\nthese sources even without ground truth labels, but they assume that a single\naccuracy parameter is sufficient to model the behavior of these sources over\nthe entire training set. In particular, they fail to model latent subsets in\nthe training data in which the supervision sources perform differently than on\naverage. We present Socratic learning, a paradigm that uses feedback from a\ncorresponding discriminative model to automatically identify these subsets and\naugments the structure of the generative model accordingly. Experimentally, we\nshow that without any ground truth labels, the augmented generative model\nreduces error by up to 56.06% for a relation extraction task compared to a\nstate-of-the-art weak supervision technique that utilizes generative models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:43:49 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 08:00:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 23:33:52 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 07:40:29 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Varma", "Paroma", ""], ["He", "Bryan", ""], ["Iter", "Dan", ""], ["Xu", "Peng", ""], ["Yu", "Rose", ""], ["De Sa", "Christopher", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1610.08127", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Jes Frellsen, Pietro Lio'", "title": "Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation", "comments": "NIPS 2016 Workshop on Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast variational Bayesian algorithm for performing non-negative\nmatrix factorisation and tri-factorisation. We show that our approach achieves\nfaster convergence per iteration and timestep (wall-clock) than Gibbs sampling\nand non-probabilistic approaches, and do not require additional samples to\nestimate the posterior. We show that in particular for matrix tri-factorisation\nconvergence is difficult, but our variational Bayesian approach offers a fast\nsolution, allowing the tri-factorisation approach to be used more effectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 00:10:44 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Brouwer", "Thomas", ""], ["Frellsen", "Jes", ""], ["Lio'", "Pietro", ""]]}, {"id": "1610.08166", "submitter": "Yossi Adi", "authors": "Yossi Adi, Joseph Keshet, Emily Cibelli, Erin Gustafson, Cynthia\n  Clopper, Matthew Goldrick", "title": "Automatic measurement of vowel duration via structured prediction", "comments": null, "journal-ref": null, "doi": "10.1121/1.4972527", "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key barrier to making phonetic studies scalable and replicable is the need\nto rely on subjective, manual annotation. To help meet this challenge, a\nmachine learning algorithm was developed for automatic measurement of a widely\nused phonetic measure: vowel duration. Manually-annotated data were used to\ntrain a model that takes as input an arbitrary length segment of the acoustic\nsignal containing a single vowel that is preceded and followed by consonants\nand outputs the duration of the vowel. The model is based on the structured\nprediction framework. The input signal and a hypothesized set of a vowel's\nonset and offset are mapped to an abstract vector space by a set of acoustic\nfeature functions. The learning algorithm is trained in this space to minimize\nthe difference in expectations between predicted and manually-measured vowel\ndurations. The trained model can then automatically estimate vowel durations\nwithout phonetic or orthographic transcription. Results comparing the model to\nthree sets of manually annotated data suggest it out-performed the current gold\nstandard for duration measurement, an HMM-based forced aligner (which requires\northographic or phonetic transcription as an input).\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 04:50:35 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Adi", "Yossi", ""], ["Keshet", "Joseph", ""], ["Cibelli", "Emily", ""], ["Gustafson", "Erin", ""], ["Clopper", "Cynthia", ""], ["Goldrick", "Matthew", ""]]}, {"id": "1610.08189", "submitter": "Yanning Shen", "authors": "Yanning Shen, Brian Baingana, Georgios B. Giannakis", "title": "Tensor Decompositions for Identifying Directed Graph Topologies and\n  Tracking Dynamic Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2698369", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed networks are pervasive both in nature and engineered systems, often\nunderlying the complex behavior observed in biological systems, microblogs and\nsocial interactions over the web, as well as global financial markets. Since\ntheir structures are often unobservable, in order to facilitate network\nanalytics, one generally resorts to approaches capitalizing on measurable nodal\nprocesses to infer the unknown topology. Structural equation models (SEMs) are\ncapable of incorporating exogenous inputs to resolve inherent directional\nambiguities. However, conventional SEMs assume full knowledge of exogenous\ninputs, which may not be readily available in some practical settings. The\npresent paper advocates a novel SEM-based topology inference approach that\nentails factorization of a three-way tensor, constructed from the observed\nnodal data, using the well-known parallel factor (PARAFAC) decomposition. It\nturns out that second-order piecewise stationary statistics of exogenous\nvariables suffice to identify the hidden topology. Capitalizing on the\nuniqueness properties inherent to high-order tensor factorizations, it is shown\nthat topology identification is possible under reasonably mild conditions. In\naddition, to facilitate real-time operation and inference of time-varying\nnetworks, an adaptive (PARAFAC) tensor decomposition scheme which tracks the\ntopology-revealing tensor factors is developed. Extensive tests on simulated\nand real stock quote data demonstrate the merits of the novel tensor-based\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 06:12:17 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Shen", "Yanning", ""], ["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1610.08239", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Things Bayes can't do", "comments": null, "journal-ref": "Proceedings of ALT, LNCS 9925, pp.253-260, Bari, Italy, 2016", "doi": "10.1007/978-3-319-46379-7_17", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of forecasting conditional probabilities of the next event given\nthe past is considered in a general probabilistic setting. Given an arbitrary\n(large, uncountable) set C of predictors, we would like to construct a single\npredictor that performs asymptotically as well as the best predictor in C, on\nany data. Here we show that there are sets C for which such predictors exist,\nbut none of them is a Bayesian predictor with a prior concentrated on C. In\nother words, there is a predictor with sublinear regret, but every Bayesian\npredictor must have a linear regret. This negative finding is in sharp contrast\nwith previous results that establish the opposite for the case when one of the\npredictors in $C$ achieves asymptotically vanishing error. In such a case, if\nthere is a predictor that achieves asymptotically vanishing error for any\nmeasure in C, then there is a Bayesian predictor that also has this property,\nand whose prior is concentrated on (a countable subset of) C.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:13:28 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 12:13:37 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1610.08401", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard", "title": "Universal adversarial perturbations", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a state-of-the-art deep neural network classifier, we show the\nexistence of a universal (image-agnostic) and very small perturbation vector\nthat causes natural images to be misclassified with high probability. We\npropose a systematic algorithm for computing universal perturbations, and show\nthat state-of-the-art deep neural networks are highly vulnerable to such\nperturbations, albeit being quasi-imperceptible to the human eye. We further\nempirically analyze these universal perturbations and show, in particular, that\nthey generalize very well across neural networks. The surprising existence of\nuniversal perturbations reveals important geometric correlations among the\nhigh-dimensional decision boundary of classifiers. It further outlines\npotential security breaches with the existence of single directions in the\ninput space that adversaries can possibly exploit to break a classifier on most\nnatural images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:30:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 07:15:00 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 17:01:25 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1610.08417", "submitter": "Onur Teymur", "authors": "Onur Teymur, Konstantinos Zygalakis, Ben Calderhead", "title": "Probabilistic Linear Multistep Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016) pp.\n  4321-4328", "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a derivation and theoretical investigation of the Adams-Bashforth\nand Adams-Moulton family of linear multistep methods for solving ordinary\ndifferential equations, starting from a Gaussian process (GP) framework. In the\nlimit, this formulation coincides with the classical deterministic methods,\nwhich have been used as higher-order initial value problem solvers for over a\ncentury. Furthermore, the natural probabilistic framework provided by the GP\nformulation allows us to derive probabilistic versions of these methods, in the\nspirit of a number of other probabilistic ODE solvers presented in the recent\nliterature. In contrast to higher-order Runge-Kutta methods, which require\nmultiple intermediate function evaluations per step, Adams family methods make\nuse of previous function evaluations, so that increased accuracy arising from a\nhigher-order multistep approach comes at very little additional computational\ncost. We show that through a careful choice of covariance function for the GP,\nthe posterior mean and standard deviation over the numerical solution can be\nmade to exactly coincide with the value given by the deterministic method and\nits local truncation error respectively. We provide a rigorous proof of the\nconvergence of these new methods, as well as an empirical investigation (up to\nfifth order) demonstrating their convergence rates in practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:53:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Teymur", "Onur", ""], ["Zygalakis", "Konstantinos", ""], ["Calderhead", "Ben", ""]]}, {"id": "1610.08445", "submitter": "Seyed Mehran Kazemi", "authors": "Seyed Mehran Kazemi, Angelika Kimmig, Guy Van den Broeck, David Poole", "title": "New Liftable Classes for First-Order Probabilistic Inference", "comments": "Accepted at NIPS-2016. 22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical relational models provide compact encodings of probabilistic\ndependencies in relational domains, but result in highly intractable graphical\nmodels. The goal of lifted inference is to carry out probabilistic inference\nwithout needing to reason about each individual separately, by instead treating\nexchangeable, undistinguished objects as a whole. In this paper, we study the\ndomain recursion inference rule, which, despite its central role in early\ntheoretical results on domain-lifted inference, has later been believed\nredundant. We show that this rule is more powerful than expected, and in fact\nsignificantly extends the range of models for which lifted inference runs in\ntime polynomial in the number of individuals in the domain. This includes an\nopen problem called S4, the symmetric transitivity model, and a first-order\nlogic encoding of the birthday paradox. We further identify new classes S2FO2\nand S2RU of domain-liftable theories, which respectively subsume FO2 and\nrecursively unary theories, the largest classes of domain-liftable theories\nknown so far, and show that using domain recursion can achieve exponential\nspeedup even in theories that cannot fully be lifted with the existing set of\ninference rules.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:13:42 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Kazemi", "Seyed Mehran", ""], ["Kimmig", "Angelika", ""], ["Broeck", "Guy Van den", ""], ["Poole", "David", ""]]}, {"id": "1610.08450", "submitter": "Dimitrije Markovic", "authors": "Dimitrije Markovi\\'c, Borjana Val\\v{c}i\\'c, and Neboj\\v{s}a\n  Male\\v{s}evi\\'c", "title": "Body movement to sound interface with vector autoregressive hierarchical\n  hidden Markov models", "comments": "12 pages, 7 figures, a pre-submission draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interfacing a kinetic action of a person to an action of a machine system is\nan important research topic in many application areas. One of the key factors\nfor intimate human-machine interaction is the ability of the control algorithm\nto detect and classify different user commands with shortest possible latency,\nthus making a highly correlated link between cause and effect. In our research,\nwe focused on the task of mapping user kinematic actions into sound samples.\nThe presented methodology relies on the wireless sensor nodes equipped with\ninertial measurement units and the real-time algorithm dedicated for early\ndetection and classification of a variety of movements/gestures performed by a\nuser. The core algorithm is based on the approximate Bayesian inference of\nVector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where\nmodels database is derived from the set of motion gestures. The performance of\nthe algorithm was compared with an online version of the K-nearest neighbours\n(KNN) algorithm, where we used offline expert based classification as the\nbenchmark. In almost all of the evaluation metrics (e.g. confusion matrix,\nrecall and precision scores) the VAR-HHMM algorithm outperformed KNN.\nFurthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement\nonset detection compared with the offline standard. The proposed concept,\nalthough envisioned for movement-to-sound application, could be implemented in\nother human-machine interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:27:27 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Markovi\u0107", "Dimitrije", ""], ["Val\u010di\u0107", "Borjana", ""], ["Male\u0161evi\u0107", "Neboj\u0161a", ""]]}, {"id": "1610.08452", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna\n  P. Gummadi", "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning\n  Classification without Disparate Mistreatment", "comments": "To appear in Proceedings of the 26th International World Wide Web\n  Conference (WWW), 2017. Code available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": "10.1145/3038912.3052660", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated data-driven decision making systems are increasingly being used to\nassist, or even replace humans in many settings. These systems function by\nlearning from historical decisions, often taken by humans. In order to maximize\nthe utility of these systems (or, classifiers), their training involves\nminimizing the errors (or, misclassifications) over the given historical data.\nHowever, it is quite possible that the optimally trained classifier makes\ndecisions for people belonging to different social groups with different\nmisclassification rates (e.g., misclassification rates for females are higher\nthan for males), thereby placing these groups at an unfair disadvantage. To\naccount for and avoid such unfairness, in this paper, we introduce a new notion\nof unfairness, disparate mistreatment, which is defined in terms of\nmisclassification rates. We then propose intuitive measures of disparate\nmistreatment for decision boundary-based classifiers, which can be easily\nincorporated into their formulation as convex-concave constraints. Experiments\non synthetic as well as real world datasets show that our methodology is\neffective at avoiding disparate mistreatment, often at a small cost in terms of\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:34:48 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 19:04:28 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "1610.08465", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Ryan P. Adams, and Jonathan W. Pillow", "title": "Bayesian latent structure discovery from multi-neuron recordings", "comments": "11 pages, 5 figures, to appear in Advances in Neural Information\n  Processing Systems 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural circuits contain heterogeneous groups of neurons that differ in type,\nlocation, connectivity, and basic response properties. However, traditional\nmethods for dimensionality reduction and clustering are ill-suited to\nrecovering the structure underlying the organization of neural circuits. In\nparticular, they do not take advantage of the rich temporal dependencies in\nmulti-neuron recordings and fail to account for the noise in neural spike\ntrains. Here we describe new tools for inferring latent structure from\nsimultaneously recorded spike train data using a hierarchical extension of a\nmulti-neuron point process model commonly known as the generalized linear model\n(GLM). Our approach combines the GLM with flexible graph-theoretic priors\ngoverning the relationship between latent features and neural connectivity\npatterns. Fully Bayesian inference via P\\'olya-gamma augmentation of the\nresulting model allows us to classify neurons and infer latent dimensions of\ncircuit organization from correlated spike trains. We demonstrate the\neffectiveness of our method with applications to synthetic data and\nmulti-neuron recordings in primate retina, revealing latent patterns of neural\ntypes and locations from spike trains alone.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:07:59 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Linderman", "Scott W.", ""], ["Adams", "Ryan P.", ""], ["Pillow", "Jonathan W.", ""]]}, {"id": "1610.08466", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei,\n  Liam Paninski, and Matthew J. Johnson", "title": "Recurrent switching linear dynamical systems", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural systems, such as neurons firing in the brain or basketball teams\ntraversing a court, give rise to time series data with complex, nonlinear\ndynamics. We can gain insight into these systems by decomposing the data into\nsegments that are each explained by simpler dynamic units. Building on\nswitching linear dynamical systems (SLDS), we present a new model class that\nnot only discovers these dynamical units, but also explains how their switching\nbehavior depends on observations or continuous latent states. These \"recurrent\"\nswitching linear dynamical systems provide further insight by discovering the\nconditions under which each unit is deployed, something that traditional SLDS\nmodels fail to do. We leverage recent algorithmic advances in approximate\ninference to make Bayesian inference in these models easy, fast, and scalable.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:08:04 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Linderman", "Scott W.", ""], ["Miller", "Andrew C.", ""], ["Adams", "Ryan P.", ""], ["Blei", "David M.", ""], ["Paninski", "Liam", ""], ["Johnson", "Matthew J.", ""]]}, {"id": "1610.08473", "submitter": "Lin Chen", "authors": "Lin Chen, Amin Karbasi, Forrest W. Crawford", "title": "Estimating the Size of a Large Network and its Communities from a Random\n  Sample", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world networks are too large to be measured or studied directly and\nthere is substantial interest in estimating global network properties from\nsmaller sub-samples. One of the most important global properties is the number\nof vertices/nodes in the network. Estimating the number of vertices in a large\nnetwork is a major challenge in computer science, epidemiology, demography, and\nintelligence analysis. In this paper we consider a population random graph G =\n(V;E) from the stochastic block model (SBM) with K communities/blocks. A sample\nis obtained by randomly choosing a subset W and letting G(W) be the induced\nsubgraph in G of the vertices in W. In addition to G(W), we observe the total\ndegree of each sampled vertex and its block membership. Given this partial\ninformation, we propose an efficient PopULation Size Estimation algorithm,\ncalled PULSE, that correctly estimates the size of the whole population as well\nas the size of each community. To support our theoretical analysis, we perform\nan exhaustive set of experiments to study the effects of sample size, K, and\nSBM model parameters on the accuracy of the estimates. The experimental results\nalso demonstrate that PULSE significantly outperforms a widely-used method\ncalled the network scale-up estimator in a wide variety of scenarios. We\nconclude with extensions and directions for future work.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:26:47 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Chen", "Lin", ""], ["Karbasi", "Amin", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1610.08495", "submitter": "Tiep Vu Tiep Vu", "authors": "Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga", "title": "Adaptive matching pursuit for sparse signal recovery", "comments": "ICASSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike and Slab priors have been of much recent interest in signal processing\nas a means of inducing sparsity in Bayesian inference. Applications domains\nthat benefit from the use of these priors include sparse recovery, regression\nand classification. It is well-known that solving for the sparse coefficient\nvector to maximize these priors results in a hard non-convex and mixed integer\nprogramming problem. Most existing solutions to this optimization problem\neither involve simplifying assumptions/relaxations or are computationally\nexpensive. We propose a new greedy and adaptive matching pursuit (AMP)\nalgorithm to directly solve this hard problem. Essentially, in each step of the\nalgorithm, the set of active elements would be updated by either adding or\nremoving one index, whichever results in better improvement. In addition, the\nintermediate steps of the algorithm are calculated via an inexpensive Cholesky\ndecomposition which makes the algorithm much faster. Results on simulated data\nsets as well as real-world image recovery challenges confirm the benefits of\nthe proposed AMP, particularly in providing a superior cost-quality trade-off\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 17:48:38 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vu", "Tiep H.", ""], ["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""]]}, {"id": "1610.08611", "submitter": "Yangbo He", "authors": "Yango He, Zhi Geng", "title": "Causal Network Learning from Multiple Interventions of Unknown\n  Manipulated Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss structure learning of causal networks from multiple\ndata sets obtained by external intervention experiments where we do not know\nwhat variables are manipulated. For example, the conditions in these\nexperiments are changed by changing temperature or using drugs, but we do not\nknow what target variables are manipulated by the external interventions. From\nsuch data sets, the structure learning becomes more difficult. For this case,\nwe first discuss the identifiability of causal structures. Next we present a\ngraph-merging method for learning causal networks for the case that the sample\nsizes are large for these interventions. Then for the case that the sample\nsizes of these interventions are relatively small, we propose a data-pooling\nmethod for learning causal networks in which we pool all data sets of these\ninterventions together for the learning. Further we propose a re-sampling\napproach to evaluate the edges of the causal network learned by the\ndata-pooling method. Finally we illustrate the proposed learning methods by\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 04:17:46 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["He", "Yango", ""], ["Geng", "Zhi", ""]]}, {"id": "1610.08623", "submitter": "Seth Flaxman", "authors": "Seth Flaxman, Yee Whye Teh, and Dino Sejdinovic", "title": "Poisson intensity estimation with reproducing kernels", "comments": "AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fundamental nature of the inhomogeneous Poisson process in the\ntheory and application of stochastic processes, and its attractive\ngeneralizations (e.g. Cox process), few tractable nonparametric modeling\napproaches of intensity functions exist, especially when observed points lie in\na high-dimensional space. In this paper we develop a new, computationally\ntractable Reproducing Kernel Hilbert Space (RKHS) formulation for the\ninhomogeneous Poisson process. We model the square root of the intensity as an\nRKHS function. Whereas RKHS models used in supervised learning rely on the\nso-called representer theorem, the form of the inhomogeneous Poisson process\nlikelihood means that the representer theorem does not apply. However, we prove\nthat the representer theorem does hold in an appropriately transformed RKHS,\nguaranteeing that the optimization of the penalized likelihood can be cast as a\ntractable finite-dimensional problem. The resulting approach is simple to\nimplement, and readily scales to high dimensions and large-scale datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:39:03 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 10:56:41 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 14:24:21 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Flaxman", "Seth", ""], ["Teh", "Yee Whye", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1610.08624", "submitter": "PeiXin Hou", "authors": "Peixin Hou, Hao Deng, Jiguang Yue, and Shuguang Liu", "title": "PCM and APCM Revisited: An Uncertainty Perspective", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a new look at the possibilistic c-means (PCM) and\nadaptive PCM (APCM) clustering algorithms from the perspective of uncertainty.\nThis new perspective offers us insights into the clustering process, and also\nprovides us greater degree of flexibility. We analyze the clustering behavior\nof PCM-based algorithms and introduce parameters $\\sigma_v$ and $\\alpha$ to\ncharacterize uncertainty of estimated bandwidth and noise level of the dataset\nrespectively. Then uncertainty (fuzziness) of membership values caused by\nuncertainty of the estimated bandwidth parameter is modeled by a conditional\nfuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show\nthat parameters $\\sigma_v$ and $\\alpha$ make the clustering process more easy\nto control, and main features of PCM and APCM are unified in this new\nclustering framework (UPCM). More specifically, UPCM reduces to PCM when we set\na small $\\alpha$ or a large $\\sigma_v$, and UPCM reduces to APCM when clusters\nare confined in their physical clusters and possible cluster elimination are\nensured. Finally we present further researches of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:41:23 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Hou", "Peixin", ""], ["Deng", "Hao", ""], ["Yue", "Jiguang", ""], ["Liu", "Shuguang", ""]]}, {"id": "1610.08628", "submitter": "Pierre Alquier", "authors": "Pierre Alquier and The Tien Mai and Massimiliano Pontil", "title": "Regret Bounds for Lifelong Learning", "comments": null, "journal-ref": "Proceedings of Machine Learning Research, 2017, vol. 54 (AISTAT\n  2017), pp. 261-269", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transfer learning in an online setting. Different\ntasks are presented sequentially and processed by a within-task algorithm. We\npropose a lifelong learning strategy which refines the underlying data\nrepresentation used by the within-task algorithm, thereby transferring\ninformation from one task to the next. We show that when the within-task\nalgorithm comes with some regret bound, our strategy inherits this good\nproperty. Our bounds are in expectation for a general loss function, and\nuniform for a convex loss. We discuss applications to dictionary learning and\nfinite set of predictors. In the latter case, we improve previous\n$O(1/\\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 06:19:27 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Alquier", "Pierre", ""], ["Mai", "The Tien", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1610.08637", "submitter": "Xin Tong Thomson", "authors": "Xi Chen and Jason D. Lee and Xin T. Tong and Yichen Zhang", "title": "Statistical Inference for Model Parameters in Stochastic Gradient\n  Descent", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic gradient descent (SGD) algorithm has been widely used in\nstatistical estimation for large-scale data due to its computational and memory\nefficiency. While most existing works focus on the convergence of the objective\nfunction or the error of the obtained solution, we investigate the problem of\nstatistical inference of true model parameters based on SGD when the population\nloss function is strongly convex and satisfies certain smoothness conditions.\nOur main contributions are two-fold. First, in the fixed dimension setup, we\npropose two consistent estimators of the asymptotic covariance of the average\niterate from SGD: (1) a plug-in estimator, and (2) a batch-means estimator,\nwhich is computationally more efficient and only uses the iterates from SGD.\nBoth proposed estimators allow us to construct asymptotically exact confidence\nintervals and hypothesis tests. Second, for high-dimensional linear regression,\nusing a variant of the SGD algorithm, we construct a debiased estimator of each\nregression coefficient that is asymptotically normal. This gives a one-pass\nalgorithm for computing both the sparse regression coefficients and confidence\nintervals, which is computationally attractive and applicable to online data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 07:04:21 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 01:50:24 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 03:52:42 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Xi", ""], ["Lee", "Jason D.", ""], ["Tong", "Xin T.", ""], ["Zhang", "Yichen", ""]]}, {"id": "1610.08696", "submitter": "Wataru Kumagai", "authors": "Wataru Kumagai", "title": "Learning Bound for Parameter Transfer Learning", "comments": "This paper was accepted at NIPS 2016 as a poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a transfer-learning problem by using the parameter transfer\napproach, where a suitable parameter of feature mapping is learned through one\ntask and applied to another objective task. Then, we introduce the notion of\nthe local stability and parameter transfer learnability of parametric feature\nmapping,and thereby derive a learning bound for parameter transfer algorithms.\nAs an application of parameter transfer learning, we discuss the performance of\nsparse coding in self-taught learning. Although self-taught learning algorithms\nwith plentiful unlabeled data often show excellent empirical performance, their\ntheoretical analysis has not been studied. In this paper, we also provide the\nfirst theoretical learning bound for self-taught learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 10:50:55 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 01:08:40 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 04:41:17 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Kumagai", "Wataru", ""]]}, {"id": "1610.08733", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke\n  Fujii, Alexis Boukouvalas, Pablo Le\\'on-Villagr\\'a, Zoubin Ghahramani, James\n  Hensman", "title": "GPflow: A Gaussian process library using TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPflow is a Gaussian process library that uses TensorFlow for its core\ncomputations and Python for its front end. The distinguishing features of\nGPflow are that it uses variational inference as the primary approximation\nmethod, provides concise code through the use of automatic differentiation, has\nbeen engineered with a particular emphasis on software testing and is able to\nexploit GPU hardware.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:08:10 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Matthews", "Alexander G. de G.", ""], ["van der Wilk", "Mark", ""], ["Nickson", "Tom", ""], ["Fujii", "Keisuke", ""], ["Boukouvalas", "Alexis", ""], ["Le\u00f3n-Villagr\u00e1", "Pablo", ""], ["Ghahramani", "Zoubin", ""], ["Hensman", "James", ""]]}, {"id": "1610.08735", "submitter": "Kieran Campbell", "authors": "Kieran R. Campbell and Christopher Yau", "title": "Stratification of patient trajectories using covariate latent variable\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard models assign disease progression to discrete categories or stages\nbased on well-characterized clinical markers. However, such a system is\npotentially at odds with our understanding of the underlying biology, which in\nhighly complex systems may support a (near-)continuous evolution of disease\nfrom inception to terminal state. To learn such a continuous disease score one\ncould infer a latent variable from dynamic \"omics\" data such as RNA-seq that\ncorrelates with an outcome of interest such as survival time. However, such\nanalyses may be confounded by additional data such as clinical covariates\nmeasured in electronic health records (EHRs). As a solution to this we\nintroduce covariate latent variable models, a novel type of latent variable\nmodel that learns a low-dimensional data representation in the presence of two\n(asymmetric) views of the same data source. We apply our model to TCGA\ncolorectal cancer RNA-seq data and demonstrate how incorporating\nmicrosatellite-instability (MSI) status as an external covariate allows us to\nidentify genes that stratify patients on an immune-response trajectory.\nFinally, we propose an extension termed Covariate Gaussian Process Latent\nVariable Models for learning nonparametric, nonlinear representations. An R\npackage implementing variational inference for covariate latent variable models\nis available at http://github.com/kieranrcampbell/clvm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:08:42 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 20:16:21 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Campbell", "Kieran R.", ""], ["Yau", "Christopher", ""]]}, {"id": "1610.08738", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven (PANAMA), Nicolas Tremblay (GIPSA-CICS), Yann\n  Traonmilin (PANAMA), R\\'emi Gribonval (PANAMA)", "title": "Compressive K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lloyd-Max algorithm is a classical approach to perform K-means\nclustering. Unfortunately, its cost becomes prohibitive as the training dataset\ngrows large. We propose a compressive version of K-means (CKM), that estimates\ncluster centers from a sketch, i.e. from a drastically compressed\nrepresentation of the training dataset. We demonstrate empirically that CKM\nperforms similarly to Lloyd-Max, for a sketch size proportional to the number\nof cen-troids times the ambient dimension, and independent of the size of the\noriginal dataset. Given the sketch, the computational complexity of CKM is also\nindependent of the size of the dataset. Unlike Lloyd-Max which requires several\nreplicates, we further demonstrate that CKM is almost insensitive to\ninitialization. For a large dataset of 10^7 data points, we show that CKM can\nrun two orders of magnitude faster than five replicates of Lloyd-Max, with\nsimilar clustering performance on artificial data. Finally, CKM achieves lower\nclassification errors on handwritten digits classification.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:13:05 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 07:58:05 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 10:40:53 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 15:22:24 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Keriven", "Nicolas", "", "PANAMA"], ["Tremblay", "Nicolas", "", "GIPSA-CICS"], ["Traonmilin", "Yann", "", "PANAMA"], ["Gribonval", "R\u00e9mi", "", "PANAMA"]]}, {"id": "1610.08749", "submitter": "Antti Honkela", "authors": "Joonas J\\\"alk\\\"o and Onur Dikmen and Antti Honkela", "title": "Differentially Private Variational Inference for Non-conjugate Models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications are based on data collected from people,\nsuch as their tastes and behaviour as well as biological traits and genetic\ndata. Regardless of how important the application might be, one has to make\nsure individuals' identities or the privacy of the data are not compromised in\nthe analysis. Differential privacy constitutes a powerful framework that\nprevents breaching of data subject privacy from the output of a computation.\nDifferentially private versions of many important Bayesian inference methods\nhave been proposed, but there is a lack of an efficient unified approach\napplicable to arbitrary models. In this contribution, we propose a\ndifferentially private variational inference method with a very wide\napplicability. It is built on top of doubly stochastic variational inference, a\nrecent advance which provides a variational solution to a large class of\nmodels. We add differential privacy into doubly stochastic variational\ninference by clipping and perturbing the gradients. The algorithm is made more\nefficient through privacy amplification from subsampling. We demonstrate the\nmethod can reach an accuracy close to non-private level under reasonably strong\nprivacy guarantees, clearly improving over previous sampling-based alternatives\nespecially in the strong privacy regime.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:34:36 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 12:59:55 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["J\u00e4lk\u00f6", "Joonas", ""], ["Dikmen", "Onur", ""], ["Honkela", "Antti", ""]]}, {"id": "1610.08813", "submitter": "Chengwei Sang", "authors": "Hong Sun, Chengwei Sang, Didier Le Ruyet", "title": "Sparse Signal Subspace Decomposition Based on Adaptive Over-complete\n  Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a subspace decomposition method based on an over-complete\ndictionary in sparse representation, called \"Sparse Signal Subspace\nDecomposition\" (or 3SD) method. This method makes use of a novel criterion\nbased on the occurrence frequency of atoms of the dictionary over the data set.\nThis criterion, well adapted to subspace-decomposition over a dependent basis\nset, adequately re ects the intrinsic characteristic of regularity of the\nsignal. The 3SD method combines variance, sparsity and component frequency\ncriteria into an unified framework. It takes benefits from using an\nover-complete dictionary which preserves details and from subspace\ndecomposition which rejects strong noise. The 3SD method is very simple with a\nlinear retrieval operation. It does not require any prior knowledge on\ndistributions or parameters. When applied to image denoising, it demonstrates\nhigh performances both at preserving fine details and suppressing strong noise.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 14:45:47 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Sun", "Hong", ""], ["Sang", "Chengwei", ""], ["Ruyet", "Didier Le", ""]]}, {"id": "1610.08838", "submitter": "Anand Rangarajan", "authors": "Anthony O. Smith and Anand Rangarajan", "title": "A Category Space Approach to Supervised Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised dimensionality reduction has emerged as an important theme in the\nlast decade. Despite the plethora of models and formulations, there is a lack\nof a simple model which aims to project the set of patterns into a space\ndefined by the classes (or categories). To this end, we set up a model in which\neach class is represented as a 1D subspace of the vector space formed by the\nfeatures. Assuming the set of classes does not exceed the cardinality of the\nfeatures, the model results in multi-class supervised learning in which the\nfeatures of each class are projected into the class subspace. Class\ndiscrimination is automatically guaranteed via the imposition of orthogonality\nof the 1D class sub-spaces. The resulting optimization problem - formulated as\nthe minimization of a sum of quadratic functions on a Stiefel manifold - while\nbeing non-convex (due to the constraints), nevertheless has a structure for\nwhich we can identify when we have reached a global minimum. After formulating\na version with standard inner products, we extend the formulation to\nreproducing kernel Hilbert spaces in a straightforward manner. The optimization\napproach also extends in a similar fashion to the kernel version. Results and\ncomparisons with the multi-class Fisher linear (and kernel) discriminants and\nprincipal component analysis (linear and kernel) showcase the relative merits\nof this approach to dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:30:35 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Smith", "Anthony O.", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1610.08861", "submitter": "Jie Chen", "authors": "Jie Chen, Dehua Cheng, Yan Liu", "title": "On Bochner's and Polya's Characterizations of Positive-Definite Kernels\n  and the Respective Random Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive-definite kernel functions are fundamental elements of kernel methods\nand Gaussian processes. A well-known construction of such functions comes from\nBochner's characterization, which connects a positive-definite function with a\nprobability distribution. Another construction, which appears to have attracted\nless attention, is Polya's criterion that characterizes a subset of these\nfunctions. In this paper, we study the latter characterization and derive a\nnumber of novel kernels little known previously.\n  In the context of large-scale kernel machines, Rahimi and Recht (2007)\nproposed a random feature map (random Fourier) that approximates a kernel\nfunction, through independent sampling of the probability distribution in\nBochner's characterization. The authors also suggested another feature map\n(random binning), which, although not explicitly stated, comes from Polya's\ncharacterization. We show that with the same number of random samples, the\nrandom binning map results in an Euclidean inner product closer to the kernel\nthan does the random Fourier map. The superiority of the random binning map is\nconfirmed empirically through regressions and classifications in the\nreproducing kernel Hilbert space.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 16:09:30 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Chen", "Jie", ""], ["Cheng", "Dehua", ""], ["Liu", "Yan", ""]]}, {"id": "1610.08927", "submitter": "Shariq Mobin", "authors": "Shariq Mobin, Joan Bruna", "title": "Voice Conversion using Convolutional Neural Networks", "comments": "Presented at the 2016 Machine Learning Summer School (MLSS) in Cadiz,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human auditory system is able to distinguish the vocal source of\nthousands of speakers, yet not much is known about what features the auditory\nsystem uses to do this. Fourier Transforms are capable of capturing the pitch\nand harmonic structure of the speaker but this alone proves insufficient at\nidentifying speakers uniquely. The remaining structure, often referred to as\ntimbre, is critical to identifying speakers but we understood little about it.\nIn this paper we use recent advances in neural networks in order to manipulate\nthe voice of one speaker into another by transforming not only the pitch of the\nspeaker, but the timbre. We review generative models built with neural networks\nas well as architectures for creating neural networks that learn analogies. Our\npreliminary results converting voices from one speaker to another are\nencouraging.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 18:58:06 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Mobin", "Shariq", ""], ["Bruna", "Joan", ""]]}, {"id": "1610.08928", "submitter": "M. Arjumand Masood", "authors": "M. Arjumand Masood and Finale Doshi-Velez", "title": "Rapid Posterior Exploration in Bayesian Non-negative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative Matrix Factorization (NMF) is a popular tool for data\nexploration. Bayesian NMF promises to also characterize uncertainty in the\nfactorization. Unfortunately, current inference approaches such as MCMC mix\nslowly and tend to get stuck on single modes. We introduce a novel approach\nusing rapidly-exploring random trees (RRTs) to asymptotically cover regions of\nhigh posterior density. These are placed in a principled Bayesian framework via\nan online extension to nonparametric variational inference. On experiments on\nreal and synthetic data, we obtain greater coverage of the posterior and higher\nELBO values than standard NMF inference approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 19:00:22 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Masood", "M. Arjumand", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1610.08936", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu,\n  Eric P. Xing", "title": "Learning Scalable Deep Kernels with Recurrent Structure", "comments": "37 pages, 7 figures, 5 tables. Updated to the final version that\n  appears in JMLR, 18(82):1-37, 2017", "journal-ref": "Journal of Machine Learning Research (JMLR), JMLR 18(82):1-37,\n  2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in speech, robotics, finance, and biology deal with\nsequential data, where ordering matters and recurrent structures are common.\nHowever, this structure cannot be easily captured by standard kernel functions.\nTo model such structure, we propose expressive closed-form kernel functions for\nGaussian processes. The resulting model, GP-LSTM, fully encapsulates the\ninductive biases of long short-term memory (LSTM) recurrent networks, while\nretaining the non-parametric probabilistic advantages of Gaussian processes. We\nlearn the properties of the proposed kernels by optimizing the Gaussian process\nmarginal likelihood using a new provably convergent semi-stochastic gradient\nprocedure and exploit the structure of these kernels for scalable training and\nprediction. This approach provides a practical representation for Bayesian\nLSTMs. We demonstrate state-of-the-art performance on several benchmarks, and\nthoroughly investigate a consequential autonomous driving application, where\nthe predictive uncertainties provided by GP-LSTM are uniquely valuable.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 19:08:57 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 06:49:55 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 01:14:56 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Wilson", "Andrew Gordon", ""], ["Saatchi", "Yunus", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1610.09033", "submitter": "Jaan Altosaar", "authors": "Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei", "title": "Operator Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:32:25 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 23:58:43 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:08:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Altosaar", "Jaan", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09034", "submitter": "Mikhail Yurochkin", "authors": "Mikhail Yurochkin, XuanLong Nguyen", "title": "Geometric Dirichlet Means algorithm for topic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric algorithm for topic learning and inference that is\nbuilt on the convex geometry of topics arising from the Latent Dirichlet\nAllocation (LDA) model and its nonparametric extensions. To this end we study\nthe optimization of a geometric loss function, which is a surrogate to the\nLDA's likelihood. Our method involves a fast optimization based weighted\nclustering procedure augmented with geometric corrections, which overcomes the\ncomputational and statistical inefficiencies encountered by other techniques\nbased on Gibbs sampling and variational inference, while achieving the accuracy\ncomparable to that of a Gibbs sampler. The topic estimates produced by our\nmethod are shown to be statistically consistent under some conditions. The\nalgorithm is evaluated with extensive experiments on simulated and real data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:35:57 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Yurochkin", "Mikhail", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1610.09038", "submitter": "Alex Lamb", "authors": "Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville,\n  Yoshua Bengio", "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "comments": "NIPS 2016 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Teacher Forcing algorithm trains recurrent networks by supplying observed\nsequence values as inputs during training and using the network's own\none-step-ahead predictions to do multi-step sampling. We introduce the\nProfessor Forcing algorithm, which uses adversarial domain adaptation to\nencourage the dynamics of the recurrent network to be the same when training\nthe network and when sampling from the network over multiple time steps. We\napply Professor Forcing to language modeling, vocal synthesis on raw waveforms,\nhandwriting generation, and image generation. Empirically we find that\nProfessor Forcing acts as a regularizer, improving test likelihood on character\nlevel Penn Treebank and sequential MNIST. We also find that the model\nqualitatively improves samples, especially when sampling for a large number of\ntime steps. This is supported by human evaluation of sample quality. Trade-offs\nbetween Professor Forcing and Scheduled Sampling are discussed. We produce\nT-SNEs showing that Professor Forcing successfully makes the dynamics of the\nnetwork during training and sampling more similar.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:54:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lamb", "Alex", ""], ["Goyal", "Anirudh", ""], ["Zhang", "Ying", ""], ["Zhang", "Saizheng", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1610.09072", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel\n  Holtmann-Rice, Sanjiv Kumar", "title": "Orthogonal Random Features", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an intriguing discovery related to Random Fourier Features: in\nGaussian kernel approximation, replacing the random Gaussian matrix by a\nproperly scaled random orthogonal matrix significantly decreases kernel\napproximation error. We call this technique Orthogonal Random Features (ORF),\nand provide theoretical and empirical justification for this behavior.\nMotivated by this discovery, we further propose Structured Orthogonal Random\nFeatures (SORF), which uses a class of structured discrete orthogonal matrices\nto speed up the computation. The method reduces the time cost from\n$\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\log d)$, where $d$ is the data\ndimensionality, with almost no compromise in kernel approximation quality\ncompared to ORF. Experiments on several datasets verify the effectiveness of\nORF and SORF over the existing methods. We also provide discussions on using\nthe same type of discrete orthogonal structure for a broader range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 03:50:00 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Yu", "Felix X.", ""], ["Suresh", "Ananda Theertha", ""], ["Choromanski", "Krzysztof", ""], ["Holtmann-Rice", "Daniel", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1610.09075", "submitter": "Jason Poulos", "authors": "Jason Poulos and Rafael Valle", "title": "Missing Data Imputation for Supervised Learning", "comments": null, "journal-ref": "Applied Artificial Intelligence, 32(2), 186-196 (2018)", "doi": "10.1080/08839514.2018.1448143", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data imputation can help improve the performance of prediction models\nin situations where missing data hide useful information. This paper compares\nmethods for imputing missing categorical data for supervised classification\ntasks. We experiment on two machine learning benchmark datasets with missing\ncategorical data, comparing classifiers trained on non-imputed (i.e., one-hot\nencoded) or imputed data with different levels of additional missing-data\nperturbation. We show imputation methods can increase predictive accuracy in\nthe presence of missing-data perturbation, which can actually improve\nprediction accuracy by regularizing the classifier. We achieve the\nstate-of-the-art on the Adult dataset with missing-data perturbation and\nk-nearest-neighbors (k-NN) imputation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 04:06:59 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:44:58 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Poulos", "Jason", ""], ["Valle", "Rafael", ""]]}, {"id": "1610.09083", "submitter": "Steven C.H. Hoi", "authors": "Yue Wu, Steven C.H. Hoi, Chenghao Liu, Jing Lu, Doyen Sahoo, Nenghai\n  Yu", "title": "SOL: A Library for Scalable Online Learning Algorithms", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SOL is an open-source library for scalable online learning algorithms, and is\nparticularly suitable for learning with high-dimensional data. The library\nprovides a family of regular and sparse online learning algorithms for\nlarge-scale binary and multi-class classification tasks with high efficiency,\nscalability, portability, and extensibility. SOL was implemented in C++, and\nprovided with a collection of easy-to-use command-line tools, python wrappers\nand library calls for users and developers, as well as comprehensive documents\nfor both beginners and advanced users. SOL is not only a practical machine\nlearning toolbox, but also a comprehensive experimental platform for online\nlearning research. Experiments demonstrate that SOL is highly efficient and\nscalable for large-scale machine learning with high-dimensional data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 05:47:51 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Wu", "Yue", ""], ["Hoi", "Steven C. H.", ""], ["Liu", "Chenghao", ""], ["Lu", "Jing", ""], ["Sahoo", "Doyen", ""], ["Yu", "Nenghai", ""]]}, {"id": "1610.09112", "submitter": "Sahar Khawatmi", "authors": "Sahar Khawatmi and Ali H. Sayed and Abdelhak M. Zoubir", "title": "Decentralized Clustering and Linking by Networked Agents", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2692736", "report-no": null, "categories": "math.OC cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decentralized clustering and estimation over\nmulti-task networks, where agents infer and track different models of interest.\nThe agents do not know beforehand which model is generating their own data.\nThey also do not know which agents in their neighborhood belong to the same\ncluster. We propose a decentralized clustering algorithm aimed at identifying\nand forming clusters of agents of similar objectives, and at guiding\ncooperation to enhance the inference performance. One key feature of the\nproposed technique is the integration of the learning and clustering tasks into\na single strategy. We analyze the performance of the procedure and show that\nthe error probabilities of types I and II decay exponentially to zero with the\nstep-size parameter. While links between agents following different objectives\nare ignored in the clustering process, we nevertheless show how to exploit\nthese links to relay critical information across the network for enhanced\nperformance. Simulation results illustrate the performance of the proposed\nmethod in comparison to other useful techniques.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 08:16:45 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Khawatmi", "Sahar", ""], ["Sayed", "Ali H.", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1610.09127", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana", "title": "Adaptive regularization for Lasso models in the context of\n  non-stationary data streams", "comments": "20 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1511.02187", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale, streaming datasets are ubiquitous in modern machine learning.\nStreaming algorithms must be scalable, amenable to incremental training and\nrobust to the presence of non-stationarity. In this work consider the problem\nof learning $\\ell_1$ regularized linear models in the context of streaming\ndata. In particular, the focus of this work revolves around how to select the\nregularization parameter when data arrives sequentially and the underlying\ndistribution is non-stationary (implying the choice of optimal regularization\nparameter is itself time-varying). We propose a framework through which to\ninfer an adaptive regularization parameter. Our approach employs an $\\ell_1$\npenalty constraint where the corresponding sparsity parameter is iteratively\nupdated via stochastic gradient descent. This serves to reformulate the choice\nof regularization parameter in a principled framework for online learning. The\nproposed method is derived for linear regression and subsequently extended to\ngeneralized linear models. We validate our approach using simulated and real\ndatasets and present an application to a neuroimaging dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:04:30 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 11:14:32 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1610.09156", "submitter": "Indranil Pan", "authors": "Indranil Pan and Dirk Bester", "title": "Fuzzy Bayesian Learning", "comments": "16 pages, 12 figures, submitted", "journal-ref": "IEEE Transactions on Fuzzy Systems, Vol. 26, Issue 3, June 2018,\n  pp. 1719-1731", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach for learning from data using rule\nbased fuzzy inference systems where the model parameters are estimated using\nBayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the\napplicability of the method for regression and classification tasks using\nsynthetic data-sets and also a real world example in the financial services\nindustry. Then we demonstrate how the method can be extended for knowledge\nextraction to select the individual rules in a Bayesian way which best explains\nthe given data. Finally we discuss the advantages and pitfalls of using this\nmethod over state-of-the-art techniques and highlight the specific class of\nproblems where this would be useful.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:23:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 10:28:07 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Pan", "Indranil", ""], ["Bester", "Dirk", ""]]}, {"id": "1610.09263", "submitter": "Vladimir Dzyuba", "authors": "Vladimir Dzyuba, Matthijs van Leeuwen, Luc De Raedt", "title": "Flexible constrained sampling with guarantees for pattern mining", "comments": "Accepted for publication in Data Mining & Knowledge Discovery journal\n  (ECML/PKDD 2017 journal track)", "journal-ref": null, "doi": "10.1007/s10618-017-0501-6", "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern sampling has been proposed as a potential solution to the infamous\npattern explosion. Instead of enumerating all patterns that satisfy the\nconstraints, individual patterns are sampled proportional to a given quality\nmeasure. Several sampling algorithms have been proposed, but each of them has\nits limitations when it comes to 1) flexibility in terms of quality measures\nand constraints that can be used, and/or 2) guarantees with respect to sampling\naccuracy. We therefore present Flexics, the first flexible pattern sampler that\nsupports a broad class of quality measures and constraints, while providing\nstrong guarantees regarding sampling accuracy. To achieve this, we leverage the\nperspective on pattern mining as a constraint satisfaction problem and build\nupon the latest advances in sampling solutions in SAT as well as existing\npattern mining algorithms. Furthermore, the proposed algorithm is applicable to\na variety of pattern languages, which allows us to introduce and tackle the\nnovel task of sampling sets of patterns. We introduce and empirically evaluate\ntwo variants of Flexics: 1) a generic variant that addresses the well-known\nitemset sampling task and the novel pattern set sampling task as well as a wide\nrange of expressive constraints within these tasks, and 2) a specialized\nvariant that exploits existing frequent itemset techniques to achieve\nsubstantial speed-ups. Experiments show that Flexics is both accurate and\nefficient, making it a useful tool for pattern-based data exploration.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:21:53 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 16:18:39 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Dzyuba", "Vladimir", ""], ["van Leeuwen", "Matthijs", ""], ["De Raedt", "Luc", ""]]}, {"id": "1610.09274", "submitter": "Guang-He Lee", "authors": "Guang-He Lee, Shao-Wen Yang, Shou-De Lin", "title": "Toward Implicit Sample Noise Modeling: Deviation-driven Matrix\n  Factorization", "comments": "6 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective function of a matrix factorization model usually aims to\nminimize the average of a regression error contributed by each element.\nHowever, given the existence of stochastic noises, the implicit deviations of\nsample data from their true values are almost surely diverse, which makes each\ndata point not equally suitable for fitting a model. In this case, simply\naveraging the cost among data in the objective function is not ideal.\nIntuitively we would like to emphasize more on the reliable instances (i.e.,\nthose contain smaller noise) while training a model. Motivated by such\nobservation, we derive our formula from a theoretical framework for optimal\nweighting under heteroscedastic noise distribution. Specifically, by modeling\nand learning the deviation of data, we design a novel matrix factorization\nmodel. Our model has two advantages. First, it jointly learns the deviation and\nconducts dynamic reweighting of instances, allowing the model to converge to a\nbetter solution. Second, during learning the deviated instances are assigned\nlower weights, which leads to faster convergence since the model does not need\nto overfit the noise. The experiments are conducted in clean recommendation and\nnoisy sensor datasets to test the effectiveness of the model in various\nscenarios. The results show that our model outperforms the state-of-the-art\nfactorization and deep learning models in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:33:25 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lee", "Guang-He", ""], ["Yang", "Shao-Wen", ""], ["Lin", "Shou-De", ""]]}, {"id": "1610.09296", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Kai Arulkumaran, Anil Anthony Bharath", "title": "Improving Sampling from Generative Autoencoders with Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on generative autoencoders, such as variational or adversarial\nautoencoders, which jointly learn a generative model alongside an inference\nmodel. Generative autoencoders are those which are trained to softly enforce a\nprior on the latent distribution learned by the inference model. We call the\ndistribution to which the inference model maps observed samples, the learned\nlatent distribution, which may not be consistent with the prior. We formulate a\nMarkov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively\ndecoding and encoding, which allows us to sample from the learned latent\ndistribution. Since, the generative model learns to map from the learned latent\ndistribution, rather than the prior, we may use MCMC to improve the quality of\nsamples drawn from the generative model, especially when the learned latent\ndistribution is far from the prior. Using MCMC sampling, we are able to reveal\npreviously unseen differences between generative autoencoders trained either\nwith or without a denoising criterion.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:17:03 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:17:32 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 16:13:14 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Creswell", "Antonia", ""], ["Arulkumaran", "Kai", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1610.09300", "submitter": "Antoine Gautier", "authors": "Antoine Gautier, Quynh Nguyen and Matthias Hein", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with\n  Nonlinear Spectral Methods", "comments": "Long version of NIPS 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization problem behind neural networks is highly non-convex.\nTraining with stochastic gradient descent and variants requires careful\nparameter tuning and provides no guarantee to achieve the global optimum. In\ncontrast we show under quite weak assumptions on the data that a particular\nclass of feedforward neural networks can be trained globally optimal with a\nlinear convergence rate with our nonlinear spectral method. Up to our knowledge\nthis is the first practically feasible method which achieves such a guarantee.\nWhile the method can in principle be applied to deep networks, we restrict\nourselves for simplicity in this paper to one and two hidden layer networks.\nOur experiments confirm that these models are rich enough to achieve good\nperformance on a series of real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:28:23 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Gautier", "Antoine", ""], ["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1610.09307", "submitter": "Han Guo", "authors": "Namrata Vaswani and Han Guo", "title": "Correlated-PCA: Principal Components' Analysis when Data and Noise are\n  Correlated", "comments": "This paper has been withdrawn. It should have been a replacement of\n  arXiv 1608.04320", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix of observed data, Principal Components Analysis (PCA) computes\na small number of orthogonal directions that contain most of its variability.\nProvably accurate solutions for PCA have been in use for a long time. However,\nto the best of our knowledge, all existing theoretical guarantees for it assume\nthat the data and the corrupting noise are mutually independent, or at least\nuncorrelated. This is valid in practice often, but not always. In this paper,\nwe study the PCA problem in the setting where the data and noise can be\ncorrelated. Such noise is often also referred to as \"data-dependent noise\". We\nobtain a correctness result for the standard eigenvalue decomposition (EVD)\nbased solution to PCA under simple assumptions on the data-noise correlation.\nWe also develop and analyze a generalization of EVD, cluster-EVD, that improves\nupon EVD in certain regimes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:37:39 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:44:44 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Vaswani", "Namrata", ""], ["Guo", "Han", ""]]}, {"id": "1610.09322", "submitter": "Yuan Deng", "authors": "Anima Anandkumar, Yuan Deng, Rong Ge, Hossein Mobahi", "title": "Homotopy Analysis for Tensor PCA", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient and guaranteed nonconvex algorithms has been an\nimportant challenge in modern machine learning. Algorithms with good empirical\nperformance such as stochastic gradient descent often lack theoretical\nguarantees. In this paper, we analyze the class of homotopy or continuation\nmethods for global optimization of nonconvex functions. These methods start\nfrom an objective function that is efficient to optimize (e.g. convex), and\nprogressively modify it to obtain the required objective, and the solutions are\npassed along the homotopy path. For the challenging problem of tensor PCA, we\nprove global convergence of the homotopy method in the \"high noise\" regime. The\nsignal-to-noise requirement for our algorithm is tight in the sense that it\nmatches the recovery guarantee for the best degree-4 sum-of-squares algorithm.\nIn addition, we prove a phase transition along the homotopy path for tensor\nPCA. This allows to simplify the homotopy method to a local search algorithm,\nviz., tensor power iterations, with a specific initialization and a noise\ninjection procedure, while retaining the theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 17:24:45 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 02:52:41 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 13:00:58 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 00:11:55 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Anandkumar", "Anima", ""], ["Deng", "Yuan", ""], ["Ge", "Rong", ""], ["Mobahi", "Hossein", ""]]}, {"id": "1610.09420", "submitter": "Liangbei Xu", "authors": "Liangbei Xu and Mark A. Davenport", "title": "Dynamic matrix recovery from incomplete observations under an exact\n  low-rank constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix factorizations arise in a wide variety of applications --\nincluding recommendation systems, topic models, and source separation, to name\njust a few. In these and many other applications, it has been widely noted that\nby incorporating temporal information and allowing for the possibility of\ntime-varying models, significant improvements are possible in practice.\nHowever, despite the reported superior empirical performance of these dynamic\nmodels over their static counterparts, there is limited theoretical\njustification for introducing these more complex models. In this paper we aim\nto address this gap by studying the problem of recovering a dynamically\nevolving low-rank matrix from incomplete observations. First, we propose the\nlocally weighted matrix smoothing (LOWEMS) framework as one possible approach\nto dynamic matrix recovery. We then establish error bounds for LOWEMS in both\nthe {\\em matrix sensing} and {\\em matrix completion} observation models. Our\nresults quantify the potential benefits of exploiting dynamic constraints both\nin terms of recovery accuracy and sample complexity. To illustrate these\nbenefits we provide both synthetic and real-world experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 22:44:29 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Xu", "Liangbei", ""], ["Davenport", "Mark A.", ""]]}, {"id": "1610.09463", "submitter": "Tadashi Wadayama", "authors": "Daisuke Ito and Tadashi Wadayama", "title": "Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting\n  Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose majority voting neural networks for sparse signal\nrecovery in binary compressed sensing. The majority voting neural network is\ncomposed of several independently trained feedforward neural networks employing\nthe sigmoid function as an activation function. Our empirical study shows that\na choice of a loss function used in training processes for the network is of\nprime importance. We found a loss function suitable for sparse signal recovery,\nwhich includes a cross entropy-like term and an $L_1$ regularized term. From\nthe experimental results, we observed that the majority voting neural network\nachieves excellent recovery performance, which is approaching the optimal\nperformance as the number of component nets grows. The simple architecture of\nthe majority voting neural networks would be beneficial for both software and\nhardware implementations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 06:12:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ito", "Daisuke", ""], ["Wadayama", "Tadashi", ""]]}, {"id": "1610.09490", "submitter": "Tommy L\\\"ofstedt", "authors": "Tommy L\\\"ofstedt, Fouad Hadj-Selem, Vincent Guillemot, Cathy Philippe,\n  Nicolas Raymond, Edouard Duchesney, Vincent Frouin and Arthur Tenenhaus", "title": "A general multiblock method for structured variable selection", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularised canonical correlation analysis was recently extended to more than\ntwo sets of variables by the multiblock method Regularised generalised\ncanonical correlation analysis (RGCCA). Further, Sparse GCCA (SGCCA) was\nproposed to address the issue of variable selection. However, for technical\nreasons, the variable selection offered by SGCCA was restricted to a covariance\nlink between the blocks (i.e., with $\\tau=1$). One of the main contributions of\nthis paper is to go beyond the covariance link and to propose an extension of\nSGCCA for the full RGCCA model (i.e., with $\\tau\\in[0, 1]$). In addition, we\npropose an extension of SGCCA that exploits structural relationships between\nvariables within blocks. Specifically, we propose an algorithm that allows\nstructured and sparsity-inducing penalties to be included in the RGCCA\noptimisation problem. The proposed multiblock method is illustrated on a real\nthree-block high-grade glioma data set, where the aim is to predict the\nlocation of the brain tumours, and on a simulated data set, where the aim is to\nillustrate the method's ability to reconstruct the true underlying weight\nvectors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 11:28:56 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["L\u00f6fstedt", "Tommy", ""], ["Hadj-Selem", "Fouad", ""], ["Guillemot", "Vincent", ""], ["Philippe", "Cathy", ""], ["Raymond", "Nicolas", ""], ["Duchesney", "Edouard", ""], ["Frouin", "Vincent", ""], ["Tenenhaus", "Arthur", ""]]}, {"id": "1610.09512", "submitter": "Nan Jiang", "authors": "Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert\n  E. Schapire", "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable", "comments": "42 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies systematic exploration for reinforcement learning with\nrich observations and function approximation. We introduce a new model called\ncontextual decision processes, that unifies and generalizes most prior\nsettings. Our first contribution is a complexity measure, the Bellman rank,\nthat we show enables tractable learning of near-optimal behavior in these\nprocesses and is naturally small for many well-studied reinforcement learning\nsettings. Our second contribution is a new reinforcement learning algorithm\nthat engages in systematic exploration to learn contextual decision processes\nwith low Bellman rank. Our algorithm provably learns near-optimal behavior with\na number of samples that is polynomial in all relevant parameters but\nindependent of the number of unique observations. The approach uses Bellman\nerror minimization with optimistic exploration and provides new insights into\nefficient exploration for reinforcement learning with function approximation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:01:58 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:21:45 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Jiang", "Nan", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1610.09540", "submitter": "Gang Wang", "authors": "Gang Wang, Georgios B. Giannakis, Jie Chen", "title": "Solving Large-scale Systems of Random Quadratic Equations via Stochastic\n  Truncated Amplitude Flow", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2652392", "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach termed \\emph{stochastic truncated amplitude flow} (STAF) is\ndeveloped to reconstruct an unknown $n$-dimensional real-/complex-valued signal\n$\\bm{x}$ from $m$ `phaseless' quadratic equations of the form\n$\\psi_i=|\\langle\\bm{a}_i,\\bm{x}\\rangle|$. This problem, also known as phase\nretrieval from magnitude-only information, is \\emph{NP-hard} in general.\nAdopting an amplitude-based nonconvex formulation, STAF leads to an iterative\nsolver comprising two stages: s1) Orthogonality-promoting initialization\nthrough a stochastic variance reduced gradient algorithm; and, s2) A series of\niterative refinements of the initialization using stochastic truncated gradient\niterations. Both stages involve a single equation per iteration, thus rendering\nSTAF a simple, scalable, and fast approach amenable to large-scale\nimplementations that is useful when $n$ is large. When $\\{\\bm{a}_i\\}_{i=1}^m$\nare independent Gaussian, STAF provably recovers exactly any\n$\\bm{x}\\in\\mathbb{R}^n$ exponentially fast based on order of $n$ quadratic\nequations. STAF is also robust in the presence of additive noise of bounded\nsupport. Simulated tests involving real Gaussian $\\{\\bm{a}_i\\}$ vectors\ndemonstrate that STAF empirically reconstructs any $\\bm{x}\\in\\mathbb{R}^n$\nexactly from about $2.3n$ magnitude-only measurements, outperforming\nstate-of-the-art approaches and narrowing the gap from the\ninformation-theoretic number of equations $m=2n-1$. Extensive experiments using\nsynthetic data and real images corroborate markedly improved performance of\nSTAF over existing alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 16:54:01 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""], ["Chen", "Jie", ""]]}, {"id": "1610.09585", "submitter": "Augustus Odena", "authors": "Augustus Odena, Christopher Olah, Jonathon Shlens", "title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high resolution photorealistic images has been a long-standing\nchallenge in machine learning. In this paper we introduce new methods for the\nimproved training of generative adversarial networks (GANs) for image\nsynthesis. We construct a variant of GANs employing label conditioning that\nresults in 128x128 resolution image samples exhibiting global coherence. We\nexpand on previous work for image quality assessment to provide two new\nanalyses for assessing the discriminability and diversity of samples from\nclass-conditional image synthesis models. These analyses demonstrate that high\nresolution samples provide class information not present in low resolution\nsamples. Across 1000 ImageNet classes, 128x128 samples are more than twice as\ndiscriminable as artificially resized 32x32 samples. In addition, 84.7% of the\nclasses have samples exhibiting diversity comparable to real ImageNet data.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 00:29:31 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 23:31:57 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 23:22:30 GMT"}, {"version": "v4", "created": "Thu, 20 Jul 2017 20:23:31 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Odena", "Augustus", ""], ["Olah", "Christopher", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1610.09600", "submitter": "Donald Lee", "authors": "Ningyuan Chen, Donald K.K. Lee, Sahand Negahban", "title": "Super-resolution estimation of cyclic arrival rates", "comments": "32 pages, 5 figures", "journal-ref": "Annals of Statistics 47:3:1754-1775 (2019)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the fact that most arrival processes exhibit cyclic behaviour, we\npropose a simple procedure for estimating the intensity of a nonhomogeneous\nPoisson process. The estimator is the super-resolution analogue to Shao 2010\nand Shao & Lii 2011, which is a sum of $p$ sinusoids where $p$ and the\nfrequency, amplitude, and phase of each wave are not known and need to be\nestimated. This results in an interpretable yet flexible specification that is\nsuitable for use in modelling as well as in high resolution simulations.\n  Our estimation procedure sits in between classic periodogram methods and\natomic/total variation norm thresholding. Through a novel use of window\nfunctions in the point process domain, our approach attains super-resolution\nwithout semidefinite programming. Under suitable conditions, finite sample\nguarantees can be derived for our procedure. These resolve some open questions\nand expand existing results in spectral estimation literature.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 03:21:05 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 07:34:20 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 05:05:43 GMT"}, {"version": "v4", "created": "Fri, 9 Feb 2018 05:54:08 GMT"}, {"version": "v5", "created": "Mon, 4 Jun 2018 08:21:16 GMT"}, {"version": "v6", "created": "Tue, 5 Jun 2018 04:25:40 GMT"}, {"version": "v7", "created": "Wed, 27 Feb 2019 23:29:14 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chen", "Ningyuan", ""], ["Lee", "Donald K. K.", ""], ["Negahban", "Sahand", ""]]}, {"id": "1610.09641", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias and Omiros Papaspiliopoulos", "title": "Auxiliary gradient-based sampling algorithms", "comments": "41 pages, 8 figures, 11 tables, To appear in Journal of the Royal\n  Statistical Society: Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of MCMC samplers that combine auxiliary variables,\nGibbs sampling and Taylor expansions of the target density. Our approach\npermits the marginalisation over the auxiliary variables yielding marginal\nsamplers, or the augmentation of the auxiliary variables, yielding auxiliary\nsamplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special\ncases. We prove that marginal samplers are superior in terms of asymptotic\nvariance and demonstrate cases where they are slower in computing time compared\nto auxiliary samplers. In the context of latent Gaussian models we propose new\nauxiliary and marginal samplers whose implementation requires a single tuning\nparameter, which can be found automatically during the transient phase.\nExtensive experimentation shows that the increase in efficiency (measured as\neffective sample size per unit of computing time) relative to (optimised\nimplementations of) pCNL, elliptical slice sampling and MALA ranges from\n10-fold in binary classification problems to 25-fold in log-Gaussian Cox\nprocesses to 100-fold in Gaussian process regression, and it is on par with\nRiemann manifold Hamiltonian Monte Carlo in an example where the latter has the\nsame complexity as the aforementioned algorithms. We explain this remarkable\nimprovement in terms of the way alternative samplers try to approximate the\neigenvalues of the target. We introduce a novel MCMC sampling scheme for\nhyperparameter learning that builds upon the auxiliary samplers. The MATLAB\ncode for reproducing the experiments in the article is publicly available and a\nSupplement to this article contains additional experiments and implementation\ndetails.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 12:13:15 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 16:03:52 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 16:38:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1610.09659", "submitter": "Gautier Marti", "authors": "Gautier Marti, Sebastien Andler, Frank Nielsen, Philippe Donnat", "title": "Exploring and measuring non-linear correlations: Copulas, Lightspeed\n  Transportation and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology to explore and measure the pairwise correlations\nthat exist between variables in a dataset. The methodology leverages copulas\nfor encoding dependence between two variables, state-of-the-art optimal\ntransport for providing a relevant geometry to the copulas, and clustering for\nsummarizing the main dependence patterns found between the variables. Some of\nthe clusters centers can be used to parameterize a novel dependence coefficient\nwhich can target or forget specific dependence patterns. Finally, we illustrate\nand benchmark the methodology on several datasets. Code and numerical\nexperiments are available online for reproducible research.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 15:03:51 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Marti", "Gautier", ""], ["Andler", "Sebastien", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1610.09704", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Ozlem Uzuner, Peter Szolovits", "title": "Feature-Augmented Neural Networks for Patient Note De-identification", "comments": "Accepted as a conference paper at COLING ClinicalNLP 2016. The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient notes contain a wealth of information of potentially great interest\nto medical investigators. However, to protect patients' privacy, Protected\nHealth Information (PHI) must be removed from the patient notes before they can\nbe legally released, a process known as patient note de-identification. The\nmain objective for a de-identification system is to have the highest possible\nrecall. Recently, the first neural-network-based de-identification system has\nbeen proposed, yielding state-of-the-art results. Unlike other systems, it does\nnot rely on human-engineered features, which allows it to be quickly deployed,\nbut does not leverage knowledge from human experts or from electronic health\nrecords (EHRs). In this work, we explore a method to incorporate\nhuman-engineered features as well as features derived from EHRs to a\nneural-network-based de-identification system. Our results show that the\naddition of features, especially the EHR-derived features, further improves the\nstate-of-the-art in patient note de-identification, including for some of the\nmost sensitive PHI types such as patient names. Since in a real-life setting\npatient notes typically come with EHRs, we recommend developers of\nde-identification systems to leverage the information EHRs contain.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 20:09:46 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Uzuner", "Ozlem", ""], ["Szolovits", "Peter", ""]]}, {"id": "1610.09730", "submitter": "Songbai Yan", "authors": "Songbai Yan, Kamalika Chaudhuri and Tara Javidi", "title": "Active Learning from Imperfect Labelers", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active learning where the labeler can not only return incorrect\nlabels but also abstain from labeling. We consider different noise and\nabstention conditions of the labeler. We propose an algorithm which utilizes\nabstention responses, and analyze its statistical consistency and query\ncomplexity under fairly natural assumptions on the noise and abstention rate of\nthe labeler. This algorithm is adaptive in a sense that it can automatically\nrequest less queries with a more informed or less noisy labeler. We couple our\nalgorithm with lower bounds to show that under some technical conditions, it\nachieves nearly optimal query complexity.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 23:39:18 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Yan", "Songbai", ""], ["Chaudhuri", "Kamalika", ""], ["Javidi", "Tara", ""]]}, {"id": "1610.09780", "submitter": "Rebecca Steorts", "authors": "Giacomo Zanella, Brenda Betancourt, Hanna Wallach, Jeffrey Miller,\n  Abbas Zaidi, and Rebecca C. Steorts", "title": "Flexible Models for Microclustering with Application to Entity\n  Resolution", "comments": "15 pages, 3 figures, 1 table, to appear NIPS 2016. arXiv admin note:\n  text overlap with arXiv:1512.00792", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some applications, this\nassumption is inappropriate. For example, when performing entity resolution,\nthe size of each cluster should be unrelated to the size of the data set, and\neach cluster should contain a negligible fraction of the total number of data\npoints. These applications require models that yield clusters whose sizes grow\nsublinearly with the size of the data set. We address this requirement by\ndefining the microclustering property and introducing a new class of models\nthat can exhibit this property. We compare models within this class to two\ncommonly used clustering models using four entity-resolution data sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:00:11 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Zanella", "Giacomo", ""], ["Betancourt", "Brenda", ""], ["Wallach", "Hanna", ""], ["Miller", "Jeffrey", ""], ["Zaidi", "Abbas", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1610.09787", "submitter": "Dustin Tran", "authors": "Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang,\n  David M. Blei", "title": "Edward: A library for probabilistic modeling, inference, and criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.PL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is a powerful approach for analyzing empirical\ninformation. We describe Edward, a library for probabilistic modeling. Edward's\ndesign reflects an iterative process pioneered by George Box: build a model of\na phenomenon, make inferences about the model given data, and criticize the\nmodel's fit to the data. Edward supports a broad class of probabilistic models,\nefficient algorithms for inference, and many techniques for model criticism.\nThe library builds on top of TensorFlow to support distributed training and\nhardware such as GPUs. Edward enables the development of complex probabilistic\nmodels and their algorithms at a massive scale.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:56:13 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 06:47:13 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 01:37:04 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tran", "Dustin", ""], ["Kucukelbir", "Alp", ""], ["Dieng", "Adji B.", ""], ["Rudolph", "Maja", ""], ["Liang", "Dawen", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09838", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni and Eric Maris", "title": "Analysis of Nonstationary Time Series Using Locally Coupled Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of nonstationary time series is of great importance in many\nscientific fields such as physics and neuroscience. In recent years, Gaussian\nprocess regression has attracted substantial attention as a robust and powerful\nmethod for analyzing time series. In this paper, we introduce a new framework\nfor analyzing nonstationary time series using locally stationary Gaussian\nprocess analysis with parameters that are coupled through a hidden Markov\nmodel. The main advantage of this framework is that arbitrary complex\nnonstationary covariance functions can be obtained by combining simpler\nstationary building blocks whose hidden parameters can be estimated in\nclosed-form. We demonstrate the flexibility of the method by analyzing two\nexamples of synthetic nonstationary signals: oscillations with time varying\nfrequency and time series with two dynamical states. Finally, we report an\nexample application on real magnetoencephalographic measurements of brain\nactivity.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 09:24:28 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ambrogioni", "Luca", ""], ["Maris", "Eric", ""]]}, {"id": "1610.09887", "submitter": "Itay Safran", "authors": "Itay Safran, Ohad Shamir", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide several new depth-based separation results for feed-forward neural\nnetworks, proving that various types of simple and natural functions can be\nbetter approximated using deeper networks than shallower ones, even if the\nshallower networks are much larger. This includes indicators of balls and\nellipses; non-linear functions which are radial with respect to the $L_1$ norm;\nand smooth non-linear functions. We also show that these gaps can be observed\nexperimentally: Increasing the depth indeed allows better learning than\nincreasing width, when training neural networks to learn an indicator of a unit\nball.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:08:46 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 18:07:37 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 12:08:04 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Safran", "Itay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1610.09900", "submitter": "Atilim Gunes Baydin", "authors": "Tuan Anh Le, Atilim Gunes Baydin, Frank Wood", "title": "Inference Compilation and Universal Probabilistic Programming", "comments": "11 pages, 6 figures", "journal-ref": "In Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017), 54:1338--1348. Proceedings of\n  Machine Learning Research. Fort Lauderdale, FL, USA: PMLR", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for using deep neural networks to amortize the cost of\ninference in models from the family induced by universal probabilistic\nprogramming languages, establishing a framework that combines the strengths of\nprobabilistic programming and deep learning methods. We call what we do\n\"compilation of inference\" because our method transforms a denotational\nspecification of an inference problem in the form of a probabilistic program\nwritten in a universal programming language into a trained neural network\ndenoted in a neural network specification language. When at test time this\nneural network is fed observational data and executed, it performs approximate\ninference in the original model specified by the probabilistic program. Our\ntraining objective and learning procedure are designed to allow the trained\nneural network to be used as a proposal distribution in a sequential importance\nsampling inference engine. We illustrate our method on mixture models and\nCaptcha solving and show significant speedups in the efficiency of inference.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:53:20 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 17:11:01 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Le", "Tuan Anh", ""], ["Baydin", "Atilim Gunes", ""], ["Wood", "Frank", ""]]}, {"id": "1610.09915", "submitter": "Rafael Boloix-Tortosa", "authors": "Rafael Boloix-Tortosa, Juan Jos\\'e Murillo-Fuentes, Irene Santos\n  Vel\\'azquez, and Fernando P\\'erez-Cruz", "title": "Complex-Valued Kernel Methods for Regression", "comments": "8 pages, 9 figures", "journal-ref": "IEEE Transactions on Signal Processing (Volume: 65, Issue: 19,\n  Oct.1, 1 2017)", "doi": "10.1109/TSP.2017.2726991", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, complex-valued RKHS are presented as an straightforward application\nof the real-valued case. In this paper we prove that this procedure yields a\nlimited solution for regression. We show that another kernel, here denoted as\npseudo kernel, is needed to learn any function in complex-valued fields.\nAccordingly, we derive a novel RKHS to include it, the widely RKHS (WRKHS).\nWhen the pseudo-kernel cancels, WRKHS reduces to complex-valued RKHS of\nprevious approaches. We address the kernel and pseudo-kernel design, paying\nattention to the kernel and the pseudo-kernel being complex-valued. In the\nexperiments included we report remarkable improvements in simple scenarios\nwhere real a imaginary parts have different similitude relations for given\ninputs or cases where real and imaginary parts are correlated. In the context\nof these novel results we revisit the problem of non-linear channel\nequalization, to show that the WRKHS helps to design more efficient solutions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:36:53 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Boloix-Tortosa", "Rafael", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["Vel\u00e1zquez", "Irene Santos", ""], ["P\u00e9rez-Cruz", "Fernando", ""]]}, {"id": "1610.10025", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger", "title": "Function Driven Diffusion for Personalized Counterfactual Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing diffusion operators high dimensional\ndata $X$ to address counterfactual functions $F$, such as individualized\ntreatment effectiveness. We propose and construct a new diffusion metric $K_F$\nthat captures both the local geometry of $X$ and the directions of variance of\n$F$. The resulting diffusion metric is then used to define a localized\nfiltration of $F$ and answer counterfactual questions pointwise, particularly\nin situations such as drug trials where an individual patient's outcomes cannot\nbe studied long term both taking and not taking a medication. We validate the\nmodel on synthetic and real world clinical trials, and create individualized\nnotions of benefit from treatment.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 17:26:27 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 14:17:18 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 21:21:08 GMT"}, {"version": "v4", "created": "Fri, 7 Apr 2017 13:47:29 GMT"}, {"version": "v5", "created": "Wed, 12 Apr 2017 13:10:32 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cloninger", "Alexander", ""]]}, {"id": "1610.10060", "submitter": "Alexandros Nathan", "authors": "Alexandros Nathan, Diego Klabjan", "title": "Optimization for Large-Scale Machine Learning with Distributed Features\n  and Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size of modern data sets exceeds the disk and memory capacities of a\nsingle computer, machine learning practitioners have resorted to parallel and\ndistributed computing. Given that optimization is one of the pillars of machine\nlearning and predictive modeling, distributed optimization methods have\nrecently garnered ample attention in the literature. Although previous research\nhas mostly focused on settings where either the observations, or features of\nthe problem at hand are stored in distributed fashion, the situation where both\nare partitioned across the nodes of a computer cluster (doubly distributed) has\nbarely been studied. In this work we propose two doubly distributed\noptimization algorithms. The first one falls under the umbrella of distributed\ndual coordinate ascent methods, while the second one belongs to the class of\nstochastic gradient/coordinate descent hybrid methods. We conduct numerical\nexperiments in Spark using real-world and simulated data sets and study the\nscaling properties of our methods. Our empirical evaluation of the proposed\nalgorithms demonstrates the out-performance of a block distributed ADMM method,\nwhich, to the best of our knowledge is the only other existing doubly\ndistributed optimization algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:43:21 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 01:10:43 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Nathan", "Alexandros", ""], ["Klabjan", "Diego", ""]]}, {"id": "1610.10064", "submitter": "Muhammad Bilal Zafar", "authors": "Miguel Ferreira, Muhammad Bilal Zafar, Krishna P. Gummadi", "title": "The Case for Temporal Transparency: Detecting Policy Change Events in\n  Black-Box Decision Making Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bringing transparency to black-box decision making systems (DMS) has been a\ntopic of increasing research interest in recent years. Traditional active and\npassive approaches to make these systems transparent are often limited by\nscalability and/or feasibility issues. In this paper, we propose a new notion\nof black-box DMS transparency, named, temporal transparency, whose goal is to\ndetect if/when the DMS policy changes over time, and is mostly invariant to the\ndrawbacks of traditional approaches. We map our notion of temporal transparency\nto time series changepoint detection methods, and develop a framework to detect\npolicy changes in real-world DMS's. Experiments on New York\nStop-question-and-frisk dataset reveal a number of publicly announced and\nunannounced policy changes, highlighting the utility of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:54:56 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ferreira", "Miguel", ""], ["Zafar", "Muhammad Bilal", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "1610.10087", "submitter": "Chuan-Yung Tsai", "authors": "Chuan-Yung Tsai, Andrew Saxe, David Cox", "title": "Tensor Switching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network algorithm, the Tensor Switching (TS)\nnetwork, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to\ntensor-valued hidden units. The TS network copies its entire input vector to\ndifferent locations in an expanded representation, with the location determined\nby its hidden unit activity. In this way, even a simple linear readout from the\nTS representation can implement a highly expressive deep-network-like function.\nThe TS network hence avoids the vanishing gradient problem by construction, at\nthe cost of larger representation size. We develop several methods to train the\nTS network, including equivalent kernels for infinitely wide and deep TS\nnetworks, a one-pass linear learning algorithm, and two\nbackpropagation-inspired representation learning algorithms. Our experimental\nresults demonstrate that the TS network is indeed more expressive and\nconsistently learns faster than standard ReLU networks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 19:44:50 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Tsai", "Chuan-Yung", ""], ["Saxe", "Andrew", ""], ["Cox", "David", ""]]}]