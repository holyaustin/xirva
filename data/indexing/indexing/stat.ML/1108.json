[{"id": "1108.0445", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T Tokdar", "title": "Adaptive Gaussian Predictive Process Approximation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of knots selection for Gaussian predictive process\nmethodology. Predictive process approximation provides an effective solution to\nthe cubic order computational complexity of Gaussian process models. This\napproximation crucially depends on a set of points, called knots, at which the\noriginal process is retained, while the rest is approximated via a\ndeterministic extrapolation. Knots should be few in number to keep the\ncomputational complexity low, but provide a good coverage of the process domain\nto limit approximation error. We present theoretical calculations to show that\ncoverage must be judged by the canonical metric of the Gaussian process. This\nnecessitates having in place a knots selection algorithm that automatically\nadapts to the changes in the canonical metric affected by changes in the\nparameter values controlling the Gaussian process covariance function. We\npresent an algorithm toward this by employing an incomplete Cholesky\nfactorization with pivoting and dynamic stopping. Although these concepts\nalready exist in the literature, our contribution lies in unifying them into a\nfast algorithm and in using computable error bounds to finesse implementation\nof the predictive process approximation. The resulting adaptive predictive\nprocess offers a substantial automatization of Guassian process model fitting,\nespecially for Bayesian applications where thousands of values of the\ncovariance parameters are to be explored.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 22:23:13 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Tokdar", "Surya T", ""]]}, {"id": "1108.0453", "submitter": "Vladimir Nikulin", "authors": "Vladimir Nikulin", "title": "On the Evaluation Criterions for the Active Learning Processes", "comments": "This paper relates to the WCCI 2010 Active Learning data mining\n  Contest. The author participated in the above Contest and attended a special\n  presentation by the Organisers at the WCCI 2010 in Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data mining applications collection of sufficiently large datasets is\nthe most time consuming and expensive. On the other hand, industrial methods of\ndata collection create huge databases, and make difficult direct applications\nof the advanced machine learning algorithms. To address the above problems, we\nconsider active learning (AL), which may be very efficient either for the\nexperimental design or for the data filtering. In this paper we demonstrate\nusing the online evaluation opportunity provided by the AL Challenge that quite\ncompetitive results may be produced using a small percentage of the available\ndata. Also, we present several alternative criteria, which may be useful for\nthe evaluation of the active learning processes. The author of this paper\nattended special presentation in Barcelona, where results of the WCCI 2010 AL\nChallenge were discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 00:23:20 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Nikulin", "Vladimir", ""]]}, {"id": "1108.0775", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt), Rodolphe Jenatton\n  (LIENS, INRIA Paris - Rocquencourt), Julien Mairal, Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Optimization with Sparsity-Inducing Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods are aimed at using or obtaining parsimonious\nrepresentations of data or models. They were first dedicated to linear variable\nselection but numerous extensions have now emerged such as structured sparsity\nor kernel selection. It turns out that many of the related estimation problems\ncan be cast as convex optimization problems by regularizing the empirical risk\nwith appropriate non-smooth norms. The goal of this paper is to present from a\ngeneral perspective optimization tools and techniques dedicated to such\nsparsity-inducing penalties. We cover proximal methods, block-coordinate\ndescent, reweighted $\\ell_2$-penalized techniques, working-set and homotopy\nmethods, as well as non-convex formulations and extensions, and provide an\nextensive set of experiments to compare various algorithms from a computational\npoint of view.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 07:55:19 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 09:59:21 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1108.0895", "submitter": "Ping Li", "authors": "Ping Li and Christian Konig", "title": "Accurate Estimators for Improving Minwise Hashing and b-Bit Minwise\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing is the standard technique in the context of search and\ndatabases for efficiently estimating set (e.g., high-dimensional 0/1 vector)\nsimilarities. Recently, b-bit minwise hashing was proposed which significantly\nimproves upon the original minwise hashing in practice by storing only the\nlowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing\nis particularly effective in applications which mainly concern sets of high\nsimilarities (e.g., the resemblance >0.5). However, there are other important\napplications in which not just pairs of high similarities matter. For example,\nmany learning algorithms require all pairwise similarities and it is expected\nthat only a small fraction of the pairs are similar. Furthermore, many\napplications care more about containment (e.g., how much one object is\ncontained by another object) than the resemblance. In this paper, we show that\nthe estimators for minwise hashing and b-bit minwise hashing used in the\ncurrent practice can be systematically improved and the improvements are most\nsignificant for set pairs of low resemblance and high containment.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 17:08:11 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Christian", ""]]}, {"id": "1108.1483", "submitter": "Paul von B\\\"unau", "authors": "Franz J. Kiraly, Paul von Buenau, Frank C. Meinecke, Duncan A. J.\n  Blythe and Klaus-Robert Mueller", "title": "Algebraic Geometric Comparison of Probability Distributions", "comments": null, "journal-ref": "Journal of Machine Learning Research 13(Mar):855-903. 2012", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algebraic framework for treating probability distributions\nrepresented by their cumulants such as the mean and covariance matrix. As an\nexample, we consider the unsupervised learning problem of finding the subspace\non which several probability distributions agree. Instead of minimizing an\nobjective function involving the estimated cumulants, we show that by treating\nthe cumulants as elements of the polynomial ring we can directly solve the\nproblem, at a lower computational cost and with higher accuracy. Moreover, the\nalgebraic viewpoint on probability distributions allows us to invoke the theory\nof Algebraic Geometry, which we demonstrate in a compact proof for an\nidentifiability criterion.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 14:02:44 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2012 15:59:00 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Kiraly", "Franz J.", ""], ["von Buenau", "Paul", ""], ["Meinecke", "Frank C.", ""], ["Blythe", "Duncan A. J.", ""], ["Mueller", "Klaus-Robert", ""]]}, {"id": "1108.1766", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Activized Learning: Transforming Passive to Active with Improved Label\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the theoretical advantages of active learning over passive learning.\nSpecifically, we prove that, in noise-free classifier learning for VC classes,\nany passive learning algorithm can be transformed into an active learning\nalgorithm with asymptotically strictly superior label complexity for all\nnontrivial target functions and distributions. We further provide a general\ncharacterization of the magnitudes of these improvements in terms of a novel\ngeneralization of the disagreement coefficient. We also extend these results to\nactive learning in the presence of label noise, and find that even under broad\nclasses of noise distributions, we can typically guarantee strict improvements\nover the known results for passive learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2011 18:04:02 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1108.1783", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy and Anand Rangarajan and Arunava Banerjee", "title": "An application of the stationary phase method for estimating probability\n  densities of function derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a novel result wherein the density function of the\ngradients---corresponding to density function of the derivatives in one\ndimension---of a thrice differentiable function S (obtained via a random\nvariable transformation of a uniformly distributed random variable) defined on\na closed, bounded interval \\Omega \\subset R is accurately approximated by the\nnormalized power spectrum of \\phi=exp(iS/\\tau) as the free parameter \\tau-->0.\nThe result is shown using the well known stationary phase approximation and\nstandard integration techniques and requires proper ordering of limits.\nExperimental results provide anecdotal visual evidence corroborating the\nresult.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2011 19:24:41 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 15:42:05 GMT"}, {"version": "v3", "created": "Tue, 8 May 2012 14:04:54 GMT"}, {"version": "v4", "created": "Sun, 3 Feb 2013 13:21:56 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Rangarajan", "Anand", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1108.2228", "submitter": "Minh Tang", "authors": "Daniel L. Sussman, Minh Tang, Donniell E. Fishkind, Carey E. Priebe", "title": "A consistent adjacency spectral embedding for stochastic blockmodel\n  graphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate block membership of nodes in a random graph\ngenerated by a stochastic blockmodel. We use an embedding procedure motivated\nby the random dot product graph model, a particular example of the latent\nposition model. The embedding associates each node with a vector; these vectors\nare clustered via minimization of a square error criterion. We prove that this\nmethod is consistent for assigning nodes to blocks, as only a negligible number\nof nodes will be mis-assigned. We prove consistency of the method for directed\nand undirected graphs. The consistent block assignment makes possible\nconsistent parameter estimation for a stochastic blockmodel. We extend the\nresult in the setting where the number of blocks grows slowly with the number\nof nodes. Our method is also computationally feasible even for very large\ngraphs. We compare our method to Laplacian spectral clustering through analysis\nof simulated data and a graph derived from Wikipedia documents.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 17:34:52 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2011 14:54:16 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2012 11:29:13 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Fishkind", "Donniell E.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1108.2401", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, Laurent J. Jacob, Martin J. Wainwright", "title": "A More Powerful Two-Sample Test in High Dimensions using Random\n  Projection", "comments": "Version 3 is an extended version of our NIPS 2011 conference paper.\n  This should be regarded as the final version and cited as a NIPS 2011 paper.\n  Note that version3=version1. Also, version 2 should be considered as defunct,\n  as it contains an error in the variance formula in equation (4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the hypothesis testing problem of detecting a shift between the\nmeans of two multivariate normal distributions in the high-dimensional setting,\nallowing for the data dimension p to exceed the sample size n. Specifically, we\npropose a new test statistic for the two-sample test of means that integrates a\nrandom projection with the classical Hotelling T^2 statistic. Working under a\nhigh-dimensional framework with (p,n) tending to infinity, we first derive an\nasymptotic power function for our test, and then provide sufficient conditions\nfor it to achieve greater power than other state-of-the-art tests. Using ROC\ncurves generated from synthetic data, we demonstrate superior performance\nagainst competing tests in the parameter regimes anticipated by our theoretical\nresults. Lastly, we illustrate an advantage of our procedure's false positive\nrate with comparisons on high-dimensional gene expression data involving the\ndiscrimination of different types of cancer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2011 13:45:47 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2012 07:44:56 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 22:33:28 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Lopes", "Miles E.", ""], ["Jacob", "Laurent J.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1108.2805", "submitter": "Daniel Rockmore", "authors": "Greg Leibon, Scott Pauls, Daniel N. Rockmore, and Robert Savell", "title": "Partition Decomposition for Roll Call Data", "comments": "45 pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we bring to bear some new tools from statistical learning on\nthe analysis of roll call data. We present a new data-driven model for roll\ncall voting that is geometric in nature. We construct the model by adapting the\n\"Partition Decoupling Method,\" an unsupervised learning technique originally\ndeveloped for the analysis of families of time series, to produce a multiscale\ngeometric description of a weighted network associated to a set of roll call\nvotes. Central to this approach is the quantitative notion of a \"motivation,\" a\ncluster-based and learned basis element that serves as a building block in the\nrepresentation of roll call data. Motivations enable the formulation of a\nquantitative description of ideology and their data-dependent nature makes\npossible a quantitative analysis of the evolution of ideological factors. This\napproach is generally applicable to roll call data and we apply it in\nparticular to the historical roll call voting of the U.S. House and Senate.\nThis methodology provides a mechanism for estimating the dimension of the\nunderlying action space. We determine that the dominant factors form a low-\n(one- or two-) dimensional representation with secondary factors adding\nhigher-dimensional features. In this way our work supports and extends the\nfindings of both Poole-Rosenthal and Heckman-Snyder concerning the\ndimensionality of the action space. We give a detailed analysis of several\nindividual Senates and use the AdaBoost technique from statistical learning to\ndetermine those votes with the most powerful discriminatory value. When used as\na predictive model, this geometric view significantly outperforms spatial\nmodels such as the Poole-Rosenthal DW-NOMINATE model and the Heckman-Snyder\n6-factor model, both in raw accuracy as well as Aggregate Proportional Reduced\nError (APRE).\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2011 16:19:09 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Leibon", "Greg", ""], ["Pauls", "Scott", ""], ["Rockmore", "Daniel N.", ""], ["Savell", "Robert", ""]]}, {"id": "1108.2820", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Ensemble Risk Modeling Method for Robust Learning on Scarce Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical risk modeling, typical data are \"scarce\": they have relatively\nsmall number of training instances (N), censoring, and high dimensionality (M).\nWe show that the problem may be effectively simplified by reducing it to\nbipartite ranking, and introduce new bipartite ranking algorithm, Smooth Rank,\nfor robust learning on scarce data. The algorithm is based on ensemble learning\nwith unsupervised aggregation of predictors. The advantage of our approach is\nconfirmed in comparison with two \"gold standard\" risk modeling methods on 10\nreal life survival analysis datasets, where the new approach has the best\nresults on all but two datasets with the largest ratio N/M. For systematic\nstudy of the effects of data scarcity on modeling by all three methods, we\nconducted two types of computational experiments: on real life data with\nrandomly drawn training sets of different sizes, and on artificial data with\nincreasing number of features. Both experiments demonstrated that Smooth Rank\nhas critical advantage over the popular methods on the scarce data; it does not\nsuffer from overfitting where other methods do.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2011 20:47:30 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2012 07:51:50 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1108.2836", "submitter": "Julien Cornebise", "authors": "J. Cornebise, E. Moulines, J. Olsson", "title": "Adaptive sequential Monte Carlo by means of mixture of experts", "comments": "24 pages, 9 figures, under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriately designing the proposal kernel of particle filters is an issue\nof significant importance, since a bad choice may lead to deterioration of the\nparticle sample and, consequently, waste of computational power. In this paper\nwe introduce a novel algorithm adaptively approximating the so-called optimal\nproposal kernel by a mixture of integrated curved exponential distributions\nwith logistic weights. This family of distributions, referred to as mixtures of\nexperts, is broad enough to be used in the presence of multi-modality or\nstrongly skewed distributions. The mixtures are fitted, via online-EM methods,\nto the optimal kernel through minimisation of the Kullback-Leibler divergence\nbetween the auxiliary target and instrumental distributions of the particle\nfilter. At each iteration of the particle filter, the algorithm is required to\nsolve only a single optimisation problem for the whole particle sample,\nyielding an algorithm with only linear complexity. In addition, we illustrate\nin a simulation study how the method can be successfully applied to optimal\nfiltering in nonlinear state-space models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:17:37 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 14:12:05 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Cornebise", "J.", ""], ["Moulines", "E.", ""], ["Olsson", "J.", ""]]}, {"id": "1108.2840", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an, Geoffrey J. Goodhill", "title": "Generalised elastic nets", "comments": "52 pages, 16 figures. Original manuscript dated August 14, 2003 and\n  not updated since. Current authors' email addresses:\n  mcarreira-perpinan@ucmerced.edu, g.goodhill@uq.edu.au", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elastic net was introduced as a heuristic algorithm for combinatorial\noptimisation and has been applied, among other problems, to biological\nmodelling. It has an energy function which trades off a fitness term against a\ntension term. In the original formulation of the algorithm the tension term was\nimplicitly based on a first-order derivative. In this paper we generalise the\nelastic net model to an arbitrary quadratic tension term, e.g. derived from a\ndiscretised differential operator, and give an efficient learning algorithm. We\nrefer to these as generalised elastic nets (GENs). We give a theoretical\nanalysis of the tension term for 1D nets with periodic boundary conditions, and\nshow that the model is sensitive to the choice of finite difference scheme that\nrepresents the discretised derivative. We illustrate some of these issues in\nthe context of cortical map models, by relating the choice of tension term to a\ncortical interaction function. In particular, we prove that this interaction\ntakes the form of a Mexican hat for the original elastic net, and of\nprogressively more oscillatory Mexican hats for higher-order derivatives. The\nresults apply not only to generalised elastic nets but also to other methods\nusing discrete differential penalties, and are expected to be useful in other\nareas, such as data analysis, computer graphics and optimisation problems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:47:14 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Goodhill", "Geoffrey J.", ""]]}, {"id": "1108.2903", "submitter": "Boumediene Hamzi", "authors": "Jake Bouvrie and Boumediene Hamzi", "title": "Kernel Methods for the Approximation of Nonlinear Systems", "comments": "Rewritten to improve readability. arXiv admin note: text overlap with\n  arXiv:1011.2952", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven order reduction method for nonlinear control\nsystems, drawing on recent progress in machine learning and statistical\ndimensionality reduction. The method rests on the assumption that the nonlinear\nsystem behaves linearly when lifted into a high (or infinite) dimensional\nfeature space where balanced truncation may be carried out implicitly. This\nleads to a nonlinear reduction map which can be combined with a representation\nof the system belonging to a reproducing kernel Hilbert space to give a closed,\nreduced order dynamical system which captures the essential input-output\ncharacteristics of the original model. Empirical simulations illustrating the\napproach are also provided.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 20:44:13 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2011 20:22:59 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 20:53:16 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Bouvrie", "Jake", ""], ["Hamzi", "Boumediene", ""]]}, {"id": "1108.2989", "submitter": "Indraneel Mukherjee", "authors": "Indraneel Mukherjee and Robert E. Schapire", "title": "A theory of multiclass boosting", "comments": "A preliminary version appeared in NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting combines weak classifiers to form highly accurate predictors.\nAlthough the case of binary classification is well understood, in the\nmulticlass setting, the \"correct\" requirements on the weak classifier, or the\nnotion of the most efficient boosting algorithms are missing. In this paper, we\ncreate a broad and general framework, within which we make precise and identify\nthe optimal requirements on the weak-classifier, as well as design the most\neffective, in a certain sense, boosting algorithms that assume such\nrequirements.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 13:26:26 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Mukherjee", "Indraneel", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1108.3072", "submitter": "Ping Li", "authors": "Ping Li, Anshumali Shrivastava, Christian Konig", "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise\n  Hashing and Comparisons with Vowpal Wabbit (VW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit\nminwise hashing algorithms for training very large-scale logistic regression\nand SVM. The results confirm our prior work that, compared with the VW hashing\nalgorithm (which has the same variance as random projections), b-bit minwise\nhashing is substantially more accurate at the same storage. For example, with\nmerely 30 hashed values per data point, b-bit minwise hashing can achieve\nsimilar accuracies as VW with 2^14 hashed values per data point.\n  We demonstrate that the preprocessing cost of b-bit minwise hashing is\nroughly on the same order of magnitude as the data loading time. Furthermore,\nby using a GPU, the preprocessing cost can be reduced to a small fraction of\nthe data loading time.\n  Minwise hashing has been widely used in industry, at least in the context of\nsearch. One reason for its popularity is that one can efficiently simulate\npermutations by (e.g.,) universal hashing. In other words, there is no need to\nstore the permutation matrix. In this paper, we empirically verify this\npractice, by demonstrating that even using the simplest 2-universal hashing\ndoes not degrade the learning performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 19:53:55 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Li", "Ping", ""], ["Shrivastava", "Anshumali", ""], ["Konig", "Christian", ""]]}, {"id": "1108.3154", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Stability Conditions for Online Learnability", "comments": "16 pages. Earlier version of this work submitted (but rejected) to\n  COLT 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is a general notion that quantifies the sensitivity of a learning\nalgorithm's output to small change in the training dataset (e.g. deletion or\nreplacement of a single training sample). Such conditions have recently been\nshown to be more powerful to characterize learnability in the general learning\nsetting under i.i.d. samples where uniform convergence is not necessary for\nlearnability, but where stability is both sufficient and necessary for\nlearnability. We here show that similar stability conditions are also\nsufficient for online learnability, i.e. whether there exists a learning\nalgorithm such that under any sequence of examples (potentially chosen\nadversarially) produces a sequence of hypotheses that has no regret in the\nlimit with respect to the best hypothesis in hindsight. We introduce online\nstability, a stability condition related to uniform-leave-one-out stability in\nthe batch setting, that is sufficient for online learnability. In particular we\nshow that popular classes of online learners, namely algorithms that fall in\nthe category of Follow-the-(Regularized)-Leader, Mirror Descent, gradient-based\nmethods and randomized algorithms like Weighted Majority and Hedge, are\nguaranteed to have no regret if they have such online stability property. We\nprovide examples that suggest the existence of an algorithm with such stability\ncondition might in fact be necessary for online learnability. For the more\nrestricted binary classification setting, we establish that such stability\ncondition is in fact both sufficient and necessary. We also show that for a\nlarge class of online learnable problems in the general learning setting,\nnamely those with a notion of sub-exponential covering, no-regret online\nalgorithms that have such stability condition exists.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 05:11:54 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 17:01:35 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1108.3259", "submitter": "Souhaib Ben Taieb", "authors": "Souhaib Ben Taieb and Gianluca Bontempi and Amir Atiya and Antti\n  Sorjamaa", "title": "A review and comparison of strategies for multi-step ahead time series\n  forecasting based on the NN5 forecasting competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step ahead forecasting is still an open challenge in time series\nforecasting. Several approaches that deal with this complex problem have been\nproposed in the literature but an extensive comparison on a large number of\ntasks is still missing. This paper aims to fill this gap by reviewing existing\nstrategies for multi-step ahead forecasting and comparing them in theoretical\nand practical terms. To attain such an objective, we performed a large scale\ncomparison of these different strategies using a large experimental benchmark\n(namely the 111 series from the NN5 forecasting competition). In addition, we\nconsidered the effects of deseasonalization, input variable selection, and\nforecast combination on these strategies and on multi-step ahead forecasting at\nlarge. The following three findings appear to be consistently supported by the\nexperimental results: Multiple-Output strategies are the best performing\napproaches, deseasonalization leads to uniformly improved forecast accuracy,\nand input selection is more effective when performed in conjunction with\ndeseasonalization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 14:55:20 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Taieb", "Souhaib Ben", ""], ["Bontempi", "Gianluca", ""], ["Atiya", "Amir", ""], ["Sorjamaa", "Antti", ""]]}, {"id": "1108.3298", "submitter": "Nando de Freitas", "authors": "Byron Knoll, Nando de Freitas", "title": "A Machine Learning Perspective on Predictive Coding with PAQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 18:06:29 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Knoll", "Byron", ""], ["de Freitas", "Nando", ""]]}, {"id": "1108.3350", "submitter": "Wei Lu", "authors": "Wei Lu and Namrata Vaswani", "title": "Exact Reconstruction Conditions for Regularized Modified Basis Pursuit", "comments": "17 pages", "journal-ref": "IEEE Transactions on Signal Processing, May 2012", "doi": "10.1109/TSP.2012.2186445", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this correspondence, we obtain exact recovery conditions for regularized\nmodified basis pursuit (reg-mod-BP) and discuss when the obtained conditions\nare weaker than those for modified-CS or for basis pursuit (BP). The discussion\nis also supported by simulation comparisons. Reg-mod-BP provides a solution to\nthe sparse recovery problem when both an erroneous estimate of the signal's\nsupport, denoted by $T$, and an erroneous estimate of the signal values on $T$\nare available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 20:47:28 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 04:32:01 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Lu", "Wei", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1108.3372", "submitter": "Miguel L\\'azaro Gredilla", "authors": "Miguel L\\'azaro-Gredilla, Steven Van Vaerenbergh, and Neil Lawrence", "title": "Overlapping Mixtures of Gaussian Processes for the Data Association\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a mixture of GPs to address the data association\nproblem, i.e. to label a group of observations according to the sources that\ngenerated them. Unlike several previously proposed GP mixtures, the novel\nmixture has the distinct characteristic of using no gating function to\ndetermine the association of samples and mixture components. Instead, all the\nGPs in the mixture are global and samples are clustered following\n\"trajectories\" across input space. We use a non-standard variational Bayesian\nalgorithm to efficiently recover sample labels and learn the hyperparameters.\nWe show how multi-object tracking problems can be disambiguated and also\nexplore the characteristics of the model in traditional regression settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 23:46:59 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["L\u00e1zaro-Gredilla", "Miguel", ""], ["Van Vaerenbergh", "Steven", ""], ["Lawrence", "Neil", ""]]}, {"id": "1108.3476", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Structured Sparsity and Generalization", "comments": null, "journal-ref": "Journal of Machine Learning Research, 13:671-690, 2012", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data dependent generalization bound for a large class of\nregularized algorithms which implement structured sparsity constraints. The\nbound can be applied to standard squared-norm regularization, the Lasso, the\ngroup Lasso, some versions of the group Lasso with overlapping groups, multiple\nkernel learning and other regularization schemes. In all these cases\ncompetitive results are obtained. A novel feature of our bound is that it can\nbe applied in an infinite dimensional setting such as the Lasso in a separable\nHilbert space or multiple kernel learning with a countable number of kernels.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 13:36:11 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 08:47:01 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1108.3829", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Trevor Hastie", "title": "Exact covariance thresholding into connected components for large-scale\n  Graphical Lasso", "comments": "Report Version 2 (adding more experiments and correcting minor typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sparse inverse covariance regularization problem or graphical\nlasso with regularization parameter $\\rho$. Suppose the co- variance graph\nformed by thresholding the entries of the sample covariance matrix at $\\rho$ is\ndecomposed into connected components. We show that the vertex-partition induced\nby the thresholded covariance graph is exactly equal to that induced by the\nestimated concentration graph. This simple rule, when used as a wrapper around\nexisting algorithms, leads to enormous performance gains. For large values of\n$\\rho$, our proposal splits a large graphical lasso problem into smaller\ntractable problems, making it possible to solve an otherwise infeasible large\nscale graphical lasso problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 19:52:38 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 03:06:36 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Mazumder", "Rahul", ""], ["Hastie", "Trevor", ""]]}, {"id": "1108.4079", "submitter": "Jason J Corso", "authors": "Jason J. Corso", "title": "Toward Parts-Based Scene Understanding with Pixel-Support Parts-Sparse\n  Pictorial Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding remains a significant challenge in the computer vision\ncommunity. The visual psychophysics literature has demonstrated the importance\nof interdependence among parts of the scene. Yet, the majority of methods in\ncomputer vision remain local. Pictorial structures have arisen as a fundamental\nparts-based model for some vision problems, such as articulated object\ndetection. However, the form of classical pictorial structures limits their\napplicability for global problems, such as semantic pixel labeling. In this\npaper, we propose an extension of the pictorial structures approach, called\npixel-support parts-sparse pictorial structures, or PS3, to overcome this\nlimitation. Our model extends the classical form in two ways: first, it defines\nparts directly based on pixel-support rather than in a parametric form, and\nsecond, it specifies a space of plausible parts-based scene models and permits\none to be used for inference on any given image. PS3 makes strides toward\nunifying object-level and pixel-level modeling of scene elements. In this\nreport, we implement the first half of our model and rely upon external\nknowledge to provide an initial graph structure for a given image. Our\nexperimental results on benchmark datasets demonstrate the capability of this\nnew parts-based view of scene modeling.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 02:08:45 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Corso", "Jason J.", ""]]}, {"id": "1108.4146", "submitter": "Xun Huan", "authors": "Xun Huan and Youssef M. Marzouk", "title": "Simulation-based optimal Bayesian experimental design for nonlinear\n  systems", "comments": "Preprint 53 pages, 17 figures (54 small figures). v1 submitted to the\n  Journal of Computational Physics on August 4, 2011; v2 submitted on August\n  12, 2012. v2 changes: (a) addition of Appendix B and Figure 17 to address the\n  bias in the expected utility estimator; (b) minor language edits; v3\n  submitted on November 30, 2012. v3 changes: minor edits", "journal-ref": "Journal of Computational Physics 232 (2013) 288-317", "doi": "10.1016/j.jcp.2012.08.013", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal selection of experimental conditions is essential to maximizing\nthe value of data for inference and prediction, particularly in situations\nwhere experiments are time-consuming and expensive to conduct. We propose a\ngeneral mathematical framework and an algorithmic approach for optimal\nexperimental design with nonlinear simulation-based models; in particular, we\nfocus on finding sets of experiments that provide the most information about\ntargeted sets of parameters.\n  Our framework employs a Bayesian statistical setting, which provides a\nfoundation for inference from noisy, indirect, and incomplete data, and a\nnatural mechanism for incorporating heterogeneous sources of information. An\nobjective function is constructed from information theoretic measures,\nreflecting expected information gain from proposed combinations of experiments.\nPolynomial chaos approximations and a two-stage Monte Carlo sampling method are\nused to evaluate the expected information gain. Stochastic approximation\nalgorithms are then used to make optimization feasible in computationally\nintensive and high-dimensional settings. These algorithms are demonstrated on\nmodel problems and on nonlinear parameter estimation problems arising in\ndetailed combustion kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 22:49:15 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 18:46:49 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 23:34:15 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1108.4324", "submitter": "Niels Lovmand  Pedersen", "authors": "Niels Lovmand Pedersen and Carles Navarro Manch\\'on and Mihai-Alin\n  Badiu and Dmitriy Shutin and Bernard Henri Fleury", "title": "Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real\n  and Complex Linear Models", "comments": "The paper provides a new comprehensive analysis of the theoretical\n  foundations of the proposed estimators. Minor modification of the title", "journal-ref": "Signal Processing 115 (2015) 94-109", "doi": "10.1016/j.sigpro.2015.03.013", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been\nused to model sparsity-inducing priors that realize a class of concave penalty\nfunctions for the regression task in real-valued signal models. Motivated by\nthe relative scarcity of formal tools for SBL in complex-valued models, this\npaper proposes a GSM model - the Bessel K model - that induces concave penalty\nfunctions for the estimation of complex sparse signals. The properties of the\nBessel K model are analyzed when it is applied to Type I and Type II\nestimation. This analysis reveals that, by tuning the parameters of the mixing\npdf different penalty functions are invoked depending on the estimation type\nused, the value of the noise variance, and whether real or complex signals are\nestimated. Using the Bessel K model, we derive a sparse estimator based on a\nmodification of the expectation-maximization algorithm formulated for Type II\nestimation. The estimator includes as a special instance the algorithms\nproposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical results\nshow the superiority of the proposed estimator over these state-of-the-art\nestimators in terms of convergence speed, sparseness, reconstruction error, and\nrobustness in low and medium signal-to-noise ratio regimes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2011 14:12:11 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 09:51:14 GMT"}, {"version": "v3", "created": "Sun, 26 Oct 2014 10:19:56 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Pedersen", "Niels Lovmand", ""], ["Manch\u00f3n", "Carles Navarro", ""], ["Badiu", "Mihai-Alin", ""], ["Shutin", "Dmitriy", ""], ["Fleury", "Bernard Henri", ""]]}, {"id": "1108.4879", "submitter": "Brendan Tracey", "authors": "Brendan Tracey, David Wolpert and Juan J. Alonso", "title": "Using Supervised Learning to Improve Monte Carlo Integral Estimation", "comments": "18 pages, 10 figures, originally published by AIAA at the 13th\n  Non-Deterministic Approaches Conference", "journal-ref": "13th AIAA Non-Deterministic Approaches Conference, Denver, CO,\n  April 2011, AIAA Paper 2011-1843", "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) techniques are often used to estimate integrals of a\nmultivariate function using randomly generated samples of the function. In\nlight of the increasing interest in uncertainty quantification and robust\ndesign applications in aerospace engineering, the calculation of expected\nvalues of such functions (e.g. performance measures) becomes important.\nHowever, MC techniques often suffer from high variance and slow convergence as\nthe number of samples increases. In this paper we present Stacked Monte Carlo\n(StackMC), a new method for post-processing an existing set of MC samples to\nimprove the associated integral estimate. StackMC is based on the supervised\nlearning techniques of fitting functions and cross validation. It should reduce\nthe variance of any type of Monte Carlo integral estimate (simple sampling,\nimportance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We\nreport on an extensive set of experiments confirming that the StackMC estimate\nof an integral is more accurate than both the associated unprocessed Monte\nCarlo estimate and an estimate based on a functional fit to the MC samples.\nThese experiments run over a wide variety of integration spaces, numbers of\nsample points, dimensions, and fitting functions. In particular, we apply\nStackMC in estimating the expected value of the fuel burn metric of future\ncommercial aircraft and in estimating sonic boom loudness measures. We compare\nthe efficiency of StackMC with that of more standard methods and show that for\nnegligible additional computational cost significant increases in accuracy are\ngained.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 16:22:55 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Tracey", "Brendan", ""], ["Wolpert", "David", ""], ["Alonso", "Juan J.", ""]]}, {"id": "1108.4988", "submitter": "Cun-Hui Zhang", "authors": "Cun-Hui Zhang and Tong Zhang", "title": "A General Theory of Concave Regularization for High Dimensional Sparse\n  Estimation Problems", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concave regularization methods provide natural procedures for sparse\nrecovery. However, they are difficult to analyze in the high dimensional\nsetting. Only recently a few sparse recovery results have been established for\nsome specific local solutions obtained via specialized numerical procedures.\nStill, the fundamental relationship between these solutions such as whether\nthey are identical or their relationship to the global minimizer of the\nunderlying nonconvex formulation is unknown. The current paper fills this\nconceptual gap by presenting a general theoretical framework showing that under\nappropriate conditions, the global solution of nonconvex regularization leads\nto desirable recovery performance; moreover, under suitable conditions, the\nglobal solution corresponds to the unique sparse local solution, which can be\nobtained via different numerical procedures. Under this unified framework, we\npresent an overview of existing results and discuss their connections. The\nunified view of this work leads to a more satisfactory treatment of concave\nhigh dimensional sparse estimation procedures, and serves as guideline for\ndeveloping further numerical procedures for concave regularization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 01:48:58 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2012 03:24:58 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1108.5244", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano", "title": "Semi-supervised logistic discrimination via labeled data and unlabeled\n  data from different sampling distributions", "comments": "19 pages", "journal-ref": "Statistical Analysis and Data Mining 6 (2013) 472-481", "doi": "10.1002/sam.11204", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of classification method based on both\nlabeled and unlabeled data, where we assume that a density function for labeled\ndata is different from that for unlabeled data. We propose a semi-supervised\nlogistic regression model for classification problem along with the technique\nof covariate shift adaptation. Unknown parameters involved in proposed models\nare estimated by regularization with EM algorithm. A crucial issue in the\nmodeling process is the choices of tuning parameters in our semi-supervised\nlogistic models. In order to select the parameters, a model selection criterion\nis derived from an information-theoretic approach. Some numerical studies show\nthat our modeling procedure performs well in various cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 05:38:58 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 09:33:24 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2012 10:26:20 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Kawano", "Shuichi", ""]]}, {"id": "1108.5397", "submitter": "Charles Bergeron PhD", "authors": "Charles Bergeron, Theresa Hepburn, C. Matthew Sundling, Michael Krein,\n  Bill Katt, Nagamani Sukumar, Curt M. Breneman, Kristin P. Bennett", "title": "Prediction of peptide bonding affinity: kernel methods for nonlinear\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents regression models obtained from a process of blind\nprediction of peptide binding affinity from provided descriptors for several\ndistinct datasets as part of the 2006 Comparative Evaluation of Prediction\nAlgorithms (COEPRA) contest. This paper finds that kernel partial least\nsquares, a nonlinear partial least squares (PLS) algorithm, outperforms PLS,\nand that the incorporation of transferable atom equivalent features improves\npredictive capability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 21:21:51 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Bergeron", "Charles", ""], ["Hepburn", "Theresa", ""], ["Sundling", "C. Matthew", ""], ["Krein", "Michael", ""], ["Katt", "Bill", ""], ["Sukumar", "Nagamani", ""], ["Breneman", "Curt M.", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1108.5669", "submitter": "Maria Florina Balcan", "authors": "Maria Florina Balcan, Florin Constantin, Satoru Iwata, Lei Wang", "title": "Learning Valuation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the approximate learnability of valuations commonly\nused throughout economics and game theory for the quantitative encoding of\nagent preferences. We provide upper and lower bounds regarding the learnability\nof important subclasses of valuation functions that express\nno-complementarities. Our main results concern their approximate learnability\nin the distributional learning (PAC-style) setting. We provide nearly tight\nlower and upper bounds of $\\tilde{\\Theta}(n^{1/2})$ on the approximation factor\nfor learning XOS and subadditive valuations, both widely studied superclasses\nof submodular valuations. Interestingly, we show that the\n$\\tilde{\\Omega}(n^{1/2})$ lower bound can be circumvented for XOS functions of\npolynomial complexity; we provide an algorithm for learning the class of XOS\nvaluations with a representation of polynomial size achieving an $O(n^{\\eps})$\napproximation factor in time $O(n^{1/\\eps})$ for any $\\eps > 0$. This\nhighlights the importance of considering the complexity of the target function\nfor polynomial time learning. We also provide new learning results for\ninteresting subclasses of submodular functions.\n  Our upper bounds for distributional learning leverage novel structural\nresults for all these valuation classes. We show that many of these results\nprovide new learnability results in the Goemans et al. model (SODA 2009) of\napproximate learning everywhere via value queries.\n  We also introduce a new model that is more realistic in economic settings, in\nwhich the learner can set prices and observe purchase decisions at these prices\nrather than observing the valuation function directly. In this model, most of\nour upper bounds continue to hold despite the fact that the learner receives\nless information (both for learning in the distributional setting and with\nvalue queries), while our lower bounds naturally extend.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 17:48:28 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 18:07:58 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Constantin", "Florin", ""], ["Iwata", "Satoru", ""], ["Wang", "Lei", ""]]}, {"id": "1108.5838", "submitter": "Zai Yang", "authors": "Zai Yang, Lihua Xie, and Cishen Zhang", "title": "Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference", "comments": "To appear in the IEEE Trans. Signal Processing. This is a revised,\n  shortened version of version 2", "journal-ref": null, "doi": "10.1109/TSP.2012.2222378", "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direction of arrival (DOA) estimation is a classical problem in signal\nprocessing with many practical applications. Its research has recently been\nadvanced owing to the development of methods based on sparse signal\nreconstruction. While these methods have shown advantages over conventional\nones, there are still difficulties in practical situations where true DOAs are\nnot on the discretized sampling grid. To deal with such an off-grid DOA\nestimation problem, this paper studies an off-grid model that takes into\naccount effects of the off-grid DOAs and has a smaller modeling error. An\niterative algorithm is developed based on the off-grid model from a Bayesian\nperspective while joint sparsity among different snapshots is exploited by\nassuming a Laplace prior for signals at all snapshots. The new approach applies\nto both single snapshot and multi-snapshot cases. Numerical simulations show\nthat the proposed algorithm has improved accuracy in terms of mean squared\nestimation error. The algorithm can maintain high estimation accuracy even\nunder a very coarse sampling grid.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 06:01:45 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 04:09:16 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2012 02:45:21 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2012 11:46:47 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Yang", "Zai", ""], ["Xie", "Lihua", ""], ["Zhang", "Cishen", ""]]}, {"id": "1108.6003", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Massimiliano Zanin, Perfecto Herrera and Xavier Serra", "title": "Characterization and exploitation of community structure in cover song\n  networks", "comments": null, "journal-ref": "Pattern Recognition Letters 33(9): 1032-1041, 2012", "doi": "10.1016/j.patrec.2012.02.013", "report-no": null, "categories": "cs.IR cs.MM cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of community detection algorithms is explored within the framework of\ncover song identification, i.e. the automatic detection of different audio\nrenditions of the same underlying musical piece. Until now, this task has been\nposed as a typical query-by-example task, where one submits a query song and\nthe system retrieves a list of possible matches ranked by their similarity to\nthe query. In this work, we propose a new approach which uses song communities\nto provide more relevant answers to a given query. Starting from the output of\na state-of-the-art system, songs are embedded in a complex weighted network\nwhose links represent similarity (related musical content). Communities inside\nthe network are then recognized as groups of covers and this information is\nused to enhance the results of the system. In particular, we show that this\napproach increases both the coherence and the accuracy of the system.\nFurthermore, we provide insight into the internal organization of individual\ncover song communities, showing that there is a tendency for the original song\nto be central within the community. We postulate that the methods and results\npresented here could be relevant to other query-by-example tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 15:25:20 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 10:46:30 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Zanin", "Massimiliano", ""], ["Herrera", "Perfecto", ""], ["Serra", "Xavier", ""]]}, {"id": "1108.6094", "submitter": "David Bailey", "authors": "Orianna DeMasi, Juan Meza, David H. Bailey", "title": "Dimension Reduction Using Rule Ensemble Machine Learning Methods: A\n  Numerical Study of Three Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Ensemble methods for supervised machine learning have become popular due to\ntheir ability to accurately predict class labels with groups of simple,\nlightweight \"base learners.\" While ensembles offer computationally efficient\nmodels that have good predictive capability they tend to be large and offer\nlittle insight into the patterns or structure in a dataset. We consider an\nensemble technique that returns a model of ranked rules. The model accurately\npredicts class labels and has the advantage of indicating which parameter\nconstraints are most useful for predicting those labels. An example of the rule\nensemble method successfully ranking rules and selecting attributes is given\nwith a dataset containing images of potential supernovas where the number of\nnecessary features is reduced from 39 to 21. We also compare the rule ensemble\nmethod on a set of multi-class problems with boosting and bagging, which are\ntwo well known ensemble techniques that use decision trees as base learners,\nbut do not have a rule ranking scheme.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 22:36:15 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["DeMasi", "Orianna", ""], ["Meza", "Juan", ""], ["Bailey", "David H.", ""]]}]