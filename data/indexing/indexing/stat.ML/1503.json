[{"id": "1503.00024", "submitter": "Sharan Vaswani", "authors": "Sharan Vaswani, Laks.V.S. Lakshmanan and Mark Schmidt", "title": "Influence Maximization with Bandits", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:59:08 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 20:42:52 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 19:53:49 GMT"}, {"version": "v4", "created": "Wed, 27 Apr 2016 18:27:20 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Vaswani", "Sharan", ""], ["Lakshmanan", "Laks. V. S.", ""], ["Schmidt", "Mark", ""]]}, {"id": "1503.00036", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "Norm-Based Capacity Control in Neural Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capacity, convexity and characterization of a general\nfamily of norm-constrained feed-forward networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:50:22 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 22:55:08 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1503.00038", "submitter": "Md Amran Siddiqui", "authors": "Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich and Weng-Keen Wong", "title": "Sequential Feature Explanations for Anomaly Detection", "comments": "9 pages, 4 figures and submitted to KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 00:04:11 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Siddiqui", "Md Amran", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.00135", "submitter": "Lucas Theis", "authors": "Lucas Theis, Philipp Berens, Emmanouil Froudarakis, Jacob Reimer,\n  Miroslav Rom\\'an Ros\\'on, Tom Baden, Thomas Euler, Andreas Tolias, Matthias\n  Bethge", "title": "Supervised learning sets benchmark for robust spike detection from\n  calcium imaging signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in calcium imaging has been to infer the timing of\naction potentials from the measured noisy calcium fluorescence traces. We\nsystematically evaluate a range of spike inference algorithms on a large\nbenchmark dataset recorded from varying neural tissue (V1 and retina) using\ndifferent calcium indicators (OGB-1 and GCamp6). We show that a new algorithm\nbased on supervised learning in flexible probabilistic models outperforms all\npreviously published techniques, setting a new standard for spike inference\nfrom calcium signals. Importantly, it performs better than other algorithms\neven on datasets not seen during training. Future data acquired in new\nexperimental conditions can easily be used to further improve its spike\nprediction accuracy and generalization performance. Finally, we show that\ncomparing algorithms on artificial data is not informative about performance on\nreal population imaging data, suggesting that a benchmark dataset may greatly\nfacilitate future algorithmic developments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 14:52:33 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Theis", "Lucas", ""], ["Berens", "Philipp", ""], ["Froudarakis", "Emmanouil", ""], ["Reimer", "Jacob", ""], ["Ros\u00f3n", "Miroslav Rom\u00e1n", ""], ["Baden", "Tom", ""], ["Euler", "Thomas", ""], ["Tolias", "Andreas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1503.00164", "submitter": "Yuan Yao", "authors": "Braxton Osting and Jiechao Xiong and Qianqian Xu and Yuan Yao", "title": "Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse\n  Random Graphs", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis, 2016", "doi": "10.1016/j.acha.2016.03.007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms are now extensively used for conducting subjective\npairwise comparison studies. In this setting, a pairwise comparison dataset is\ntypically gathered via random sampling, either \\emph{with} or \\emph{without}\nreplacement. In this paper, we use tools from random graph theory to analyze\nthese two random sampling methods for the HodgeRank estimator. Using the\nFiedler value of the graph as a measurement for estimator stability\n(informativeness), we provide a new estimate of the Fiedler value for these two\nrandom graph models. In the asymptotic limit as the number of vertices tends to\ninfinity, we prove the validity of the estimate. Based on our findings, for a\nsmall number of items to be compared, we recommend a two-stage sampling\nstrategy where a greedy sampling method is used initially and random sampling\n\\emph{without} replacement is used in the second stage. When a large number of\nitems is to be compared, we recommend random sampling with replacement as this\nis computationally inexpensive and trivially parallelizable. Experiments on\nsynthetic and real-world datasets support our analysis.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 18:32:45 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 11:47:10 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Osting", "Braxton", ""], ["Xiong", "Jiechao", ""], ["Xu", "Qianqian", ""], ["Yao", "Yuan", ""]]}, {"id": "1503.00173", "submitter": "Jonathan Mei", "authors": "Jonathan Mei and Jos\\'e M. F. Moura", "title": "Signal Processing on Graphs: Causal Modeling of Unstructured Data", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 8, pp.\n  2077-2092, April 15, 2017", "doi": "10.1109/TSP.2016.2634543", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications collect a large number of time series, for example, the\nfinancial data of companies quoted in a stock exchange, the health care data of\nall patients that visit the emergency room of a hospital, or the temperature\nsequences continuously measured by weather stations across the US. These data\nare often referred to as unstructured. A first task in its analytics is to\nderive a low dimensional representation, a graph or discrete manifold, that\ndescribes well the interrelations among the time series and their\nintrarelations across time. This paper presents a computationally tractable\nalgorithm for estimating this graph that structures the data. The resulting\ngraph is directed and weighted, possibly capturing causal relations, not just\nreciprocal correlations as in many existing approaches in the literature. A\nconvergence analysis is carried out. The algorithm is demonstrated on random\ngraph datasets and real network time series datasets, and its performance is\ncompared to that of related methods. The adjacency matrices estimated with the\nnew method are close to the true graph in the simulated data and consistent\nwith prior physical knowledge in the real dataset tested.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 20:28:05 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 20:58:45 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 13:19:02 GMT"}, {"version": "v4", "created": "Mon, 31 Oct 2016 22:05:33 GMT"}, {"version": "v5", "created": "Wed, 30 Nov 2016 19:12:41 GMT"}, {"version": "v6", "created": "Wed, 8 Feb 2017 15:49:58 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Mei", "Jonathan", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1503.00214", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Thomas C. M. Lee", "title": "Matrix Completion with Noisy Entries and Outliers", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of matrix completion when the observed\nentries are noisy and contain outliers. It begins with introducing a new\noptimization criterion for which the recovered matrix is defined as its\nsolution. This criterion uses the celebrated Huber function from the robust\nstatistics literature to downweigh the effects of outliers. A practical\nalgorithm is developed to solve the optimization involved. This algorithm is\nfast, straightforward to implement, and monotonic convergent. Furthermore, the\nproposed methodology is theoretically shown to be stable in a well defined\nsense. Its promising empirical performance is demonstrated via a sequence of\nsimulation experiments, including image inpainting.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 04:24:42 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 20:56:46 GMT"}, {"version": "v3", "created": "Wed, 27 Dec 2017 05:27:42 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1503.00269", "submitter": "Marco Loog", "authors": "Marco Loog", "title": "Contrastive Pessimistic Likelihood Estimation for Semi-Supervised\n  Classification", "comments": "32 pages, minor revision submitted to TPAMI, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvement guarantees for semi-supervised classifiers can currently only be\ngiven under restrictive conditions on the data. We propose a general way to\nperform semi-supervised parameter estimation for likelihood-based classifiers\nfor which, on the full training set, the estimates are never worse than the\nsupervised solution in terms of the log-likelihood. We argue, moreover, that we\nmay expect these solutions to really improve upon the supervised classifier in\nparticular cases. In a worked-out example for LDA, we take it one step further\nand essentially prove that its semi-supervised version is strictly better than\nits supervised counterpart. The two new concepts that form the core of our\nestimation principle are contrast and pessimism. The former refers to the fact\nthat our objective function takes the supervised estimates into account,\nenabling the semi-supervised solution to explicitly control the potential\nimprovements over this estimate. The latter refers to the fact that our\nestimates are conservative and therefore resilient to whatever form the true\nlabeling of the unlabeled data takes on. Experiments demonstrate the\nimprovements in terms of both the log-likelihood and the classification error\nrate on independent test sets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 13:16:43 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 21:36:53 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Loog", "Marco", ""]]}, {"id": "1503.00282", "submitter": "Vladimir Temlyakov", "authors": "V.N. Temlyakov", "title": "Constructive sparse trigonometric approximation for functions with small\n  mixed smoothness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper gives a constructive method, based on greedy algorithms, that\nprovides for the classes of functions with small mixed smoothness the best\npossible in the sense of order approximation error for the $m$-term\napproximation with respect to the trigonometric system.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 14:13:41 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Temlyakov", "V. N.", ""]]}, {"id": "1503.00323", "submitter": "Efr\\'en Cruz Cort\\'es", "authors": "E. Cruz Cort\\'es, C. Scott", "title": "Sparse Approximation of a Kernel Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel means are frequently used to represent probability distributions in\nmachine learning problems. In particular, the well known kernel density\nestimator and the kernel mean embedding both have the form of a kernel mean.\nUnfortunately, kernel means are faced with scalability issues. A single point\nevaluation of the kernel density estimator, for example, requires a computation\ntime linear in the training sample size. To address this challenge, we present\na method to efficiently construct a sparse approximation of a kernel mean. We\ndo so by first establishing an incoherence-based bound on the approximation\nerror, and then noticing that, for the case of radial kernels, the bound can be\nminimized by solving the $k$-center problem. The outcome is a linear time\nconstruction of a sparse kernel mean, which also lends itself naturally to an\nautomatic sparsity selection scheme. We show the computational gains of our\nmethod by looking at three problems involving kernel means: Euclidean embedding\nof distributions, class proportion estimation, and clustering using the\nmean-shift algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 18:30:07 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Cort\u00e9s", "E. Cruz", ""], ["Scott", "C.", ""]]}, {"id": "1503.00332", "submitter": "Ardavan Saeedi", "authors": "Jonathan H. Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash K.\n  Mansinghka", "title": "JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": "JMLR: W&CP Volume 37, 2015 pp. 693-701", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov jump processes (MJPs) are used to model a wide range of phenomena from\ndisease progression to RNA path folding. However, maximum likelihood estimation\nof parametric models leads to degenerate trajectories and inferential\nperformance is poor in nonparametric models. We take a small-variance\nasymptotics (SVA) approach to overcome these limitations. We derive the\nsmall-variance asymptotics for parametric and nonparametric MJPs for both\ndirectly observed and hidden state models. In the parametric case we obtain a\nnovel objective function which leads to non-degenerate trajectories. To derive\nthe nonparametric version we introduce the gamma-gamma process, a novel\nextension to the gamma-exponential process. We propose algorithms for each of\nthese formulations, which we call \\emph{JUMP-means}. Our experiments\ndemonstrate that JUMP-means is competitive with or outperforms widely used MJP\ninference approaches in terms of both speed and reconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 18:59:12 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 23:26:53 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 16:11:10 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Narasimhan", "Karthik", ""], ["Saeedi", "Ardavan", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1503.00338", "submitter": "Thibault Lesieur", "authors": "Thibault Lesieur, Florent Krzakala, Lenka Zdeborova", "title": "Phase Transitions in Sparse PCA", "comments": "6 pages, 3 figures", "journal-ref": "IEEE International Symposium on Information Theory (ISIT),\n  pp.1635-1639 (2015)", "doi": "10.1109/ISIT.2015.7282733", "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal estimation for sparse principal component analysis when the\nnumber of non-zero elements is small but on the same order as the dimension of\nthe data. We employ approximate message passing (AMP) algorithm and its state\nevolution to analyze what is the information theoretically minimal mean-squared\nerror and the one achieved by AMP in the limit of large sizes. For a special\ncase of rank one and large enough density of non-zeros Deshpande and Montanari\n[1] proved that AMP is asymptotically optimal. We show that both for low\ndensity and for large rank the problem undergoes a series of phase transitions\nsuggesting existence of a region of parameters where estimation is information\ntheoretically possible, but AMP (and presumably every other polynomial\nalgorithm) fails. The analysis of the large rank limit is particularly\ninstructive.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 19:26:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lesieur", "Thibault", ""], ["Krzakala", "Florent", ""], ["Zdeborova", "Lenka", ""]]}, {"id": "1503.00547", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail", "title": "Recovering PCA from Hybrid-$(\\ell_1,\\ell_2)$ Sparse Sampling of Data\n  Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how well we can recover a data matrix when only given a\nfew of its elements. We present a randomized algorithm that element-wise\nsparsifies the data, retaining only a few its elements. Our new algorithm\nindependently samples the data using sampling probabilities that depend on both\nthe squares ($\\ell_2$ sampling) and absolute values ($\\ell_1$ sampling) of the\nentries. We prove that the hybrid algorithm recovers a near-PCA reconstruction\nof the data from a sublinear sample-size: hybrid-($\\ell_1,\\ell_2$) inherits the\n$\\ell_2$-ability to sample the important elements as well as the regularization\nproperties of $\\ell_1$ sampling, and gives strictly better performance than\neither $\\ell_1$ or $\\ell_2$ on their own. We also give a one-pass version of\nour algorithm and show experiments to corroborate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 14:34:48 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kundu", "Abhisek", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1503.00623", "submitter": "Yiming Ying", "authors": "Yiming Ying and Ding-Xuan Zhou", "title": "Unregularized Online Learning Algorithms with General Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider unregularized online learning algorithms in a\nReproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit\nconvergence rates of the unregularized online learning algorithms for\nclassification associated with a general gamma-activating loss (see Definition\n1 in the paper). Our results extend and refine the results in Ying and Pontil\n(2008) for the least-square loss and the recent result in Bach and Moulines\n(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we\nestablish a very general condition on the step sizes which guarantees the\nconvergence of the last iterate of such algorithms. Secondly, we establish, for\nthe first time, the convergence of the unregularized pairwise learning\nalgorithm with a general loss function and derive explicit rates under the\nassumption of polynomially decaying step sizes. Concrete examples are used to\nillustrate our main results. The main techniques are tools from convex\nanalysis, refined inequalities of Gaussian averages, and an induction approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:21:23 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 17:58:51 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Ying", "Yiming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1503.00669", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Tao Hu, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A\n  Derivation from Multidimensional Scaling of Streaming Data", "comments": "Accepted for publication in Neural Computation", "journal-ref": null, "doi": "10.1162/NECO_a_00745", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models of early sensory processing typically reduce the\ndimensionality of streaming input data. Such networks learn the principal\nsubspace, in the sense of principal component analysis (PCA), by adjusting\nsynaptic weights according to activity-dependent learning rules. When derived\nfrom a principled cost function these rules are nonlocal and hence biologically\nimplausible. At the same time, biologically plausible local rules have been\npostulated rather than derived from a principled cost function. Here, to bridge\nthis gap, we derive a biologically plausible network for subspace learning on\nstreaming data by minimizing a principled cost function. In a departure from\nprevious work, where cost was quantified by the representation, or\nreconstruction, error, we adopt a multidimensional scaling (MDS) cost function\nfor streaming data. The resulting algorithm relies only on biologically\nplausible Hebbian and anti-Hebbian local learning rules. In a stochastic\nsetting, synaptic weights converge to a stationary state which projects the\ninput data onto the principal subspace. If the data are generated by a\nnonstationary distribution, the network can track the principal subspace. Thus,\nour result makes a step towards an algorithmic theory of neural computation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 19:39:33 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00680", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix\n  Factorization Can Cluster and Discover Sparse Features", "comments": "2014 Asilomar Conference on Signals, Systems and Computers", "journal-ref": null, "doi": "10.1109/ACSSC.2014.7094553", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite our extensive knowledge of biophysical properties of neurons, there\nis no commonly accepted algorithmic theory of neuronal function. Here we\nexplore the hypothesis that single-layer neuronal networks perform online\nsymmetric nonnegative matrix factorization (SNMF) of the similarity matrix of\nthe streamed data. By starting with the SNMF cost function we derive an online\nalgorithm, which can be implemented by a biologically plausible network with\nlocal learning rules. We demonstrate that such network performs soft clustering\nof the data as well as sparse feature discovery. The derived algorithm\nreplicates many known aspects of sensory anatomy and biophysical properties of\nneurons including unipolar nature of neuronal activity and synaptic weights,\nlocal synaptic plasticity rules and the dependence of learning rate on\ncumulative neuronal activity. Thus, we make a step towards an algorithmic\ntheory of neuronal function, which should facilitate large-scale neural circuit\nsimulations and biologically inspired artificial intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 19:57:28 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00687", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "A review of mean-shift algorithms for clustering", "comments": "28 pages, 9 figures. Invited book chapter to appear in the CRC\n  Handbook of Cluster Analysis (eds. Roberto Rocci, Fionn Murtagh, Marina Meila\n  and Christian Hennig)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to characterize the cluster structure of a dataset is by\nfinding regions containing a high density of data. This can be done in a\nnonparametric way with a kernel density estimate, whose modes and hence\nclusters can be found using mean-shift algorithms. We describe the theory and\npractice behind clustering based on kernel density estimates and mean-shift\nalgorithms. We discuss the blurring and non-blurring versions of mean-shift;\ntheoretical results about mean-shift algorithms and Gaussian mixtures;\nrelations with scale-space theory, spectral clustering and other algorithms;\nextensions to tracking, to manifold and graph data, and to manifold denoising;\nK-modes and Laplacian K-modes algorithms; acceleration strategies for large\ndatasets; and applications to image segmentation, manifold denoising and\nmultivalued regression.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:09:14 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1503.00690", "submitter": "Cengiz Pehlevan", "authors": "Tao Hu, Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning\n  Derived from Symmetric Matrix Factorization", "comments": "2014 Asilomar Conference on Signals, Systems and Computers. v2: fixed\n  a typo in equation 23", "journal-ref": null, "doi": "10.1109/ACSSC.2014.7094519", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Olshausen and Field (OF) proposed that neural computations in the primary\nvisual cortex (V1) can be partially modeled by sparse dictionary learning. By\nminimizing the regularized representation error they derived an online\nalgorithm, which learns Gabor-filter receptive fields from a natural image\nensemble in agreement with physiological experiments. Whereas the OF algorithm\ncan be mapped onto the dynamics and synaptic plasticity in a single-layer\nneural network, the derived learning rule is nonlocal - the synaptic weight\nupdate depends on the activity of neurons other than just pre- and postsynaptic\nones - and hence biologically implausible. Here, to overcome this problem, we\nderive sparse dictionary learning from a novel cost-function - a regularized\nerror of the symmetric factorization of the input's similarity matrix. Our\nalgorithm maps onto a neural network of the same architecture as OF but using\nonly biologically plausible local learning rules. When trained on natural\nimages our network learns Gabor-filter receptive fields and reproduces the\ncorrelation among synaptic weights hard-wired in the OF network. Therefore,\nonline symmetric matrix factorization may serve as an algorithmic theory of\nneural computation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:16:19 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 17:09:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Hu", "Tao", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1503.00693", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Noah A. Smith", "title": "Bayesian Optimization of Text Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying machine learning to problems in NLP, there are many choices to\nmake about how to represent input texts. These choices can have a big effect on\nperformance, but they are often uninteresting to researchers or practitioners\nwho simply need a module that performs well. We propose an approach to\noptimizing over this space of choices, formulating the problem as global\noptimization. We apply a sequential model-based optimization technique and show\nthat our method makes standard linear models competitive with more\nsophisticated, expensive state-of-the-art methods based on latent variable\nmodels or neural networks on various topic classification and sentiment\nanalysis problems. Our approach is a first step towards black-box NLP systems\nthat work with raw text and do not require manual tuning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:23:18 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Yogatama", "Dani", ""], ["Smith", "Noah A.", ""]]}, {"id": "1503.00759", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich", "title": "A Review of Relational Machine Learning for Knowledge Graphs", "comments": "To appear in Proceedings of the IEEE", "journal-ref": null, "doi": "10.1109/JPROC.2015.2483592", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational machine learning studies methods for the statistical analysis of\nrelational, or graph-structured, data. In this paper, we provide a review of\nhow such statistical models can be \"trained\" on large knowledge graphs, and\nthen used to predict new facts about the world (which is equivalent to\npredicting new edges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can scale to\nmassive datasets. The first is based on latent feature models such as tensor\nfactorization and multiway neural networks. The second is based on mining\nobservable patterns in the graph. We also show how to combine these latent and\nobservable models to get improved modeling power at decreased computational\ncost. Finally, we discuss how such statistical models of graphs can be combined\nwith text-based information extraction methods for automatically constructing\nknowledge graphs from the Web. To this end, we also discuss Google's Knowledge\nVault project as an example of such combination.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:35:41 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 16:35:31 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 17:40:35 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nickel", "Maximilian", ""], ["Murphy", "Kevin", ""], ["Tresp", "Volker", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1503.00778", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra", "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 23:02:56 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1503.01057", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Hannes Nickisch", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes\n  (KISS-GP)", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new structured kernel interpolation (SKI) framework, which\ngeneralises and unifies inducing point methods for scalable Gaussian processes\n(GPs). SKI methods produce kernel approximations for fast computations through\nkernel interpolation. The SKI framework clarifies how the quality of an\ninducing point approach depends on the number of inducing (aka interpolation)\npoints, interpolation strategy, and GP covariance kernel. SKI also provides a\nmechanism to create new scalable kernel methods, through choosing different\nkernel interpolation strategies. Using SKI, with local cubic kernel\ninterpolation, we introduce KISS-GP, which is 1) more scalable than inducing\npoint alternatives, 2) naturally enables Kronecker and Toeplitz algebra for\nsubstantial additional gains in scalability, without requiring any grid data,\nand 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)\ntime and storage for GP inference. We evaluate KISS-GP for kernel matrix\napproximation, kernel learning, and natural sound modelling.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:06:17 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1503.01158", "submitter": "Andrew Emmott", "authors": "Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern and\n  Weng-Keen Wong", "title": "A Meta-Analysis of the Anomaly Detection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a thorough meta-analysis of the anomaly detection\nproblem. To accomplish this we first identify approaches to benchmarking\nanomaly detection algorithms across the literature and produce a large corpus\nof anomaly detection benchmarks that vary in their construction across several\ndimensions we deem important to real-world applications: (a) point difficulty,\n(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)\nrelevance of features. We apply a representative set of anomaly detection\nalgorithms to this corpus, yielding a very large collection of experimental\nresults. We analyze these results to understand many phenomena observed in\nprevious work. First we observe the effects of experimental design on\nexperimental results. Second, results are evaluated with two metrics, ROC Area\nUnder the Curve and Average Precision. We employ statistical hypothesis testing\nto demonstrate the value (or lack thereof) of our benchmarks. We then offer\nseveral approaches to summarizing our experimental results, drawing several\nconclusions about the impact of our methodology as well as the strengths and\nweaknesses of some algorithms. Last, we compare results against a trivial\nsolution as an alternate means of normalizing the reported performance of\nalgorithms. The intended contributions of this article are many; in addition to\nproviding a large publicly-available corpus of anomaly detection benchmarks, we\nprovide an ontology for describing anomaly detection contexts, a methodology\nfor controlling various aspects of benchmark creation, guidelines for future\nexperimental design and a discussion of the many potential pitfalls of trying\nto measure success in this field.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 23:07:37 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:26:36 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Emmott", "Andrew", ""], ["Das", "Shubhomoy", ""], ["Dietterich", "Thomas", ""], ["Fern", "Alan", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.01161", "submitter": "Been Kim", "authors": "Been Kim, Cynthia Rudin and Julie Shah", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning\n  and Prototype Classification", "comments": "Published in Neural Information Processing Systems (NIPS) 2014,\n  Neural Information Processing Systems (NIPS) 2014", "journal-ref": "NIPS 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bayesian Case Model (BCM), a general framework for Bayesian\ncase-based reasoning (CBR) and prototype classification and clustering. BCM\nbrings the intuitive power of CBR to a Bayesian generative framework. The BCM\nlearns prototypes, the \"quintessential\" observations that best represent\nclusters in a dataset, by performing joint inference on cluster labels,\nprototypes and important features. Simultaneously, BCM pursues sparsity by\nlearning subspaces, the sets of features that play important roles in the\ncharacterization of the prototypes. The prototype and subspace representation\nprovides quantitative benefits in interpretability while preserving\nclassification accuracy. Human subject experiments verify statistically\nsignificant improvements to participants' understanding when using explanations\nproduced by BCM, compared to those given by prior art.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 23:25:55 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kim", "Been", ""], ["Rudin", "Cynthia", ""], ["Shah", "Julie", ""]]}, {"id": "1503.01183", "submitter": "Saeid Amiri", "authors": "Saeid Amiri, Bertrand Clarke, Jennifer Clarke and Hoyt A. Koepke", "title": "A General Hybrid Clustering Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we propose a clustering technique for general clustering problems\nincluding those that have non-convex clusters. For a given desired number of\nclusters $K$, we use three stages to find a clustering. The first stage uses a\nhybrid clustering technique to produce a series of clusterings of various sizes\n(randomly selected). They key steps are to find a $K$-means clustering using\n$K_\\ell$ clusters where $K_\\ell \\gg K$ and then joins these small clusters by\nusing single linkage clustering. The second stage stabilizes the result of\nstage one by reclustering via the `membership matrix' under Hamming distance to\ngenerate a dendrogram. The third stage is to cut the dendrogram to get $K^*$\nclusters where $K^* \\geq K$ and then prune back to $K$ to give a final\nclustering. A variant on our technique also gives a reasonable estimate for\n$K_T$, the true number of clusters.\n  We provide a series of arguments to justify the steps in the stages of our\nmethods and we provide numerous examples involving real and simulated data to\ncompare our technique with other related techniques.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:08:17 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 22:50:24 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Amiri", "Saeid", ""], ["Clarke", "Bertrand", ""], ["Clarke", "Jennifer", ""], ["Koepke", "Hoyt A.", ""]]}, {"id": "1503.01190", "submitter": "Michael Bloodgood", "authors": "Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr,\n  Lori Levin, Christine D. Piatko, Owen Rambow and Benjamin Van Durme", "title": "Statistical modality tagging from rule-based annotations and\n  crowdsourcing", "comments": "8 pages, 6 tables; appeared in Proceedings of the Workshop on\n  Extra-Propositional Aspects of Meaning in Computational Linguistics, July\n  2012; In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "journal-ref": "In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore training an automatic modality tagger. Modality is the attitude\nthat a speaker might have toward an event or state. One of the main hurdles for\ntraining a linguistic tagger is gathering training data. This is particularly\nproblematic for training a tagger for modality because modality triggers are\nsparse for the overwhelming majority of sentences. We investigate an approach\nto automatically training a modality tagger where we first gathered sentences\nbased on a high-recall simple rule-based modality tagger and then provided\nthese sentences to Mechanical Turk annotators for further annotation. We used\nthe resulting set of training data to train a precise modality tagger using a\nmulti-class SVM that delivers good performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:34:36 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Prabhakaran", "Vinodkumar", ""], ["Bloodgood", "Michael", ""], ["Diab", "Mona", ""], ["Dorr", "Bonnie", ""], ["Levin", "Lori", ""], ["Piatko", "Christine D.", ""], ["Rambow", "Owen", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1503.01210", "submitter": "Borhan Sanandaji", "authors": "Borhan M. Sanandaji, Akin Tascikaraoglu, Kameshwar Poolla, and Pravin\n  Varaiya", "title": "Low-dimensional Models in Spatio-Temporal Wind Speed Forecasting", "comments": "Initially submitted for review to the 2015 American Control\n  Conference on September 22, 2014; Accepted for publication on January 22,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating wind power into the grid is challenging because of its random\nnature. Integration is facilitated with accurate short-term forecasts of wind\npower. The paper presents a spatio-temporal wind speed forecasting algorithm\nthat incorporates the time series data of a target station and data of\nsurrounding stations. Inspired by Compressive Sensing (CS) and\nstructured-sparse recovery algorithms, we claim that there usually exists an\nintrinsic low-dimensional structure governing a large collection of stations\nthat should be exploited. We cast the forecasting problem as recovery of a\nblock-sparse signal $\\boldsymbol{x}$ from a set of linear equations\n$\\boldsymbol{b} = A\\boldsymbol{x}$ for which we propose novel structure-sparse\nrecovery algorithms. Results of a case study in the east coast show that the\nproposed Compressive Spatio-Temporal Wind Speed Forecasting (CST-WSF) algorithm\nsignificantly improves the short-term forecasts compared to a set of\nwidely-used benchmark models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 04:00:19 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Sanandaji", "Borhan M.", ""], ["Tascikaraoglu", "Akin", ""], ["Poolla", "Kameshwar", ""], ["Varaiya", "Pravin", ""]]}, {"id": "1503.01212", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "Hierarchies of Relaxations for Online Prediction Problems with Evolving\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online prediction where regret of the algorithm is measured against\na benchmark defined via evolving constraints. This framework captures online\nprediction on graphs, as well as other prediction problems with combinatorial\nstructure. A key aspect here is that finding the optimal benchmark predictor\n(even in hindsight, given all the data) might be computationally hard due to\nthe combinatorial nature of the constraints. Despite this, we provide\npolynomial-time \\emph{prediction} algorithms that achieve low regret against\ncombinatorial benchmark sets. We do so by building improper learning algorithms\nbased on two ideas that work together. The first is to alleviate part of the\ncomputational burden through random playout, and the second is to employ\nLasserre semidefinite hierarchies to approximate the resulting integer program.\nInterestingly, for our prediction algorithms, we only need to compute the\nvalues of the semidefinite programs and not the rounded solutions. However, the\nintegrality gap for Lasserre hierarchy \\emph{does} enter the generic regret\nbound in terms of Rademacher complexity of the benchmark set. This establishes\na trade-off between the computation time and the regret bound of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 04:05:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 00:04:05 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1503.01228", "submitter": "Kui Tang", "authors": "Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara", "title": "Bethe Learning of Conditional Random Fields via MAP Decoding", "comments": "19 pages (9 supplementary), 10 figures (3 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be formulated in terms of predicting\nstructured outputs. In frameworks such as the structured support vector machine\n(SVM-Struct) and the structured perceptron, discriminative functions are\nlearned by iteratively applying efficient maximum a posteriori (MAP) decoding.\nHowever, maximum likelihood estimation (MLE) of probabilistic models over these\nsame structured spaces requires computing partition functions, which is\ngenerally intractable. This paper presents a method for learning discrete\nexponential family models using the Bethe approximation to the MLE. Remarkably,\nthis problem also reduces to iterative (MAP) decoding. This connection emerges\nby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a\nconvex dual objective which circumvents the intractable partition function. The\nresult is a new single loop algorithm MLE-Struct, which is substantially more\nefficient than previous double-loop methods for approximate maximum likelihood\nestimation. Our algorithm outperforms existing methods in experiments involving\nimage segmentation, matching problems from vision, and a new dataset of\nuniversity roommate assignments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 05:41:29 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Tang", "Kui", ""], ["Ruozzi", "Nicholas", ""], ["Belanger", "David", ""], ["Jebara", "Tony", ""]]}, {"id": "1503.01243", "submitter": "Weijie Su", "authors": "Weijie Su, Stephen Boyd, Emmanuel J. Candes", "title": "A Differential Equation for Modeling Nesterov's Accelerated Gradient\n  Method: Theory and Insights", "comments": "To appear in Journal of Machine Learning Research. Added more\n  simulation studies. Preliminary version appeared in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.CA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a second-order ordinary differential equation (ODE) which is the\nlimit of Nesterov's accelerated gradient method. This ODE exhibits approximate\nequivalence to Nesterov's scheme and thus can serve as a tool for analysis. We\nshow that the continuous time ODE allows for a better understanding of\nNesterov's scheme. As a byproduct, we obtain a family of schemes with similar\nconvergence rates. The ODE interpretation also suggests restarting Nesterov's\nscheme leading to an algorithm, which can be rigorously proven to converge at a\nlinear rate whenever the objective is strongly convex.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 07:03:50 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 21:57:18 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Su", "Weijie", ""], ["Boyd", "Stephen", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1503.01245", "submitter": "David Morales-Jimenez", "authors": "David Morales-Jimenez, Romain Couillet, Matthew R. McKay", "title": "Large Dimensional Analysis of Robust M-Estimators of Covariance with\n  Outliers", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2460225", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large dimensional characterization of robust M-estimators of covariance (or\nscatter) is provided under the assumption that the dataset comprises\nindependent (essentially Gaussian) legitimate samples as well as arbitrary\ndeterministic samples, referred to as outliers. Building upon recent random\nmatrix advances in the area of robust statistics, we specifically show that the\nso-called Maronna M-estimator of scatter asymptotically behaves similar to\nwell-known random matrices when the population and sample sizes grow together\nto infinity. The introduction of outliers leads the robust estimator to behave\nasymptotically as the weighted sum of the sample outer products, with a\nconstant weight for all legitimate samples and different weights for the\noutliers. A fine analysis of this structure reveals importantly that the\npropensity of the M-estimator to attenuate (or enhance) the impact of outliers\nis mostly dictated by the alignment of the outliers with the inverse population\ncovariance matrix of the legitimate samples. Thus, robust M-estimators can\nbring substantial benefits over more simplistic estimators such as the\nper-sample normalized version of the sample covariance matrix, which is not\ncapable of differentiating the outlying samples. The analysis shows that,\nwithin the class of Maronna's estimators of scatter, the Huber estimator is\nmost favorable for rejecting outliers. On the contrary, estimators more similar\nto Tyler's scale invariant estimator (often preferred in the literature) run\nthe risk of inadvertently enhancing some outliers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 07:28:27 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Morales-Jimenez", "David", ""], ["Couillet", "Romain", ""], ["McKay", "Matthew R.", ""]]}, {"id": "1503.01291", "submitter": "Giovanni Montana", "authors": "Zi Wang, Wei Yuan, Giovanni Montana", "title": "Sparse multi-view matrix factorisation: a multivariate approach to\n  multiple tissue comparisons", "comments": "in Bioinformatics 2015", "journal-ref": null, "doi": "10.1093/bioinformatics/btv344", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression levels in a population vary extensively across tissues. Such\nheterogeneity is caused by genetic variability and environmental factors, and\nis expected to be linked to disease development. The abundance of experimental\ndata now enables the identification of features of gene expression profiles\nthat are shared across tissues, and those that are tissue-specific. While most\ncurrent research is concerned with characterising differential expression by\ncomparing mean expression profiles across tissues, it is also believed that a\nsignificant difference in a gene expression's variance across tissues may also\nbe associated to molecular mechanisms that are important for tissue development\nand function. We propose a sparse multi-view matrix factorisation (sMVMF)\nalgorithm to jointly analyse gene expression measurements in multiple tissues,\nwhere each tissue provides a different \"view\" of the underlying organism. The\nproposed methodology can be interpreted as an extension of principal component\nanalysis in that it provides the means to decompose the total sample variance\nin each tissue into the sum of two components: one capturing the variance that\nis shared across tissues, and one isolating the tissue-specific variances.\nsMVMF has been used to jointly model mRNA expression profiles in three tissues\n- adipose, skin and LCL - which are available for a large and well-phenotyped\ntwins cohort, TwinsUK. Using sMVMF, we are able to prioritise genes based on\nwhether their variation patterns are specific to each tissue. Furthermore,\nusing DNA methylation profiles available, we provide supporting evidence that\nadipose-specific gene expression patterns may be driven by epigenetic effects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 11:50:45 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:27:54 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Wang", "Zi", ""], ["Yuan", "Wei", ""], ["Montana", "Giovanni", ""]]}, {"id": "1503.01397", "submitter": "David Belanger", "authors": "Luke Vilnis and David Belanger and Daniel Sheldon and Andrew McCallum", "title": "Bethe Projections for Non-Local Inference", "comments": "minor bug fix to appendix. appeared in UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems in structured prediction are naturally solved by\naugmenting a tractable dependency structure with complex, non-local auxiliary\nobjectives. This includes the mean field family of variational inference\nalgorithms, soft- or hard-constrained inference using Lagrangian relaxation or\nlinear programming, collective graphical models, and forms of semi-supervised\nlearning such as posterior regularization. We present a method to\ndiscriminatively learn broad families of inference objectives, capturing\npowerful non-local statistics of the latent variables, while maintaining\ntractable and provably fast inference using non-Euclidean projected gradient\ndescent with a distance-generating function given by the Bethe entropy. We\ndemonstrate the performance and flexibility of our method by (1) extracting\nstructured citations from research papers by learning soft global constraints,\n(2) achieving state-of-the-art results on a widely-used handwriting recognition\ntask using a novel learned non-convex inference procedure, and (3) providing a\nfast and highly scalable algorithm for the challenging problem of inference in\na collective graphical model applied to bird migration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 07:32:25 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 18:44:53 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Vilnis", "Luke", ""], ["Belanger", "David", ""], ["Sheldon", "Daniel", ""], ["McCallum", "Andrew", ""]]}, {"id": "1503.01401", "submitter": "Kyle Hickmann", "authors": "Kyle S. Hickmann, James M. Hyman, Sara Y. Del Valle", "title": "Quantifying Uncertainty in Stochastic Models with Parametric Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to quantify uncertainty in the predictions made by\nsimulations of mathematical models that can be applied to a broad class of\nstochastic, discrete, and differential equation models. Quantifying uncertainty\nis crucial for determining how accurate the model predictions are and\nidentifying which input parameters affect the outputs of interest. Most of the\nexisting methods for uncertainty quantification require many samples to\ngenerate accurate results, are unable to differentiate where the uncertainty is\ncoming from (e.g., parameters or model assumptions), or require a lot of\ncomputational resources. Our approach addresses these challenges and\nopportunities by allowing different types of uncertainty, that is, uncertainty\nin input parameters as well as uncertainty created through stochastic model\ncomponents. This is done by combining the Karhunen-Loeve decomposition,\npolynomial chaos expansion, and Bayesian Gaussian process regression to create\na statistical surrogate for the stochastic model. The surrogate separates the\nanalysis of variation arising through stochastic simulation and variation\narising through uncertainty in the model parameterization. We illustrate our\napproach by quantifying the uncertainty in a stochastic ordinary differential\nequation epidemic model. Specifically, we estimate four quantities of interest\nfor the epidemic model and show agreement between the surrogate and the actual\nmodel results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 18:03:16 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Hickmann", "Kyle S.", ""], ["Hyman", "James M.", ""], ["Del Valle", "Sara Y.", ""]]}, {"id": "1503.01436", "submitter": "Qinxun Bai", "authors": "Qinxun Bai, Steven Rosenberg, Zheng Wu, Stan Sclaroff", "title": "Class Probability Estimation via Differential Geometric Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of supervised learning for both binary and multiclass\nclassification from a unified geometric perspective. In particular, we propose\na geometric regularization technique to find the submanifold corresponding to a\nrobust estimator of the class probability $P(y|\\pmb{x})$. The regularization\nterm measures the volume of this submanifold, based on the intuition that\noverfitting produces rapid local oscillations and hence large volume of the\nestimator. This technique can be applied to regularize any classification\nfunction that satisfies two requirements: firstly, an estimator of the class\nprobability can be obtained; secondly, first and second derivatives of the\nclass probability estimator can be calculated. In experiments, we apply our\nregularization technique to standard loss functions for classification, our\nRBF-based implementation compares favorably to widely used regularization\nmethods for both binary and multiclass classification.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 19:51:19 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 10:50:07 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 15:16:32 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 01:54:17 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2015 03:08:39 GMT"}, {"version": "v6", "created": "Thu, 2 Jul 2015 04:52:13 GMT"}, {"version": "v7", "created": "Thu, 11 Feb 2016 05:35:31 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Bai", "Qinxun", ""], ["Rosenberg", "Steven", ""], ["Wu", "Zheng", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1503.01442", "submitter": "Han Liu", "authors": "Zhaoran Wang, Quanquan Gu, Han Liu", "title": "Statistical Limits of Convex Relaxations", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional sparse learning problems are formulated as nonconvex\noptimization. A popular approach to solve these nonconvex optimization problems\nis through convex relaxations such as linear and semidefinite programming. In\nthis paper, we study the statistical limits of convex relaxations.\nParticularly, we consider two problems: Mean estimation for sparse principal\nsubmatrix and edge probability estimation for stochastic block model. We\nexploit the sum-of-squares relaxation hierarchy to sharply characterize the\nlimits of a broad class of convex relaxations. Our result shows statistical\noptimality needs to be compromised for achieving computational tractability\nusing convex relaxations. Compared with existing results on computational lower\nbounds for statistical problems, which consider general polynomial-time\nalgorithms and rely on computational hardness hypotheses on problems like\nplanted clique detection, our theory focuses on a broad class of convex\nrelaxations and does not rely on unproven hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:12:11 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2015 01:06:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wang", "Zhaoran", ""], ["Gu", "Quanquan", ""], ["Liu", "Han", ""]]}, {"id": "1503.01445", "submitter": "Thomas Unterthiner", "authors": "Thomas Unterthiner, Andreas Mayr, G\\\"unter Klambauer, Sepp Hochreiter", "title": "Toxicity Prediction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday we are exposed to various chemicals via food additives, cleaning and\ncosmetic products and medicines -- and some of them might be toxic. However\ntesting the toxicity of all existing compounds by biological experiments is\nneither financially nor logistically feasible. Therefore the government\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\nwas to assess the performance of computational methods in predicting the\ntoxicity of chemical compounds. State of the art toxicity prediction methods\nbuild upon specifically-designed chemical descriptors developed over decades.\nThough Deep Learning is new to the field and was never applied to toxicity\nprediction before, it clearly outperformed all other participating methods. In\nthis application paper we show that deep nets automatically learn features\nresembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:18:55 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Unterthiner", "Thomas", ""], ["Mayr", "Andreas", ""], ["Klambauer", "G\u00fcnter", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1503.01494", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias", "title": "Local Expectation Gradients for Doubly Stochastic Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce local expectation gradients which is a general purpose\nstochastic variational inference algorithm for constructing stochastic\ngradients through sampling from the variational distribution. This algorithm\ndivides the problem of estimating the stochastic gradients over multiple\nvariational parameters into smaller sub-tasks so that each sub-task exploits\nintelligently the information coming from the most relevant part of the\nvariational distribution. This is achieved by performing an exact expectation\nover the single random variable that mostly correlates with the variational\nparameter of interest resulting in a Rao-Blackwellized estimate that has low\nvariance and can work efficiently for both continuous and discrete random\nvariables. Furthermore, the proposed algorithm has interesting similarities\nwith Gibbs sampling but at the same time, unlike Gibbs sampling, it can be\ntrivially parallelized.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 22:53:41 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Titsias", "Michalis K.", ""]]}, {"id": "1503.01521", "submitter": "Liwen Zhang", "authors": "Liwen Zhang, Subhransu Maji, Ryota Tomioka", "title": "Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 02:57:19 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:09:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 21:42:55 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhang", "Liwen", ""], ["Maji", "Subhransu", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.01538", "submitter": "Natalia Bilenko", "authors": "Natalia Y. Bilenko and Jack L. Gallant", "title": "Pyrcca: regularized kernel canonical correlation analysis in Python and\n  its applications to neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a valuable method for interpreting\ncross-covariance across related datasets of different dimensionality. There are\nmany potential applications of CCA to neuroimaging data analysis. For instance,\nCCA can be used for finding functional similarities across fMRI datasets\ncollected from multiple subjects without resampling individual datasets to a\ntemplate anatomy. In this paper, we introduce Pyrcca, an open-source Python\nmodule for executing CCA between two or more datasets. Pyrcca can be used to\nimplement CCA with or without regularization, and with or without linear or a\nGaussian kernelization of the datasets. We demonstrate an application of CCA\nimplemented with Pyrcca to neuroimaging data analysis. We use CCA to find a\ndata-driven set of functional response patterns that are similar across\nindividual subjects in a natural movie experiment. We then demonstrate how this\nset of response patterns discovered by CCA can be used to accurately predict\nsubject responses to novel natural movie stimuli.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 04:57:22 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Bilenko", "Natalia Y.", ""], ["Gallant", "Jack L.", ""]]}, {"id": "1503.01596", "submitter": "Sungjin Ahn", "authors": "Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, Max Welling", "title": "Large-Scale Distributed Bayesian Matrix Factorization using Stochastic\n  Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having various attractive qualities such as high prediction accuracy\nand the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix\nFactorization has not been widely adopted because of the prohibitive cost of\ninference. In this paper, we propose a scalable distributed Bayesian matrix\nfactorization algorithm using stochastic gradient MCMC. Our algorithm, based on\nDistributed Stochastic Gradient Langevin Dynamics, can not only match the\nprediction accuracy of standard MCMC methods like Gibbs sampling, but at the\nsame time is as fast and simple as stochastic gradient descent. In our\nexperiments, we show that our algorithm can achieve the same level of\nprediction accuracy as Gibbs sampling an order of magnitude faster. We also\nshow that our method reduces the prediction error as fast as distributed\nstochastic gradient descent, achieving a 4.1% improvement in RMSE for the\nNetflix dataset and an 1.8% for the Yahoo music dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 10:17:16 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 02:28:41 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Ahn", "Sungjin", ""], ["Korattikara", "Anoop", ""], ["Liu", "Nathan", ""], ["Rajan", "Suju", ""], ["Welling", "Max", ""]]}, {"id": "1503.01673", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos", "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models", "comments": "Proceedings of The 32nd International Conference on Machine Learning\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimisation (BO) is a technique used in optimising a\n$D$-dimensional function which is typically expensive to evaluate. While there\nhave been many successes for BO in low dimensions, scaling it to high\ndimensions has been notoriously difficult. Existing literature on the topic are\nunder very restrictive settings. In this paper, we identify two key challenges\nin this endeavour. We tackle these challenges by assuming an additive structure\nfor the function. This setting is substantially more expressive and contains a\nricher class of functions than previous work. We prove that, for additive\nfunctions the regret has only linear dependence on $D$ even though the function\ndepends on all $D$ dimensions. We also demonstrate several other statistical\nand computational benefits in our framework. Via synthetic examples, a\nscientific simulation and a face detection problem we demonstrate that our\nmethod outperforms naive BO on additive functions and on several examples where\nthe function is not additive.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 15:56:08 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 21:59:15 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 15:31:03 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1503.01737", "submitter": "Ping Li", "authors": "Ping Li", "title": "Min-Max Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The min-max kernel is a generalization of the popular resemblance kernel\n(which is designed for binary data). In this paper, we demonstrate, through an\nextensive classification study using kernel machines, that the min-max kernel\noften provides an effective measure of similarity for nonnegative data. As the\nmin-max kernel is nonlinear and might be difficult to be used for industrial\napplications with massive data, we show that the min-max kernel can be\nlinearized via hashing techniques. This allows practitioners to apply min-max\nkernel to large-scale applications using well matured linear algorithms such as\nlinear SVM or logistic regression.\n  The previous remarkable work on consistent weighted sampling (CWS) produces\nsamples in the form of ($i^*, t^*$) where the $i^*$ records the location (and\nin fact also the weights) information analogous to the samples produced by\nclassical minwise hashing on binary data. Because the $t^*$ is theoretically\nunbounded, it was not immediately clear how to effectively implement CWS for\nbuilding large-scale linear classifiers. In this paper, we provide a simple\nsolution by discarding $t^*$ (which we refer to as the \"0-bit\" scheme). Via an\nextensive empirical study, we show that this 0-bit scheme does not lose\nessential information. We then apply the \"0-bit\" CWS for building linear\nclassifiers to approximate min-max kernel classifiers, as extensively validated\non a wide range of publicly available classification datasets. We expect this\nwork will generate interests among data mining practitioners who would like to\nefficiently utilize the nonlinear information of non-binary and nonnegative\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 19:29:03 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.01811", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "Optimally Combining Classifiers Using Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a worst-case analysis of aggregation of classifier ensembles for\nbinary classification. The task of predicting to minimize error is formulated\nas a game played over a given set of unlabeled data (a transductive setting),\nwhere prior label information is encoded as constraints on the game. The\nminimax solution of this game identifies cases where a weighted combination of\nthe classifiers can perform significantly better than any single classifier.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:56:07 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 07:55:42 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 20:26:54 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1503.01916", "submitter": "Edward Meeds", "authors": "Edward Meeds, Robert Leenders, and Max Welling", "title": "Hamiltonian ABC", "comments": "Submission to UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a powerful and elegant framework\nfor performing inference in simulation-based models. However, due to the\ndifficulty in scaling likelihood estimates, ABC remains useful for relatively\nlow-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of\nlikelihood-free algorithms that apply recent advances in scaling Bayesian\nlearning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find\nthat a small number forward simulations can effectively approximate the ABC\ngradient, allowing Hamiltonian dynamics to efficiently traverse parameter\nspaces. We also describe a new simple yet general approach of incorporating\nrandom seeds into the state of the Markov chain, further reducing the random\nwalk behavior of HABC. We demonstrate HABC on several typical ABC problems, and\nshow that HABC samples comparably to regular Bayesian inference using true\ngradients on a high-dimensional problem from machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:16:58 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Meeds", "Edward", ""], ["Leenders", "Robert", ""], ["Welling", "Max", ""]]}, {"id": "1503.02031", "submitter": "Vivek Kulkarni", "authors": "Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams", "title": "To Drop or Not to Drop: Robustness, Consistency and Differential Privacy\n  Properties of Dropout", "comments": "Currently under review for ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep belief networks (DBNs) requires optimizing a non-convex\nfunction with an extremely large number of parameters. Naturally, existing\ngradient descent (GD) based methods are prone to arbitrarily poor local minima.\nIn this paper, we rigorously show that such local minima can be avoided (upto\nan approximation error) by using the dropout technique, a widely used heuristic\nin this domain. In particular, we show that by randomly dropping a few nodes of\na one-hidden layer neural network, the training objective function, up to a\ncertain approximation error, decreases by a multiplicative factor.\n  On the flip side, we show that for training convex empirical risk minimizers\n(ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple\ndropout based GD method for convex ERMs is stable in the face of arbitrary\nchanges to any one of the training points. Using the above assertion, we show\nthat dropout provides fast rates for generalization error in learning (convex)\ngeneralized linear models (GLM). Moreover, using the above mentioned stability\nproperties of dropout, we design dropout based differentially private\nalgorithms for solving ERMs. The learned GLM thus, preserves privacy of each of\nthe individual training points while providing accurate predictions for new\ntest points. Finally, we empirically validate our stability assertions for\ndropout in the context of convex ERMs and show that surprisingly, dropout\nsignificantly outperforms (in terms of prediction accuracy) the L2\nregularization based methods for several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 18:39:53 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Jain", "Prateek", ""], ["Kulkarni", "Vivek", ""], ["Thakurta", "Abhradeep", ""], ["Williams", "Oliver", ""]]}, {"id": "1503.02101", "submitter": "Chi Jin", "authors": "Rong Ge, Furong Huang, Chi Jin, Yang Yuan", "title": "Escaping From Saddle Points --- Online Stochastic Gradient for Tensor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic gradient descent for optimizing non-convex functions.\nIn many cases for non-convex functions the goal is to find a reasonable local\nminimum, and the main concern is that gradient updates are trapped in saddle\npoints. In this paper we identify strict saddle property for non-convex problem\nthat allows for efficient optimization. Using this property we show that\nstochastic gradient descent converges to a local minimum in a polynomial number\nof iterations. To the best of our knowledge this is the first work that gives\nglobal convergence guarantees for stochastic gradient descent on non-convex\nfunctions with exponentially many local minima and saddle points. Our analysis\ncan be applied to orthogonal tensor decomposition, which is widely used in\nlearning a rich class of latent variable models. We propose a new optimization\nformulation for the tensor decomposition problem that has strict saddle\nproperty. As a result we get the first online algorithm for orthogonal tensor\ndecomposition with global convergence guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:07:05 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Ge", "Rong", ""], ["Huang", "Furong", ""], ["Jin", "Chi", ""], ["Yuan", "Yang", ""]]}, {"id": "1503.02115", "submitter": "Avanti Athreya", "authors": "Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, Carey E.\n  Priebe", "title": "Community Detection and Classification in Hierarchical Stochastic\n  Blockmodels", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust, scalable, integrated methodology for community detection\nand community comparison in graphs. In our procedure, we first embed a graph\ninto an appropriate Euclidean space to obtain a low-dimensional representation,\nand then cluster the vertices into communities. We next employ nonparametric\ngraph inference techniques to identify structural similarity among these\ncommunities. These two steps are then applied recursively on the communities,\nallowing us to detect more fine-grained structure. We describe a hierarchical\nstochastic blockmodel---namely, a stochastic blockmodel with a natural\nhierarchical structure---and establish conditions under which our algorithm\nyields consistent estimates of model parameters and motifs, which we define to\nbe stochastically similar groups of subgraphs. Finally, we demonstrate the\neffectiveness of our algorithm in both simulated and real data. Specifically,\nwe address the problem of locating similar subcommunities in a partially\nreconstructed Drosophila connectome and in the social network Friendster.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 01:40:25 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 23:36:37 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 04:30:47 GMT"}, {"version": "v4", "created": "Thu, 25 Aug 2016 18:59:30 GMT"}, {"version": "v5", "created": "Fri, 26 Aug 2016 00:13:19 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Lyzinski", "Vince", ""], ["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1503.02120", "submitter": "Jake Williams", "authors": "Jake Ryland Williams, Eric M. Clark, James P. Bagrow, Christopher M.\n  Danforth, and Peter Sheridan Dodds", "title": "Identifying missing dictionary entries with frequency-conserving context\n  models", "comments": "16 pages, 6 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to better understand meaning from natural language texts, we\nexplore methods aimed at organizing lexical objects into contexts. A number of\nthese methods for organization fall into a family defined by word ordering.\nUnlike demographic or spatial partitions of data, these collocation models are\nof special importance for their universal applicability. While we are\ninterested here in text and have framed our treatment appropriately, our work\nis potentially applicable to other areas of research (e.g., speech, genomics,\nand mobility patterns) where one has ordered categorical data, (e.g., sounds,\ngenes, and locations). Our approach focuses on the phrase (whether word or\nlarger) as the primary meaning-bearing lexical unit and object of study. To do\nso, we employ our previously developed framework for generating word-conserving\nphrase-frequency data. Upon training our model with the Wiktionary---an\nextensive, online, collaborative, and open-source dictionary that contains over\n100,000 phrasal-definitions---we develop highly effective filters for the\nidentification of meaningful, missing phrase-entries. With our predictions we\nthen engage the editorial community of the Wiktionary and propose short lists\nof potential missing entries for definition, developing a breakthrough, lexical\nextraction technique, and expanding our knowledge of the defined English\nlexicon of phrases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 02:45:06 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 18:59:35 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 01:36:24 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Williams", "Jake Ryland", ""], ["Clark", "Eric M.", ""], ["Bagrow", "James P.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1503.02128", "submitter": "Qingming Tang", "authors": "Qingming Tang, Chao Yang, Jian Peng and Jinbo Xu", "title": "Exact Hybrid Covariance Thresholding for Joint Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating multiple related Gaussian\ngraphical models from a $p$-dimensional dataset consisting of different\nclasses. Our work is based upon the formulation of this problem as group\ngraphical lasso. This paper proposes a novel hybrid covariance thresholding\nalgorithm that can effectively identify zero entries in the precision matrices\nand split a large joint graphical lasso problem into small subproblems. Our\nhybrid covariance thresholding method is superior to existing uniform\nthresholding methods in that our method can split the precision matrix of each\nindividual class using different partition schemes and thus split group\ngraphical lasso into much smaller subproblems, each of which can be solved very\nfast. In addition, this paper establishes necessary and sufficient conditions\nfor our hybrid covariance thresholding algorithm. The superior performance of\nour thresholding method is thoroughly analyzed and illustrated by a few\nexperiments on simulated data and real gene expression data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:34:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 02:52:51 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Yang", "Chao", ""], ["Peng", "Jian", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02129", "submitter": "Qingming Tang", "authors": "Qingming Tang, Siqi Sun, and Jinbo Xu", "title": "Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the network structure underlying data is an important problem in\nmachine learning. This paper introduces a novel prior to study the inference of\nscale-free networks, which are widely used to model social and biological\nnetworks. The prior not only favors a desirable global node degree\ndistribution, but also takes into consideration the relative strength of all\nthe possible edges adjacent to the same node and the estimated degree of each\nindividual node.\n  To fulfill this, ranking is incorporated into the prior, which makes the\nproblem challenging to solve. We employ an ADMM (alternating direction method\nof multipliers) framework to solve the Gaussian Graphical model regularized by\nthis prior. Our experiments on both synthetic and real data show that our prior\nnot only yields a scale-free network, but also produces many more correctly\npredicted edges than the others such as the scale-free inducing prior, the\nhub-inducing prior and the $l_1$ norm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:35:26 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 04:13:31 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 03:37:44 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Sun", "Siqi", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02182", "submitter": "Yarin Gal", "authors": "Yarin Gal, Yutian Chen, Zoubin Ghahramani", "title": "Latent Gaussian Processes for Distribution Estimation of Multivariate\n  Categorical Data", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate categorical data occur in many applications of machine learning.\nOne of the main difficulties with these vectors of categorical variables is\nsparsity. The number of possible observations grows exponentially with vector\nlength, but dataset diversity might be poor in comparison. Recent models have\ngained significant improvement in supervised tasks with this data. These models\nembed observations in a continuous space to capture similarities between them.\nBuilding on these ideas we propose a Bayesian model for the unsupervised task\nof distribution estimation of multivariate categorical data. We model vectors\nof categorical variables as generated from a non-linear transformation of a\ncontinuous latent space. Non-linearity captures multi-modality in the\ndistribution. The continuous representation addresses sparsity. Our model ties\ntogether many existing models, linking the linear categorical latent Gaussian\nmodel, the Gaussian process latent variable model, and Gaussian process\nclassification. We derive inference for our model based on recent developments\nin sampling based variational inference. We show empirically that the model\noutperforms its linear and discrete counterparts in imputation tasks of sparse\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 14:53:39 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Gal", "Yarin", ""], ["Chen", "Yutian", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1503.02216", "submitter": "Yuning Yang", "authors": "Yuning Yang, Siamak Mehrkanoon and Johan A.K. Suykens", "title": "Higher order Matching Pursuit for Low Rank Tensor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor learning, such as tensor completion and multilinear multitask\nlearning, has received much attention in recent years. In this paper, we\npropose higher order matching pursuit for low rank tensor learning problems\nwith a convex or a nonconvex cost function, which is a generalization of the\nmatching pursuit type methods. At each iteration, the main cost of the proposed\nmethods is only to compute a rank-one tensor, which can be done efficiently,\nmaking the proposed methods scalable to large scale problems. Moreover, storing\nthe resulting rank-one tensors is of low storage requirement, which can help to\nbreak the curse of dimensionality. The linear convergence rate of the proposed\nmethods is established in various circumstances. Along with the main methods,\nwe also provide a method of low computational complexity for approximately\ncomputing the rank-one tensors, with provable approximation ratio, which helps\nto improve the efficiency of the main methods and to analyze the convergence\nrate. Experimental results on synthetic as well as real datasets verify the\nefficiency and effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 21:38:07 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Yang", "Yuning", ""], ["Mehrkanoon", "Siamak", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1503.02356", "submitter": "Masayuki Ohzeki", "authors": "M. Ohzeki and A. Ichiki", "title": "Mathematical understanding of detailed balance condition violation and\n  its application to Langevin dynamics", "comments": "18pages, 3 figures, proceeedings of STATPHYS KOLKATA VIII", "journal-ref": null, "doi": "10.1088/1742-6596/638/1/012003", "report-no": null, "categories": "cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient sampling method by simulating Langevin dynamics with\nan artificial force rather than a natural force by using the gradient of the\npotential energy. The standard technique for sampling following the\npredetermined distribution such as the Gibbs-Boltzmann one is performed under\nthe detailed balance condition. In the present study, we propose a modified\nLangevin dynamics violating the detailed balance condition on the\ntransition-probability formulation. We confirm that the numerical\nimplementation of the proposed method actually demonstrates two major\nbeneficial improvements: acceleration of the relaxation to the predetermined\ndistribution and reduction of the correlation time between two different\nrealizations in the steady state.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 01:46:06 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Ohzeki", "M.", ""], ["Ichiki", "A.", ""]]}, {"id": "1503.02398", "submitter": "Matthias Seibert", "authors": "Matthias Seibert, Julian W\\\"ormann, R\\'emi Gribonval, Martin\n  Kleinsteuber", "title": "Learning Co-Sparse Analysis Operators with Separable Structures", "comments": "11 pages double column, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TSP.2015.2481875", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the co-sparse analysis model a set of filters is applied to a signal out\nof the signal class of interest yielding sparse filter responses. As such, it\nmay serve as a prior in inverse problems, or for structural analysis of signals\nthat are known to belong to the signal class. The more the model is adapted to\nthe class, the more reliable it is for these purposes. The task of learning\nsuch operators for a given class is therefore a crucial problem. In many\napplications, it is also required that the filter responses are obtained in a\ntimely manner, which can be achieved by filters with a separable structure. Not\nonly can operators of this sort be efficiently used for computing the filter\nresponses, but they also have the advantage that less training samples are\nrequired to obtain a reliable estimate of the operator. The first contribution\nof this work is to give theoretical evidence for this claim by providing an\nupper bound for the sample complexity of the learning process. The second is a\nstochastic gradient descent (SGD) method designed to learn an analysis operator\nwith separable structures, which includes a novel and efficient step size\nselection rule. Numerical experiments are provided that link the sample\ncomplexity to the convergence speed of the SGD algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 08:53:33 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 10:36:58 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 13:02:02 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2015 14:08:50 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2015 19:04:19 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Seibert", "Matthias", ""], ["W\u00f6rmann", "Julian", ""], ["Gribonval", "R\u00e9mi", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1503.02424", "submitter": "Yarin Gal", "authors": "Yarin Gal, Richard Turner", "title": "Improving the Gaussian Process Sparse Spectrum Approximation by\n  Representing Uncertainty in Frequency Inputs", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard sparse pseudo-input approximations to the Gaussian process (GP)\ncannot handle complex functions well. Sparse spectrum alternatives attempt to\nanswer this but are known to over-fit. We suggest the use of variational\ninference for the sparse spectrum approximation to avoid both issues. We model\nthe covariance function with a finite Fourier series approximation and treat it\nas a random variable. The random covariance function has a posterior, on which\na variational distribution is placed. The variational distribution transforms\nthe random covariance function to fit the data. We study the properties of our\napproximate inference, compare it to alternative ones, and extend it to the\ndistributed and stochastic domains. Our approximation captures complex\nfunctions better than standard approaches and avoids over-fitting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 11:04:58 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 15:49:34 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Gal", "Yarin", ""], ["Turner", "Richard", ""]]}, {"id": "1503.02531", "submitter": "Oriol Vinyals", "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean", "title": "Distilling the Knowledge in a Neural Network", "comments": "NIPS 2014 Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:44:49 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Hinton", "Geoffrey", ""], ["Vinyals", "Oriol", ""], ["Dean", "Jeff", ""]]}, {"id": "1503.02533", "submitter": "Gabriel Peyr\\'e", "authors": "Marco Cuturi, Gabriel Peyr\\'e", "title": "A Smoothed Dual Approach for Variational Wasserstein Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational problems that involve Wasserstein distances have been recently\nproposed to summarize and learn from probability measures. Despite being\nconceptually simple, such problems are computationally challenging because they\ninvolve minimizing over quantities (Wasserstein distances) that are themselves\nhard to compute. We show that the dual formulation of Wasserstein variational\nproblems introduced recently by Carlier et al. (2014) can be regularized using\nan entropic smoothing, which leads to smooth, differentiable, convex\noptimization problems that are simpler to implement and numerically more\nstable. We illustrate the versatility of this approach by applying it to the\ncomputation of Wasserstein barycenters and gradient flows of spacial\nregularization functionals.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:49:21 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 21:47:10 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1503.02551", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami,\n  Balaji Lakshminarayanan, Dino Sejdinovic, Zolt\\'an Szab\\'o", "title": "Kernel-Based Just-In-Time Learning for Passing Expectation Propagation\n  Messages", "comments": "accepted to UAI 2015. Correct typos. Add more content to the\n  appendix. Main results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient nonparametric strategy for learning a message\noperator in expectation propagation (EP), which takes as input the set of\nincoming messages to a factor node, and produces an outgoing message as output.\nThis learned operator replaces the multivariate integral required in classical\nEP, which may not have an analytic expression. We use kernel-based regression,\nwhich is trained on a set of probability distributions representing the\nincoming messages, and the associated outgoing messages. The kernel approach\nhas two main advantages: first, it is fast, as it is implemented using a novel\ntwo-layer random feature representation of the input message distributions;\nsecond, it has principled uncertainty estimates, and can be cheaply updated\nonline, meaning it can request and incorporate new training data when it\nencounters inputs on which it is uncertain. In experiments, our approach is\nable to solve learning problems where a single message operator is required for\nmultiple, substantially different data sets (logistic regression for a variety\nof classification problems), where it is essential to accurately assess\nuncertainty and to efficiently and robustly update the message operator.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 16:30:17 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 09:17:38 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Gretton", "Arthur", ""], ["Heess", "Nicolas", ""], ["Eslami", "S. M. Ali", ""], ["Lakshminarayanan", "Balaji", ""], ["Sejdinovic", "Dino", ""], ["Szab\u00f3", "Zolt\u00e1n", ""]]}, {"id": "1503.02596", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on, Nigel Boston, Robert D. Nowak", "title": "A Characterization of Deterministic Sampling Patterns for Low-Rank\n  Matrix Completion", "comments": "This update corrects an error in version 2 of this paper, where we\n  erroneously assumed that columns with more than r+1 observed entries would\n  yield multiple independent constraints", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 10, no.\n  4, pp. 623-636, June, 2016", "doi": "10.1109/JSTSP.2016.2537145", "report-no": null, "categories": "stat.ML cs.LG math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion (LRMC) problems arise in a wide variety of\napplications. Previous theory mainly provides conditions for completion under\nmissing-at-random samplings. This paper studies deterministic conditions for\ncompletion. An incomplete $d \\times N$ matrix is finitely rank-$r$ completable\nif there are at most finitely many rank-$r$ matrices that agree with all its\nobserved entries. Finite completability is the tipping point in LRMC, as a few\nadditional samples of a finitely completable matrix guarantee its unique\ncompletability. The main contribution of this paper is a deterministic sampling\ncondition for finite completability. We use this to also derive deterministic\nsampling conditions for unique completability that can be efficiently verified.\nWe also show that under uniform random sampling schemes, these conditions are\nsatisfied with high probability if $O(\\max\\{r,\\log d\\})$ entries per column are\nobserved. These findings have several implications on LRMC regarding lower\nbounds, sample and computational complexity, the role of coherence, adaptive\nsettings and the validation of any completion algorithm. We complement our\ntheoretical results with experiments that support our findings and motivate\nfuture analysis of uncharted sampling regimes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 18:12:58 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 17:13:34 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 13:54:25 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""], ["Boston", "Nigel", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1503.02698", "submitter": "Zhe Liu", "authors": "Zhe Liu", "title": "Graphical Exponential Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensions we propose and analyze an aggregation estimator of the\nprecision matrix for Gaussian graphical models. This estimator, called\ngraphical Exponential Screening (gES), linearly combines a suitable set of\nindividual estimators with different underlying graphs, and balances the\nestimation error and sparsity. We study the risk of this aggregation estimator\nand show that it is comparable to that of the best estimator based on a single\ngraph, chosen by an oracle. Numerical performance of our method is investigated\nusing both simulated and real datasets, in comparison with some state-of-art\nestimation procedures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 21:10:48 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 00:55:28 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Liu", "Zhe", ""]]}, {"id": "1503.02761", "submitter": "Ava Bargi", "authors": "Ava Bargi, Richard Yi Da Xu, Massimo Piccardi", "title": "An Adaptive Online HDP-HMM for Segmentation and Classification of\n  Sequential Data", "comments": "23 pages, 9 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, the desire and need to understand sequential data has\nbeen increasing, with particular interest in sequential contexts such as\npatient monitoring, understanding daily activities, video surveillance, stock\nmarket and the like. Along with the constant flow of data, it is critical to\nclassify and segment the observations on-the-fly, without being limited to a\nrigid number of classes. In addition, the model needs to be capable of updating\nits parameters to comply with possible evolutions. This interesting problem,\nhowever, is not adequately addressed in the literature since many studies focus\non offline classification over a pre-defined class set. In this paper, we\npropose a principled solution to this gap by introducing an adaptive online\nsystem based on Markov switching models with hierarchical Dirichlet process\npriors. This infinite adaptive online approach is capable of segmenting and\nclassifying the sequential data over unlimited number of classes, while meeting\nthe memory and delay constraints of streaming contexts. The model is further\nenhanced by introducing a learning rate, responsible for balancing the extent\nto which the model sustains its previous learning (parameters) or adapts to the\nnew streaming observations. Experimental results on several variants of\nstationary and evolving synthetic data and two video datasets, TUM Assistive\nKitchen and collatedWeizmann, show remarkable performance in segmentation and\nclassification, particularly for evolutionary sequences with changing\ndistributions and/or containing new, unseen classes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 03:27:34 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 01:36:18 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Bargi", "Ava", ""], ["Da Xu", "Richard Yi", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1503.02768", "submitter": "Bahman Yari Saeed Khanloo", "authors": "Bahman Yari Saeed Khanloo, Gholamreza Haffari", "title": "Novel Bernstein-like Concentration Inequalities for the Missing Mass", "comments": "arXiv admin note: text overlap with arXiv:1402.6262. Appears in 31st\n  Conference on Uncertainty in Artificial Intelligence (UAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with obtaining novel concentration inequalities for the\nmissing mass, i.e. the total probability mass of the outcomes not observed in\nthe sample. We not only derive - for the first time - distribution-free\nBernstein-like deviation bounds with sublinear exponents in deviation size for\nmissing mass, but also improve the results of McAllester and Ortiz (2003)\nandBerend and Kontorovich (2013, 2012) for small deviations which is the most\ninteresting case in learning theory. It is known that the majority of standard\ninequalities cannot be directly used to analyze heterogeneous sums i.e. sums\nwhose terms have large difference in magnitude. Our generic and intuitive\napproach shows that the heterogeneity issue introduced in McAllester and Ortiz\n(2003) is resolvable at least in the case of missing mass via regulating the\nterms using our novel thresholding technique.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 04:38:46 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 07:52:20 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Khanloo", "Bahman Yari Saeed", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1503.02817", "submitter": "Ming Yuan", "authors": "Ming Yuan and Ding-Xuan Zhou", "title": "Minimax Optimal Rates of Estimation in High Dimensional Additive Models:\n  Universal Phase Transition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish minimax optimal rates of convergence for estimation in a high\ndimensional additive model assuming that it is approximately sparse. Our\nresults reveal an interesting phase transition behavior universal to this class\nof high dimensional problems. In the {\\it sparse regime} when the components\nare sufficiently smooth or the dimensionality is sufficiently large, the\noptimal rates are identical to those for high dimensional linear regression,\nand therefore there is no additional cost to entertain a nonparametric model.\nOtherwise, in the so-called {\\it smooth regime}, the rates coincide with the\noptimal rates for estimating a univariate function, and therefore they are\nimmune to the \"curse of dimensionality\".\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 08:56:34 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Yuan", "Ming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1503.02893", "submitter": "Weiyu Xu", "authors": "Jian-Feng Cai, Xiaobo Qu, Weiyu Xu, Gui-Bo Ye", "title": "Robust recovery of complex exponential signals from random Gaussian\n  projections via low rank Hankel matrix reconstruction", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores robust recovery of a superposition of $R$ distinct\ncomplex exponential functions from a few random Gaussian projections. We assume\nthat the signal of interest is of $2N-1$ dimensional and $R<<2N-1$. This\nframework covers a large class of signals arising from real applications in\nbiology, automation, imaging science, etc. To reconstruct such a signal, our\nalgorithm is to seek a low-rank Hankel matrix of the signal by minimizing its\nnuclear norm subject to the consistency on the sampled data. Our theoretical\nresults show that a robust recovery is possible as long as the number of\nprojections exceeds $O(R\\ln^2N)$. No incoherence or separation condition is\nrequired in our proof. Our method can be applied to spectral compressed sensing\nwhere the signal of interest is a superposition of $R$ complex sinusoids.\nCompared to existing results, our result here does not need any separation\ncondition on the frequencies, while achieving better or comparable bounds on\nthe number of measurements. Furthermore, our method provides theoretical\nguidance on how many samples are required in the state-of-the-art non-uniform\nsampling in NMR spectroscopy. The performance of our algorithm is further\ndemonstrated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 13:16:45 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Qu", "Xiaobo", ""], ["Xu", "Weiyu", ""], ["Ye", "Gui-Bo", ""]]}, {"id": "1503.02978", "submitter": "Junwei Lu", "authors": "Junwei Lu, Mladen Kolar and Han Liu", "title": "Kernel Meets Sieve: Post-Regularization Confidence Bands for Sparse\n  Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel procedure for constructing confidence bands for components\nof a sparse additive model. Our procedure is based on a new kernel-sieve hybrid\nestimator that combines two most popular nonparametric estimation methods in\nthe literature, the kernel regression and the spline method, and is of interest\nin its own right. Existing methods for fitting sparse additive model are\nprimarily based on sieve estimators, while the literature on confidence bands\nfor nonparametric models are primarily based upon kernel or local polynomial\nestimators. Our kernel-sieve hybrid estimator combines the best of both worlds\nand allows us to provide a simple procedure for constructing confidence bands\nin high-dimensional sparse additive models. We prove that the confidence bands\nare asymptotically honest by studying approximation with a Gaussian process.\nThorough numerical results on both synthetic data and real-world neuroscience\ndata are provided to demonstrate the efficacy of the theory.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 16:26:31 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 20:44:44 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Lu", "Junwei", ""], ["Kolar", "Mladen", ""], ["Liu", "Han", ""]]}, {"id": "1503.03082", "submitter": "Nino Shervashidze", "authors": "Nino Shervashidze (SIERRA, LIENS), Francis Bach (SIERRA, LIENS)", "title": "Learning the Structure for Structured Sparsity", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Institute of Electrical\n  and Electronics Engineers (IEEE), 2015, 63 (18), pp.4894 - 4902.\n  \\&lt;10.1109/TSP.2015.2446432\\&gt;", "doi": "10.1109/TSP.2015.2446432", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sparsity has recently emerged in statistics, machine learning and\nsignal processing as a promising paradigm for learning in high-dimensional\nsettings. All existing methods for learning under the assumption of structured\nsparsity rely on prior knowledge on how to weight (or how to penalize)\nindividual subsets of variables during the subset selection process, which is\nnot available in general. Inferring group weights from data is a key open\nresearch problem in structured sparsity.In this paper, we propose a Bayesian\napproach to the problem of group weight learning. We model the group weights as\nhyperparameters of heavy-tailed priors on groups of variables and derive an\napproximate inference scheme to infer these hyperparameters. We empirically\nshow that we are able to recover the model hyperparameters when the data are\ngenerated from the model, and we demonstrate the utility of learning weights in\nsynthetic and real denoising problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 20:09:13 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 19:40:46 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Shervashidze", "Nino", "", "SIERRA, LIENS"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1503.03132", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "L_1-regularized Boltzmann machine learning using majorizer minimization", "comments": "16pages, 6 figures", "journal-ref": null, "doi": "10.7566/JPSJ.84.054801", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inference method to estimate sparse interactions and biases\naccording to Boltzmann machine learning. The basis of this method is $L_1$\nregularization, which is often used in compressed sensing, a technique for\nreconstructing sparse input signals from undersampled outputs. $L_1$\nregularization impedes the simple application of the gradient method, which\noptimizes the cost function that leads to accurate estimations, owing to the\ncost function's lack of smoothness. In this study, we utilize the majorizer\nminimization method, which is a well-known technique implemented in\noptimization problems, to avoid the non-smoothness of the cost function. By\nusing the majorizer minimization method, we elucidate essentially relevant\nbiases and interactions from given data with seemingly strongly-correlated\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 00:21:51 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1503.03148", "submitter": "Jayadeva", "authors": "Jayadeva, Sumit Soman, Amit Bhaya", "title": "A Neurodynamical System for finding a Minimal VC Dimension Classifier", "comments": "15 pages, 3 figures", "journal-ref": "Neural Networks, Volume 132, 2020, Pages 405-415", "doi": "10.1016/j.neunet.2020.08.013", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane\nclassifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)\ndimension. The VC dimension measures the capacity of a learning machine, and a\nsmaller VC dimension leads to improved generalization. On many benchmark\ndatasets, the MCM generalizes better than SVMs and uses far fewer support\nvectors than the number used by SVMs. In this paper, we describe a neural\nnetwork based on a linear dynamical system, that converges to the MCM solution.\nThe proposed MCM dynamical system is conducive to an analogue circuit\nimplementation on a chip or simulation using Ordinary Differential Equation\n(ODE) solvers. Numerical experiments on benchmark datasets from the UCI\nrepository show that the proposed approach is scalable and accurate, as we\nobtain improved accuracies and fewer number of support vectors (upto 74.3%\nreduction) with the MCM dynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 02:10:26 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jayadeva", "", ""], ["Soman", "Sumit", ""], ["Bhaya", "Amit", ""]]}, {"id": "1503.03188", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan", "title": "Optimal prediction for sparse linear models? Lower bounds for\n  coordinate-separable M-estimators", "comments": "Add more coverage on related work; add a new lower bound for design\n  matrices satisfying the restricted eigenvalue condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of high-dimensional sparse linear regression, it is known\nthat an $\\ell_0$-based estimator can achieve a $1/n$ \"fast\" rate on the\nprediction error without any conditions on the design matrix, whereas in\nabsence of restrictive conditions on the design matrix, popular polynomial-time\nmethods only guarantee the $1/\\sqrt{n}$ \"slow\" rate. In this paper, we show\nthat the slow rate is intrinsic to a broad class of M-estimators. In\nparticular, for estimators based on minimizing a least-squares cost function\ntogether with a (possibly non-convex) coordinate-wise separable regularizer,\nthere is always a \"bad\" local optimum such that the associated prediction error\nis lower bounded by a constant multiple of $1/\\sqrt{n}$. For convex\nregularizers, this lower bound applies to all global optima. The theory is\napplicable to many popular estimators, including convex $\\ell_1$-based methods\nas well as M-estimators based on nonconvex regularizers, including the SCAD\npenalty or the MCP regularizer. In addition, for a broad class of nonconvex\nregularizers, we show that the bad local optima are very common, in that a\nbroad class of local minimization algorithms with random initialization will\ntypically converge to a bad solution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 06:22:44 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 01:28:33 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhang", "Yuchen", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1503.03231", "submitter": "Joao Mota", "authors": "Joao F. C. Mota, Nikos Deligiannis, Aswin C. Sankaranarayanan, Volkan\n  Cevher, Miguel R. D. Rodrigues", "title": "Adaptive-Rate Sparse Signal Reconstruction With Application in\n  Compressive Background Subtraction", "comments": "submitted to IEEE Trans. Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze an online algorithm for reconstructing a sequence of\nsignals from a limited number of linear measurements. The signals are assumed\nsparse, with unknown support, and evolve over time according to a generic\nnonlinear dynamical model. Our algorithm, based on recent theoretical results\nfor $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of\nmeasurements to be taken at each time on-the-fly. As an example, we apply the\nalgorithm to compressive video background subtraction, a problem that can be\nstated as follows: given a set of measurements of a sequence of images with a\nstatic background, simultaneously reconstruct each image while separating its\nforeground from the background. The performance of our method is illustrated on\nsequences of real images: we observe that it allows a dramatic reduction in the\nnumber of measurements with respect to state-of-the-art compressive background\nsubtraction schemes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:16:39 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Mota", "Joao F. C.", ""], ["Deligiannis", "Nikos", ""], ["Sankaranarayanan", "Aswin C.", ""], ["Cevher", "Volkan", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1503.03355", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis", "title": "Automatic Unsupervised Tensor Mining with Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 14:34:46 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Papalexakis", "Evangelos E.", ""]]}, {"id": "1503.03438", "submitter": "Mark Tygert", "authors": "Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur\n  Szlam, and Mark Tygert", "title": "A mathematical motivation for complex-valued convolutional networks", "comments": "11 pages, 3 figures; this is the retitled version submitted to the\n  journal, \"Neural Computation\"", "journal-ref": "Neural Computation, 28 (5): 815-825, May 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex-valued convolutional network (convnet) implements the repeated\napplication of the following composition of three operations, recursively\napplying the composition to an input vector of nonnegative real numbers: (1)\nconvolution with complex-valued vectors followed by (2) taking the absolute\nvalue of every entry of the resulting vectors followed by (3) local averaging.\nFor processing real-valued random vectors, complex-valued convnets can be\nviewed as \"data-driven multiscale windowed power spectra,\" \"data-driven\nmultiscale windowed absolute spectra,\" \"data-driven multiwavelet absolute\nvalues,\" or (in their most general configuration) \"data-driven nonlinear\nmultiwavelet packets.\" Indeed, complex-valued convnets can calculate multiscale\nwindowed spectra when the convnet filters are windowed complex-valued\nexponentials. Standard real-valued convnets, using rectified linear units\n(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.\npooling, etc., do not obviously exhibit the same exact correspondence with\ndata-driven wavelets (whereas for complex-valued convnets, the correspondence\nis much more than just a vague analogy). Courtesy of the exact correspondence,\nthe remarkably rich and rigorous body of mathematical analysis for wavelets\napplies directly to (complex-valued) convnets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 18:24:13 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 00:42:09 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2015 19:04:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Bruna", "Joan", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""], ["Piantino", "Serkan", ""], ["Szlam", "Arthur", ""], ["Tygert", "Mark", ""]]}, {"id": "1503.03467", "submitter": "Houman Owhadi", "authors": "Houman Owhadi", "title": "Multigrid with rough coefficients and Multiresolution operator\n  decomposition from Hierarchical Information Games", "comments": "Presented at SIAM CSE 15. Final (published) version.\n  http://epubs.siam.org/doi/abs/10.1137/15M1013894", "journal-ref": "SIAM Rev. 59-1, pp. 99-149 (2017)", "doi": null, "report-no": null, "categories": "math.NA cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a near-linear complexity (geometric and meshless/algebraic)\nmultigrid/multiresolution method for PDEs with rough ($L^\\infty$) coefficients\nwith rigorous a-priori accuracy and performance estimates. The method is\ndiscovered through a decision/game theory formulation of the problems of (1)\nidentifying restriction and interpolation operators (2) recovering a signal\nfrom incomplete measurements based on norm constraints on its image under a\nlinear operator (3) gambling on the value of the solution of the PDE based on a\nhierarchy of nested measurements of its solution or source term. The resulting\nelementary gambles form a hierarchy of (deterministic) basis functions of\n$H^1_0(\\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands\nwith respect to the scalar product induced by the energy norm of the PDE (2)\nenable sparse compression of the solution space in $H^1_0(\\Omega)$ (3) induce\nan orthogonal multiresolution operator decomposition. The operating diagram of\nthe multigrid method is that of an inverted pyramid in which gamblets are\ncomputed locally (by virtue of their exponential decay), hierarchically (from\nfine to coarse scales) and the PDE is decomposed into a hierarchy of\nindependent linear systems with uniformly bounded condition numbers. The\nresulting algorithm is parallelizable both in space (via localization) and in\nbandwith/subscale (subscales can be computed independently from each other).\nAlthough the method is deterministic it has a natural Bayesian interpretation\nunder the measure of probability emerging (as a mixed strategy) from the\ninformation game formulation and multiresolution approximations form a\nmartingale with respect to the filtration induced by the hierarchy of nested\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:52:13 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 19:27:55 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 01:09:11 GMT"}, {"version": "v4", "created": "Tue, 8 Mar 2016 05:57:10 GMT"}, {"version": "v5", "created": "Fri, 10 Feb 2017 07:02:43 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Owhadi", "Houman", ""]]}, {"id": "1503.03517", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Mohammad Amin Rahimian, Ali Jadbabaie", "title": "Switching to Learn", "comments": "6 pages, To appear in American Control Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network of agents attempt to learn some unknown state of the world drawn by\nnature from a finite set. Agents observe private signals conditioned on the\ntrue state, and form beliefs about the unknown state accordingly. Each agent\nmay face an identification problem in the sense that she cannot distinguish the\ntruth in isolation. However, by communicating with each other, agents are able\nto benefit from side observations to learn the truth collectively. Unlike many\ndistributed algorithms which rely on all-time communication protocols, we\npropose an efficient method by switching between Bayesian and non-Bayesian\nregimes. In this model, agents exchange information only when their private\nsignals are not informative enough; thence, by switching between the two\nregimes, agents efficiently learn the truth using only a few rounds of\ncommunications. The proposed algorithm preserves learnability while incurring a\nlower communication cost. We also verify our theoretical findings by simulation\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 22:05:50 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rahimian", "Mohammad Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1503.03524", "submitter": "Mohamed Kafsi", "authors": "Mohamed Kafsi, Henriette Cramer, Bart Thomee and David A. Shamma", "title": "Describing and Understanding Neighborhood Characteristics through Online\n  Social Media", "comments": "Accepted in WWW 2015, 2015, Florence, Italy", "journal-ref": null, "doi": "10.1145/2736277.2741133", "report-no": "ACM 978-1-4503-3469-3/15/05", "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geotagged data can be used to describe regions in the world and discover\nlocal themes. However, not all data produced within a region is necessarily\nspecifically descriptive of that area. To surface the content that is\ncharacteristic for a region, we present the geographical hierarchy model (GHM),\na probabilistic model based on the assumption that data observed in a region is\na random mixture of content that pertains to different levels of a hierarchy.\nWe apply the GHM to a dataset of 8 million Flickr photos in order to\ndiscriminate between content (i.e., tags) that specifically characterizes a\nregion (e.g., neighborhood) and content that characterizes surrounding areas or\nmore general themes. Knowledge of the discriminative and non-discriminative\nterms used throughout the hierarchy enables us to quantify the uniqueness of a\ngiven region and to compare similar but distant regions. Our evaluation\ndemonstrates that our model improves upon traditional Naive Bayes\nclassification by 47% and hierarchical TF-IDF by 27%. We further highlight the\ndifferences and commonalities with human reasoning about what is locally\ncharacteristic for a neighborhood, distilled from ten interviews and a survey\nthat covered themes such as time, events, and prior regional knowledge\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 22:13:38 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kafsi", "Mohamed", ""], ["Cramer", "Henriette", ""], ["Thomee", "Bart", ""], ["Shamma", "David A.", ""]]}, {"id": "1503.03525", "submitter": "Namrata Vaswani", "authors": "Brian Lois and Namrata Vaswani", "title": "Online Matrix Completion and Online Robust PCA", "comments": "Presented at ISIT (IEEE Intnl. Symp. on Information Theory), 2015.\n  Submitted to IEEE Transactions on Information Theory. This version: changes\n  are in blue; the main changes are just to explain the model assumptions\n  better (added based on ISIT reviewers' comments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies two interrelated problems - online robust PCA (RPCA) and\nonline low-rank matrix completion (MC). In recent work by Cand\\`{e}s et al.,\nRPCA has been defined as a problem of separating a low-rank matrix (true data),\n$L:=[\\ell_1, \\ell_2, \\dots \\ell_{t}, \\dots , \\ell_{t_{\\max}}]$ and a sparse\nmatrix (outliers), $S:=[x_1, x_2, \\dots x_{t}, \\dots, x_{t_{\\max}}]$ from their\nsum, $M:=L+S$. Our work uses this definition of RPCA. An important application\nwhere both these problems occur is in video analytics in trying to separate\nsparse foregrounds (e.g., moving objects) and slowly changing backgrounds.\n  While there has been a large amount of recent work on both developing and\nanalyzing batch RPCA and batch MC algorithms, the online problem is largely\nopen. In this work, we develop a practical modification of our recently\nproposed algorithm to solve both the online RPCA and online MC problems. The\nmain contribution of this work is that we obtain correctness results for the\nproposed algorithms under mild assumptions. The assumptions that we need are:\n(a) a good estimate of the initial subspace is available (easy to obtain using\na short sequence of background-only frames in video surveillance); (b) the\n$\\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors for\nthe subspace from which $\\ell_t$ is generated are dense (non-sparse); (d) the\nsupport of $x_t$ changes by at least a certain amount at least every so often;\nand (e) algorithm parameters are appropriately set\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 22:20:16 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 18:19:36 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Lois", "Brian", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1503.03585", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya\n  Ganguli", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 04:51:37 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 06:48:02 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 06:00:20 GMT"}, {"version": "v4", "created": "Wed, 13 May 2015 01:57:49 GMT"}, {"version": "v5", "created": "Wed, 20 May 2015 03:19:10 GMT"}, {"version": "v6", "created": "Thu, 9 Jul 2015 16:16:33 GMT"}, {"version": "v7", "created": "Tue, 21 Jul 2015 19:44:20 GMT"}, {"version": "v8", "created": "Wed, 18 Nov 2015 21:50:51 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Weiss", "Eric A.", ""], ["Maheswaranathan", "Niru", ""], ["Ganguli", "Surya", ""]]}, {"id": "1503.03613", "submitter": "Mesrob Ohannessian", "authors": "Elchanan Mossel and Mesrob I. Ohannessian", "title": "On the Impossibility of Learning the Missing Mass", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\".\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 07:27:24 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mossel", "Elchanan", ""], ["Ohannessian", "Mesrob I.", ""]]}, {"id": "1503.03673", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen, Su-Yun Huang, Yanyuan Ma and I-Ping Tu", "title": "Functional Inverse Regression in an Enlarged Dimension Reduction Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider an enlarged dimension reduction space in functional inverse\nregression. Our operator and functional analysis based approach facilitates a\ncompact and rigorous formulation of the functional inverse regression problem.\nIt also enables us to expand the possible space where the dimension reduction\nfunctions belong. Our formulation provides a unified framework so that the\nclassical notions, such as covariance standardization, Mahalanobis distance,\nSIR and linear discriminant analysis, can be naturally and smoothly carried out\nin our enlarged space. This enlarged dimension reduction space also links to\nthe linear discriminant space of Gaussian measures on a separable Hilbert\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 11:14:38 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Chen", "Ting-Li", ""], ["Huang", "Su-Yun", ""], ["Ma", "Yanyuan", ""], ["Tu", "I-Ping", ""]]}, {"id": "1503.03701", "submitter": "Alessandro Perina", "authors": "Nebojsa Jojic and Alessandro Perina and Dongwoo Kim", "title": "Hierarchical learning of grids of microtopics", "comments": "To Appear in Uncertainty in Artificial Intelligence - UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The counting grid is a grid of microtopics, sparse word/feature\ndistributions. The generative model associated with the grid does not use these\nmicrotopics individually. Rather, it groups them in overlapping rectangular\nwindows and uses these grouped microtopics as either mixture or admixture\ncomponents. This paper builds upon the basic counting grid model and it shows\nthat hierarchical reasoning helps avoid bad local minima, produces better\nclassification accuracy and, most interestingly, allows for extraction of large\nnumbers of coherent microtopics even from small datasets. We evaluate this in\nterms of consistency, diversity and clarity of the indexed content, as well as\nin a user study on word intrusion tasks. We demonstrate that these models work\nwell as a technique for embedding raw images and discuss interesting parallels\nbetween hierarchical CG models and other deep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 12:59:25 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 16:38:24 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 16:46:07 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 15:05:38 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jojic", "Nebojsa", ""], ["Perina", "Alessandro", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1503.03879", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri", "title": "Qualitative inequalities for squared partial correlations of a Gaussian\n  random vector", "comments": "21 pages, 13 figures", "journal-ref": "Annals of the Institute of Statistical Mathematics, 66(2),\n  345-367, 2014", "doi": "10.1007/s10463-013-0417-x", "report-no": "Department of Statistics and Applied Probability, National\n  University of Singapore technical report 201301", "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe various sets of conditional independence relationships,\nsufficient for qualitatively comparing non-vanishing squared partial\ncorrelations of a Gaussian random vector. These sufficient conditions are\nsatisfied by several graphical Markov models. Rules for comparing degree of\nassociation among the vertices of such Gaussian graphical models are also\ndeveloped. We apply these rules to compare conditional dependencies on Gaussian\ntrees. In particular for trees, we show that such dependence can be completely\ncharacterized by the length of the paths joining the dependent vertices to each\nother and to the vertices conditioned on. We also apply our results to\npostulate rules for model selection for polytree models. Our rules apply to\nmutual information of Gaussian random vectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:15:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhuri", "Sanjay", ""]]}, {"id": "1503.03893", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Sanjiv Kumar, Henry Rowley, Shih-Fu Chang", "title": "Compact Nonlinear Maps and Circulant Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel approximation via nonlinear random feature maps is widely used in\nspeeding up kernel machines. There are two main challenges for the conventional\nkernel approximation methods. First, before performing kernel approximation, a\ngood kernel has to be chosen. Picking a good kernel is a very challenging\nproblem in itself. Second, high-dimensional maps are often required in order to\nachieve good performance. This leads to high computational cost in both\ngenerating the nonlinear maps, and in the subsequent learning and prediction\nprocess. In this work, we propose to optimize the nonlinear maps directly with\nrespect to the classification objective in a data-dependent fashion. The\nproposed approach achieves kernel approximation and kernel learning in a joint\nframework. This leads to much more compact maps without hurting the\nperformance. As a by-product, the same framework can also be used to achieve\nmore compact kernel maps to approximate a known kernel. We also introduce\nCirculant Nonlinear Maps, which uses a circulant-structured projection matrix\nto speed up the nonlinear maps for high-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 21:19:13 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Yu", "Felix X.", ""], ["Kumar", "Sanjiv", ""], ["Rowley", "Henry", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1503.03903", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail", "title": "Approximating Sparse PCA from Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how well one can recover sparse principal components of a data\nmatrix using a sketch formed from a few of its elements. We show that for a\nwide class of optimization problems, if the sketch is close (in the spectral\nnorm) to the original data matrix, then one can recover a near optimal solution\nto the optimization problem by using the sketch. In particular, we use this\napproach to obtain sparse principal components and show that for \\math{m} data\npoints in \\math{n} dimensions, \\math{O(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})}\nelements gives an \\math{\\epsilon}-additive approximation to the sparse PCA\nproblem (\\math{\\tilde k} is the stable rank of the data matrix). We demonstrate\nour algorithms extensively on image, text, biological and financial data. The\nresults show that not only are we able to recover the sparse PCAs from the\nincomplete data, but by using our sparse sketch, the running time drops by a\nfactor of five or more.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 22:16:55 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Kundu", "Abhisek", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1503.03964", "submitter": "Shintaro Mori", "authors": "Shunsuke Yoshida, Masato Hisakado and Shintaro Mori", "title": "Interactive Restless Multi-armed Bandit Game and Swarm Intelligence\n  Effect", "comments": "18 pages, 4 figures", "journal-ref": "New generation computing, vol.34, No. 3, 291-306, 2016", "doi": "10.1007/s00354-016-0306-y", "report-no": null, "categories": "cs.AI cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the conditions for the emergence of the swarm intelligence effect\nin an interactive game of restless multi-armed bandit (rMAB). A player competes\nwith multiple agents. Each bandit has a payoff that changes with a probability\n$p_{c}$ per round. The agents and player choose one of three options: (1)\nExploit (a good bandit), (2) Innovate (asocial learning for a good bandit among\n$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good\nbandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:\n(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability\nfor Observe in learning. The parameters $(c,p_{obs})$ are uniformly\ndistributed. We determine the optimal strategies for the player using complete\nknowledge about the rMAB. We show whether or not social or asocial learning is\nmore optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence\neffect. We conduct a laboratory experiment (67 subjects) and observe the swarm\nintelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning\nis far more optimal than asocial learning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 06:53:01 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Yoshida", "Shunsuke", ""], ["Hisakado", "Masato", ""], ["Mori", "Shintaro", ""]]}, {"id": "1503.04337", "submitter": "Yuekai Sun", "authors": "Jason D. Lee, Yuekai Sun, Qiang Liu, Jonathan E. Taylor", "title": "Communication-efficient sparse regression: a one-shot approach", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a one-shot approach to distributed sparse regression in the\nhigh-dimensional setting. The key idea is to average \"debiased\" or\n\"desparsified\" lasso estimators. We show the approach converges at the same\nrate as the lasso as long as the dataset is not split across too many machines.\nWe also extend the approach to generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 19:43:30 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 13:57:12 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2015 17:16:01 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Liu", "Qiang", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1503.04474", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey\n  Simmons and Alfred Hero", "title": "Statistical Estimation and Clustering of Group-invariant Orientation\n  Parameters", "comments": "arXiv admin note: text overlap with arXiv:1411.2540", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat the problem of estimation of orientation parameters whose values are\ninvariant to transformations from a spherical symmetry group. Previous work has\nshown that any such group-invariant distribution must satisfy a restricted\nfinite mixture representation, which allows the orientation parameter to be\nestimated using an Expectation Maximization (EM) maximum likelihood (ML)\nestimation algorithm. In this paper, we introduce two parametric models for\nthis spherical symmetry group estimation problem: 1) the hyperbolic Von Mises\nFisher (VMF) mixture distribution and 2) the Watson mixture distribution. We\nalso introduce a new EM-ML algorithm for clustering samples that come from\nmixtures of group-invariant distributions with different parameters. We apply\nthe models to the problem of mean crystal orientation estimation under the\nspherically symmetric group associated with the crystal form, e.g., cubic or\noctahedral or hexahedral. Simulations and experiments establish the advantages\nof the extended EM-VMF and EM-Watson estimators for data acquired by Electron\nBackscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloy\nsample.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 20:46:40 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 15:55:45 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Wei", "Dennis", ""], ["Newstadt", "Gregory", ""], ["DeGraef", "Marc", ""], ["Simmons", "Jeffrey", ""], ["Hero", "Alfred", ""]]}, {"id": "1503.04549", "submitter": "Makoto Aoshima", "authors": "Makoto Aoshima, Kazuyoshi Yata", "title": "High-dimensional quadratic classifiers in non-sparse settings", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional quadratic classifiers in non-sparse settings.\nThe target of classification rules is not Bayes error rates in the context. The\nclassifier based on the Mahalanobis distance does not always give a preferable\nperformance even if the populations are normal distributions having known\ncovariance matrices. The quadratic classifiers proposed in this paper draw\ninformation about heterogeneity effectively through both the differences of\nexpanding mean vectors and covariance matrices. We show that they hold a\nconsistency property in which misclassification rates tend to zero as the\ndimension goes to infinity under non-sparse settings. We verify that they are\nasymptotically distributed as a normal distribution under certain conditions.\nWe also propose a quadratic classifier after feature selection by using both\nthe differences of mean vectors and covariance matrices. Finally, we discuss\nperformances of the classifiers in actual data analyses. The proposed\nclassifiers achieve highly accurate classification with very low computational\ncosts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 07:35:22 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 08:12:49 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Aoshima", "Makoto", ""], ["Yata", "Kazuyoshi", ""]]}, {"id": "1503.04567", "submitter": "Hanie Sedghi", "authors": "Anima Anandkumar and Hanie Sedghi", "title": "Learning Mixed Membership Community Models in Social Tagging Networks\n  through Tensor Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in graphs has been extensively studied both in theory and\nin applications. However, detecting communities in hypergraphs is more\nchallenging. In this paper, we propose a tensor decomposition approach for\nguaranteed learning of communities in a special class of hypergraphs modeling\nsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform\nhypergraph consisting of (user, tag, resource) hyperedges. We posit a\nprobabilistic mixed membership community model, and prove that the tensor\nmethod consistently learns the communities under efficient sample complexity\nand separation requirements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 08:27:54 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 21:29:55 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Anandkumar", "Anima", ""], ["Sedghi", "Hanie", ""]]}, {"id": "1503.04585", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka", "title": "Statistical Analysis of Loopy Belief Propagation in Random Fields", "comments": null, "journal-ref": "Phys. Rev. E 92, 042120 (2015)", "doi": "10.1103/PhysRevE.92.042120", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loopy belief propagation (LBP), which is equivalent to the Bethe\napproximation in statistical mechanics, is a message-passing-type inference\nmethod that is widely used to analyze systems based on Markov random fields\n(MRFs). In this paper, we propose a message-passing-type method to analytically\nevaluate the quenched average of LBP in random fields by using the replica\ncluster variation method. The proposed analytical method is applicable to\ngeneral pair-wise MRFs with random fields whose distributions differ from each\nother and can give the quenched averages of the Bethe free energies over random\nfields, which are consistent with numerical results. The order of its\ncomputational cost is equivalent to that of standard LBP. In the latter part of\nthis paper, we describe the application of the proposed method to Bayesian\nimage restoration, in which we observed that our theoretical results are in\ngood agreement with the numerical results for natural images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:08:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 10:22:41 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 09:17:43 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Yasuda", "Muneki", ""], ["Kataoka", "Shun", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1503.05087", "submitter": "Gergely Neu", "authors": "Gergely Neu and G\\'abor Bart\\'ok", "title": "Importance weighting without importance weights: An efficient algorithm\n  for combinatorial semi-bandits", "comments": "To appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sample-efficient alternative for importance weighting for\nsituations where one only has sample access to the probability distribution\nthat generates the observations. Our new method, called Geometric Resampling\n(GR), is described and analyzed in the context of online combinatorial\noptimization under semi-bandit feedback, where a learner sequentially selects\nits actions from a combinatorial decision set so as to minimize its cumulative\nloss. In particular, we show that the well-known Follow-the-Perturbed-Leader\n(FPL) prediction method coupled with Geometric Resampling yields the first\ncomputationally efficient reduction from offline to online optimization in this\nsetting. We provide a thorough theoretical analysis for the resulting\nalgorithm, showing that its performance is on par with previous, inefficient\nsolutions. Our main contribution is showing that, despite the relatively large\nvariance induced by the GR procedure, our performance guarantees hold with high\nprobability rather than only in expectation. As a side result, we also improve\nthe best known regret bounds for FPL in online combinatorial optimization with\nfull feedback, closing the perceived performance gap between FPL and\nexponential weights in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 15:26:15 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 22:00:25 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Neu", "Gergely", ""], ["Bart\u00f3k", "G\u00e1bor", ""]]}, {"id": "1503.05160", "submitter": "Enayetur Raheem", "authors": "A. K. Md. Ehsanes Saleh and Enayetur Raheem", "title": "Improved LASSO", "comments": "17 pages, 12 figures, 24 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved LASSO estimation technique based on Stein-rule. We\nshrink classical LASSO estimator using preliminary test, shrinkage, and\npositive-rule shrinkage principle. Simulation results have been carried out for\nvarious configurations of correlation coefficients ($r$), size of the parameter\nvector ($\\beta$), error variance ($\\sigma^2$) and number of non-zero\ncoefficients ($k$) in the model parameter vector. Several real data examples\nhave been used to demonstrate the practical usefulness of the proposed\nestimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$\nStein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly\nin the divergence parameter $\\Delta^2$ as in the traditional case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 18:47:28 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Saleh", "A. K. Md. Ehsanes", ""], ["Raheem", "Enayetur", ""]]}, {"id": "1503.05459", "submitter": "Tingran Gao", "authors": "Tingran Gao", "title": "Hypoelliptic Diffusion Maps I: Tangent Bundles", "comments": "80 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a framework\ngeneralizing Diffusion Maps in the context of manifold learning and\ndimensionality reduction. Standard non-linear dimensionality reduction methods\n(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on mining\nmassive data sets using weighted affinity graphs; Orientable Diffusion Maps and\nVector Diffusion Maps enrich these graphs by attaching to each node also some\nlocal geometry. HDM likewise considers a scenario where each node possesses\nadditional structure, which is now itself of interest to investigate.\nVirtually, HDM augments the original data set with attached structures, and\nprovides tools for studying and organizing the augmented ensemble. The goal is\nto obtain information on individual structures attached to the nodes and on the\nrelationship between structures attached to nearby nodes, so as to study the\nunderlying manifold from which the nodes are sampled. In this paper, we analyze\nHDM on tangent bundles, revealing its intimate connection with sub-Riemannian\ngeometry and a family of hypoelliptic differential operators. In a later paper,\nwe shall consider more general fibre bundles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 03:36:50 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gao", "Tingran", ""]]}, {"id": "1503.05471", "submitter": "Danila Doroshin", "authors": "Danila Doroshin, Alexander Yamshinin, Nikolay Lubimov, Marina\n  Nastasenko, Mikhail Kotov, Maxim Tkachenko", "title": "Shared latent subspace modelling within Gaussian-Binary Restricted\n  Boltzmann Machines for NIST i-Vector Challenge 2014", "comments": "5 pages, 3 figures, submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to speaker subspace modelling based on\nGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is\nbased on the idea of shared factors as in the Probabilistic Linear Discriminant\nAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,\nherein the speaker factor is shared over all vectors of the speaker. Then\nMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.\nVarious new scoring techniques for speaker verification using GRBM are\nproposed. The results for NIST i-vector Challenge 2014 dataset are presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:28:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Doroshin", "Danila", ""], ["Yamshinin", "Alexander", ""], ["Lubimov", "Nikolay", ""], ["Nastasenko", "Marina", ""], ["Kotov", "Mikhail", ""], ["Tkachenko", "Maxim", ""]]}, {"id": "1503.05479", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, Ryota Tomioka", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the\n  Subspace Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a low-rank tensor from its noisy\nobservation. Previous work has shown a recovery guarantee with signal to noise\nratio $O(n^{\\lceil K/2 \\rceil /2})$ for recovering a $K$th order rank one\ntensor of size $n\\times \\cdots \\times n$ by recursive unfolding. In this paper,\nwe first improve this bound to $O(n^{K/4})$ by a much simpler approach, but\nwith a more careful analysis. Then we propose a new norm called the subspace\nnorm, which is based on the Kronecker products of factors obtained by the\nproposed simple estimator. The imposed Kronecker structure allows us to show a\nnearly ideal $O(\\sqrt{n}+\\sqrt{H^{K-1}})$ bound, in which the parameter $H$\ncontrols the blend from the non-convex estimator to mode-wise nuclear norm\nminimization. Furthermore, we empirically demonstrate that the subspace norm\nachieves the nearly ideal denoising performance even with $H=O(1)$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:45:04 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 01:44:23 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Zheng", "Qinqing", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.05509", "submitter": "Sebastien Marmin", "authors": "S\\'ebastien Marmin (I2M, IRSN, IMSV), Cl\\'ement Chevalier, David\n  Ginsbourger (IMSV)", "title": "Differentiating the multipoint Expected Improvement for optimal batch\n  design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with parallel optimization of expensive objective functions\nwhich are modeled as sample realizations of Gaussian processes. The study is\nformalized as a Bayesian optimization problem, or continuous multi-armed bandit\nproblem, where a batch of q > 0 arms is pulled in parallel at each iteration.\nSeveral algorithms have been developed for choosing batches by trading off\nexploitation and exploration. As of today, the maximum Expected Improvement\n(EI) and Upper Confidence Bound (UCB) selection rules appear as the most\nprominent approaches for batch selection. Here, we build upon recent work on\nthe multipoint Expected Improvement criterion, for which an analytic expansion\nrelying on Tallis' formula was recently established. The computational burden\nof this selection rule being still an issue in application, we derive a\nclosed-form expression for the gradient of the multipoint Expected Improvement,\nwhich aims at facilitating its maximization using gradient-based ascent\nalgorithms. Substantial computational savings are shown in application. In\naddition, our algorithms are tested numerically and compared to\nstate-of-the-art UCB-based batch-sequential algorithms. Combining starting\ndesigns relying on UCB with gradient-based EI local optimization finally\nappears as a sound option for batch design in distributed Gaussian Process\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 17:45:44 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 15:15:37 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 06:38:43 GMT"}, {"version": "v4", "created": "Mon, 2 Sep 2019 12:56:21 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Marmin", "S\u00e9bastien", "", "I2M, IRSN, IMSV"], ["Chevalier", "Cl\u00e9ment", "", "IMSV"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1503.05526", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Interpretable Aircraft Engine Diagnostic via Expert Indicator\n  Aggregation", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.6214,\n  arXiv:1409.4747, arXiv:1407.0880", "journal-ref": "Transactions on Machine Learning and Data Mining, 2014, 7 (2),\n  pp.39-64", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting early signs of failures (anomalies) in complex systems is one of\nthe main goal of preventive maintenance. It allows in particular to avoid\nactual failures by (re)scheduling maintenance operations in a way that\noptimizes maintenance costs. Aircraft engine health monitoring is one\nrepresentative example of a field in which anomaly detection is crucial.\nManufacturers collect large amount of engine related data during flights which\nare used, among other applications, to detect anomalies. This article\nintroduces and studies a generic methodology that allows one to build automatic\nearly signs of anomaly detection in a way that builds upon human expertise and\nthat remains understandable by human operators who make the final maintenance\ndecision. The main idea of the method is to generate a very large number of\nbinary indicators based on parametric anomaly scores designed by experts,\ncomplemented by simple aggregations of those scores. A feature selection method\nis used to keep only the most discriminant indicators which are used as inputs\nof a Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:30:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1503.05567", "submitter": "Yan Li", "authors": "Yan Li, Han Liu, Warren Powell", "title": "The Knowledge Gradient Policy Using A Sparse Additive Belief Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential learning policy for noisy discrete global\noptimization and ranking and selection (R\\&S) problems with high dimensional\nsparse belief functions, where there are hundreds or even thousands of\nfeatures, but only a small portion of these features contain explanatory power.\nWe aim to identify the sparsity pattern and select the best alternative before\nthe finite budget is exhausted. We derive a knowledge gradient policy for\nsparse linear models (KGSpLin) with group Lasso penalty. This policy is a\nunique and novel hybrid of Bayesian R\\&S with frequentist learning.\nParticularly, our method naturally combines B-spline basis expansion and\ngeneralizes to the nonparametric additive model (KGSpAM) and functional ANOVA\nmodel. Theoretically, we provide the estimation error bounds of the posterior\nmean estimate and the functional estimate. Controlled experiments show that the\nalgorithm efficiently learns the correct set of nonzero parameters even when\nthe model is imbedded with hundreds of dummy parameters. Also it outperforms\nthe knowledge gradient for a linear model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 20:01:22 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Li", "Yan", ""], ["Liu", "Han", ""], ["Powell", "Warren", ""]]}, {"id": "1503.05671", "submitter": "James Martens", "authors": "James Martens, Roger Grosse", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "comments": "Reduction ratio formula corrected. Removed incorrect claim about\n  geodesics in footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method for approximating natural gradient descent in\nneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).\nK-FAC is based on an efficiently invertible approximation of a neural network's\nFisher information matrix which is neither diagonal nor low-rank, and in some\ncases is completely non-sparse. It is derived by approximating various large\nblocks of the Fisher (corresponding to entire layers) as being the Kronecker\nproduct of two much smaller matrices. While only several times more expensive\nto compute than the plain stochastic gradient, the updates produced by K-FAC\nmake much more progress optimizing the objective, which results in an algorithm\nthat can be much faster than stochastic gradient descent with momentum in\npractice. And unlike some previously proposed approximate\nnatural-gradient/Newton methods which use high-quality non-diagonal curvature\nmatrices (such as Hessian-free optimization), K-FAC works very well in highly\nstochastic optimization regimes. This is because the cost of storing and\ninverting K-FAC's approximation to the curvature matrix does not depend on the\namount of data used to estimate it, which is a feature typically associated\nonly with diagonal or low-rank approximations to the curvature matrix.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 08:30:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 20:19:14 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 05:48:59 GMT"}, {"version": "v4", "created": "Thu, 21 May 2015 00:25:06 GMT"}, {"version": "v5", "created": "Fri, 24 Jul 2015 02:30:35 GMT"}, {"version": "v6", "created": "Wed, 4 May 2016 00:29:33 GMT"}, {"version": "v7", "created": "Mon, 8 Jun 2020 01:28:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Martens", "James", ""], ["Grosse", "Roger", ""]]}, {"id": "1503.05684", "submitter": "Ond\\v{r}ej Tich\\'y", "authors": "Ond\\v{r}ej Tich\\'y, V\\'aclav \\v{S}m\\'idl", "title": "Non-parametric Bayesian Models of Response Function in Dynamic Image\n  Sequences", "comments": "19 pages, Preprint submitted to Elsevier", "journal-ref": "Computer Vision and Image Understanding, Volume 151, Pages 90-100,\n  ISSN 1077-3142 (2016)", "doi": "10.1016/j.cviu.2015.11.010", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of response functions is an important task in dynamic medical\nimaging. This task arises for example in dynamic renal scintigraphy, where\nimpulse response or retention functions are estimated, or in functional\nmagnetic resonance imaging where hemodynamic response functions are required.\nThese functions can not be observed directly and their estimation is\ncomplicated because the recorded images are subject to superposition of\nunderlying signals. Therefore, the response functions are estimated via blind\nsource separation and deconvolution. Performance of this algorithm heavily\ndepends on the used models of the response functions. Response functions in\nreal image sequences are rather complicated and finding a suitable parametric\nform is problematic. In this paper, we study estimation of the response\nfunctions using non-parametric Bayesian priors. These priors were designed to\nfavor desirable properties of the functions, such as sparsity or smoothness.\nThese assumptions are used within hierarchical priors of the blind source\nseparation and deconvolution algorithm. Comparison of the resulting algorithms\nwith these priors is performed on synthetic dataset as well as on real datasets\nfrom dynamic renal scintigraphy. It is shown that flexible non-parametric\npriors improve estimation of response functions in both cases. MATLAB\nimplementation of the resulting algorithms is freely available for download.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 09:53:34 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Tich\u00fd", "Ond\u0159ej", ""], ["\u0160m\u00eddl", "V\u00e1clav", ""]]}, {"id": "1503.05724", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Patrick van der Smagt", "title": "A Neural Transfer Function for a Smooth and Differentiable Transition\n  Between Additive and Multiplicative Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. This leads either to an\ninefficient distribution of computational resources or an extensive increase in\nthe computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 11:48:14 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 18:01:52 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 14:45:03 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1503.05743", "submitter": "Ken Miura", "authors": "Ken Miura and Tatsuya Harada", "title": "Implementation of a Practical Distributed Calculation System with\n  Browsers and JavaScript, and Application to Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning can achieve outstanding results in various fields. However, it\nrequires so significant computational power that graphics processing units\n(GPUs) and/or numerous computers are often required for the practical\napplication. We have developed a new distributed calculation framework called\n\"Sashimi\" that allows any computer to be used as a distribution node only by\naccessing a website. We have also developed a new JavaScript neural network\nframework called \"Sukiyaki\" that uses general purpose GPUs with web browsers.\nSukiyaki performs 30 times faster than a conventional JavaScript library for\ndeep convolutional neural networks (deep CNNs) learning. The combination of\nSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the\ndistributed deep learning of deep CNNs only with web browsers on various\ndevices. The libraries that comprise the proposed methods are available under\nMIT license at http://mil-tokyo.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 12:41:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Miura", "Ken", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1503.06058", "submitter": "Johan Dahlin", "authors": "Thomas B. Sch\\\"on, Fredrik Lindsten, Johan Dahlin, Johan W{\\aa}gberg,\n  Christian A. Naesseth, Andreas Svensson and Liang Dai", "title": "Sequential Monte Carlo Methods for System Identification", "comments": "In proceedings of the 17th IFAC Symposium on System Identification\n  (SYSID). Added cover page", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.224", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in identifying nonlinear and possibly non-Gaussian\nstate space models (SSMs) is the intractability of estimating the system state.\nSequential Monte Carlo (SMC) methods, such as the particle filter (introduced\nmore than two decades ago), provide numerical solutions to the nonlinear state\nestimation problems arising in SSMs. When combined with additional\nidentification techniques, these algorithms provide solid solutions to the\nnonlinear system identification problem. We describe two general strategies for\ncreating such combinations and discuss why SMC is a natural tool for\nimplementing these strategies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 13:06:38 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 21:22:57 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 14:44:32 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""], ["Dahlin", "Johan", ""], ["W\u00e5gberg", "Johan", ""], ["Naesseth", "Christian A.", ""], ["Svensson", "Andreas", ""], ["Dai", "Liang", ""]]}, {"id": "1503.06060", "submitter": "Dominique Gay", "authors": "Romain Guigour\\`es, Dominique Gay, Marc Boull\\'e, Fabrice Cl\\'erot,\n  Fabrice Rossi", "title": "Country-scale Exploratory Analysis of Call Detail Records through the\n  Lens of Data Grid Models", "comments": "Submitted to Industrial Track of ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Call Detail Records (CDRs) are data recorded by telecommunications companies,\nconsisting of basic informations related to several dimensions of the calls\nmade through the network: the source, destination, date and time of calls. CDRs\ndata analysis has received much attention in the recent years since it might\nreveal valuable information about human behavior. It has shown high added value\nin many application domains like e.g., communities analysis or network\nplanning. In this paper, we suggest a generic methodology for summarizing\ninformation contained in CDRs data. The method is based on a parameter-free\nestimation of the joint distribution of the variables that describe the calls.\nWe also suggest several well-founded criteria that allows one to browse the\nsummary at various granularities and to explore the summary by means of\ninsightful visualizations. The method handles network graph data, temporal\nsequence data as well as user mobility data stemming from original CDRs data.\nWe show the relevance of our methodology for various case studies on real-world\nCDRs data from Ivory Coast.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 13:13:42 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Guigour\u00e8s", "Romain", ""], ["Gay", "Dominique", ""], ["Boull\u00e9", "Marc", ""], ["Cl\u00e9rot", "Fabrice", ""], ["Rossi", "Fabrice", ""]]}, {"id": "1503.06134", "submitter": "Bahman Yari Saeed Khanloo", "authors": "Bahman Yari Saeed Khanloo", "title": "A Bennett Inequality for the Missing Mass", "comments": "It is not possible to derive a Bennett inequality using this approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel concentration inequalities are obtained for the missing mass, i.e. the\ntotal probability mass of the outcomes not observed in the sample. We derive\ndistribution-free deviation bounds with sublinear exponents in deviation size\nfor missing mass and improve the results of Berend and Kontorovich (2013) and\nYari Saeed Khanloo and Haffari (2015) for small deviations which is the most\nimportant case in learning theory.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 16:08:56 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 01:23:45 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Khanloo", "Bahman Yari Saeed", ""]]}, {"id": "1503.06236", "submitter": "Rahul Agarwal", "authors": "Rahul Agarwal, Zhe Chen, Sridevi V. Sarma", "title": "Nonparametric Estimation of Band-limited Probability Density Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonparametric maximum likelihood (ML) estimator for\nband-limited (BL) probability density functions (pdfs) is proposed. The BLML\nestimator is consistent and computationally efficient. To compute the BLML\nestimator, three approximate algorithms are presented: a binary quadratic\nprogramming (BQP) algorithm for medium scale problems, a Trivial algorithm for\nlarge-scale problems that yields a consistent estimate if the underlying pdf is\nstrictly positive and BL, and a fast implementation of the Trivial algorithm\nthat exploits the band-limited assumption and the Nyquist sampling theorem\n(\"BLMLQuick\"). All three BLML estimators outperform kernel density estimation\n(KDE) algorithms (adaptive and higher order KDEs) with respect to the mean\nintegrated squared error for data generated from both BL and infinite-band\npdfs. Further, the BLMLQuick estimate is remarkably faster than the KD\nalgorithms. Finally, the BLML method is applied to estimate the conditional\nintensity function of a neuronal spike train (point process) recorded from a\nrat's entorhinal cortex grid cell, for which it outperforms state-of-the-art\nestimators used in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 21:51:39 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 21:39:46 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 21:08:39 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2015 01:40:31 GMT"}, {"version": "v5", "created": "Mon, 29 Jun 2015 01:32:21 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Agarwal", "Rahul", ""], ["Chen", "Zhe", ""], ["Sarma", "Sridevi V.", ""]]}, {"id": "1503.06239", "submitter": "Jinye Zhang", "authors": "Jinye Zhang, Zhijian Ou", "title": "Block-Wise MAP Inference for Determinantal Point Processes with\n  Application to Change-Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Existing MAP inference algorithms for determinantal point processes (DPPs)\nneed to calculate determinants or conduct eigenvalue decomposition generally at\nthe scale of the full kernel, which presents a great challenge for real-world\napplications. In this paper, we introduce a class of DPPs, called BwDPPs, that\nare characterized by an almost block diagonal kernel matrix and thus can allow\nefficient block-wise MAP inference. Furthermore, BwDPPs are successfully\napplied to address the difficulty of selecting change-points in the problem of\nchange-point detection (CPD), which results in a new BwDPP-based CPD method,\nnamed BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is\nfirst created based on existing well-studied metrics. Then, these change-point\ncandidates are treated as DPP items, and DPP-based subset selection is\nconducted to give the final estimate of the change-points that favours both\nquality and diversity. The effectiveness of BwDppCpd is demonstrated through\nextensive experiments on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 22:01:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Jinye", ""], ["Ou", "Zhijian", ""]]}, {"id": "1503.06250", "submitter": "Ilya Safro", "authors": "Talayeh Razzaghi and Oleg Roderick and Ilya Safro and Nick Marko", "title": "Fast Imbalanced Classification of Healthcare Data with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical domain, data features often contain missing values. This can\ncreate serious bias in the predictive modeling. Typical standard data mining\nmethods often produce poor performance measures. In this paper, we propose a\nnew method to simultaneously classify large datasets and reduce the effects of\nmissing values. The proposed method is based on a multilevel framework of the\ncost-sensitive SVM and the expected maximization imputation method for missing\nvalues, which relies on iterated regression analyses. We compare classification\nresults of multilevel SVM-based algorithms on public benchmark datasets with\nimbalanced classes and missing values as well as real data in health\napplications, and show that our multilevel SVM-based method produces fast, and\nmore accurate and robust classification results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 00:13:54 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Roderick", "Oleg", ""], ["Safro", "Ilya", ""], ["Marko", "Nick", ""]]}, {"id": "1503.06267", "submitter": "Yong Huang", "authors": "Yong Huang and James L. Beck", "title": "Hierarchical sparse Bayesian learning: theory and application for\n  inferring structural damage from incomplete modal data", "comments": "41 pages, 8 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1408.3685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural damage due to excessive loading or environmental degradation\ntypically occurs in localized areas in the absence of collapse. This prior\ninformation about the spatial sparseness of structural damage is exploited here\nby a hierarchical sparse Bayesian learning framework with the goal of reducing\nthe source of ill-conditioning in the stiffness loss inversion problem for\ndamage detection. Sparse Bayesian learning methodologies automatically prune\naway irrelevant or inactive features from a set of potential candidates, and so\nthey are effective probabilistic tools for producing sparse explanatory\nsubsets. We have previously proposed such an approach to establish the\nprobability of localized stiffness reductions that serve as a proxy for damage\nby using noisy incomplete modal data from before and after possible damage. The\ncore idea centers on a specific hierarchical Bayesian model that promotes\nspatial sparseness in the inferred stiffness reductions in a way that is\nconsistent with the Bayesian Ockham razor. In this paper, we improve the theory\nof our previously proposed sparse Bayesian learning approach by eliminating an\napproximation and, more importantly, incorporating a constraint on stiffness\nincreases. Our approach has many appealing features that are summarized at the\nend of the paper. We validate the approach by applying it to the Phase II\nsimulated and experimental benchmark studies sponsored by the IASC-ASCE Task\nGroup on Structural Health Monitoring. The results show that it can reliably\ndetect, locate and assess damage by inferring substructure stiffness losses\nfrom the identified modal parameters. The occurrence of missed and false damage\nalerts is effectively suppressed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 05:45:16 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""]]}, {"id": "1503.06379", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu", "title": "Relaxed Leverage Sampling for Low-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exact recovery of any $m\\times n$ matrix of rank\n$\\varrho$ from a small number of observed entries via the standard nuclear norm\nminimization framework. Such low-rank matrices have degrees of freedom\n$(m+n)\\varrho - \\varrho^2$. We show that any arbitrary low-rank matrices can be\nrecovered exactly from a $\\Theta\\left(((m+n)\\varrho -\n\\varrho^2)\\log^2(m+n)\\right)$ randomly sampled entries, thus matching the lower\nbound on the required number of entries (in terms of degrees of freedom), with\nan additional factor of $O(\\log^2(m+n))$. To achieve this bound on sample size\nwe observe each entry with probabilities proportional to the sum of\ncorresponding row and column leverage scores, minus their product. We show that\nthis relaxation in sampling probabilities (as opposed to sum of leverage scores\nin Chen et al, 2014) can give us an $O(\\varrho^2\\log^2(m+n))$ additive\nimprovement on the (best known) sample size obtained by Chen et al, 2014, for\nthe nuclear norm minimization. Experiments on real data corroborate the\ntheoretical improvement on sample size. Further, exact recovery of $(a)$\nincoherent matrices (with restricted leverage scores), and $(b)$ matrices with\nonly one of the row or column spaces to be incoherent, can be performed using\nour relaxed leverage score sampling, via nuclear norm minimization, without\nknowing the leverage scores a priori. In such settings also we can achieve\nimprovement on sample size.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 03:27:15 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 16:20:52 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 08:13:52 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Kundu", "Abhisek", ""]]}, {"id": "1503.06388", "submitter": "Stefan Wager", "authors": "Stefan Wager and Guenther Walther", "title": "Adaptive Concentration of Regression Trees, with Application to Random\n  Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence of the predictive surface of regression trees and\nforests. To support our analysis we introduce a notion of adaptive\nconcentration for regression trees. This approach breaks tree training into a\nmodel selection phase in which we pick the tree splits, followed by a model\nfitting phase where we find the best regression model consistent with these\nsplits. We then show that the fitted regression tree concentrates around the\noptimal predictor with the same splits: as d and n get large, the discrepancy\nis with high probability bounded on the order of sqrt(log(d) log(n)/k)\nuniformly over the whole regression surface, where d is the dimension of the\nfeature space, n is the number of training examples, and k is the minimum leaf\nsize for each tree. We also provide rate-matching lower bounds for this\nadaptive concentration statement. From a practical perspective, our result\nenables us to prove consistency results for adaptively grown forests in high\ndimensions, and to carry out valid post-selection inference in the sense of\nBerk et al. [2013] for subgroups defined by tree leaves.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 06:07:14 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 02:15:16 GMT"}, {"version": "v3", "created": "Sat, 30 Apr 2016 22:32:52 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Wager", "Stefan", ""], ["Walther", "Guenther", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06429", "submitter": "Conrado Miranda", "authors": "Conrado S. Miranda and Fernando J. Von Zuben", "title": "Asymmetric Distributions from Constrained Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces constrained mixtures for continuous distributions,\ncharacterized by a mixture of distributions where each distribution has a shape\nsimilar to the base distribution and disjoint domains. This new concept is used\nto create generalized asymmetric versions of the Laplace and normal\ndistributions, which are shown to define exponential families, with known\nconjugate priors, and to have maximum likelihood estimates for the original\nparameters, with known closed-form expressions. The asymmetric and symmetric\nnormal distributions are compared in a linear regression example, showing that\nthe asymmetric version performs at least as well as the symmetric one, and in a\nreal world time-series problem, where a hidden Markov model is used to fit a\nstock index, indicating that the asymmetric version provides higher likelihood\nand may learn distribution models over states and transition distributions with\nconsiderably less entropy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 13:55:10 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Miranda", "Conrado S.", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1503.06432", "submitter": "Mauricio A. \\'Alvarez", "authors": "Cristian Guarnizo, Mauricio A. \\'Alvarez", "title": "Indian Buffet process for model selection in convolved multiple-output\n  Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output Gaussian processes have received increasing attention during the\nlast few years as a natural mechanism to extend the powerful flexibility of\nGaussian processes to the setup of multiple output variables. The key point\nhere is the ability to design kernel functions that allow exploiting the\ncorrelations between the outputs while fulfilling the positive definiteness\nrequisite for the covariance function. Alternatives to construct these\ncovariance functions are the linear model of coregionalization and process\nconvolutions. Each of these methods demand the specification of the number of\nlatent Gaussian process used to build the covariance function for the outputs.\nWe propose in this paper, the use of an Indian Buffet process as a way to\nperform model selection over the number of latent Gaussian processes. This type\nof model is particularly important in the context of latent force models, where\nthe latent forces are associated to physical quantities like protein profiles\nor latent forces in mechanical systems. We use variational inference to\nestimate posterior distributions over the variables involved, and show examples\nof the model performance over artificial data, a motion capture dataset, and a\ngene expression dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 14:15:04 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Guarnizo", "Cristian", ""], ["\u00c1lvarez", "Mauricio A.", ""]]}, {"id": "1503.06452", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Unsupervised model compression for multilayer bootstrap networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multilayer bootstrap network (MBN) has demonstrated promising\nperformance in unsupervised dimensionality reduction. It can learn compact\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\npropose an unsupervised model compression framework for this general problem of\nunsupervised bootstrap methods. The framework compresses a large unsupervised\nbootstrap model into a small model by taking the bootstrap model and its\napplication together as a black box and learning a mapping function from the\ninput of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\nneural network (DNN) as the supervised learner. Our initial result on MNIST\nshowed that compressive MBN not only maintains the high prediction accuracy of\nMBN but also is over thousands of times faster than MBN at the prediction\nstage. Our result suggests that the new technique integrates the effectiveness\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\nsupervised learning together for the effectiveness and efficiency of\ncompressive MBN on unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 18:22:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1503.06567", "submitter": "Andrej Risteski", "authors": "Pranjal Awasthi and Andrej Risteski", "title": "On some provably correct cases of variational inference for topic models", "comments": "46 pages, Compared to previous version: clarified notation, a number\n  of typos fixed throughout paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a very efficient and popular heuristic used in\nvarious forms in the context of latent variable models. It's closely related to\nExpectation Maximization (EM), and is applied when exact EM is computationally\ninfeasible. Despite being immensely popular, current theoretical understanding\nof the effectiveness of variaitonal inference based algorithms is very limited.\nIn this work we provide the first analysis of instances where variational\ninference algorithms converge to the global optimum, in the setting of topic\nmodels.\n  More specifically, we show that variational inference provably learns the\noptimal parameters of a topic model under natural assumptions on the topic-word\nmatrix and the topic priors. The properties that the topic word matrix must\nsatisfy in our setting are related to the topic expansion assumption introduced\nin (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora\net al., 2012c). The assumptions on the topic priors are related to the well\nknown Dirichlet prior, introduced to the area of topic modeling by (Blei et\nal., 2003).\n  It is well known that initialization plays a crucial role in how well\nvariational based algorithms perform in practice. The initializations that we\nuse are fairly natural. One of them is similar to what is currently used in\nLDA-c, the most popular implementation of variational inference for topic\nmodels. The other one is an overlapping clustering algorithm, inspired by a\nwork by (Arora et al., 2014) on dictionary learning, which is very simple and\nefficient.\n  While our primary goal is to provide insights into when variational inference\nmight work in practice, the multiplicative, rather than the additive nature of\nthe variational inference updates forces us to use fairly non-standard proof\narguments, which we believe will be of general interest.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:20:39 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 11:24:43 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Risteski", "Andrej", ""]]}, {"id": "1503.06910", "submitter": "Enayetur Raheem", "authors": "Enayetur Raheem, A. K. Md. Ehsanes Saleh", "title": "Penalty, Shrinkage, and Preliminary Test Estimators under Full Model\n  Hypothesis", "comments": "28 pages, 4 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:1503.05160", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multiple regression model and compares, under full\nmodel hypothesis, analytically as well as by simulation, the performance\ncharacteristics of some popular penalty estimators such as ridge regression,\nLASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,\nrestricted estimator, preliminary test estimator, and Stein-type estimators\nwhen the dimension of the parameter space is smaller than the sample space\ndimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE while\nLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it is\nobserved that neither penalty estimators nor Stein-type estimator dominate one\nanother.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 04:40:53 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Raheem", "Enayetur", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1503.06944", "submitter": "Emilie Morvant", "authors": "Pascal Germain (SIERRA), Amaury Habrard (LHC), Fran\\c{c}ois\n  Laviolette, Emilie Morvant (LHC)", "title": "PAC-Bayesian Theorems for Domain Adaptation with Specialization to\n  Linear Classifiers", "comments": "This report is a long version of our paper entitled A PAC-Bayesian\n  Approach for Domain Adaptation with Specialization to Linear Classifiers\n  published in the proceedings of the International Conference on Machine\n  Learning (ICML) 2013. We improved our main results, extended our experiments,\n  and proposed an extension to multisource domain adaptation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide two main contributions in PAC-Bayesian theory for\ndomain adaptation where the objective is to learn, from a source distribution,\na well-performing majority vote on a different target distribution. On the one\nhand, we propose an improvement of the previous approach proposed by Germain et\nal. (2013), that relies on a novel distribution pseudodistance based on a\ndisagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain\nadaptation bound for the stochastic Gibbs classifier. We specialize it to\nlinear classifiers, and design a learning algorithm which shows interesting\nresults on a synthetic problem and on a popular sentiment annotation task. On\nthe other hand, we generalize these results to multisource domain adaptation\nallowing us to take into account different source domains. This study opens the\ndoor to tackle domain adaptation tasks by making use of all the PAC-Bayesian\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 08:17:44 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 12:10:09 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 08:34:04 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Germain", "Pascal", "", "SIERRA"], ["Habrard", "Amaury", "", "LHC"], ["Laviolette", "Fran\u00e7ois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "1503.07077", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Kyle W. Willett, Joni Dambre", "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction", "comments": "Accepted for publication in MNRAS. 20 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/stv632", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 15:34:06 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Dieleman", "Sander", ""], ["Willett", "Kyle W.", ""], ["Dambre", "Joni", ""]]}, {"id": "1503.07211", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Universal Approximation of Markov Kernels by Shallow Stochastic\n  Feedforward Networks", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish upper bounds for the minimal number of hidden units for which a\nbinary stochastic feedforward network with sigmoid activation probabilities and\na single hidden layer is a universal approximator of Markov kernels. We show\nthat each possible probabilistic assignment of the states of $n$ output units,\ngiven the states of $k\\geq1$ input units, can be approximated arbitrarily well\nby a network with $2^{k-1}(2^{n-1}-1)$ hidden units.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 21:38:59 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1503.07240", "submitter": "Dengyong  Zhou", "authors": "Dengyong Zhou, Qiang Liu, John C. Platt, Christopher Meek, Nihar B.\n  Shah", "title": "Regularized Minimax Conditional Entropy for Crowdsourcing", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rapidly increasing interest in crowdsourcing for data labeling. By\ncrowdsourcing, a large number of labels can be often quickly gathered at low\ncost. However, the labels provided by the crowdsourcing workers are usually not\nof high quality. In this paper, we propose a minimax conditional entropy\nprinciple to infer ground truth from noisy crowdsourced labels. Under this\nprinciple, we derive a unique probabilistic labeling model jointly\nparameterized by worker ability and item difficulty. We also propose an\nobjective measurement principle, and show that our method is the only method\nwhich satisfies this objective measurement principle. We validate our method\nthrough a variety of real crowdsourcing datasets with binary, multiclass or\nordinal labels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 00:10:11 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Zhou", "Dengyong", ""], ["Liu", "Qiang", ""], ["Platt", "John C.", ""], ["Meek", "Christopher", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1503.07340", "submitter": "Mattia  Zorzi", "authors": "Mattia Zorzi and Alessandro Chiuso", "title": "A Bayesian Approach to Sparse plus Low rank Network Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling multivariate time series with\nparsimonious dynamical models which can be represented as sparse dynamic\nBayesian networks with few latent nodes. This structure translates into a\nsparse plus low rank model. In this paper, we propose a Gaussian regression\napproach to identify such a model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 11:32:03 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 10:57:20 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zorzi", "Mattia", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1503.07368", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu and John Lafferty", "title": "Quantized Nonparametric Estimation over Sobolev Ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the notion of minimax estimation under storage or communication\nconstraints, and prove an extension to Pinsker's theorem for nonparametric\nestimation over Sobolev ellipsoids. Placing limits on the number of bits used\nto encode any estimator, we give tight lower and upper bounds on the excess\nrisk due to quantization in terms of the number of bits, the signal size, and\nthe noise level. This establishes the Pareto optimal tradeoff between storage\nand risk under quantization constraints for Sobolev spaces. Our results and\nproof techniques combine elements of rate distortion theory and minimax\nanalysis. The proposed quantized estimation scheme, which shows achievability\nof the lower bounds, is adaptive in the usual statistical sense, achieving the\noptimal quantized minimax rate without knowledge of the smoothness parameter of\nthe Sobolev space. It is also adaptive in a computational sense, as it\nconstructs the code only after observing the data, to dynamically allocate more\ncodewords to blocks where the estimated signal size is large. Simulations are\nincluded that illustrate the effect of quantization on statistical risk.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 13:08:49 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 15:03:31 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 22:42:30 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Lafferty", "John", ""]]}, {"id": "1503.07508", "submitter": "Bo Xin", "authors": "Bo Xin, Lingjing Hu, Yizhou Wang and Wen Gao", "title": "Stable Feature Selection from Brain sMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimage analysis usually involves learning thousands or even millions of\nvariables using only a limited number of samples. In this regard, sparse\nmodels, e.g. the lasso, are applied to select the optimal features and achieve\nhigh diagnosis accuracy. The lasso, however, usually results in independent\nunstable features. Stability, a manifest of reproducibility of statistical\nresults subject to reasonable perturbations to data and the model, is an\nimportant focus in statistics, especially in the analysis of high dimensional\ndata. In this paper, we explore a nonnegative generalized fused lasso model for\nstable feature selection in the diagnosis of Alzheimer's disease. In addition\nto sparsity, our model incorporates two important pathological priors: the\nspatial cohesion of lesion voxels and the positive correlation between the\nfeatures and the disease labels. To optimize the model, we propose an efficient\nalgorithm by proving a novel link between total variation and fast network flow\nalgorithms via conic duality. Experiments show that the proposed nonnegative\nmodel performs much better in exploring the intrinsic structure of data via\nselecting stable features compared with other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 19:30:14 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Xin", "Bo", ""], ["Hu", "Lingjing", ""], ["Wang", "Yizhou", ""], ["Gao", "Wen", ""]]}, {"id": "1503.07689", "submitter": "Jean-Michel Marin", "authors": "Jean-Michel Marin (U. Montpellier), Pierre Pudlo (Aix-Marseille U.),\n  Arnaud Estoup (CBGP, INRA, Montpellier) and Christian P. Robert (U.\n  Paris-Dauphine and U. Warwick)", "title": "Likelihood-free Model Choice", "comments": "21 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an invited chapter covering the specificities of ABC model\nchoice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont\n(2017). Beyond exposing the potential pitfalls of ABC based posterior\nprobabilities, the review emphasizes mostly the solution proposed by Pudlo et\nal. (2016) on the use of random forests for aggregating summary statistics and\nand for estimating the posterior probability of the most likely model via a\nsecondary random fores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:17:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:02:41 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 09:13:14 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Marin", "Jean-Michel", "", "U. Montpellier"], ["Pudlo", "Pierre", "", "Aix-Marseille U."], ["Estoup", "Arnaud", "", "CBGP, INRA, Montpellier"], ["Robert", "Christian P.", "", "U.\n  Paris-Dauphine and U. Warwick"]]}, {"id": "1503.07810", "submitter": "Jiaming Zeng", "authors": "Jiaming Zeng, Berk Ustun, Cynthia Rudin", "title": "Interpretable Classification Models for Recidivism Prediction", "comments": "45 pages, 17 figures", "journal-ref": "Journal of Royal Statistics - Series A (2017)", "doi": "10.1111/rssa.12227", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a long-debated question, which is how to create predictive\nmodels of recidivism that are sufficiently accurate, transparent, and\ninterpretable to use for decision-making. This question is complicated as these\nmodels are used to support different decisions, from sentencing, to determining\nrelease on probation, to allocating preventative social services. Each use case\nmight have an objective other than classification accuracy, such as a desired\ntrue positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is\na point on the receiver operator characteristic (ROC) curve. We use popular\nmachine learning methods to create models along the full ROC curve on a wide\nrange of recidivism prediction problems. We show that many methods (SVM, Ridge\nRegression) produce equally accurate models along the full ROC curve. However,\nmethods that designed for interpretability (CART, C5.0) cannot be tuned to\nproduce models that are accurate and/or interpretable. To handle this\nshortcoming, we use a new method known as SLIM (Supersparse Linear Integer\nModels) to produce accurate, transparent, and interpretable models along the\nfull ROC curve. These models can be used for decision-making for many different\nuse cases, since they are just as accurate as the most powerful black-box\nmachine learning models, but completely transparent, and highly interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 18:21:29 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 04:32:31 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 01:09:31 GMT"}, {"version": "v4", "created": "Fri, 6 May 2016 14:50:11 GMT"}, {"version": "v5", "created": "Fri, 10 Jun 2016 02:05:32 GMT"}, {"version": "v6", "created": "Fri, 8 Jul 2016 01:22:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zeng", "Jiaming", ""], ["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1503.07906", "submitter": "Gang Chen", "authors": "Gang Chen and Sargur N. Srihari", "title": "Generalized K-fan Multimodal Deep Model with Shared Representations", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Multimodal learning with deep Boltzmann machines (DBMs) is an generative\napproach to fuse multimodal inputs, and can learn the shared representation via\nContrastive Divergence (CD) for classification and information retrieval tasks.\nHowever, it is a 2-fan DBM model, and cannot effectively handle multiple\nprediction tasks. Moreover, this model cannot recover the hidden\nrepresentations well by sampling from the conditional distribution when more\nthan one modalities are missing. In this paper, we propose a K-fan deep\nstructure model, which can handle the multi-input and muti-output learning\nproblems effectively. In particular, the deep structure has K-branch for\ndifferent inputs where each branch can be composed of a multi-layer deep model,\nand a shared representation is learned in an discriminative manner to tackle\nmultimodal tasks. Given the deep structure, we propose two objective functions\nto handle two multi-input and multi-output tasks: joint visual restoration and\nlabeling, and the multi-view multi-calss object recognition tasks. To estimate\nthe model parameters, we initialize the deep model parameters with CD to\nmaximize the joint distribution, and then we use backpropagation to update the\nmodel according to specific objective function. The experimental results\ndemonstrate that the model can effectively leverages multi-source information\nand predict multiple tasks well over competitive baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 21:17:46 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Chen", "Gang", ""], ["Srihari", "Sargur N.", ""]]}, {"id": "1503.07970", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Bayesian Cross Validation and WAIC for Predictive Prior Design in\n  Regular Asymptotic Theory", "comments": "33 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior design is one of the most important problems in both statistics and\nmachine learning. The cross validation (CV) and the widely applicable\ninformation criterion (WAIC) are predictive measures of the Bayesian\nestimation, however, it has been difficult to apply them to find the optimal\nprior because their mathematical properties in prior evaluation have been\nunknown and the region of the hyperparameters is too wide to be examined. In\nthis paper, we derive a new formula by which the theoretical relation among CV,\nWAIC, and the generalization loss is clarified and the optimal hyperparameter\ncan be directly found.\n  By the formula, three facts are clarified about predictive prior design.\nFirstly, CV and WAIC have the same second order asymptotic expansion, hence\nthey are asymptotically equivalent to each other as the optimizer of the\nhyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes\nthe average generalization loss to be minimized asymptotically but does not the\nrandom generalization loss. And lastly, by using the mathematical relation\nbetween priors, the variances of the optimized hyperparameters by CV and WAIC\nare made smaller with small computational costs. Also we show that the\noptimized hyperparameter by DIC or the marginal likelihood does not minimize\nthe average or random generalization loss in general.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 06:21:06 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "1503.07990", "submitter": "Anders Ellern Bilgrau", "authors": "Anders Ellern Bilgrau, Rasmus Froberg Br{\\o}ndum, Poul Svante Eriksen,\n  Karen Dybk{\\ae}r, and Martin B{\\o}gsted", "title": "Estimating a common covariance matrix for network meta-analysis of gene\n  expression datasets in diffuse large B-cell lymphoma", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of covariance matrices of gene expressions has many\napplications in cancer systems biology. Many gene expression studies, however,\nare hampered by low sample size and it has therefore become popular to increase\nsample size by collecting gene expression data across studies. Motivated by the\ntraditional meta-analysis using random effects models, we present a\nhierarchical random covariance model and use it for the meta-analysis of gene\ncorrelation networks across 11 large-scale gene expression studies of diffuse\nlarge B-cell lymphoma (DLBCL). We suggest to use a maximum likelihood estimator\nfor the underlying common covariance matrix and introduce an EM algorithm for\nestimation. By simulation experiments comparing the estimated covariance\nmatrices by cophenetic correlation and Kullback-Leibler divergence the\nsuggested estimator showed to perform better or not worse than a simple pooled\nestimator. In a posthoc analysis of the estimated common covariance matrix for\nthe DLBCL data we were able to identify novel biologically meaningful gene\ncorrelation networks with eigengenes of prognostic value. In conclusion, the\nmethod seems to provide a generally applicable framework for meta-analysis,\nwhen multiple features are measured and believed to share a common covariance\nmatrix obscured by study dependent noise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 08:43:50 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 07:13:57 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 20:34:51 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Bilgrau", "Anders Ellern", ""], ["Br\u00f8ndum", "Rasmus Froberg", ""], ["Eriksen", "Poul Svante", ""], ["Dybk\u00e6r", "Karen", ""], ["B\u00f8gsted", "Martin", ""]]}, {"id": "1503.08195", "submitter": "Tilmann Gneiting", "authors": "Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Kr\\\"uger", "title": "Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n  Representations, and Forecast Rankings", "comments": "References updated; a few minor edits in response to initial comments\n  (merely for clarity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the practice of point prediction, it is desirable that forecasters receive\na directive in the form of a statistical functional, such as the mean or a\nquantile of the predictive distribution. When evaluating and comparing\ncompeting forecasts, it is then critical that the scoring function used for\nthese purposes be consistent for the functional at hand, in the sense that the\nexpected score is minimized when following the directive.\n  We show that any scoring function that is consistent for a quantile or an\nexpectile functional, respectively, can be represented as a mixture of extremal\nscoring functions that form a linearly parameterized family. Scoring functions\nfor the mean value and probability forecasts of binary events constitute\nimportant examples. The quantile and expectile functionals along with the\nrespective extremal scoring functions admit appealing economic interpretations\nin terms of thresholds in decision making.\n  The Choquet type mixture representations give rise to simple checks of\nwhether a forecast dominates another in the sense that it is preferable under\nany consistent scoring function. In empirical settings it suffices to compare\nthe average scores for only a finite number of extremal elements. Plots of the\naverage scores with respect to the extremal scoring functions, which we call\nMurphy diagrams, permit detailed comparisons of the relative merits of\ncompeting forecasts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 19:35:55 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 14:08:40 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Ehm", "Werner", ""], ["Gneiting", "Tilmann", ""], ["Jordan", "Alexander", ""], ["Kr\u00fcger", "Fabian", ""]]}, {"id": "1503.08272", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck, Stephen Wu, Hui Li", "title": "Robust Bayesian compressive sensing with data loss recovery for\n  structural health monitoring signals", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of compressive sensing (CS) to structural health monitoring\nis an emerging research topic. The basic idea in CS is to use a\nspecially-designed wireless sensor to sample signals that are sparse in some\nbasis (e.g. wavelet basis) directly in a compressed form, and then to\nreconstruct (decompress) these signals accurately using some inversion\nalgorithm after transmission to a central processing unit. However, most\nsignals in structural health monitoring are only approximately sparse, i.e.\nonly a relatively small number of the signal coefficients in some basis are\nsignificant, but the other coefficients are usually not exactly zero. In this\ncase, perfect reconstruction from compressed measurements is not expected. A\nnew Bayesian CS algorithm is proposed in which robust treatment of the\nuncertain parameters is explored, including integration over the\nprediction-error precision parameter to remove it as a \"nuisance\" parameter.\nThe performance of the new CS algorithm is investigated using compressed data\nfrom accelerometers installed on a space-frame structure and on a cable-stayed\nbridge. Compared with other state-of-the-art CS methods including our\npreviously-published Bayesian method which uses MAP (maximum a posteriori)\nestimation of the prediction-error precision parameter, the new algorithm shows\nsuperior performance in reconstruction robustness and posterior uncertainty\nquantification. Furthermore, our method can be utilized for recovery of lost\ndata during wireless transmission, regardless of the level of sparseness in the\nsignal.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 06:14:45 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Wu", "Stephen", ""], ["Li", "Hui", ""]]}, {"id": "1503.08329", "submitter": "Pascal Germain", "authors": "Pascal Germain, Alexandre Lacasse, Fran\\c{c}ois Laviolette, Mario\n  Marchand, Jean-Francis Roy", "title": "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a\n  Learning Algorithm", "comments": "Published in JMLR http://jmlr.org/papers/v16/germain15a.html", "journal-ref": "Journal of Machine Learning Research 2015, vol. 16, p. 787-860", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extensive analysis of the behavior of majority votes in binary\nclassification. In particular, we introduce a risk bound for majority votes,\ncalled the C-bound, that takes into account the average quality of the voters\nand their average disagreement. We also propose an extensive PAC-Bayesian\nanalysis that shows how the C-bound can be estimated from various observations\ncontained in the training data. The analysis intends to be self-contained and\ncan be used as introductory material to PAC-Bayesian statistical learning\ntheory. It starts from a general PAC-Bayesian perspective and ends with\nuncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler\ndivergence and others allow kernel functions to be used as voters (via the\nsample compression setting). Finally, out of the analysis, we propose the MinCq\nlearning algorithm that basically minimizes the C-bound. MinCq reduces to a\nsimple quadratic program. Aside from being theoretically grounded, MinCq\nachieves state-of-the-art performance, as shown in our extensive empirical\ncomparison with both AdaBoost and the Support Vector Machine.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 17:19:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 20:08:16 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Germain", "Pascal", ""], ["Lacasse", "Alexandre", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1503.08348", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Rebecca M. Willett", "title": "Sparse Linear Regression With Missing Data", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fast and accurate method for sparse regression in the\npresence of missing data. The underlying statistical model encapsulates the\nlow-dimensional structure of the incomplete data matrix and the sparsity of the\nregression coefficients, and the proposed algorithm jointly learns the\nlow-dimensional structure of the data and a linear regressor with sparse\ncoefficients. The proposed stochastic optimization method, Sparse Linear\nRegression with Missing Data (SLRM), performs an alternating minimization\nprocedure and scales well with the problem size. Large deviation inequalities\nshed light on the impact of the various problem-dependent parameters on the\nexpected squared loss of the learned regressor. Extensive simulations on both\nsynthetic and real datasets show that SLRM performs better than competing\nalgorithms in a variety of contexts.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 21:03:32 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1503.08356", "submitter": "Jie Shen", "authors": "Jie Shen, Ping Li, Huan Xu", "title": "Efficient Online Minimization for Low-Rank Subspace Clustering", "comments": "Short version accepted to ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank representation~(LRR) has been a significant method for segmenting\ndata that are generated from a union of subspaces. It is, however, known that\nsolving the LRR program is challenging in terms of time complexity and memory\nfootprint, in that the size of the nuclear norm regularized matrix is\n$n$-by-$n$ (where $n$ is the number of samples). In this paper, we thereby\ndevelop a fast online implementation of LRR that reduces the memory cost from\n$O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and $d$ being some\nestimated rank~($d < p \\ll n$). The crux for this end is a non-convex\nreformulation of the LRR program, which pursues the basis dictionary that\ngenerates the (uncorrupted) observations. We build the theoretical guarantee\nthat the sequence of the solutions produced by our algorithm converges to a\nstationary point of the empirical and the expected loss function\nasymptotically. Extensive experiments on synthetic and realistic datasets\nfurther substantiate that our algorithm is fast, robust and memory efficient.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 21:56:35 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 18:22:23 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 04:28:24 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 05:52:29 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Shen", "Jie", ""], ["Li", "Ping", ""], ["Xu", "Huan", ""]]}, {"id": "1503.08363", "submitter": "Ravi Ganti", "authors": "Ravi Ganti", "title": "Active Model Aggregation via Stochastic Mirror Descent", "comments": "12 pages, 20 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning convex aggregation of models, that is as\ngood as the best convex aggregation, for the binary classification problem.\nWorking in the stream based active learning setting, where the active learner\nhas to make a decision on-the-fly, if it wants to query for the label of the\npoint currently seen in the stream, we propose a stochastic-mirror descent\nalgorithm, called SMD-AMA, with entropy regularization. We establish an excess\nrisk bounds for the loss of the convex aggregate returned by SMD-AMA to be of\nthe order of $O\\left(\\sqrt{\\frac{\\log(M)}{{T^{1-\\mu}}}}\\right)$, where $\\mu\\in\n[0,1)$ is an algorithm dependent parameter, that trades-off the number of\nlabels queried, and excess risk.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:54:12 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""]]}, {"id": "1503.08471", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "Cross-validation of matching correlation analysis by resampling matching\n  weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength of association between a pair of data vectors is represented by\na nonnegative real number, called matching weight. For dimensionality\nreduction, we consider a linear transformation of data vectors, and define a\nmatching error as the weighted sum of squared distances between transformed\nvectors with respect to the matching weights. Given data vectors and matching\nweights, the optimal linear transformation minimizing the matching error is\nsolved by the spectral graph embedding of Yan et al. (2007). This method is a\ngeneralization of the canonical correlation analysis, and will be called as\nmatching correlation analysis (MCA). In this paper, we consider a novel\nsampling scheme where the observed matching weights are randomly sampled from\nunderlying true matching weights with small probability, whereas the data\nvectors are treated as constants. We then investigate a cross-validation by\nresampling the matching weights. Our asymptotic theory shows that the\ncross-validation, if rescaled properly, computes an unbiased estimate of the\nmatching error with respect to the true matching weights. Existing ideas of\ncross-validation for resampling data vectors, instead of resampling matching\nweights, are not applicable here. MCA can be used for data vectors from\nmultiple domains with different dimensions via an embarrassingly simple idea of\ncoding the data vectors. This method will be called as cross-domain matching\ncorrelation analysis (CDMCA), and an interesting connection to the classical\nassociative memory model of neural networks is also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 18:21:22 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 14:50:44 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2015 02:03:47 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1503.08479", "submitter": "Lex Fridman", "authors": "Lex Fridman, Steven Weber, Rachel Greenstadt, Moshe Kam", "title": "Active Authentication on Mobile Devices via Stylometry, Application\n  Usage, Web Browsing, and GPS Location", "comments": "Accepted for Publication in the IEEE Systems Journal", "journal-ref": null, "doi": "10.1109/JSYST.2015.2472579", "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active authentication is the problem of continuously verifying the identity\nof a person based on behavioral aspects of their interaction with a computing\ndevice. In this study, we collect and analyze behavioral biometrics data from\n200subjects, each using their personal Android mobile device for a period of at\nleast 30 days. This dataset is novel in the context of active authentication\ndue to its size, duration, number of modalities, and absence of restrictions on\ntracked activity. The geographical colocation of the subjects in the study is\nrepresentative of a large closed-world environment such as an organization\nwhere the unauthorized user of a device is likely to be an insider threat:\ncoming from within the organization. We consider four biometric modalities: (1)\ntext entered via soft keyboard, (2) applications used, (3) websites visited,\nand (4) physical location of the device as determined from GPS (when outdoors)\nor WiFi (when indoors). We implement and test a classifier for each modality\nand organize the classifiers as a parallel binary decision fusion architecture.\nWe are able to characterize the performance of the system with respect to\nintruder detection time and to quantify the contribution of each modality to\nthe overall performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 18:59:23 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fridman", "Lex", ""], ["Weber", "Steven", ""], ["Greenstadt", "Rachel", ""], ["Kam", "Moshe", ""]]}, {"id": "1503.08535", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Infinite Author Topic Model based on Mixed Gamma-Negative Binomial\n  Process", "comments": "10 pages, 5 figures, submitted to KDD conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating the side information of text corpus, i.e., authors, time\nstamps, and emotional tags, into the traditional text mining models has gained\nsignificant interests in the area of information retrieval, statistical natural\nlanguage processing, and machine learning. One branch of these works is the\nso-called Author Topic Model (ATM), which incorporates the authors's interests\nas side information into the classical topic model. However, the existing ATM\nneeds to predefine the number of topics, which is difficult and inappropriate\nin many real-world settings. In this paper, we propose an Infinite Author Topic\n(IAT) model to resolve this issue. Instead of assigning a discrete probability\non fixed number of topics, we use a stochastic process to determine the number\nof topics from the data itself. To be specific, we extend a gamma-negative\nbinomial process to three levels in order to capture the\nauthor-document-keyword hierarchical structure. Furthermore, each document is\nassigned a mixed gamma process that accounts for the multi-author's\ncontribution towards this document. An efficient Gibbs sampling inference\nalgorithm with each conditional distribution being closed-form is developed for\nthe IAT model. Experiments on several real-world datasets show the capabilities\nof our IAT model to learn the hidden topics, authors' interests on these topics\nand the number of topics simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:03:37 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08542", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Nonparametric Relational Topic Models through Dependent Gamma Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:40:41 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08727", "submitter": "Mauricio A. \\'Alvarez", "authors": "Carlos D. Zuluaga, Edgar A. Valencia, Mauricio A. \\'Alvarez", "title": "A Parzen-based distance between probability measures as an alternative\n  of summary statistics in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo\nmethods. ABC methods use a comparison between simulated data, using different\nparameters drew from a prior distribution, and observed data. This comparison\nprocess is based on computing a distance between the summary statistics from\nthe simulated data and the observed data. For complex models, it is usually\ndifficult to define a methodology for choosing or constructing the summary\nstatistics. Recently, a nonparametric ABC has been proposed, that uses a\ndissimilarity measure between discrete distributions based on empirical kernel\nembeddings as an alternative for summary statistics. The nonparametric ABC\noutperforms other methods including ABC, kernel ABC or synthetic likelihood\nABC. However, it assumes that the probability distributions are discrete, and\nit is not robust when dealing with few observations. In this paper, we propose\nto apply kernel embeddings using an smoother density estimator or Parzen\nestimator for comparing the empirical data distributions, and computing the ABC\nposterior. Synthetic data and real data were used to test the Bayesian\ninference of our method. We compare our method with respect to state-of-the-art\nmethods, and demonstrate that our method is a robust estimator of the posterior\ndistribution in terms of the number of observations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 16:10:23 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Zuluaga", "Carlos D.", ""], ["Valencia", "Edgar A.", ""], ["\u00c1lvarez", "Mauricio A.", ""]]}, {"id": "1503.08855", "submitter": "Gonzalo Mateos", "authors": "Georgios B. Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D. Schizas,\n  and Hao Zhu", "title": "Decentralized learning for wireless communications and networking", "comments": "Contributed chapter to appear in Splitting Methods in Communication\n  and Imaging, Science and Engineering, R. Glowinski, S. Osher, and W. Yin,\n  Editors, New York, Springer, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 21:18:38 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Giannakis", "Georgios B.", ""], ["Ling", "Qing", ""], ["Mateos", "Gonzalo", ""], ["Schizas", "Ioannis D.", ""], ["Zhu", "Hao", ""]]}, {"id": "1503.08985", "submitter": "Lorenzo Rosasco", "authors": "Junhong Lin, Lorenzo Rosasco, Ding-Xuan Zhou", "title": "Iterative Regularization for Learning with Convex Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of supervised learning with convex loss functions and\npropose a new form of iterative regularization based on the subgradient method.\nUnlike other regularization approaches, in iterative regularization no\nconstraint or penalization is considered, and generalization is achieved by\n(early) stopping an empirical iteration. We consider a nonparametric setting,\nin the framework of reproducing kernel Hilbert spaces, and prove finite sample\nbounds on the excess risk under general regularity conditions. Our study\nprovides a new class of efficient regularized learning algorithms and gives\ninsights on the interplay between statistics and optimization in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 09:49:30 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 08:39:47 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Lin", "Junhong", ""], ["Rosasco", "Lorenzo", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1503.09022", "submitter": "Jesse Read", "authors": "Jesse Read and Jaakko Hollm\\'en", "title": "Multi-label Classification using Labels as Hidden Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive methods for multi-label classification typically invest in\nlearning labels together. To do so in a beneficial way, analysis of label\ndependence is often seen as a fundamental step, separate and prior to\nconstructing a classifier. Some methods invest up to hundreds of times more\ncomputational effort in building dependency models, than training the final\nclassifier itself. We extend some recent discussion in the literature and\nprovide a deeper analysis, namely, developing the view that label dependence is\noften introduced by an inadequate base classifier, rather than being inherent\nto the data or underlying concept; showing how even an exhaustive analysis of\nlabel dependence may not lead to an optimal classification structure. Viewing\nlabels as additional features (a transformation of the input), we create\nneural-network inspired novel methods that remove the emphasis of a prior\ndependency structure. Our methods have an important advantage particular to\nmulti-label data: they leverage labels to create effective units in middle\nlayers, rather than learning these units from scratch in an unsupervised\nfashion with gradient-based methods. Results are promising. The methods we\npropose perform competitively, and also have very important qualities of\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 12:29:13 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 10:54:26 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 12:12:49 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Read", "Jesse", ""], ["Hollm\u00e9n", "Jaakko", ""]]}, {"id": "1503.09105", "submitter": "Prasenjit Karmakar", "authors": "Prasenjit Karmakar and Shalabh Bhatnagar", "title": "Two Timescale Stochastic Approximation with Controlled Markov noise and\n  Off-policy temporal difference learning", "comments": "23 pages (relaxed some important assumptions from the previous\n  version), accepted in Mathematics of Operations Research in Feb, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present for the first time an asymptotic convergence analysis of two\ntime-scale stochastic approximation driven by `controlled' Markov noise. In\nparticular, both the faster and slower recursions have non-additive controlled\nMarkov noise components in addition to martingale difference noise. We analyze\nthe asymptotic behavior of our framework by relating it to limiting\ndifferential inclusions in both time-scales that are defined in terms of the\nergodic occupation measures associated with the controlled Markov processes.\nFinally, we present a solution to the off-policy convergence problem for\ntemporal difference learning with linear function approximation, using our\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 16:10:55 GMT"}, {"version": "v10", "created": "Sat, 26 Mar 2016 04:53:48 GMT"}, {"version": "v11", "created": "Sun, 17 Apr 2016 13:11:17 GMT"}, {"version": "v12", "created": "Thu, 16 Feb 2017 09:37:38 GMT"}, {"version": "v13", "created": "Wed, 22 Feb 2017 17:06:39 GMT"}, {"version": "v14", "created": "Sat, 25 Feb 2017 18:46:13 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 17:18:37 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 04:11:39 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2015 12:49:32 GMT"}, {"version": "v5", "created": "Wed, 5 Aug 2015 14:02:19 GMT"}, {"version": "v6", "created": "Thu, 6 Aug 2015 12:53:51 GMT"}, {"version": "v7", "created": "Fri, 1 Jan 2016 12:10:22 GMT"}, {"version": "v8", "created": "Mon, 18 Jan 2016 15:29:21 GMT"}, {"version": "v9", "created": "Mon, 21 Mar 2016 19:25:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Karmakar", "Prasenjit", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1503.09113", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli, Rodolphe Sepulchre", "title": "On the Projective Geometry of Kalman Filter", "comments": "6 pages", "journal-ref": "Proc. of the 54th IEEE Conference on Decision and Control (CDC\n  2015)", "doi": "10.1109/CDC.2015.7402570", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convergence of the Kalman filter is best analyzed by studying the contraction\nof the Riccati map in the space of positive definite (covariance) matrices. In\nthis paper, we explore how this contraction property relates to a more\nfundamental non-expansiveness property of filtering maps in the space of\nprobability distributions endowed with the Hilbert metric. This is viewed as a\npreliminary step towards improving the convergence analysis of filtering\nalgorithms over general graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 16:27:22 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 20:00:18 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Carli", "Francesca Paola", ""], ["Sepulchre", "Rodolphe", ""]]}]