[{"id": "1003.0060", "submitter": "Z.X. Guo", "authors": "Z.X. Guo", "title": "Comment on \"Fastest learning in small-world neural networks\"", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This comment reexamines Simard et al.'s work in [D. Simard, L. Nadeau, H.\nKroger, Phys. Lett. A 336 (2005) 8-15]. We found that Simard et al. calculated\nmistakenly the local connectivity lengths Dlocal of networks. The right results\nof Dlocal are presented and the supervised learning performance of feedforward\nneural networks (FNNs) with different rewirings are re-investigated in this\ncomment. This comment discredits Simard et al's work by two conclusions: 1)\nRewiring connections of FNNs cannot generate networks with small-world\nconnectivity; 2) For different training sets, there do not exist networks with\na certain number of rewirings generating reduced learning errors than networks\nwith other numbers of rewiring.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 04:38:43 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Guo", "Z. X.", ""]]}, {"id": "1003.0078", "submitter": "Marius Kloft", "authors": "Marius Kloft and Pavel Laskov", "title": "Security Analysis of Online Centroid Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCB/EECS-2010-22", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security issues are crucial in a number of machine learning applications,\nespecially in scenarios dealing with human activity rather than natural\nphenomena (e.g., information ranking, spam detection, malware detection, etc.).\nIt is to be expected in such cases that learning algorithms will have to deal\nwith manipulated data aimed at hampering decision making. Although some\nprevious work addressed the handling of malicious data in the context of\nsupervised learning, very little is known about the behavior of anomaly\ndetection methods in such scenarios. In this contribution we analyze the\nperformance of a particular method -- online centroid anomaly detection -- in\nthe presence of adversarial noise. Our analysis addresses the following\nsecurity-related issues: formalization of learning and attack processes,\nderivation of an optimal attack, analysis of its efficiency and constraints. We\nderive bounds on the effectiveness of a poisoning attack against centroid\nanomaly under different conditions: bounded and unbounded percentage of\ntraffic, and bounded false positive rate. Our bounds show that whereas a\npoisoning attack can be effectively staged in the unconstrained case, it can be\nmade arbitrarily difficult (a strict upper bound on the attacker's gain) if\nexternal constraints are properly used. Our experimental evaluation carried out\non real HTTP and exploit traces confirms the tightness of our theoretical\nbounds and practicality of our protection mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 08:35:51 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Kloft", "Marius", ""], ["Laskov", "Pavel", ""]]}, {"id": "1003.0079", "submitter": "Marius Kloft", "authors": "Marius Kloft, Ulf Brefeld, Soeren Sonnenburg, Alexander Zien", "title": "Non-Sparse Regularization for Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCB/EECS-2010-21", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning linear combinations of multiple kernels is an appealing strategy\nwhen the right choice of features is unknown. Previous approaches to multiple\nkernel learning (MKL) promote sparse kernel combinations to support\ninterpretability and scalability. Unfortunately, this 1-norm MKL is rarely\nobserved to outperform trivial baselines in practical applications. To allow\nfor robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new\ninsights on the connection between several existing MKL formulations and\ndevelop two efficient interleaved optimization strategies for arbitrary norms,\nlike p-norms with p>1. Empirically, we demonstrate that the interleaved\noptimization strategies are much faster compared to the commonly used wrapper\napproaches. A theoretical analysis and an experiment on controlled artificial\ndata experiment sheds light on the appropriateness of sparse, non-sparse and\n$\\ell_\\infty$-norm MKL in various scenarios. Empirical applications of p-norm\nMKL to three real-world problems from computational biology show that\nnon-sparse MKL achieves accuracies that go beyond the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2010 08:54:29 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2010 06:18:11 GMT"}, {"version": "v3", "created": "Tue, 26 Oct 2010 20:21:35 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Kloft", "Marius", ""], ["Brefeld", "Ulf", ""], ["Sonnenburg", "Soeren", ""], ["Zien", "Alexander", ""]]}, {"id": "1003.0783", "submitter": "David Blei", "authors": "David M. Blei and Jon D. McAuliffe", "title": "Supervised Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce supervised latent Dirichlet allocation (sLDA), a statistical\nmodel of labelled documents. The model accommodates a variety of response\ntypes. We derive an approximate maximum-likelihood procedure for parameter\nestimation, which relies on variational methods to handle intractable posterior\nexpectations. Prediction problems motivate this research: we use the fitted\nmodel to predict response values for new documents. We test sLDA on two\nreal-world problems: movie ratings predicted from reviews, and the political\ntone of amendments in the U.S. Senate based on the amendment text. We\nillustrate the benefits of sLDA versus modern regularized regression, as well\nas versus an unsupervised LDA analysis followed by a separate regression.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 11:36:56 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Blei", "David M.", ""], ["McAuliffe", "Jon D.", ""]]}, {"id": "1003.0887", "submitter": "Bharath Sriperumbudur", "authors": "Bharath K. Sriperumbudur, Kenji Fukumizu and Gert R. G. Lanckriet", "title": "Universality, Characteristic Kernels and RKHS Embedding of Measures", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hilbert space embedding for probability measures has recently been\nproposed, wherein any probability measure is represented as a mean element in a\nreproducing kernel Hilbert space (RKHS). Such an embedding has found\napplications in homogeneity testing, independence testing, dimensionality\nreduction, etc., with the requirement that the reproducing kernel is\ncharacteristic, i.e., the embedding is injective.\n  In this paper, we generalize this embedding to finite signed Borel measures,\nwherein any finite signed Borel measure is represented as a mean element in an\nRKHS. We show that the proposed embedding is injective if and only if the\nkernel is universal. This therefore, provides a novel characterization of\nuniversal kernels, which are proposed in the context of achieving the Bayes\nrisk by kernel-based classification/regression algorithms. By exploiting this\nrelation between universality and the embedding of finite signed Borel measures\ninto an RKHS, we establish the relation between universal and characteristic\nkernels.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 20:30:07 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Sriperumbudur", "Bharath K.", ""], ["Fukumizu", "Kenji", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "1003.1954", "submitter": "D\\'avid P\\'al", "authors": "D\\'avid P\\'al, Barnab\\'as P\\'oczos, Csaba Szepesv\\'ari", "title": "Estimation of R\\'enyi Entropy and Mutual Information Based on\n  Generalized Nearest-Neighbor Graphs", "comments": "to appear at NIPS 2010 (Neural Information Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple and computationally efficient nonparametric estimators of\nR\\'enyi entropy and mutual information based on an i.i.d. sample drawn from an\nunknown, absolutely continuous distribution over $\\R^d$. The estimators are\ncalculated as the sum of $p$-th powers of the Euclidean lengths of the edges of\nthe `generalized nearest-neighbor' graph of the sample and the empirical copula\nof the sample respectively. For the first time, we prove the almost sure\nconsistency of these estimators and upper bounds on their rates of convergence,\nthe latter of which under the assumption that the density underlying the sample\nis Lipschitz continuous. Experiments demonstrate their usefulness in\nindependent subspace analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 19:01:51 GMT"}, {"version": "v2", "created": "Mon, 25 Oct 2010 23:18:35 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["P\u00e1l", "D\u00e1vid", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1003.2245", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Peter Bartlett and Max Dama", "title": "Optimal Allocation Strategies for the Dark Pool Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating stocks to dark pools. We propose and\nanalyze an optimal approach for allocations, if continuous-valued allocations\nare allowed. We also propose a modification for the case when only\ninteger-valued allocations are possible. We extend the previous work on this\nproblem to adversarial scenarios, while also improving on their results in the\niid setup. The resulting algorithms are efficient, and perform well in\nsimulations under stochastic and adversarial inputs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 03:05:59 GMT"}], "update_date": "2010-03-12", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bartlett", "Peter", ""], ["Dama", "Max", ""]]}, {"id": "1003.2469", "submitter": "Daniel  Romero", "authors": "Daniel M. Romero and Jon Kleinberg", "title": "The Directed Closure Process in Hybrid Social-Information Networks, with\n  an Analysis of Link Formation on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has often been taken as a working assumption that directed links in\ninformation networks are frequently formed by \"short-cutting\" a two-step path\nbetween the source and the destination -- a kind of implicit \"link copying\"\nanalogous to the process of triadic closure in social networks. Despite the\nrole of this assumption in theoretical models such as preferential attachment,\nit has received very little direct empirical investigation. Here we develop a\nformalization and methodology for studying this type of directed closure\nprocess, and we provide evidence for its important role in the formation of\nlinks on Twitter. We then analyze a sequence of models designed to capture the\nstructural phenomena related to directed closure that we observe in the Twitter\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 03:53:22 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Romero", "Daniel M.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1003.2941", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ramirez and Guillermo Sapiro", "title": "Universal Regularizers For Robust Sparse Coding and Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse data models, where data is assumed to be well represented as a linear\ncombination of a few elements from a dictionary, have gained considerable\nattention in recent years, and their use has led to state-of-the-art results in\nmany signal and image processing tasks. It is now well understood that the\nchoice of the sparsity regularization term is critical in the success of such\nmodels. Based on a codelength minimization interpretation of sparse coding, and\nusing tools from universal coding theory, we propose a framework for designing\nsparsity regularization terms which have theoretical and practical advantages\nwhen compared to the more standard l0 or l1 ones. The presentation of the\nframework and theoretical foundations is complemented with examples that show\nits practical advantages in image denoising, zooming and classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2010 15:25:28 GMT"}, {"version": "v2", "created": "Tue, 3 Aug 2010 16:39:34 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Ramirez", "Ignacio", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1003.3570", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala and Antti Airola and Tapio Salakoski", "title": "Linear Time Feature Selection for Regularized Least-Squares", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for greedy forward feature selection for\nregularized least-squares (RLS) regression and classification, also known as\nthe least-squares support vector machine or ridge regression. The algorithm,\nwhich we call greedy RLS, starts from the empty feature set, and on each\niteration adds the feature whose addition provides the best leave-one-out\ncross-validation performance. Our method is considerably faster than the\npreviously proposed ones, since its time complexity is linear in the number of\ntraining examples, the number of features in the original data set, and the\ndesired size of the set of selected features. Therefore, as a side effect we\nobtain a new training algorithm for learning sparse linear RLS predictors which\ncan be used for large scale learning. This speed is possible due to matrix\ncalculus based short-cuts for leave-one-out and feature addition. We\nexperimentally demonstrate the scalability of our algorithm and its ability to\nfind good quality feature sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 12:39:34 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["Salakoski", "Tapio", ""]]}, {"id": "1003.3829", "submitter": "Emily Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky", "title": "Bayesian Nonparametric Inference of Switching Linear Dynamical Systems", "comments": "50 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TSP.2010.2102756", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex dynamical phenomena can be effectively modeled by a system that\nswitches among a set of conditionally linear dynamical modes. We consider two\nsuch models: the switching linear dynamical system (SLDS) and the switching\nvector autoregressive (VAR) process. Our Bayesian nonparametric approach\nutilizes a hierarchical Dirichlet process prior to learn an unknown number of\npersistent, smooth dynamical modes. We additionally employ automatic relevance\ndetermination to infer a sparse set of dynamic dependencies allowing us to\nlearn SLDS with varying state dimension or switching VAR processes with varying\nautoregressive order. We develop a sampling algorithm that combines a truncated\napproximation to the Dirichlet process with efficient joint sampling of the\nmode and state sequences. The utility and flexibility of our model are\ndemonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA\nstock index, and a maneuvering target tracking application.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 16:22:02 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1003.4741", "submitter": "David Rogers", "authors": "David M. Rogers and Thomas L. Beck", "title": "Resolution and Scale Independent Function Matching Using a String Energy\n  Penalized Spline Prior", "comments": "Submitted to Annals of Statistics, Mar. 2009, but unpublished in\n  current form.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.mes-hall", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of the classical Bayesian penalized spline method to inference\non vector-valued functions is considered, with an emphasis on characterizing\nthe suitability of the method for general application.We show that the standard\nquadratic penalty is exactly analogous to the energy of a stretched string,\nwith the penalty parameter corresponding to its tension. This physical analogy\nmotivates a discussion of resolution independence, which we define as the\nconvergence of a computational function estimate to arbitrary accuracy with\nincreasing resolution.The multidimensional context makes direct application of\nstandard procedures for choosing the penalty parameter difficult, and a new\nmethod is proposed and compared to the established generalized cross-validation\n(GCV) and Akaike information criterion (AIC) functions.Our Bayesian method for\nchoosing this parameter is derived by introducing a scal e independence\ncriterion to ensure that simultaneously scaling the function samples and their\nvariances does not significantly change the posterior parameter distribution.\nDue to the possibility of an exact polynomial fit, numerical issues prevent the\nuse of this prior, and a solution is presented based on adding a st ring\nzero-point energy. This makes more complicated approaches recently propose d in\nthe literature unnecessary, and eliminates the requirement for sensitivity\nanalysis when the function deviates from the above mentioned polynomial. An\nimportant class of problems which can be analyzed by this method are stochastic\nnumerical integrators, which are considered as an example problem. This work\nrepresents the first extension of penalized spline methods to inference on\nmultidimensional numerical integrators reported in the literature. Several\nnumerical calculations illustrate the above points and address practical\napplication issues.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2010 20:06:55 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Rogers", "David M.", ""], ["Beck", "Thomas L.", ""]]}, {"id": "1003.4944", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, George E. Dahl, Iain Murray", "title": "Incorporating Side Information in Probabilistic Matrix Factorization\n  with Gaussian Processes", "comments": "18 pages, 4 figures, Submitted to UAI 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic matrix factorization (PMF) is a powerful method for modeling\ndata associated with pairwise relationships, finding use in collaborative\nfiltering, computational biology, and document analysis, among other areas. In\nmany domains, there is additional information that can assist in prediction.\nFor example, when modeling movie ratings, we might know when the rating\noccurred, where the user lives, or what actors appear in the movie. It is\ndifficult, however, to incorporate this side information into the PMF model. We\npropose a framework for incorporating side information by coupling together\nmultiple PMF problems via Gaussian process priors. We replace scalar latent\nfeatures with functions that vary over the space of side information. The GP\npriors on these functions require them to vary smoothly and share information.\nWe successfully use this new method to predict the scores of professional\nbasketball games, where side information about the venue and date of the game\nare relevant for the outcome.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 16:12:48 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Dahl", "George E.", ""], ["Murray", "Iain", ""]]}, {"id": "1003.5930", "submitter": "Mu Zhu", "authors": "Lu Xin, Mu Zhu", "title": "Stochastic Stepwise Ensembles for Variable Selection", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, June 2012, Vol.\n  21, No. 2, Pages 275 - 294", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advocate the ensemble approach for variable selection. We\npoint out that the stochastic mechanism used to generate the variable-selection\nensemble (VSE) must be picked with care. We construct a VSE using a stochastic\nstepwise algorithm, and compare its performance with numerous state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 21:35:26 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 21:12:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Xin", "Lu", ""], ["Zhu", "Mu", ""]]}, {"id": "1003.5956", "submitter": "Lihong Li", "authors": "Lihong Li and Wei Chu and John Langford and Xuanhui Wang", "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article\n  Recommendation Algorithms", "comments": "10 pages, 7 figures, revised from the published version at the WSDM\n  2011 conference", "journal-ref": null, "doi": "10.1145/1935826.1935878", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms have become popular for online recommendation\nsystems such as Digg, Yahoo! Buzz, and news recommendation in general.\n\\emph{Offline} evaluation of the effectiveness of new algorithms in these\napplications is critical for protecting online user experiences but very\nchallenging due to their \"partial-label\" nature. Common practice is to create a\nsimulator which simulates the online environment for the problem at hand and\nthen run an algorithm against this simulator. However, creating simulator\nitself is often difficult and modeling bias is usually unavoidably introduced.\nIn this paper, we introduce a \\emph{replay} methodology for contextual bandit\nalgorithm evaluation. Different from simulator-based approaches, our method is\ncompletely data-driven and very easy to adapt to different applications. More\nimportantly, our method can provide provably unbiased evaluations. Our\nempirical results on a large-scale news article recommendation dataset\ncollected from Yahoo! Front Page conform well with our theoretical results.\nFurthermore, comparisons between our offline replay and online bucket\nevaluation of several contextual bandit algorithms show accuracy and\neffectiveness of our offline evaluation method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2010 01:20:07 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:33:07 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Wang", "Xuanhui", ""]]}]