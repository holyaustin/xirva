[{"id": "0801.0848", "submitter": "Nathalie Villa", "authors": "Romain Boulet (IMT), Bertrand Jouve (IMT), Fabrice Rossi (INRIA\n  Rocquencourt / INRIA Sophia Antipolis), Nathalie Villa (IMT)", "title": "Batch kernel SOM and related Laplacian methods for social network\n  analysis", "comments": null, "journal-ref": "Neurocomputing / EEG Neurocomputing (2008) A para\\^itre", "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.ML stat.TH", "license": null, "abstract": "  Large graphs are natural mathematical models for describing the structure of\nthe data in a wide variety of fields, such as web mining, social networks,\ninformation retrieval, biological networks, etc. For all these applications,\nautomatic tools are required to get a synthetic view of the graph and to reach\na good understanding of the underlying problem. In particular, discovering\ngroups of tightly connected vertices and understanding the relations between\nthose groups is very important in practice. This paper shows how a kernel\nversion of the batch Self Organizing Map can be used to achieve these goals via\nkernels derived from the Laplacian matrix of the graph, especially when it is\nused in conjunction with more classical methods based on the spectral analysis\nof the graph. The proposed method is used to explore the structure of a\nmedieval social network modeled through a weighted graph that has been directly\nbuilt from a large corpus of agrarian contracts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2008 08:28:15 GMT"}], "update_date": "2008-01-08", "authors_parsed": [["Boulet", "Romain", "", "IMT"], ["Jouve", "Bertrand", "", "IMT"], ["Rossi", "Fabrice", "", "INRIA\n  Rocquencourt / INRIA Sophia Antipolis"], ["Villa", "Nathalie", "", "IMT"]]}, {"id": "0801.1196", "submitter": "Gert De Cooman", "authors": "Gert de Cooman, Filip Hermans", "title": "Imprecise probability trees: Bridging two theories of imprecise\n  probability", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": null, "abstract": "  We give an overview of two approaches to probability theory where lower and\nupper probabilities, rather than probabilities, are used: Walley's behavioural\ntheory of imprecise probabilities, and Shafer and Vovk's game-theoretic account\nof probability. We show that the two theories are more closely related than\nwould be suspected at first sight, and we establish a correspondence between\nthem that (i) has an interesting interpretation, and (ii) allows us to freely\nimport results from one theory into the other. Our approach leads to an account\nof probability trees and random processes in the framework of Walley's theory.\nWe indicate how our results can be used to reduce the computational complexity\nof dealing with imprecision in probability trees, and we prove an interesting\nand quite general version of the weak law of large numbers.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2008 09:46:44 GMT"}], "update_date": "2008-01-09", "authors_parsed": [["de Cooman", "Gert", ""], ["Hermans", "Filip", ""]]}, {"id": "0801.1440", "submitter": "Monia Lupparelli", "authors": "Monia Lupparelli, Giovanni M. Marchetti and Wicher P. Bergsma", "title": "Parameterizations and fitting of bi-directed graph models to categorical\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": null, "abstract": "  We discuss two parameterizations of models for marginal independencies for\ndiscrete distributions which are representable by bi-directed graph models,\nunder the global Markov property. Such models are useful data analytic tools\nespecially if used in combination with other graphical models. The first\nparameterization, in the saturated case, is also known as the multivariate\nlogistic transformation, the second is a variant that allows, in some (but not\nall) cases, variation independent parameters. An algorithm for maximum\nlikelihood fitting is proposed, based on an extension of the Aitchison and\nSilvey method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2008 14:29:17 GMT"}], "update_date": "2008-01-10", "authors_parsed": [["Lupparelli", "Monia", ""], ["Marchetti", "Giovanni M.", ""], ["Bergsma", "Wicher P.", ""]]}, {"id": "0801.2934", "submitter": "Lutz D\\\"umbgen", "authors": "Lutz Duembgen, Bernd-Wolfgang Igl, Axel Munk", "title": "P-values for classification", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS245 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2008, Vol. 2, 468-493", "doi": "10.1214/08-EJS245", "report-no": "IMS-EJS-EJS_2008_245", "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  Let $(X,Y)$ be a random variable consisting of an observed feature vector\n$X\\in \\mathcal{X}$ and an unobserved class label $Y\\in \\{1,2,...,L\\}$ with\nunknown joint distribution. In addition, let $\\mathcal{D}$ be a training data\nset consisting of $n$ completely observed independent copies of $(X,Y)$. Usual\nclassification procedures provide point predictors (classifiers)\n$\\widehat{Y}(X,\\mathcal{D})$ of $Y$ or estimate the conditional distribution of\n$Y$ given $X$. In order to quantify the certainty of classifying $X$ we propose\nto construct for each $\\theta =1,2,...,L$ a p-value\n$\\pi_{\\theta}(X,\\mathcal{D})$ for the null hypothesis that $Y=\\theta$, treating\n$Y$ temporarily as a fixed parameter. In other words, the point predictor\n$\\widehat{Y}(X,\\mathcal{D})$ is replaced with a prediction region for $Y$ with\na certain confidence. We argue that (i) this approach is advantageous over\ntraditional approaches and (ii) any reasonable classifier can be modified to\nyield nonparametric p-values. We discuss issues such as optimality, single use\nand multiple use validity, as well as computational and graphical aspects.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2008 16:44:02 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2008 09:34:53 GMT"}, {"version": "v3", "created": "Thu, 26 Jun 2008 08:14:11 GMT"}], "update_date": "2008-06-26", "authors_parsed": [["Duembgen", "Lutz", ""], ["Igl", "Bernd-Wolfgang", ""], ["Munk", "Axel", ""]]}, {"id": "0801.4627", "submitter": "Ulrike Schneider", "authors": "Benedikt M. P\\\"otscher and Ulrike Schneider", "title": "On the Distribution of the Adaptive LASSO Estimator", "comments": "revised version; minor changes and some material added", "journal-ref": "J. Stat. Plann. Inference 139 (2009) 2775-2790", "doi": "10.1016/j.jspi.2009.01.003", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of the adaptive LASSO estimator (Zou (2006)) in\nfinite samples as well as in the large-sample limit. The large-sample\ndistributions are derived both for the case where the adaptive LASSO estimator\nis tuned to perform conservative model selection as well as for the case where\nthe tuning results in consistent model selection. We show that the\nfinite-sample as well as the large-sample distributions are typically highly\nnon-normal, regardless of the choice of the tuning parameter. The uniform\nconvergence rate is also obtained, and is shown to be slower than $n^{-1/2}$ in\ncase the estimator is tuned to perform consistent model selection. In\nparticular, these results question the statistical relevance of the `oracle'\nproperty of the adaptive LASSO estimator established in Zou (2006). Moreover,\nwe also provide an impossibility result regarding the estimation of the\ndistribution function of the adaptive LASSO estimator.The theoretical results,\nwhich are obtained for a regression model with orthogonal design, are\ncomplemented by a Monte Carlo study using non-orthogonal regressors.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 09:57:44 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2008 12:42:12 GMT"}], "update_date": "2009-04-28", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Schneider", "Ulrike", ""]]}, {"id": "0801.4629", "submitter": "Eric Matzner-Lober", "authors": "Pierre Andre Cornillon, Nicolas Hengartner, Eric Matzner-Lober", "title": "Recursive Bias Estimation and $L_2$ Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": null, "abstract": "  This paper presents a general iterative bias correction procedure for\nregression smoothers. This bias reduction schema is shown to correspond\noperationally to the $L_2$ Boosting algorithm and provides a new statistical\ninterpretation for $L_2$ Boosting. We analyze the behavior of the Boosting\nalgorithm applied to common smoothers $S$ which we show depend on the spectrum\nof $I-S$. We present examples of common smoother for which Boosting generates a\ndivergent sequence. The statistical interpretation suggest combining algorithm\nwith an appropriate stopping rule for the iterative procedure. Finally we\nillustrate the practical finite sample performances of the iterative smoother\nvia a simulation study. simulations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 10:22:22 GMT"}], "update_date": "2008-01-31", "authors_parsed": [["Cornillon", "Pierre Andre", ""], ["Hengartner", "Nicolas", ""], ["Matzner-Lober", "Eric", ""]]}]