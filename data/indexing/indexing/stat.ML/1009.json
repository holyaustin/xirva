[{"id": "1009.0499", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin", "title": "A PAC-Bayesian Analysis of Graph Clustering and Pairwise Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate weighted graph clustering as a prediction problem: given a\nsubset of edge weights we analyze the ability of graph clustering to predict\nthe remaining edge weights. This formulation enables practical and theoretical\ncomparison of different approaches to graph clustering as well as comparison of\ngraph clustering with other possible ways to model the graph. We adapt the\nPAC-Bayesian analysis of co-clustering (Seldin and Tishby, 2008; Seldin, 2009)\nto derive a PAC-Bayesian generalization bound for graph clustering. The bound\nshows that graph clustering should optimize a trade-off between empirical data\nfit and the mutual information that clusters preserve on the graph nodes. A\nsimilar trade-off derived from information-theoretic considerations was already\nshown to produce state-of-the-art results in practice (Slonim et al., 2005;\nYom-Tov and Slonim, 2009). This paper supports the empirical evidence by\nproviding a better theoretical foundation, suggesting formal generalization\nguarantees, and offering a more accurate way to deal with finite sample issues.\nWe derive a bound minimization algorithm and show that it provides good results\nin real-life problems and that the derived PAC-Bayesian bound is reasonably\ntight.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 18:28:22 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Seldin", "Yevgeny", ""]]}, {"id": "1009.0530", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, Philipp Rutimann, Min Xu, and Peter Buhlmann", "title": "High-dimensional covariance estimation based on Gaussian graphical\n  models", "comments": "50 Pages, 6 figures. Major revision", "journal-ref": "Journal of Machine Learning Research, Volume 12, pp 2975-3026,\n  2011", "doi": null, "report-no": "University of Michigan, Department of Statistics Technical Report\n  512", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphs are often used to describe high dimensional distributions.\nUnder sparsity conditions, the graph can be estimated using\n$\\ell_1$-penalization methods. We propose and study the following method. We\ncombine a multiple regression approach with ideas of thresholding and\nrefitting: first we infer a sparse undirected graphical model structure via\nthresholding of each among many $\\ell_1$-norm penalized regression functions;\nwe then estimate the covariance matrix and its inverse using the maximum\nlikelihood estimator. We show that under suitable conditions, this approach\nyields consistent estimation in terms of graphical structure and fast\nconvergence rates with respect to the operator and Frobenius norm for the\ncovariance matrix and its inverse. We also derive an explicit bound for the\nKullback Leibler divergence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 20:06:40 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2011 22:49:08 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Zhou", "Shuheng", ""], ["Rutimann", "Philipp", ""], ["Xu", "Min", ""], ["Buhlmann", "Peter", ""]]}, {"id": "1009.0571", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, Martin J.\n  Wainwright", "title": "Information-theoretic lower bounds on the oracle complexity of\n  stochastic convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative to the large literature on upper bounds on complexity of convex\noptimization, lesser attention has been paid to the fundamental hardness of\nthese problems. Given the extensive use of convex optimization in machine\nlearning and statistics, gaining an understanding of these complexity-theoretic\nissues is important. In this paper, we study the complexity of stochastic\nconvex optimization in an oracle model of computation. We improve upon known\nresults and obtain tight minimax complexity estimates for various function\nclasses.\n", "versions": [{"version": "v1", "created": "Fri, 3 Sep 2010 02:49:20 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2011 02:51:27 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2011 07:11:57 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bartlett", "Peter L.", ""], ["Ravikumar", "Pradeep", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1009.0861", "submitter": "Ameet Talwalkar", "authors": "Mehryar Mohri, Ameet Talwalkar", "title": "On the Estimation of Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix approximations are often used to help scale standard machine\nlearning algorithms to large-scale problems. Recently, matrix coherence has\nbeen used to characterize the ability to extract global information from a\nsubset of matrix entries in the context of these low-rank approximations and\nother sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since\ncoherence is defined in terms of the singular vectors of a matrix and is\nexpensive to compute, the practical significance of these results largely\nhinges on the following question: Can we efficiently and accurately estimate\nthe coherence of a matrix? In this paper we address this question. We propose a\nnovel algorithm for estimating coherence from a small number of columns,\nformally analyze its behavior, and derive a new coherence-based matrix\napproximation bound based on this analysis. We then present extensive\nexperimental results on synthetic and real datasets that corroborate our\nworst-case theoretical analysis, yet provide strong support for the use of our\nproposed algorithm whenever low-rank approximation is being considered. Our\nalgorithm efficiently and accurately estimates matrix coherence across a wide\nrange of datasets, and these coherence estimates are excellent predictors of\nthe effectiveness of sampling-based matrix approximation on a case-by-case\nbasis.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 19:18:54 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Mohri", "Mehryar", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1009.1318", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi and Nathalie Villa-Vialaneix", "title": "Optimizing an Organized Modularity Measure for Topographic Graph\n  Clustering: a Deterministic Annealing Approach", "comments": null, "journal-ref": "Neurocomputing, 73(7--9):1142--1163, March 2010", "doi": "10.1016/j.neucom.2009.11.023", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an organized generalization of Newman and Girvan's\nmodularity measure for graph clustering. Optimized via a deterministic\nannealing scheme, this measure produces topologically ordered graph clusterings\nthat lead to faithful and readable graph representations based on clustering\ninduced graphs. Topographic graph clustering provides an alternative to more\nclassical solutions in which a standard graph clustering method is applied to\nbuild a simpler graph that is then represented with a graph layout algorithm. A\ncomparative study on four real world graphs ranging from 34 to 1 133 vertices\nshows the interest of the proposed approach with respect to classical solutions\nand to self-organizing maps for graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 14:57:15 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Rossi", "Fabrice", ""], ["Villa-Vialaneix", "Nathalie", ""]]}, {"id": "1009.1690", "submitter": "Truyen Tran", "authors": "Tran The Truyen, Dinh Q. Phung and Svetha Venkatesh", "title": "Probabilistic Models over Ordered Partitions with Application in\n  Learning to Rank", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the general problem of modelling and learning rank data\nwith ties. We propose a probabilistic generative model, that models the process\nas permutations over partitions. This results in super-exponential\ncombinatorial state space with unknown numbers of partitions and unknown\nordering among them. We approach the problem from the discrete choice theory,\nwhere subsets are chosen in a stagewise manner, reducing the state space per\neach stage significantly. Further, we show that with suitable parameterisation,\nwe can still learn the models in linear time. We evaluate the proposed models\non the problem of learning to rank with the data from the recently held Yahoo!\nchallenge, and demonstrate that the models are competitive against well-known\nrivals.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 06:15:39 GMT"}, {"version": "v2", "created": "Mon, 4 Oct 2010 07:30:32 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Truyen", "Tran The", ""], ["Phung", "Dinh Q.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1009.2009", "submitter": "Truyen Tran", "authors": "Tran The Truyen, Dinh Q. Phung, Hung H. Bui, and Svetha Venkatesh", "title": "Hierarchical Semi-Markov Conditional Random Fields for Recursive\n  Sequential Data", "comments": "56 pages, short version presented at NIPS'08", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the hierarchical hidden Markov models (HHMM), we present the\nhierarchical semi-Markov conditional random field (HSCRF), a generalisation of\nembedded undirectedMarkov chains tomodel complex hierarchical, nestedMarkov\nprocesses. It is parameterised in a discriminative framework and has polynomial\ntime algorithms for learning and inference. Importantly, we consider\npartiallysupervised learning and propose algorithms for generalised\npartially-supervised learning and constrained inference. We demonstrate the\nHSCRF in two applications: (i) recognising human activities of daily living\n(ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show\nthat the HSCRF is capable of learning rich hierarchical models with reasonable\naccuracy in both fully and partially observed data cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 13:25:05 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Truyen", "Tran The", ""], ["Phung", "Dinh Q.", ""], ["Bui", "Hung H.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1009.2139", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton (INRIA Paris - Rocquencourt, LIENS), Julien Mairal\n  (INRIA Paris - Rocquencourt, LIENS), Guillaume Obozinski (INRIA Paris -\n  Rocquencourt, LIENS), Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Proximal Methods for Hierarchical Sparse Coding", "comments": null, "journal-ref": "Journal of Machine Learning Research, 12 (2011) 2297-2334", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding consists in representing signals as sparse linear combinations\nof atoms selected from a dictionary. We consider an extension of this framework\nwhere the atoms are further assumed to be embedded in a tree. This is achieved\nusing a recently introduced tree-structured sparse regularization norm, which\nhas proven useful in several applications. This norm leads to regularized\nproblems that are difficult to optimize, and we propose in this paper efficient\nalgorithms for solving them. More precisely, we show that the proximal operator\nassociated with this norm is computable exactly via a dual approach that can be\nviewed as the composition of elementary proximal operators. Our procedure has a\ncomplexity linear, or close to linear, in the number of atoms, and allows the\nuse of accelerated gradient techniques to solve the tree-structured sparse\napproximation problem at the same computational cost as traditional ones using\nthe L1-norm. Our method is efficient and scales gracefully to millions of\nvariables, which we illustrate in two types of applications: first, we consider\nfixed hierarchical dictionaries of wavelets to denoise natural images. Then, we\napply our optimization tools in the context of dictionary learning, where\nlearned dictionary elements naturally organize in a prespecified arborescent\nstructure, leading to a better performance in reconstruction of natural image\npatches. When applied to text documents, our method learns hierarchies of\ntopics, thus providing a competitive alternative to probabilistic topic models.\n", "versions": [{"version": "v1", "created": "Sat, 11 Sep 2010 05:46:55 GMT"}, {"version": "v2", "created": "Thu, 16 Sep 2010 18:22:27 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2011 16:21:37 GMT"}, {"version": "v4", "created": "Tue, 5 Jul 2011 15:04:02 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Jenatton", "Rodolphe", "", "INRIA Paris - Rocquencourt, LIENS"], ["Mairal", "Julien", "", "INRIA Paris - Rocquencourt, LIENS"], ["Obozinski", "Guillaume", "", "INRIA Paris -\n  Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1009.2646", "submitter": "Ioannis Psorakis", "authors": "Ioannis Psorakis and Stephen Roberts and Ben Sheldon", "title": "Efficient Bayesian Community Detection using Non-negative Matrix\n  Factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": "PARG-10-02", "categories": "stat.ML cond-mat.stat-mech physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying overlapping communities in networks is a challenging task. In\nthis work we present a novel approach to community detection that utilises the\nBayesian non-negative matrix factorisation (NMF) model to produce a\nprobabilistic output for node memberships. The scheme has the advantage of\ncomputational efficiency, soft community membership and an intuitive\nfoundation. We present the performance of the method against a variety of\nbenchmark problems and compare and contrast it to several other algorithms for\ncommunity detection. Our approach performs favourably compared to other methods\nat a fraction of the computational costs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 13:11:14 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 16:47:15 GMT"}, {"version": "v3", "created": "Wed, 22 Sep 2010 17:09:30 GMT"}, {"version": "v4", "created": "Thu, 23 Sep 2010 16:52:15 GMT"}, {"version": "v5", "created": "Sat, 25 Sep 2010 18:10:24 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Psorakis", "Ioannis", ""], ["Roberts", "Stephen", ""], ["Sheldon", "Ben", ""]]}, {"id": "1009.2718", "submitter": "Clayton Scott", "authors": "Clayton Scott", "title": "Calibrated Surrogate Losses for Classification with Label-Dependent\n  Costs", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present surrogate regret bounds for arbitrary surrogate losses in the\ncontext of binary classification with label-dependent costs. Such bounds relate\na classifier's risk, assessed with respect to a surrogate loss, to its\ncost-sensitive classification risk. Two approaches to surrogate regret bounds\nare developed. The first is a direct generalization of Bartlett et al. [2006],\nwho focus on margin-based losses and cost-insensitive classification, while the\nsecond adopts the framework of Steinwart [2007] based on calibration functions.\nNontrivial surrogate regret bounds are shown to exist precisely when the\nsurrogate loss satisfies a \"calibration\" condition that is easily verified for\nmany common losses. We apply this theory to the class of uneven margin losses,\nand characterize when these losses are properly calibrated. The uneven hinge,\nsquared error, exponential, and sigmoid losses are then treated in detail.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 17:02:42 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Scott", "Clayton", ""]]}, {"id": "1009.2722", "submitter": "Myung Jin Choi", "authors": "Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar, Alan S.\n  Willsky", "title": "Learning Latent Tree Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a latent tree graphical model where samples\nare available only from a subset of variables. We propose two consistent and\ncomputationally efficient algorithms for learning minimal latent trees, that\nis, trees without any redundant hidden nodes. Unlike many existing methods, the\nobserved nodes (or variables) are not constrained to be leaf nodes. Our first\nalgorithm, recursive grouping, builds the latent tree recursively by\nidentifying sibling groups using so-called information distances. One of the\nmain contributions of this work is our second algorithm, which we refer to as\nCLGrouping. CLGrouping starts with a pre-processing procedure in which a tree\nover the observed variables is constructed. This global step groups the\nobserved nodes that are likely to be close to each other in the true latent\ntree, thereby guiding subsequent recursive grouping (or equivalent procedures)\non much smaller subsets of variables. This results in more accurate and\nefficient learning of latent trees. We also present regularized versions of our\nalgorithms that learn latent tree approximations of arbitrary distributions. We\ncompare the proposed algorithms to other methods by performing extensive\nnumerical experiments on various latent tree graphical models such as hidden\nMarkov models and star graphs. In addition, we demonstrate the applicability of\nour methods on real-world datasets by modeling the dependency structure of\nmonthly stock returns in the S&P index and of the words in the 20 newsgroups\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 17:37:44 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Choi", "Myung Jin", ""], ["Tan", "Vincent Y. F.", ""], ["Anandkumar", "Animashree", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1009.3601", "submitter": "David Hardoon", "authors": "David R. Hardoon and Kristiaan Pelcksman", "title": "Pair-Wise Cluster Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning clusters which are consistently\npresent in different (continuously valued) representations of observed data.\nOur setup differs slightly from the standard approach of (co-) clustering as we\nuse the fact that some form of `labeling' becomes available in this setup: a\ncluster is only interesting if it has a counterpart in the alternative\nrepresentation. The contribution of this paper is twofold: (i) the problem\nsetting is explored and an analysis in terms of the PAC-Bayesian theorem is\npresented, (ii) a practical kernel-based algorithm is derived exploiting the\ninherent relation to Canonical Correlation Analysis (CCA), as well as its\nextension to multiple views. A content based information retrieval (CBIR) case\nstudy is presented on the multi-lingual aligned Europal document dataset which\nsupports the above findings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 02:28:35 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Hardoon", "David R.", ""], ["Pelcksman", "Kristiaan", ""]]}, {"id": "1009.3669", "submitter": "Michael Finegold", "authors": "Michael Finegold, Mathias Drton", "title": "Robust graphical modeling of gene networks using classical and\n  alternative T-distributions", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS410 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 1057-1080", "doi": "10.1214/10-AOAS410", "report-no": "IMS-AOAS-AOAS410", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models have proven to be useful tools for exploring\nnetwork structures based on multivariate data. Applications to studies of gene\nexpression have generated substantial interest in these models, and resulting\nrecent progress includes the development of fitting methodology involving\npenalization of the likelihood function. In this paper we advocate the use of\nmultivariate $t$-distributions for more robust inference of graphs. In\nparticular, we demonstrate that penalized likelihood inference combined with an\napplication of the EM algorithm provides a computationally efficient approach\nto model selection in the $t$-distribution case. We consider two versions of\nmultivariate $t$-distributions, one of which requires the use of approximation\ntechniques. For this distribution, we describe a Markov chain Monte Carlo EM\nalgorithm based on a Gibbs sampler as well as a simple variational\napproximation that makes the resulting method feasible in large problems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 23:36:42 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 12:46:28 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2011 08:02:29 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Finegold", "Michael", ""], ["Drton", "Mathias", ""]]}, {"id": "1009.3890", "submitter": "Arash Ali Amini", "authors": "Arash Ali Amini, Massoud Babaie-Zadeh, Christian Jutten", "title": "Fast Sparse Decomposition by Iterative Detection-Estimation", "comments": "Unpublished work, sent to IEEE trans. of Sig. Proc. around November\n  or December of 2006 and rejected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding sparse solutions of underdetermined systems of linear equations is a\nfundamental problem in signal processing and statistics which has become a\nsubject of interest in recent years. In general, these systems have infinitely\nmany solutions. However, it may be shown that sufficiently sparse solutions may\nbe identified uniquely. In other words, the corresponding linear transformation\nwill be invertible if we restrict its domain to sufficiently sparse vectors.\nThis property may be used, for example, to solve the underdetermined Blind\nSource Separation (BSS) problem, or to find sparse representation of a signal\nin an `overcomplete' dictionary of primitive elements (i.e., the so-called\natomic decomposition). The main drawback of current methods of finding sparse\nsolutions is their computational complexity. In this paper, we will show that\nby detecting `active' components of the (potential) solution, i.e., those\ncomponents having a considerable value, a framework for fast solution of the\nproblem may be devised. The idea leads to a family of algorithms, called\n`Iterative Detection-Estimation (IDE)', which converge to the solution by\nsuccessive detection and estimation of its active part. Comparing the\nperformance of IDE(s) with one of the most successful method to date, which is\nbased on Linear Programming (LP), an improvement in speed of about two to three\norders of magnitude is observed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 17:12:37 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Amini", "Arash Ali", ""], ["Babaie-Zadeh", "Massoud", ""], ["Jutten", "Christian", ""]]}, {"id": "1009.3958", "submitter": "Konrad Rawlik", "authors": "Konrad Rawlik, Marc Toussaint and Sethu Vijayakumar", "title": "Approximate Inference and Stochastic Optimal Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel reformulation of the stochastic optimal control problem as\nan approximate inference problem, demonstrating, that such a interpretation\nleads to new practical methods for the original problem. In particular we\ncharacterise a novel class of iterative solutions to the stochastic optimal\ncontrol problem based on a natural relaxation of the exact dual formulation.\nThese theoretical insights are applied to the Reinforcement Learning problem\nwhere they lead to new model free, off policy methods for discrete and\ncontinuous problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 21:44:30 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Rawlik", "Konrad", ""], ["Toussaint", "Marc", ""], ["Vijayakumar", "Sethu", ""]]}, {"id": "1009.5358", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Paris - Rocquencourt, LIENS), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt), Jean Ponce (INRIA Paris - Rocquencourt,\n  LIENS)", "title": "Task-Driven Dictionary Learning", "comments": "final draft post-refereeing", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34,\n  4 (2012) 30", "doi": "10.1109/TPAMI.2011.156", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data with linear combinations of a few elements from a learned\ndictionary has been the focus of much recent research in machine learning,\nneuroscience and signal processing. For signals such as natural images that\nadmit such sparse representations, it is now well established that these models\nare well suited to restoration tasks. In this context, learning the dictionary\namounts to solving a large-scale matrix factorization problem, which can be\ndone efficiently with classical optimization tools. The same approach has also\nbeen used for learning features from data for other purposes, e.g., image\nclassification, but tuning the dictionary in a supervised way for these tasks\nhas proven to be more difficult. In this paper, we present a general\nformulation for supervised dictionary learning adapted to a wide variety of\ntasks, and present an efficient algorithm for solving the corresponding\noptimization problem. Experiments on handwritten digit classification, digital\nart identification, nonlinear inverse image problems, and compressed sensing\ndemonstrate that our approach is effective in large-scale settings, and is well\nsuited to supervised and semi-supervised classification, as well as regression\ntasks for data that admit sparse representations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 19:06:49 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2013 11:51:58 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Mairal", "Julien", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Ponce", "Jean", "", "INRIA Paris - Rocquencourt,\n  LIENS"]]}, {"id": "1009.5736", "submitter": "Kenji Fukumizu", "authors": "Kenji Fukumizu, Le Song, and Arthur Gretton", "title": "Kernel Bayes' rule", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric kernel-based method for realizing Bayes' rule is proposed,\nbased on representations of probabilities in reproducing kernel Hilbert spaces.\nProbabilities are uniquely characterized by the mean of the canonical map to\nthe RKHS. The prior and conditional probabilities are expressed in terms of\nRKHS functions of an empirical sample: no explicit parametric model is needed\nfor these quantities. The posterior is likewise an RKHS mean of a weighted\nsample. The estimator for the expectation of a function of the posterior is\nderived, and rates of consistency are shown. Some representative applications\nof the kernel Bayes' rule are presented, including Baysian computation without\nlikelihood and filtering with a nonparametric state-space model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 01:51:49 GMT"}, {"version": "v2", "created": "Fri, 15 Oct 2010 14:24:45 GMT"}, {"version": "v3", "created": "Thu, 11 Nov 2010 04:31:17 GMT"}, {"version": "v4", "created": "Wed, 28 Sep 2011 14:01:48 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Fukumizu", "Kenji", ""], ["Song", "Le", ""], ["Gretton", "Arthur", ""]]}, {"id": "1009.5839", "submitter": "Nicole Kraemer", "authors": "Gilles Blanchard, Nicole Kraemer", "title": "Optimal learning rates for Kernel Conjugate Gradient regression", "comments": "to appear in Neural Information Processing Systems 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove rates of convergence in the statistical sense for kernel-based least\nsquares regression using a conjugate gradient algorithm, where regularization\nagainst overfitting is obtained by early stopping. This method is directly\nrelated to Kernel Partial Least Squares, a regression method that combines\nsupervised dimensionality reduction with least squares projection. The rates\ndepend on two key quantities: first, on the regularity of the target regression\nfunction and second, on the intrinsic dimensionality of the data mapped into\nthe kernel space. Lower bounds on attainable rates depending on these two\nquantities were established in earlier literature, and we obtain upper bounds\nfor the considered method that match these lower bounds (up to a log factor) if\nthe true regression function belongs to the reproducing kernel Hilbert space.\nIf this assumption is not fulfilled, we obtain similar convergence rates\nprovided additional unlabeled data are available. The order of the learning\nrates match state-of-the-art results that were recently obtained for least\nsquares support vector machines and for linear regularization operators.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 11:05:55 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kraemer", "Nicole", ""]]}]