[{"id": "1912.00003", "submitter": "David Zimmerer", "authors": "David Zimmerer, Jens Petersen, Simon A. A. Kohl and Klaus H.\n  Maier-Hein", "title": "A Case for the Score: Identifying Image Anomalies using Variational\n  Autoencoder Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through training on unlabeled data, anomaly detection has the potential to\nimpact computer-aided diagnosis by outlining suspicious regions. Previous work\non deep-learning-based anomaly detection has primarily focused on the\nreconstruction error. We argue instead, that pixel-wise anomaly ratings derived\nfrom a Variational Autoencoder based score approximation yield a theoretically\nbetter grounded and more faithful estimate. In our experiments, Variational\nAutoencoder gradient-based rating outperforms other approaches on unsupervised\npixel-wise tumor detection on the BraTS-2017 dataset with a ROC-AUC of 0.94.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 09:40:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zimmerer", "David", ""], ["Petersen", "Jens", ""], ["Kohl", "Simon A. A.", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1912.00009", "submitter": "Shiyuan Li", "authors": "Shiyuan Li", "title": "MSTDP: A More Biologically Plausible Learning", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-timing dependent plasticity (STDP) which observed in the brain has\nproven to be important in biological learning. On the other hand, artificial\nneural networks use a different way to learn, such as Back-Propagation or\nContrastive Hebbian Learning. In this work, we propose a new framework called\nmstdp that learn almost the same way biological learning use, it only uses STDP\nrules for supervised and unsupervised learning and don' t need a global loss or\nother supervise information. The framework works like an auto-encoder by making\neach input neuron also an output neuron. It can make predictions or generate\npatterns in one model without additional configuration. We also brought a new\niterative inference method using momentum to make the framework more efficient,\nwhich can be used in training and testing phases. Finally, we verified our\nframework on MNIST dataset for classification and generation task.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 05:42:50 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 02:33:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "Shiyuan", ""]]}, {"id": "1912.00013", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Youssef Esstafa (LMB), Bruno\n  Saussereau (LMB)", "title": "Diagnostic checking in FARIMA models with uncorrelated but\n  non-independent error terms", "comments": "arXiv admin note: text overlap with arXiv:1902.03000,\n  arXiv:1910.07213", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of modified portmanteau tests for testing the\nadequacy of FARIMA models under the assumption that the errors are uncorrelated\nbut not necessarily independent (i.e. weak FARIMA). We first study the joint\ndistribution of the least squares estimator and the noise empirical\nautocovariances. We then derive the asymp-totic distribution of residual\nempirical autocovariances and autocorrelations. We deduce the asymptotic\ndistribution of the Ljung-Box (or Box-Pierce) modified portmanteau statistics\nfor weak FARIMA models. We also propose another method based on a\nself-normalization approach to test the adequacy of FARIMA models. Finally some\nsimulation studies are presented to corroborate our theoretical work. An\napplication to the Standard \\& Poor's 500 and Nikkei returns also illustrate\nthe practical relevance of our theoretical results. AMS 2000 subject\nclassifications: Primary 62M10, 62F03, 62F05; secondary 91B84, 62P05.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:51:14 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:40:06 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Esstafa", "Youssef", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1912.00015", "submitter": "Simone Rossi", "authors": "Simone Rossi and Sebastien Marmin and Maurizio Filippone", "title": "Efficient Approximate Inference with Walsh-Hadamard Variational\n  Inference", "comments": "Paper accepted at the 4th Workshop on Bayesian Deep Learning (NeurIPS\n  2019), Vancouver, Canada. arXiv admin note: substantial text overlap with\n  arXiv:1905.11248", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference offers scalable and flexible tools to tackle\nintractable Bayesian inference of modern statistical models like Bayesian\nneural networks and Gaussian processes. For largely over-parameterized models,\nhowever, the over-regularization property of the variational objective makes\nthe application of variational inference challenging. Inspired by the\nliterature on kernel methods, and in particular on structured approximations of\ndistributions of random matrices, this paper proposes Walsh-Hadamard\nVariational Inference, which uses Walsh-Hadamard-based factorization strategies\nto reduce model parameterization, accelerate computations, and increase the\nexpressiveness of the approximate posterior beyond fully factorized ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:22:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Rossi", "Simone", ""], ["Marmin", "Sebastien", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1912.00018", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli, Mert G\\\"urb\\\"uzbalaban, Thanh Huy Nguyen,\n  Ga\\\"el Richard, Levent Sagun", "title": "On the Heavy-Tailed Theory of Stochastic Gradient Descent for Deep\n  Neural Networks", "comments": "32 pages. arXiv admin note: substantial text overlap with\n  arXiv:1901.06053", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is\noften considered to be Gaussian in the large data regime by assuming that the\n\\emph{classical} central limit theorem (CLT) kicks in. This assumption is often\nmade for mathematical convenience, since it enables SGD to be analyzed as a\nstochastic differential equation (SDE) driven by a Brownian motion. We argue\nthat the Gaussianity assumption might fail to hold in deep learning settings\nand hence render the Brownian motion-based analyses inappropriate. Inspired by\nnon-Gaussian natural phenomena, we consider the GN in a more general context\nand invoke the \\emph{generalized} CLT, which suggests that the GN converges to\na \\emph{heavy-tailed} $\\alpha$-stable random vector, where \\emph{tail-index}\n$\\alpha$ determines the heavy-tailedness of the distribution. Accordingly, we\npropose to analyze SGD as a discretization of an SDE driven by a L\\'{e}vy\nmotion. Such SDEs can incur `jumps', which force the SDE and its discretization\n\\emph{transition} from narrow minima to wider minima, as proven by existing\nmetastability theory and the extensions that we proved recently. In this study,\nunder the $\\alpha$-stable GN assumption, we further establish an explicit\nconnection between the convergence rate of SGD to a local minimum and the\ntail-index $\\alpha$. To validate the $\\alpha$-stable assumption, we conduct\nexperiments on common deep learning scenarios and show that in all settings,\nthe GN is highly non-Gaussian and admits heavy-tails. We investigate the tail\nbehavior in varying network architectures and sizes, loss functions, and\ndatasets. Our results open up a different perspective and shed more light on\nthe belief that SGD prefers wide minima.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:56:02 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""], ["G\u00fcrb\u00fczbalaban", "Mert", ""], ["Nguyen", "Thanh Huy", ""], ["Richard", "Ga\u00ebl", ""], ["Sagun", "Levent", ""]]}, {"id": "1912.00020", "submitter": "Tinghao Zhang", "authors": "Tinghao Zhang, Jingxu Li, Jingfeng Li, Ling Wang, Feng Li, Jie Liu", "title": "Model Embedded DRL for Intelligent Greenhouse Control", "comments": "Submitted to AAAI-20 Workshop on Artificial Intelligence of Things", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greenhouse environment is the key to influence crops production. However, it\nis difficult for classical control methods to give precise environment\nsetpoints, such as temperature, humidity, light intensity and carbon dioxide\nconcentration for greenhouse because it is uncertain nonlinear system.\nTherefore, an intelligent close loop control framework based on model embedded\ndeep reinforcement learning (MEDRL) is designed for greenhouse environment\ncontrol. Specifically, computer vision algorithms are used to recognize growing\nperiods and sex of crops, followed by the crop growth models, which can be\ntrained with different growing periods and sex. These model outputs combined\nwith the cost factor provide the setpoints for greenhouse and feedback to the\ncontrol system in real-time. The whole MEDRL system has capability to conduct\noptimization control precisely and conveniently, and costs will be greatly\nreduced compared with traditional greenhouse control approaches.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:25:19 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Tinghao", ""], ["Li", "Jingxu", ""], ["Li", "Jingfeng", ""], ["Wang", "Ling", ""], ["Li", "Feng", ""], ["Liu", "Jie", ""]]}, {"id": "1912.00042", "submitter": "Christina Winkler", "authors": "Christina Winkler, Daniel Worrall, Emiel Hoogeboom, Max Welling", "title": "Learning Likelihoods with Conditional Normalizing Flows", "comments": "18 pages, 8 Tables, 9 Figures, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing Flows (NFs) are able to model complicated distributions p(y) with\nstrong inter-dimensional correlations and high multimodality by transforming a\nsimple base density p(z) through an invertible neural network under the change\nof variables formula. Such behavior is desirable in multivariate structured\nprediction tasks, where handcrafted per-pixel loss-based methods inadequately\ncapture strong correlations between output dimensions. We present a study of\nconditional normalizing flows (CNFs), a class of NFs where the base density to\noutput space mapping is conditioned on an input x, to model conditional\ndensities p(y|x). CNFs are efficient in sampling and inference, they can be\ntrained with a likelihood-based objective, and CNFs, being generative flows, do\nnot suffer from mode collapse or training instabilities. We provide an\neffective method to train continuous CNFs for binary problems and in\nparticular, we apply these CNFs to super-resolution and vessel segmentation\ntasks demonstrating competitive performance on standard benchmark datasets in\nterms of likelihood and conventional metrics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:17:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Winkler", "Christina", ""], ["Worrall", "Daniel", ""], ["Hoogeboom", "Emiel", ""], ["Welling", "Max", ""]]}, {"id": "1912.00043", "submitter": "Serguei Barannikov", "authors": "Serguei Barannikov, Alexander Korotin, Dmitry Oganesyan, Daniil\n  Emtsev, Evgeny Burnaev", "title": "Barcodes as summary of objective function's topology", "comments": "19 pages, description of experiments for calculating barcodes of\n  local minima for benchmark functions is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AT math.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the canonical forms (barcodes) of gradient Morse complexes to\nexplore topology of loss surfaces. We present a novel algorithm for\ncalculations of the objective function's barcodes of local minima. We have\nconducted experiments for calculating barcodes of local minima for benchmark\nfunctions and for loss surfaces of neural networks. Our experiments confirm two\nprincipal observations for loss surfaces of neural networks. First, the\nbarcodes of local minima are located in a small lower part of the range of\nvalues of loss function of neural networks. Second, increase of the neural\nnetwork's depth brings down the barcodes of local minima. This has natural\nimplications for the neural network learning and the generalization ability.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:22:36 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 19:02:35 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Barannikov", "Serguei", ""], ["Korotin", "Alexander", ""], ["Oganesyan", "Dmitry", ""], ["Emtsev", "Daniil", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1912.00049", "submitter": "Maksym Andriushchenko", "authors": "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, Matthias\n  Hein", "title": "Square Attack: a query-efficient black-box adversarial attack via random\n  search", "comments": "Accepted at ECCV 2020; added imperceptible perturbations, analysis of\n  examples that require more queries, results on dilated CNNs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Square Attack, a score-based black-box $l_2$- and\n$l_\\infty$-adversarial attack that does not rely on local gradient information\nand thus is not affected by gradient masking. Square Attack is based on a\nrandomized search scheme which selects localized square-shaped updates at\nrandom positions so that at each iteration the perturbation is situated\napproximately at the boundary of the feasible set. Our method is significantly\nmore query efficient and achieves a higher success rate compared to the\nstate-of-the-art methods, especially in the untargeted setting. In particular,\non ImageNet we improve the average query efficiency in the untargeted setting\nfor various deep networks by a factor of at least $1.8$ and up to $3$ compared\nto the recent state-of-the-art $l_\\infty$-attack of Al-Dujaili & O'Reilly.\nMoreover, although our attack is black-box, it can also outperform\ngradient-based white-box attacks on the standard benchmarks achieving a new\nstate-of-the-art in terms of the success rate. The code of our attack is\navailable at https://github.com/max-andr/square-attack.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:29:32 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:30:48 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 07:53:10 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Andriushchenko", "Maksym", ""], ["Croce", "Francesco", ""], ["Flammarion", "Nicolas", ""], ["Hein", "Matthias", ""]]}, {"id": "1912.00058", "submitter": "Linara Adilova", "authors": "Henning Petzka, Linara Adilova, Michael Kamp, Cristian Sminchisescu", "title": "A Reparameterization-Invariant Flatness Measure for Deep Neural Networks", "comments": "14 pages; accepted at Workshop \"Science meets Engineering of Deep\n  Learning\", 33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks is often attributed to their\nautomated, task-related feature construction. It remains an open question,\nthough, why this leads to solutions with good generalization, even in cases\nwhere the number of parameters is larger than the number of samples. Back in\nthe 90s, Hochreiter and Schmidhuber observed that flatness of the loss surface\naround a local minimum correlates with low generalization error. For several\nflatness measures, this correlation has been empirically validated. However, it\nhas recently been shown that existing measures of flatness cannot theoretically\nbe related to generalization due to a lack of invariance with respect to\nreparameterizations. We propose a natural modification of existing flatness\nmeasures that results in invariance to reparameterization.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 20:05:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Petzka", "Henning", ""], ["Adilova", "Linara", ""], ["Kamp", "Michael", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1912.00071", "submitter": "Kyriakos Polymenakos", "authors": "Kyriakos Polymenakos, Luca Laurenti, Andrea Patane, Jan-Peter\n  Calliess, Luca Cardelli, Marta Kwiatkowska, Alessandro Abate, Stephen Roberts", "title": "Safety Guarantees for Planning Based on Iterative Gaussian Processes", "comments": "An earlier version of this work presented in NeurIPS-2019 Workshop on\n  Safety and Robustness in Decision Making. A shorter (but otherwise\n  equivalent) paper was accepted to the 59th Conference on Decision and Control\n  (CDC2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are widely employed in control and learning because\nof their principled treatment of uncertainty. However, tracking uncertainty for\niterative, multi-step predictions in general leads to an analytically\nintractable problem. While approximation methods exist, they do not come with\nguarantees, making it difficult to estimate their reliability and to trust\ntheir predictions. In this work, we derive formal probability error bounds for\niterative prediction and planning with GPs. Building on GP properties, we bound\nthe probability that random trajectories lie in specific regions around the\npredicted values. Namely, given a tolerance $\\epsilon > 0 $, we compute regions\naround the predicted trajectory values, such that GP trajectories are\nguaranteed to lie inside them with probability at least $1-\\epsilon$. We verify\nexperimentally that our method tracks the predictive uncertainty correctly,\neven when current approximation techniques fail. Furthermore, we show how the\nproposed bounds can be employed within a safe reinforcement learning framework\nto verify the safety of candidate control policies, guiding the synthesis of\nprovably safe controllers.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:13:05 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 19:01:42 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 08:33:10 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Polymenakos", "Kyriakos", ""], ["Laurenti", "Luca", ""], ["Patane", "Andrea", ""], ["Calliess", "Jan-Peter", ""], ["Cardelli", "Luca", ""], ["Kwiatkowska", "Marta", ""], ["Abate", "Alessandro", ""], ["Roberts", "Stephen", ""]]}, {"id": "1912.00074", "submitter": "Pin Wang", "authors": "Pin Wang, Hanhan Li, Ching-Yao Chan", "title": "Quadratic Q-network for Learning Continuous Control for Autonomous\n  Vehicles", "comments": "Machine Learning for Autonomous Driving Workshop on NeurIPS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning algorithms have recently been proposed to learn\ntime-sequential control policies in the field of autonomous driving. Direct\napplications of Reinforcement Learning algorithms with discrete action space\nwill yield unsatisfactory results at the operational level of driving where\ncontinuous control actions are actually required. In addition, the design of\nneural networks often fails to incorporate the domain knowledge of the\ntargeting problem such as the classical control theories in our case. In this\npaper, we propose a hybrid model by combining Q-learning and classic PID\n(Proportion Integration Differentiation) controller for handling continuous\nvehicle control problems under dynamic driving environment. Particularly,\ninstead of using a big neural network as Q-function approximation, we design a\nQuadratic Q-function over actions with multiple simple neural networks for\nfinding optimal values within a continuous space. We also build an action\nnetwork based on the domain knowledge of the control mechanism of a PID\ncontroller to guide the agent to explore optimal actions more efficiently.We\ntest our proposed approach in simulation under two common but challenging\ndriving situations, the lane change scenario and ramp merge scenario. Results\nshow that the autonomous vehicle agent can successfully learn a smooth and\nefficient driving behavior in both situations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:32:32 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Pin", ""], ["Li", "Hanhan", ""], ["Chan", "Ching-Yao", ""]]}, {"id": "1912.00079", "submitter": "Eli (Omid) David", "authors": "Itay Mosafi, Eli David, Nathan S. Netanyahu", "title": "DeepMimic: Mentor-Student Unlabeled Data Based Training", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11731, pp. 440-455, Munich, Germany, September 2019", "doi": "10.1007/978-3-030-30493-5_44", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep neural network (DNN) training approach\ncalled the \"DeepMimic\" training method. Enormous amounts of data are available\nnowadays for training usage. Yet, only a tiny portion of these data is manually\nlabeled, whereas almost all of the data are unlabeled. The training approach\npresented utilizes, in a most simplified manner, the unlabeled data to the\nfullest, in order to achieve remarkable (classification) results. Our DeepMimic\nmethod uses a small portion of labeled data and a large amount of unlabeled\ndata for the training process, as expected in a real-world scenario. It\nconsists of a mentor model and a student model. Employing a mentor model\ntrained on a small portion of the labeled data and then feeding it only with\nunlabeled data, we show how to obtain a (simplified) student model that reaches\nthe same accuracy and loss as the mentor model, on the same test set, without\nusing any of the original data labels in the training of the student model. Our\nexperiments demonstrate that even on challenging classification tasks the\nstudent network architecture can be simplified significantly with a minor\ninfluence on the performance, i.e., we need not even know the original network\narchitecture of the mentor. In addition, the time required for training the\nstudent model to reach the mentor's performance level is shorter, as a result\nof a simplified architecture and more available data. The proposed method\nhighlights the disadvantages of regular supervised training and demonstrates\nthe benefits of a less traditional training approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:31:36 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mosafi", "Itay", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.00106", "submitter": "Siming Ma", "authors": "Siming Ma, David Brooks, Gu-Yeon Wei", "title": "A binary-activation, multi-level weight RNN and training algorithm for\n  ADC-/DAC-free and noise-resilient processing-in-memory inference with eNVM", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training neural networks with binary\nactivations and multi-level weights, which enables efficient\nprocessing-in-memory circuits with embedded nonvolatile memories (eNVM). Binary\nactivations obviate costly DACs and ADCs. Multi-level weights leverage\nmulti-level eNVM cells. Compared to existing algorithms, our method not only\nworks for feed-forward networks (e.g., fully-connected and convolutional), but\nalso achieves higher accuracy and noise resilience for recurrent networks. In\nparticular, we present an RNN-based trigger-word detection PIM accelerator,\nwith detailed hardware noise models and circuit co-design techniques, and\nvalidate our algorithm's high inference accuracy and robustness against a\nvariety of real hardware non-idealities.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:25:51 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 13:26:56 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 13:23:20 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ma", "Siming", ""], ["Brooks", "David", ""], ["Wei", "Gu-Yeon", ""]]}, {"id": "1912.00120", "submitter": "Shunshi Zhang", "authors": "Matthew Shunshi Zhang and Bradly Stadie", "title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the sparse neural network literature have made it possible\nto prune many large feed forward and convolutional networks with only a small\nquantity of data. Yet, these same techniques often falter when applied to the\nproblem of recovering sparse recurrent networks. These failures are\nquantitative: when pruned with recent techniques, RNNs typically obtain worse\nperformance than they do under a simple random pruning scheme. The failures are\nalso qualitative: the distribution of active weights in a pruned LSTM or GRU\nnetwork tend to be concentrated in specific neurons and gates, and not well\ndispersed across the entire architecture. We seek to rectify both the\nquantitative and qualitative issues with recurrent network pruning by\nintroducing a new recurrent pruning objective derived from the spectrum of the\nrecurrent Jacobian. Our objective is data efficient (requiring only 64 data\npoints to prune the network), easy to implement, and produces 95% sparse GRUs\nthat significantly improve on existing baselines. We evaluate on sequential\nMNIST, Billion Words, and Wikitext.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:22:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Matthew Shunshi", ""], ["Stadie", "Bradly", ""]]}, {"id": "1912.00127", "submitter": "Chowdhury Rahman", "authors": "Md. Hasibur Rahman, Chowdhury Rafeed Rahman, Ruhul Amin, Md. Habibur\n  Rahman Sifat and Afra Anika", "title": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-52856-0_36", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 04:00:31 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 02:15:32 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 03:53:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rahman", "Md. Hasibur", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Amin", "Ruhul", ""], ["Sifat", "Md. Habibur Rahman", ""], ["Anika", "Afra", ""]]}, {"id": "1912.00131", "submitter": "Keith Bonawitz", "authors": "Keith Bonawitz, Fariborz Salehi, Jakub Kone\\v{c}n\\'y, Brendan McMahan,\n  Marco Gruteser", "title": "Federated Learning with Autotuned Communication-Efficient Secure\n  Aggregation", "comments": "5 pages, 3 figures. To appear at the IEEE Asilomar Conference on\n  Signals, Systems, and Computers 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning enables mobile devices to collaboratively learn a shared\ninference model while keeping all the training data on a user's device,\ndecoupling the ability to do machine learning from the need to store the data\nin the cloud. Existing work on federated learning with limited communication\ndemonstrates how random rotation can enable users' model updates to be\nquantized much more efficiently, reducing the communication cost between users\nand the server. Meanwhile, secure aggregation enables the server to learn an\naggregate of at least a threshold number of device's model contributions\nwithout observing any individual device's contribution in unaggregated form. In\nthis paper, we highlight some of the challenges of setting the parameters for\nsecure aggregation to achieve communication efficiency, especially in the\ncontext of the aggressively quantized inputs enabled by random rotation. We\nthen develop a recipe for auto-tuning communication-efficient secure\naggregation, based on specific properties of random rotation and secure\naggregation -- namely, the predictable distribution of vector entries\npost-rotation and the modular wrapping inherent in secure aggregation. We\npresent both theoretical results and initial experiments.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 04:27:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bonawitz", "Keith", ""], ["Salehi", "Fariborz", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["McMahan", "Brendan", ""], ["Gruteser", "Marco", ""]]}, {"id": "1912.00134", "submitter": "Rafaela Castro Nascimento", "authors": "Rafaela Castro, Yania M. Souto, Eduardo Ogasawara, Fabio Porto and\n  Eduardo Bezerra", "title": "STConvS2S: Spatiotemporal Convolutional Sequence to Sequence Network for\n  Weather Forecasting", "comments": "Accepted manuscript. Submitted to Neurocomputing, Elsevier", "journal-ref": null, "doi": "10.1016/j.neucom.2020.09.060", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Applying machine learning models to meteorological data brings many\nopportunities to the Geosciences field, such as predicting future weather\nconditions more accurately. In recent years, modeling meteorological data with\ndeep neural networks has become a relevant area of investigation. These works\napply either recurrent neural networks (RNN) or some hybrid approach mixing RNN\nand convolutional neural networks (CNN). In this work, we propose STConvS2S\n(Spatiotemporal Convolutional Sequence to Sequence Network), a deep learning\narchitecture built for learning both spatial and temporal data dependencies\nusing only convolutional layers. Our proposed architecture resolves two\nlimitations of convolutional networks to predict sequences using historical\ndata: (1) they violate the temporal order during the learning process and (2)\nthey require the lengths of the input and output sequences to be equal.\nComputational experiments using air temperature and rainfall data from South\nAmerica show that our architecture captures spatiotemporal context and that it\noutperforms or matches the results of state-of-the-art architectures for\nforecasting tasks. In particular, one of the variants of our proposed\narchitecture is 23% better at predicting future sequences and five times faster\nat training than the RNN-based model used as a baseline.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 05:19:04 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 19:36:53 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 21:07:04 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2020 02:00:23 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Castro", "Rafaela", ""], ["Souto", "Yania M.", ""], ["Ogasawara", "Eduardo", ""], ["Porto", "Fabio", ""], ["Bezerra", "Eduardo", ""]]}, {"id": "1912.00155", "submitter": "Jie Qiao", "authors": "Jie Qiao, Zijian Li, Boyan Xu, Ruichu Cai, Kun Zhang", "title": "Disentanglement Challenge: From Regularization to Reconstruction", "comments": "NeurIPS2019 Disentanglement Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of learning disentangled representation has recently attracted\nmuch attention and boils down to a competition using a new real world\ndisentanglement dataset (Gondal et al., 2019). Various methods based on\nvariational auto-encoder have been proposed to solve this problem, by enforcing\nthe independence between the representation and modifying the regularization\nterm in the variational lower bound. However recent work by Locatello et al.\n(2018) has demonstrated that the proposed methods are heavily influenced by\nrandomness and the choice of the hyper-parameter. In this work, instead of\ndesigning a new regularization term, we adopt the FactorVAE but improve the\nreconstruction performance and increase the capacity of network and the\ntraining step. The strategy turns out to be very effective and achieve the 1st\nplace in the challenge.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 08:01:24 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Qiao", "Jie", ""], ["Li", "Zijian", ""], ["Xu", "Boyan", ""], ["Cai", "Ruichu", ""], ["Zhang", "Kun", ""]]}, {"id": "1912.00163", "submitter": "Gaurav Sinha", "authors": "Gaurav Sinha, Ayush Chauhan, Aurghya Maiti, Naman Poddar, Pulkit Goel", "title": "Dis-entangling Mixture of Interventions on a Causal Bayesian Network\n  Using Aggregate Observations", "comments": "Accepted at the Ninth International Workshop on Statistical\n  Relational AI, AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of separating a mixture of distributions, all of which\ncome from interventions on a known causal bayesian network. Given oracle access\nto marginals of all distributions resulting from interventions on the network,\nand estimates of marginals from the mixture distribution, we want to recover\nthe mixing proportions of different mixture components.\n  We show that in the worst case, mixing proportions cannot be identified using\nmarginals only. If exact marginals of the mixture distribution were known,\nunder a simple assumption of excluding a few distributions from the mixture, we\nshow that the mixing proportions become identifiable. Our identifiability proof\nis constructive and gives an efficient algorithm recovering the mixing\nproportions exactly. When exact marginals are not available, we design an\noptimization framework to estimate the mixing proportions.\n  Our problem is motivated from a real-world scenario of an e-commerce\nbusiness, where multiple interventions occur at a given time, leading to\ndeviations in expected metrics. We conduct experiments on the well known\npublicly available ALARM network and on a proprietary dataset from a large\ne-commerce company validating the performance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 09:36:23 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 11:19:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Sinha", "Gaurav", ""], ["Chauhan", "Ayush", ""], ["Maiti", "Aurghya", ""], ["Poddar", "Naman", ""], ["Goel", "Pulkit", ""]]}, {"id": "1912.00167", "submitter": "Michael Luo Zhiyu", "authors": "Michael Luo, Jiahao Yao, Richard Liaw, Eric Liang, Ion Stoica", "title": "IMPACT: Importance Weighted Asynchronous Architectures with Clipped\n  Target Networks", "comments": "ICLR 2020 Publication; 14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical usage of reinforcement learning agents is often bottlenecked by\nthe duration of training time. To accelerate training, practitioners often turn\nto distributed reinforcement learning architectures to parallelize and\naccelerate the training process. However, modern methods for scalable\nreinforcement learning (RL) often tradeoff between the throughput of samples\nthat an RL agent can learn from (sample throughput) and the quality of learning\nfrom each sample (sample efficiency). In these scalable RL architectures, as\none increases sample throughput (i.e. increasing parallelization in IMPALA),\nsample efficiency drops significantly. To address this, we propose a new\ndistributed reinforcement learning algorithm, IMPACT. IMPACT extends IMPALA\nwith three changes: a target network for stabilizing the surrogate objective, a\ncircular buffer, and truncated importance sampling. In discrete action-space\nenvironments, we show that IMPACT attains higher reward and, simultaneously,\nachieves up to 30% decrease in training wall-time than that of IMPALA. For\ncontinuous control environments, IMPACT trains faster than existing scalable\nagents while preserving the sample efficiency of synchronous PPO.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 09:44:19 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 09:23:15 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 07:30:51 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Luo", "Michael", ""], ["Yao", "Jiahao", ""], ["Liaw", "Richard", ""], ["Liang", "Eric", ""], ["Stoica", "Ion", ""]]}, {"id": "1912.00181", "submitter": "Yang Song", "authors": "Yang Song, Qiyu Kang, and Wee Peng Tay", "title": "Error-Correcting Output Codes with Ensemble Diversity for Robust\n  Learning in Neural Networks", "comments": "Published in Proc. AAAI Conference on Artificial Intelligence, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has been applied successfully in many scenarios,\nmalicious inputs with human-imperceptible perturbations can make it vulnerable\nin real applications. This paper proposes an error-correcting neural network\n(ECNN) that combines a set of binary classifiers to combat adversarial examples\nin the multi-class classification problem. To build an ECNN, we propose to\ndesign a code matrix so that the minimum Hamming distance between any two rows\n(i.e., two codewords) and the minimum shared information distance between any\ntwo columns (i.e., two partitions of class labels) are simultaneously\nmaximized. Maximizing row distances can increase the system fault tolerance\nwhile maximizing column distances helps increase the diversity between binary\nclassifiers. We propose an end-to-end training method for our ECNN, which\nallows further improvement of the diversity between binary classifiers. The\nend-to-end training renders our proposed ECNN different from the traditional\nerror-correcting output code (ECOC) based methods that train binary classifiers\nindependently. ECNN is complementary to other existing defense approaches such\nas adversarial training and can be applied in conjunction with them. We\nempirically demonstrate that our proposed ECNN is effective against the\nstate-of-the-art white-box and black-box attacks on several datasets while\nmaintaining good classification accuracy on normal examples.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 10:32:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 08:28:26 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 03:51:04 GMT"}, {"version": "v4", "created": "Fri, 7 May 2021 08:10:00 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Song", "Yang", ""], ["Kang", "Qiyu", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1912.00183", "submitter": "Isac Arnekvist", "authors": "Isac Arnekvist, Dmytro Kalpakchi", "title": "[Re] Learning to Learn By Self-Critique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a reproducibility study of the paper of Antoniou and Storkey\n[2019], published at NeurIPS 2019. Our results are in parts similar to the ones\nreported in the original paper, supporting the central claim of the paper that\nthe proposed novel method, called Self-Critique and Adapt (SCA), improves the\nperformance of MAML++. The conducted additional experiments on the Caltech-UCSD\nBirds 200 dataset confirm the superiority of SCA compared to MAML++. In\naddition, the reproduced paper suggests a novel high-end version of MAML++ for\nwhich we could not reproduce the same results. We hypothesize that this is due\nto the many implementation details that were omitted in the original paper.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 10:49:35 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 10:39:37 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Arnekvist", "Isac", ""], ["Kalpakchi", "Dmytro", ""]]}, {"id": "1912.00195", "submitter": "Guohao Li", "authors": "Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias M\\\"uller, Ali\n  Thabet, Bernard Ghanem", "title": "SGAS: Sequential Greedy Architecture Search", "comments": "Accepted at CVPR'2020. Project website:\n  https://www.deepgcns.org/auto/sgas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architecture design has become a crucial component of successful deep\nlearning. Recent progress in automatic neural architecture search (NAS) shows a\nlot of promise. However, discovered architectures often fail to generalize in\nthe final evaluation. Architectures with a higher validation accuracy during\nthe search phase may perform worse in the evaluation. Aiming to alleviate this\ncommon issue, we introduce sequential greedy architecture search (SGAS), an\nefficient method for neural architecture search. By dividing the search\nprocedure into sub-problems, SGAS chooses and prunes candidate operations in a\ngreedy fashion. We apply SGAS to search architectures for Convolutional Neural\nNetworks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments\nshow that SGAS is able to find state-of-the-art architectures for tasks such as\nimage classification, point cloud classification and node classification in\nprotein-protein interaction graphs with minimal computational cost. Please\nvisit https://www.deepgcns.org/auto/sgas for more information about SGAS.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 12:39:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 12:55:03 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Li", "Guohao", ""], ["Qian", "Guocheng", ""], ["Delgadillo", "Itzel C.", ""], ["M\u00fcller", "Matthias", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1912.00215", "submitter": "Cinjon Resnick", "authors": "Cinjon Resnick, Zeping Zhan, Joan Bruna", "title": "Probing the State of the Art: A Critical Look at Visual Representation\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised research improved greatly over the past half decade, with\nmuch of the growth being driven by objectives that are hard to quantitatively\ncompare. These techniques include colorization, cyclical consistency, and\nnoise-contrastive estimation from image patches. Consequently, the field has\nsettled on a handful of measurements that depend on linear probes to adjudicate\nwhich approaches are the best. Our first contribution is to show that this test\nis insufficient and that models which perform poorly (strongly) on linear\nclassification can perform strongly (weakly) on more involved tasks like\ntemporal activity localization. Our second contribution is to analyze the\ncapabilities of five different representations. And our third contribution is a\nmuch needed new dataset for temporal activity localization.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 15:05:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Resnick", "Cinjon", ""], ["Zhan", "Zeping", ""], ["Bruna", "Joan", ""]]}, {"id": "1912.00290", "submitter": "Yue Zhao", "authors": "Yue Zhao and Maciej K. Hryniewicki", "title": "XGBOD: Improving Supervised Outlier Detection with Unsupervised\n  Representation Learning", "comments": "Proceedings of the 2018 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489605", "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient\nBoosting Outlier Detection) is proposed, described and demonstrated for the\nenhanced detection of outliers from normal observations in various practical\ndatasets. The proposed framework combines the strengths of both supervised and\nunsupervised machine learning methods by creating a hybrid approach that\nexploits each of their individual performance capabilities in outlier\ndetection. XGBOD uses multiple unsupervised outlier mining algorithms to\nextract useful representations from the underlying data that augment the\npredictive capabilities of an embedded supervised classifier on an improved\nfeature space. The novel approach is shown to provide superior performance in\ncomparison to competing individual detectors, the full ensemble and two\nexisting representation learning based algorithms across seven outlier\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:09:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yue", ""], ["Hryniewicki", "Maciej K.", ""]]}, {"id": "1912.00306", "submitter": "Ezequiel Smucler", "authors": "Andrea Rotnitzky and Ezequiel Smucler", "title": "Efficient adjustment sets for population average treatment effect\n  estimation in non-parametric causal graphical models", "comments": "Fixed a typo in Example 1, an arrow was missing from L1 to Y in the\n  DAG and L1 was missing in the second adjustment set", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of covariate adjustment is often used for estimation of population\naverage treatment effects in observational studies. Graphical rules for\ndetermining all valid covariate adjustment sets from an assumed causal\ngraphical model are well known. Restricting attention to causal linear models,\na recent article derived two novel graphical criteria: one to compare the\nasymptotic variance of linear regression treatment effect estimators that\ncontrol for certain distinct adjustment sets and another to identify the\noptimal adjustment set that yields the least squares treatment effect estimator\nwith the smallest asymptotic variance among consistent adjusted least squares\nestimators. In this paper we show that the same graphical criteria can be used\nin non-parametric causal graphical models when treatment effects are estimated\nby contrasts involving non-parametrically adjusted estimators of the\ninterventional means. We also provide a graphical criterion for determining the\noptimal adjustment set among the minimal adjustment sets, which is valid for\nboth linear and non-parametric estimators. We provide a new graphical criterion\nfor comparing time dependent adjustment sets, that is, sets comprised by\ncovariates that adjust for future treatments and that are themselves affected\nby earlier treatments. We show by example that uniformly optimal time dependent\nadjustment sets do not always exist. In addition, for point interventions, we\nprovide a sound and complete graphical criterion for determining when a\nnon-parametric optimally adjusted estimator of an interventional mean, or of a\ncontrast of interventional means, is as efficient as an efficient estimator of\nthe same parameter that exploits the information in the conditional\nindependencies encoded in the non-parametric causal graphical model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 02:35:53 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 23:24:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Rotnitzky", "Andrea", ""], ["Smucler", "Ezequiel", ""]]}, {"id": "1912.00314", "submitter": "Xiao Zhang", "authors": "Xiao Zhang and Manish Marwah and I-ta Lee and Martin Arlitt and Dan\n  Goldwasser", "title": "ACE -- An Anomaly Contribution Explainer for Cyber-Security Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Anomaly Contribution Explainer or ACE, a tool to\nexplain security anomaly detection models in terms of the model features\nthrough a regression framework, and its variant, ACE-KL, which highlights the\nimportant anomaly contributors. ACE and ACE-KL provide insights in diagnosing\nwhich attributes significantly contribute to an anomaly by building a\nspecialized linear model to locally approximate the anomaly score that a\nblack-box model generates. We conducted experiments with these anomaly\ndetection models to detect security anomalies on both synthetic data and real\ndata. In particular, we evaluate performance on three public data sets: CERT\ninsider threat, netflow logs, and Android malware. The experimental results are\nencouraging: our methods consistently identify the correct contributing feature\nin the synthetic data where ground truth is available; similarly, for real data\nsets, our methods point a security analyst in the direction of the underlying\ncauses of an anomaly, including in one case leading to the discovery of\npreviously overlooked network scanning activity. We have made our source code\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:16:12 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 21:26:15 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhang", "Xiao", ""], ["Marwah", "Manish", ""], ["Lee", "I-ta", ""], ["Arlitt", "Martin", ""], ["Goldwasser", "Dan", ""]]}, {"id": "1912.00315", "submitter": "Hanbaek Lyu", "authors": "Yuchen Guo, Nicholas Hanoian, Zhexiao Lin, Nicholas Liskij, Hanbaek\n  Lyu, Deanna Needell, Jiahao Qu, Henry Sojico, Yuliang Wang, Zhe Xiong,\n  Zhenhong Zou", "title": "Topic-aware chatbot using Recurrent Neural Networks and Nonnegative\n  Matrix Factorization", "comments": "14 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for a topic-aware chatbot by combining the\ntraditional Recurrent Neural Network (RNN) encoder-decoder model with a topic\nattention layer based on Nonnegative Matrix Factorization (NMF). After learning\ntopic vectors from an auxiliary text corpus via NMF, the decoder is trained so\nthat it is more likely to sample response words from the most correlated topic\nvectors. One of the main advantages in our architecture is that the user can\neasily switch the NMF-learned topic vectors so that the chatbot obtains desired\ntopic-awareness. We demonstrate our model by training on a single\nconversational data set which is then augmented with topic matrices learned\nfrom different auxiliary data sets. We show that our topic-aware chatbot not\nonly outperforms the non-topic counterpart, but also that each topic-aware\nmodel qualitatively and contextually gives the most relevant answer depending\non the topic of question.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:22:51 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:28:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Guo", "Yuchen", ""], ["Hanoian", "Nicholas", ""], ["Lin", "Zhexiao", ""], ["Liskij", "Nicholas", ""], ["Lyu", "Hanbaek", ""], ["Needell", "Deanna", ""], ["Qu", "Jiahao", ""], ["Sojico", "Henry", ""], ["Wang", "Yuliang", ""], ["Xiong", "Zhe", ""], ["Zou", "Zhenhong", ""]]}, {"id": "1912.00320", "submitter": "Dongrui Wu", "authors": "Wen Zhang and Dongrui Wu", "title": "Discriminative Joint Probability Maximum Mean Discrepancy (DJP-MMD) for\n  Domain Adaptation", "comments": "Int'l Joint Conf. on Neural Networks (IJCNN), Glasgow, UK, July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum mean discrepancy (MMD) has been widely adopted in domain adaptation\nto measure the discrepancy between the source and target domain distributions.\nMany existing domain adaptation approaches are based on the joint MMD, which is\ncomputed as the (weighted) sum of the marginal distribution discrepancy and the\nconditional distribution discrepancy; however, a more natural metric may be\ntheir joint probability distribution discrepancy. Additionally, most metrics\nonly aim to increase the transferability between domains, but ignores the\ndiscriminability between different classes, which may result in insufficient\nclassification performance. To address these issues, discriminative joint\nprobability MMD (DJP-MMD) is proposed in this paper to replace the\nfrequently-used joint MMD in domain adaptation. It has two desirable\nproperties: 1) it provides a new theoretical basis for computing the\ndistribution discrepancy, which is simpler and more accurate; 2) it increases\nthe transferability and discriminability simultaneously. We validate its\nperformance by embedding it into a joint probability domain adaptation\nframework. Experiments on six image classification datasets demonstrated that\nthe proposed DJP-MMD can outperform traditional MMDs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:52:41 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:47:44 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 08:04:37 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 15:13:57 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Wen", ""], ["Wu", "Dongrui", ""]]}, {"id": "1912.00330", "submitter": "Zhaoyuan Gu", "authors": "Zhaoyuan Gu, Zhenzhong Jia, Howie Choset", "title": "Adversary A3C for Robust Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement\nLearning (RL) algorithm for a wide range of tasks, such as Atari games and\nrobot control. The agent learns policies and value function through\ntrial-and-error interactions with the environment until converging to an\noptimal policy. Robustness and stability are critical in RL; however, neural\nnetwork can be vulnerable to noise from unexpected sources and is not likely to\nwithstand very slight disturbances. We note that agents generated from mild\nenvironment using A3C are not able to handle challenging environments. Learning\nfrom adversarial examples, we proposed an algorithm called Adversary Robust A3C\n(AR-A3C) to improve the agent's performance under noisy environments. In this\nalgorithm, an adversarial agent is introduced to the learning process to make\nit more robust against adversarial disturbances, thereby making it more\nadaptive to noisy environments. Both simulations and real-world experiments are\ncarried out to illustrate the stability of the proposed algorithm. The AR-A3C\nalgorithm outperforms A3C in both clean and noisy environments.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 06:26:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gu", "Zhaoyuan", ""], ["Jia", "Zhenzhong", ""], ["Choset", "Howie", ""]]}, {"id": "1912.00349", "submitter": "Lanqing Xue", "authors": "Lanqing Xue, Xiaopeng Li, Nevin L. Zhang", "title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks generally have fixed network structures, the\nconcept of dynamic mechanism has drawn more and more attention in recent years.\nAttention mechanisms compute input-dependent dynamic attention weights for\naggregating a sequence of hidden states. Dynamic network configuration in\nconvolutional neural networks (CNNs) selectively activates only part of the\nnetwork at a time for different inputs. In this paper, we combine the two\ndynamic mechanisms for text classification tasks. Traditional attention\nmechanisms attend to the whole sequence of hidden states for an input sentence,\nwhile in most cases not all attention is needed especially for long sequences.\nWe propose a novel method called Gated Attention Network (GA-Net) to\ndynamically select a subset of elements to attend to using an auxiliary\nnetwork, and compute attention weights to aggregate the selected elements. It\navoids a significant amount of unnecessary computation on unattended elements,\nand allows the model to pay attention to important parts of the sequence.\nExperiments in various datasets show that the proposed method achieves better\nperformance compared with all baseline models with global or local attention\nwhile requiring less computation and achieving better interpretability. It is\nalso promising to extend the idea to more complex attention-based models, such\nas transformers and seq-to-seq models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 07:57:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Xue", "Lanqing", ""], ["Li", "Xiaopeng", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1912.00350", "submitter": "Defang Chen", "authors": "Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, Chun Chen", "title": "Online Knowledge Distillation with Diverse Peers", "comments": "Accepted to AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distillation is an effective knowledge-transfer technique that uses predicted\ndistributions of a powerful teacher model as soft targets to train a\nless-parameterized student model. A pre-trained high capacity teacher, however,\nis not always available. Recently proposed online variants use the aggregated\nintermediate predictions of multiple student models as targets to train each\nstudent model. Although group-derived targets give a good recipe for\nteacher-free distillation, group members are homogenized quickly with simple\naggregation functions, leading to early saturated solutions. In this work, we\npropose Online Knowledge Distillation with Diverse peers (OKDDip), which\nperforms two-level distillation during training with multiple auxiliary peers\nand one group leader. In the first-level distillation, each auxiliary peer\nholds an individual set of aggregation weights generated with an\nattention-based mechanism to derive its own targets from predictions of other\nauxiliary peers. Learning from distinct target distributions helps to boost\npeer diversity for effectiveness of group-based distillation. The second-level\ndistillation is performed to transfer the knowledge in the ensemble of\nauxiliary peers further to the group leader, i.e., the model used for\ninference. Experimental results show that the proposed framework consistently\ngives better performance than state-of-the-art approaches without sacrificing\ntraining or inference complexity, demonstrating the effectiveness of the\nproposed two-level distillation framework.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 08:19:09 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 11:59:14 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Chen", "Defang", ""], ["Mei", "Jian-Ping", ""], ["Wang", "Can", ""], ["Feng", "Yan", ""], ["Chen", "Chun", ""]]}, {"id": "1912.00354", "submitter": "Farah Shamout", "authors": "Pulkit Sharma, Farah E Shamout, David A Clifton", "title": "Preserving Patient Privacy while Training a Predictive Model of\n  In-hospital Mortality", "comments": "AI for Social Good Workshop, Neurips 2019, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models can be used for pattern recognition in medical data\nin order to improve patient outcomes, such as the prediction of in-hospital\nmortality. Deep learning models, in particular, require large amounts of data\nfor model training. However, the data is often collected at different hospitals\nand sharing is restricted due to patient privacy concerns. In this paper, we\naimed to demonstrate the potential of distributed training in achieving\nstate-of-the-art performance while maintaining data privacy. Our results show\nthat training the model in the federated learning framework leads to comparable\nperformance to the traditional centralised setting. We also suggest several\nconsiderations for the success of such frameworks in future work.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 08:26:13 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sharma", "Pulkit", ""], ["Shamout", "Farah E", ""], ["Clifton", "David A", ""]]}, {"id": "1912.00362", "submitter": "Ke Ma", "authors": "Ke Ma and Jinshan Zeng and Qianqian Xu and Xiaochun Cao and Wei Liu\n  and Yuan Yao", "title": "Fast Stochastic Ordinal Embedding with Variance Reduction and Adaptive\n  Step Size", "comments": "19 pages, 5 figures, accepted by IEEE Transaction on Knowledge and\n  Data Engineering, Conference Version: arXiv:1711.06446", "journal-ref": null, "doi": "10.1109/TKDE.2019.2956700", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation from relative similarity comparisons, often called\nordinal embedding, gains rising attention in recent years. Most of the existing\nmethods are based on semi-definite programming (\\textit{SDP}), which is\ngenerally time-consuming and degrades the scalability, especially confronting\nlarge-scale data. To overcome this challenge, we propose a stochastic algorithm\ncalled \\textit{SVRG-SBB}, which has the following features: i) achieving good\nscalability via dropping positive semi-definite (\\textit{PSD}) constraints as\nserving a fast algorithm, i.e., stochastic variance reduced gradient\n(\\textit{SVRG}) method, and ii) adaptive learning via introducing a new,\nadaptive step size called the stabilized Barzilai-Borwein (\\textit{SBB}) step\nsize. Theoretically, under some natural assumptions, we show the\n$\\boldsymbol{O}(\\frac{1}{T})$ rate of convergence to a stationary point of the\nproposed algorithm, where $T$ is the number of total iterations. Under the\nfurther Polyak-\\L{}ojasiewicz assumption, we can show the global linear\nconvergence (i.e., exponentially fast converging to a global optimum) of the\nproposed algorithm. Numerous simulations and real-world data experiments are\nconducted to show the effectiveness of the proposed algorithm by comparing with\nthe state-of-the-art methods, notably, much lower computational cost with good\nprediction performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 09:05:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ma", "Ke", ""], ["Zeng", "Jinshan", ""], ["Xu", "Qianqian", ""], ["Cao", "Xiaochun", ""], ["Liu", "Wei", ""], ["Yao", "Yuan", ""]]}, {"id": "1912.00370", "submitter": "Mahdi Abolghasemi", "authors": "Mahdi Abolghasemi, Rob J Hyndman, Garth Tarr, Christoph Bergmeir", "title": "Machine learning applications in time series hierarchical forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical forecasting (HF) is needed in many situations in the supply\nchain (SC) because managers often need different levels of forecasts at\ndifferent levels of SC to make a decision. Top-Down (TD), Bottom-Up (BU) and\nOptimal Combination (COM) are common HF models. These approaches are static and\noften ignore the dynamics of the series while disaggregating them.\nConsequently, they may fail to perform well if the investigated group of time\nseries are subject to large changes such as during the periods of promotional\nsales. We address the HF problem of predicting real-world sales time series\nthat are highly impacted by promotion. We use three machine learning (ML)\nmodels to capture sales variations over time. Artificial neural networks (ANN),\nextreme gradient boosting (XGboost), and support vector regression (SVR)\nalgorithms are used to estimate the proportions of lower-level time series from\nthe upper level. We perform an in-depth analysis of 61 groups of time series\nwith different volatilities and show that ML models are competitive and\noutperform some well-established models in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 09:49:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Abolghasemi", "Mahdi", ""], ["Hyndman", "Rob J", ""], ["Tarr", "Garth", ""], ["Bergmeir", "Christoph", ""]]}, {"id": "1912.00385", "submitter": "Ismail Elezi", "authors": "Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello\n  Pelillo, Laura Leal-Taixe", "title": "The Group Loss for Deep Metric Learning", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2020,\n  includes non-archival supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that \"similar objects\nshould belong to the same group\", the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We show state-of-the-art results on clustering and image\nretrieval on several datasets, and show the potential of our method when\ncombined with other techniques such as ensembles\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 11:09:57 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 20:19:30 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 03:32:58 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 17:28:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Elezi", "Ismail", ""], ["Vascon", "Sebastiano", ""], ["Torcinovich", "Alessandro", ""], ["Pelillo", "Marcello", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1912.00402", "submitter": "Shuhan Zhang", "authors": "Shuhan Zhang, Wenlong Lyu, Fan Yang, Changhao Yan, Dian Zhou, Xuan\n  Zeng", "title": "Bayesian Optimization Approach for Analog Circuit Synthesis Using Neural\n  Network", "comments": null, "journal-ref": "2019 Design, Automation & Test in Europe Conference & Exhibition\n  (DATE)", "doi": "10.23919/DATE.2019.8714788", "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian optimization with Gaussian process as surrogate model has been\nsuccessfully applied to analog circuit synthesis. In the traditional Gaussian\nprocess regression model, the kernel functions are defined explicitly. The\ncomputational complexity of training is O(N 3 ), and the computation complexity\nof prediction is O(N 2 ), where N is the number of training data. Gaussian\nprocess model can also be derived from a weight space view, where the original\ndata are mapped to feature space, and the kernel function is defined as the\ninner product of nonlinear features. In this paper, we propose a Bayesian\noptimization approach for analog circuit synthesis using neural network. We use\ndeep neural network to extract good feature representations, and then define\nGaussian process using the extracted features. Model averaging method is\napplied to improve the quality of uncertainty prediction. Compared to Gaussian\nprocess model with explicitly defined kernel functions, the\nneural-network-based Gaussian process model can automatically learn a kernel\nfunction from data, which makes it possible to provide more accurate\npredictions and thus accelerate the follow-up optimization procedure. Also, the\nneural-network-based model has O(N) training time and constant prediction time.\nThe efficiency of the proposed method has been verified by two real-world\nanalog circuits.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 13:13:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Shuhan", ""], ["Lyu", "Wenlong", ""], ["Yang", "Fan", ""], ["Yan", "Changhao", ""], ["Zhou", "Dian", ""], ["Zeng", "Xuan", ""]]}, {"id": "1912.00438", "submitter": "Senthil Yogamani", "authors": "Mohamed Ramzy, Hazem Rashed, Ahmad El Sallab and Senthil Yogamani", "title": "RST-MODNet: Real-time Spatio-temporal Moving Object Detection for\n  Autonomous Driving", "comments": "Accepted for presentation at NeurIPS 2019 Workshop on Machine\n  Learning for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving Object Detection (MOD) is a critical task for autonomous vehicles as\nmoving objects represent higher collision risk than static ones. The trajectory\nof the ego-vehicle is planned based on the future states of detected moving\nobjects. It is quite challenging as the ego-motion has to be modelled and\ncompensated to be able to understand the motion of the surrounding objects. In\nthis work, we propose a real-time end-to-end CNN architecture for MOD utilizing\nspatio-temporal context to improve robustness. We construct a novel time-aware\narchitecture exploiting temporal motion information embedded within sequential\nimages in addition to explicit motion maps using optical flow images.We\ndemonstrate the impact of our algorithm on KITTI dataset where we obtain an\nimprovement of 8% relative to the baselines. We compare our algorithm with\nstate-of-the-art methods and achieve competitive results on KITTI-Motion\ndataset in terms of accuracy at three times better run-time. The proposed\nalgorithm runs at 23 fps on a standard desktop GPU targeting deployment on\nembedded platforms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 16:14:59 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ramzy", "Mohamed", ""], ["Rashed", "Hazem", ""], ["Sallab", "Ahmad El", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1912.00444", "submitter": "Anirudh Srinivasan", "authors": "Anirudh Srinivasan, Dzmitry Bahdanau, Maxime Chevalier-Boisvert and\n  Yoshua Bengio", "title": "Automated curriculum generation for Policy Gradients from Demonstrations", "comments": "Accepted to Deep RL Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a technique that improves the process of training\nan agent (using RL) for instruction following. We develop a training curriculum\nthat uses a nominal number of expert demonstrations and trains the agent in a\nmanner that draws parallels from one of the ways in which humans learn to\nperform complex tasks, i.e by starting from the goal and working backwards. We\ntest our method on the BabyAI platform and show an improvement in sample\nefficiency for some of its tasks compared to a PPO (proximal policy\noptimization) baseline.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 17:08:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Srinivasan", "Anirudh", ""], ["Bahdanau", "Dzmitry", ""], ["Chevalier-Boisvert", "Maxime", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1912.00458", "submitter": "Leena Chennuru Vankadara", "authors": "Leena Chennuru Vankadara and Debarghya Ghoshdastidar", "title": "On the optimality of kernels for high-dimensional clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the optimality of kernel methods in high-dimensional data\nclustering. Recent works have studied the large sample performance of kernel\nclustering in the high-dimensional regime, where Euclidean distance becomes\nless informative. However, it is unknown whether popular methods, such as\nkernel k-means, are optimal in this regime. We consider the problem of\nhigh-dimensional Gaussian clustering and show that, with the exponential kernel\nfunction, the sufficient conditions for partial recovery of clusters using the\nNP-hard kernel k-means objective matches the known information-theoretic limit\nup to a factor of $\\sqrt{2}$ for large $k$. It also exactly matches the known\nupper bounds for the non-kernel setting. We also show that a semi-definite\nrelaxation of the kernel k-means procedure matches up to constant factors, the\nspectral threshold, below which no polynomial-time algorithm is known to\nsucceed. This is the first work that provides such optimality guarantees for\nthe kernel k-means as well as its convex relaxation. Our proofs demonstrate the\nutility of the less known polynomial concentration results for random variables\nwith exponentially decaying tails in a higher-order analysis of kernel methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:05:49 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Vankadara", "Leena Chennuru", ""], ["Ghoshdastidar", "Debarghya", ""]]}, {"id": "1912.00466", "submitter": "Nupur Kumari", "authors": "Tejus Gupta, Abhishek Sinha, Nupur Kumari, Mayank Singh, Balaji\n  Krishnamurthy", "title": "A Method for Computing Class-wise Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for computing class-specific universal adversarial\nperturbations for deep neural networks. Such perturbations can induce\nmisclassification in a large fraction of images of a specific class. Unlike\nprevious methods that use iterative optimization for computing a universal\nperturbation, the proposed method employs a perturbation that is a linear\nfunction of weights of the neural network and hence can be computed much\nfaster. The method does not require any training data and has no\nhyper-parameters. The attack obtains 34% to 51% fooling rate on\nstate-of-the-art deep neural networks on ImageNet and transfers across models.\nWe also study the characteristics of the decision boundaries learned by\nstandard and adversarially trained models to understand the universal\nadversarial perturbations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:22:14 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gupta", "Tejus", ""], ["Sinha", "Abhishek", ""], ["Kumari", "Nupur", ""], ["Singh", "Mayank", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1912.00508", "submitter": "Chang Li", "authors": "Chang Li, Haoyun Feng and Maarten de Rijke", "title": "Cascading Hybrid Bandits: Online Learning to Rank for Relevance and\n  Diversity", "comments": null, "journal-ref": null, "doi": "10.1145/3383313.3412245", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance ranking and result diversification are two core areas in modern\nrecommender systems. Relevance ranking aims at building a ranked list sorted in\ndecreasing order of item relevance, while result diversification focuses on\ngenerating a ranked list of items that covers a broad range of topics. In this\npaper, we study an online learning setting that aims to recommend a ranked list\nwith $K$ items that maximizes the ranking utility, i.e., a list whose items are\nrelevant and whose topics are diverse. We formulate it as the cascade hybrid\nbandits (CHB) problem. CHB assumes the cascading user behavior, where a user\nbrowses the displayed list from top to bottom, clicks the first attractive\nitem, and stops browsing the rest. We propose a hybrid contextual bandit\napproach, called CascadeHybrid, for solving this problem. CascadeHybrid models\nitem relevance and topical diversity using two independent functions and\nsimultaneously learns those functions from user click feedback. We conduct\nexperiments to evaluate CascadeHybrid on two real-world recommendation\ndatasets: MovieLens and Yahoo music datasets. Our experimental results show\nthat CascadeHybrid outperforms the baselines. In addition, we prove theoretical\nguarantees on the $n$-step performance demonstrating the soundness of\nCascadeHybrid.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:03:18 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:57:55 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 06:46:20 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Li", "Chang", ""], ["Feng", "Haoyun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1912.00513", "submitter": "Kai Yang", "authors": "Kai Yang, Tao Fan, Tianjian Chen, Yuanming Shi, Qiang Yang", "title": "A Quasi-Newton Method Based Vertical Federated Learning Framework for\n  Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy and security becomes a major concern in building machine\nlearning models from different data providers. Federated learning shows promise\nby leaving data at providers locally and exchanging encrypted information. This\npaper studies the vertical federated learning structure for logistic regression\nwhere the data sets at two parties have the same sample IDs but own disjoint\nsubsets of features. Existing frameworks adopt the first-order stochastic\ngradient descent algorithm, which requires large number of communication\nrounds. To address the communication challenge, we propose a quasi-Newton\nmethod based vertical federated learning framework for logistic regression\nunder the additively homomorphic encryption scheme. Our approach can\nconsiderably reduce the number of communication rounds with a little additional\ncommunication cost per round. Numerical results demonstrate the advantages of\nour approach over the first-order method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:36:50 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 02:18:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Yang", "Kai", ""], ["Fan", "Tao", ""], ["Chen", "Tianjian", ""], ["Shi", "Yuanming", ""], ["Yang", "Qiang", ""]]}, {"id": "1912.00519", "submitter": "Jayanta Dey", "authors": "Jayanta Dey, Mohammad Ariful Haque", "title": "Location Forensics of Media Recordings Utilizing Cascaded SVM and\n  Pole-matching Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information regarding the location of power distribution grid can be\nextracted from the power signature embedded in the multimedia signals (e.g.,\naudio, video data) recorded near electrical activities. This implicit mechanism\nof identifying the origin-of-recording can be a very promising tool for\nmultimedia forensics and security applications. In this work, we have developed\na novel grid-of-origin identification system from media recording that consists\nof a number of support vector machine (SVM) followed by pole-matching (PM)\nclassifiers. First, we determine the nominal frequency of the grid (50 or 60\nHz) based on the spectral observation. Then an SVM classifier, trained for the\ndetection of a grid with a particular nominal frequency, narrows down the list\nof possible grids on the basis of different discriminating features extracted\nfrom the electric network frequency (ENF) signal. The decision of the SVM\nclassifier is then passed to the PM classifier that detects the final grid\nbased on the minimum distance between the estimated poles of test and training\ngrids. Thus, we start from the problem of classifying grids with different\nnominal frequencies and simplify the problem of classification in three stages\nbased on nominal frequency, SVM and finally using PM classifier. This cascaded\nsystem of classification ensures better accuracy (15.57% higher) compared to\ntraditional ENF-based SVM classifiers described in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 23:20:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Dey", "Jayanta", ""], ["Haque", "Mohammad Ariful", ""]]}, {"id": "1912.00520", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Tatiana Gaintseva, Andrey Ustyuzhanin", "title": "Adaptive Divergence for Rapid Adversarial Optimization", "comments": null, "journal-ref": "PeerJ Computer Science. 2020 May;6:e274", "doi": "10.7717/peerj-cs.274", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Optimization (AO) provides a reliable, practical way to match two\nimplicitly defined distributions, one of which is usually represented by a\nsample of real data, and the other is defined by a generator. Typically, AO\ninvolves training of a high-capacity model on each step of the optimization. In\nthis work, we consider computationally heavy generators, for which training of\nhigh-capacity models is associated with substantial computational costs. To\naddress this problem, we introduce a novel family of divergences, which varies\nthe capacity of the underlying model, and allows for a significant acceleration\nwith respect to the number of samples drawn from the generator. We demonstrate\nthe performance of the proposed divergences on several tasks, including tuning\nparameters of a physics simulator, namely, Pythia event generator.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 23:29:56 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Borisyak", "Maxim", ""], ["Gaintseva", "Tatiana", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1912.00524", "submitter": "Namjoon Suh", "authors": "Namjoon Suh, Xiaoming Huo, Eric Heim, Lee Seversky", "title": "Factor Analysis on Citation, Using a Combined Latent and Logistic\n  Regression Model", "comments": "Citation network, matrix decomposition, latent variable model,\n  logistic regression model, convex optimization, alternating direction method\n  of multiplier", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a combined model, which integrates the latent factor model and the\nlogistic regression model, for the citation network. It is noticed that neither\na latent factor model nor a logistic regression model alone is sufficient to\ncapture the structure of the data. The proposed model has a latent (i.e.,\nfactor analysis) model to represents the main technological trends (a.k.a.,\nfactors), and adds a sparse component that captures the remaining ad-hoc\ndependence. Parameter estimation is carried out through the construction of a\njoint-likelihood function of edges and properly chosen penalty terms. The\nconvexity of the objective function allows us to develop an efficient\nalgorithm, while the penalty terms push towards a low-dimensional latent\ncomponent and a sparse graphical structure. Simulation results show that the\nproposed method works well in practical situations. The proposed method has\nbeen applied to a real application, which contains a citation network of\nstatisticians (Ji and Jin, 2016). Some interesting findings are reported.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 00:04:10 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Suh", "Namjoon", ""], ["Huo", "Xiaoming", ""], ["Heim", "Eric", ""], ["Seversky", "Lee", ""]]}, {"id": "1912.00528", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji and Behnam Neyshabur and Hanie Sedghi", "title": "The intriguing role of module criticality in the generalization of deep\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the phenomenon that some modules of deep neural networks (DNNs) are\nmore critical than others. Meaning that rewinding their parameter values back\nto initialization, while keeping other modules fixed at the trained parameters,\nresults in a large drop in the network's performance. Our analysis reveals\ninteresting properties of the loss landscape which leads us to propose a\ncomplexity measure, called module criticality, based on the shape of the\nvalleys that connects the initial and final values of the module parameters. We\nformulate how generalization relates to the module criticality, and show that\nthis measure is able to explain the superior generalization performance of some\narchitectures over others, whereas earlier measures fail to do so.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 00:27:26 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 18:58:50 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 20:39:53 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Neyshabur", "Behnam", ""], ["Sedghi", "Hanie", ""]]}, {"id": "1912.00536", "submitter": "Bhagya Hettige", "authors": "Bhagya Hettige, Yuan-Fang Li, Weiqing Wang and Wray Buntine", "title": "Gaussian Embedding of Large-scale Attributed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding methods transform high-dimensional and complex graph contents\ninto low-dimensional representations. They are useful for a wide range of graph\nanalysis tasks including link prediction, node classification, recommendation\nand visualization. Most existing approaches represent graph nodes as point\nvectors in a low-dimensional embedding space, ignoring the uncertainty present\nin the real-world graphs. Furthermore, many real-world graphs are large-scale\nand rich in content (e.g. node attributes). In this work, we propose GLACE, a\nnovel, scalable graph embedding method that preserves both graph structure and\nnode attributes effectively and efficiently in an end-to-end manner. GLACE\neffectively models uncertainty through Gaussian embeddings, and supports\ninductive inference of new nodes based on their attributes. In our\ncomprehensive experiments, we evaluate GLACE on real-world graphs, and the\nresults demonstrate that GLACE significantly outperforms state-of-the-art\nembedding methods on multiple graph analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:28:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hettige", "Bhagya", ""], ["Li", "Yuan-Fang", ""], ["Wang", "Weiqing", ""], ["Buntine", "Wray", ""]]}, {"id": "1912.00537", "submitter": "San Gultekin", "authors": "San Gultekin and John Paisley", "title": "Risk Bounds for Low Cost Bipartite Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite ranking is an important supervised learning problem; however,\nunlike regression or classification, it has a quadratic dependence on the\nnumber of samples. To circumvent the prohibitive sample cost, many recent work\nfocus on stochastic gradient-based methods. In this paper we consider an\nalternative approach, which leverages the structure of the widely-adopted\npairwise squared loss, to obtain a stochastic and low cost algorithm that does\nnot require stochastic gradients or learning rates. Using a novel uniform risk\nbound---based on matrix and vector concentration inequalities---we show that\nthe sample size required for competitive performance against the all-pairs\nbatch algorithm does not have a quadratic dependence. Generalization bounds for\nboth the batch and low cost stochastic algorithms are presented. Experimental\nresults show significant speed gain against the batch algorithm, as well as\ncompetitive performance against state-of-the-art bipartite ranking algorithms\non real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:35:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gultekin", "San", ""], ["Paisley", "John", ""]]}, {"id": "1912.00543", "submitter": "Eric Chen", "authors": "Puyang Wang, Eric Z. Chen, Terrence Chen, Vishal M. Patel, Shanhui Sun", "title": "Pyramid Convolutional RNN for MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate MRI image reconstruction from undersampled data is\ncritically important in clinical practice. Compressed sensing based methods are\nwidely used in image reconstruction but the speed is slow due to the iterative\nalgorithms. Deep learning based methods have shown promising advances in recent\nyears. However, recovering the fine details from highly undersampled data is\nstill challenging. In this paper, we introduce a novel deep learning-based\nmethod, Pyramid Convolutional RNN (PC-RNN), to reconstruct the image from\nmultiple scales. We evaluated our model on the fastMRI dataset and the results\nshow that the proposed model achieves significant improvements than other\nmethods and can recover more fine details.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:06:46 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:50:17 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 14:17:57 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 02:18:32 GMT"}, {"version": "v5", "created": "Mon, 27 Apr 2020 18:18:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Puyang", ""], ["Chen", "Eric Z.", ""], ["Chen", "Terrence", ""], ["Patel", "Vishal M.", ""], ["Sun", "Shanhui", ""]]}, {"id": "1912.00552", "submitter": "Yang Ye", "authors": "Yang Ye, and Shihao Ji", "title": "Sparse Graph Attention Networks", "comments": "Published as a journal paper at IEEE TKDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have proved to be an effective representation\nlearning framework for graph-structured data, and have achieved\nstate-of-the-art performance on many practical predictive tasks, such as node\nclassification, link prediction and graph classification. Among the variants of\nGNNs, Graph Attention Networks (GATs) learn to assign dense attention\ncoefficients over all neighbors of a node for feature aggregation, and improve\nthe performance of many graph learning tasks. However, real-world graphs are\noften very large and noisy, and GATs are prone to overfitting if not\nregularized properly. Even worse, the local aggregation mechanism of GATs may\nfail on disassortative graphs, where nodes within local neighborhood provide\nmore noise than useful information for feature aggregation. In this paper, we\npropose Sparse Graph Attention Networks (SGATs) that learn sparse attention\ncoefficients under an $L_0$-norm regularization, and the learned sparse\nattentions are then used for all GNN layers, resulting in an edge-sparsified\ngraph. By doing so, we can identify noisy/task-irrelevant edges, and thus\nperform feature aggregation on most informative neighbors. Extensive\nexperiments on synthetic and real-world graph learning benchmarks demonstrate\nthe superior performance of SGATs. In particular, SGATs can remove about\n50\\%-80\\% edges from large assortative graphs, while retaining similar\nclassification accuracies. On disassortative graphs, SGATs prune majority of\nnoisy edges and outperform GATs in classification accuracies by significant\nmargins. Furthermore, the removed edges can be interpreted intuitively and\nquantitatively. To the best of our knowledge, this is the first graph learning\nalgorithm that shows significant redundancies in graphs and edge-sparsified\ngraphs can achieve similar or sometimes higher predictive performances than\noriginal graphs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:25:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 02:54:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ye", "Yang", ""], ["Ji", "Shihao", ""]]}, {"id": "1912.00574", "submitter": "Zhaoyang Lyu", "authors": "Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, Luca\n  Daniel", "title": "Fastened CROWN: Tightened Neural Network Robustness Certificates", "comments": "Zhaoyang Lyu and Ching-Yun Ko contributed equally, accepted to AAAI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of deep learning applications in real life is accompanied by\nsevere safety concerns. To mitigate this uneasy phenomenon, much research has\nbeen done providing reliable evaluations of the fragility level in different\ndeep neural networks. Apart from devising adversarial attacks, quantifiers that\ncertify safeguarded regions have also been designed in the past five years. The\nsummarizing work of Salman et al. unifies a family of existing verifiers under\na convex relaxation framework. We draw inspiration from such work and further\ndemonstrate the optimality of deterministic CROWN (Zhang et al. 2018) solutions\nin a given linear programming problem under mild constraints. Given this\ntheoretical result, the computationally expensive linear programming based\nmethod is shown to be unnecessary. We then propose an optimization-based\napproach \\textit{FROWN} (\\textbf{F}astened C\\textbf{ROWN}): a general algorithm\nto tighten robustness certificates for neural networks. Extensive experiments\non various networks trained individually verify the effectiveness of FROWN in\nsafeguarding larger robust regions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:54:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Lyu", "Zhaoyang", ""], ["Ko", "Ching-Yun", ""], ["Kong", "Zhifeng", ""], ["Wong", "Ngai", ""], ["Lin", "Dahua", ""], ["Daniel", "Luca", ""]]}, {"id": "1912.00589", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai,\n  Ying Nian Wu", "title": "Flow Contrastive Estimation of Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a training method to jointly estimate an energy-based\nmodel and a flow-based model, in which the two models are iteratively updated\nbased on a shared adversarial value function. This joint training method has\nthe following traits. (1) The update of the energy-based model is based on\nnoise contrastive estimation, with the flow model serving as a strong noise\ndistribution. (2) The update of the flow model approximately minimizes the\nJensen-Shannon divergence between the flow model and the data distribution. (3)\nUnlike generative adversarial networks (GAN) which estimates an implicit\nprobability distribution defined by a generator model, our method estimates two\nexplicit probabilistic distributions on the data. Using the proposed method we\ndemonstrate a significant improvement on the synthesis quality of the flow\nmodel, and show the effectiveness of unsupervised feature learning by the\nlearned energy-based model. Furthermore, the proposed training method can be\neasily adapted to semi-supervised learning. We achieve competitive results to\nthe state-of-the-art semi-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 06:29:36 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:53:41 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gao", "Ruiqi", ""], ["Nijkamp", "Erik", ""], ["Kingma", "Diederik P.", ""], ["Xu", "Zhen", ""], ["Dai", "Andrew M.", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1912.00594", "submitter": "Shuang Song", "authors": "Shuang Song, David Berthelot, Afshin Rostamizadeh", "title": "Combining MixMatch and Active Learning for Better Accuracy with Fewer\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using active learning based techniques to further improve the\nstate-of-the-art semi-supervised learning MixMatch algorithm. We provide a\nthorough empirical evaluation of several active-learning and baseline methods,\nwhich successfully demonstrate a significant improvement on the benchmark\nCIFAR-10, CIFAR-100, and SVHN datasets (as much as 1.5% in absolute accuracy).\nWe also provide an empirical analysis of the cost trade-off between\nincrementally gathering more labeled versus unlabeled data. This analysis can\nbe used to measure the relative value of labeled/unlabeled data at different\npoints of the learning curve, where we find that although the incremental value\nof labeled data can be as much as 20x that of unlabeled, it quickly diminishes\nto less than 3x once more than 2,000 labeled example are observed. Code can be\nfound at https://github.com/google-research/mma.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 06:39:47 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 01:57:47 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Song", "Shuang", ""], ["Berthelot", "David", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1912.00602", "submitter": "Chunnnan Wang", "authors": "Chunnan Wang, Hongzhi Wang, Chang Zhou, Hanxiao Chen", "title": "ExperienceThinking: Constrained Hyperparameter Optimization based on\n  Knowledge and Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are very sensitive to the hyperparameters, and\ntheir evaluations are generally expensive. Users desperately need intelligent\nmethods to quickly optimize hyperparameter settings according to known\nevaluation information, and thus reduce computational cost and promote\noptimization efficiency. Motivated by this, we propose ExperienceThinking\nalgorithm to quickly find the best possible hyperparameter configuration of\nmachine learning algorithms within a few configuration evaluations.\nExperienceThinking design two novel methods, which intelligently infer optimal\nconfigurations from two aspects: search space pruning and knowledge utilization\nrespectively. Two methods complement each other and solve the constrained\nhyperparameter optimization problems effectively. To demonstrate the benefit of\nExperienceThinking, we compare it with 3 classical hyperparameter optimization\nalgorithms with a small number of configuration evaluations. The experimental\nresults present that our proposed algorithm provides superior results and\nachieve better performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:21:05 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 05:48:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Chunnan", ""], ["Wang", "Hongzhi", ""], ["Zhou", "Chang", ""], ["Chen", "Hanxiao", ""]]}, {"id": "1912.00636", "submitter": "Vrettos Moulos", "authors": "Vrettos Moulos", "title": "Optimal Best Markovian Arm Identification with Fixed Confidence", "comments": "Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of the sampling complexity of best\nMarkovian arm identification in one-parameter Markovian bandit models. We\nderive instance specific nonasymptotic and asymptotic lower bounds which\ngeneralize those of the IID setting. We analyze the Track-and-Stop strategy,\ninitially proposed for the IID setting, and we prove that asymptotically it is\nat most a factor of four apart from the lower bound. Our one-parameter\nMarkovian bandit model is based on the notion of an exponential family of\nstochastic matrices for which we establish many useful properties. For the\nanalysis of the Track-and-Stop strategy we derive a novel concentration\ninequality for Markov chains that may be of interest in its own right.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:54:55 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 07:15:56 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 07:36:17 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Moulos", "Vrettos", ""]]}, {"id": "1912.00643", "submitter": "Shreeviknesh Sankaran", "authors": "Sukavanan Nanjundan, Shreeviknesh Sankaran, C.R. Arjun, G. Paavai\n  Anand", "title": "Identifying the number of clusters for K-Means: A hypersphere density\n  based approach", "comments": "5 pages, 13 figures, International Conference on Computers,\n  Communication and Signal Processing - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of K-Means algorithm is restricted by the fact that the number of\nclusters should be known beforehand. Previously suggested methods to solve this\nproblem are either ad hoc or require parametric assumptions and complicated\ncalculations. The proposed method aims to solve this conundrum by considering\ncluster hypersphere density as the factor to determine the number of clusters\nin the given dataset. The density is calculated by assuming a hypersphere\naround the cluster centroid for n-different number of clusters. The calculated\nvalues are plotted against their corresponding number of clusters and then the\noptimum number of clusters is obtained after assaying the elbow region of the\ngraph. The method is simple, easy to comprehend, and provides robust and\nreliable results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:12:15 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 17:37:55 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nanjundan", "Sukavanan", ""], ["Sankaran", "Shreeviknesh", ""], ["Arjun", "C. R.", ""], ["Anand", "G. Paavai", ""]]}, {"id": "1912.00646", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Rob Brekelmans, Daniel Moyer, Greg Ver Steeg, Wael\n  AbdAlmageed, Premkumar Natarajan", "title": "Discovery and Separation of Features for Invariant Representation\n  Learning", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models often associate irrelevant nuisance\nfactors with the prediction target, which hurts generalization. We propose a\nframework for training robust neural networks that induces invariance to\nnuisances through learning to discover and separate predictive and nuisance\nfactors of data. We present an information theoretic formulation of our\napproach, from which we derive training objectives and its connections with\nprevious methods. Empirical results on a wide array of datasets show that the\nproposed framework achieves state-of-the-art performance, without requiring\nnuisance annotations during training.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:17:32 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Brekelmans", "Rob", ""], ["Moyer", "Daniel", ""], ["Steeg", "Greg Ver", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1912.00650", "submitter": "Chunlin Ji", "authors": "Chunlin Ji and Haige Shen", "title": "Stochastic Variational Inference via Upper Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) plays a key role in Bayesian deep\nlearning. Recently various divergences have been proposed to design the\nsurrogate loss for variational inference. We present a simple upper bound of\nthe evidence as the surrogate loss. This evidence upper bound (EUBO) equals to\nthe log marginal likelihood plus the KL-divergence between the posterior and\nthe proposal. We show that the proposed EUBO is tighter than previous upper\nbounds introduced by $\\chi$-divergence or $\\alpha$-divergence. To facilitate\nscalable inference, we present the numerical approximation of the gradient of\nthe EUBO and apply the SGD algorithm to optimize the variational parameters\niteratively. Simulation study with Bayesian logistic regression shows that the\nupper and lower bounds well sandwich the evidence and the proposed upper bound\nis favorably tight. For Bayesian neural network, the proposed EUBO-VI algorithm\noutperforms state-of-the-art results for various examples.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:30:40 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ji", "Chunlin", ""], ["Shen", "Haige", ""]]}, {"id": "1912.00656", "submitter": "Niklas Heim", "authors": "Niklas Heim, V\\'aclav \\v{S}m\\'idl, Tom\\'a\\v{s} Pevn\\'y", "title": "Rodent: Relevance determination in differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to identify the generating, ordinary differential equation (ODE) from\na set of trajectories of a partially observed system. Our approach does not\nneed prescribed basis functions to learn the ODE model, but only a rich set of\nNeural Arithmetic Units. For maximal explainability of the learnt model, we\nminimise the state size of the ODE as well as the number of non-zero parameters\nthat are needed to solve the problem. This sparsification is realized through a\ncombination of the Variational Auto-Encoder (VAE) and Automatic Relevance\nDetermination (ARD). We show that it is possible to learn not only one specific\nmodel for a single process, but a manifold of models representing harmonic\nsignals as well as a manifold of Lotka-Volterra systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:56:43 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 11:05:28 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Heim", "Niklas", ""], ["\u0160m\u00eddl", "V\u00e1clav", ""], ["Pevn\u00fd", "Tom\u00e1\u0161", ""]]}, {"id": "1912.00662", "submitter": "Javier Fernandez", "authors": "Javier Fernandez-Anakabe, Ekhi Zugasti Uriguen and Urko Zurutuza\n  Ortega", "title": "An Attribute Oriented Induction based Methodology for Data Driven\n  Predictive Maintenance", "comments": "Submitted to Journal of Intelligent Manufacturing, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute Oriented Induction (AOI) is a data mining algorithm used for\nextracting knowledge of relational data, taking into account expert knowledge.\nIt is a clustering algorithm that works by transforming the values of the\nattributes and converting an instance into others that are more generic or\nambiguous. In this way, it seeks similarities between elements to generate data\ngroupings. AOI was initially conceived as an algorithm for knowledge discovery\nin databases, but over the years it has been applied to other areas such as\nspatial patterns, intrusion detection or strategy making. In this paper, AOI\nhas been extended to the field of Predictive Maintenance. The objective is to\ndemonstrate that combining expert knowledge and data collected from the machine\ncan provide good results in the Predictive Maintenance of industrial assets. To\nthis end we adapted the algorithm and used an LSTM approach to perform both the\nAnomaly Detection (AD) and the Remaining Useful Life (RUL). The results\nobtained confirm the validity of the proposal, as the methodology was able to\ndetect anomalies, and calculate the RUL until breakage with considerable degree\nof accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:03:19 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fernandez-Anakabe", "Javier", ""], ["Uriguen", "Ekhi Zugasti", ""], ["Ortega", "Urko Zurutuza", ""]]}, {"id": "1912.00666", "submitter": "Martino Centonze", "authors": "Francesco Alemanno, Martino Centonze, Alberto Fachechi", "title": "Interpolating between boolean and extremely high noisy patterns through\n  Minimal Dense Associative Memories", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": "10.1088/1751-8121/ab6943", "report-no": null, "categories": "cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Hopfield and Krotov introduced the concept of {\\em dense\nassociative memories} [DAM] (close to spin-glasses with $P$-wise interactions\nin a disordered statistical mechanical jargon): they proved a number of\nremarkable features these networks share and suggested their use to (partially)\nexplain the success of the new generation of Artificial Intelligence. Thanks to\na remarkable ante-litteram analysis by Baldi \\& Venkatesh, among these\nproperties, it is known these networks can handle a maximal amount of stored\npatterns $K$ scaling as $K \\sim N^{P-1}$.\\\\ In this paper, once introduced a\n{\\em minimal dense associative network} as one of the most elementary\ncost-functions falling in this class of DAM, we sacrifice this high-load regime\n-namely we force the storage of {\\em solely} a linear amount of patterns, i.e.\n$K = \\alpha N$ (with $\\alpha>0$)- to prove that, in this regime, these networks\ncan correctly perform pattern recognition even if pattern signal is $O(1)$ and\nis embedded in a sea of noise $O(\\sqrt{N})$, also in the large $N$ limit. To\nprove this statement, by extremizing the quenched free-energy of the model over\nits natural order-parameters (the various magnetizations and overlaps), we\nderived its phase diagram, at the replica symmetric level of description and in\nthe thermodynamic limit: as a sideline, we stress that, to achieve this task,\naiming at cross-fertilization among disciplines, we pave two hegemon routes in\nthe statistical mechanics of spin glasses, namely the replica trick and the\ninterpolation technique.\\\\ Both the approaches reach the same conclusion: there\nis a not-empty region, in the noise-$T$ vs load-$\\alpha$ phase diagram plane,\nwhere these networks can actually work in this challenging regime; in\nparticular we obtained a quite high critical (linear) load in the (fast)\nnoiseless case resulting in $\\lim_{\\beta \\to \\infty}\\alpha_c(\\beta)=0.65$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:15:29 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alemanno", "Francesco", ""], ["Centonze", "Martino", ""], ["Fachechi", "Alberto", ""]]}, {"id": "1912.00671", "submitter": "Tim Sullivan", "authors": "Ilja Klebanov and Ingmar Schuster and T. J. Sullivan", "title": "A Rigorous Theory of Conditional Mean Embeddings", "comments": "30 pages, 3 figures", "journal-ref": "SIAM Journal on Mathematics of Data Science 2(3):583--606, 2020", "doi": "10.1137/19M1305069", "report-no": null, "categories": "math.ST math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional mean embeddings (CMEs) have proven themselves to be a powerful\ntool in many machine learning applications. They allow the efficient\nconditioning of probability distributions within the corresponding reproducing\nkernel Hilbert spaces (RKHSs) by providing a linear-algebraic relation for the\nkernel mean embeddings of the respective joint and conditional probability\ndistributions. Both centred and uncentred covariance operators have been used\nto define CMEs in the existing literature. In this paper, we develop a\nmathematically rigorous theory for both variants, discuss the merits and\nproblems of each, and significantly weaken the conditions for applicability of\nCMEs. In the course of this, we demonstrate a beautiful connection to Gaussian\nconditioning in Hilbert spaces.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:30:21 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:51:09 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 15:26:51 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 14:04:20 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Klebanov", "Ilja", ""], ["Schuster", "Ingmar", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1912.00672", "submitter": "Zina Ibrahim", "authors": "Zina Ibrahim and Honghan Wu and Ahmed Hamoud and Lukas Stappen and\n  Richard Dobson and Andrea Agarossi", "title": "On Classifying Sepsis Heterogeneity in the ICU: Insight Using Machine\n  Learning", "comments": "3 Figures and 2 tables. Accepted for publication at the Journal of\n  American Medical Informatics Association", "journal-ref": "Journal of the American Medical Informatics Association 27 (2020)\n  437-443", "doi": "10.1093/jamia/ocz211", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current machine learning models aiming to predict sepsis from Electronic\nHealth Records (EHR) do not account for the heterogeneity of the condition,\ndespite its emerging importance in prognosis and treatment. This work\ndemonstrates the added value of stratifying the types of organ dysfunction\nobserved in patients who develop sepsis in the ICU in improving the ability to\nrecognise patients at risk of sepsis from their EHR data. Using an ICU dataset\nof 13,728 records, we identify clinically significant sepsis subpopulations\nwith distinct organ dysfunction patterns. Classification experiments using\nRandom Forest, Gradient Boost Trees and Support Vector Machines, aiming to\ndistinguish patients who develop sepsis in the ICU from those who do not, show\nthat features selected using sepsis subpopulations as background knowledge\nyield a superior performance regardless of the classification model used. Our\nfindings can steer machine learning efforts towards more personalised models\nfor complex conditions including sepsis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:32:40 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 12:42:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ibrahim", "Zina", ""], ["Wu", "Honghan", ""], ["Hamoud", "Ahmed", ""], ["Stappen", "Lukas", ""], ["Dobson", "Richard", ""], ["Agarossi", "Andrea", ""]]}, {"id": "1912.00673", "submitter": "Henry Howard-Jenkins", "authors": "Henry Howard-Jenkins, Yiwen Li, Victor A. Prisacariu", "title": "GroSS: Group-Size Series Decomposition for Grouped Architecture Search", "comments": "Accepted for publication at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach which is able to explore the configuration of\ngrouped convolutions within neural networks. Group-size Series (GroSS)\ndecomposition is a mathematical formulation of tensor factorisation into a\nseries of approximations of increasing rank terms. GroSS allows for dynamic and\ndifferentiable selection of factorisation rank, which is analogous to a grouped\nconvolution. Therefore, to the best of our knowledge, GroSS is the first method\nto enable simultaneous training of differing numbers of groups within a single\nlayer, as well as all possible combinations between layers. In doing so, GroSS\nis able to train an entire grouped convolution architecture search-space\nconcurrently. We demonstrate this through architecture searches with\nperformance objectives on multiple datasets and networks. GroSS enables more\neffective and efficient search for grouped convolutional architectures.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:32:50 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 12:26:25 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 16:28:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Howard-Jenkins", "Henry", ""], ["Li", "Yiwen", ""], ["Prisacariu", "Victor A.", ""]]}, {"id": "1912.00682", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Rodolphe Vadaine, Guillaume Hajduch, Ren\\'e Garello, and\n  Ronan Fablet", "title": "GeoTrackNet-A Maritime Anomaly Detector using Probabilistic Neural\n  Network Representation of AIS Tracks and A Contrario Detection", "comments": "IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3055614", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing maritime traffic patterns and detecting anomalies from them are\nkey to vessel monitoring and maritime situational awareness. We propose a novel\napproach -- referred to as GeoTrackNet -- for maritime anomaly detection from\nAIS data streams. Our model exploits state-of-the-art neural network schemes to\nlearn a probabilistic representation of AIS tracks and a contrario detection to\ndetect abnormal events. The neural network provides a new means to capture\ncomplex and heterogeneous patterns in vessels' behaviours, while the \\textit{a\ncontrario} detector takes into account the fact that the learnt distribution\nmay be location-dependent. Experiments on a real AIS dataset comprising more\nthan 4.2 million AIS messages demonstrate the relevance of the proposed method\ncompared with state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:04:56 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 12:22:43 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 21:03:37 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 12:57:09 GMT"}, {"version": "v5", "created": "Mon, 8 Feb 2021 21:08:13 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Nguyen", "Duong", ""], ["Vadaine", "Rodolphe", ""], ["Hajduch", "Guillaume", ""], ["Garello", "Ren\u00e9", ""], ["Fablet", "Ronan", ""]]}, {"id": "1912.00700", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio and Vojtech Mrazek and Muhammad Abudllah Hanif and\n  Muhammad Shafique", "title": "ReD-CaNe: A Systematic Methodology for Resilience Analysis and Design of\n  Capsule Networks under Approximations", "comments": "To appear at the 23rd Design, Automation and Test in Europe (DATE\n  2020). Grenoble, France", "journal-ref": null, "doi": "10.23919/DATE48585.2020.9116393", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Capsule Networks (CapsNets) have shown their superior\nlearning capability, compared to the traditional Convolutional Neural Networks\n(CNNs). However, the extremely high complexity of CapsNets limits their fast\ndeployment in real-world applications. Moreover, while the resilience of CNNs\nhave been extensively investigated to enable their energy-efficient\nimplementations, the analysis of CapsNets' resilience is a largely unexplored\narea, that can provide a strong foundation to investigate techniques to\novercome the CapsNets' complexity challenge.\n  Following the trend of Approximate Computing to enable energy-efficient\ndesigns, we perform an extensive resilience analysis of the CapsNets inference\nsubjected to the approximation errors. Our methodology models the errors\narising from the approximate components (like multipliers), and analyze their\nimpact on the classification accuracy of CapsNets. This enables the selection\nof approximate components based on the resilience of each operation of the\nCapsNet inference. We modify the TensorFlow framework to simulate the injection\nof approximation noise (based on the models of the approximate components) at\ndifferent computational operations of the CapsNet inference. Our results show\nthat the CapsNets are more resilient to the errors injected in the computations\nthat occur during the dynamic routing (the softmax and the update of the\ncoefficients), rather than other stages like convolutions and activation\nfunctions. Our analysis is extremely useful towards designing efficient CapsNet\nhardware accelerators with approximate components. To the best of our\nknowledge, this is the first proof-of-concept for employing approximations on\nthe specialized CapsNet hardware.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:55:46 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Marchisio", "Alberto", ""], ["Mrazek", "Vojtech", ""], ["Hanif", "Muhammad Abudllah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1912.00706", "submitter": "Roman Feldbauer", "authors": "Roman Feldbauer, Thomas Rattei and Arthur Flexer", "title": "scikit-hubness: Hubness Reduction and Approximate Neighbor Search", "comments": null, "journal-ref": null, "doi": "10.21105/joss.01957", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces scikit-hubness, a Python package for efficient nearest\nneighbor search in high-dimensional spaces. Hubness is an aspect of the curse\nof dimensionality, and is known to impair various learning tasks, including\nclassification, clustering, and visualization. scikit-hubness provides\nalgorithms for hubness analysis (\"Is my data affected by hubness?\"), hubness\nreduction (\"How can we improve neighbor retrieval in high dimensions?\"), and\napproximate neighbor search (\"Does it work for large data sets?\"). It is\nintegrated into the scikit-learn environment, enabling rapid adoption by\nPython-based machine learning researchers and practitioners. Users will find\nall functionality of the scikit-learn neighbors package, plus additional\nsupport for transparent hubness reduction and approximate nearest neighbor\nsearch. scikit-hubness is developed using several quality assessment tools and\nprinciples, such as PEP8 compliance, unit tests with high code coverage,\ncontinuous integration on all major platforms (Linux, MacOS, Windows), and\nadditional checks by LGTM. The source code is available at\nhttps://github.com/VarIr/scikit-hubness under the BSD 3-clause license. Install\nfrom the Python package index with $ pip install scikit-hubness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:04:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Feldbauer", "Roman", ""], ["Rattei", "Thomas", ""], ["Flexer", "Arthur", ""]]}, {"id": "1912.00735", "submitter": "Edouard Pineau", "authors": "Edouard Pineau", "title": "Using Laplacian Spectrum as Graph Feature Representation", "comments": "10 pages, 3 figures, 7 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs possess exotic features like variable size and absence of natural\nordering of the nodes that make them difficult to analyze and compare. To\ncircumvent this problem and learn on graphs, graph feature representation is\nrequired. A good graph representation must satisfy the preservation of\nstructural information, with two particular key attributes: consistency under\ndeformation and invariance under isomorphism. While state-of-the-art methods\nseek such properties with powerful graph neural-networks, we propose to\nleverage a simple graph feature: the graph Laplacian spectrum (GLS). We first\nremind and show that GLS satisfies the aforementioned key attributes, using a\ngraph perturbation approach. In particular, we derive bounds for the distance\nbetween two GLS that are related to the \\textit{divergence to isomorphism}, a\nstandard computationally expensive graph divergence. We finally experiment GLS\nas graph representation through consistency tests and classification tasks, and\nshow that it is a strong graph feature representation baseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:01:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pineau", "Edouard", ""]]}, {"id": "1912.00759", "submitter": "Antonio M. Sudoso", "authors": "Veronica Piccialli, Antonio M. Sudoso", "title": "Improving Non-Intrusive Load Disaggregation through an Attention-Based\n  Deep Neural Network", "comments": null, "journal-ref": "Energies 2021, 14(4), 847", "doi": "10.3390/en14040847", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation, known in the literature as Non-Intrusive Load\nMonitoring (NILM), is the task of inferring the power demand of the individual\nappliances given the aggregate power demand recorded by a single smart meter\nwhich monitors multiple appliances. In this paper, we propose a deep neural\nnetwork that combines a regression subnetwork with a classification subnetwork\nfor solving the NILM problem. Specifically, we improve the generalization\ncapability of the overall architecture by including an encoder-decoder with a\ntailored attention mechanism in the regression subnetwork. The attention\nmechanism is inspired by the temporal attention that has been successfully\napplied in neural machine translation, text summarization, and speech\nrecognition. The experiments conducted on two publicly available datasets--REDD\nand UK-DALE--show that our proposed deep neural network outperforms the\nstate-of-the-art in all the considered experimental conditions. We also show\nthat modeling attention translates into the network's ability to correctly\ndetect the turning on or off an appliance and to locate signal sections with\nhigh power consumption, which are of extreme interest in the field of energy\ndisaggregation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:48:27 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 21:02:13 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 18:52:00 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Piccialli", "Veronica", ""], ["Sudoso", "Antonio M.", ""]]}, {"id": "1912.00761", "submitter": "Alice Xiang", "authors": "Alice Xiang and Inioluwa Deborah Raji", "title": "On the Legal Compatibility of Fairness Definitions", "comments": "6 pages, Workshop on Human-Centric Machine Learning at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Past literature has been effective in demonstrating ideological gaps in\nmachine learning (ML) fairness definitions when considering their use in\ncomplex socio-technical systems. However, we go further to demonstrate that\nthese definitions often misunderstand the legal concepts from which they\npurport to be inspired, and consequently inappropriately co-opt legal language.\nIn this paper, we demonstrate examples of this misalignment and discuss the\ndifferences in ML terminology and their legal counterparts, as well as what\nboth the legal and ML fairness communities can learn from these tensions. We\nfocus this paper on U.S. anti-discrimination law since the ML fairness research\ncommunity regularly references terms from this body of law.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:28:46 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Xiang", "Alice", ""], ["Raji", "Inioluwa Deborah", ""]]}, {"id": "1912.00772", "submitter": "Cameron Wolfe", "authors": "Cameron R. Wolfe, Keld T. Lundgaard", "title": "E-Stitchup: Data Augmentation for Pre-Trained Embeddings", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose data augmentation methods for embeddings from\npre-trained deep learning models that take a weighted combination of a pair of\ninput embeddings, as inspired by Mixup, and combine such augmentation with\nextra label softening. These methods are shown to significantly increase\nclassification accuracy, reduce training time, and improve confidence\ncalibration of a downstream model that is trained with them. As a result of\nsuch improved confidence calibration, the model output can be more intuitively\ninterpreted and used to accurately identify out-of-distribution data by\napplying an appropriate confidence threshold to model predictions. The\nidentified out-of-distribution data can then be prioritized for labeling, thus\nfocusing labeling effort on data that is more likely to boost model\nperformance. These findings, we believe, lay a solid foundation for improving\nthe classification performance and calibration of models that use pre-trained\nembeddings as input and provide several benefits that prove extremely useful in\na production-level deep learning system.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 04:10:31 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 14:14:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wolfe", "Cameron R.", ""], ["Lundgaard", "Keld T.", ""]]}, {"id": "1912.00778", "submitter": "Itay Lieder", "authors": "Itay Lieder, Meirav Segal, Eran Avidan, Asaf Cohen, Tom Hope", "title": "Learning a faceted customer segmentation for discovering new business\n  opportunities at Intel", "comments": "3 pages, 4 figures, Published in proceedings of IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sales and marketing organizations within large enterprises, identifying\nand understanding new markets, customers and partners is a key challenge.\nIntel's Sales and Marketing Group (SMG) faces similar challenges while growing\nin new markets and domains and evolving its existing business. In today's\ncomplex technological and commercial landscape, there is need for intelligent\nautomation supporting a fine-grained understanding of businesses in order to\nhelp SMG sift through millions of companies across many geographies and\nlanguages and identify relevant directions. We present a system developed in\nour company that mines millions of public business web pages, and extracts a\nfaceted customer representation. We focus on two key customer aspects that are\nessential for finding relevant opportunities: industry segments (ranging from\nbroad verticals such as healthcare, to more specific fields such as 'video\nanalytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address\nthe challenge of labeled data collection, we enrich our data with external\ninformation gleaned from Wikipedia, and develop a semi-supervised multi-label,\nmulti-lingual deep learning model that parses customer website texts and\nclassifies them into their respective facets. Our system scans and indexes\ncompanies as part of a large-scale knowledge graph that currently holds tens of\nmillions of connected entities with thousands being fetched, enriched and\nconnected to the graph by the hour in real time, and also supports knowledge\nand insight discovery. In experiments conducted in our company, we are able to\nsignificantly boost the performance of sales personnel in the task of\ndiscovering new customers and commercial partnership opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:48:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Lieder", "Itay", ""], ["Segal", "Meirav", ""], ["Avidan", "Eran", ""], ["Cohen", "Asaf", ""], ["Hope", "Tom", ""]]}, {"id": "1912.00789", "submitter": "Xin Mao", "authors": "Xin Mao, Zhaoyu Su, Pin Siang Tan, Jun Kang Chow, Yu-Hsing Wang", "title": "Is Discriminator a Good Feature Extractor?", "comments": "12 pages, 3 figures, two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discriminator from generative adversarial nets (GAN) has been used by\nresearchers as a feature extractor in transfer learning and appeared worked\nwell. However, there are also studies that believe this is the wrong research\ndirection because intuitively the task of the discriminator focuses on\nseparating the real samples from the generated ones, making features extracted\nin this way useless for most of the downstream tasks. To avoid this dilemma, we\nfirst conducted a thorough theoretical analysis of the relationship between the\ndiscriminator task and the features extracted. We found that the connection\nbetween the task of the discriminator and the feature is not as strong as was\nthought, for that the main factor restricting the feature learned by the\ndiscriminator is not the task, but is the need to prevent the entire GAN model\nfrom mode collapse during the training. From this perspective and combined with\nfurther analyses, we found that to avoid mode collapse, the features extracted\nby the discriminator are not guided to be different for the real samples, but\ndivergence without noise is indeed allowed and occupies a large proportion of\nthe feature space. This makes the features more robust and helps answer the\nquestion as to why the discriminator can succeed as a feature extractor in\nrelated research. Consequently, to expose the essence of the discriminator\nextractor as different from other extractors, we analyze the counterpart of the\ndiscriminator extractor, the classifier extractor that assigns the target\nsamples to different categories. We found the performance of the discriminator\nextractor may be inferior to the classifier based extractor when the source\nclassification task is similar to the target task, which is the common case,\nbut the ability to avoid noise prevents the discriminator from being replaced\nby the classifier.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:59:32 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 07:41:13 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Mao", "Xin", ""], ["Su", "Zhaoyu", ""], ["Tan", "Pin Siang", ""], ["Chow", "Jun Kang", ""], ["Wang", "Yu-Hsing", ""]]}, {"id": "1912.00796", "submitter": "Andreas Look", "authors": "Andreas Look and Melih Kandemir", "title": "Differential Bayesian Neural Nets", "comments": null, "journal-ref": "4th workshop on Bayesian Deep Learning (NeurIPS 2019), Vancouver,\n  Canada", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Ordinary Differential Equations (N-ODEs) are a powerful building block\nfor learning systems, which extend residual networks to a continuous-time\ndynamical system. We propose a Bayesian version of N-ODEs that enables\nwell-calibrated quantification of prediction uncertainty, while maintaining the\nexpressive power of their deterministic counterpart. We assign Bayesian Neural\nNets (BNNs) to both the drift and the diffusion terms of a Stochastic\nDifferential Equation (SDE) that models the flow of the activation map in time.\nWe infer the posterior on the BNN weights using a straightforward adaptation of\nStochastic Gradient Langevin Dynamics (SGLD). We illustrate significantly\nimproved stability on two synthetic time series prediction tasks and report\nbetter model fit on UCI regression benchmarks with our method when compared to\nits non-Bayesian counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:03:55 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 10:14:31 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Look", "Andreas", ""], ["Kandemir", "Melih", ""]]}, {"id": "1912.00818", "submitter": "Manoj Ghuhan Arivazhagan", "authors": "Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, Sunav\n  Choudhary", "title": "Federated Learning with Personalization Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emerging paradigm of federated learning strives to enable collaborative\ntraining of machine learning models on the network edge without centrally\naggregating raw data and hence, improving data privacy. This sharply deviates\nfrom traditional machine learning and necessitates the design of algorithms\nrobust to various sources of heterogeneity. Specifically, statistical\nheterogeneity of data across user devices can severely degrade the performance\nof standard federated averaging for traditional machine learning applications\nlike personalization with deep learning. This paper pro-posesFedPer, a base +\npersonalization layer approach for federated training of deep feedforward\nneural networks, which can combat the ill-effects of statistical heterogeneity.\nWe demonstrate effectiveness ofFedPerfor non-identical data partitions\nofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:29:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Arivazhagan", "Manoj Ghuhan", ""], ["Aggarwal", "Vinay", ""], ["Singh", "Aaditya Kumar", ""], ["Choudhary", "Sunav", ""]]}, {"id": "1912.00827", "submitter": "Ben Adlam", "authors": "Ben Adlam, Jake Levinson, and Jeffrey Pennington", "title": "A Random Matrix Perspective on Mixtures of Nonlinearities for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the distinguishing characteristics of modern deep learning systems is\nthat they typically employ neural network architectures that utilize enormous\nnumbers of parameters, often in the millions and sometimes even in the\nbillions. While this paradigm has inspired significant research on the\nproperties of large networks, relatively little work has been devoted to the\nfact that these networks are often used to model large complex datasets, which\nmay themselves contain millions or even billions of constraints. In this work,\nwe focus on this high-dimensional regime in which both the dataset size and the\nnumber of features tend to infinity. We analyze the performance of a simple\nregression model trained on the random features $F=f(WX+B)$ for a random weight\nmatrix $W$ and random bias vector $B$, obtaining an exact formula for the\nasymptotic training error on a noisy autoencoding task. The role of the bias\ncan be understood as parameterizing a distribution over activation functions,\nand our analysis directly generalizes to such distributions, even those not\nexpressible with a traditional additive bias. Intriguingly, we find that a\nmixture of nonlinearities can outperform the best single nonlinearity on the\nnoisy autoecndoing task, suggesting that mixtures of nonlinearities might be\nuseful for approximate kernel methods or neural network architecture design.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:43:16 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Adlam", "Ben", ""], ["Levinson", "Jake", ""], ["Pennington", "Jeffrey", ""]]}, {"id": "1912.00832", "submitter": "Geir Kjetil Nilsen Mr", "authors": "Geir K. Nilsen and Antonella Z. Munthe-Kaas and Hans J. Skaug and\n  Morten Brun", "title": "Epistemic Uncertainty Quantification in Deep Learning Classification by\n  the Delta Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Delta method is a classical procedure for quantifying epistemic\nuncertainty in statistical models, but its direct application to deep neural\nnetworks is prevented by the large number of parameters $P$. We propose a low\ncost variant of the Delta method applicable to $L_2$-regularized deep neural\nnetworks based on the top $K$ eigenpairs of the Fisher information matrix. We\naddress efficient computation of full-rank approximate eigendecompositions in\nterms of either the exact inverse Hessian, the inverse outer-products of\ngradients approximation or the so-called Sandwich estimator. Moreover, we\nprovide a bound on the approximation error for the uncertainty of the\npredictive class probabilities. We observe that when the smallest eigenvalue of\nthe Fisher information matrix is near the $L_2$-regularization rate, the\napproximation error is close to zero even when $K\\ll P$. A demonstration of the\nmethodology is presented using a TensorFlow implementation, and we show that\nmeaningful rankings of images based on predictive uncertainty can be obtained\nfor two LeNet-based neural networks using the MNIST and CIFAR-10 datasets.\nFurther, we observe that false positives have on average a higher predictive\nepistemic uncertainty than true positives. This suggests that there is\nsupplementing information in the uncertainty measure not captured by the\nclassification alone.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:53:10 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 12:20:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nilsen", "Geir K.", ""], ["Munthe-Kaas", "Antonella Z.", ""], ["Skaug", "Hans J.", ""], ["Brun", "Morten", ""]]}, {"id": "1912.00838", "submitter": "Peng Xiao Dr", "authors": "Peng Xiao, Bin Liao and Nikos Deligiannis", "title": "DeepFPC: Deep Unfolding of a Fixed-Point Continuation Algorithm for\n  Sparse Signal Recovery from Quantized Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepFPC, a novel deep neural network designed by unfolding the\niterations of the fixed-point continuation algorithm with one-sided l1-norm\n(FPC-l1), which has been proposed for solving the 1-bit compressed sensing\nproblem. The network architecture resembles that of deep residual learning and\nincorporates prior knowledge about the signal structure (i.e., sparsity),\nthereby offering interpretability by design. Once DeepFPC is properly trained,\na sparse signal can be recovered fast and accurately from quantized\nmeasurements. The proposed model is evaluated in the task of\ndirection-of-arrival (DOA) estimation and is shown to outperform\nstate-of-the-art algorithms, namely, the iterative FPC-l1 algorithm and the\n1-bit MUSIC method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:00:21 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 14:51:54 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 08:43:11 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Xiao", "Peng", ""], ["Liao", "Bin", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1912.00846", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Subhadeep Dey, Hwanhee Lee, Kyomin Jung", "title": "Attentive Modality Hopping Mechanism for Speech Emotion Recognition", "comments": "5 pages, Accepted as a conference paper at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the impact of visual modality in addition to speech\nand text for improving the accuracy of the emotion detection system. The\ntraditional approaches tackle this task by fusing the knowledge from the\nvarious modalities independently for performing emotion classification. In\ncontrast to these approaches, we tackle the problem by introducing an attention\nmechanism to combine the information. In this regard, we first apply a neural\nnetwork to obtain hidden representations of the modalities. Then, the attention\nmechanism is defined to select and aggregate important parts of the video data\nby conditioning on the audio and text data. Furthermore, the attention\nmechanism is again applied to attend important parts of the speech and textual\ndata, by considering other modality. Experiments are performed on the standard\nIEMOCAP dataset using all three modalities (audio, text, and video). The\nachieved results show a significant improvement of 3.65% in terms of weighted\naccuracy compared to the baseline system.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:23:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 02:18:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Dey", "Subhadeep", ""], ["Lee", "Hwanhee", ""], ["Jung", "Kyomin", ""]]}, {"id": "1912.00848", "submitter": "Pieter-Jan Kindermans", "authors": "Wei Wen, Hanxiao Liu, Hai Li, Yiran Chen, Gabriel Bender, Pieter-Jan\n  Kindermans", "title": "Neural Predictor for Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search methods are effective but often use complex\nalgorithms to come up with the best architecture. We propose an approach with\nthree basic steps that is conceptually much simpler. First we train N random\narchitectures to generate N (architecture, validation accuracy) pairs and use\nthem to train a regression model that predicts accuracy based on the\narchitecture. Next, we use this regression model to predict the validation\naccuracies of a large number of random architectures. Finally, we train the\ntop-K predicted architectures and deploy the model with the best validation\nresult. While this approach seems simple, it is more than 20 times as sample\nefficient as Regularized Evolution on the NASBench-101 benchmark and can\ncompete on ImageNet with more complex approaches based on weight sharing, such\nas ProxylessNAS.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:10:59 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wen", "Wei", ""], ["Liu", "Hanxiao", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""], ["Bender", "Gabriel", ""], ["Kindermans", "Pieter-Jan", ""]]}, {"id": "1912.00852", "submitter": "Nora Vogt", "authors": "Nora Vogt", "title": "CNNs, LSTMs, and Attention Networks for Pathology Detection in Medical\n  Data", "comments": "Master thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the weakly supervised task of electrocardiogram (ECG) rhythm\nclassification, convolutional neural networks (CNNs) and long short-term memory\n(LSTM) networks are two increasingly popular classification models. This work\ninvestigates whether a combination of both architectures to so-called\nconvolutional long short-term memory (ConvLSTM) networks can improve\nclassification performances by explicitly capturing morphological as well as\ntemporal features of raw ECG records. In addition, various attention mechanisms\nare studied to localize and visualize record sections of abnormal morphology\nand irregular rhythm. The resulting saliency maps are supposed to not only\nallow for a better network understanding but to also improve clinicians'\nacceptance of automatic diagnosis in order to avoid the technique being labeled\nas a black box. In further experiments, attention mechanisms are actively\nincorporated into the training process by learning a few additional attention\ngating parameters in a CNN model. An 8-fold cross validation is finally carried\nout on the PhysioNet Computing in Cardiology (CinC) challenge 2017 to compare\nthe performances of standard CNN models, ConvLSTMs, and attention gated CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:19:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Vogt", "Nora", ""]]}, {"id": "1912.00858", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Bingkun Wei, Hongying Liu, Yuanyuan Liu and Jiacheng\n  Zhuo", "title": "Efficient Relaxed Gradient Support Pursuit for Sparsity Constrained\n  Non-convex Optimization", "comments": "7 pages, 3 figures, Appeared at the Data Science Meets Optimization\n  Workshop (DSO) at IJCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale non-convex sparsity-constrained problems have recently gained\nextensive attention. Most existing deterministic optimization methods (e.g.,\nGraSP) are not suitable for large-scale and high-dimensional problems, and thus\nstochastic optimization methods with hard thresholding (e.g., SVRGHT) become\nmore attractive. Inspired by GraSP, this paper proposes a new general relaxed\ngradient support pursuit (RGraSP) framework, in which the sub-algorithm only\nrequires to satisfy a slack descent condition. We also design two specific\nsemi-stochastic gradient hard thresholding algorithms. In particular, our\nalgorithms have much less hard thresholding operations than SVRGHT, and their\naverage per-iteration cost is much lower (i.e., O(d) vs. O(d log(d)) for\nSVRGHT), which leads to faster convergence. Our experimental results on both\nsynthetic and real-world datasets show that our algorithms are superior to the\nstate-of-the-art gradient hard thresholding methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:25:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Shang", "Fanhua", ""], ["Wei", "Bingkun", ""], ["Liu", "Hongying", ""], ["Liu", "Yuanyuan", ""], ["Zhuo", "Jiacheng", ""]]}, {"id": "1912.00871", "submitter": "Kaden Griffith", "authors": "Kaden Griffith and Jugal Kalita", "title": "Solving Arithmetic Word Problems Automatically Using Transformer and\n  Unambiguous Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing accurate and automatic solvers of math word problems has proven\nto be quite challenging. Prior attempts using machine learning have been\ntrained on corpora specific to math word problems to produce arithmetic\nexpressions in infix notation before answer computation. We find that\ncustom-built neural networks have struggled to generalize well. This paper\noutlines the use of Transformer networks trained to translate math word\nproblems to equivalent arithmetic expressions in infix, prefix, and postfix\nnotations. In addition to training directly on domain-specific corpora, we use\nan approach that pre-trains on a general text corpus to provide foundational\nlanguage abilities to explore if it improves performance. We compare results\nproduced by a large number of neural configurations and find that most\nconfigurations outperform previously reported approaches on three of four\ndatasets with significant increases in accuracy of over 20 percentage points.\nThe best neural approaches boost accuracy by almost 10% on average when\ncompared to the previous state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:42:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Griffith", "Kaden", ""], ["Kalita", "Jugal", ""]]}, {"id": "1912.00872", "submitter": "Matt Chapman-Rounds", "authors": "Matt Chapman-Rounds, Marc-Andre Schulz, Erik Pazos, Konstantinos\n  Georgatzis", "title": "EMAP: Explanation by Minimal Adversarial Perturbation", "comments": "9 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern instance-based model-agnostic explanation methods (LIME, SHAP, L2X)\nare of great use in data-heavy industries for model diagnostics, and for\nend-user explanations. These methods generally return either a weighting or\nsubset of input features as an explanation of the classification of an\ninstance. An alternative literature argues instead that counterfactual\ninstances provide a more useable characterisation of a black box classifier's\ndecisions. We present EMAP, a neural network based approach which returns as\nExplanation the Minimal Adversarial Perturbation to an instance required to\ncause the underlying black box model to missclassify. We show that this\napproach combines the two paradigms, recovering the output of feature-weighting\nmethods in continuous feature spaces, whilst also indicating the direction in\nwhich the nearest counterfactuals can be found. Our method also provides an\nimplicit confidence estimate in its own explanations, adding a clarity to model\ndiagnostics other methods lack. Additionally, EMAP improves upon the speed of\nsampling-based methods such as LIME by an order of magnitude, allowing for\nmodel explanations in time-critical applications, or at the dataset level,\nwhere sampling-based methods are infeasible. We extend our approach to\ncategorical features using a partitioned Gumbel layer, and demonstrate its\nefficacy on several standard datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:48:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Chapman-Rounds", "Matt", ""], ["Schulz", "Marc-Andre", ""], ["Pazos", "Erik", ""], ["Georgatzis", "Konstantinos", ""]]}, {"id": "1912.00873", "submitter": "Ehsan Kharazmi", "authors": "E. Kharazmi, Z. Zhang, G. E. Karniadakis", "title": "Variational Physics-Informed Neural Networks For Solving Partial\n  Differential Equations", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) [31] use automatic differentiation\nto solve partial differential equations (PDEs) by penalizing the PDE in the\nloss function at a random set of points in the domain of interest. Here, we\ndevelop a Petrov-Galerkin version of PINNs based on the nonlinear approximation\nof deep neural networks (DNNs) by selecting the {\\em trial space} to be the\nspace of neural networks and the {\\em test space} to be the space of Legendre\npolynomials. We formulate the \\textit{variational residual} of the PDE using\nthe DNN approximation by incorporating the variational form of the problem into\nthe loss function of the network and construct a \\textit{variational\nphysics-informed neural network} (VPINN). By integrating by parts the integrand\nin the variational form, we lower the order of the differential operators\nrepresented by the neural networks, hence effectively reducing the training\ncost in VPINNs while increasing their accuracy compared to PINNs that\nessentially employ delta test functions. For shallow networks with one hidden\nlayer, we analytically obtain explicit forms of the \\textit{variational\nresidual}. We demonstrate the performance of the new formulation for several\nexamples that show clear advantages of VPINNs over PINNs in terms of both\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:51:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kharazmi", "E.", ""], ["Zhang", "Z.", ""], ["Karniadakis", "G. E.", ""]]}, {"id": "1912.00874", "submitter": "Sebastian Schmon", "authors": "Jack K Fitzsimons, Sebastian M Schmon, Stephen J Roberts", "title": "Implicit Priors for Knowledge Sharing in Bayesian Neural Networks", "comments": "5 pages, 2 figures", "journal-ref": "4th workshop on Bayesian Deep Learning (NeurIPS 2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian interpretations of neural network have a long history, dating back\nto early work in the 1990's and have recently regained attention because of\ntheir desirable properties like uncertainty estimation, model robustness and\nregularisation. We want to discuss here the application of Bayesian models to\nknowledge sharing between neural networks. Knowledge sharing comes in different\nfacets, such as transfer learning, model distillation and shared embeddings.\nAll of these tasks have in common that learned \"features\" ought to be shared\nacross different networks. Theoretically rooted in the concepts of Bayesian\nneural networks this work has widespread application to general deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:52:33 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fitzsimons", "Jack K", ""], ["Schmon", "Sebastian M", ""], ["Roberts", "Stephen J", ""]]}, {"id": "1912.00888", "submitter": "Nils Lukas", "authors": "Nils Lukas, Yuxuan Zhang, Florian Kerschbaum", "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Machine Learning as a Service, a provider trains a deep neural network and\ngives many users access. The hosted (source) model is susceptible to model\nstealing attacks, where an adversary derives a surrogate model from API access\nto the source model. For post hoc detection of such attacks, the provider needs\na robust method to determine whether a suspect model is a surrogate of their\nmodel. We propose a fingerprinting method for deep neural network classifiers\nthat extracts a set of inputs from the source model so that only surrogates\nagree with the source model on the classification of such inputs. These inputs\nare a subclass of transferable adversarial examples which we call conferrable\nadversarial examples that exclusively transfer with a target label from a\nsource model to its surrogates. We propose a new method to generate these\nconferrable adversarial examples. We present an extensive study on the\nirremovability of our fingerprint against fine-tuning, weight pruning,\nretraining, retraining with different architectures, three model extraction\nattacks from related work, transfer learning, adversarial training, and two new\nadaptive attacks. Our fingerprint is robust against distillation, related model\nextraction attacks, and even transfer learning when the attacker has no access\nto the model provider's dataset. Our fingerprint is the first method that\nreaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63\nby previous fingerprints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:11:56 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 01:09:43 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 00:00:56 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 18:19:24 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lukas", "Nils", ""], ["Zhang", "Yuxuan", ""], ["Kerschbaum", "Florian", ""]]}, {"id": "1912.00894", "submitter": "Nikolas Nuesken", "authors": "A. Duncan and N. Nuesken and L. Szpruch", "title": "On the geometry of Stein variational gradient descent", "comments": "39 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference problems require sampling or approximating\nhigh-dimensional probability distributions. The focus of this paper is on the\nrecently introduced Stein variational gradient descent methodology, a class of\nalgorithms that rely on iterated steepest descent steps with respect to a\nreproducing kernel Hilbert space norm. This construction leads to interacting\nparticle systems, the mean-field limit of which is a gradient flow on the space\nof probability distributions equipped with a certain geometrical structure. We\nleverage this viewpoint to shed some light on the convergence properties of the\nalgorithm, in particular addressing the problem of choosing a suitable positive\ndefinite kernel function. Our analysis leads us to considering certain\nnondifferentiable kernels with adjusted tails. We demonstrate significant\nperforms gains of these in various numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:20:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Duncan", "A.", ""], ["Nuesken", "N.", ""], ["Szpruch", "L.", ""]]}, {"id": "1912.00895", "submitter": "Kehinde Owoeye Mr", "authors": "Kehinde Owoeye", "title": "Learning to smell for wellness", "comments": "10 pages, 1 figure", "journal-ref": "Workshop on AI for Social Good workshop NeurIPS (2019), Vancouver,\n  Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning to automatically perceive smell is becoming increasingly important\nwith applications in monitoring the quality of food and drinks for healthy\nliving. In todays age of proliferation of internet of things devices, the\ndeployment of electronic nose otherwise known as smell sensors is on the\nincrease for a variety of olfaction applications with the aid of machine\nlearning models. These models are trained to classify food and drink quality\ninto several categories depending on the granularity of interest. However,\nmodels trained to smell in one domain rarely perform adequately when used in\nanother domain. In this work, we consider a problem where only few samples are\navailable in the target domain and we are faced with the task of leveraging\nknowledge from another domain with relatively abundant data to make reliable\ninference in the target domain. We propose a weakly supervised domain\nadaptation framework where we demonstrate that by building multiple models in a\nmixture of supervised and unsupervised framework, we can generalise effectively\nfrom one domain to another. We evaluate our approach on several datasets of\nbeef cuts and quality collected across different conditions and environments.\nWe empirically show via several experiments that our approach perform\ncompetitively compared to a variety of baselines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:20:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Owoeye", "Kehinde", ""]]}, {"id": "1912.00905", "submitter": "Roberta Falcone", "authors": "Roberta Falcone, Angela Montanari, Laura Anderlucci", "title": "Matrix sketching for supervised classification with imbalanced classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix sketching is a recently developed data compression technique. An input\nmatrix A is efficiently approximated with a smaller matrix B, so that B\npreserves most of the properties of A up to some guaranteed approximation\nratio. In so doing numerical operations on big data sets become faster.\nSketching algorithms generally use random projections to compress the original\ndataset and this stochastic generation process makes them amenable to\nstatistical analysis. The statistical properties of sketching algorithms have\nbeen widely studied in the context of multiple linear regression. In this paper\nwe propose matrix sketching as a tool for rebalancing class sizes in supervised\nclassification with imbalanced classes. It is well-known in fact that class\nimbalance may lead to poor classification performances especially as far as the\nminority class is concerned.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:33:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Falcone", "Roberta", ""], ["Montanari", "Angela", ""], ["Anderlucci", "Laura", ""]]}, {"id": "1912.00941", "submitter": "Le-Ha Hoang", "authors": "Le-Ha Hoang, Muhammad Abdullah Hanif, Muhammad Shafique", "title": "FT-ClipAct: Resilience Analysis of Deep Neural Networks and Improving\n  their Fault Tolerance using Clipped Activation", "comments": "The 23rd Design, Automation and Test in Europe (DATE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are widely being adopted for safety-critical\napplications, e.g., healthcare and autonomous driving. Inherently, they are\nconsidered to be highly error-tolerant. However, recent studies have shown that\nhardware faults that impact the parameters of a DNN (e.g., weights) can have\ndrastic impacts on its classification accuracy. In this paper, we perform a\ncomprehensive error resilience analysis of DNNs subjected to hardware faults\n(e.g., permanent faults) in the weight memory. The outcome of this analysis is\nleveraged to propose a novel error mitigation technique which squashes the\nhigh-intensity faulty activation values to alleviate their impact. We achieve\nthis by replacing the unbounded activation functions with their clipped\nversions. We also present a method to systematically define the clipping values\nof the activation functions that result in increased resilience of the networks\nagainst faults. We evaluate our technique on the AlexNet and the VGG-16 DNNs\ntrained for the CIFAR-10 dataset. The experimental results show that our\nmitigation technique significantly improves the resilience of the DNNs to\nfaults. For example, the proposed technique offers on average 68.92%\nimprovement in the classification accuracy of resilience-optimized VGG-16 model\nat 1e-5 fault rate, when compared to the base network without any fault\nmitigation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:14:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hoang", "Le-Ha", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1912.00949", "submitter": "Feng Wu", "authors": "Yixiang Wang and Feng Wu", "title": "Multi-Agent Deep Reinforcement Learning with Adaptive Policies", "comments": "arXiv admin note: text overlap with arXiv:1706.02275 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to address one aspect of the non-stationarity\nproblem in multi-agent reinforcement learning (RL), where the other agents may\nalter their policies due to environment changes during execution. This violates\nthe Markov assumption that governs most single-agent RL methods and is one of\nthe key challenges in multi-agent RL. To tackle this, we propose to train\nmultiple policies for each agent and postpone the selection of the best policy\nat execution time. Specifically, we model the environment non-stationarity with\na finite set of scenarios and train policies fitting each scenario. In addition\nto multiple policies, each agent also learns a policy predictor to determine\nwhich policy is the best with its local information. By doing so, each agent is\nable to adapt its policy when the environment changes and consequentially the\nother agents alter their policies during execution. We empirically evaluated\nour method on a variety of common benchmark problems proposed for multi-agent\ndeep RL in the literature. Our experimental results show that the agents\ntrained by our algorithm have better adaptiveness in changing environments and\noutperform the state-of-the-art methods in all the tested environments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 07:23:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Yixiang", ""], ["Wu", "Feng", ""]]}, {"id": "1912.00953", "submitter": "Yan Wu", "authors": "Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, Timothy\n  Lillicrap", "title": "LOGAN: Latent Optimisation for Generative Adversarial Networks", "comments": "Improved writing, added new analysis and evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks requires balancing of delicate\nadversarial dynamics. Even with careful tuning, training may diverge or end up\nin a bad equilibrium with dropped modes. In this work, we improve CS-GAN with\nnatural gradient-based latent optimisation and show that it improves\nadversarial dynamics by enhancing interactions between the discriminator and\nthe generator. Our experiments demonstrate that latent optimisation can\nsignificantly improve GAN training, obtaining state-of-the-art performance for\nthe ImageNet ($128 \\times 128$) dataset. Our model achieves an Inception Score\n(IS) of $148$ and an Fr\\'echet Inception Distance (FID) of $3.4$, an\nimprovement of $17\\%$ and $32\\%$ in IS and FID respectively, compared with the\nbaseline BigGAN-deep model with the same architecture and number of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:30:05 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 16:53:32 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Wu", "Yan", ""], ["Donahue", "Jeff", ""], ["Balduzzi", "David", ""], ["Simonyan", "Karen", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1912.00965", "submitter": "Rizal Fathony", "authors": "Rizal Fathony and J. Zico Kolter", "title": "AP-Perf: Incorporating Generic Performance Metrics in Differentiable\n  Learning", "comments": "Appears in the Proceedings of the 23rd International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that enables practitioners to conveniently incorporate\ncustom non-decomposable performance metrics into differentiable learning\npipelines, notably those based upon neural network architectures. Our approach\nis based on the recently developed adversarial prediction framework, a\ndistributionally robust approach that optimizes a metric in the worst case\ngiven the statistical summary of the empirical distribution. We formulate a\nmarginal distribution technique to reduce the complexity of optimizing the\nadversarial prediction formulation over a vast range of non-decomposable\nmetrics. We demonstrate how easy it is to write and incorporate complex custom\nmetrics using our provided tool. Finally, we show the effectiveness of our\napproach various classification tasks on tabular datasets from the UCI\nrepository and benchmark datasets, as well as image classification tasks. The\ncode for our proposed method is available at\nhttps://github.com/rizalzaf/AdversarialPrediction.jl.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:53:05 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 13:58:44 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Fathony", "Rizal", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1912.00967", "submitter": "Louis-Pascal A. C. Xhonneux", "authors": "Louis-Pascal A. C. Xhonneux, Meng Qu, and Jian Tang", "title": "Continuous Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds on the connection between graph neural networks and\ntraditional dynamical systems. We propose continuous graph neural networks\n(CGNN), which generalise existing graph neural networks with discrete dynamics\nin that they can be viewed as a specific discretisation scheme. The key idea is\nhow to characterise the continuous dynamics of node representations, i.e. the\nderivatives of node representations, w.r.t. time. Inspired by existing\ndiffusion-based methods on graphs (e.g. PageRank and epidemic models on social\nnetworks), we define the derivatives as a combination of the current node\nrepresentations, the representations of neighbors, and the initial values of\nthe nodes. We propose and analyse two possible dynamics on graphs---including\neach dimension of node representations (a.k.a. the feature channel) change\nindependently or interact with each other---both with theoretical\njustification. The proposed continuous graph neural networks are robust to\nover-smoothing and hence allow us to build deeper networks, which in turn are\nable to capture the long-range dependencies between nodes. Experimental results\non the task of node classification demonstrate the effectiveness of our\nproposed approach over competitive baselines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:59:12 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 21:55:28 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 14:32:36 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Xhonneux", "Louis-Pascal A. C.", ""], ["Qu", "Meng", ""], ["Tang", "Jian", ""]]}, {"id": "1912.00979", "submitter": "Yufan Zhou", "authors": "Yufan Zhou, Changyou Chen, Jinhui Xu", "title": "KernelNet: A Data-Dependent Kernel Parameterization for Deep Generative\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with kernels is an important concept in machine learning. Standard\napproaches for kernel methods often use predefined kernels that require careful\nselection of hyperparameters. To mitigate this burden, we propose in this paper\na framework to construct and learn a data-dependent kernel based on random\nfeatures and implicit spectral distributions that are parameterized by deep\nneural networks. The constructed network (called KernelNet) can be applied to\ndeep generative modeling in various scenarios, including two popular learning\nparadigms in deep generative models, MMD-GAN and implicit Variational\nAutoencoder (VAE). We show that our proposed kernel indeed exists in\napplications and is guaranteed to be positive definite. Furthermore, the\ninduced Maximum Mean Discrepancy (MMD) can endow the continuity property in\nweak topology by simple regularization. Extensive experiments indicate that our\nproposed KernelNet consistently achieves better performance compared to related\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:15:43 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 04:45:45 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 02:50:37 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhou", "Yufan", ""], ["Chen", "Changyou", ""], ["Xu", "Jinhui", ""]]}, {"id": "1912.00982", "submitter": "Nils Rethmeier", "authors": "Nils Rethmeier and Vageesh Kumar Saxena and Isabelle Augenstein", "title": "TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in\n  (Un-)Supervised NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art NLP explainability (XAI) methods focus on explaining\nper-sample decisions in supervised end or probing tasks, this is insufficient\nto explain and quantify model knowledge transfer during (un-)supervised\ntraining. Thus, for TX-Ray, we modify the established computer vision\nexplainability principle of 'visualizing preferred inputs of neurons' to make\nit usable transfer analysis and NLP. This allows one to analyze, track and\nquantify how self- or supervised NLP models first build knowledge abstractions\nin pretraining (1), and then transfer these abstractions to a new domain (2),\nor adapt them during supervised fine-tuning (3). TX-Ray expresses neurons as\nfeature preference distributions to quantify fine-grained knowledge transfer or\nadaptation and guide human analysis. We find that, similar to Lottery Ticket\nbased pruning, TX-Ray based pruning can improve test set generalization and\nthat it can reveal how early stages of self-supervision automatically learn\nlinguistic abstractions like parts-of-speech.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:21:31 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:18:41 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 14:24:44 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Rethmeier", "Nils", ""], ["Saxena", "Vageesh Kumar", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "1912.00993", "submitter": "Pierre-Luc Delisle", "authors": "Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers,\n  Herve Lombaert", "title": "Adversarial normalization for multi domain image segmentation", "comments": "Submitted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image normalization is a critical step in medical imaging. This step is often\ndone on a per-dataset basis, preventing current segmentation algorithms from\nthe full potential of exploiting jointly normalized information across multiple\ndatasets. To solve this problem, we propose an adversarial normalization\napproach for image segmentation which learns common normalizing functions\nacross multiple datasets while retaining image realism. The adversarial\ntraining provides an optimal normalizer that improves both the segmentation\naccuracy and the discrimination of unrealistic normalizing functions. Our\ncontribution therefore leverages common imaging information from multiple\ndomains. The optimality of our common normalizer is evaluated by combining\nbrain images from both infants and adults. Results on the challenging iSEG and\nMRBrainS datasets reveal the potential of our adversarial normalization\napproach for segmentation, with Dice improvements of up to 59.6% over the\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:52:45 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 18:43:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Delisle", "Pierre-Luc", ""], ["Anctil-Robitaille", "Benoit", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "1912.01038", "submitter": "Enrico Camporeale", "authors": "Enrico Camporeale, M. D. Cash, H. J. Singer, C. C. Balch, Z. Huang, G.\n  Toth", "title": "A gray-box model for a probabilistic estimate of regional ground\n  magnetic perturbations: Enhancing the NOAA operational Geospace model with\n  machine learning", "comments": "under review", "journal-ref": null, "doi": "10.1029/2019JA027684", "report-no": null, "categories": "physics.space-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm that predicts the probability that the time\nderivative of the horizontal component of the ground magnetic field $dB/dt$\nexceeds a specified threshold at a given location. This quantity provides\nimportant information that is physically relevant to Geomagnetically Induced\nCurrents (GIC), which are electric currents { associated to} sudden changes in\nthe Earth's magnetic field due to Space Weather events. The model follows a\n'gray-box' approach by combining the output of a physics-based model with\nmachine learning. Specifically, we combine the University of Michigan's\nGeospace model that is operational at the NOAA Space Weather Prediction Center,\nwith a boosted ensemble of classification trees. We discuss the problem of\nre-calibrating the output of the decision tree to obtain reliable\nprobabilities. The performance of the model is assessed by typical metrics for\nprobabilistic forecasts: Probability of Detection and False Detection, True\nSkill Statistic, Heidke Skill Score, and Receiver Operating Characteristic\ncurve. We show that the ML enhanced algorithm consistently improves all the\nmetrics considered.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:07:49 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:12:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Camporeale", "Enrico", ""], ["Cash", "M. D.", ""], ["Singer", "H. J.", ""], ["Balch", "C. C.", ""], ["Huang", "Z.", ""], ["Toth", "G.", ""]]}, {"id": "1912.01089", "submitter": "Zhengze Zhou", "authors": "Zhengze Zhou, Lucas Mentch, Giles Hooker", "title": "$V$-statistics and Variance Estimation", "comments": "This version supersedes the previous technical report titled\n  \"Asymptotic Normality and Variance Estimation For Supervised Ensembles\".\n  Extensive simulations are added and we also provide a more detailed\n  discussion on the bias phenomenon in variance estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for analyzing asymptotics of\n$V$-statistics. Previous literature on limiting distribution mainly focuses on\nthe cases when $n \\to \\infty$ with fixed kernel size $k$. Under some regularity\nconditions, we demonstrate asymptotic normality when $k$ grows with $n$ by\nutilizing existing results for $U$-statistics. The key in our approach lies in\na mathematical reduction to $U$-statistics by designing an equivalent kernel\nfor $V$-statistics. We also provide a unified treatment on variance estimation\nfor both $U$- and $V$-statistics by observing connections to existing methods\nand proposing an empirically more accurate estimator. Ensemble methods such as\nrandom forests, where multiple base learners are trained and aggregated for\nprediction purposes, serve as a running example throughout the paper because\nthey are a natural and flexible application of $V$-statistics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:42:19 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 02:08:01 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhou", "Zhengze", ""], ["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1912.01094", "submitter": "Kevin Matthew Stangl", "authors": "Avrim Blum, Kevin Stangl", "title": "Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple fairness constraints have been proposed in the literature, motivated\nby a range of concerns about how demographic groups might be treated unfairly\nby machine learning classifiers. In this work we consider a different\nmotivation; learning from biased training data. We posit several ways in which\ntraining data may be biased, including having a more noisy or negatively biased\nlabeling process on members of a disadvantaged group, or a decreased prevalence\nof positive or negative examples from the disadvantaged group, or both.\n  Given such biased training data, Empirical Risk Minimization (ERM) may\nproduce a classifier that not only is biased but also has suboptimal accuracy\non the true data distribution. We examine the ability of fairness-constrained\nERM to correct this problem. In particular, we find that the Equal Opportunity\nfairness constraint (Hardt, Price, and Srebro 2016) combined with ERM will\nprovably recover the Bayes Optimal Classifier under a range of bias models. We\nalso consider other recovery methods including reweighting the training data,\nEqualized Odds, and Demographic Parity. These theoretical results provide\nadditional motivation for considering fairness interventions even if an actor\ncares primarily about accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:00:14 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Blum", "Avrim", ""], ["Stangl", "Kevin", ""]]}, {"id": "1912.01096", "submitter": "Shen Zhang", "authors": "Shen Zhang, Fei Ye, Bingnan Wang, Thomas G. Habetler", "title": "Semi-Supervised Learning of Bearing Anomaly Detection via Deep\n  Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the data-driven approaches applied to bearing fault diagnosis up to\ndate are established in the supervised learning paradigm, which usually\nrequires a large set of labeled data collected a priori. In practical\napplications, however, obtaining accurate labels based on real-time bearing\nconditions can be far more challenging than simply collecting a huge amount of\nunlabeled data using various sensors. In this paper, we thus propose a\nsemi-supervised learning approach for bearing anomaly detection using\nvariational autoencoder (VAE) based deep generative models, which allows for\neffective utilization of dataset when only a small subset of data have labels.\nFinally, a series of experiments is performed using both the Case Western\nReserve University (CWRU) bearing dataset and the University of Cincinnati's\nCenter for Intelligent Maintenance Systems (IMS) dataset. The experimental\nresults demonstrate that the proposed semi-supervised learning scheme greatly\noutperforms two mainstream semi-supervised learning approaches and a baseline\nsupervised convolutional neural network approach, with the overall accuracy\nimprovement ranging between 3% to 30% using different proportions of labeled\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:10:39 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 03:29:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Shen", ""], ["Ye", "Fei", ""], ["Wang", "Bingnan", ""], ["Habetler", "Thomas G.", ""]]}, {"id": "1912.01098", "submitter": "Sandeep Silwal", "authors": "Rikhav Shah, Sandeep Silwal", "title": "Using Dimensionality Reduction to Optimize t-SNE", "comments": "11th Annual Workshop on Optimization for Machine Learning (OPT2019 )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-SNE is a popular tool for embedding multi-dimensional datasets into two or\nthree dimensions. However, it has a large computational cost, especially when\nthe input data has many dimensions. Many use t-SNE to embed the output of a\nneural network, which is generally of much lower dimension than the original\ndata. This limits the use of t-SNE in unsupervised scenarios. We propose using\n\\textit{random} projections to embed high dimensional datasets into relatively\nfew dimensions, and then using t-SNE to obtain a two dimensional embedding. We\nshow that random projections preserve the desirable clustering achieved by\nt-SNE, while dramatically reducing the runtime of finding the embedding.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:12:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Shah", "Rikhav", ""], ["Silwal", "Sandeep", ""]]}, {"id": "1912.01100", "submitter": "Vincenzo Lomonaco PhD", "authors": "Lorenzo Pellegrini, Gabriele Graffieti, Vincenzo Lomonaco, Davide\n  Maltoni", "title": "Latent Replay for Real-Time Continual Learning", "comments": "Pre-print v3: 13 pages, 9 figures, 10 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks at the edge on light computational devices,\nembedded systems and robotic platforms is nowadays very challenging. Continual\nlearning techniques, where complex models are incrementally trained on small\nbatches of new data, can make the learning problem tractable even for CPU-only\nembedded devices enabling remarkable levels of adaptiveness and autonomy.\nHowever, a number of practical problems need to be solved: catastrophic\nforgetting before anything else. In this paper we introduce an original\ntechnique named \"Latent Replay\" where, instead of storing a portion of past\ndata in the input space, we store activations volumes at some intermediate\nlayer. This can significantly reduce the computation and storage required by\nnative rehearsal. To keep the representation stable and the stored activations\nvalid we propose to slow-down learning at all the layers below the latent\nreplay one, leaving the layers above free to learn at full pace. In our\nexperiments we show that Latent Replay, combined with existing continual\nlearning techniques, achieves state-of-the-art performance on complex video\nbenchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.\nbatches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly\nreal-time continual learning on the edge through the deployment of the proposed\ntechnique on a smartphone device.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:16:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 09:50:32 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Pellegrini", "Lorenzo", ""], ["Graffieti", "Gabriele", ""], ["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""]]}, {"id": "1912.01103", "submitter": "Tianhong Sheng", "authors": "Tianhong Sheng and Bharath K. Sriperumbudur", "title": "On Distance and Kernel Measures of Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional independence is one of the important tasks in\nstatistical inference and is fundamental in causal discovery, feature\nselection, dimensionality reduction, Bayesian network learning, and others. In\nthis work, we explore the connection between conditional independence measures\ninduced by distances on a metric space and reproducing kernels associated with\na reproducing kernel Hilbert space (RKHS). For certain distance and kernel\npairs, we show the distance-based conditional independence measures to be\nequivalent to that of kernel-based measures. On the other hand, we also show\nthat some popular---in machine learning---kernel conditional independence\nmeasures based on the Hilbert-Schmidt norm of a certain cross-conditional\ncovariance operator, do not have a simple distance representation, except in\nsome limiting cases. This paper, therefore, shows the distance and kernel\nmeasures of conditional independence to be not quite equivalent unlike in the\ncase of joint independence as shown by Sejdinovic et al. (2013).\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:37:21 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 16:05:52 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sheng", "Tianhong", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1912.01105", "submitter": "Javier Trejos", "authors": "Jeffry Chavarria-Molina, Juan Jose Fallas-Monge, Javier Trejos-Zelaya", "title": "Clustering via Ant Colonies: Parameter Analysis and Improvement of the\n  Algorithm", "comments": "19 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An ant colony optimization approach for partitioning a set of objects is\nproposed. In order to minimize the intra-variance, or within sum-of-squares, of\nthe partitioned classes, we construct ant-like solutions by a constructive\napproach that selects objects to be put in a class with a probability that\ndepends on the distance between the object and the centroid of the class\n(visibility) and the pheromone trail; the latter depends on the class\nmemberships that have been defined along the iterations. The procedure is\nimproved with the application of K-means algorithm in some iterations of the\nant colony method. We performed a simulation study in order to evaluate the\nmethod with a Monte Carlo experiment that controls some sensitive parameters of\nthe clustering problem. After some tuning of the parameters, the method has\nalso been applied to some benchmark real-data sets. Encouraging results were\nobtained in nearly all cases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:38:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chavarria-Molina", "Jeffry", ""], ["Fallas-Monge", "Juan Jose", ""], ["Trejos-Zelaya", "Javier", ""]]}, {"id": "1912.01108", "submitter": "David Inouye", "authors": "David I. Inouye, Liu Leqi, Joon Sik Kim, Bryon Aragam, Pradeep\n  Ravikumar", "title": "Automated Dependence Plots", "comments": "In Uncertainty in Artificial Intelligence (UAI 2020). Camera-ready\n  version. Code is available at https://github.com/davidinouye/adp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications of machine learning, it is necessary to look beyond\nstandard metrics such as test accuracy in order to validate various qualitative\nproperties of a model. Partial dependence plots (PDP), including\ninstance-specific PDPs (i.e., ICE plots), have been widely used as a visual\ntool to understand or validate a model. Yet, current PDPs suffer from two main\ndrawbacks: (1) a user must manually sort or select interesting plots, and (2)\nPDPs are usually limited to plots along a single feature. To address these\ndrawbacks, we formalize a method for automating the selection of interesting\nPDPs and extend PDPs beyond showing single features to show the model response\nalong arbitrary directions, for example in raw feature space or a latent space\narising from some generative model. We demonstrate the usefulness of our\nautomated dependence plots (ADP) across multiple use-cases and datasets\nincluding model selection, bias detection, understanding out-of-sample\nbehavior, and exploring the latent space of a generative model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:46:55 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 15:56:08 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 21:00:01 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Inouye", "David I.", ""], ["Leqi", "Liu", ""], ["Kim", "Joon Sik", ""], ["Aragam", "Bryon", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1912.01109", "submitter": "Ngoc L\\^e", "authors": "Ngoc C. L\\^e, Ngoc-Yen Nguyen, and Anh-Duong Trinh", "title": "On the Vietnamese Name Entity Recognition: A Deep Learning Method\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) plays an important role in text-based\ninformation retrieval. In this paper, we combine Bidirectional Long Short-Term\nMemory (Bi-LSTM) \\cite{hochreiter1997,schuster1997} with Conditional Random\nField (CRF) \\cite{lafferty2001} to create a novel deep learning model for the\nNER problem. Each word as input of the deep learning model is represented by a\nWord2vec-trained vector. A word embedding set trained from about one million\narticles in 2018 collected through a Vietnamese news portal (baomoi.com). In\naddition, we concatenate a Word2Vec\\cite{mikolov2013}-trained vector with\nsemantic feature vector (Part-Of-Speech (POS) tagging, chunk-tag) and hidden\nsyntactic feature vector (extracted by Bi-LSTM nerwork) to achieve the (so far\nbest) result in Vietnamese NER system. The result was conducted on the data set\nVLSP2016 (Vietnamese Language and Speech Processing 2016 \\cite{vlsp2016})\ncompetition.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 13:28:37 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["L\u00ea", "Ngoc C.", ""], ["Nguyen", "Ngoc-Yen", ""], ["Trinh", "Anh-Duong", ""]]}, {"id": "1912.01110", "submitter": "Vijini Supun Keerthisrini Pilana Liyanage", "authors": "Vijini Liyanage and Surangika Ranathunga", "title": "A Multi-language Platform for Generating Algebraic Mathematical Word\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for automatically generating mathematical word problems\nare deprived of customizability and creativity due to the inherent nature of\ntemplate-based mechanisms they employ. We present a solution to this problem\nwith the use of deep neural language generation mechanisms. Our approach uses a\nCharacter Level Long Short Term Memory Network (LSTM) to generate word\nproblems, and uses POS (Part of Speech) tags to resolve the constraints found\nin the generated problems. Our approach is capable of generating Mathematics\nWord Problems in both English and Sinhala languages with an accuracy over 90%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 04:50:45 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Liyanage", "Vijini", ""], ["Ranathunga", "Surangika", ""]]}, {"id": "1912.01115", "submitter": "Karol Chlasta", "authors": "Karol Chlasta, Krzysztof Wo{\\l}k, Izabela Krejtz", "title": "Automated speech-based screening of depression using deep convolutional\n  neural networks", "comments": "10 pages, 8 figures and 2 tables, HCist 2019 - 8th International\n  Conference on Health and Social Care Information Systems and Technologies\n  (16-18 October 2019, Sousse, Tunisia)", "journal-ref": "Procedia Computer Science 164 (2019) 618-628", "doi": "10.1016/j.procs.2019.12.228", "report-no": null, "categories": "cs.LG cs.CV cs.CY cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and treatment of depression is essential in promoting\nremission, preventing relapse, and reducing the emotional burden of the\ndisease. Current diagnoses are primarily subjective, inconsistent across\nprofessionals, and expensive for individuals who may be in urgent need of help.\nThis paper proposes a novel approach to automated depression detection in\nspeech using convolutional neural network (CNN) and multipart interactive\ntraining. The model was tested using 2568 voice samples obtained from 77\nnon-depressed and 30 depressed individuals. In experiment conducted, data were\napplied to residual CNNs in the form of spectrograms, images auto-generated\nfrom audio samples. The experimental results obtained using different ResNet\narchitectures gave a promising baseline accuracy reaching 77%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:58:40 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Chlasta", "Karol", ""], ["Wo\u0142k", "Krzysztof", ""], ["Krejtz", "Izabela", ""]]}, {"id": "1912.01116", "submitter": "Jeremy Gordon", "authors": "Jeremy Gordon, David Rawlinson, Subutai Ahmad", "title": "Long Distance Relationships without Time Travel: Boosting the\n  Performance of a Sparse Predictive Autoencoder in Sequence Modeling", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequence learning tasks such as language modelling, Recurrent Neural\nNetworks must learn relationships between input features separated by time.\nState of the art models such as LSTM and Transformer are trained by\nbackpropagation of losses into prior hidden states and inputs held in memory.\nThis allows gradients to flow from present to past and effectively learn with\nperfect hindsight, but at a significant memory cost. In this paper we show that\nit is possible to train high performance recurrent networks using information\nthat is local in time, and thereby achieve a significantly reduced memory\nfootprint. We describe a predictive autoencoder called bRSM featuring recurrent\nconnections, sparse activations, and a boosting rule for improved cell\nutilization. The architecture demonstrates near optimal performance on a\nnon-deterministic (stochastic) partially-observable sequence learning task\nconsisting of high-Markov-order sequences of MNIST digits. We find that this\nmodel learns these sequences faster and more completely than an LSTM, and offer\nseveral possible explanations why the LSTM architecture might struggle with the\npartially observable sequence structure in this task. We also apply our model\nto a next word prediction task on the Penn Treebank (PTB) dataset. We show that\na 'flattened' RSM network, when paired with a modern semantic word embedding\nand the addition of boosting, achieves 103.5 PPL (a 20-point improvement over\nthe best N-gram models), beating ordinary RNNs trained with BPTT and\napproaching the scores of early LSTM implementations. This work provides\nencouraging evidence that strong results on challenging tasks such as language\nmodelling may be possible using less memory intensive, biologically-plausible\ntraining regimes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:00:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gordon", "Jeremy", ""], ["Rawlinson", "David", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1912.01130", "submitter": "Zhou Yang", "authors": "Zhou Yang, Vinay Jayachandra Reddy, Rashmi Kesidi, Fang Jin", "title": "Addict Free -- A Smart and Connected Relapse Intervention Mobile App", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is widely acknowledged that addiction relapse is highly associated with\nspatial-temporal factors such as some specific places or time periods. Current\nstudies suggest that those factors can be utilized for better relapse\ninterventions, however, there is no relapse prevention application that makes\nuse of those factors. In this paper, we introduce a mobile app called \"Addict\nFree\", which records user profiles, tracks relapse history and summarizes\nrecovering statistics to help users better understand their recovering\nsituations. Also, this app builds a relapse recovering community, which allows\nusers to ask for advice and encouragement, and share relapse prevention\nexperience. Moreover, machine learning algorithms that ingest spatial and\ntemporal factors are utilized to predict relapse, based on which helpful\naddiction diversion activities are recommended by a recovering recommendation\nalgorithm. By interacting with users, this app targets at providing smart\nsuggestions that aim to stop relapse, especially for alcohol and tobacco\naddiction users.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:38:22 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Yang", "Zhou", ""], ["Reddy", "Vinay Jayachandra", ""], ["Kesidi", "Rashmi", ""], ["Jin", "Fang", ""]]}, {"id": "1912.01137", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Mixing autoencoder with classifier: conceptual data visualization", "comments": null, "journal-ref": "IEEE Access, vol. 8, no. 1, pp. 105301-105310, 2020", "doi": "10.1109/ACCESS.2020.2999155", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, a neural network that is able to form a low dimensional\ntopological hidden representation is explained. The neural network can be\ntrained as an autoencoder, a classifier or mix of both, and produces different\nlow dimensional topological map for each of them. When it is trained as an\nautoencoder, the inherent topological structure of the data can be visualized,\nwhile when it is trained as a classifier, the topological structure is further\nconstrained by the concept, for example the labels the data, hence the\nvisualization is not only structural but also conceptual. The proposed neural\nnetwork significantly differ from many dimensional reduction models, primarily\nin its ability to execute both supervised and unsupervised dimensional\nreduction. The neural network allows multi perspective visualization of the\ndata, and thus giving more flexibility in data analysis. This paper is\nsupported by preliminary but intuitive visualization experiments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:33:26 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:46:11 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 10:27:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "1912.01139", "submitter": "Hao Huang", "authors": "Fei Huang and Hao Huang", "title": "Event Ticket Price Prediction with Deep Neural Network on\n  Spatial-Temporal Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event ticket price prediction is important to marketing strategy for any\nsports team or musical ensemble. An accurate prediction model can help the\nmarketing team to make promotion plan more effectively and efficiently.\nHowever, given all the historical transaction records, it is challenging to\npredict the sale price of the remaining seats at any future timestamp, not only\nbecause that the sale price is relevant to a lot of features (seat locations,\ndate-to-event of the transaction, event date, team performance, etc.), but also\nbecause of the temporal and spatial sparsity in the dataset. For a\ngame/concert, the ticket selling price of one seat is only observable once at\nthe time of sale. Furthermore, some seats may not even be purchased (therefore\nno record available). In fact, data sparsity is commonly encountered in many\nprediction problems. Here, we propose a bi-level optimizing deep neural network\nto address the curse of spatio-temporal sparsity. Specifically, we introduce\ncoarsening and refining layers, and design a bi-level loss function to\nintegrate different level of loss for better prediction accuracy. Our model can\ndiscover the interrelations among ticket sale price, seat locations, selling\ntime, event information, etc. Experiments show that our proposed model\noutperforms other benchmark methods in real-world ticket selling price\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:53:57 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 22:03:40 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Huang", "Fei", ""], ["Huang", "Hao", ""]]}, {"id": "1912.01163", "submitter": "Brighter Agyemang", "authors": "Brighter Agyemang and Wei-Ping Wu and Michael Y. Kpiebaareh and\n  Ebenezer Nanor", "title": "Drug-Target Indication Prediction by Integrating End-to-End Learning and\n  Fingerprints", "comments": "Accepted at IEEE ICCWAMTIP 2019", "journal-ref": null, "doi": "10.1109/ICCWAMTIP47768.2019.9067510", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Drug Discovery research has proven to be a promising direction\nin drug discovery. In recent years, Deep Learning approaches have been applied\nto problems in the domain such as Drug-Target Interaction Prediction and have\nshown improvements over traditional screening methods. An existing challenge is\nhow to represent compound-target pairs in deep learning models. While several\nrepresentation methods exist, such descriptor schemes tend to complement one\nanother in many instances, as reported in the literature. In this study, we\npropose a multi-view architecture trained adversarially to leverage this\ncomplementary behavior by integrating both differentiable and predefined\nmolecular descriptors. We conduct experiments on clinically relevant benchmark\ndatasets to demonstrate the potential of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:43:19 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 04:25:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Agyemang", "Brighter", ""], ["Wu", "Wei-Ping", ""], ["Kpiebaareh", "Michael Y.", ""], ["Nanor", "Ebenezer", ""]]}, {"id": "1912.01170", "submitter": "Mahdi Haghifam", "authors": "Mahdi Haghifam, Vincent Y. F. Tan, Ashish Khisti", "title": "Sequential Classification with Empirically Observed Statistics", "comments": "17 Pages, 5 Figures. To appear in the IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by real-world machine learning applications, we consider a\nstatistical classification task in a sequential setting where test samples\narrive sequentially. In addition, the generating distributions are unknown and\nonly a set of empirically sampled sequences are available to a decision maker.\nThe decision maker is tasked to classify a test sequence which is known to be\ngenerated according to either one of the distributions. In particular, for the\nbinary case, the decision maker wishes to perform the classification task with\nminimum number of the test samples, so, at each step, she declares that either\nhypothesis 1 is true, hypothesis 2 is true, or she requests for an additional\ntest sample. We propose a classifier and analyze the type-I and type-II error\nprobabilities. We demonstrate the significant advantage of our sequential\nscheme compared to an existing non-sequential classifier proposed by Gutman.\nFinally, we extend our setup and results to the multi-class classification\nscenario and again demonstrate that the variable-length nature of the problem\naffords significant advantages as one can achieve the same set of exponents as\nGutman's fixed-length setting but without having the rejection option.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:59:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:27:16 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 03:04:07 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Haghifam", "Mahdi", ""], ["Tan", "Vincent Y. F.", ""], ["Khisti", "Ashish", ""]]}, {"id": "1912.01172", "submitter": "Smitha Milli", "authors": "Ravit Dotan and Smitha Milli", "title": "Value-laden Disciplinary Shifts in Machine Learning", "comments": "Accepted to FAT* 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning models are increasingly used for high-stakes decision\nmaking, scholars have sought to intervene to ensure that such models do not\nencode undesirable social and political values. However, little attention thus\nfar has been given to how values influence the machine learning discipline as a\nwhole. How do values influence what the discipline focuses on and the way it\ndevelops? If undesirable values are at play at the level of the discipline,\nthen intervening on particular models will not suffice to address the problem.\nInstead, interventions at the disciplinary-level are required. This paper\nanalyzes the discipline of machine learning through the lens of philosophy of\nscience. We develop a conceptual framework to evaluate the process through\nwhich types of machine learning models (e.g. neural networks, support vector\nmachines, graphical models) become predominant. The rise and fall of\nmodel-types is often framed as objective progress. However, such disciplinary\nshifts are more nuanced. First, we argue that the rise of a model-type is\nself-reinforcing--it influences the way model-types are evaluated. For example,\nthe rise of deep learning was entangled with a greater focus on evaluations in\ncompute-rich and data-rich environments. Second, the way model-types are\nevaluated encodes loaded social and political values. For example, a greater\nfocus on evaluations in compute-rich and data-rich environments encodes values\nabout centralization of power, privacy, and environmental concerns.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:01:27 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Dotan", "Ravit", ""], ["Milli", "Smitha", ""]]}, {"id": "1912.01188", "submitter": "Kevin Lu", "authors": "Kevin Lu, Igor Mordatch, Pieter Abbeel", "title": "Adaptive Online Planning for Continual Lifelong Learning", "comments": "Originally published in NeurIPS Deep RL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning control in an online reset-free lifelong learning scenario,\nwhere mistakes can compound catastrophically into the future and the underlying\ndynamics of the environment may change. Traditional model-free policy learning\nmethods have achieved successes in difficult tasks due to their broad\nflexibility, but struggle in this setting, as they can activate failure modes\nearly in their lifetimes which are difficult to recover from and face\nperformance degradation as dynamics change. On the other hand, model-based\nplanning methods learn and adapt quickly, but require prohibitive levels of\ncomputational resources. We present a new algorithm, Adaptive Online Planning\n(AOP), that achieves strong performance in this setting by combining\nmodel-based planning with model-free learning. By approximating the uncertainty\nof the model-free components and the planner performance, AOP is able to call\nupon more extensive planning only when necessary, leading to reduced\ncomputation times, while still gracefully adapting behaviors in the face of\nunpredictable changes in the world -- even when traditional RL fails.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 04:29:01 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 05:28:56 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lu", "Kevin", ""], ["Mordatch", "Igor", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1912.01189", "submitter": "Jeremiah Zhe Liu", "authors": "Jeremiah Zhe Liu", "title": "Variable Selection with Rigorous Uncertainty Quantification using Deep\n  Bayesian Neural Networks: Posterior Concentration and Bernstein-von Mises\n  Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops rigorous theoretical basis for the fact that deep Bayesian\nneural network (BNN) is an effective tool for high-dimensional variable\nselection with rigorous uncertainty quantification. We develop new Bayesian\nnon-parametric theorems to show that a properly configured deep BNN (1) learns\nthe variable importance effectively in high dimensions, and its learning rate\ncan sometimes \"break\" the curse of dimensionality. (2) BNN's uncertainty\nquantification for variable importance is rigorous, in the sense that its 95%\ncredible intervals for variable importance indeed covers the truth 95% of the\ntime (i.e., the Bernstein-von Mises (BvM) phenomenon). The theoretical results\nsuggest a simple variable selection algorithm based on the BNN's credible\nintervals. Extensive simulation confirms the theoretical findings and shows\nthat the proposed algorithm outperforms existing classic and\nneural-network-based variable selection methods, particularly in high\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 04:36:21 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Liu", "Jeremiah Zhe", ""]]}, {"id": "1912.01192", "submitter": "Tiancheng Jin", "authors": "Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, Tiancheng Yu", "title": "Learning Adversarial MDPs with Bandit Feedback and Unknown Transition", "comments": "Fix a bug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning in episodic finite-horizon Markov\ndecision processes with an unknown transition function, bandit feedback, and\nadversarial losses. We propose an efficient algorithm that achieves\n$\\mathcal{\\tilde{O}}(L|X|\\sqrt{|A|T})$ regret with high probability, where $L$\nis the horizon, $|X|$ is the number of states, $|A|$ is the number of actions,\nand $T$ is the number of episodes. To the best of our knowledge, our algorithm\nis the first to ensure $\\mathcal{\\tilde{O}}(\\sqrt{T})$ regret in this\nchallenging setting; in fact it achieves the same regret bound as (Rosenberg &\nMansour, 2019a) that considers an easier setting with full-information\nfeedback. Our key technical contributions are two-fold: a tighter confidence\nset for the transition function, and an optimistic loss estimator that is\ninversely weighted by an $\\textit{upper occupancy bound}$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:04:40 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 05:06:43 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 04:36:59 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 13:38:22 GMT"}, {"version": "v5", "created": "Mon, 2 Nov 2020 07:13:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Jin", "Chi", ""], ["Jin", "Tiancheng", ""], ["Luo", "Haipeng", ""], ["Sra", "Suvrit", ""], ["Yu", "Tiancheng", ""]]}, {"id": "1912.01197", "submitter": "Zhao Kang", "authors": "Zhao Kang and Xiao Lu and Yiwei Lu and Chong Peng and Zenglin Xu", "title": "Structure Learning with Similarity Preserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging on the underlying low-dimensional structure of data, low-rank and\nsparse modeling approaches have achieved great success in a wide range of\napplications. However, in many applications the data can display structures\nbeyond simply being low-rank or sparse. Fully extracting and exploiting hidden\nstructure information in the data is always desirable and favorable. To reveal\nmore underlying effective manifold structure, in this paper, we explicitly\nmodel the data relation. Specifically, we propose a structure learning\nframework that retains the pairwise similarities between the data points.\nRather than just trying to reconstruct the original data based on\nself-expression, we also manage to reconstruct the kernel matrix, which\nfunctions as similarity preserving. Consequently, this technique is\nparticularly suitable for the class of learning problems that are sensitive to\nsample similarity, e.g., clustering and semisupervised classification. To take\nadvantage of representation power of deep neural network, a deep auto-encoder\narchitecture is further designed to implement our model. Extensive experiments\non benchmark data sets demonstrate that our proposed framework can consistently\nand significantly improve performance on both evaluation tasks. We conclude\nthat the quality of structure learning can be enhanced if similarity\ninformation is incorporated.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:25:08 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Lu", "Yiwei", ""], ["Peng", "Chong", ""], ["Xu", "Zenglin", ""]]}, {"id": "1912.01198", "submitter": "Quanquan Gu", "authors": "Yuan Cao and Zhiying Fang and Yue Wu and Ding-Xuan Zhou and Quanquan\n  Gu", "title": "Towards Understanding the Spectral Bias of Deep Learning", "comments": "29 pages, 7 figures. This version adds more experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intriguing phenomenon observed during training neural networks is the\nspectral bias, which states that neural networks are biased towards learning\nless complex functions. The priority of learning functions with low complexity\nmight be at the core of explaining generalization ability of neural network,\nand certain efforts have been made to provide theoretical explanation for\nspectral bias. However, there is still no satisfying theoretical result\njustifying the underlying mechanism of spectral bias. In this paper, we give a\ncomprehensive and rigorous explanation for spectral bias and relate it with the\nneural tangent kernel function proposed in recent work. We prove that the\ntraining process of neural networks can be decomposed along different\ndirections defined by the eigenfunctions of the neural tangent kernel, where\neach direction has its own convergence rate and the rate is determined by the\ncorresponding eigenvalue. We then provide a case study when the input data is\nuniformly distributed over the unit sphere, and show that lower degree\nspherical harmonics are easier to be learned by over-parameterized neural\nnetworks. Finally, we provide numerical experiments to demonstrate the\ncorrectness of our theory. Our experimental results also show that our theory\ncan tolerate certain model misspecification in terms of the input data\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:34:30 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 04:19:21 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 17:51:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Cao", "Yuan", ""], ["Fang", "Zhiying", ""], ["Wu", "Yue", ""], ["Zhou", "Ding-Xuan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1912.01201", "submitter": "Zhao Kang", "authors": "Juncheng Lv and Zhao Kang and Boyu Wang and Luping Ji and Zenglin Xu", "title": "Multi-view Subspace Clustering via Partition Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering is an important approach to analyze multi-view data in\nan unsupervised way. Among various methods, the multi-view subspace clustering\napproach has gained increasing attention due to its encouraging performance.\nBasically, it integrates multi-view information into graphs, which are then fed\ninto spectral clustering algorithm for final result. However, its performance\nmay degrade due to noises existing in each individual view or inconsistency\nbetween heterogeneous features. Orthogonal to current work, we propose to fuse\nmulti-view information in a partition space, which enhances the robustness of\nMulti-view clustering. Specifically, we generate multiple partitions and\nintegrate them to find the shared partition. The proposed model unifies graph\nlearning, generation of basic partitions, and view weight learning. These three\ncomponents co-evolve towards better quality outputs. We have conducted\ncomprehensive experiments on benchmark datasets and our empirical results\nverify the effectiveness and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:48:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lv", "Juncheng", ""], ["Kang", "Zhao", ""], ["Wang", "Boyu", ""], ["Ji", "Luping", ""], ["Xu", "Zenglin", ""]]}, {"id": "1912.01203", "submitter": "Lifeng Tan", "authors": "Lifeng Tan, Cong Jin, Zhiyuan Cheng, Xin Lv, Leiyu Song", "title": "Music Style Classification with Compared Methods in XGB and BPNN", "comments": "5 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists have used many different classification methods to solve the\nproblem of music classification. But the efficiency of each classification is\ndifferent. In this paper, we propose two compared methods on the task of music\nstyle classification. More specifically, feature extraction for representing\ntimbral texture, rhythmic content and pitch content are proposed. Comparative\nevaluations on performances of two classifiers were conducted for music\nclassification with different styles. The result shows that XGB is better\nsuited for small datasets than BPNN\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:54:52 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Tan", "Lifeng", ""], ["Jin", "Cong", ""], ["Cheng", "Zhiyuan", ""], ["Lv", "Xin", ""], ["Song", "Leiyu", ""]]}, {"id": "1912.01206", "submitter": "Nilesh Ahuja", "authors": "Mahesh Subedar, Nilesh Ahuja, Ranganath Krishnan, Ibrahima J. Ndiour,\n  Omesh Tickoo", "title": "Deep Probabilistic Models to Detect Data Poisoning Attacks", "comments": "To appear in Bayesian Deep Learning Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning attacks compromise the integrity of machine-learning models by\nintroducing malicious training samples to influence the results during test\ntime. In this work, we investigate backdoor data poisoning attack on deep\nneural networks (DNNs) by inserting a backdoor pattern in the training images.\nThe resulting attack will misclassify poisoned test samples while maintaining\nhigh accuracies for the clean test-set. We present two approaches for detection\nof such poisoned samples by quantifying the uncertainty estimates associated\nwith the trained models. In the first approach, we model the outputs of the\nvarious layers (deep features) with parametric probability distributions learnt\nfrom the clean held-out dataset. At inference, the likelihoods of deep features\nw.r.t these distributions are calculated to derive uncertainty estimates. In\nthe second approach, we use Bayesian deep neural networks trained with\nmean-field variational inference to estimate model uncertainty associated with\nthe predictions. The uncertainty estimates from these methods are used to\ndiscriminate clean from the poisoned samples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:58:51 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Subedar", "Mahesh", ""], ["Ahuja", "Nilesh", ""], ["Krishnan", "Ranganath", ""], ["Ndiour", "Ibrahima J.", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1912.01211", "submitter": "Quanquan Gu", "authors": "Tao Jin and Pan Xu and Quanquan Gu and Farzad Farnoud", "title": "Rank Aggregation via Heterogeneous Thurstone Preference Models", "comments": "36 pages, 2 figures, 8 tables. In AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Heterogeneous Thurstone Model (HTM) for aggregating ranked\ndata, which can take the accuracy levels of different users into account. By\nallowing different noise distributions, the proposed HTM model maintains the\ngenerality of Thurstone's original framework, and as such, also extends the\nBradley-Terry-Luce (BTL) model for pairwise comparisons to heterogeneous\npopulations of users. Under this framework, we also propose a rank aggregation\nalgorithm based on alternating gradient descent to estimate the underlying item\nscores and accuracy levels of different users simultaneously from noisy\npairwise comparisons. We theoretically prove that the proposed algorithm\nconverges linearly up to a statistical error which matches that of the\nstate-of-the-art method for the single-user BTL model. We evaluate the proposed\nHTM model and algorithm on both synthetic and real data, demonstrating that it\noutperforms existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:23:19 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jin", "Tao", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""], ["Farnoud", "Farzad", ""]]}, {"id": "1912.01234", "submitter": "Armin K\\\"uper", "authors": "Armin K\\\"uper and Steffen Waldherr", "title": "Numerical Gaussian process Kalman filtering", "comments": "6 pages, 3 figures, this work has been accepted by IFAC for\n  publication (\\copyright 2020 IFAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we introduce numerical Gaussian process Kalman filtering\n(GPKF). Numerical Gaussian processes have recently been developed to simulate\nspatiotemporal models. The contribution of this paper is to embed numerical\nGaussian processes into the recursive Kalman filter equations. This embedding\nenables us to do Kalman filtering on infinite-dimensional systems using\nGaussian processes. This is possible because i) we are obtaining a linear model\nfrom numerical Gaussian processes, and ii) the states of this model are by\ndefinition Gaussian distributed random variables. Convenient properties of the\nnumerical GPKF are that no spatial discretization of the model is necessary,\nand manual setting up of the Kalman filter, that is fine-tuning the process and\nmeasurement noise levels by hand is not required, as they are learned online\nfrom the data stream. We showcase the capability of the numerical GPKF in a\nsimulation study of the advection equation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:09:27 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 13:27:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["K\u00fcper", "Armin", ""], ["Waldherr", "Steffen", ""]]}, {"id": "1912.01238", "submitter": "Patrick Chen", "authors": "Patrick H. Chen, Wei Wei, Cho-jui Hsieh, Bo Dai", "title": "Overcoming Catastrophic Forgetting by Generative Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method to overcome catastrophic forgetting by\nadding generative regularization to Bayesian inference framework. Bayesian\nmethod provides a general framework for continual learning. We could further\nconstruct a generative regularization term for all given classification models\nby leveraging energy-based models and Langevin-dynamic sampling to enrich the\nfeatures learned in each task. By combining discriminative and generative loss\ntogether, we empirically show that the proposed method outperforms\nstate-of-the-art methods on a variety of tasks, avoiding catastrophic\nforgetting in continual learning. In particular, the proposed method\noutperforms baseline methods over 15% on the Fashion-MNIST dataset and 10% on\nthe CUB dataset\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:17:46 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 07:43:43 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 06:22:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Patrick H.", ""], ["Wei", "Wei", ""], ["Hsieh", "Cho-jui", ""], ["Dai", "Bo", ""]]}, {"id": "1912.01241", "submitter": "Jaemoon Lee", "authors": "Jaemoon Lee, Hoda Shajari", "title": "A Hidden Variables Approach to Multilabel Logistic Regression", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilabel classification is an important problem in a wide range of domains\nsuch as text categorization and music annotation. In this paper, we present a\nprobabilistic model, Multilabel Logistic Regression with Hidden variables\n(MLRH), which extends the standard logistic regression by introducing hidden\nvariables. Hidden variables make it possible to go beyond the conventional\nmulticlass logistic regression by relaxing the one-hot-encoding constraint. We\ndefine a new joint distribution of labels and hidden variables which enables us\nto obtain one classifier for multilabel classification. Our experimental\nstudies on a set of benchmark datasets demonstrate that the probabilistic model\ncan achieve competitive performance compared with other multilabel learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:33:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lee", "Jaemoon", ""], ["Shajari", "Hoda", ""]]}, {"id": "1912.01242", "submitter": "Qinge Xie", "authors": "Qinge Xie and Tiancheng Guo and Yang Chen and Yu Xiao and Xin Wang and\n  Ben Y. Zhao", "title": "\"How do urban incidents affect traffic speed?\" A Deep Graph\n  Convolutional Network for Incident-driven Traffic Speed Prediction", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate traffic speed prediction is an important and challenging topic for\ntransportation planning. Previous studies on traffic speed prediction\npredominately used spatio-temporal and context features for prediction.\nHowever, they have not made good use of the impact of urban traffic incidents.\nIn this work, we aim to make use of the information of urban incidents to\nachieve a better prediction of traffic speed. Our incident-driven prediction\nframework consists of three processes. First, we propose a critical incident\ndiscovery method to discover urban traffic incidents with high impact on\ntraffic speed. Second, we design a binary classifier, which uses deep learning\nmethods to extract the latent incident impact features from the middle layer of\nthe classifier. Combining above methods, we propose a Deep Incident-Aware Graph\nConvolutional Network (DIGC-Net) to effectively incorporate urban traffic\nincident, spatio-temporal, periodic and context features for traffic speed\nprediction. We conduct experiments on two real-world urban traffic datasets of\nSan Francisco and New York City. The results demonstrate the superior\nperformance of our model compare to the competing benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:39:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Xie", "Qinge", ""], ["Guo", "Tiancheng", ""], ["Chen", "Yang", ""], ["Xiao", "Yu", ""], ["Wang", "Xin", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "1912.01261", "submitter": "Jonathan Lee", "authors": "Jonathan Lee, Ching-An Cheng, Ken Goldberg, Byron Boots", "title": "Continuous Online Learning and New Insights to Online Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning is a powerful tool for analyzing iterative algorithms.\nHowever, the classic adversarial setup sometimes fails to capture certain\nregularity in online problems in practice. Motivated by this, we establish a\nnew setup, called Continuous Online Learning (COL), where the gradient of\nonline loss function changes continuously across rounds with respect to the\nlearner's decisions. We show that COL covers and more appropriately describes\nmany interesting applications, from general equilibrium problems (EPs) to\noptimization in episodic MDPs. Using this new setup, we revisit the difficulty\nof achieving sublinear dynamic regret. We prove that there is a fundamental\nequivalence between achieving sublinear dynamic regret in COL and solving\ncertain EPs, and we present a reduction from dynamic regret to both static\nregret and convergence rate of the associated EP. At the end, we specialize\nthese new insights into online imitation learning and show improved\nunderstanding of its learning stability.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:44:56 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lee", "Jonathan", ""], ["Cheng", "Ching-An", ""], ["Goldberg", "Ken", ""], ["Boots", "Byron", ""]]}, {"id": "1912.01266", "submitter": "Simon Meyer Lauritsen", "authors": "Simon Meyer Lauritsen, Mads Kristensen, Mathias Vassard Olsen, Morten\n  Skaarup Larsen, Katrine Meyer Lauritsen, Marianne Johansson J{\\o}rgensen,\n  Jeppe Lange, Bo Thiesson", "title": "Explainable artificial intelligence model to predict acute critical\n  illness from electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an explainable artificial intelligence (AI) early warning score\n(xAI-EWS) system for early detection of acute critical illness. While\nmaintaining a high predictive performance, our system explains to the clinician\non which relevant electronic health records (EHRs) data the prediction is\ngrounded. Acute critical illness is often preceded by deterioration of\nroutinely measured clinical parameters, e.g., blood pressure and heart rate.\nEarly clinical prediction is typically based on manually calculated screening\nmetrics that simply weigh these parameters, such as Early Warning Scores (EWS).\nThe predictive performance of EWSs yields a tradeoff between sensitivity and\nspecificity that can lead to negative outcomes for the patient. Previous work\non EHR-trained AI systems offers promising results with high levels of\npredictive performance in relation to the early, real-time prediction of acute\ncritical illness. However, without insight into the complex decisions by such\nsystem, clinical translation is hindered. In this letter, we present our\nxAI-EWS system, which potentiates clinical translation by accompanying a\nprediction with information on the EHR data explaining it.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:52:20 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lauritsen", "Simon Meyer", ""], ["Kristensen", "Mads", ""], ["Olsen", "Mathias Vassard", ""], ["Larsen", "Morten Skaarup", ""], ["Lauritsen", "Katrine Meyer", ""], ["J\u00f8rgensen", "Marianne Johansson", ""], ["Lange", "Jeppe", ""], ["Thiesson", "Bo", ""]]}, {"id": "1912.01274", "submitter": "Matan Haroush", "authors": "Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry", "title": "The Knowledge Within: Methods for Data-Free Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an extensive amount of research has been focused on compressing and\naccelerating Deep Neural Networks (DNN). So far, high compression rate\nalgorithms require part of the training dataset for a low precision\ncalibration, or a fine-tuning process. However, this requirement is\nunacceptable when the data is unavailable or contains sensitive information, as\nin medical and biometric use-cases. We present three methods for generating\nsynthetic samples from trained models. Then, we demonstrate how these samples\ncan be used to calibrate and fine-tune quantized models without using any real\ndata in the process. Our best performing method has a negligible accuracy\ndegradation compared to the original training set. This method, which leverages\nintrinsic batch normalization layers' statistics of the trained model, can be\nused to evaluate data similarity. Our approach opens a path towards genuine\ndata-free model compression, alleviating the need for training data during\nmodel deployment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:01:51 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 19:00:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Haroush", "Matan", ""], ["Hubara", "Itay", ""], ["Hoffer", "Elad", ""], ["Soudry", "Daniel", ""]]}, {"id": "1912.01277", "submitter": "Christian Sch\\\"on", "authors": "Christian Sch\\\"on, Jens Dittrich", "title": "Make Thunderbolts Less Frightening -- Predicting Extreme Weather Using\n  Deep Learning", "comments": "similar to the version accepted as poster for the workshop \"Tackling\n  Climate Change with Machine Learning\" at the 33rd Conference on Neural\n  Information Processing Systems (NeurIPS 2019) in Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting severe weather conditions is still a very challenging and\ncomputationally expensive task due to the enormous amount of data and the\ncomplexity of the underlying physics. Machine learning approaches and\nespecially deep learning have however shown huge improvements in many research\nareas dealing with large datasets in recent years. In this work, we tackle one\nspecific sub-problem of weather forecasting, namely the prediction of\nthunderstorms and lightning. We propose the use of a convolutional neural\nnetwork architecture inspired by UNet++ and ResNet to predict thunderstorms as\na binary classification problem based on satellite images and lightnings\nrecorded in the past. We achieve a probability of detection of more than 94%\nfor lightnings within the next 15 minutes while at the same time minimizing the\nfalse alarm ratio compared to previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:17:48 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 08:01:37 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Sch\u00f6n", "Christian", ""], ["Dittrich", "Jens", ""]]}, {"id": "1912.01303", "submitter": "Quoc Hung Ngo", "authors": "Quoc Hung Ngo and Nhien-An Le-Khac and Tahar Kechadi", "title": "Predicting Soil pH by Using Nearest Fields", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-34885-4", "report-no": "LNAI, volume 11927", "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In precision agriculture (PA), soil sampling and testing operation is prior\nto planting any new crop. It is an expensive operation since there are many\nsoil characteristics to take into account. This paper gives an overview of soil\ncharacteristics and their relationships with crop yield and soil profiling. We\npropose an approach for predicting soil pH based on nearest neighbour fields.\nIt implements spatial radius queries and various regression techniques in data\nmining. We use soil dataset containing about 4,000 fields profiles to evaluate\nthem and analyse their robustness. A comparative study indicates that LR, SVR,\nand GBRT techniques achieved high accuracy, with the R_2 values of about 0.718\nand MAE values of 0.29. The experimental results showed that the proposed\napproach is very promising and can contribute significantly to PA.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:20:45 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Ngo", "Quoc Hung", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1912.01308", "submitter": "Othmane Mazhar", "authors": "Othmane Mazhar, Cristian R. Rojas, Carlo Fischione and Mohammad R.\n  Hesamzadeh", "title": "Bayesian Model Selection for Change Point Detection and Clustering", "comments": "37 page, 4 figures, Proceedings of the 35th International Conference\n  on Machine Learning (ICML), PMLR 80:3433-3442, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the new problem of estimating a piece-wise constant signal with\nthe purpose of detecting its change points and the levels of clusters. Our\napproach is to model it as a nonparametric penalized least square model\nselection on a family of models indexed over the collection of partitions of\nthe design points and propose a computationally efficient algorithm to\napproximately solve it. Statistically, minimizing such a penalized criterion\nyields an approximation to the maximum a posteriori probability (MAP)\nestimator. The criterion is then analyzed and an oracle inequality is derived\nusing a Gaussian concentration inequality. The oracle inequality is used to\nderive on one hand conditions for consistency and on the other hand an adaptive\nupper bound on the expected square risk of the estimator, which statistically\nmotivates our approximation. Finally, we apply our algorithm to simulated data\nto experimentally validate the statistical guarantees and illustrate its\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:28:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mazhar", "Othmane", ""], ["Rojas", "Cristian R.", ""], ["Fischione", "Carlo", ""], ["Hesamzadeh", "Mohammad R.", ""]]}, {"id": "1912.01321", "submitter": "Zifeng Wang", "authors": "Zifeng Wang and Hong Zhu and Zhenhua Dong and Xiuqiang He and Shao-Lun\n  Huang", "title": "Less Is Better: Unweighted Data Subsampling via Influence Function", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the time of Big Data, training complex models on large-scale data sets is\nchallenging, making it appealing to reduce data volume for saving computation\nresources by subsampling. Most previous works in subsampling are weighted\nmethods designed to help the performance of subset-model approach the\nfull-set-model, hence the weighted methods have no chance to acquire a\nsubset-model that is better than the full-set-model. However, we question that\nhow can we achieve better model with less data? In this work, we propose a\nnovel Unweighted Influence Data Subsampling (UIDS) method, and prove that the\nsubset-model acquired through our method can outperform the full-set-model.\nBesides, we show that overly confident on a given test set for sampling is\ncommon in Influence-based subsampling methods, which can eventually cause our\nsubset-model's failure in out-of-sample test. To mitigate it, we develop a\nprobabilistic sampling scheme to control the worst-case risk over all\ndistributions close to the empirical distribution. The experiment results\ndemonstrate our methods superiority over existed subsampling methods in diverse\ntasks, such as text classification, image classification, click-through\nprediction, etc.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:48:53 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 00:56:52 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 05:16:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Zifeng", ""], ["Zhu", "Hong", ""], ["Dong", "Zhenhua", ""], ["He", "Xiuqiang", ""], ["Huang", "Shao-Lun", ""]]}, {"id": "1912.01329", "submitter": "Jingyue Lu", "authors": "Jingyue Lu and M. Pawan Kumar", "title": "Neural Network Branching for Neural Network Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal verification of neural networks is essential for their deployment in\nsafety-critical areas. Many available formal verification methods have been\nshown to be instances of a unified Branch and Bound (BaB) formulation. We\npropose a novel framework for designing an effective branching strategy for\nBaB. Specifically, we learn a graph neural network (GNN) to imitate the strong\nbranching heuristic behaviour. Our framework differs from previous methods for\nlearning to branch in two main aspects. Firstly, our framework directly treats\nthe neural network we want to verify as a graph input for the GNN. Secondly, we\ndevelop an intuitive forward and backward embedding update schedule.\nEmpirically, our framework achieves roughly $50\\%$ reduction in both the number\nof branches and the time required for verification on various convolutional\nnetworks when compared to the best available hand-designed branching strategy.\nIn addition, we show that our GNN model enjoys both horizontal and vertical\ntransferability. Horizontally, the model trained on easy properties performs\nwell on properties of increased difficulty levels. Vertically, the model\ntrained on small neural networks achieves similar performance on large neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 12:12:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lu", "Jingyue", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1912.01389", "submitter": "Taesun Moon", "authors": "Taesun Moon, Parul Awasthy, Jian Ni, Radu Florian", "title": "Towards Lingua Franca Named Entity Recognition with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction is an important task in NLP, enabling the automatic\nextraction of data for relational database filling. Historically, research and\ndata was produced for English text, followed in subsequent years by datasets in\nArabic, Chinese (ACE/OntoNotes), Dutch, Spanish, German (CoNLL evaluations),\nand many others. The natural tendency has been to treat each language as a\ndifferent dataset and build optimized models for each. In this paper we\ninvestigate a single Named Entity Recognition model, based on a multilingual\nBERT, that is trained jointly on many languages simultaneously, and is able to\ndecode these languages with better accuracy than models trained only on one\nlanguage. To improve the initial model, we study the use of regularization\nstrategies such as multitask learning and partial gradient updates. In addition\nto being a single model that can tackle multiple languages (including code\nswitch), the model could be used to make zero-shot predictions on a new\nlanguage, even ones for which training data is not available, out of the box.\nThe results show that this model not only performs competitively with\nmonolingual models, but it also achieves state-of-the-art results on the\nCoNLL02 Dutch and Spanish datasets, OntoNotes Arabic and Chinese datasets.\nMoreover, it performs reasonably well on unseen languages, achieving\nstate-of-the-art for zero-shot on three CoNLL languages.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:48:02 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:23:41 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Moon", "Taesun", ""], ["Awasthy", "Parul", ""], ["Ni", "Jian", ""], ["Florian", "Radu", ""]]}, {"id": "1912.01398", "submitter": "So Takamoto", "authors": "So Takamoto, Satoshi Izumi, Ju Li", "title": "TeaNet: universal neural network interatomic potential inspired by\n  iterative electronic relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A universal interatomic potential applicable to arbitrary elements and\nstructures is urgently needed in computational materials science. Graph\nconvolution-based neural network is a promising approach by virtue of its\nability to express complex relations. Thus far, it has been thought to\nrepresent a completely different approach from physics-based interatomic\npotentials. In this paper, we show that these two methods can be regarded as\ndifferent representations of the same tight-binding electronic relaxation\nframework, where atom-based and overlap integral or \"bond\"-based Hamiltonian\ninformation are propagated in a directional fashion. Based on this unified\nview, we propose a new model, named the tensor embedded atom network (TeaNet),\nwhere the stacked network model is associated with the electronic total energy\nrelaxation calculation. Furthermore, Tersoff-style angular interaction is\ntranslated into graph convolution architecture through the incorporation of\nEuclidean tensor values. Our model can represent and transfer spatial\ninformation. TeaNet shows great performance in both the robustness of\ninteratomic potentials and the expressive power of neural networks. We\ndemonstrate that arbitrary chemistry involving the first 18 elements on the\nperiodic table (H to Ar) can be realized by our model, including C-H molecular\nstructures, metals, amorphous SiO${}_2$, and water.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:47:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Takamoto", "So", ""], ["Izumi", "Satoshi", ""], ["Li", "Ju", ""]]}, {"id": "1912.01417", "submitter": "Dominic Richards", "authors": "Dominic Richards, Sahand N. Negahban, Patrick Rebeschini", "title": "Decentralised Sparse Multi-Task Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse multi-task regression framework for fitting a collection\nof related sparse models. Representing models as nodes in a graph with edges\nbetween related models, a framework that fuses lasso regressions with the total\nvariation penalty is investigated. Under a form of restricted eigenvalue\nassumption, bounds on prediction and squared error are given that depend upon\nthe sparsity of each model and the differences between related models. This\nassumption relates to the smallest eigenvalue restricted to the intersection of\ntwo cone sets of the covariance matrix constructed from each of the agents'\ncovariances. We show that this assumption can be satisfied if the constructed\ncovariance matrix satisfies a restricted isometry property. In the case of a\ngrid topology high-probability bounds are given that match, up to log factors,\nthe no-communication setting of fitting a lasso on each model, divided by the\nnumber of agents. A decentralised dual method that exploits a convex-concave\nformulation of the penalised problem is proposed to fit the models and its\neffectiveness demonstrated on simulations against the group lasso and variants.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:39:22 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Richards", "Dominic", ""], ["Negahban", "Sahand N.", ""], ["Rebeschini", "Patrick", ""]]}, {"id": "1912.01419", "submitter": "Lorenzo Dall'Amico", "authors": "Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay", "title": "Optimal Laplacian regularization for sparse spectral community detection", "comments": null, "journal-ref": "ICASSP 2020-2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization of the classical Laplacian matrices was empirically shown to\nimprove spectral clustering in sparse networks. It was observed that small\nregularizations are preferable, but this point was left as a heuristic\nargument. In this paper we formally determine a proper regularization which is\nintimately related to alternative state-of-the-art spectral techniques for\nsparse graphs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:43:07 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 08:21:44 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Dall'Amico", "Lorenzo", ""], ["Couillet", "Romain", ""], ["Tremblay", "Nicolas", ""]]}, {"id": "1912.01422", "submitter": "Anthony Constantinou", "authors": "Norman Fenton, Martin Neil and Anthony Constantinou", "title": "Simpson's Paradox and the implications for medical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Simpson's paradox, and explains its serious implications\nfor randomised control trials. In particular, we show that for any number of\nvariables we can simulate the result of a controlled trial which uniformly\npoints to one conclusion (such as 'drug is effective') for every possible\ncombination of the variable states, but when a previously unobserved\nconfounding variable is included every possible combination of the variables\nstate points to the opposite conclusion ('drug is not effective'). In other\nwords no matter how many variables are considered, and no matter how\n'conclusive' the result, one cannot conclude the result is truly 'valid' since\nthere is theoretically an unobserved confounding variable that could completely\nreverse the result.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:47:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fenton", "Norman", ""], ["Neil", "Martin", ""], ["Constantinou", "Anthony", ""]]}, {"id": "1912.01443", "submitter": "Aleksey Buzmakov", "authors": "Aleksey Buzmakov, Daria Semenova, Maria Temirkaeva", "title": "The Comparison of Methods for Individual Treatment Effect Detection", "comments": "12 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, treatment effect estimation at the individual level is a vital problem\nin many areas of science and business. For example, in marketing, estimates of\nthe treatment effect are used to select the most efficient promo-mechanics; in\nmedicine, individual treatment effects are used to determine the optimal dose\nof medication for each patient and so on. At the same time, the question on\nchoosing the best method, i.e., the method that ensures the smallest predictive\nerror (for instance, RMSE) or the highest total (average) value of the effect,\nremains open. Accordingly, in this paper we compare the effectiveness of\nmachine learning methods for estimation of individual treatment effects. The\ncomparison is performed on the Criteo Uplift Modeling Dataset. In this paper we\nshow that the combination of the Logistic Regression method and the Difference\nScore method as well as Uplift Random Forest method provide the best\ncorrectness of Individual Treatment Effect prediction on the top 30\\%\nobservations of the test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 15:05:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Semenova", "Daria", ""], ["Temirkaeva", "Maria", ""]]}, {"id": "1912.01447", "submitter": "Xu Shen", "authors": "Xu Shen, Xinmei Tian, Anfeng He, Shaoyan Sun, Dacheng Tao", "title": "Transform-Invariant Convolutional Neural Networks for Image\n  Classification and Search", "comments": "Accepted by ACM Multimedia. arXiv admin note: text overlap with\n  arXiv:1911.12682", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved state-of-the-art results\non many visual recognition tasks. However, current CNN models still exhibit a\npoor ability to be invariant to spatial transformations of images. Intuitively,\nwith sufficient layers and parameters, hierarchical combinations of convolution\n(matrix multiplication and non-linear activation) and pooling operations should\nbe able to learn a robust mapping from transformed input images to\ntransform-invariant representations. In this paper, we propose randomly\ntransforming (rotation, scale, and translation) feature maps of CNNs during the\ntraining stage. This prevents complex dependencies of specific rotation, scale,\nand translation levels of training images in CNN models. Rather, each\nconvolutional kernel learns to detect a feature that is generally helpful for\nproducing the transform-invariant answer given the combinatorially large\nvariety of transform levels of its input feature maps. In this way, we do not\nrequire any extra training supervision or modification to the optimization\nprocess and training images. We show that random transformation provides\nsignificant improvements of CNNs on many benchmark tasks, including small-scale\nimage recognition, large-scale image recognition, and image retrieval. The code\nis available at https://github.com/jasonustc/caffe-multigpu/tree/TICNN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:09:21 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["He", "Anfeng", ""], ["Sun", "Shaoyan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1912.01448", "submitter": "Daniel McNamee", "authors": "Daniel McNamee", "title": "Hierarchical model-based policy optimization: from actions to action\n  sequences and back", "comments": "NeurIPS 2019 Optimization Foundations of Reinforcement Learning\n  Workshop. v2: typos fixed, minor edits for improved clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop a normative framework for hierarchical model-based policy\noptimization based on applying second-order methods in the space of all\npossible state-action paths. The resulting natural path gradient performs\npolicy updates in a manner which is sensitive to the long-range correlational\nstructure of the induced stationary state-action densities. We demonstrate that\nthe natural path gradient can be computed exactly given an environment dynamics\nmodel and depends on expressions akin to higher-order successor\nrepresentations. In simulation, we show that the priorization of local policy\nupdates in the resulting policy flow indeed reflects the intuitive state-space\nhierarchy in several toy problems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 19:01:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 13:11:31 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["McNamee", "Daniel", ""]]}, {"id": "1912.01449", "submitter": "Min Yang", "authors": "Cong Xu, Min Yang and Jin Zhang", "title": "A Fast deflation Method for Sparse Principal Component Analysis via\n  Subspace Projections", "comments": "4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of conventional sparse principal component analysis (SPCA)\non high-dimensional data sets has become a time consuming work. In this paper,\na series of subspace projections are constructed efficiently by using Household\nQR factorization. With the aid of these subspace projections, a fast deflation\nmethod, called SPCA-SP, is developed for SPCA. This method keeps a good\ntradeoff between various criteria, including sparsity, orthogonality, explained\nvariance, balance of sparsity, and computational cost. Comparative experiments\non the benchmark data sets confirm the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 15:10:11 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 00:04:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Xu", "Cong", ""], ["Yang", "Min", ""], ["Zhang", "Jin", ""]]}, {"id": "1912.01450", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang and Beilun Wang", "title": "Fast and Scalable Estimator for Sparse and Unit-Rank Higher-Order\n  Regression Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.12965", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because tensor data appear more and more frequently in various scientific\nresearches and real-world applications, analyzing the relationship between\ntensor features and the univariate outcome becomes an elementary task in many\nfields. To solve this task, we propose \\underline{Fa}st \\underline{S}parse\n\\underline{T}ensor \\underline{R}egression model (FasTR) based on so-called\nunit-rank CANDECOMP/PARAFAC decomposition. FasTR first decomposes the tensor\ncoefficient into component vectors and then estimates each vector with $\\ell_1$\nregularized regression. Because of the independence of component vectors, FasTR\nis able to solve in a parallel way and the time complexity is proved to be\nsuperior to previous models. We evaluate the performance of FasTR on several\nsimulated datasets and a real-world fMRI dataset. Experiment results show that,\ncompared with four baseline models, in every case, FasTR can compute a better\nsolution within less time.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 06:59:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhang", "Jiaqi", ""], ["Wang", "Beilun", ""]]}, {"id": "1912.01451", "submitter": "Richard Tomsett", "authors": "Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram,\n  Alun Preece", "title": "Sanity Checks for Saliency Metrics", "comments": "Accepted for publication at the Thirty Fourth AAAI conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps are a popular approach to creating post-hoc explanations of\nimage classifier outputs. These methods produce estimates of the relevance of\neach pixel to the classification output score, which can be displayed as a\nsaliency map that highlights important pixels. Despite a proliferation of such\nmethods, little effort has been made to quantify how good these saliency maps\nare at capturing the true relevance of the pixels to the classifier output\n(i.e. their \"fidelity\"). We therefore investigate existing metrics for\nevaluating the fidelity of saliency methods (i.e. saliency metrics). We find\nthat there is little consistency in the literature in how such metrics are\ncalculated, and show that such inconsistencies can have a significant effect on\nthe measured fidelity. Further, we apply measures of reliability developed in\nthe psychometric testing literature to assess the consistency of saliency\nmetrics when applied to individual saliency maps. Our results show that\nsaliency metrics can be statistically unreliable and inconsistent, indicating\nthat comparative rankings between saliency methods generated using such metrics\ncan be untrustworthy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:30:56 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Tomsett", "Richard", ""], ["Harborne", "Dan", ""], ["Chakraborty", "Supriyo", ""], ["Gurram", "Prudhvi", ""], ["Preece", "Alun", ""]]}, {"id": "1912.01455", "submitter": "Ali Al-Aradi", "authors": "Ali Al-Aradi, Adolfo Correia, Danilo de Frietas Naiff, Gabriel Jardim,\n  Yuri Saporito", "title": "Applications of the Deep Galerkin Method to Solving Partial\n  Integro-Differential and Hamilton-Jacobi-Bellman Equations", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.08782", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Deep Galerkin Method (DGM) introduced in Sirignano and\nSpiliopoulos (2018) to solve a number of partial differential equations (PDEs)\nthat arise in the context of optimal stochastic control and mean field games.\nFirst, we consider PDEs where the function is constrained to be positive and\nintegrate to unity, as is the case with Fokker-Planck equations. Our approach\ninvolves reparameterizing the solution as the exponential of a neural network\nappropriately normalized to ensure both requirements are satisfied. This then\ngives rise to a partial integro-differential equation (PIDE) where the integral\nappearing in the equation is handled using importance sampling. Secondly, we\ntackle a number of Hamilton-Jacobi-Bellman (HJB) equations that appear in\nstochastic optimal control problems. The key contribution is that these\nequations are approached in their unsimplified primal form which includes an\noptimization problem as part of the equation. We extend the DGM algorithm to\nsolve for the value function and the optimal control simultaneously by\ncharacterizing both as deep neural networks. Training the networks is performed\nby taking alternating stochastic gradient descent steps for the two functions,\na technique similar in spirit to policy improvement algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:07:29 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 20:18:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Al-Aradi", "Ali", ""], ["Correia", "Adolfo", ""], ["Naiff", "Danilo de Frietas", ""], ["Jardim", "Gabriel", ""], ["Saporito", "Yuri", ""]]}, {"id": "1912.01493", "submitter": "Eli (Omid) David", "authors": "Ishai Rosenberg, Guillaume Sicard, Eli David", "title": "End-to-End Deep Neural Networks and Transfer Learning for Automatic\n  Analysis of Nation-State Malware", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09666", "journal-ref": "Entropy, Vol. 20, No. 5, pp. 390-401, May 2018", "doi": "10.3390/e20050390", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware allegedly developed by nation-states, also known as advanced\npersistent threats (APT), are becoming more common. The task of attributing an\nAPT to a specific nation-state or classifying it to the correct APT family is\nchallenging for several reasons. First, each nation-state has more than a\nsingle cyber unit that develops such malware, rendering traditional authorship\nattribution algorithms useless. Furthermore, the dataset of such available APTs\nis still extremely small. Finally, those APTs use state-of-the-art evasion\ntechniques, making feature extraction challenging. In this paper, we use a deep\nneural network (DNN) as a classifier for nation-state APT attribution. We\nrecord the dynamic behavior of the APT when run in a sandbox and use it as raw\ninput for the neural network, allowing the DNN to learn high level feature\nabstractions of the APTs itself. We also use the same raw features for APT\nfamily classification. Finally, we use the feature abstractions learned by the\nAPT family classifier to solve the attribution problem. Using a test set of\n1000 Chinese and Russian developed APTs, we achieved an accuracy rate of 98.6%.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:21:26 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Sicard", "Guillaume", ""], ["David", "Eli", ""]]}, {"id": "1912.01494", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu", "title": "Supervised and Unsupervised End-to-End Deep Learning for Gene Ontology\n  Classification of Neural In Situ Hybridization Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09663", "journal-ref": "Entropy, Vol. 21, No. 3, pp. 221-238, February 2019", "doi": "10.3390/e21030221", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large datasets of high-resolution mammalian neural images\nhave become available, which has prompted active research on the analysis of\ngene expression data. Traditional image processing methods are typically\napplied for learning functional representations of genes, based on their\nexpressions in these brain images. In this paper, we describe a novel\nend-to-end deep learning-based method for generating compact representations of\nin situ hybridization (ISH) images, which are invariant-to-translation. In\ncontrast to traditional image processing methods, our method relies, instead,\non deep convolutional denoising autoencoders (CDAE) for processing raw pixel\ninputs, and generating the desired compact image representations. We provide an\nin-depth description of our deep learning-based approach, and present extensive\nexperimental results, demonstrating that representations extracted by CDAE can\nhelp learn features of functional gene ontology categories for their\nclassification in a highly accurate manner. Our methods improve the previous\nstate-of-the-art classification rate (Liscovitch, et al.) from an average AUC\nof 0.92 to 0.997, i.e., it achieves 96% reduction in error rate. Furthermore,\nthe representation vectors generated due to our method are more compact in\ncomparison to previous state-of-the-art methods, allowing for a more efficient\nhigh-level representation of images. These results are obtained with\nsignificantly downsampled images in comparison to the original high-resolution\nones, further underscoring the robustness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:20:12 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.01506", "submitter": "Rodrigo de Lamare", "authors": "H. Ruan and R. C. de Lamare", "title": "Study of Distributed Robust Beamforming with Low-Rank and\n  Cross-Correlation Techniques", "comments": "14 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1712.01115", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel robust distributed beamforming (RDB)\napproach based on low-rank and cross-correlation techniques. The proposed RDB\napproach mitigates the effects of channel errors in wireless networks equipped\nwith relays based on the exploitation of the cross-correlation between the\nreceived data from the relays at the destination and the system output and\nlow-rank techniques. The relay nodes are equipped with an amplify-and-forward\n(AF) protocol and the channel errors are modeled using an additive matrix\nperturbation, which results in degradation of the system performance. The\nproposed method, denoted low-rank and cross-correlation RDB (LRCC-RDB),\nconsiders a total relay transmit power constraint in the system and the goal of\nmaximizing the output signal-to-interference-plus-noise ratio (SINR). We carry\nout a performance analysis of the proposed LRCC-RDB technique along with a\ncomputational complexity study. The proposed LRCC-RDB does not require any\ncostly online optimization procedure and simulations show an excellent\nperformance as compared to previously reported algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 01:29:47 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ruan", "H.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "1912.01521", "submitter": "Oren Barkan", "authors": "Oren Barkan", "title": "Multiscale Self Attentive Convolutions for Vision and Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self attention mechanisms have become a key building block in many\nstate-of-the-art language understanding models. In this paper, we show that the\nself attention operator can be formulated in terms of 1x1 convolution\noperations. Following this observation, we propose several novel operators:\nFirst, we introduce a 2D version of self attention that is applicable for 2D\nsignals such as images. Second, we present the 1D and 2D Self Attentive\nConvolutions (SAC) operator that generalizes self attention beyond 1x1\nconvolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self\nattention operate on individual words and pixels, SAC operates on m-grams and\nimage patches, respectively. Third, we present a multiscale version of SAC\n(MSAC) which analyzes the input by employing multiple SAC operators that vary\nby filter size, in parallel. Finally, we explain how MSAC can be utilized for\nvision and language modeling, and further harness MSAC to form a cross\nattentive image similarity machinery.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:51:09 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Barkan", "Oren", ""]]}, {"id": "1912.01530", "submitter": "Joani Mitro", "authors": "John Mitros and Brian Mac Namee", "title": "On the Validity of Bayesian Neural Networks for Uncertainty Estimation", "comments": "AICS2019, fixed typos, figures, tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) are versatile parametric models utilised\nsuccessfully in a diverse number of tasks and domains. However, they have\nlimitations---particularly from their lack of robustness and over-sensitivity\nto out of distribution samples. Bayesian Neural Networks, due to their\nformulation under the Bayesian framework, provide a principled approach to\nbuilding neural networks that address these limitations. This paper describes a\nstudy that empirically evaluates and compares Bayesian Neural Networks to their\nequivalent point estimate Deep Neural Networks to quantify the predictive\nuncertainty induced by their parameters, as well as their performance in view\nof this uncertainty. In this study, we evaluated and compared three point\nestimate deep neural networks against comparable Bayesian neural network\nalternatives using two well-known benchmark image classification datasets\n(CIFAR-10 and SVHN).\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:18:08 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 21:38:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mitros", "John", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1912.01553", "submitter": "Joel Michelson", "authors": "Joel Michelson, Joshua H. Palmer, Aneesha Dasari, Maithilee Kunda", "title": "Learning Spatially Structured Image Transformations Using Planar Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning image transformations is essential to the idea of mental simulation\nas a method of cognitive inference. We take a connectionist modeling approach,\nusing planar neural networks to learn fundamental imagery transformations, like\ntranslation, rotation, and scaling, from perceptual experiences in the form of\nimage sequences. We investigate how variations in network topology, training\ndata, and image shape, among other factors, affect the efficiency and\neffectiveness of learning visual imagery transformations, including\neffectiveness of transfer to operating on new types of data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:54:35 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 00:46:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Michelson", "Joel", ""], ["Palmer", "Joshua H.", ""], ["Dasari", "Aneesha", ""], ["Kunda", "Maithilee", ""]]}, {"id": "1912.01557", "submitter": "Xinyang Gu", "authors": "Jingbin Liu, Xinyang Gu, Shuai Liu", "title": "Policy Optimization Reinforcement Learning with Entropy Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy regularization is an important idea in reinforcement learning, with\ngreat success in recent algorithms like Soft Q Network (SQN) and Soft\nActor-Critic (SAC1). In this work, we extend this idea into the on-policy\nrealm. We propose the soft policy gradient theorem (SPGT) for on-policy maximum\nentropy reinforcement learning. With SPGT, a series of new policy optimization\nalgorithms are derived, such as SPG, SA2C, SA3C, SDDPG, STRPO, SPPO, SIMPALA\nand so on. We find that SDDPG is equivalent to SAC1. For policy gradient, the\npolicy network is often represented as a Gaussian distribution with a global\naction variance, which damages the representation capacity. We introduce a\nlocal action variance for policy network and find it can work collaboratively\nwith the idea of entropy regularization. Our method outperforms prior works on\na range of benchmark tasks. Furthermore, our method can be easily extended to\nlarge scale experiment with great stability and parallelism.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:01:32 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 03:28:45 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 05:51:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liu", "Jingbin", ""], ["Gu", "Xinyang", ""], ["Liu", "Shuai", ""]]}, {"id": "1912.01580", "submitter": "Thanapapas Horsuwan", "authors": "Thanapapas Horsuwan, Kasidis Kanwatchara, Peerapon Vateekul, Boonserm\n  Kijsirikul", "title": "A Comparative Study of Pretrained Language Models on Thai Social Text\n  Categorization", "comments": "12 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ever-growing volume of data of user-generated content on social media\nprovides a nearly unlimited corpus of unlabeled data even in languages where\nresources are scarce. In this paper, we demonstrate that state-of-the-art\nresults on two Thai social text categorization tasks can be realized by\npretraining a language model on a large noisy Thai social media corpus of over\n1.26 billion tokens and later fine-tuned on the downstream classification\ntasks. Due to the linguistically noisy and domain-specific nature of the\ncontent, our unique data preprocessing steps designed for Thai social media\nwere utilized to ease the training comprehension of the model. We compared four\nmodern language models: ULMFiT, ELMo with biLSTM, OpenAI GPT, and BERT. We\nsystematically compared the models across different dimensions including speed\nof pretraining and fine-tuning, perplexity, downstream classification\nbenchmarks, and performance in limited pretraining data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:26:13 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 07:47:56 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Horsuwan", "Thanapapas", ""], ["Kanwatchara", "Kasidis", ""], ["Vateekul", "Peerapon", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1912.01588", "submitter": "Karl Cobbe", "authors": "Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman", "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Procgen Benchmark, a suite of 16 procedurally generated\ngame-like environments designed to benchmark both sample efficiency and\ngeneralization in reinforcement learning. We believe that the community will\nbenefit from increased access to high quality training environments, and we\nprovide detailed experimental protocols for using this benchmark. We\nempirically demonstrate that diverse environment distributions are essential to\nadequately train and evaluate RL agents, thereby motivating the extensive use\nof procedural content generation. We then use this benchmark to investigate the\neffects of scaling model size, finding that larger models significantly improve\nboth sample efficiency and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:34:03 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 18:39:26 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cobbe", "Karl", ""], ["Hesse", "Christopher", ""], ["Hilton", "Jacob", ""], ["Schulman", "John", ""]]}, {"id": "1912.01592", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela, Maria Perez-Ortiz, Emine Yilmaz and John\n  Shawe-Taylor", "title": "Towards an Integrative Educational Recommender for Lifelong Learners", "comments": "In Proceedings of AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most ambitious use cases of computer-assisted learning is to build\na recommendation system for lifelong learning. Most recommender algorithms\nexploit similarities between content and users, overseeing the necessity to\nleverage sensible learning trajectories for the learner. Lifelong learning thus\npresents unique challenges, requiring scalable and transparent models that can\naccount for learner knowledge and content novelty simultaneously, while also\nretaining accurate learners representations for long periods of time. We\nattempt to build a novel educational recommender, that relies on an integrative\napproach combining multiple drivers of learners engagement. Our first step\ntowards this goal is TrueLearn, which models content novelty and background\nknowledge of learners and achieves promising performance while retaining a\nhuman interpretable learner model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:40:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bulathwela", "Sahan", ""], ["Perez-Ortiz", "Maria", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1912.01597", "submitter": "Konstantin Mishchenko", "authors": "Dmitry Kovalev and Konstantin Mishchenko and Peter Richt\\'arik", "title": "Stochastic Newton and Cubic Newton Methods with Simple Local\n  Linear-Quadratic Rates", "comments": "16 pages, 2 figures, 3 algorithms, 2 theorems, 7 lemmas; to be\n  presented at the NeurIPS workshop \"Beyond First Order Methods in ML\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new remarkably simple stochastic second-order methods for\nminimizing the average of a very large number of sufficiently smooth and\nstrongly convex functions. The first is a stochastic variant of Newton's method\n(SN), and the second is a stochastic variant of cubically regularized Newton's\nmethod (SCN). We establish local linear-quadratic convergence results. Unlike\nexisting stochastic variants of second order methods, which require the\nevaluation of a large number of gradients and/or Hessians in each iteration to\nguarantee convergence, our methods do not have this shortcoming. For instance,\nthe simplest variants of our methods in each iteration need to compute the\ngradient and Hessian of a {\\em single} randomly selected function only. In\ncontrast to most existing stochastic Newton and quasi-Newton methods, our\napproach guarantees local convergence faster than with first-order oracle and\nadapts to the problem's curvature. Interestingly, our method is not unbiased,\nso our theory provides new intuition for designing new stochastic methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:51:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Kovalev", "Dmitry", ""], ["Mishchenko", "Konstantin", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1912.01599", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, Ilias Zadik", "title": "Stationary Points of Shallow Neural Networks with Quadratic Activation\n  Function", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the teacher-student setting of learning shallow neural networks\nwith quadratic activations and planted weight matrix $W^*\\in\\mathbb{R}^{m\\times\nd}$, where $m$ is the width of the hidden layer and $d\\le m$ is the data\ndimension. We study the optimization landscape associated with the empirical\nand the population squared risk of the problem. Under the assumption the\nplanted weights are full-rank we obtain the following results. First, we\nestablish that the landscape of the empirical risk admits an \"energy barrier\"\nseparating rank-deficient $W$ from $W^*$: if $W$ is rank deficient, then its\nrisk is bounded away from zero by an amount we quantify. We then couple this\nresult by showing that, assuming number $N$ of samples grows at least like a\npolynomial function of $d$, all full-rank approximate stationary points of the\nempirical risk are nearly global optimum. These two results allow us to prove\nthat gradient descent, when initialized below the energy barrier, approximately\nminimizes the empirical risk and recovers the planted weights in\npolynomial-time. Next, we show that initializing below this barrier is in fact\neasily achieved when the weights are randomly generated under relatively weak\nassumptions. We show that provided the network is sufficiently\noverparametrized, initializing with an appropriate multiple of the identity\nsuffices to obtain a risk below the energy barrier. At a technical level, the\nlast result is a consequence of the semicircle law for the Wishart ensemble and\ncould be of independent interest. Finally, we study the minimizers of the\nempirical risk and identify a simple necessary and sufficient geometric\ncondition on the training data under which any minimizer has necessarily zero\ngeneralization error. We show that as soon as $N\\ge N^*=d(d+1)/2$, randomly\ngenerated data enjoys this geometric condition almost surely, while that ceases\nto be true if $N<N^*$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:52:37 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:21:23 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 22:02:14 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "1912.01649", "submitter": "Samuel Ainsworth", "authors": "Samuel Ainsworth, Matt Barnes, Siddhartha Srinivasa", "title": "Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many environments, only a relatively small subset of the complete state\nspace is necessary in order to accomplish a given task. We develop a simple\ntechnique using emergency stops (e-stops) to exploit this phenomenon. Using\ne-stops significantly improves sample complexity by reducing the amount of\nrequired exploration, while retaining a performance bound that efficiently\ntrades off the rate of convergence with a small asymptotic sub-optimality gap.\nWe analyze the regret behavior of e-stops and present empirical results in\ndiscrete and continuous settings demonstrating that our reset mechanism can\nprovide order-of-magnitude speedups on top of existing reinforcement learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 19:41:37 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Ainsworth", "Samuel", ""], ["Barnes", "Matt", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1912.01666", "submitter": "Leena Chennuru Vankadara", "authors": "Leena Chennuru Vankadara, Siavash Haghiri, Michael Lohaus, Faiz Ul\n  Wahab, Ulrike von Luxburg", "title": "Insights into Ordinal Embedding Algorithms: A Systematic Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of ordinal embedding is to find a Euclidean representation of a\nset of abstract items, using only answers to triplet comparisons of the form\n\"Is item $i$ closer to the item $j$ or item $k$?\". In recent years, numerous\nalgorithms have been proposed to solve this problem. However, there does not\nexist a fair and thorough assessment of these embedding methods and therefore\nseveral key questions remain unanswered: Which algorithms scale better with\nincreasing sample size or dimension? Which ones perform better when the\nembedding dimension is small or few triplet comparisons are available? In our\npaper, we address these questions and provide the first comprehensive and\nsystematic empirical evaluation of existing algorithms as well as a new neural\nnetwork approach. In the large triplet regime, we find that simple, relatively\nunknown, non-convex methods consistently outperform all other algorithms,\nincluding elaborate approaches based on neural networks or landmark approaches.\nThis finding can be explained by our insight that many of the non-convex\noptimization approaches do not suffer from local optima. In the low triplet\nregime, our neural network approach is either competitive or significantly\noutperforms all the other methods. Our comprehensive assessment is enabled by\nour unified library of popular embedding algorithms that leverages GPU\nresources and allows for fast and accurate embeddings of millions of data\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:06:36 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 18:05:04 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 17:00:46 GMT"}, {"version": "v4", "created": "Fri, 6 Nov 2020 15:41:20 GMT"}, {"version": "v5", "created": "Wed, 11 Nov 2020 13:46:48 GMT"}, {"version": "v6", "created": "Wed, 2 Dec 2020 22:09:59 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Vankadara", "Leena Chennuru", ""], ["Haghiri", "Siavash", ""], ["Lohaus", "Michael", ""], ["Wahab", "Faiz Ul", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1912.01667", "submitter": "Siddhant Bhambri", "authors": "Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, Arun Balaji Buduru", "title": "A Survey of Black-Box Adversarial Attacks on Computer Vision Models", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has seen tremendous advances in the past few years, which\nhas lead to deep learning models being deployed in varied applications of\nday-to-day life. Attacks on such models using perturbations, particularly in\nreal-life scenarios, pose a severe challenge to their applicability, pushing\nresearch into the direction which aims to enhance the robustness of these\nmodels. After the introduction of these perturbations by Szegedy et al. [1],\nsignificant amount of research has focused on the reliability of such models,\nprimarily in two aspects - white-box, where the adversary has access to the\ntargeted model and related parameters; and the black-box, which resembles a\nreal-life scenario with the adversary having almost no knowledge of the model\nto be attacked. To provide a comprehensive security cover, it is essential to\nidentify, study, and build defenses against such attacks. Hence, in this paper,\nwe propose to present a comprehensive comparative study of various black-box\nadversarial attacks and defense techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:06:49 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 07:33:59 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 09:17:38 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Bhambri", "Siddhant", ""], ["Muku", "Sumanyu", ""], ["Tulasi", "Avinash", ""], ["Buduru", "Arun Balaji", ""]]}, {"id": "1912.01698", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Yifang Chen, Krishnakumar Balasubramanian, Prasant\n  Mohapatra", "title": "Online and Bandit Algorithms for Nonstationary Stochastic Saddle-Point\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saddle-point optimization problems are an important class of optimization\nproblems with applications to game theory, multi-agent reinforcement learning\nand machine learning. A majority of the rich literature available for\nsaddle-point optimization has focused on the offline setting. In this paper, we\nstudy nonstationary versions of stochastic, smooth, strongly-convex and\nstrongly-concave saddle-point optimization problem, in both online (or\nfirst-order) and multi-point bandit (or zeroth-order) settings. We first\npropose natural notions of regret for such nonstationary saddle-point\noptimization problems. We then analyze extragradient and Frank-Wolfe\nalgorithms, for the unconstrained and constrained settings respectively, for\nthe above class of nonstationary saddle-point optimization problems. We\nestablish sub-linear regret bounds on the proposed notions of regret in both\nthe online and bandit setting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:52:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Roy", "Abhishek", ""], ["Chen", "Yifang", ""], ["Balasubramanian", "Krishnakumar", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1912.01703", "submitter": "Soumith Chintala", "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\n  Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\n  Alban Desmaison, Andreas K\\\"opf, Edward Yang, Zach DeVito, Martin Raison,\n  Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\n  Soumith Chintala", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "comments": "12 pages, 3 figures, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks have often focused on either usability or speed, but\nnot both. PyTorch is a machine learning library that shows that these two goals\nare in fact compatible: it provides an imperative and Pythonic programming\nstyle that supports code as a model, makes debugging easy and is consistent\nwith other popular scientific computing libraries, while remaining efficient\nand supporting hardware accelerators such as GPUs.\n  In this paper, we detail the principles that drove the implementation of\nPyTorch and how they are reflected in its architecture. We emphasize that every\naspect of PyTorch is a regular Python program under the full control of its\nuser. We also explain how the careful and pragmatic implementation of the key\ncomponents of its runtime enables them to work together to achieve compelling\nperformance.\n  We demonstrate the efficiency of individual subsystems, as well as the\noverall speed of PyTorch on several common benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:06:05 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Paszke", "Adam", ""], ["Gross", "Sam", ""], ["Massa", "Francisco", ""], ["Lerer", "Adam", ""], ["Bradbury", "James", ""], ["Chanan", "Gregory", ""], ["Killeen", "Trevor", ""], ["Lin", "Zeming", ""], ["Gimelshein", "Natalia", ""], ["Antiga", "Luca", ""], ["Desmaison", "Alban", ""], ["K\u00f6pf", "Andreas", ""], ["Yang", "Edward", ""], ["DeVito", "Zach", ""], ["Raison", "Martin", ""], ["Tejani", "Alykhan", ""], ["Chilamkurthy", "Sasank", ""], ["Steiner", "Benoit", ""], ["Fang", "Lu", ""], ["Bai", "Junjie", ""], ["Chintala", "Soumith", ""]]}, {"id": "1912.01706", "submitter": "Nicolas Garneau", "authors": "Nicolas Garneau, Mathieu Godbout, David Beauchemin, Audrey Durand, Luc\n  Lamontagne", "title": "A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual\n  Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well", "comments": "Accept in REPROLANG@LREC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we reproduce the experiments of Artetxe et al. (2018b)\nregarding the robust self-learning method for fully unsupervised cross-lingual\nmappings of word embeddings. We show that the reproduction of their method is\nindeed feasible with some minor assumptions. We further investigate the\nrobustness of their model by introducing four new languages that are less\nsimilar to English than the ones proposed by the original paper. In order to\nassess the stability of their model, we also conduct a grid search over\nsensible hyperparameters. We then propose key recommendations applicable to any\nresearch project in order to deliver fully reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:07:47 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 14:30:50 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Garneau", "Nicolas", ""], ["Godbout", "Mathieu", ""], ["Beauchemin", "David", ""], ["Durand", "Audrey", ""], ["Lamontagne", "Luc", ""]]}, {"id": "1912.01718", "submitter": "Dylan Troop", "authors": "Dylan Troop, Fr\\'ed\\'eric Godin, Jia Yuan Yu", "title": "Risk-Averse Action Selection Using Extreme Value Theory Estimates of the\n  CVaR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide variety of sequential decision making problems, it can be important\nto estimate the impact of rare events in order to minimize risk exposure. A\npopular risk measure is the conditional value-at-risk (CVaR), which is commonly\nestimated by averaging observations that occur beyond a quantile at a given\nconfidence level. When this confidence level is very high, this estimation\nmethod can exhibit high variance due to the limited number of samples above the\ncorresponding quantile. To mitigate this problem, extreme value theory can be\nused to derive an estimator for the CVaR that uses extrapolation beyond\navailable samples. This estimator requires the selection of a threshold\nparameter to work well, which is a difficult challenge that has been widely\nstudied in the extreme value theory literature. In this paper, we present an\nestimation procedure for the CVaR that combines extreme value theory and a\nrecently introduced method of automated threshold selection by\n\\cite{bader2018automated}. Under appropriate conditions, we estimate the tail\nrisk using a generalized Pareto distribution. We compare empirically this\nestimation procedure with the commonly used method of sample averaging, and\nshow an improvement in performance for some distributions. We finally show how\nthe estimation procedure can be used in reinforcement learning by applying our\nmethod to the multi-arm bandit problem where the goal is to avoid catastrophic\nrisk.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:19:35 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 18:28:54 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Troop", "Dylan", ""], ["Godin", "Fr\u00e9d\u00e9ric", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1912.01730", "submitter": "Chen Xing", "authors": "Chen Xing, Sercan Arik, Zizhao Zhang, Tomas Pfister", "title": "Distance-Based Learning from Errors for Confidence Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are poorly calibrated when trained in\nconventional ways. To improve confidence calibration of DNNs, we propose a\nnovel training method, distance-based learning from errors (DBLE). DBLE bases\nits confidence estimation on distances in the representation space. In DBLE, we\nfirst adapt prototypical learning to train classification models. It yields a\nrepresentation space where the distance between a test sample and its ground\ntruth class center can calibrate the model's classification performance. At\ninference, however, these distances are not available due to the lack of ground\ntruth labels. To circumvent this by inferring the distance for every test\nsample, we propose to train a confidence model jointly with the classification\nmodel. We integrate this into training by merely learning from mis-classified\ntraining samples, which we show to be highly beneficial for effective learning.\nOn multiple datasets and DNN architectures, we demonstrate that DBLE\noutperforms alternative single-model confidence calibration approaches. DBLE\nalso achieves comparable performance with computationally-expensive ensemble\napproaches with lower computational cost and lower number of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:51:51 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 03:44:43 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Xing", "Chen", ""], ["Arik", "Sercan", ""], ["Zhang", "Zizhao", ""], ["Pfister", "Tomas", ""]]}, {"id": "1912.01745", "submitter": "Diego Cifuentes", "authors": "Diego Cifuentes and Ankur Moitra", "title": "Polynomial time guarantees for the Burer-Monteiro method", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burer-Monteiro method is one of the most widely used techniques for\nsolving large-scale semidefinite programs (SDP). The basic idea is to solve a\nnonconvex program in $Y$, where $Y$ is an $n \\times p$ matrix such that $X = Y\nY^T$. In this paper, we show that this method can solve SDPs in polynomial time\nin a smoothed analysis setting. More precisely, we consider an SDP whose domain\nsatisfies some compactness and smoothness assumptions, and slightly perturb the\ncost matrix and the constraints. We show that if $p \\gtrsim \\sqrt{2(1+\\eta)m}$,\nwhere $m$ is the number of constraints and $\\eta>0$ is any fixed constant, then\nthe Burer-Monteiro method can solve SDPs to any desired accuracy in polynomial\ntime, in the setting of smooth analysis. Our bound on $p$ approaches the\ncelebrated Barvinok-Pataki bound in the limit as $\\eta$ goes to zero, beneath\nwhich it is known that the nonconvex program can be suboptimal.\n  Previous analyses were unable to give polynomial time guarantees for the\nBurer-Monteiro method, since they either assumed that the criticality\nconditions are satisfied exactly, or ignored the nontrivial problem of\ncomputing an approximately feasible solution. We address the first problem\nthrough a novel connection with tubular neighborhoods of algebraic varieties.\nFor the feasibility problem we consider a least squares formulation, and\nprovide the first guarantees that do not rely on the restricted isometry\nproperty.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 23:59:41 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 13:00:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cifuentes", "Diego", ""], ["Moitra", "Ankur", ""]]}, {"id": "1912.01752", "submitter": "Benjamin Toms", "authors": "Benjamin A. Toms, Elizabeth A. Barnes, Imme Ebert-Uphoff", "title": "Physically Interpretable Neural Networks for the Geosciences:\n  Applications to Earth System Variability", "comments": "The second version of this manuscript is currently under review at\n  the Journal of Advances in Modeling Earth Systems (JAMES)", "journal-ref": null, "doi": "10.1029/2019MS002002", "report-no": null, "categories": "physics.ao-ph cs.AI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become increasingly prevalent within the geosciences,\nalthough a common limitation of their usage has been a lack of methods to\ninterpret what the networks learn and how they make decisions. As such, neural\nnetworks have often been used within the geosciences to most accurately\nidentify a desired output given a set of inputs, with the interpretation of\nwhat the network learns used as a secondary metric to ensure the network is\nmaking the right decision for the right reason. Neural network interpretation\ntechniques have become more advanced in recent years, however, and we therefore\npropose that the ultimate objective of using a neural network can also be the\ninterpretation of what the network has learned rather than the output itself.\n  We show that the interpretation of neural networks can enable the discovery\nof scientifically meaningful connections within geoscientific data. In\nparticular, we use two methods for neural network interpretation called\nbackwards optimization and layerwise relevance propagation, both of which\nproject the decision pathways of a network back onto the original input\ndimensions. To the best of our knowledge, LRP has not yet been applied to\ngeoscientific research, and we believe it has great potential in this area. We\nshow how these interpretation techniques can be used to reliably infer\nscientifically meaningful information from neural networks by applying them to\ncommon climate patterns. These results suggest that combining interpretable\nneural networks with novel scientific hypotheses will open the door to many new\navenues in neural network-related geoscience research.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 00:37:17 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 05:43:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Toms", "Benjamin A.", ""], ["Barnes", "Elizabeth A.", ""], ["Ebert-Uphoff", "Imme", ""]]}, {"id": "1912.01762", "submitter": "Yuan Xue", "authors": "Yuan Xue, Denny Zhou, Nan Du, Andrew Dai, Zhen Xu, Kun Zhang, Claire\n  Cui", "title": "Deep Physiological State Space Model for Clinical Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical forecasting based on electronic medical records (EMR) can uncover\nthe temporal correlations between patients' conditions and outcomes from\nsequences of longitudinal clinical measurements. In this work, we propose an\nintervention-augmented deep state space generative model to capture the\ninteractions among clinical measurements and interventions by explicitly\nmodeling the dynamics of patients' latent states. Based on this model, we are\nable to make a joint prediction of the trajectories of future observations and\ninterventions. Empirical evaluations show that our proposed model compares\nfavorably to several state-of-the-art methods on real EMR data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 01:38:55 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Xue", "Yuan", ""], ["Zhou", "Denny", ""], ["Du", "Nan", ""], ["Dai", "Andrew", ""], ["Xu", "Zhen", ""], ["Zhang", "Kun", ""], ["Cui", "Claire", ""]]}, {"id": "1912.01790", "submitter": "Abulikemu Abuduweili", "authors": "Abulikemu Abuduweili and Changliu Liu", "title": "Robust Online Model Adaptation by Extended Kalman Filter with\n  Exponential Moving Average and Dynamic Multi-Epoch Strategy", "comments": "2nd Annual Conference on Learning for Dynamics and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High fidelity behavior prediction of intelligent agents is critical in many\napplications. However, the prediction model trained on the training set may not\ngeneralize to the testing set due to domain shift and time variance. The\nchallenge motivates the adoption of online adaptation algorithms to update\nprediction models in real-time to improve the prediction performance. Inspired\nby Extended Kalman Filter (EKF), this paper introduces a series of online\nadaptation methods, which are applicable to neural network-based models. A base\nadaptation algorithm Modified EKF with forgetting factor (MEKF$_\\lambda$) is\nintroduced first, followed by exponential moving average filtering techniques.\nThen this paper introduces a dynamic multi-epoch update strategy to effectively\nutilize samples received in real time. With all these extensions, we propose a\nrobust online adaptation algorithm: MEKF with Exponential Moving Average and\nDynamic Multi-Epoch strategy (MEKF$_{\\text{EMA-DME}}$). The proposed algorithm\noutperforms existing methods as demonstrated in experiments. The source code is\nopen-sourced in the following link\nhttps://github.com/intelligent-control-lab/MEKF_MAME.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 04:16:42 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 08:20:53 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 06:33:43 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Abuduweili", "Abulikemu", ""], ["Liu", "Changliu", ""]]}, {"id": "1912.01792", "submitter": "Yawen Zhang", "authors": "Songtao Lu, Yawen Zhang, Yunlong Wang, Christina Mack", "title": "Learn Electronic Health Records by Fully Decentralized Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning opens a number of research opportunities due to its high\ncommunication efficiency in distributed training problems within a star\nnetwork. In this paper, we focus on improving the communication efficiency for\nfully decentralized federated learning over a graph, where the algorithm\nperforms local updates for several iterations and then enables communications\namong the nodes. In such a way, the communication rounds of exchanging the\ncommon interest of parameters can be saved significantly without loss of\noptimality of the solutions. Multiple numerical simulations based on large,\nreal-world electronic health record databases showcase the superiority of the\ndecentralized federated learning compared with classic methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 04:28:05 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 01:31:36 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lu", "Songtao", ""], ["Zhang", "Yawen", ""], ["Wang", "Yunlong", ""], ["Mack", "Christina", ""]]}, {"id": "1912.01808", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Robert Tibshirani", "title": "Reluctant generalized additive modeling", "comments": "Change of method name, R package now available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse generalized additive models (GAMs) are an extension of sparse\ngeneralized linear models which allow a model's prediction to vary non-linearly\nwith an input variable. This enables the data analyst build more accurate\nmodels, especially when the linearity assumption is known to be a poor\napproximation of reality. Motivated by reluctant interaction modeling (Yu et\nal. 2019), we propose a multi-stage algorithm, called $\\textit{reluctant\ngeneralized additive modeling (RGAM)}$, that can fit sparse generalized\nadditive models at scale. It is guided by the principle that, if all else is\nequal, one should prefer a linear feature over a non-linear feature. Unlike\nexisting methods for sparse GAMs, RGAM can be extended easily to binary, count\nand survival data. We demonstrate the method's effectiveness on real and\nsimulated examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:52:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 22:28:46 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1912.01810", "submitter": "Xiulong Yang", "authors": "Xiulong Yang, and Shihao Ji", "title": "Learning with Multiplicative Perturbations", "comments": "Accepted as a conference paper at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Training (AT) and Virtual Adversarial Training (VAT) are the\nregularization techniques that train Deep Neural Networks (DNNs) with\nadversarial examples generated by adding small but worst-case perturbations to\ninput examples. In this paper, we propose xAT and xVAT, new adversarial\ntraining algorithms, that generate \\textbf{multiplicative} perturbations to\ninput examples for robust training of DNNs. Such perturbations are much more\nperceptible and interpretable than their \\textbf{additive} counterparts\nexploited by AT and VAT. Furthermore, the multiplicative perturbations can be\ngenerated transductively or inductively while the standard AT and VAT only\nsupport a transductive implementation. We conduct a series of experiments that\nanalyze the behavior of the multiplicative perturbations and demonstrate that\nxAT and xVAT match or outperform state-of-the-art classification accuracies\nacross multiple established benchmarks while being about 30\\% faster than their\nadditive counterparts. Furthermore, the resulting DNNs also demonstrate\ndistinct weight distributions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:58:45 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 22:36:47 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Yang", "Xiulong", ""], ["Ji", "Shihao", ""]]}, {"id": "1912.01816", "submitter": "Eli (Omid) David", "authors": "Evyatar Illouz, Eli David, and Nathan S. Netanyahu", "title": "Handwriting-Based Gender Classification Using End-to-End Deep Neural\n  Networks", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 613-621, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_60", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting-based gender classification is a well-researched problem that has\nbeen approached mainly by traditional machine learning techniques. In this\npaper, we propose a novel deep learning-based approach for this task.\nSpecifically, we present a convolutional neural network (CNN), which performs\nautomatic feature extraction from a given handwritten image, followed by\nclassification of the writer's gender. Also, we introduce a new dataset of\nlabeled handwritten samples, in Hebrew and English, of 405 participants.\nComparing the gender classification accuracy on this dataset against human\nexaminers, our results show that the proposed deep learning-based approach is\nsubstantially more accurate than that of humans.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:31 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Illouz", "Evyatar", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.01823", "submitter": "Pedro Savarese", "authors": "Pedro Savarese and David McAllester and Sudarshan Babu and Michael\n  Maire", "title": "Domain-independent Dominance of Adaptive Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a simplified analysis of adaptive methods, we derive AvaGrad, a new\noptimizer which outperforms SGD on vision tasks when its adaptability is\nproperly tuned. We observe that the power of our method is partially explained\nby a decoupling of learning rate and adaptability, greatly simplifying\nhyperparameter search. In light of this observation, we demonstrate that,\nagainst conventional wisdom, Adam can also outperform SGD on vision tasks, as\nlong as the coupling between its learning rate and adaptability is taken into\naccount. In practice, AvaGrad matches the best results, as measured by\ngeneralization accuracy, delivered by any existing optimizer (SGD or adaptive)\nacross image classification (CIFAR, ImageNet) and character-level language\nmodelling (Penn Treebank) tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:58:53 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:03:32 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 01:25:23 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Savarese", "Pedro", ""], ["McAllester", "David", ""], ["Babu", "Sudarshan", ""], ["Maire", "Michael", ""]]}, {"id": "1912.01825", "submitter": "Lars Ruthotto", "authors": "Lars Ruthotto, Stanley Osher, Wuchen Li, Levon Nurbekyan, Samy Wu Fung", "title": "A Machine Learning Framework for Solving High-Dimensional Mean Field\n  Game and Mean Field Control Problems", "comments": "21 pages, 13 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field games (MFG) and mean field control (MFC) are critical classes of\nmulti-agent models for efficient analysis of massive populations of interacting\nagents. Their areas of application span topics in economics, finance, game\ntheory, industrial engineering, crowd motion, and more. In this paper, we\nprovide a flexible machine learning framework for the numerical solution of\npotential MFG and MFC models. State-of-the-art numerical methods for solving\nsuch problems utilize spatial discretization that leads to a\ncurse-of-dimensionality. We approximately solve high-dimensional problems by\ncombining Lagrangian and Eulerian viewpoints and leveraging recent advances\nfrom machine learning. More precisely, we work with a Lagrangian formulation of\nthe problem and enforce the underlying Hamilton-Jacobi-Bellman (HJB) equation\nthat is derived from the Eulerian formulation. Finally, a tailored neural\nnetwork parameterization of the MFG/MFC solution helps us avoid any spatial\ndiscretization. Our numerical results include the approximate solution of\n100-dimensional instances of optimal transport and crowd motion problems on a\nstandard work station and a validation using an Eulerian solver in two\ndimensions. These results open the door to much-anticipated applications of MFG\nand MFC models that were beyond reach with existing numerical methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:59:59 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 18:41:41 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 01:15:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ruthotto", "Lars", ""], ["Osher", "Stanley", ""], ["Li", "Wuchen", ""], ["Nurbekyan", "Levon", ""], ["Fung", "Samy Wu", ""]]}, {"id": "1912.01849", "submitter": "Dominik Linzner", "authors": "Dominik Linzner and Heinz Koeppl", "title": "A Variational Perturbative Approach to Planning in Graph-based Markov\n  Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinating multiple interacting agents to achieve a common goal is a\ndifficult task with huge applicability. This problem remains hard to solve,\neven when limiting interactions to be mediated via a static interaction-graph.\nWe present a novel approximate solution method for multi-agent Markov decision\nproblems on graphs, based on variational perturbation theory. We adopt the\nstrategy of planning via inference, which has been explored in various prior\nworks. We employ a non-trivial extension of a novel high-order variational\nmethod that allows for approximate inference in large networks and has been\nshown to surpass the accuracy of existing variational methods. To compare our\nmethod to two state-of-the-art methods for multi-agent planning on graphs, we\napply the method different standard GMDP problems. We show that in cases, where\nthe goal is encoded as a non-local cost function, our method performs well,\nwhile state-of-the-art methods approach the performance of random guess. In a\nfinal experiment, we demonstrate that our method brings significant improvement\nfor synchronization tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:36:58 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 14:43:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Linzner", "Dominik", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1912.01853", "submitter": "Sumon Bose Mr.", "authors": "Sumon Kumar Bose, Bapi Kar, Mohendra Roy, Pradeep Kumar\n  Gopalakrishnan, Zhang Lei, Aakash Patil and Arindam Basu", "title": "ADEPOS: A Novel Approximate Computing Framework for Anomaly Detection\n  Systems and its Implementation in 65nm CMOS", "comments": "14 pages", "journal-ref": "Preprint TCAS-I 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To overcome the energy and bandwidth limitations of traditional IoT systems,\nedge computing or information extraction at the sensor node has become popular.\nHowever, now it is important to create very low energy information extraction\nor pattern recognition systems. In this paper, we present an approximate\ncomputing method to reduce the computation energy of a specific type of IoT\nsystem used for anomaly detection (e.g. in predictive maintenance, epileptic\nseizure detection, etc). Termed as Anomaly Detection Based Power Savings\n(ADEPOS), our proposed method uses low precision computing and low complexity\nneural networks at the beginning when it is easy to distinguish healthy data.\nHowever, on the detection of anomalies, the complexity of the network and\ncomputing precision are adaptively increased for accurate predictions. We show\nthat ensemble approaches are well suited for adaptively changing network size.\nTo validate our proposed scheme, a chip has been fabricated in UMC65nm process\nthat includes an MSP430 microprocessor along with an on-chip switching mode\nDC-DC converter for dynamic voltage and frequency scaling. Using NASA bearing\ndataset for machine health monitoring, we show that using ADEPOS we can achieve\n8.95X saving of energy along the lifetime without losing any detection\naccuracy. The energy savings are obtained by reducing the execution time of the\nneural network on the microprocessor.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:01:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Bose", "Sumon Kumar", ""], ["Kar", "Bapi", ""], ["Roy", "Mohendra", ""], ["Gopalakrishnan", "Pradeep Kumar", ""], ["Lei", "Zhang", ""], ["Patil", "Aakash", ""], ["Basu", "Arindam", ""]]}, {"id": "1912.01899", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Zhenfeng Zhu, Xingxing Zhang, Zhizhe Liu, Jian Cheng, Yao\n  Zhao", "title": "Distribution-induced Bidirectional Generative Adversarial Network for\n  Graph Representation Learning", "comments": "Accepted to CVPR2020. 10 pages, 5 figures, 4 tables, fixed a error in\n  the Figure.1", "journal-ref": "booktitle={Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition}, pages={7224--7233}, year={2020}", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning aims to encode all nodes of a graph into\nlow-dimensional vectors that will serve as input of many compute vision tasks.\nHowever, most existing algorithms ignore the existence of inherent data\ndistribution and even noises. This may significantly increase the phenomenon of\nover-fitting and deteriorate the testing accuracy. In this paper, we propose a\nDistribution-induced Bidirectional Generative Adversarial Network (named DBGAN)\nfor graph representation learning. Instead of the widely used normal\ndistribution assumption, the prior distribution of latent representation in our\nDBGAN is estimated in a structure-aware way, which implicitly bridges the graph\nand feature spaces by prototype learning. Thus discriminative and robust\nrepresentations are generated for all nodes. Furthermore, to improve their\ngeneralization ability while preserving representation ability, the\nsample-level and distribution-level consistency is well balanced via a\nbidirectional adversarial learning framework. An extensive group of experiments\nare then carefully designed and presented, demonstrating that our DBGAN obtains\nremarkably more favorable trade-off between representation and robustness, and\nmeanwhile is dimension-efficient, over currently available alternatives in\nvarious tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 11:23:36 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:14:18 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 01:35:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhu", "Zhenfeng", ""], ["Zhang", "Xingxing", ""], ["Liu", "Zhizhe", ""], ["Cheng", "Jian", ""], ["Zhao", "Yao", ""]]}, {"id": "1912.01909", "submitter": "Erik Nijkamp", "authors": "Erik Nijkamp, Bo Pang, Tian Han, Linqi Zhou, Song-Chun Zhu, Ying Nian\n  Wu", "title": "Learning Multi-layer Latent Variable Model via Variational Optimization\n  of Short Run MCMC for Approximate Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the fundamental problem of learning deep generative models\nthat consist of multiple layers of latent variables organized in top-down\narchitectures. Such models have high expressivity and allow for learning\nhierarchical representations. Learning such a generative model requires\ninferring the latent variables for each training example based on the posterior\ndistribution of these latent variables. The inference typically requires Markov\nchain Monte Caro (MCMC) that can be time consuming. In this paper, we propose\nto use noise initialized non-persistent short run MCMC, such as finite step\nLangevin dynamics initialized from the prior distribution of the latent\nvariables, as an approximate inference engine, where the step size of the\nLangevin dynamics is variationally optimized by minimizing the Kullback-Leibler\ndivergence between the distribution produced by the short run MCMC and the\nposterior distribution. Our experiments show that the proposed method\noutperforms variational auto-encoder (VAE) in terms of reconstruction error and\nsynthesis quality. The advantage of the proposed method is that it is simple\nand automatic without the need to design an inference model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 11:42:14 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 20:14:18 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 21:20:30 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 10:16:11 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 22:54:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Nijkamp", "Erik", ""], ["Pang", "Bo", ""], ["Han", "Tian", ""], ["Zhou", "Linqi", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1912.01927", "submitter": "Holger Trittenbach", "authors": "Holger Trittenbach, Klemens B\\\"ohm, Ira Assent", "title": "Active Learning of SVDD Hyperparameter Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description is a popular method for outlier detection.\nHowever, its usefulness largely depends on selecting good hyperparameter values\n-- a difficult problem that has received significant attention in literature.\nExisting methods to estimate hyperparameter values are purely heuristic, and\nthe conditions under which they work well are unclear. In this article, we\npropose LAMA (Local Active Min-Max Alignment), the first principled approach to\nestimate SVDD hyperparameter values by active learning. The core idea bases on\nkernel alignment, which we adapt to active learning with small sample sizes. In\ncontrast to many existing approaches, LAMA provides estimates for both SVDD\nhyperparameters. These estimates are evidence-based, i.e., rely on actual class\nlabels, and come with a quality score. This eliminates the need for manual\nvalidation, an issue with current heuristics. LAMA outperforms state-of-the-art\ncompetitors in extensive experiments on real-world data. In several cases, LAMA\neven yields results close to the empirical upper bound.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:25:43 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Trittenbach", "Holger", ""], ["B\u00f6hm", "Klemens", ""], ["Assent", "Ira", ""]]}, {"id": "1912.01933", "submitter": "Ahmed Abdelwahab", "authors": "Ahmed Abdelwahab and Niels Landwehr", "title": "Deep Distributional Sequence Embeddings Based on a Wasserstein Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning employs deep neural networks to embed instances into a\nmetric space such that distances between instances of the same class are small\nand distances between instances from different classes are large. In most\nexisting deep metric learning techniques, the embedding of an instance is given\nby a feature vector produced by a deep neural network and Euclidean distance or\ncosine similarity defines distances between these vectors. In this paper, we\nstudy deep distributional embeddings of sequences, where the embedding of a\nsequence is given by the distribution of learned deep features across the\nsequence. This has the advantage of capturing statistical information about the\ndistribution of patterns within the sequence in the embedding. When embeddings\nare distributions rather than vectors, measuring distances between embeddings\ninvolves comparing their respective distributions. We propose a distance metric\nbased on Wasserstein distances between the distributions and a corresponding\nloss function for metric learning, which leads to a novel end-to-end trainable\nembedding model. We empirically observe that distributional embeddings\noutperform standard vector embeddings and that training with the proposed\nWasserstein metric outperforms training with other distance functions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:43:28 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Abdelwahab", "Ahmed", ""], ["Landwehr", "Niels", ""]]}, {"id": "1912.01937", "submitter": "Ziming Liu", "authors": "Ziming Liu, Zheng Zhang", "title": "Quantum-Inspired Hamiltonian Monte Carlo for Bayesian Sampling", "comments": "38 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is an efficient Bayesian sampling method that\ncan make distant proposals in the parameter space by simulating a Hamiltonian\ndynamical system. Despite its popularity in machine learning and data science,\nHMC is inefficient to sample from spiky and multimodal distributions. Motivated\nby the energy-time uncertainty relation from quantum mechanics, we propose a\nQuantum-Inspired Hamiltonian Monte Carlo algorithm (QHMC). This algorithm\nallows a particle to have a random mass matrix with a probability distribution\nrather than a fixed mass. We prove the convergence property of QHMC and further\nshow why such a random mass can improve the performance when we sample a broad\nclass of distributions. In order to handle the big training data sets in\nlarge-scale machine learning, we develop a stochastic gradient version of QHMC\nusing Nos{\\'e}-Hoover thermostat called QSGNHT, and we also provide theoretical\njustifications about its steady-state distributions. Finally in the\nexperiments, we demonstrate the effectiveness of QHMC and QSGNHT on synthetic\nexamples, bridge regression, image denoising and neural network pruning. The\nproposed QHMC and QSGNHT can indeed achieve much more stable and accurate\nsampling results on the test cases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:56:35 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 07:14:03 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 02:53:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Liu", "Ziming", ""], ["Zhang", "Zheng", ""]]}, {"id": "1912.01956", "submitter": "Eva-Maria Walz", "authors": "Tilmann Gneiting and Eva-Maria Walz", "title": "Receiver operating characteristic (ROC) movies, universal ROC (UROC)\n  curves, and coefficient of predictive ability (CPA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout science and technology, receiver operating characteristic (ROC)\ncurves and associated area under the curve (AUC) measures constitute powerful\ntools for assessing the predictive abilities of features, markers and tests in\nbinary classification problems. Despite its immense popularity, ROC analysis\nhas been subject to a fundamental restriction, in that it applies to\ndichotomous (yes or no) outcomes only. Here we introduce ROC movies and\nuniversal ROC (UROC) curves that apply to just any linearly ordered outcome,\nalong with an associated coefficient of predictive ability (CPA) measure. CPA\nequals the area under the UROC curve, and admits appealing interpretations in\nterms of probabilities and rank based covariances. For binary outcomes CPA\nequals AUC, and for pairwise distinct outcomes CPA relates linearly to\nSpearman's coefficient, in the same way that the C index relates linearly to\nKendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the\ntools of classical ROC analysis, and are bound to supersede them in a wealth of\napplications. Their usage is illustrated in data examples from biomedicine and\nmeteorology, where rank based measures yield new insights in the WeatherBench\ncomparison of the predictive performance of convolutional neural networks and\nphysical-numerical models for weather prediction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:16:47 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 13:43:45 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 14:46:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Gneiting", "Tilmann", ""], ["Walz", "Eva-Maria", ""]]}, {"id": "1912.01969", "submitter": "Fabian Hinder", "authors": "Fabian Hinder, Andr\\'e Artelt and Barbara Hammer", "title": "A probability theoretic approach to drifting data in continuous time\n  domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of drift refers to the phenomenon that the distribution, which is\nunderlying the observed data, changes over time. Albeit many attempts were made\nto deal with drift, formal notions of drift are application-dependent and\nformulated in various degrees of abstraction and mathematical coherence. In\nthis contribution, we provide a probability theoretical framework, that allows\na formalization of drift in continuous time, which subsumes popular notions of\ndrift. In particular, it sheds some light on common practice such as\nchange-point detection or machine learning methodologies in the presence of\ndrift. It gives rise to a new characterization of drift in terms of stochastic\ndependency between data and time. This particularly intuitive formalization\nenables us to design a new, efficient drift detection method. Further, it\ninduces a technology, to decompose observed data into a drifting and a\nnon-drifting part.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:42:07 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Hinder", "Fabian", ""], ["Artelt", "Andr\u00e9", ""], ["Hammer", "Barbara", ""]]}, {"id": "1912.01978", "submitter": "Mahum Naseer", "authors": "Mahum Naseer, Mishal Fatima Minhas, Faiq Khalid, Muhammad Abdullah\n  Hanif, Osman Hasan, Muhammad Shafique", "title": "FANNet: Formal Analysis of Noise Tolerance, Training Bias and Input\n  Sensitivity in Neural Networks", "comments": "To appear at the 23rd Design, Automation and Test in Europe (DATE\n  2020). Grenoble, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a constant improvement in the network architectures and training\nmethodologies, Neural Networks (NNs) are increasingly being deployed in\nreal-world Machine Learning systems. However, despite their impressive\nperformance on \"known inputs\", these NNs can fail absurdly on the \"unseen\ninputs\", especially if these real-time inputs deviate from the training dataset\ndistributions, or contain certain types of input noise. This indicates the low\nnoise tolerance of NNs, which is a major reason for the recent increase of\nadversarial attacks. This is a serious concern, particularly for\nsafety-critical applications, where inaccurate results lead to dire\nconsequences. We propose a novel methodology that leverages model checking for\nthe Formal Analysis of Neural Network (FANNet) under different input noise\nranges. Our methodology allows us to rigorously analyze the noise tolerance of\nNNs, their input node sensitivity, and the effects of training bias on their\nperformance, e.g., in terms of classification accuracy. For evaluation, we use\na feed-forward fully-connected NN architecture trained for the Leukemia\nclassification. Our experimental results show $\\pm 11\\%$ noise tolerance for\nthe given trained network, identify the most sensitive input nodes, and confirm\nthe biasness of the available training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 12:42:47 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 22:38:46 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Naseer", "Mahum", ""], ["Minhas", "Mishal Fatima", ""], ["Khalid", "Faiq", ""], ["Hanif", "Muhammad Abdullah", ""], ["Hasan", "Osman", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1912.01987", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson, Iryna Gurevych", "title": "Scalable Bayesian Preference Learning for Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable Bayesian preference learning method for jointly\npredicting the preferences of individuals as well as the consensus of a crowd\nfrom pairwise labels. Peoples' opinions often differ greatly, making it\ndifficult to predict their preferences from small amounts of personal data.\nIndividual biases also make it harder to infer the consensus of a crowd when\nthere are few labels per item. We address these challenges by combining matrix\nfactorisation with Gaussian processes, using a Bayesian approach to account for\nuncertainty arising from noisy and sparse data. Our method exploits input\nfeatures, such as text embeddings and user metadata, to predict preferences for\nnew items and users that are not in the training set. As previous solutions\nbased on Gaussian processes do not scale to large numbers of users, items or\npairwise labels, we propose a stochastic variational inference approach that\nlimits computational and memory costs. Our experiments on a recommendation task\nshow that our method is competitive with previous approaches despite our\nscalable inference approximation. We demonstrate the method's scalability on a\nnatural language processing task with thousands of users and items, and show\nimprovements over the state of the art on this task. We make our software\npublicly available for future work.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:56:38 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 20:01:44 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1912.02008", "submitter": "Benjamin Aubin", "authors": "Benjamin Aubin, Bruno Loureiro, Antoine Baker, Florent Krzakala and\n  Lenka Zdeborov\\'a", "title": "Exact asymptotics for phase retrieval and compressed sensing with random\n  generative priors", "comments": "13+3 pages, 7 figures, v2 revised and accepted at MSML", "journal-ref": "Proceedings of The First Mathematical and Scientific Machine\n  Learning Conference, PMLR 107:55-73, 2020", "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of compressed sensing and of (real-valued) phase\nretrieval with random measurement matrix. We derive sharp asymptotics for the\ninformation-theoretically optimal performance and for the best known polynomial\nalgorithm for an ensemble of generative priors consisting of fully connected\ndeep neural networks with random weight matrices and arbitrary activations. We\ncompare the performance to sparse separable priors and conclude that generative\npriors might be advantageous in terms of algorithmic performance. In\nparticular, while sparsity does not allow to perform compressive phase\nretrieval efficiently close to its information-theoretic limit, it is found\nthat under the random generative prior compressed phase retrieval becomes\ntractable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:20:34 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 07:40:52 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Aubin", "Benjamin", ""], ["Loureiro", "Bruno", ""], ["Baker", "Antoine", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1912.02065", "submitter": "Harry Clifford MSci DPhil", "authors": "Geoffroy Dubourg-Felonneau, Omar Darwish, Christopher Parsons, Dami\n  Rebergen, John W Cassidy, Nirmesh Patel, Harry W Clifford", "title": "Safety and Robustness in Decision Making: Deep Bayesian Recurrent Neural\n  Networks for Somatic Variant Calling in Cancer", "comments": "Safety and Robustness in Decision Making Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genomic profile underlying an individual tumor can be highly informative\nin the creation of a personalized cancer treatment strategy for a given\npatient; a practice known as precision oncology. This involves next generation\nsequencing of a tumor sample and the subsequent identification of genomic\naberrations, such as somatic mutations, to provide potential candidates of\ntargeted therapy. The identification of these aberrations from sequencing noise\nand germline variant background poses a classic classification-style problem.\nThis has been previously broached with many different supervised machine\nlearning methods, including deep-learning neural networks. However, these\nneural networks have thus far not been tailored to give any indication of\nconfidence in the mutation call, meaning an oncologist could be targeting a\nmutation with a low probability of being true. To address this, we present here\na deep bayesian recurrent neural network for cancer variant calling, which\nshows no degradation in performance compared to standard neural networks. This\napproach enables greater flexibility through different priors to avoid\noverfitting to a single dataset. We will be incorporating this approach into\nsoftware for oncologists to obtain safe, robust, and statistically confident\nsomatic mutation calls for precision oncology treatment choices.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:47:56 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Dubourg-Felonneau", "Geoffroy", ""], ["Darwish", "Omar", ""], ["Parsons", "Christopher", ""], ["Rebergen", "Dami", ""], ["Cassidy", "John W", ""], ["Patel", "Nirmesh", ""], ["Clifford", "Harry W", ""]]}, {"id": "1912.02098", "submitter": "Siddarth Srinivasan", "authors": "Sandesh Adhikary, Siddarth Srinivasan, Geoff Gordon, Byron Boots", "title": "Expressiveness and Learning of Hidden Quantum Markov Models", "comments": "arXiv admin note: text overlap with arXiv:1903.03730", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending classical probabilistic reasoning using the quantum mechanical view\nof probability has been of recent interest, particularly in the development of\nhidden quantum Markov models (HQMMs) to model stochastic processes. However,\nthere has been little progress in characterizing the expressiveness of such\nmodels and learning them from data. We tackle these problems by showing that\nHQMMs are a special subclass of the general class of observable operator models\n(OOMs) that do not suffer from the \\emph{negative probability problem} by\ndesign. We also provide a feasible retraction-based learning algorithm for\nHQMMs using constrained gradient descent on the Stiefel manifold of model\nparameters. We demonstrate that this approach is faster and scales to larger\nmodels than previous learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:51:19 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Adhikary", "Sandesh", ""], ["Srinivasan", "Siddarth", ""], ["Gordon", "Geoff", ""], ["Boots", "Byron", ""]]}, {"id": "1912.02143", "submitter": "Antoine Maillard", "authors": "Antoine Maillard and G\\'erard Ben Arous and Giulio Biroli", "title": "Landscape Complexity for the Empirical Risk of Generalized Linear Models", "comments": "18 pages and 18 pages appendix. Update to match the published version\n  (v2). Corrections of remaining small typos (v3)", "journal-ref": "Proceedings of The First Mathematical and Scientific Machine\n  Learning Conference, PMLR 107:287-327, 2020", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to obtain the average and the typical value of the number\nof critical points of the empirical risk landscape for generalized linear\nestimation problems and variants. This represents a substantial extension of\nprevious applications of the Kac-Rice method since it allows to analyze the\ncritical points of high dimensional non-Gaussian random functions. We obtain a\nrigorous explicit variational formula for the annealed complexity, which is the\nlogarithm of the average number of critical points at fixed value of the\nempirical risk. This result is simplified, and extended, using the non-rigorous\nKac-Rice replicated method from theoretical physics. In this way we find an\nexplicit variational formula for the quenched complexity, which is generally\ndifferent from its annealed counterpart, and allows to obtain the number of\ncritical points for typical instances up to exponential accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:47:24 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 09:12:42 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 09:22:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Maillard", "Antoine", ""], ["Arous", "G\u00e9rard Ben", ""], ["Biroli", "Giulio", ""]]}, {"id": "1912.02154", "submitter": "Francisco Ibarrola", "authors": "Nicol\\'as Nieto, Francisco Ibarrola, Victoria Peterson, Hugo Rufiner\n  and Ruben Spies", "title": "Extreme Learning Machine design for dealing with unrepresentative\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme Learning Machines (ELMs) have become a popular tool in the field of\nArtificial Intelligence due to their very high training speed and\ngeneralization capabilities. Another advantage is that they have a single\nhyper-parameter that must be tuned up: the number of hidden nodes. Most\ntraditional approaches dictate that this parameter should be chosen smaller\nthan the number of available training samples in order to avoid over-fitting.\nIn fact, it has been proved that choosing the number of hidden nodes equal to\nthe number of training samples yields a perfect training classification with\nprobability 1 (w.r.t. the random parameter initialization). In this article we\nargue that in spite of this, in some cases it may be beneficial to choose a\nmuch larger number of hidden nodes, depending on certain properties of the\ndata. We explain why this happens and show some examples to illustrate how the\nmodel behaves. In addition, we present a pruning algorithm to cope with the\nadditional computational burden associated to the enlarged ELM. Experimental\nresults using electroencephalography (EEG) signals show an improvement in\nperformance with respect to traditional ELM approaches, while diminishing the\nextra computing time associated to the use of large architectures.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:06:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nieto", "Nicol\u00e1s", ""], ["Ibarrola", "Francisco", ""], ["Peterson", "Victoria", ""], ["Rufiner", "Hugo", ""], ["Spies", "Ruben", ""]]}, {"id": "1912.02160", "submitter": "Tao Wu", "authors": "Pierre Br\\'echet, Tao Wu, Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Informative GANs via Structured Regularization of Optimal Transport", "comments": "Presented at the Optimal Transport and Machine Learning Workshop,\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenge of disentangled representation learning in generative\nadversarial networks (GANs) from the perspective of regularized optimal\ntransport (OT). Specifically, a smoothed OT loss gives rise to an implicit\ntransportation plan between the latent space and the data space. Based on this\ntheoretical observation, we exploit a structured regularization on the\ntransportation plan to encourage a prescribed latent subspace to be\ninformative. This yields the formulation of a novel informative OT-based GAN.\nBy convex duality, we obtain the equivalent view that this leads to perturbed\nground costs favoring sparsity in the informative latent dimensions.\nPractically, we devise a stable training algorithm for the proposed informative\nGAN. Our experiments support the hypothesis that such regularizations\neffectively yield the discovery of disentangled and interpretable latent\nrepresentations. Our work showcases potential power of a regularized OT\nframework in the context of generative modeling through its access to the\ntransport plan. Further challenges are addressed in this line.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:25:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Br\u00e9chet", "Pierre", ""], ["Wu", "Tao", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1912.02163", "submitter": "Nicholas Wilkins", "authors": "Nicholas Wilkins, Michael Johnson, Ifeoma Nwogu", "title": "Regression with Uncertainty Quantification in Large Scale Complex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several methods for predicting uncertainty on deep networks have been\nrecently proposed, they do not readily translate to large and complex datasets.\nIn this paper we utilize a simplified form of the Mixture Density Networks\n(MDNs) to produce a one-shot approach to quantify uncertainty in regression\nproblems. We show that our uncertainty bounds are on-par or better than other\nreported existing methods. When applied to standard regression benchmark\ndatasets, we show an improvement in predictive log-likelihood and\nroot-mean-square-error when compared to existing state-of-the-art methods. We\nalso demonstrate this method's efficacy on stochastic, highly volatile\ntime-series data where stock prices are predicted for the next time interval.\nThe resulting uncertainty graph summarizes significant anomalies in the stock\nprice chart. Furthermore, we apply this method to the task of age estimation\nfrom the challenging IMDb-Wiki dataset of half a million face images. We\nsuccessfully predict the uncertainties associated with the prediction and\nempirically analyze the underlying causes of the uncertainties. This\nuncertainty quantification can be used to pre-process low quality datasets and\nfurther enable learning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:29:14 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wilkins", "Nicholas", ""], ["Johnson", "Michael", ""], ["Nwogu", "Ifeoma", ""]]}, {"id": "1912.02166", "submitter": "Roland Molontay", "authors": "G\\'abor Horv\\'ath, Edith Kov\\'acs, Roland Molontay, Szabolcs\n  Nov\\'aczki", "title": "Copula-based anomaly scoring and localization for large-scale,\n  high-dimensional continuous data", "comments": "27 pages, 12 figures, accepted at ACM Transactions on Intelligent\n  Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anomaly detection method presented by this paper has a special feature:\nit does not only indicate whether an observation is anomalous or not but also\ntells what exactly makes an anomalous observation unusual. Hence, it provides\nsupport to localize the reason of the anomaly.\n  The proposed approach is model-based; it relies on the multivariate\nprobability distribution associated with the observations. Since the rare\nevents are present in the tails of the probability distributions, we use copula\nfunctions, that are able to model the fat-tailed distributions well. The\npresented procedure scales well; it can cope with a large number of\nhigh-dimensional samples. Furthermore, our procedure can cope with missing\nvalues, too, which occur frequently in high-dimensional data sets.\n  In the second part of the paper, we demonstrate the usability of the method\nthrough a case study, where we analyze a large data set consisting of the\nperformance counters of a real mobile telecommunication network. Since such\nnetworks are complex systems, the signs of sub-optimal operation can remain\nhidden for a potentially long time. With the proposed procedure, many such\nhidden issues can be isolated and indicated to the network operator.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:35:54 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Horv\u00e1th", "G\u00e1bor", ""], ["Kov\u00e1cs", "Edith", ""], ["Molontay", "Roland", ""], ["Nov\u00e1czki", "Szabolcs", ""]]}, {"id": "1912.02175", "submitter": "V\\'it Musil", "authors": "Marin Vlastelica and Anselm Paulus and V\\'it Musil and Georg Martius\n  and Michal Rol\\'inek", "title": "Differentiation of Blackbox Combinatorial Solvers", "comments": "ICLR 2020 conference paper (spotlight). The first two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving fusion of deep learning with combinatorial algorithms promises\ntransformative changes to artificial intelligence. One possible approach is to\nintroduce combinatorial building blocks into neural networks. Such end-to-end\narchitectures have the potential to tackle combinatorial problems on raw input\ndata such as ensuring global consistency in multi-object tracking or route\nplanning on maps in robotics. In this work, we present a method that implements\nan efficient backward pass through blackbox implementations of combinatorial\nsolvers with linear objective functions. We provide both theoretical and\nexperimental backing. In particular, we incorporate the Gurobi MIP solver,\nBlossom V algorithm, and Dijkstra's algorithm into architectures that extract\nsuitable features from raw inputs for the traveling salesman problem, the\nmin-cost perfect matching problem and the shortest path problem. The code is\navailable at https://github.com/martius-lab/blackbox-backprop.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:54:42 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 20:44:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Vlastelica", "Marin", ""], ["Paulus", "Anselm", ""], ["Musil", "V\u00edt", ""], ["Martius", "Georg", ""], ["Rol\u00ednek", "Michal", ""]]}, {"id": "1912.02178", "submitter": "Hossein Mobahi", "authors": "Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, Samy\n  Bengio", "title": "Fantastic Generalization Measures and Where to Find Them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization of deep networks has been of great interest in recent years,\nresulting in a number of theoretically and empirically motivated complexity\nmeasures. However, most papers proposing such measures study only a small set\nof models, leaving open the question of whether the conclusion drawn from those\nexperiments would remain valid in other settings. We present the first large\nscale study of generalization in deep networks. We investigate more then 40\ncomplexity measures taken from both theoretical bounds and empirical studies.\nWe train over 10,000 convolutional networks by systematically varying commonly\nused hyperparameters. Hoping to uncover potentially causal relationships\nbetween each measure and generalization, we analyze carefully controlled\nexperiments and show surprising failures of some measures as well as promising\nmeasures for further research.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:58:26 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jiang", "Yiding", ""], ["Neyshabur", "Behnam", ""], ["Mobahi", "Hossein", ""], ["Krishnan", "Dilip", ""], ["Bengio", "Samy", ""]]}, {"id": "1912.02233", "submitter": "Li Wang", "authors": "Zitong Wang and Li Wang and Raymond Chan and Tieyong Zeng", "title": "Large-Scale Semi-Supervised Learning via Graph Structure Learning over\n  High-Dense Points", "comments": "25 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on developing a novel scalable graph-based semi-supervised learning\n(SSL) method for a small number of labeled data and a large amount of unlabeled\ndata. Due to the lack of labeled data and the availability of large-scale\nunlabeled data, existing SSL methods usually encounter either suboptimal\nperformance because of an improper graph or the high computational complexity\nof the large-scale optimization problem. In this paper, we propose to address\nboth challenging problems by constructing a proper graph for graph-based SSL\nmethods. Different from existing approaches, we simultaneously learn a small\nset of vertexes to characterize the high-dense regions of the input data and a\ngraph to depict the relationships among these vertexes. A novel approach is\nthen proposed to construct the graph of the input data from the learned graph\nof a small number of vertexes with some preferred properties. Without\nexplicitly calculating the constructed graph of inputs, two transductive\ngraph-based SSL approaches are presented with the computational complexity in\nlinear with the number of input data. Extensive experiments on synthetic data\nand real datasets of varied sizes demonstrate that the proposed method is not\nonly scalable for large-scale data, but also achieve good classification\nperformance, especially for extremely small number of labels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:00:47 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Wang", "Zitong", ""], ["Wang", "Li", ""], ["Chan", "Raymond", ""], ["Zeng", "Tieyong", ""]]}, {"id": "1912.02254", "submitter": "Yongcan Cao", "authors": "Huixin Zhan, Wei-Ming Lin, and Yongcan Cao", "title": "Deep Model Compression Via Two-Stage Deep Reinforcement Learning", "comments": "To appear in ECML/PKDD 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides accuracy, the model size of convolutional neural networks (CNN)\nmodels is another important factor considering limited hardware resources in\npractical applications. For example, employing deep neural networks on mobile\nsystems requires the design of accurate yet fast CNN for low latency in\nclassification and object detection. To fulfill the need, we aim at obtaining\nCNN models with both high testing accuracy and small size to address resource\nconstraints in many embedded devices. In particular, this paper focuses on\nproposing a generic reinforcement learning-based model compression approach in\na two-stage compression pipeline: pruning and quantization. The first stage of\ncompression, i.e., pruning, is achieved via exploiting deep reinforcement\nlearning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise\nchannel pruning and element-wise variational pruning via information dropout.\nThe second stage, i.e., quantization, is achieved via a similar DRL approach\nbut focuses on obtaining the optimal bits representation for individual layers.\nWe further conduct experimental results on CIFAR-10 and ImageNet datasets. For\nthe CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x\nfrom 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet\ndataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to\n4.14MB with no accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:34:19 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 12:20:48 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zhan", "Huixin", ""], ["Lin", "Wei-Ming", ""], ["Cao", "Yongcan", ""]]}, {"id": "1912.02258", "submitter": "Raj Dasgupta", "authors": "Prithviraj Dasgupta and Joseph B. Collins", "title": "A Survey of Game Theoretic Approaches for Adversarial Machine Learning\n  in Cybersecurity Tasks", "comments": "13 pages, 2 figures, 1 table", "journal-ref": "AI Magazine, 40(2), 31-43 (2019)", "doi": "10.1609/aimag.v40i2.2847", "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are currently used extensively for automating\nvarious cybersecurity tasks. Most of these techniques utilize supervised\nlearning algorithms that rely on training the algorithm to classify incoming\ndata into different categories, using data encountered in the relevant domain.\nA critical vulnerability of these algorithms is that they are susceptible to\nadversarial attacks where a malicious entity called an adversary deliberately\nalters the training data to misguide the learning algorithm into making\nclassification errors. Adversarial attacks could render the learning algorithm\nunsuitable to use and leave critical systems vulnerable to cybersecurity\nattacks. Our paper provides a detailed survey of the state-of-the-art\ntechniques that are used to make a machine learning algorithm robust against\nadversarial attacks using the computational framework of game theory. We also\ndiscuss open problems and challenges and possible directions for further\nresearch that would make deep machine learning-based systems more robust and\nreliable for cybersecurity tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:42:15 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Dasgupta", "Prithviraj", ""], ["Collins", "Joseph B.", ""]]}, {"id": "1912.02260", "submitter": "Jessica Thompson", "authors": "Jessica A.F. Thompson, Yoshua Bengio, Marc Schoenwiesner", "title": "The effect of task and training on intermediate representations in\n  convolutional neural networks revealed with modified RV similarity analysis", "comments": "4 pages, 4 figures, Conference on Cognitive Computational\n  Neuroscience 2019", "journal-ref": null, "doi": "10.32470/CCN.2019.1300-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Centered Kernel Alignment (CKA) was recently proposed as a similarity metric\nfor comparing activation patterns in deep networks. Here we experiment with the\nmodified RV-coefficient (RV2), which has very similar properties as CKA while\nbeing less sensitive to dataset size. We compare the representations of\nnetworks that received varying amounts of training on different layers: a\nstandard trained network (all parameters updated at every step), a freeze\ntrained network (layers gradually frozen during training), random networks\n(only some layers trained), and a completely untrained network. We found that\nRV2 was able to recover expected similarity patterns and provide interpretable\nsimilarity matrices that suggested hypotheses about how representations are\naffected by different training recipes. We propose that the superior\nperformance achieved by freeze training can be attributed to representational\ndifferences in the penultimate layer. Our comparisons of random networks\nsuggest that the inputs and targets serve as anchors on the representations in\nthe lowest and highest layers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:43:57 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Thompson", "Jessica A. F.", ""], ["Bengio", "Yoshua", ""], ["Schoenwiesner", "Marc", ""]]}, {"id": "1912.02276", "submitter": "Kiwan Maeng", "authors": "Kiwan Maeng, Iskender Kushan, Brandon Lucia, Ashish Kapoor", "title": "Enhancing Stratospheric Weather Analyses and Forecasts by Deploying\n  Sensors from a Weather Balloon", "comments": "NeurIPS 2019 Workshop: Tackling Climate Change with Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to analyze and forecast stratospheric weather conditions is\nfundamental to addressing climate change. However, our capacity to collect data\nin the stratosphere is limited by sparsely deployed weather balloons. We\npropose a framework to collect stratospheric data by releasing a contrail of\ntiny sensor devices as a weather balloon ascends. The key machine learning\nchallenges are determining when and how to deploy a finite collection of\nsensors to produce a useful data set. We decide when to release sensors by\nmodeling the deviation of a forecast from actual stratospheric conditions as a\nGaussian process. We then implement a novel hardware system that is capable of\noptimally releasing sensors from a rising weather balloon. We show that this\ndata engineering framework is effective through real weather balloon flights,\nas well as simulations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:07:05 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Maeng", "Kiwan", ""], ["Kushan", "Iskender", ""], ["Lucia", "Brandon", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1912.02279", "submitter": "Beidi Chen", "authors": "Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava,\n  Animesh Garg, Anima Anandkumar", "title": "Angular Visual Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent convolutional neural networks (CNNs) have led to impressive\nperformance but often suffer from poor calibration. They tend to be\noverconfident, with the model confidence not always reflecting the underlying\ntrue ambiguity and hardness. In this paper, we propose angular visual hardness\n(AVH), a score given by the normalized angular distance between the sample\nfeature embedding and the target classifier to measure sample hardness. We\nvalidate this score with an in-depth and extensive scientific study, and\nobserve that CNN models with the highest accuracy also have the best AVH\nscores. This agrees with an earlier finding that state-of-art models improve on\nthe classification of harder examples. We observe that the training dynamics of\nAVH is vastly different compared to the training loss. Specifically, AVH\nquickly reaches a plateau for all samples even though the training loss keeps\nimproving. This suggests the need for designing better loss functions that can\ntarget harder examples more effectively. We also find that AVH has a\nstatistically significant correlation with human visual hardness. Finally, we\ndemonstrate the benefit of AVH to a variety of applications such as\nself-training for domain adaptation and domain generalization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:12:42 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:23:12 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 00:23:39 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 20:58:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chen", "Beidi", ""], ["Liu", "Weiyang", ""], ["Yu", "Zhiding", ""], ["Kautz", "Jan", ""], ["Shrivastava", "Anshumali", ""], ["Garg", "Animesh", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1912.02280", "submitter": "Riccardo Volpi", "authors": "Riccardo Volpi, Luigi Malag\\`o", "title": "Natural Alpha Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an embedding for a large collection of items is a popular approach\nto overcome the computational limitations associated to one-hot encodings. The\naim of item embedding is to learn a low dimensional space for the\nrepresentations, able to capture with its geometry relevant features or\nrelationships for the data at hand. This can be achieved for example by\nexploiting adjacencies among items in large sets of unlabelled data. In this\npaper we interpret in an Information Geometric framework the item embeddings\nobtained from conditional models. By exploiting the $\\alpha$-geometry of the\nexponential family, first introduced by Amari, we introduce a family of natural\n$\\alpha$-embeddings represented by vectors in the tangent space of the\nprobability simplex, which includes as a special case standard approaches\navailable in the literature. A typical example is given by word embeddings,\ncommonly used in natural language processing, such as Word2Vec and GloVe. In\nour analysis, we show how the $\\alpha$-deformation parameter can impact on\nstandard evaluation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:13:16 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 12:55:44 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Volpi", "Riccardo", ""], ["Malag\u00f2", "Luigi", ""]]}, {"id": "1912.02290", "submitter": "Samuel Kessler", "authors": "Samuel Kessler, Vu Nguyen, Stefan Zohren, Stephen Roberts", "title": "Hierarchical Indian Buffet Neural Networks for Bayesian Continual\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We place an Indian Buffet process (IBP) prior over the structure of a\nBayesian Neural Network (BNN), thus allowing the complexity of the BNN to\nincrease and decrease automatically. We further extend this model such that the\nprior on the structure of each hidden layer is shared globally across all\nlayers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of\nresource allocation in Continual Learning (CL) where new tasks occur and the\nnetwork requires extra resources. Our model uses online variational inference\nwith reparameterisation of the Bernoulli and Beta distributions, which\nconstitute the IBP and H-IBP priors. As we automatically learn the number of\nweights in each layer of the BNN, overfitting and underfitting problems are\nlargely overcome. We show empirically that our approach offers a competitive\nedge over existing methods in CL.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:43:31 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 15:48:35 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 19:54:01 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 16:50:00 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kessler", "Samuel", ""], ["Nguyen", "Vu", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "1912.02292", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\n  Ilya Sutskever", "title": "Deep Double Descent: Where Bigger Models and More Data Hurt", "comments": "G.K. and Y.B. contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a variety of modern deep learning tasks exhibit a\n\"double-descent\" phenomenon where, as we increase model size, performance first\ngets worse and then gets better. Moreover, we show that double descent occurs\nnot just as a function of model size, but also as a function of the number of\ntraining epochs. We unify the above phenomena by defining a new complexity\nmeasure we call the effective model complexity and conjecture a generalized\ndouble descent with respect to this measure. Furthermore, our notion of model\ncomplexity allows us to identify certain regimes where increasing (even\nquadrupling) the number of train samples actually hurts test performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:47:31 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Kaplun", "Gal", ""], ["Bansal", "Yamini", ""], ["Yang", "Tristan", ""], ["Barak", "Boaz", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1912.02338", "submitter": "Liliang Ren", "authors": "Liliang Ren, Gen Sun and Jiaman Wu", "title": "RoNGBa: A Robustly Optimized Natural Gradient Boosting Training Approach\n  with Leaf Number Clipping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural gradient has been recently introduced to the field of boosting to\nenable the generic probabilistic predication capability. Natural gradient\nboosting shows promising performance improvements on small datasets due to\nbetter training dynamics, but it suffers from slow training speed overhead\nespecially for large datasets. We present a replication study of NGBoost(Duan\net al., 2019) training that carefully examines the impacts of key\nhyper-parameters under the circumstance of best-first decision tree learning.\nWe find that with the regularization of leaf number clipping, the performance\nof NGBoost can be largely improved via a better choice of hyperparameters.\nExperiments show that our approach significantly beats the state-of-the-art\nperformance on various kinds of datasets from the UCI Machine Learning\nRepository while still has up to 4.85x speed up compared with the original\napproach of NGBoost.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 01:38:34 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Ren", "Liliang", ""], ["Sun", "Gen", ""], ["Wu", "Jiaman", ""]]}, {"id": "1912.02351", "submitter": "Joshua Chang", "authors": "Joshua C. Chang and Shashaank Vattikuti and Carson C. Chow", "title": "Probabilistically-autoencoded horseshoe-disentangled multidomain\n  item-response theory models", "comments": "Presented as poster at the NeurIPS 2019 Bayesian Deep Learning\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Item response theory (IRT) is a non-linear generative probabilistic paradigm\nfor using exams to identify, quantify, and compare latent traits of\nindividuals, relative to their peers, within a population of interest. In\npre-existing multidimensional IRT methods, one requires a factorization of the\ntest items. For this task, linear exploratory factor analysis is used, making\nIRT a posthoc model. We propose skipping the initial factor analysis by using a\nsparsity-promoting horseshoe prior to perform factorization directly within the\nIRT model so that all training occurs in a single self-consistent step. Being a\nhierarchical Bayesian model, we adapt the WAIC to the problem of dimensionality\nselection. IRT models are analogous to probabilistic autoencoders. By binding\nthe generative IRT model to a Bayesian neural network (forming a probabilistic\nautoencoder), one obtains a scoring algorithm consistent with the interpretable\nBayesian model. In some IRT applications the black-box nature of a neural\nnetwork scoring machine is desirable. In this manuscript, we demonstrate\nwithin-IRT factorization and comment on scoring approaches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 02:28:47 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Chang", "Joshua C.", ""], ["Vattikuti", "Shashaank", ""], ["Chow", "Carson C.", ""]]}, {"id": "1912.02365", "submitter": "Yair Carmon", "authors": "Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan\n  Srebro and Blake Woodworth", "title": "Lower Bounds for Non-Convex Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We lower bound the complexity of finding $\\epsilon$-stationary points (with\ngradient norm at most $\\epsilon$) using stochastic first-order methods. In a\nwell-studied model where algorithms access smooth, potentially non-convex\nfunctions through queries to an unbiased stochastic gradient oracle with\nbounded variance, we prove that (in the worst case) any algorithm requires at\nleast $\\epsilon^{-4}$ queries to find an $\\epsilon$ stationary point. The lower\nbound is tight, and establishes that stochastic gradient descent is minimax\noptimal in this model. In a more restrictive model where the noisy gradient\nestimates satisfy a mean-squared smoothness property, we prove a lower bound of\n$\\epsilon^{-3}$ queries, establishing the optimality of recently proposed\nvariance reduction techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:37:44 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Arjevani", "Yossi", ""], ["Carmon", "Yair", ""], ["Duchi", "John C.", ""], ["Foster", "Dylan J.", ""], ["Srebro", "Nathan", ""], ["Woodworth", "Blake", ""]]}, {"id": "1912.02368", "submitter": "Abdul Rahman Kreidieh", "authors": "Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak\n  Parajuli, Sergey Levine, Alexandre M. Bayen", "title": "Inter-Level Cooperation in Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models for deep reinforcement learning (RL) have emerged as\npowerful methods for generating meaningful control strategies in difficult long\ntime horizon tasks. Training of said hierarchical models, however, continue to\nsuffer from instabilities that limit their applicability. In this paper, we\naddress instabilities that arise from the concurrent optimization of\ngoal-assignment and goal-achievement policies. Drawing connections between this\nconcurrent optimization scheme and communication and cooperation in multi-agent\nRL, we redefine the standard optimization procedure to explicitly promote\ncooperation between these disparate tasks. Our method is demonstrated to\nachieve superior results to existing techniques in a set of difficult long time\nhorizon tasks, and serves to expand the scope of solvable tasks by hierarchical\nreinforcement learning. Videos of the results are available at:\nhttps://sites.google.com/berkeley.edu/cooperative-hrl.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:56:44 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 08:46:11 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kreidieh", "Abdul Rahman", ""], ["Berseth", "Glen", ""], ["Trabucco", "Brandon", ""], ["Parajuli", "Samyak", ""], ["Levine", "Sergey", ""], ["Bayen", "Alexandre M.", ""]]}, {"id": "1912.02373", "submitter": "Hossein Kamalzadeh", "authors": "Hossein Kamalzadeh, Saeid Nassim Sobhan, Azam Boskabadi, Mohsen\n  Hatami, Amin Gharehyakheh", "title": "Modeling and Prediction of Iran's Steel Consumption Based on Economic\n  Activity Using Support Vector Machines", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steel industry has great impacts on the economy and the environment of\nboth developed and underdeveloped countries. The importance of this industry\nand these impacts have led many researchers to investigate the relationship\nbetween a country's steel consumption and its economic activity resulting in\nthe so-called intensity of use model. This paper investigates the validity of\nthe intensity of use model for the case of Iran's steel consumption and extends\nthis hypothesis by using the indexes of economic activity to model the steel\nconsumption. We use the proposed model to train support vector machines and\npredict the future values for Iran's steel consumption. The paper provides\ndetailed correlation tests for the factors used in the model to check for their\nrelationships with the steel consumption. The results indicate that Iran's\nsteel consumption is strongly correlated with its economic activity following\nthe same pattern as the economy has been in the last four decades.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:13:35 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kamalzadeh", "Hossein", ""], ["Sobhan", "Saeid Nassim", ""], ["Boskabadi", "Azam", ""], ["Hatami", "Mohsen", ""], ["Gharehyakheh", "Amin", ""]]}, {"id": "1912.02379", "submitter": "Vishvak Murahari", "authors": "Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das", "title": "Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art\n  Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in visual dialog has focused on training deep neural models on\nVisDial in isolation. Instead, we present an approach to leverage pretraining\non related vision-language datasets before transferring to visual dialog. We\nadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn\nvisually-grounded conversations. Our model is pretrained on the Conceptual\nCaptions and Visual Question Answering datasets, and finetuned on VisDial. Our\nbest single model outperforms prior published work (including model ensembles)\nby more than 1% absolute on NDCG and MRR. Next, we find that additional\nfinetuning using \"dense\" annotations in VisDial leads to even higher NDCG --\nmore than 10% over our base model -- but hurts MRR -- more than 17% below our\nbase model! This highlights a trade-off between the two primary metrics -- NDCG\nand MRR -- which we find is due to dense annotations not correlating well with\nthe original ground-truth answers to questions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:51:11 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 03:12:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Murahari", "Vishvak", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Das", "Abhishek", ""]]}, {"id": "1912.02386", "submitter": "Justin Cosentino", "authors": "Justin Cosentino, Federico Zaiter, Dan Pei, Jun Zhu", "title": "The Search for Sparse, Robust Neural Networks", "comments": "The Safety and Robustness in Decision Making Workshop at the 33rd\n  Conference on Neural InformationProcessing Systems (NeurIPS 2019), Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on deep neural network pruning has shown there exist sparse\nsubnetworks that achieve equal or improved accuracy, training time, and loss\nusing fewer network parameters when compared to their dense counterparts.\nOrthogonal to pruning literature, deep neural networks are known to be\nsusceptible to adversarial examples, which may pose risks in security- or\nsafety-critical applications. Intuition suggests that there is an inherent\ntrade-off between sparsity and robustness such that these characteristics could\nnot co-exist. We perform an extensive empirical evaluation and analysis testing\nthe Lottery Ticket Hypothesis with adversarial training and show this approach\nenables us to find sparse, robust neural networks. Code for reproducing\nexperiments is available here:\nhttps://github.com/justincosentino/robust-sparse-networks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:06:35 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Cosentino", "Justin", ""], ["Zaiter", "Federico", ""], ["Pei", "Dan", ""], ["Zhu", "Jun", ""]]}, {"id": "1912.02390", "submitter": "Vasant Honavar", "authors": "Sanghack Lee and Vasant Honavar", "title": "Towards Robust Relational Causal Discovery", "comments": "14 pages", "journal-ref": "Proceedings of the 35th Conference on Uncertainty in Artificial\n  Intelligence, UAI 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal relationships from relational\ndata. Existing approaches rely on queries to a relational conditional\nindependence (RCI) oracle to establish and orient causal relations in such a\nsetting. In practice, queries to a RCI oracle have to be replaced by reliable\ntests for RCI against available data. Relational data present several unique\nchallenges in testing for RCI. We study the conditions under which traditional\niid-based conditional independence (CI) tests yield reliable answers to RCI\nqueries against relational data. We show how to conduct CI tests against\nrelational data to robustly recover the underlying relational causal structure.\nResults of our experiments demonstrate the effectiveness of our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:13:22 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lee", "Sanghack", ""], ["Honavar", "Vasant", ""]]}, {"id": "1912.02392", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "KoPA: Automated Kronecker Product Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matrix approximation and denoising induced by the\nKronecker product decomposition. Specifically, we propose to approximate a\ngiven matrix by the sum of a few Kronecker products of matrices, which we refer\nto as the Kronecker product approximation (KoPA). Because the Kronecker product\nis an extension of the outer product from vectors to matrices, KoPA extends the\nlow rank matrix approximation, and includes it as a special case. Comparing\nwith the latter, KoPA also offers a greater flexibility, since it allows the\nuser to choose the configuration, which are the dimensions of the two smaller\nmatrices forming the Kronecker product. On the other hand, the configuration to\nbe used is usually unknown, and needs to be determined from the data in order\nto achieve the optimal balance between accuracy and parsimony. We propose to\nuse extended information criteria to select the configuration. Under the\nparadigm of high dimensional analysis, we show that the proposed procedure is\nable to select the true configuration with probability tending to one, under\nsuitable conditions on the signal-to-noise ratio. We demonstrate the\nsuperiority of KoPA over the low rank approximations through numerical studies,\nand several benchmark image examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:27:01 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:52:55 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 20:30:09 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1912.02399", "submitter": "Yujia Li", "authors": "Tanbin Rahman, Yujia Li, Tianzhou Ma, Lu Tang, George Tseng", "title": "A sparse negative binomial mixture model for clustering RNA-seq count\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with variable selection is a challenging yet critical task for\nmodern small-n-large-p data. Existing methods based on sparse Gaussian mixture\nmodels or sparse K-means provide solutions to continuous data. With the\nprevalence of RNA-seq technology and lack of count data modeling for\nclustering, the current practice is to normalize count expression data into\ncontinuous measures and apply existing models with Gaussian assumption. In this\npaper, we develop a negative binomial mixture model with lasso or fused lasso\ngene regularization to cluster samples (small n) with high-dimensional gene\nfeatures (large p). EM algorithm and Bayesian information criterion are used\nfor inference and determining tuning parameters. The method is compared with\nexisting methods using extensive simulations and two real transcriptomic\napplications in rat brain and breast cancer studies. The result shows superior\nperformance of the proposed count data model in clustering accuracy, feature\nselection and biological interpretation in pathways.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:55:36 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 21:49:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rahman", "Tanbin", ""], ["Li", "Yujia", ""], ["Ma", "Tianzhou", ""], ["Tang", "Lu", ""], ["Tseng", "George", ""]]}, {"id": "1912.02400", "submitter": "Matthew Fontaine", "authors": "Matthew C. Fontaine, Julian Togelius, Stefanos Nikolaidis, Amy K.\n  Hoover", "title": "Covariance Matrix Adaptation for the Rapid Illumination of Behavior\n  Space", "comments": "Accepted to GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3390232", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the challenge of finding a diverse collection of quality\nsolutions on complex continuous domains. While quality diver-sity (QD)\nalgorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are\ndesigned to generate a diverse range of solutions, these algorithms require a\nlarge number of evaluations for exploration of continuous spaces. Meanwhile,\nvariants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are\namong the best-performing derivative-free optimizers in single-objective\ncontinuous domains. This paper proposes a new QD algorithm called Covariance\nMatrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the\nself-adaptation techniques of CMA-ES with archiving and mapping techniques for\nmaintaining diversity in QD. Results from experiments based on standard\ncontinuous optimization benchmarks show that CMA-ME finds better-quality\nsolutions than MAP-Elites; similarly, results on the strategic game Hearthstone\nshow that CMA-ME finds both a higher overall quality and broader diversity of\nstrategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles\nthe performance of MAP-Elites using standard QD performance metrics. These\nresults suggest that QD algorithms augmented by operators from state-of-the-art\noptimization algorithms can yield high-performing methods for simultaneously\nexploring and optimizing continuous search spaces, with significant\napplications to design, testing, and reinforcement learning among other\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 06:06:42 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 09:47:00 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Fontaine", "Matthew C.", ""], ["Togelius", "Julian", ""], ["Nikolaidis", "Stefanos", ""], ["Hoover", "Amy K.", ""]]}, {"id": "1912.02405", "submitter": "Hossein Kamalzadeh", "authors": "Hossein Kamalzadeh, Abbas Ahmadi, Saeed Mansour", "title": "Clustering Time-Series by a Novel Slope-Based Similarity Measure\n  Considering Particle Swarm Optimization", "comments": "27 pages, 8 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an increase in the studies on time-series data mining\nspecifically time-series clustering due to the vast existence of time-series in\nvarious domains. The large volume of data in the form of time-series makes it\nnecessary to employ various techniques such as clustering to understand the\ndata and to extract information and hidden patterns. In the field of clustering\nspecifically, time-series clustering, the most important aspects are the\nsimilarity measure used and the algorithm employed to conduct the clustering.\nIn this paper, a new similarity measure for time-series clustering is developed\nbased on a combination of a simple representation of time-series, slope of each\nsegment of time-series, Euclidean distance and the so-called dynamic time\nwarping. It is proved in this paper that the proposed distance measure is\nmetric and thus indexing can be applied. For the task of clustering, the\nParticle Swarm Optimization algorithm is employed. The proposed similarity\nmeasure is compared to three existing measures in terms of various criteria\nused for the evaluation of clustering algorithms. The results indicate that the\nproposed similarity measure outperforms the rest in almost every dataset used\nin this paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 06:22:04 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kamalzadeh", "Hossein", ""], ["Ahmadi", "Abbas", ""], ["Mansour", "Saeed", ""]]}, {"id": "1912.02427", "submitter": "Qing Qu", "authors": "Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, Zhihui Zhu", "title": "Analysis of the Optimization Landscapes for Overcomplete Representation\n  Learning", "comments": "68 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonconvex optimization landscapes for learning overcomplete\nrepresentations, including learning (i) sparsely used overcomplete dictionaries\nand (ii) convolutional dictionaries, where these unsupervised learning problems\nfind many applications in high-dimensional data analysis. Despite the empirical\nsuccess of simple nonconvex algorithms, theoretical justifications of why these\nmethods work so well are far from satisfactory. In this work, we show these\nproblems can be formulated as $\\ell^4$-norm optimization problems with\nspherical constraint, and study the geometric properties of their nonconvex\noptimization landscapes. For both problems, we show the nonconvex objectives\nhave benign (global) geometric structures, in the sense that every local\nminimizer is close to one of the target solutions and every saddle point\nexhibits negative curvature. This discovery enables the development of\nguaranteed global optimization methods using simple initializations. For both\nproblems, we show the nonconvex objectives have benign geometric structures --\nevery local minimizer is close to one of the target solutions and every saddle\npoint exhibits negative curvature -- either in the entire space or within a\nsufficiently large region. This discovery ensures local search algorithms (such\nas Riemannian gradient descent) with simple initializations approximately find\nthe target solutions. Finally, numerical experiments justify our theoretical\ndiscoveries.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 08:14:24 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 18:54:46 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Qu", "Qing", ""], ["Zhai", "Yuexiang", ""], ["Li", "Xiao", ""], ["Zhang", "Yuqian", ""], ["Zhu", "Zhihui", ""]]}, {"id": "1912.02493", "submitter": "Victor Picheny", "authors": "Victor Picheny, Sattar Vakili, Artem Artemev", "title": "Ordinal Bayesian Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation is a powerful tool to solve expensive black-box\nproblems, but fails when the stationary assumption made on the objective\nfunction is strongly violated, which is the case in particular for\nill-conditioned or discontinuous objectives. We tackle this problem by\nproposing a new Bayesian optimisation framework that only considers the\nordering of variables, both in the input and output spaces, to fit a Gaussian\nprocess in a latent space. By doing so, our approach is agnostic to the\noriginal metrics on the original spaces. We propose two algorithms,\nrespectively based on an optimistic strategy and on Thompson sampling. For the\noptimistic strategy we prove an optimal performance under the measure of regret\nin the latent space. We illustrate the capability of our framework on several\nchallenging toy problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:46:06 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Picheny", "Victor", ""], ["Vakili", "Sattar", ""], ["Artemev", "Artem", ""]]}, {"id": "1912.02494", "submitter": "Tomaso Fontanini", "authors": "Tomaso Fontanini, Eleonora Iotti, Luca Donati and Andrea Prati", "title": "MetalGAN: Multi-Domain Label-Less Image Synthesis Using cGANs and\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis is currently one of the most addressed image processing topic\nin computer vision and deep learning fields of study. Researchers have tackled\nthis problem focusing their efforts on its several challenging problems, e.g.\nimage quality and size, domain and pose changing, architecture of the networks,\nand so on. Above all, producing images belonging to different domains by using\na single architecture is a very relevant goal for image generation. In fact, a\nsingle multi-domain network would allow greater flexibility and robustness in\nthe image synthesis task than other approaches. This paper proposes a novel\narchitecture and a training algorithm, which are able to produce multi-domain\noutputs using a single network. A small portion of a dataset is intentionally\nused, and there are no hard-coded labels (or classes). This is achieved by\ncombining a conditional Generative Adversarial Network (cGAN) for image\ngeneration and a Meta-Learning algorithm for domain switch, and we called our\napproach MetalGAN. The approach has proved to be appropriate for solving the\nmulti-domain problem and it is validated on facial attribute transfer, using\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:47:08 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 09:40:52 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Fontanini", "Tomaso", ""], ["Iotti", "Eleonora", ""], ["Donati", "Luca", ""], ["Prati", "Andrea", ""]]}, {"id": "1912.02503", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Azar, Bilal\n  Piot, Nicolas Heess, Hado van Hasselt, Greg Wayne, Satinder Singh, Doina\n  Precup, Remi Munos", "title": "Hindsight Credit Assignment", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficient credit assignment in reinforcement\nlearning. In order to efficiently and meaningfully utilize new data, we propose\nto explicitly assign credit to past decisions based on the likelihood of them\nhaving led to the observed outcome. This approach uses new information in\nhindsight, rather than employing foresight. Somewhat surprisingly, we show that\nvalue functions can be rewritten through this lens, yielding a new family of\nalgorithms. We study the properties of these algorithms, and empirically show\nthat they successfully address important credit assignment challenges, through\na set of illustrative tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 11:05:27 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Dabney", "Will", ""], ["Mesnard", "Thomas", ""], ["Azar", "Mohammad", ""], ["Piot", "Bilal", ""], ["Heess", "Nicolas", ""], ["van Hasselt", "Hado", ""], ["Wayne", "Greg", ""], ["Singh", "Satinder", ""], ["Precup", "Doina", ""], ["Munos", "Remi", ""]]}, {"id": "1912.02522", "submitter": "Arsha Nagrani", "authors": "Joon Son Chung, Arsha Nagrani, Ernesto Coto, Weidi Xie, Mitchell\n  McLaren, Douglas A Reynolds and Andrew Zisserman", "title": "VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge", "comments": "ISCA Archive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VoxCeleb Speaker Recognition Challenge 2019 aimed to assess how well\ncurrent speaker recognition technology is able to identify speakers in\nunconstrained or `in the wild' data. It consisted of: (i) a publicly available\nspeaker recognition dataset from YouTube videos together with ground truth\nannotation and standardised evaluation software; and (ii) a public challenge\nand workshop held at Interspeech 2019 in Graz, Austria. This paper outlines the\nchallenge and provides its baselines, results and discussions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:00:45 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Chung", "Joon Son", ""], ["Nagrani", "Arsha", ""], ["Coto", "Ernesto", ""], ["Xie", "Weidi", ""], ["McLaren", "Mitchell", ""], ["Reynolds", "Douglas A", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1912.02527", "submitter": "David Tolpin", "authors": "David Tolpin", "title": "Warped Input Gaussian Processes for Time Series Forecasting", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Gaussian process-based model for handling of non-stationarity.\nThe warping is achieved non-parametrically, through imposing a prior on the\nrelative change of distance between subsequent observation inputs. The model\nallows the use of general gradient optimization algorithms for training and\nincurs only a small computational overhead on training and prediction. The\nmodel finds its applications in forecasting in non-stationary time series with\neither gradually varying volatility, presence of change points, or a\ncombination thereof. We evaluate the model on synthetic and real-world time\nseries data comparing against both baseline and known state-of-the-art\napproaches and show that the model exhibits state-of-the-art forecasting\nperformance at a lower implementation and computation cost.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:11:54 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Tolpin", "David", ""]]}, {"id": "1912.02532", "submitter": "Jan Malte Lichtenberg", "authors": "Jan Malte Lichtenberg and \\\"Ozg\\\"ur \\c{S}im\\c{s}ek", "title": "Iterative Policy-Space Expansion in Reinforcement Learning", "comments": "Workshop on Biological and Artificial Reinforcement Learning at the\n  33rd Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals solve a difficult problem much more easily when they are\npresented with a sequence of problems that starts simple and slowly increases\nin difficulty. We explore this idea in the context of reinforcement learning.\nRather than providing the agent with an externally provided curriculum of\nprogressively more difficult tasks, the agent solves a single task utilizing a\ndecreasingly constrained policy space. The algorithm we propose first learns to\ncategorize features into positive and negative before gradually learning a more\nrefined policy. Experimental results in Tetris demonstrate superior learning\nrate of our approach when compared to existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:32:15 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lichtenberg", "Jan Malte", ""], ["\u015eim\u015fek", "\u00d6zg\u00fcr", ""]]}, {"id": "1912.02566", "submitter": "Gr\\'egoire Mialon", "authors": "Gr\\'egoire Mialon, Alexandre d'Aspremont, Julien Mairal", "title": "Screening Data Points in Empirical Risk Minimization via Ellipsoidal\n  Regions and Safe Loss Functions", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design simple screening tests to automatically discard data samples in\nempirical risk minimization without losing optimization guarantees. We derive\nloss functions that produce dual objectives with a sparse solution. We also\nshow how to regularize convex losses to ensure such a dual sparsity-inducing\nproperty, and propose a general method to design screening tests for\nclassification or regression based on ellipsoidal approximations of the optimal\nset. In addition to producing computational gains, our approach also allows us\nto compress a dataset into a subset of representative points.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 13:30:01 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:52:16 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 14:17:24 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Mialon", "Gr\u00e9goire", ""], ["d'Aspremont", "Alexandre", ""], ["Mairal", "Julien", ""]]}, {"id": "1912.02572", "submitter": "Jiaxi Liu", "authors": "Jiaxi Liu, Yidong Zhang, Xiaoqing Wang, Yuming Deng, Xingyu Wu", "title": "Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an end-to-end framework for addressing the problem\nof dynamic pricing on E-commerce platform using methods based on deep\nreinforcement learning (DRL). By using four groups of different business data\nto represent the states of each time period, we model the dynamic pricing\nproblem as a Markov Decision Process (MDP). Compared with the state-of-the-art\nDRL-based dynamic pricing algorithms, our approaches make the following three\ncontributions. First, we extend the discrete set problem to the continuous\nprice set. Second, instead of using revenue as the reward function directly, we\ndefine a new function named difference of revenue conversion rates (DRCR).\nThird, the cold-start problem of MDP is tackled by pre-training and evaluation\nusing some carefully chosen historical sales data. Our approaches are evaluated\nby both offline evaluation method using real dataset of Alibaba Inc., and\nonline field experiments on Tmall.com, a major online shopping website owned by\nAlibaba Inc.. In particular, experiment results suggest that DRCR is a more\nappropriate reward function than revenue, which is widely used by current\nliterature. In the end, field experiments, which last for months on 1000 stock\nkeeping units (SKUs) of products demonstrate that continuous price sets have\nbetter performance than discrete sets and show that our approaches\nsignificantly outperformed the manual pricing by operation experts.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 13:41:03 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Liu", "Jiaxi", ""], ["Zhang", "Yidong", ""], ["Wang", "Xiaoqing", ""], ["Deng", "Yuming", ""], ["Wu", "Xingyu", ""]]}, {"id": "1912.02574", "submitter": "Sanchita Basak", "authors": "Sanchita Basak, Fangzhou Sun, Saptarshi Sengupta and Abhishek Dubey", "title": "Data-Driven Optimization of Public Transit Schedule", "comments": "20 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bus transit systems are the backbone of public transportation in the United\nStates. An important indicator of the quality of service in such\ninfrastructures is on-time performance at stops, with published transit\nschedules playing an integral role governing the level of success of the\nservice. However there are relatively few optimization architectures leveraging\nstochastic search that focus on optimizing bus timetables with the objective of\nmaximizing probability of bus arrivals at timepoints with delays within desired\non-time ranges. In addition to this, there is a lack of substantial research\nconsidering monthly and seasonal variations of delay patterns integrated with\nsuch optimization strategies. To address these,this paper makes the following\ncontributions to the corpus of studies on transit on-time performance\noptimization: (a) an unsupervised clustering mechanism is presented which\ngroups months with similar seasonal delay patterns, (b) the problem is\nformulated as a single-objective optimization task and a greedy algorithm, a\ngenetic algorithm (GA) as well as a particle swarm optimization (PSO) algorithm\nare employed to solve it, (c) a detailed discussion on empirical results\ncomparing the algorithms are provided and sensitivity analysis on\nhyper-parameters of the heuristics are presented along with execution times,\nwhich will help practitioners looking at similar problems. The analyses\nconducted are insightful in the local context of improving public transit\nscheduling in the Nashville metro region as well as informative from a global\nperspective as an elaborate case study which builds upon the growing corpus of\nempirical studies using nature-inspired approaches to transit schedule\noptimization.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:28:11 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Basak", "Sanchita", ""], ["Sun", "Fangzhou", ""], ["Sengupta", "Saptarshi", ""], ["Dubey", "Abhishek", ""]]}, {"id": "1912.02580", "submitter": "Francesco Farina", "authors": "Francesco Farina", "title": "Collective Learning", "comments": "update references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of collective learning (CL) which\nexploits the notion of collective intelligence in the field of distributed\nsemi-supervised learning. The proposed framework draws inspiration from the\nlearning behavior of human beings, who alternate phases involving\ncollaboration, confrontation and exchange of views with other consisting of\nstudying and learning on their own. On this regard, CL comprises two main\nphases: a self-training phase in which learning is performed on local private\n(labeled) data only and a collective training phase in which proxy-labels are\nassigned to shared (unlabeled) data by means of a consensus-based algorithm. In\nthe considered framework, heterogeneous systems can be connected over the same\nnetwork, each with different computational capabilities and resources and\neveryone in the network may take advantage of the cooperation and will\neventually reach higher performance with respect to those it can reach on its\nown. An extensive experimental campaign on an image classification problem\nemphasizes the properties of CL by analyzing the performance achieved by the\ncooperating agents.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:08:34 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:39:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Farina", "Francesco", ""]]}, {"id": "1912.02588", "submitter": "Ren Wang", "authors": "Ren Wang, Meng Wang, Jinjun Xiong", "title": "Tensor Recovery from Noisy and Multi-Level Quantized Measurements", "comments": null, "journal-ref": null, "doi": "10.1186/s13634-020-00698-z", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order tensors can represent scores in a rating system, frames in a\nvideo, and images of the same subject. In practice, the measurements are often\nhighly quantized due to the sampling strategies or the quality of devices.\nExisting works on tensor recovery have focused on data losses and random\nnoises. Only a few works consider tensor recovery from quantized measurements\nbut are restricted to binary measurements. This paper, for the first time,\naddresses the problem of tensor recovery from multi-level quantized\nmeasurements. Leveraging the low-rank property of the tensor, this paper\nproposes a nonconvex optimization problem for tensor recovery. We provide a\ntheoretical upper bound of the recovery error, which diminishes to zero when\nthe sizes of dimensions increase to infinity. Our error bound significantly\nimproves over the existing results in one-bit tensor recovery and quantized\nmatrix recovery. A tensor-based alternating proximal gradient descent algorithm\nwith a convergence guarantee is proposed to solve the nonconvex problem. Our\nrecovery method can handle data losses and do not need the information of the\nquantization rule. The method is validated on synthetic data, image datasets,\nand music recommender datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:27:25 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Ren", ""], ["Wang", "Meng", ""], ["Xiong", "Jinjun", ""]]}, {"id": "1912.02590", "submitter": "Nikita Kazeev", "authors": "Maxim Borisyak, Nikita Kazeev", "title": "Machine Learning on sWeighted Data", "comments": "Submitted to Journal of Physics: Conference Series (ACAT-2019\n  proceedings)", "journal-ref": null, "doi": "10.1088/1742-6596/1525/1/012088", "report-no": null, "categories": "hep-ex cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis in high energy physics has to deal with data samples produced\nfrom different sources. One of the most widely used ways to unfold their\ncontributions is the sPlot technique. It uses the results of a maximum\nlikelihood fit to assign weights to events. Some weights produced by sPlot are\nby design negative. Negative weights make it difficult to apply machine\nlearning methods. The loss function becomes unbounded. This leads to divergent\nneural network training. In this paper we propose a mathematically rigorous way\nto transform the weights obtained by sPlot into class probabilities conditioned\non observables, thus enabling to apply any machine learning algorithm\nout-of-the-box.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:06:11 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Borisyak", "Maxim", ""], ["Kazeev", "Nikita", ""]]}, {"id": "1912.02591", "submitter": "Woosung Choi", "authors": "Woosung Choi and Minseok Kim and Jaehwa Chung and Daewon Lee and\n  Soonyoung Jung", "title": "Investigating U-Nets with various Intermediate Blocks for\n  Spectrogram-based Singing Voice Separation", "comments": "8 pages 4 tables 6 figures, accepted to ISMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.MM cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing Voice Separation (SVS) tries to separate singing voice from a given\nmixed musical signal. Recently, many U-Net-based models have been proposed for\nthe SVS task, but there were no existing works that evaluate and compare\nvarious types of intermediate blocks that can be used in the U-Net\narchitecture. In this paper, we introduce a variety of intermediate spectrogram\ntransformation blocks. We implement U-nets based on these blocks and train them\non complex-valued spectrograms to consider both magnitude and phase. These\nnetworks are then compared on the SDR metric. When using a particular block\ncomposed of convolutional and fully-connected layers, it achieves\nstate-of-the-art SDR on the MUSDB singing voice separation task by a large\nmargin of 0.9 dB. Our code and models are available online.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:46:19 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 13:56:59 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 16:39:49 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Choi", "Woosung", ""], ["Kim", "Minseok", ""], ["Chung", "Jaehwa", ""], ["Lee", "Daewon", ""], ["Jung", "Soonyoung", ""]]}, {"id": "1912.02605", "submitter": "Shihua Zhang", "authors": "Zhiyang Zhang and Shihua Zhang", "title": "Towards Understanding Residual and Dilated Dense Neural Networks via\n  Convolutional Sparse Coding", "comments": "13 pages, 8 figures", "journal-ref": "National Science Review (2020)", "doi": null, "report-no": "nwaa159", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) and its variants have led to many\nstate-of-art results in various fields. However, a clear theoretical\nunderstanding about them is still lacking. Recently, multi-layer convolutional\nsparse coding (ML-CSC) has been proposed and proved to equal such simply\nstacked networks (plain networks). Here, we think three factors in each layer\nof it including the initialization, the dictionary design and the number of\niterations greatly affect the performance of ML-CSC. Inspired by these\nconsiderations, we propose two novel multi-layer models--residual convolutional\nsparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding\nmodel (MSD-CSC), which have close relationship with the residual neural network\n(ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively.\nMathematically, we derive the shortcut connection in ResNet as a special case\nof a new forward propagation rule on ML-CSC. We find a theoretical\ninterpretation of the dilated convolution and dense connection in MSDNet by\nanalyzing MSD-CSC, which gives a clear mathematical understanding about them.\nWe implement the iterative soft thresholding algorithm (ISTA) and its fast\nversion to solve Res-CSC and MSD-CSC, which can employ the unfolding operation\nfor further improvements. At last, extensive numerical experiments and\ncomparison with competing methods demonstrate their effectiveness using three\ntypical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:46:01 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 03:01:51 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhang", "Zhiyang", ""], ["Zhang", "Shihua", ""]]}, {"id": "1912.02606", "submitter": "Vineet Kumar", "authors": "Karthikeya Racharla, Vineet Kumar, Chaudhari Bhushan Jayant, Ankit\n  Khairkar, Paturu Harish", "title": "Predominant Musical Instrument Classification based on Spectral Features", "comments": "Appeared in Proceedings of SPIN 2020", "journal-ref": null, "doi": "10.1109/SPIN48934.2020.9071125", "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to examine one of the cornerstone problems of Musical\nInstrument Retrieval (MIR), in particular, instrument classification. IRMAS\n(Instrument recognition in Musical Audio Signals) data set is chosen for this\npurpose. The data includes musical clips recorded from various sources in the\nlast century, thus having a wide variety of audio quality. We have presented a\nvery concise summary of past work in this domain. Having implemented various\nsupervised learning algorithms for this classification task, SVM classifier has\noutperformed the other state-of-the-art models with an accuracy of 79%. We also\nimplemented Unsupervised techniques out of which Hierarchical Clustering has\nperformed well.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:43:24 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 18:53:52 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Racharla", "Karthikeya", ""], ["Kumar", "Vineet", ""], ["Jayant", "Chaudhari Bhushan", ""], ["Khairkar", "Ankit", ""], ["Harish", "Paturu", ""]]}, {"id": "1912.02610", "submitter": "Verena Heu{\\ss}er", "authors": "Verena Heusser, Niklas Freymuth, Stefan Constantin, Alex Waibel", "title": "Bimodal Speech Emotion Recognition Using Pre-Trained Language Models", "comments": "Life-Long Learning for Spoken Language Systems ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion recognition is a challenging task and an important step\ntowards more natural human-machine interaction. We show that pre-trained\nlanguage models can be fine-tuned for text emotion recognition, achieving an\naccuracy of 69.5% on Task 4A of SemEval 2017, improving upon the previous state\nof the art by over 3% absolute. We combine these language models with speech\nemotion recognition, achieving results of 73.5% accuracy when using provided\ntranscriptions and speech data on a subset of four classes of the IEMOCAP\ndataset. The use of noise-induced transcriptions and speech data results in an\naccuracy of 71.4%. For our experiments, we created IEmoNet, a modular and\nadaptable bimodal framework for speech emotion recognition based on pre-trained\nlanguage models. Lastly, we discuss the idea of using an emotional classifier\nas a reward for reinforcement learning as a step towards more successful and\nconvenient human-machine interaction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 23:25:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Heusser", "Verena", ""], ["Freymuth", "Niklas", ""], ["Constantin", "Stefan", ""], ["Waibel", "Alex", ""]]}, {"id": "1912.02613", "submitter": "Yin-Jyun Luo", "authors": "Yin-Jyun Luo, Chin-Chen Hsu, Kat Agres, Dorien Herremans", "title": "Singing Voice Conversion with Disentangled Representations of Singer and\n  Vocal Technique Using Variational Autoencoders", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a flexible framework that deals with both singer conversion and\nsingers vocal technique conversion. The proposed model is trained on\nnon-parallel corpora, accommodates many-to-many conversion, and leverages\nrecent advances of variational autoencoders. It employs separate encoders to\nlearn disentangled latent representations of singer identity and vocal\ntechnique separately, with a joint decoder for reconstruction. Conversion is\ncarried out by simple vector arithmetic in the learned latent spaces. Both a\nquantitative analysis as well as a visualization of the converted spectrograms\nshow that our model is able to disentangle singer identity and vocal technique\nand successfully perform conversion of these attributes. To the best of our\nknowledge, this is the first work to jointly tackle conversion of singer\nidentity and vocal technique based on a deep learning approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 03:50:08 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 15:45:57 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 03:33:46 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Luo", "Yin-Jyun", ""], ["Hsu", "Chin-Chen", ""], ["Agres", "Kat", ""], ["Herremans", "Dorien", ""]]}, {"id": "1912.02615", "submitter": "Wim Boes", "authors": "Wim Boes and Hugo Van hamme", "title": "Audiovisual Transformer Architectures for Large-Scale Classification and\n  Synchronization of Weakly Labeled Audio Events", "comments": null, "journal-ref": "Proceedings of the 27th ACM International Conference on Multimedia\n  (MM '19). ACM, New York, NY, USA, 1961-1969", "doi": "10.1145/3343031.3350873", "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of environmental event classification by drawing\ninspiration from the transformer neural network architecture used in machine\ntranslation. We modify this attention-based feedforward structure in such a way\nthat allows the resulting model to use audio as well as video to compute sound\nevent predictions. We perform extensive experiments with these adapted\ntransformers on an audiovisual data set, obtained by appending relevant visual\ninformation to an existing large-scale weakly labeled audio collection. The\nemployed multi-label data contains clip-level annotation indicating the\npresence or absence of 17 classes of environmental sounds, and does not include\ntemporal information. We show that the proposed modified transformers strongly\nimprove upon previously introduced models and in fact achieve state-of-the-art\nresults. We also make a compelling case for devoting more attention to research\nin multimodal audiovisual classification by proving the usefulness of visual\ninformation for the task at hand,namely audio event recognition. In addition,\nwe visualize internal attention patterns of the audiovisual transformers and in\ndoing so demonstrate their potential for performing multimodal synchronization.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:26:37 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Boes", "Wim", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1912.02624", "submitter": "Ruihan Zhao", "authors": "Ruihan Zhao, Stas Tiomkin, Pieter Abbeel", "title": "Learning Efficient Representation for Intrinsic Motivation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual Information between agent Actions and environment States (MIAS)\nquantifies the influence of agent on its environment. Recently, it was found\nthat the maximization of MIAS can be used as an intrinsic motivation for\nartificial agents. In literature, the term empowerment is used to represent the\nmaximum of MIAS at a certain state. While empowerment has been shown to solve a\nbroad range of reinforcement learning problems, its calculation in arbitrary\ndynamics is a challenging problem because it relies on the estimation of mutual\ninformation. Existing approaches, which rely on sampling, are limited to low\ndimensional spaces, because high-confidence distribution-free lower bounds for\nmutual information require exponential number of samples. In this work, we\ndevelop a novel approach for the estimation of empowerment in unknown dynamics\nfrom visual observation only, without the need to sample for MIAS. The core\nidea is to represent the relation between action sequences and future states\nusing a stochastic dynamic model in latent space with a specific form. This\nallows us to efficiently compute empowerment with the \"Water-Filling\" algorithm\nfrom information theory. We construct this embedding with deep neural networks\ntrained on a sophisticated objective function. Our experimental results show\nthat the designed embedding preserves information-theoretic properties of the\noriginal dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:48:40 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 01:06:37 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 23:07:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhao", "Ruihan", ""], ["Tiomkin", "Stas", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1912.02628", "submitter": "Song Fang", "authors": "Song Fang, Quanyan Zhu", "title": "Fundamental Limitations in Sequential Prediction and Recursive\n  Algorithms: $\\mathcal{L}_{p}$ Bounds via an Entropic Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.06742.\n  text overlap with arXiv:1912.05541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain fundamental $\\mathcal{L}_{p}$ bounds in sequential\nprediction and recursive algorithms via an entropic analysis. Both classes of\nproblems are examined by investigating the underlying entropic relationships of\nthe data and/or noises involved, and the derived lower bounds may all be\nquantified in a conditional entropy characterization. We also study the\nconditions to achieve the generic bounds from an innovations' viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:52:15 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:56:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1912.02631", "submitter": "Ajith Suresh", "authors": "Harsh Chaudhari, Rahul Rachuri, Ajith Suresh", "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning", "comments": "This work appeared at the 26th Annual Network and Distributed System\n  Security Symposium (NDSS) 2020. Update: An improved version of this framework\n  is available at arXiv:2106.02850", "journal-ref": null, "doi": "10.14722/ndss.2020.23005", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning has started to be deployed in fields such as healthcare and\nfinance, which propelled the need for and growth of privacy-preserving machine\nlearning (PPML). We propose an actively secure four-party protocol (4PC), and a\nframework for PPML, showcasing its applications on four of the most\nwidely-known machine learning algorithms -- Linear Regression, Logistic\nRegression, Neural Networks, and Convolutional Neural Networks. Our 4PC\nprotocol tolerating at most one malicious corruption is practically efficient\nas compared to the existing works. We use the protocol to build an efficient\nmixed-world framework (Trident) to switch between the Arithmetic, Boolean, and\nGarbled worlds. Our framework operates in the offline-online paradigm over\nrings and is instantiated in an outsourced setting for machine learning. Also,\nwe propose conversions especially relevant to privacy-preserving machine\nlearning. The highlights of our framework include using a minimal number of\nexpensive circuits overall as compared to ABY3. This can be seen in our\ntechnique for truncation, which does not affect the online cost of\nmultiplication and removes the need for any circuits in the offline phase. Our\nB2A conversion has an improvement of $\\mathbf{7} \\times$ in rounds and\n$\\mathbf{18} \\times$ in the communication complexity. The practicality of our\nframework is argued through improvements in the benchmarking of the\naforementioned algorithms when compared with ABY3. All the protocols are\nimplemented over a 64-bit ring in both LAN and WAN settings. Our improvements\ngo up to $\\mathbf{187} \\times$ for the training phase and $\\mathbf{158} \\times$\nfor the prediction phase when observed over LAN and WAN.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:06:39 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:08:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chaudhari", "Harsh", ""], ["Rachuri", "Rahul", ""], ["Suresh", "Ajith", ""]]}, {"id": "1912.02641", "submitter": "Kian Hsiang Low", "authors": "Tong Teng and Jie Chen and Yehong Zhang and Kian Hsiang Low", "title": "Scalable Variational Bayesian Kernel Selection for Sparse Gaussian\n  Process Regression", "comments": "34th AAAI Conference on Artificial Intelligence (AAAI 2020), Extended\n  version with derivations, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a variational Bayesian kernel selection (VBKS) algorithm\nfor sparse Gaussian process regression (SGPR) models. In contrast to existing\nGP kernel selection algorithms that aim to select only one kernel with the\nhighest model evidence, our proposed VBKS algorithm considers the kernel as a\nrandom variable and learns its belief from data such that the uncertainty of\nthe kernel can be interpreted and exploited to avoid overconfident GP\npredictions. To achieve this, we represent the probabilistic kernel as an\nadditional variational variable in a variational inference (VI) framework for\nSGPR models where its posterior belief is learned together with that of the\nother variational variables (i.e., inducing variables and kernel\nhyperparameters). In particular, we transform the discrete kernel belief into a\ncontinuous parametric distribution via reparameterization in order to apply VI.\nThough it is computationally challenging to jointly optimize a large number of\nhyperparameters due to many kernels being evaluated simultaneously by our VBKS\nalgorithm, we show that the variational lower bound of the log-marginal\nlikelihood can be decomposed into an additive form such that each additive term\ndepends only on a disjoint subset of the variational variables and can thus be\noptimized independently. Stochastic optimization is then used to maximize the\nvariational lower bound by iteratively improving the variational approximation\nof the exact posterior belief via stochastic gradient ascent, which incurs\nconstant time per iteration and hence scales to big data. We empirically\nevaluate the performance of our VBKS algorithm on synthetic and massive\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:23:10 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Teng", "Tong", ""], ["Chen", "Jie", ""], ["Zhang", "Yehong", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1912.02644", "submitter": "Marissa Connor", "authors": "Marissa Connor, Christopher Rozell", "title": "Representing Closed Transformation Paths in Encoded Network Latent Space", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative networks have been widely used for learning mappings from a\nlow-dimensional latent space to a high-dimensional data space. In many cases,\ndata transformations are defined by linear paths in this latent space. However,\nthe Euclidean structure of the latent space may be a poor match for the\nunderlying latent structure in the data. In this work, we incorporate a\ngenerative manifold model into the latent space of an autoencoder in order to\nlearn the low-dimensional manifold structure from the data and adapt the latent\nspace to accommodate this structure. In particular, we focus on applications in\nwhich the data has closed transformation paths which extend from a starting\npoint and return to nearly the same point. Through experiments on data with\nnatural closed transformation paths, we show that this model introduces the\nability to learn the latent dynamics of complex systems, generate\ntransformation paths, and classify samples that belong on the same\ntransformation path.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:27:26 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Connor", "Marissa", ""], ["Rozell", "Christopher", ""]]}, {"id": "1912.02686", "submitter": "Koki Kishimoto", "authors": "Koki Kishimoto, Katsuhiko Hayashi, Genki Akai, Masashi Shimbo", "title": "Binarized Canonical Polyadic Decomposition for Knowledge Graph\n  Completion", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.02970", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on vector embeddings of knowledge graphs have been actively\npursued as a promising approach to knowledge graph completion.However,\nembedding models generate storage-inefficient representations, particularly\nwhen the number of entities and relations, and the dimensionality of the\nreal-valued embedding vectors are large. We present a binarized\nCANDECOMP/PARAFAC(CP) decomposition algorithm, which we refer to as B-CP, where\nreal-valued parameters are replaced by binary values to reduce model size.\nMoreover, we show that a fast score computation technique can be developed with\nbitwise operations. We prove that B-CP is fully expressive by deriving a bound\non the size of its embeddings. Experimental results on several benchmark\ndatasets demonstrate that the proposed method successfully reduces model size\nby more than an order of magnitude while maintaining task performance at the\nsame level as the real-valued CP model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:13:53 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kishimoto", "Koki", ""], ["Hayashi", "Katsuhiko", ""], ["Akai", "Genki", ""], ["Shimbo", "Masashi", ""]]}, {"id": "1912.02696", "submitter": "Reazul Hasan Russel", "authors": "Reazul Hasan Russel, Bahram Behzadian, Marek Petrik", "title": "Optimizing Norm-Bounded Weighted Ambiguity Sets for Robust MDPs", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.10786", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal policies in Markov decision processes (MDPs) are very sensitive to\nmodel misspecification. This raises serious concerns about deploying them in\nhigh-stake domains. Robust MDPs (RMDP) provide a promising framework to\nmitigate vulnerabilities by computing policies with worst-case guarantees in\nreinforcement learning. The solution quality of an RMDP depends on the\nambiguity set, which is a quantification of model uncertainties. In this paper,\nwe propose a new approach for optimizing the shape of the ambiguity sets for\nRMDPs. Our method departs from the conventional idea of constructing a\nnorm-bounded uniform and symmetric ambiguity set. We instead argue that the\nstructure of a near-optimal ambiguity set is problem specific. Our proposed\nmethod computes a weight parameter from the value functions, and these weights\nthen drive the shape of the ambiguity sets. Our theoretical analysis\ndemonstrates the rationale of the proposed idea. We apply our method to several\ndifferent problem domains, and the empirical results further furnish the\npractical promise of weighted near-optimal ambiguity sets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:38:57 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Russel", "Reazul Hasan", ""], ["Behzadian", "Bahram", ""], ["Petrik", "Marek", ""]]}, {"id": "1912.02703", "submitter": "Craig Ganoe", "authors": "Xing Meng, Craig H. Ganoe, Ryan T. Sieberg, Yvonne Y. Cheung, Saeed\n  Hassanpour", "title": "Self-Supervised Contextual Language Representation of Radiology Reports\n  to Improve the Identification of Communication Urgency", "comments": "Accepted in AMIA 2020 Informatics Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods have recently achieved high-performance in\nbiomedical text analysis. However, a major bottleneck in the widespread\napplication of these methods is obtaining the required large amounts of\nannotated training data, which is resource intensive and time consuming. Recent\nprogress in self-supervised learning has shown promise in leveraging large text\ncorpora without explicit annotations. In this work, we built a self-supervised\ncontextual language representation model using BERT, a deep bidirectional\ntransformer architecture, to identify radiology reports requiring prompt\ncommunication to the referring physicians. We pre-trained the BERT model on a\nlarge unlabeled corpus of radiology reports and used the resulting contextual\nrepresentations in a final text classifier for communication urgency. Our model\nachieved a precision of 97.0%, recall of 93.3%, and F-measure of 95.1% on an\nindependent test set in identifying radiology reports for prompt communication,\nand significantly outperformed the previous state-of-the-art model based on\nword2vec representations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 16:33:23 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Meng", "Xing", ""], ["Ganoe", "Craig H.", ""], ["Sieberg", "Ryan T.", ""], ["Cheung", "Yvonne Y.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1912.02707", "submitter": "Eli (Omid) David", "authors": "Daniel Rika, Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the\n  Reconstruction of Portuguese Tile Panels", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1319-1327, Prague, Czech Republic, July 2019", "doi": "10.1145/3321707.3321821", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel scheme, based on a unique combination of genetic\nalgorithms (GAs) and deep learning (DL), for the automatic reconstruction of\nPortuguese tile panels, a challenging real-world variant of the jigsaw puzzle\nproblem (JPP) with important national heritage implications. Specifically, we\nintroduce an enhanced GA-based puzzle solver, whose integration with a novel\nDL-based compatibility measure (DLCM) yields state-of-the-art performance,\nregarding the above application. Current compatibility measures consider\ntypically (the chromatic information of) edge pixels (between adjacent tiles),\nand help achieve high accuracy for the synthetic JPP variant. However, such\nmeasures exhibit rather poor performance when applied to the Portuguese tile\npanels, which are susceptible to various real-world effects, e.g.,\nmonochromatic panels, non-squared tiles, edge degradation, etc. To overcome\nsuch difficulties, we have developed a novel DLCM to extract high-level\ntexture/color statistics from the entire tile information.\n  Integrating this measure with our enhanced GA-based puzzle solver, we have\ndemonstrated, for the first time, how to deal most effectively with large-scale\nreal-world problems, such as the Portuguese tile problem. Specifically, we have\nachieved 82% accuracy for the reconstruction of Portuguese tile panels with\nunknown piece rotation and puzzle dimension (compared to merely 3.5% average\naccuracy achieved by the best method known for solving this problem variant).\nThe proposed method outperforms even human experts in several cases, correcting\ntheir mistakes in the manual tile assembly.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 06:24:21 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rika", "Daniel", ""], ["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02714", "submitter": "Brandon Trabucco", "authors": "Brandon Trabucco, Albert Qu, Simon Li, Ganeshkumar Ashokavardhanan", "title": "Inferring the Optimal Policy using Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates methods for estimating the optimal stochastic control\npolicy for a Markov Decision Process with unknown transition dynamics and an\nunknown reward function. This form of model-free reinforcement learning\ncomprises many real world systems such as playing video games, simulated\ncontrol tasks, and real robot locomotion. Existing methods for estimating the\noptimal stochastic control policy rely on high variance estimates of the policy\ndescent. However, these methods are not guaranteed to find the optimal\nstochastic policy, and the high variance gradient estimates make convergence\nunstable. In order to resolve these problems, we propose a technique using\nMarkov Chain Monte Carlo to generate samples from the posterior distribution of\nthe parameters conditioned on being optimal. Our method provably converges to\nthe globally optimal stochastic policy, and empirically similar variance\ncompared to the policy gradient.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:08:24 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Trabucco", "Brandon", ""], ["Qu", "Albert", ""], ["Li", "Simon", ""], ["Ashokavardhanan", "Ganeshkumar", ""]]}, {"id": "1912.02724", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Kailash Budhathoki, Lenon Minorics, and Patrick\n  Bl\\\"obaum", "title": "Causal structure based root cause analysis of outliers", "comments": "11 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a formal approach to identify 'root causes' of outliers observed\nin $n$ variables $X_1,\\dots,X_n$ in a scenario where the causal relation\nbetween the variables is a known directed acyclic graph (DAG). To this end, we\nfirst introduce a systematic way to define outlier scores. Further, we\nintroduce the concept of 'conditional outlier score' which measures whether a\nvalue of some variable is unexpected *given the value of its parents* in the\nDAG, if one were to assume that the causal structure and the corresponding\nconditional distributions are also valid for the anomaly. Finally, we quantify\nto what extent the high outlier score of some target variable can be attributed\nto outliers of its ancestors. This quantification is defined via Shapley values\nfrom cooperative game theory.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:00:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Janzing", "Dominik", ""], ["Budhathoki", "Kailash", ""], ["Minorics", "Lenon", ""], ["Bl\u00f6baum", "Patrick", ""]]}, {"id": "1912.02729", "submitter": "Benjamin Aubin", "authors": "Alia Abbara, Benjamin Aubin, Florent Krzakala, Lenka Zdeborov\\'a", "title": "Rademacher complexity and spin glasses: A link between the replica and\n  statistical theories of learning", "comments": "15 + 10 pages, v2 revised and accepted at MSML", "journal-ref": "Proceedings of The First Mathematical and Scientific Machine\n  Learning Conference, PMLR 107:27-54, 2020", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning theory provides bounds of the generalization gap, using\nin particular the Vapnik-Chervonenkis dimension and the Rademacher complexity.\nAn alternative approach, mainly studied in the statistical physics literature,\nis the study of generalization in simple synthetic-data models. Here we discuss\nthe connections between these approaches and focus on the link between the\nRademacher complexity in statistical learning and the theories of\ngeneralization for typical-case synthetic models from statistical physics,\ninvolving quantities known as Gardner capacity and ground state energy. We show\nthat in these models the Rademacher complexity is closely related to the ground\nstate energy computed by replica theories. Using this connection, one may\nreinterpret many results of the literature as rigorous Rademacher bounds in a\nvariety of models in the high-dimensional statistics limit. Somewhat\nsurprisingly, we also show that statistical learning theory provides\npredictions for the behavior of the ground-state energies in some full replica\nsymmetry breaking models.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:09:17 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 09:00:11 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Abbara", "Alia", ""], ["Aubin", "Benjamin", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1912.02738", "submitter": "Jin Xu", "authors": "Jin Xu, Jean-Francois Ton, Hyunjik Kim, Adam R. Kosiorek, Yee Whye Teh", "title": "MetaFun: Meta-Learning with Iterative Functional Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a functional encoder-decoder approach to supervised meta-learning,\nwhere labeled data is encoded into an infinite-dimensional functional\nrepresentation rather than a finite-dimensional one. Furthermore, rather than\ndirectly producing the representation, we learn a neural update rule resembling\nfunctional gradient descent which iteratively improves the representation. The\nfinal representation is used to condition the decoder to make predictions on\nunlabeled data. Our approach is the first to demonstrates the success of\nencoder-decoder style meta-learning methods like conditional neural processes\non large-scale few-shot classification benchmarks such as miniImageNet and\ntieredImageNet, where it achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:25:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 23:18:39 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 11:41:43 GMT"}, {"version": "v4", "created": "Sun, 16 Aug 2020 10:21:10 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Xu", "Jin", ""], ["Ton", "Jean-Francois", ""], ["Kim", "Hyunjik", ""], ["Kosiorek", "Adam R.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1912.02757", "submitter": "Balaji Lakshminarayanan", "authors": "Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan", "title": "Deep Ensembles: A Loss Landscape Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep ensembles have been empirically shown to be a promising approach for\nimproving accuracy, uncertainty and out-of-distribution robustness of deep\nlearning models. While deep ensembles were theoretically motivated by the\nbootstrap, non-bootstrap ensembles trained with just random initialization also\nperform well in practice, which suggests that there could be other explanations\nfor why deep ensembles work well. Bayesian neural networks, which learn\ndistributions over the parameters of the network, are theoretically\nwell-motivated by Bayesian principles, but do not perform as well as deep\nensembles in practice, particularly under dataset shift. One possible\nexplanation for this gap between theory and practice is that popular scalable\nvariational Bayesian methods tend to focus on a single mode, whereas deep\nensembles tend to explore diverse modes in function space. We investigate this\nhypothesis by building on recent work on understanding the loss landscape of\nneural networks and adding our own exploration to measure the similarity of\nfunctions in the space of predictions. Our results show that random\ninitializations explore entirely different modes, while functions along an\noptimization trajectory or sampled from the subspace thereof cluster within a\nsingle mode predictions-wise, while often deviating significantly in the weight\nspace. Developing the concept of the diversity--accuracy plane, we show that\nthe decorrelation power of random initializations is unmatched by popular\nsubspace sampling methods. Finally, we evaluate the relative effects of\nensembling, subspace based methods and ensembles of subspace based methods, and\nthe experimental results validate our hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:48:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 03:57:04 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Fort", "Stanislav", ""], ["Hu", "Huiyi", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1912.02762", "submitter": "George Papamakarios", "authors": "George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir\n  Mohamed, Balaji Lakshminarayanan", "title": "Normalizing Flows for Probabilistic Modeling and Inference", "comments": "Review article, 64 pages, 9 figures. Published in the Journal of\n  Machine Learning Research (see https://jmlr.org/papers/v22/19-1028.html)", "journal-ref": "Journal of Machine Learning Research, 22(57):1-64, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows provide a general mechanism for defining expressive\nprobability distributions, only requiring the specification of a (usually\nsimple) base distribution and a series of bijective transformations. There has\nbeen much recent work on normalizing flows, ranging from improving their\nexpressive power to expanding their application. We believe the field has now\nmatured and is in need of a unified perspective. In this review, we attempt to\nprovide such a perspective by describing flows through the lens of\nprobabilistic modeling and inference. We place special emphasis on the\nfundamental principles of flow design, and discuss foundational topics such as\nexpressive power and computational trade-offs. We also broaden the conceptual\nframing of flows by relating them to more general probability transformations.\nLastly, we summarize the use of flows for tasks such as generative modeling,\napproximate inference, and supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:55:27 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:47:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Papamakarios", "George", ""], ["Nalisnick", "Eric", ""], ["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1912.02765", "submitter": "Ishaq Aden-Ali", "authors": "Ishaq Aden-Ali, Hassan Ashtiani", "title": "On the Sample Complexity of Learning Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPNs) can be regarded as a form of deep graphical\nmodels that compactly represent deeply factored and mixed distributions. An SPN\nis a rooted directed acyclic graph (DAG) consisting of a set of leaves\n(corresponding to base distributions), a set of sum nodes (which represent\nmixtures of their children distributions) and a set of product nodes\n(representing the products of its children distributions).\n  In this work, we initiate the study of the sample complexity of PAC-learning\nthe set of distributions that correspond to SPNs. We show that the sample\ncomplexity of learning tree structured SPNs with the usual type of leaves\n(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)\nwith the number of parameters of the SPN. More specifically, we show that the\nclass of distributions that corresponds to tree structured Gaussian SPNs with\n$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned\nwithin Total Variation error $\\epsilon$ using at most\n$\\widetilde{O}(\\frac{ed^2+k}{\\epsilon^2})$ samples. A similar result holds for\ntree structured SPNs with discrete leaves.\n  We obtain the upper bounds based on the recently proposed notion of\ndistribution compression schemes. More specifically, we show that if a (base)\nclass of distributions $\\mathcal{F}$ admits an \"efficient\" compression, then\nthe class of tree structured SPNs with leaves from $\\mathcal{F}$ also admits an\nefficient compression.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:57:58 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:18:11 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Aden-Ali", "Ishaq", ""], ["Ashtiani", "Hassan", ""]]}, {"id": "1912.02771", "submitter": "Dimitris Tsipras", "authors": "Alexander Turner, Dimitris Tsipras, Aleksander Madry", "title": "Label-Consistent Backdoor Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been demonstrated to be vulnerable to backdoor\nattacks. Specifically, by injecting a small number of maliciously constructed\ninputs into the training set, an adversary is able to plant a backdoor into the\ntrained model. This backdoor can then be activated during inference by a\nbackdoor trigger to fully control the model's behavior. While such attacks are\nvery effective, they crucially rely on the adversary injecting arbitrary inputs\nthat are---often blatantly---mislabeled. Such samples would raise suspicion\nupon human inspection, potentially revealing the attack. Thus, for backdoor\nattacks to remain undetected, it is crucial that they maintain\nlabel-consistency---the condition that injected inputs are consistent with\ntheir labels. In this work, we leverage adversarial perturbations and\ngenerative models to execute efficient, yet label-consistent, backdoor attacks.\nOur approach is based on injecting inputs that appear plausible, yet are hard\nto classify, hence causing the model to rely on the (easier-to-learn) backdoor\ntrigger.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:05:59 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 23:16:45 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Turner", "Alexander", ""], ["Tsipras", "Dimitris", ""], ["Madry", "Aleksander", ""]]}, {"id": "1912.02781", "submitter": "Balaji Lakshminarayanan", "authors": "Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer,\n  Balaji Lakshminarayanan", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and\n  Uncertainty", "comments": "Code available at https://github.com/google-research/augmix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks can achieve high accuracy when the training\ndistribution and test distribution are identically distributed, but this\nassumption is frequently violated in practice. When the train and test\ndistributions are mismatched, accuracy can plummet. Currently there are few\ntechniques that improve robustness to unforeseen data shifts encountered during\ndeployment. In this work, we propose a technique to improve the robustness and\nuncertainty estimates of image classifiers. We propose AugMix, a data\nprocessing technique that is simple to implement, adds limited computational\noverhead, and helps models withstand unforeseen corruptions. AugMix\nsignificantly improves robustness and uncertainty measures on challenging image\nclassification benchmarks, closing the gap between previous methods and the\nbest possible performance in some cases by more than half.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:18:10 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 06:16:13 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mu", "Norman", ""], ["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Gilmer", "Justin", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1912.02794", "submitter": "Muni Sreenivas Pydi", "authors": "Muni Sreenivas Pydi, Varun Jog", "title": "Adversarial Risk via Optimal Transport and Optimal Couplings", "comments": "Revised version with 43 pages and 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning algorithms perform poorly on adversarially\nmanipulated data. Adversarial risk quantifies the error of classifiers in\nadversarial settings; adversarial classifiers minimize adversarial risk. In\nthis paper, we analyze adversarial risk and adversarial classifiers from an\noptimal transport perspective. We show that the optimal adversarial risk for\nbinary classification with 0-1 loss is determined by an optimal transport cost\nbetween the probability distributions of the two classes. We develop optimal\ntransport plans (probabilistic couplings) for univariate distributions such as\nthe normal, the uniform, and the triangular distribution. We also derive\noptimal adversarial classifiers in these settings. Our analysis leads to\nalgorithm-independent fundamental limits on adversarial risk, which we\ncalculate for several real-world datasets. We extend our results to general\nloss functions under convexity and smoothness assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:39:07 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 04:27:28 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Pydi", "Muni Sreenivas", ""], ["Jog", "Varun", ""]]}, {"id": "1912.02803", "submitter": "Samuel Schoenholz", "authors": "Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi,\n  Jascha Sohl-Dickstein, Samuel S. Schoenholz", "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Tangents is a library designed to enable research into infinite-width\nneural networks. It provides a high-level API for specifying complex and\nhierarchical neural network architectures. These networks can then be trained\nand evaluated either at finite-width as usual or in their infinite-width limit.\nInfinite-width networks can be trained analytically using exact Bayesian\ninference or using gradient descent via the Neural Tangent Kernel.\nAdditionally, Neural Tangents provides tools to study gradient descent training\ndynamics of wide but finite networks in either function space or weight space.\n  The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations\ncan be automatically distributed over multiple accelerators with near-linear\nscaling in the number of devices. Neural Tangents is available at\nwww.github.com/google/neural-tangents. We also provide an accompanying\ninteractive Colab notebook.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:51:57 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Novak", "Roman", ""], ["Xiao", "Lechao", ""], ["Hron", "Jiri", ""], ["Lee", "Jaehoon", ""], ["Alemi", "Alexander A.", ""], ["Sohl-Dickstein", "Jascha", ""], ["Schoenholz", "Samuel S.", ""]]}, {"id": "1912.02807", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Tobias\n  Pfaff, Theophane Weber, Lars Buesing, Peter W. Battaglia", "title": "Combining Q-Learning and Search with Amortized Value Estimates", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce \"Search with Amortized Value Estimates\" (SAVE), an approach for\ncombining model-free Q-learning with model-based Monte-Carlo Tree Search\n(MCTS). In SAVE, a learned prior over state-action values is used to guide\nMCTS, which estimates an improved set of state-action values. The new\nQ-estimates are then used in combination with real experience to update the\nprior. This effectively amortizes the value computation performed by MCTS,\nresulting in a cooperative relationship between model-free learning and\nmodel-based search. SAVE can be implemented on top of any Q-learning agent with\naccess to a model, which we demonstrate by incorporating it into agents that\nperform challenging physical reasoning tasks and Atari. SAVE consistently\nachieves higher rewards with fewer training steps, and---in contrast to typical\nmodel-based search approaches---yields strong performance with very small\nsearch budgets. By combining real experience with information computed during\nsearch, SAVE demonstrates that it is possible to improve on both the\nperformance of model-free learning and the computational cost of planning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:54:23 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 13:59:10 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Bapst", "Victor", ""], ["Sanchez-Gonzalez", "Alvaro", ""], ["Pfaff", "Tobias", ""], ["Weber", "Theophane", ""], ["Buesing", "Lars", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1912.02864", "submitter": "Stanislav Sobolevsky", "authors": "Urwa Muaz, Stanislav Sobolevsky", "title": "Transfer Learning from an Auxiliary Discriminative Task for Unsupervised\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised anomaly detection from high dimensional data like mobility\nnetworks is a challenging task. Study of different approaches of feature\nengineering from such high dimensional data have been a focus of research in\nthis field. This study aims to investigate the transferability of features\nlearned by network classification to unsupervised anomaly detection. We propose\nuse of an auxiliary classification task to extract features from unlabelled\ndata by supervised learning, which can be used for unsupervised anomaly\ndetection. We validate this approach by designing experiments to detect\nanomalies in mobility network data from New York and Taipei, and compare the\nresults to traditional unsupervised feature learning approaches of PCA and\nautoencoders. We find that our feature learning approach yields best anomaly\ndetection performance for both datasets, outperforming other studied\napproaches. This establishes the utility of this approach to feature\nengineering, which can be applied to other problems of similar nature.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:26:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Muaz", "Urwa", ""], ["Sobolevsky", "Stanislav", ""]]}, {"id": "1912.02893", "submitter": "Miguel L\\'azaro-Gredilla", "authors": "Miguel Lazaro-Gredilla, Wolfgang Lehrach, Dileep George", "title": "Learning undirected models via query training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical amortized inference in variational autoencoders is specialized for a\nsingle probabilistic query. Here we propose an inference network architecture\nthat generalizes to unseen probabilistic queries. Instead of an encoder-decoder\npair, we can train a single inference network directly from data, using a cost\nfunction that is stochastic not only over samples, but also over queries. We\ncan use this network to perform the same inference tasks as we would in an\nundirected graphical model with hidden variables, without having to deal with\nthe intractable partition function. The results can be mapped to the learning\nof an actual undirected model, which is a notoriously hard problem. Our network\nalso marginalizes nuisance variables as required. We show that our approach\ngeneralizes to unseen probabilistic queries on also unseen test data, providing\nfast and flexible inference. Experiments show that this approach outperforms or\nmatches PCD and AdVIL on 9 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:42:52 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Lazaro-Gredilla", "Miguel", ""], ["Lehrach", "Wolfgang", ""], ["George", "Dileep", ""]]}, {"id": "1912.02911", "submitter": "Davood Karimi", "authors": "Davood Karimi, Haoran Dou, Simon K. Warfield, Ali Gholipour", "title": "Deep learning with noisy labels: exploring techniques and remedies in\n  medical image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Supervised training of deep learning models requires large labeled datasets.\nThere is a growing interest in obtaining such datasets for medical image\nanalysis applications. However, the impact of label noise has not received\nsufficient attention. Recent studies have shown that label noise can\nsignificantly impact the performance of deep learning models in many machine\nlearning and computer vision applications. This is especially concerning for\nmedical applications, where datasets are typically small, labeling requires\ndomain expertise and suffers from high inter- and intra-observer variability,\nand erroneous predictions may influence decisions that directly impact human\nhealth. In this paper, we first review the state-of-the-art in handling label\nnoise in deep learning. Then, we review studies that have dealt with label\nnoise in deep learning for medical image analysis. Our review shows that recent\nprogress on handling label noise in deep learning has gone largely unnoticed by\nthe medical image analysis community. To help achieve a better understanding of\nthe extent of the problem and its potential remedies, we conducted experiments\nwith three medical imaging datasets with different types of label noise, where\nwe investigated several existing strategies and developed new methods to combat\nthe negative effect of label noise. Based on the results of these experiments\nand our review of the literature, we have made recommendations on methods that\ncan be used to alleviate the effects of different types of label noise on deep\nmodels trained for medical image analysis. We hope that this article helps the\nmedical image analysis researchers and developers in choosing and devising new\ntechniques that effectively handle label noise in deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:58:55 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 16:14:37 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 04:18:09 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 22:45:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Karimi", "Davood", ""], ["Dou", "Haoran", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "1912.02915", "submitter": "Reza Soleymanifar", "authors": "Reza Soleymanifar, Amber Srivastava, Carolyn Beck, Srinivasa Salapaka", "title": "A Clustering Approach to Edge Controller Placement in Software Defined\n  Networks with Cost Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce two novel deterministic annealing based clustering\nalgorithms to address the problem of Edge Controller Placement (ECP) in\nwireless edge networks. These networks lie at the core of the fifth generation\n(5G) wireless systems and beyond. These algorithms, ECP-LL and ECP-LB, address\nthe dominant leader-less and leader-based controller placement topologies and\nhave linear computational complexity in terms of network size, maximum number\nof clusters and dimensionality of data. Each algorithm tries to place\ncontrollers close to edge node clusters and not far away from other controllers\nto maintain a reasonable balance between synchronization and delay costs. While\nthe ECP problem can be conveniently expressed as a multi-objective mixed\ninteger non-linear program (MINLP), our algorithms outperform state of art\nMINLP solver, BARON both in terms of accuracy and speed. Our proposed\nalgorithms have the competitive edge of avoiding poor local minima through a\nShannon entropy term in the clustering objective function. Most ECP algorithms\nare highly susceptible to poor local minima and greatly depend on\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:07:35 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Soleymanifar", "Reza", ""], ["Srivastava", "Amber", ""], ["Beck", "Carolyn", ""], ["Salapaka", "Srinivasa", ""]]}, {"id": "1912.02919", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland and Shruti Tople", "title": "An Empirical Study on the Intrinsic Privacy of SGD", "comments": "17 pages, 11 figures, 7 tables; v3 edits: emphasised empirical nature\n  of work, added more analyses, fixed some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take the first step towards understanding whether the intrinsic randomness\nof stochastic gradient descent (SGD) can be leveraged for privacy, for any\ngiven dataset and model. In doing so, we hope to mitigate the trade-off between\nprivacy and performance for models trained with differential-privacy (DP)\nguarantees. Our main contribution is a large-scale empirical analysis of SGD on\nconvex and non-convex objectives, on four datasets. We evaluate the inherent\nvariability in SGD and calculate the intrinsic data-dependent\n$\\epsilon_i(\\mathcal{D})$ values due to the inherent noise. We show that the\nvariability in model parameters due to random sampling almost always exceeds\nthat due to changes in the data. We show that the existing theoretical bound on\nthe sensitivity of SGD with convex objectives is not tight. For logistic\nregression, we observe that SGD provides intrinsic $\\epsilon_i(\\mathcal{D})$\nvalues between 3.95 and 23.10 across four datasets, dropping to between 1.25\nand 4.22 using the tight empirical sensitivity bound. For neural networks, we\nreport high $\\epsilon_i(\\mathcal{D})$ values (>40) owing to their larger\nparameter count. Next, we propose a method to augment the intrinsic noise of\nSGD to achieve the desired target $\\epsilon$. Our augmented SGD produces models\nthat outperform existing approaches with the same privacy target, closing the\ngap to noiseless utility between 0.03% and 36.31% for logistic regression. We\nfurther explore the role of the number of steps of SGD, and demonstrate that\nour estimates are stable. Our experiments provide concrete evidence that\nchanging the seed in SGD has a far greater impact on the model's weights than\nexcluding any given training example. By accounting for this intrinsic\nrandomness - subject to necessary assumptions, we can achieve a consistent and\nstatistically significant improvement in utility, without sacrificing further\nprivacy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:28:05 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 16:08:31 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 11:46:04 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Tople", "Shruti", ""]]}, {"id": "1912.02928", "submitter": "Alessandro Bravetti", "authors": "Alessandro Bravetti, Maria L. Daza-Torres, Hugo Flores-Arguedas,\n  Michael Betancourt", "title": "Optimization algorithms inspired by the geometry of dissipative systems", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated gradient methods are a powerful optimization tool in machine\nlearning and statistics but their development has traditionally been driven by\nheuristic motivations. Recent research, however, has demonstrated that these\nmethods can be derived as discretizations of dynamical systems, which in turn\nhas provided a basis for more systematic investigations, especially into the\nstructure of those dynamical systems and their structure-preserving\ndiscretizations. In this work we introduce dynamical systems defined through a\ncontact geometry which are not only naturally suited to the optimization goal\nbut also subsume all previous methods based on geometric dynamical systems.\nThese contact dynamical systems also admit a natural, robust discretization\nthrough geometric contact integrators. We illustrate these features in\nparadigmatic examples which show that we can indeed obtain optimization\nalgorithms that achieve oracle lower bounds on convergence rates while also\nimproving on previous proposals in terms of stability.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:14:47 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 17:52:46 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 04:57:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bravetti", "Alessandro", ""], ["Daza-Torres", "Maria L.", ""], ["Flores-Arguedas", "Hugo", ""], ["Betancourt", "Michael", ""]]}, {"id": "1912.02933", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Luiz F. O. Chamon, George J. Pappas,\n  Alejandro Ribeiro", "title": "Risk-Aware MMSE Estimation", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.SY eess.SP eess.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the simplicity and intuitive interpretation of Minimum Mean Squared\nError (MMSE) estimators, their effectiveness in certain scenarios is\nquestionable. Indeed, minimizing squared errors on average does not provide any\nform of stability, as the volatility of the estimation error is left\nunconstrained. When this volatility is statistically significant, the\ndifference between the average and realized performance of the MMSE estimator\ncan be drastically different. To address this issue, we introduce a new\nrisk-aware MMSE formulation which trades between mean performance and risk by\nexplicitly constraining the expected predictive variance of the involved\nsquared error. We show that, under mild moment boundedness conditions, the\ncorresponding risk-aware optimal solution can be evaluated explicitly, and has\nthe form of an appropriately biased nonlinear MMSE estimator. We further\nillustrate the effectiveness of our approach via several numerical examples,\nwhich also showcase the advantages of risk-aware MMSE estimation against\nrisk-neutral MMSE estimation, especially in models involving skewed,\nheavy-tailed distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:33:33 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Chamon", "Luiz F. O.", ""], ["Pappas", "George J.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1912.02945", "submitter": "Thanh-Trung Trinh", "authors": "Thanh-Trung Trinh, Dinh-Minh Vu, Masaomi Kimura", "title": "A pedestrian path-planning model in accordance with obstacle's danger\n  with reinforcement learning", "comments": null, "journal-ref": null, "doi": "10.1145/3388176.3388187", "report-no": null, "categories": "cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most microscopic pedestrian navigation models use the concept of \"forces\"\napplied to the pedestrian agents to replicate the navigation environment. While\nthe approach could provide believable results in regular situations, it does\nnot always resemble natural pedestrian navigation behaviour in many typical\nsettings. In our research, we proposed a novel approach using reinforcement\nlearning for simulation of pedestrian agent path planning and collision\navoidance problem. The primary focus of this approach is using human perception\nof the environment and danger awareness of interferences. The implementation of\nour model has shown that the path planned by the agent shares many similarities\nwith a human pedestrian in several aspects such as following common walking\nconventions and human behaviours.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:40:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Trinh", "Thanh-Trung", ""], ["Vu", "Dinh-Minh", ""], ["Kimura", "Masaomi", ""]]}, {"id": "1912.02955", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "Hybrid Kronecker Product Decomposition and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the underlying low dimensional structure of high dimensional data\nhas attracted a significant amount of researches recently and has shown to have\na wide range of applications. As an effective dimension reduction tool,\nsingular value decomposition is often used to analyze high dimensional\nmatrices, which are traditionally assumed to have a low rank matrix\napproximation. In this paper, we propose a new approach. We assume a high\ndimensional matrix can be approximated by a sum of a small number of Kronecker\nproducts of matrices with potentially different configurations, named as a\nhybird Kronecker outer Product Approximation (hKoPA). It provides an extremely\nflexible way of dimension reduction compared to the low-rank matrix\napproximation. Challenges arise in estimating a hKoPA when the configurations\nof component Kronecker products are different or unknown. We propose an\nestimation procedure when the set of configurations are given and a joint\nconfiguration determination and component estimation procedure when the\nconfigurations are unknown. Specifically, a least squares backfitting algorithm\nis used when the configuration is given. When the configuration is unknown, an\niterative greedy algorithm is used. Both simulation and real image examples\nshow that the proposed algorithms have promising performances. The hybrid\nKronecker product approximation may have potentially wider applications in low\ndimensional representation of high dimensional data\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 02:57:31 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1912.02968", "submitter": "Alexandre Tartakovsky", "authors": "QiZhi He and David Brajas-Solano and Guzel Tartakovsky and Alexandre\n  M. Tartakovsky", "title": "Physics-Informed Neural Networks for Multiphysics Data Assimilation with\n  Application to Subsurface Transport", "comments": null, "journal-ref": null, "doi": "10.1016/j.advwatres.2020.103610", "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation for parameter and state estimation in subsurface transport\nproblems remains a significant challenge due to the sparsity of measurements,\nthe heterogeneity of porous media, and the high computational cost of forward\nnumerical models. We present a physics-informed deep neural networks (DNNs)\nmachine learning method for estimating space-dependent hydraulic conductivity,\nhydraulic head, and concentration fields from sparse measurements. In this\napproach, we employ individual DNNs to approximate the unknown parameters\n(e.g., hydraulic conductivity) and states (e.g., hydraulic head and\nconcentration) of a physical system, and jointly train these DNNs by minimizing\nthe loss function that consists of the governing equations residuals in\naddition to the error with respect to measurement data. We apply this approach\nto assimilate conductivity, hydraulic head, and concentration measurements for\njoint inversion of the conductivity, hydraulic head, and concentration fields\nin a steady-state advection--dispersion problem. We study the accuracy of the\nphysics-informed DNN approach with respect to data size, number of variables\n(conductivity and head versus conductivity, head, and concentration), DNNs\nsize, and DNN initialization during training. We demonstrate that the\nphysics-informed DNNs are significantly more accurate than standard data-driven\nDNNs when the training set consists of sparse data. We also show that the\naccuracy of parameter estimation increases as additional variables are inverted\njointly.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 03:33:25 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["He", "QiZhi", ""], ["Brajas-Solano", "David", ""], ["Tartakovsky", "Guzel", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1912.02975", "submitter": "Xingyou Song", "authors": "Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur", "title": "Observational Overfitting in Reinforcement Learning", "comments": "Published as a conference paper in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major component of overfitting in model-free reinforcement learning (RL)\ninvolves the case where the agent may mistakenly correlate reward with certain\nspurious features from the observations generated by the Markov Decision\nProcess (MDP). We provide a general framework for analyzing this scenario,\nwhich we use to design multiple synthetic benchmarks from only modifying the\nobservation space of an MDP. When an agent overfits to different observation\nspaces even if the underlying MDP dynamics is fixed, we term this observational\noverfitting. Our experiments expose intriguing properties especially with\nregards to implicit regularization, and also corroborate results from previous\nworks in RL generalization and supervised learning (SL).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 04:52:16 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 04:04:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Song", "Xingyou", ""], ["Jiang", "Yiding", ""], ["Tu", "Stephen", ""], ["Du", "Yilun", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "1912.02983", "submitter": "Eli (Omid) David", "authors": "Katia Huri, Eli David, Nathan S. Netanyahu", "title": "DeepEthnic: Multi-Label Ethnic Classification from Face Images", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 11141, pp. 604-612, Rhodes, Greece, October 2018", "doi": "10.1007/978-3-030-01424-7_59", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethnic group classification is a well-researched problem, which has been\npursued mainly during the past two decades via traditional approaches of image\nprocessing and machine learning. In this paper, we propose a method of\nclassifying an image face into an ethnic group by applying transfer learning\nfrom a previously trained classification network for large-scale data\nrecognition. Our proposed method yields state-of-the-art success rates of\n99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups:\nAfrican, Asian, Caucasian, and Indian.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:59:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Huri", "Katia", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.02986", "submitter": "Fei Feng Ms.", "authors": "Fei Feng, Wotao Yin, Lin F. Yang", "title": "How Does an Approximate Model Help in Reinforcement Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key approaches to save samples in reinforcement learning (RL) is\nto use knowledge from an approximate model such as its simulator. However, how\nmuch does an approximate model help to learn a near-optimal policy of the true\nunknown model? Despite numerous empirical studies of transfer reinforcement\nlearning, an answer to this question is still elusive. In this paper, we study\nthe sample complexity of RL while an approximate model of the environment is\nprovided. For an unknown Markov decision process (MDP), we show that the\napproximate model can effectively reduce the complexity by eliminating\nsub-optimal actions from the policy searching space. In particular, we provide\nan algorithm that uses $\\widetilde{O}(N/(1-\\gamma)^3/\\varepsilon^2)$ samples in\na generative model to learn an $\\varepsilon$-optimal policy, where $\\gamma$ is\nthe discount factor and $N$ is the number of near-optimal actions in the\napproximate model. This can be much smaller than the learning-from-scratch\ncomplexity $\\widetilde{\\Theta}(SA/(1-\\gamma)^3/\\varepsilon^2)$, where $S$ and\n$A$ are the sizes of state and action spaces respectively. We also provide a\nlower bound showing that the above upper bound is nearly-tight if the value gap\nbetween near-optimal actions and sub-optimal actions in the approximate model\nis sufficiently large. Our results provide a very precise characterization of\nhow an approximate model helps reinforcement learning when no additional\nassumption on the model is posed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:05:59 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:42:20 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Feng", "Fei", ""], ["Yin", "Wotao", ""], ["Yang", "Lin F.", ""]]}, {"id": "1912.02989", "submitter": "Ziming Liu", "authors": "Ziming Liu, Yixuan Wang, Zizhao Han and Dian Wu", "title": "Influenza Modeling Based on Massive Feature Engineering and\n  International Flow Deconvolution", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we focus on the analysis of the potential factors driving\nthe spread of influenza, and possible policies to mitigate the adverse effects\nof the disease. To be precise, we first invoke discrete Fourier transform (DFT)\nto conclude a yearly periodic regional structure in the influenza activity,\nthus safely restricting ourselves to the analysis of the yearly influenza\nbehavior. Then we collect a massive number of possible region-wise indicators\ncontributing to the influenza mortality, such as consumption, immunization,\nsanitation, water quality, and other indicators from external data, with $1170$\ndimensions in total. We extract significant features from the high dimensional\nindicators using a combination of data analysis techniques, including matrix\ncompletion, support vector machines (SVM), autoencoders, and principal\ncomponent analysis (PCA). Furthermore, we model the international flow of\nmigration and trade as a convolution on regional influenza activity, and solve\nthe deconvolution problem as higher-order perturbations to the linear\nregression, thus separating regional and international factors related to the\ninfluenza mortality. Finally, both the original model and the perturbed model\nare tested on regional examples, as validations of our models. Pertaining to\nthe policy, we make a proposal based on the connectivity data along with the\npreviously extracted significant features to alleviate the impact of influenza,\nas well as efficiently propagate and carry out the policies. We conclude that\nenvironmental features and economic features are of significance to the\ninfluenza mortality. The model can be easily adapted to model other types of\ninfectious diseases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:11:31 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Liu", "Ziming", ""], ["Wang", "Yixuan", ""], ["Han", "Zizhao", ""], ["Wu", "Dian", ""]]}, {"id": "1912.02992", "submitter": "Jiahao Su", "authors": "Jiahao Su, Milan Cvitkovic, Furong Huang", "title": "Sampling-Free Learning of Bayesian Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning of model parameters in neural networks is important in\nscenarios where estimates with well-calibrated uncertainty are important. In\nthis paper, we propose Bayesian quantized networks (BQNs), quantized neural\nnetworks (QNNs) for which we learn a posterior distribution over their discrete\nparameters. We provide a set of efficient algorithms for learning and\nprediction in BQNs without the need to sample from their parameters or\nactivations, which not only allows for differentiable learning in QNNs, but\nalso reduces the variance in gradients. We evaluate BQNs on MNIST,\nFashion-MNIST, KMNIST and CIFAR10 image classification datasets, compared\nagainst bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve both\nlower predictive errors and better-calibrated uncertainties than E-QNN (with\nless than 20% of the negative log-likelihood).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:27:06 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Su", "Jiahao", ""], ["Cvitkovic", "Milan", ""], ["Huang", "Furong", ""]]}, {"id": "1912.02997", "submitter": "Tomohiko Mizutani", "authors": "Tomohiko Mizutani", "title": "Improved Analysis of Spectral Algorithm for Clustering", "comments": "20 pages. Revised Theorem 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral algorithms are graph partitioning algorithms that partition a node\nset of a graph into groups by using a spectral embedding map. Clustering\ntechniques based on the algorithms are referred to as spectral clustering and\nare widely used in data analysis. To gain a better understanding of why\nspectral clustering is successful, Peng et al. (2015) and Kolev and Mehlhorn\n(2016) studied the behavior of a certain type of spectral algorithm for a class\nof graphs, called well-clustered graphs. Specifically, they put an assumption\non graphs and showed the performance guarantee of the spectral algorithm under\nit. The algorithm they studied used the spectral embedding map developed by Shi\nand Malic (2000). In this paper, we improve on their results, giving a better\nperformance guarantee under a weaker assumption. We also evaluate the\nperformance of the spectral algorithm with the spectral embedding map developed\nby Ng et al. (2001).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:32:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:59:34 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 07:29:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Mizutani", "Tomohiko", ""]]}, {"id": "1912.03011", "submitter": "Chengchao Zhao", "authors": "Zhi-Qin John Xu, Jiwei Zhang, Yaoyu Zhang, Chengchao Zhao", "title": "A priori generalization error for two-layer ReLU neural network through\n  minimum norm solution", "comments": "There is a error in this paper that the scale of initialization in\n  this paper is different from the NTK regime. So, the generalization error of\n  the neural network in the NTK regime is baseless", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on estimating \\emph{a priori} generalization error of two-layer ReLU\nneural networks (NNs) trained by mean squared error, which only depends on\ninitial parameters and the target function, through the following research\nline. We first estimate \\emph{a priori} generalization error of finite-width\ntwo-layer ReLU NN with constraint of minimal norm solution, which is proved by\n\\cite{zhang2019type} to be an equivalent solution of a linearized (w.r.t.\nparameter) finite-width two-layer NN. As the width goes to infinity, the\nlinearized NN converges to the NN in Neural Tangent Kernel (NTK) regime\n\\citep{jacot2018neural}. Thus, we can derive the \\emph{a priori} generalization\nerror of two-layer ReLU NN in NTK regime. The distance between NN in a NTK\nregime and a finite-width NN with gradient training is estimated by\n\\cite{arora2019exact}. Based on the results in \\cite{arora2019exact}, our work\nproves an \\emph{a priori} generalization error bound of two-layer ReLU NNs.\nThis estimate uses the intrinsic implicit bias of the minimum norm solution\nwithout requiring extra regularity in the loss function. This \\emph{a priori}\nestimate also implies that NN does not suffer from curse of dimensionality, and\na small generalization error can be achieved without requiring exponentially\nlarge number of neurons. In addition the research line proposed in this paper\ncan also be used to study other properties of the finite-width network, such as\nthe posterior generalization error.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:04:02 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 14:15:21 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 05:47:41 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Xu", "Zhi-Qin John", ""], ["Zhang", "Jiwei", ""], ["Zhang", "Yaoyu", ""], ["Zhao", "Chengchao", ""]]}, {"id": "1912.03015", "submitter": "Nam Hee Kim", "authors": "Nam Hee Kim, Zhaoming Xie, and Michiel van de Panne", "title": "Learning to Correspond Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamical systems exhibit similar structure, as often captured by\nhand-designed simplified models that can be used for analysis and control. We\ndevelop a method for learning to correspond pairs of dynamical systems via a\nlearned latent dynamical system. Given trajectory data from two dynamical\nsystems, we learn a shared latent state space and a shared latent dynamics\nmodel, along with an encoder-decoder pair for each of the original systems.\nWith the learned correspondences in place, we can use a simulation of one\nsystem to produce an imagined motion of its counterpart. We can also simulate\nin the learned latent dynamics and synthesize the motions of both corresponding\nsystems, as a form of bisimulation. We demonstrate the approach using pairs of\ncontrolled bipedal walkers, as well as by pairing a walker with a controlled\npendulum.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:21:49 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 23:25:07 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 20:39:08 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kim", "Nam Hee", ""], ["Xie", "Zhaoming", ""], ["van de Panne", "Michiel", ""]]}, {"id": "1912.03036", "submitter": "Vera Shalaeva", "authors": "Vera Shalaeva (LIG), Alireza Fakhrizadeh Esfahani (CRIStAL), Pascal\n  Germain (SIERRA), Mihaly Petreczky (CRIStAL)", "title": "Improved PAC-Bayesian Bounds for Linear Regression", "comments": null, "journal-ref": "Thirty-Fourth AAAI Conference on Artificial Intelligence, Feb\n  2020, New York, United States", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve the PAC-Bayesian error bound for linear regression\nderived in Germain et al. [10]. The improvements are twofold. First, the\nproposed error bound is tighter, and converges to the generalization loss with\na well-chosen temperature parameter. Second, the error bound also holds for\ntraining data that are not independently sampled. In particular, the error\nbound applies to certain time series generated by well-known classes of\ndynamical models, such as ARX models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:24:56 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Shalaeva", "Vera", "", "LIG"], ["Esfahani", "Alireza Fakhrizadeh", "", "CRIStAL"], ["Germain", "Pascal", "", "SIERRA"], ["Petreczky", "Mihaly", "", "CRIStAL"]]}, {"id": "1912.03046", "submitter": "Yiding Zhang", "authors": "Yiding Zhang, Xiao Wang, Xunqiang Jiang, Chuan Shi, Yanfang Ye", "title": "Hyperbolic Graph Attention Network", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural network (GNN) has shown superior performance in dealing with\ngraphs, which has attracted considerable research attention recently. However,\nmost of the existing GNN models are primarily designed for graphs in Euclidean\nspaces. Recent research has proven that the graph data exhibits non-Euclidean\nlatent anatomy. Unfortunately, there was rarely study of GNN in non-Euclidean\nsettings so far. To bridge this gap, in this paper, we study the GNN with\nattention mechanism in hyperbolic spaces at the first attempt. The research of\nhyperbolic GNN has some unique challenges: since the hyperbolic spaces are not\nvector spaces, the vector operations (e.g., vector addition, subtraction, and\nscalar multiplication) cannot be carried. To tackle this problem, we employ the\ngyrovector spaces, which provide an elegant algebraic formalism for hyperbolic\ngeometry, to transform the features in a graph; and then we propose the\nhyperbolic proximity based attention mechanism to aggregate the features.\nMoreover, as mathematical operations in hyperbolic spaces could be more\ncomplicated than those in Euclidean spaces, we further devise a novel\nacceleration strategy using logarithmic and exponential mappings to improve the\nefficiency of our proposed model. The comprehensive experimental results on\nfour real-world datasets demonstrate the performance of our proposed hyperbolic\ngraph attention network model, by comparisons with other state-of-the-art\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:54:36 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zhang", "Yiding", ""], ["Wang", "Xiao", ""], ["Jiang", "Xunqiang", ""], ["Shi", "Chuan", ""], ["Ye", "Yanfang", ""]]}, {"id": "1912.03049", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Andrei Stoian, David Filliat", "title": "Regularization Shortcomings for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most machine learning algorithms, training data is assumed to be\nindependent and identically distributed (iid). When it is not the case, the\nalgorithm's performances are challenged, leading to the famous phenomenon of\ncatastrophic forgetting. Algorithms dealing with it are gathered in the\nContinual Learning research field. In this paper, we study the regularization\nbased approaches to continual learning and show that those approaches can not\nlearn to discriminate classes from different tasks in an elemental continual\nbenchmark: the class-incremental scenario. We make theoretical reasoning to\nprove this shortcoming and illustrate it with examples and experiments.\nMoreover, we show that it can have some important consequences on continual\nmulti-tasks reinforcement learning or in pre-trained models used for continual\nlearning. We believe that highlighting and understanding the shortcomings of\nregularization strategies will help us to use them more efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 10:11:18 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 12:10:55 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 17:25:56 GMT"}, {"version": "v4", "created": "Sun, 4 Apr 2021 00:21:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["Stoian", "Andrei", ""], ["Filliat", "David", ""]]}, {"id": "1912.03074", "submitter": "Emilie Kaufmann", "authors": "Cindy Trinh (ENS Paris Saclay), Emilie Kaufmann (CNRS, CRIStAL,\n  SEQUEL), Claire Vernade, Richard Combes (L2S)", "title": "Solving Bernoulli Rank-One Bandits with Unimodal Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Rank-One Bandits (Katarya et al, (2017a,b)) are a simple framework\nfor regret minimization problems over rank-one matrices of arms. The initially\nproposed algorithms are proved to have logarithmic regret, but do not match the\nexisting lower bound for this problem. We close this gap by first proving that\nrank-one bandits are a particular instance of unimodal bandits, and then\nproviding a new analysis of Unimodal Thompson Sampling (UTS), initially\nproposed by Paladino et al (2017). We prove an asymptotically optimal regret\nbound on the frequentist regret of UTS and we support our claims with\nsimulations showing the significant improvement of our method compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 11:53:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Trinh", "Cindy", "", "ENS Paris Saclay"], ["Kaufmann", "Emilie", "", "CNRS, CRIStAL,\n  SEQUEL"], ["Vernade", "Claire", "", "L2S"], ["Combes", "Richard", "", "L2S"]]}, {"id": "1912.03120", "submitter": "Amir Abdi", "authors": "Amir H. Abdi, Mohammad H. Jafari, Sidney Fels, Theresa Tsang, Purang\n  Abolmaesumi", "title": "A Study into Echocardiography View Conversion", "comments": "Workshop of Medical Imaging Meets NeurIPS, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transthoracic echo is one of the most common means of cardiac studies in the\nclinical routines. During the echo exam, the sonographer captures a set of\nstandard cross sections (echo views) of the heart. Each 2D echo view cuts\nthrough the 3D cardiac geometry via a unique plane. Consequently, different\nviews share some limited information. In this work, we investigate the\nfeasibility of generating a 2D echo view using another view based on\nadversarial generative models. The objective optimized to train the\nview-conversion model is based on the ideas introduced by LSGAN, PatchGAN and\nConditional GAN (cGAN). The size and length of the left ventricle in the\ngenerated target echo view is compared against that of the target ground-truth\nto assess the validity of the echo view conversion. Results show that there is\na correlation of 0.50 between the LV areas and 0.49 between the LV lengths of\nthe generated target frames and the real target frames.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:44:59 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Abdi", "Amir H.", ""], ["Jafari", "Mohammad H.", ""], ["Fels", "Sidney", ""], ["Tsang", "Theresa", ""], ["Abolmaesumi", "Purang", ""]]}, {"id": "1912.03126", "submitter": "Nicolas Rougier", "authors": "Ikram Chraibi Kaadoud and Nicolas P. Rougier and Fr\\'ed\\'eric\n  Alexandre", "title": "Knowledge extraction from the learning of sequences in a long short term\n  memory (LSTM) architecture", "comments": "18 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a general method to extract knowledge from a recurrent neural\nnetwork (Long Short Term Memory) that has learnt to detect if a given input\nsequence is valid or not, according to an unknown generative automaton. Based\non the clustering of the hidden states, we explain how to build and validate an\nautomaton that corresponds to the underlying (unknown) automaton, and allows to\npredict if a given sequence is valid or not. The method is illustrated on\nartificial grammars (Reber's grammar variations) as well as on a real use-case\nwhose underlying grammar is unknown.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:00:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kaadoud", "Ikram Chraibi", ""], ["Rougier", "Nicolas P.", ""], ["Alexandre", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1912.03132", "submitter": "Vladimir Kobzar", "authors": "Vladimir A. Kobzar, Robert V. Kohn, Zhilei Wang", "title": "New Potential-Based Bounds for the Geometric-Stopping Version of\n  Prediction with Expert Advice", "comments": "To appear in MSML2020. arXiv admin note: text overlap with\n  arXiv:1911.01641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.AP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the classic machine learning problem of online prediction\nwith expert advice. A new potential-based framework for the fixed horizon\nversion of this problem has been recently developed using verification\narguments from optimal control theory. This paper extends this framework to the\nrandom (geometric) stopping version. To obtain explicit bounds, we construct\npotentials for the geometric version from potentials used for the fixed horizon\nversion of the problem. This construction leads to new explicit lower and upper\nbounds associated with specific adversary and player strategies. While there\nare several known lower bounds in the fixed horizon setting, our lower bounds\nappear to be the first such results in the geometric stopping setting with an\narbitrary number of experts. Our framework also leads in some cases to improved\nupper bounds. For two and three experts, our bounds are optimal to leading\norder.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:53:55 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:00:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kobzar", "Vladimir A.", ""], ["Kohn", "Robert V.", ""], ["Wang", "Zhilei", ""]]}, {"id": "1912.03133", "submitter": "Aristotelis Papadopoulos", "authors": "Aristotelis-Angelos Papadopoulos, Nazim Shaikh, Mohammad Reza Rajati", "title": "Why Should we Combine Training and Post-Training Methods for\n  Out-of-Distribution Detection?", "comments": "Preprint, 9 pages. arXiv admin note: text overlap with\n  arXiv:1906.03509", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to achieve superior results in classification\ntasks. However, it has been recently shown that they are incapable to detect\nexamples that are generated by a distribution which is different than the one\nthey have been trained on since they are making overconfident prediction for\nOut-Of-Distribution (OOD) examples. OOD detection has attracted a lot of\nattention recently. In this paper, we review some of the most seminal recent\nalgorithms in the OOD detection field, we divide those methods into training\nand post-training and we experimentally show how the combination of the former\nwith the latter can achieve state-of-the-art results in the OOD detection task.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:24:14 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Papadopoulos", "Aristotelis-Angelos", ""], ["Shaikh", "Nazim", ""], ["Rajati", "Mohammad Reza", ""]]}, {"id": "1912.03154", "submitter": "Tim Zajic", "authors": "Tim Zajic", "title": "Non-asymptotic error bounds for scaled underdamped Langevin MCMC", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have derived non-asymptotic upper bounds for convergence of\nunderdamped Langevin MCMC. We revisit these bound and consider introducing\nscaling terms in the underlying underdamped Langevin equation. In particular,\nwe provide conditions under which an appropriate scaling allows to improve the\nerror bounds in terms of the condition number of the underlying density of\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:39:04 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zajic", "Tim", ""]]}, {"id": "1912.03192", "submitter": "Sven Gowal", "authors": "Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy\n  Dvijotham, Timothy Mann, Pushmeet Kohli", "title": "Achieving Robustness in the Wild via Adversarial Mixing with\n  Disentangled Representations", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has made the surprising finding that state-of-the-art deep\nlearning models sometimes fail to generalize to small variations of the input.\nAdversarial training has been shown to be an effective approach to overcome\nthis problem. However, its application has been limited to enforcing invariance\nto analytically defined transformations like $\\ell_p$-norm bounded\nperturbations. Such perturbations do not necessarily cover plausible real-world\nvariations that preserve the semantics of the input (such as a change in\nlighting conditions). In this paper, we propose a novel approach to express and\nformalize robustness to these kinds of real-world transformations of the input.\nThe two key ideas underlying our formulation are (1) leveraging disentangled\nrepresentations of the input to define different factors of variations, and (2)\ngenerating new input images by adversarially composing the representations of\ndifferent images. We use a StyleGAN model to demonstrate the efficacy of this\nframework. Specifically, we leverage the disentangled latent representations\ncomputed by a StyleGAN model to generate perturbations of an image that are\nsimilar to real-world variations (like adding make-up, or changing the\nskin-tone of a person) and train models to be invariant to these perturbations.\nExtensive experiments show that our method improves generalization and reduces\nthe effect of spurious correlations (reducing the error rate of a \"smile\"\ndetector by 21% for example).\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:56:53 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 09:33:57 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Gowal", "Sven", ""], ["Qin", "Chongli", ""], ["Huang", "Po-Sen", ""], ["Cemgil", "Taylan", ""], ["Dvijotham", "Krishnamurthy", ""], ["Mann", "Timothy", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1912.03193", "submitter": "Luca Sabbioni", "authors": "Lorenzo Bisi, Luca Sabbioni, Edoardo Vittori, Matteo Papini, Marcello\n  Restelli", "title": "Risk-Averse Trust Region Optimization for Reward-Volatility Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world decision-making problems, for instance in the fields of\nfinance, robotics or autonomous driving, keeping uncertainty under control is\nas important as maximizing expected returns. Risk aversion has been addressed\nin the reinforcement learning literature through risk measures related to the\nvariance of returns. However, in many cases, the risk is measured not only on a\nlong-term perspective, but also on the step-wise rewards (e.g., in trading, to\nensure the stability of the investment bank, it is essential to monitor the\nrisk of portfolio positions on a daily basis). In this paper, we define a novel\nmeasure of risk, which we call reward volatility, consisting of the variance of\nthe rewards under the state-occupancy measure. We show that the reward\nvolatility bounds the return variance so that reducing the former also\nconstrains the latter. We derive a policy gradient theorem with a new objective\nfunction that exploits the mean-volatility relationship, and develop an\nactor-only algorithm. Furthermore, thanks to the linearity of the Bellman\nequations defined under the new objective function, it is possible to adapt the\nwell-known policy gradient algorithms with monotonic improvement guarantees\nsuch as TRPO in a risk-averse manner. Finally, we test the proposed approach in\ntwo simulated financial environments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:57:06 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bisi", "Lorenzo", ""], ["Sabbioni", "Luca", ""], ["Vittori", "Edoardo", ""], ["Papini", "Matteo", ""], ["Restelli", "Marcello", ""]]}, {"id": "1912.03201", "submitter": "Rene Larisch", "authors": "Ren\\'e Larisch and Michael Teichmann and Fred H. Hamker", "title": "A Neural Spiking Approach Compared to Deep Feedforward Networks on\n  Stepwise Pixel Erasement", "comments": "Published in ICANN 2018: Artificial Neural Networks and Machine\n  Learning - ICANN 2018\n  https://link.springer.com/chapter/10.1007/978-3-030-01418-6_25 The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-01418-6_25", "journal-ref": null, "doi": "10.1007/978-3-030-01418-6_25", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world scenarios, objects are often partially occluded. This requires\na robustness for object recognition against these perturbations. Convolutional\nnetworks have shown good performances in classification tasks. The learned\nconvolutional filters seem similar to receptive fields of simple cells found in\nthe primary visual cortex. Alternatively, spiking neural networks are more\nbiological plausible. We developed a two layer spiking network, trained on\nnatural scenes with a biologically plausible learning rule. It is compared to\ntwo deep convolutional neural networks using a classification task of stepwise\npixel erasement on MNIST. In comparison to these networks the spiking approach\nachieves good accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:08:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Larisch", "Ren\u00e9", ""], ["Teichmann", "Michael", ""], ["Hamker", "Fred H.", ""]]}, {"id": "1912.03221", "submitter": "Martin Robert", "authors": "Martin Robert, Patrick Dallaire and Philippe Gigu\\`ere", "title": "Tree bark re-identification using a deep-learning feature descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to visually re-identify objects is a fundamental capability in\nvision systems. Oftentimes, it relies on collections of visual signatures based\non descriptors, such as SIFT or SURF. However, these traditional descriptors\nwere designed for a certain domain of surface appearances and geometries\n(limited relief). Consequently, highly-textured surfaces such as tree bark pose\na challenge to them. In turn, this makes it more difficult to use trees as\nidentifiable landmarks for navigational purposes (robotics) or to track felled\nlumber along a supply chain (logistics). We thus propose to use data-driven\ndescriptors trained on bark images for tree surface re-identification. To this\neffect, we collected a large dataset containing 2,400 bark images with strong\nillumination changes, annotated by surface and with the ability to pixel-align\nthem. We used this dataset to sample from more than 2 million 64x64 pixel\npatches to train our novel local descriptors DeepBark and SqueezeBark. Our\nDeepBark method has shown a clear advantage against the hand-crafted\ndescriptors SIFT and SURF. For instance, we demonstrated that DeepBark can\nreach a mAP of 87.2% when retrieving 11 relevant bark images, i.e.\ncorresponding to the same physical surface, to a bark query against 7,900\nimages. Our work thus suggests that re-identifying tree surfaces in a\nchallenging illuminations context is possible. We also make public our dataset,\nwhich can be used to benchmark surface re-identification techniques.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:43:02 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 15:14:40 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Robert", "Martin", ""], ["Dallaire", "Patrick", ""], ["Gigu\u00e8re", "Philippe", ""]]}, {"id": "1912.03234", "submitter": "Alejandro Mottini", "authors": "Alejandro Mottini, Amber Roy Chowdhury", "title": "What Do You Mean I'm Funny? Personalizing the Joke Skill of a\n  Voice-Controlled Virtual Assistant", "comments": "Presented at the AAAI 2020 Workshop on Interactive and Conversational\n  Recommendation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable part of the success experienced by Voice-controlled virtual\nassistants (VVA) is due to the emotional and personalized experience they\ndeliver, with humor being a key component in providing an engaging interaction.\nIn this paper we describe methods used to improve the joke skill of a VVA\nthrough personalization. The first method, based on traditional NLP techniques,\nis robust and scalable. The others combine self-attentional network and\nmulti-task learning to obtain better results, at the cost of added complexity.\nA significant challenge facing these systems is the lack of explicit user\nfeedback needed to provide labels for the models. Instead, we explore the use\nof two implicit feedback-based labelling strategies. All models were evaluated\non real production data. Online results show that models trained on any of the\nconsidered labels outperform a heuristic method, presenting a positive\nreal-world impact on user satisfaction. Offline results suggest that the\ndeep-learning approaches can improve the joke experience with respect to the\nother considered methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:17:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mottini", "Alejandro", ""], ["Chowdhury", "Amber Roy", ""]]}, {"id": "1912.03241", "submitter": "Eugene Ie", "authors": "Larry Lansing, Vihan Jain, Harsh Mehta, Haoshuo Huang, Eugene Ie", "title": "VALAN: Vision and Language Agent Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  VALAN is a lightweight and scalable software framework for deep reinforcement\nlearning based on the SEED RL architecture. The framework facilitates the\ndevelopment and evaluation of embodied agents for solving grounded language\nunderstanding tasks, such as Vision-and-Language Navigation and\nVision-and-Dialog Navigation, in photo-realistic environments, such as\nMatterport3D and Google StreetView. We have added a minimal set of abstractions\non top of SEED RL allowing us to generalize the architecture to solve a variety\nof other RL problems. In this article, we will describe VALAN's software\nabstraction and architecture, and also present an example of using VALAN to\ndesign agents for instruction-conditioned indoor navigation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:29:43 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Lansing", "Larry", ""], ["Jain", "Vihan", ""], ["Mehta", "Harsh", ""], ["Huang", "Haoshuo", ""], ["Ie", "Eugene", ""]]}, {"id": "1912.03249", "submitter": "Arno Solin", "authors": "Yuxin Hou, Ari Heljakka, Arno Solin", "title": "Gaussian Process Priors for View-Aware Inference", "comments": "Appearing in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While frame-independent predictions with deep neural networks have become the\nprominent solutions to many computer vision tasks, the potential benefits of\nutilizing correlations between frames have received less attention. Even though\nprobabilistic machine learning provides the ability to encode correlation as\nprior knowledge for inference, there is a tangible gap between the theory and\npractice of applying probabilistic methods to modern vision problems. For this,\nwe derive a principled framework to combine information coupling between camera\nposes (translation and orientation) with deep models. We proposed a novel view\nkernel that generalizes the standard periodic kernel in $\\mathrm{SO}(3)$. We\nshow how this soft-prior knowledge can aid several pose-related vision tasks\nlike novel view synthesis and predict arbitrary points in the latent space of\ngenerative models, pointing towards a range of new applications for inter-frame\nreasoning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:41:37 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 20:02:15 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Hou", "Yuxin", ""], ["Heljakka", "Ari", ""], ["Solin", "Arno", ""]]}, {"id": "1912.03250", "submitter": "Rachel Cummings", "authors": "Uthaipon Tantipongpipat, Chris Waites, Digvijay Boob, Amaresh Ankit\n  Siva, Rachel Cummings", "title": "Differentially Private Synthetic Mixed-Type Data Generation For\n  Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the DP-auto-GAN framework for synthetic data generation, which\ncombines the low dimensional representation of autoencoders with the\nflexibility of Generative Adversarial Networks (GANs). This framework can be\nused to take in raw sensitive data and privately train a model for generating\nsynthetic data that will satisfy similar statistical properties as the original\ndata. This learned model can generate an arbitrary amount of synthetic data,\nwhich can then be freely shared due to the post-processing guarantee of\ndifferential privacy. Our framework is applicable to unlabeled mixed-type data,\nthat may include binary, categorical, and real-valued data. We implement this\nframework on both binary data (MIMIC-III) and mixed-type data (ADULT), and\ncompare its performance with existing private algorithms on metrics in\nunsupervised settings. We also introduce a new quantitative metric able to\ndetect diversity, or lack thereof, of synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:46:07 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 00:46:37 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tantipongpipat", "Uthaipon", ""], ["Waites", "Chris", ""], ["Boob", "Digvijay", ""], ["Siva", "Amaresh Ankit", ""], ["Cummings", "Rachel", ""]]}, {"id": "1912.03263", "submitter": "Will Grathwohl", "authors": "Will Grathwohl, Kuan-Chieh Wang, J\\\"orn-Henrik Jacobsen, David\n  Duvenaud, Mohammad Norouzi, Kevin Swersky", "title": "Your Classifier is Secretly an Energy Based Model and You Should Treat\n  it Like One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to reinterpret a standard discriminative classifier of p(y|x) as\nan energy based model for the joint distribution p(x,y). In this setting, the\nstandard class probabilities can be easily computed as well as unnormalized\nvalues of p(x) and p(x|y). Within this framework, standard discriminative\narchitectures may beused and the model can also be trained on unlabeled data.\nWe demonstrate that energy based training of the joint distribution improves\ncalibration, robustness, andout-of-distribution detection while also enabling\nour models to generate samplesrivaling the quality of recent GAN approaches. We\nimprove upon recently proposed techniques for scaling up the training of energy\nbased models and presentan approach which adds little overhead compared to\nstandard classification training. Our approach is the first to achieve\nperformance rivaling the state-of-the-artin both generative and discriminative\nlearning within one hybrid model.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:00:36 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 19:57:55 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 15:40:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Grathwohl", "Will", ""], ["Wang", "Kuan-Chieh", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""], ["Duvenaud", "David", ""], ["Norouzi", "Mohammad", ""], ["Swersky", "Kevin", ""]]}, {"id": "1912.03277", "submitter": "Divyat Mahajan", "authors": "Divyat Mahajan, Chenhao Tan, Amit Sharma", "title": "Preserving Causal Constraints in Counterfactual Explanations for Machine\n  Learning Classifiers", "comments": "2019 NeurIPS Workshop on Do the right thing: Machine learning and\n  Causal Inference for improved decision making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To construct interpretable explanations that are consistent with the original\nML model, counterfactual examples---showing how the model's output changes with\nsmall perturbations to the input---have been proposed. This paper extends the\nwork in counterfactual explanations by addressing the challenge of feasibility\nof such examples. For explanations of ML models in critical domains such as\nhealthcare and finance, counterfactual examples are useful for an end-user only\nto the extent that perturbation of feature inputs is feasible in the real\nworld. We formulate the problem of feasibility as preserving causal\nrelationships among input features and present a method that uses (partial)\nstructural causal models to generate actionable counterfactuals. When\nfeasibility constraints cannot be easily expressed, we consider an alternative\nmechanism where people can label generated CF examples on feasibility: whether\nit is feasible to intervene and realize the candidate CF example from the\noriginal input. To learn from this labelled feasibility data, we propose a\nmodified variational auto encoder loss for generating CF examples that\noptimizes for feasibility as people interact with its output. Our experiments\non Bayesian networks and the widely used ''Adult-Income'' dataset show that our\nproposed methods can generate counterfactual explanations that better satisfy\nfeasibility constraints than existing methods.. Code repository can be accessed\nhere: \\textit{https://github.com/divyat09/cf-feasibility}\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:16:29 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 10:18:41 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 23:46:46 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Mahajan", "Divyat", ""], ["Tan", "Chenhao", ""], ["Sharma", "Amit", ""]]}, {"id": "1912.03280", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco and Renato A. Krohling", "title": "Recent advances in deep learning applied to skin cancer detection", "comments": "Paper accepted in the Retrospectives Workshop @ NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is a major public health problem around the world. Its early\ndetection is very important to increase patient prognostics. However, the lack\nof qualified professionals and medical instruments are significant issues in\nthis field. In this context, over the past few years, deep learning models\napplied to automated skin cancer detection have become a trend. In this paper,\nwe present an overview of the recent advances reported in this field as well as\na discussion about the challenges and opportunities for improvement in the\ncurrent models. In addition, we also present some important aspects regarding\nthe use of these models in smartphones and indicate future directions we\nbelieve the field will take.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:23:30 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Pacheco", "Andre G. C.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1912.03306", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj and Markus Pauly", "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random\n  Forest Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in sparse regression models is an important task as\napplications ranging from biomedical research to econometrics have shown.\nEspecially for higher dimensional regression problems, for which the link\nfunction between response and covariates cannot be directly detected, the\nselection of informative variables is challenging. Under these circumstances,\nthe Random Forest method is a helpful tool to predict new outcomes while\ndelivering measures for variable selection. One common approach is the usage of\nthe permutation importance. Due to its intuitive idea and flexible usage, it is\nimportant to explore circumstances, for which the permutation importance based\non Random Forest correctly indicates informative covariates. Regarding the\nlatter, we deliver theoretical guarantees for the validity of the permutation\nimportance measure under specific assumptions and prove its (asymptotic)\nunbiasedness. An extensive simulation study verifies our findings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:11:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Ramosaj", "Burim", ""], ["Pauly", "Markus", ""]]}, {"id": "1912.03310", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov", "title": "Geometric Capsule Autoencoders for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn object representations from 3D point clouds\nusing bundles of geometrically interpretable hidden units, which we call\ngeometric capsules. Each geometric capsule represents a visual entity, such as\nan object or a part, and consists of two components: a pose and a feature. The\npose encodes where the entity is, while the feature encodes what it is. We use\nthese capsules to construct a Geometric Capsule Autoencoder that learns to\ngroup 3D points into parts (small local surfaces), and these parts into the\nwhole object, in an unsupervised manner. Our novel Multi-View Agreement voting\nmechanism is used to discover an object's canonical pose and its pose-invariant\nfeature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the\nproperties of the learned representations and show the benefits of having\nmultiple votes agree. We perform alignment and retrieval of arbitrarily rotated\nobjects -- tasks that evaluate our model's object identification and canonical\npose recovery capabilities -- and obtained insightful results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:10:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Srivastava", "Nitish", ""], ["Goh", "Hanlin", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1912.03321", "submitter": "Minxiang Ye", "authors": "Minxiang Ye, Vladimir Stankovic, Lina Stankovic, Gene Cheung", "title": "Robust Deep Graph Based Learning for Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based feature learning has become state of\nthe art, since given sufficient training data, CNN can significantly outperform\ntraditional methods for various classification tasks. However, feature learning\nbecomes more difficult if some training labels are noisy. With traditional\nregularization techniques, CNN often overfits to the noisy training labels,\nresulting in sub-par classification performance. In this paper, we propose a\nrobust binary classifier, based on CNNs, to learn deep metric functions, which\nare then used to construct an optimal underlying graph structure used to clean\nnoisy labels via graph Laplacian regularization (GLR). GLR is posed as a convex\nmaximum a posteriori (MAP) problem solved via convex quadratic programming\n(QP). To penalize samples around the decision boundary, we propose two\nregularized loss functions for semi-supervised learning. The binary\nclassification experiments on three datasets, varying in number and type of\nfeatures, demonstrate that given a noisy training dataset, our proposed\nnetworks outperform several state-of-the-art classifiers, including label-noise\nrobust support vector machine, CNNs with three different robust loss functions,\nmodel-based GLR, and dynamic graph CNN classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 19:11:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Ye", "Minxiang", ""], ["Stankovic", "Vladimir", ""], ["Stankovic", "Lina", ""], ["Cheung", "Gene", ""]]}, {"id": "1912.03359", "submitter": "Mohamed K. Abdel-Aziz", "authors": "Mohamed K. Abdel-Aziz, Sumudu Samarakoon, Mehdi Bennis, and Walid Saad", "title": "Ultra-Reliable and Low-Latency Vehicular Communication: An Active\n  Learning Approach", "comments": "Accepted for publication in IEEE Communication Letters with 4 pages\n  and 4 figures", "journal-ref": null, "doi": "10.1109/LCOMM.2019.2956929", "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, an age of information (AoI)-aware transmission power and\nresource block (RB) allocation technique for vehicular communication networks\nis proposed. Due to the highly dynamic nature of vehicular networks, gaining a\nprior knowledge about the network dynamics, i.e., wireless channels and\ninterference, in order to allocate resources, is challenging. Therefore, to\neffectively allocate power and RBs, the proposed approach allows the network to\nactively learn its dynamics by balancing a tradeoff between minimizing the\nprobability that the vehicles' AoI exceeds a predefined threshold and\nmaximizing the knowledge about the network dynamics. In this regard, using a\nGaussian process regression (GPR) approach, an online decentralized strategy is\nproposed to actively learn the network dynamics, estimate the vehicles' future\nAoI, and proactively allocate resources. Simulation results show a significant\nimprovement in terms of AoI violation probability, compared to several\nbaselines, with a reduction of at least 50%.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 12:50:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Abdel-Aziz", "Mohamed K.", ""], ["Samarakoon", "Sumudu", ""], ["Bennis", "Mehdi", ""], ["Saad", "Walid", ""]]}, {"id": "1912.03406", "submitter": "Malhar Jere", "authors": "Malhar Jere, Sandro Herbig, Christine Lind, Farinaz Koushanfar", "title": "Principal Component Properties of Adversarial Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks for image classification have been found to be\nvulnerable to adversarial samples, which consist of sub-perceptual noise added\nto a benign image that can easily fool trained neural networks, posing a\nsignificant risk to their commercial deployment. In this work, we analyze\nadversarial samples through the lens of their contributions to the principal\ncomponents of each image, which is different than prior works in which authors\nperformed PCA on the entire dataset. We investigate a number of\nstate-of-the-art deep neural networks trained on ImageNet as well as several\nattacks for each of the networks. Our results demonstrate empirically that\nadversarial samples across several attacks have similar properties in their\ncontributions to the principal components of neural network inputs. We propose\na new metric for neural networks to measure their robustness to adversarial\nsamples, termed the (k,p) point. We utilize this metric to achieve 93.36%\naccuracy in detecting adversarial samples independent of architecture and\nattack type for models trained on ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 01:15:40 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jere", "Malhar", ""], ["Herbig", "Sandro", ""], ["Lind", "Christine", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1912.03430", "submitter": "Boyang Li", "authors": "Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li", "title": "An Empirical Study on the Relation between Network Interpretability and\n  Adversarial Robustness", "comments": "Accepted by the journal Springer Nature Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have had many successes, but they suffer from two\nmajor issues: (1) a vulnerability to adversarial examples and (2) a tendency to\nelude human interpretation. Interestingly, recent empirical and theoretical\nevidence suggests these two seemingly disparate issues are actually connected.\nIn particular, robust models tend to provide more interpretable gradients than\nnon-robust models. However, whether this relationship works in the opposite\ndirection remains obscure. With this paper, we seek empirical answers to the\nfollowing question: can models acquire adversarial robustness when they are\ntrained to have interpretable gradients? We introduce a theoretically inspired\ntechnique called Interpretation Regularization (IR), which encourages a model's\ngradients to (1) match the direction of interpretable target salience maps and\n(2) have small magnitude. To assess model performance and tease apart factors\nthat contribute to adversarial robustness, we conduct extensive experiments on\nMNIST and CIFAR-10 with both $\\ell_2$ and $\\ell_\\infty$ attacks. We demonstrate\nthat training the networks to have interpretable gradients improves their\nrobustness to adversarial perturbations. Applying the network interpretation\ntechnique SmoothGrad yields additional performance gains, especially in\ncross-norm attacks and under heavy perturbations. The results indicate that the\ninterpretability of the model gradients is a crucial factor for adversarial\nrobustness. Code for the experiments can be found at\nhttps://github.com/a1noack/interp_regularization.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 03:54:58 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 17:16:02 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 18:05:00 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 05:49:36 GMT"}, {"version": "v5", "created": "Tue, 10 Nov 2020 16:35:50 GMT"}, {"version": "v6", "created": "Fri, 4 Dec 2020 03:00:58 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Noack", "Adam", ""], ["Ahern", "Isaac", ""], ["Dou", "Dejing", ""], ["Li", "Boyang", ""]]}, {"id": "1912.03433", "submitter": "Aniket Pramanik", "authors": "Aniket Pramanik, Hemant Aggarwal and Mathews Jacob", "title": "Deep Generalization of Structured Low-Rank Algorithms (Deep-SLR)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured low-rank (SLR) algorithms, which exploit annihilation relations\nbetween the Fourier samples of a signal resulting from different properties, is\na powerful image reconstruction framework in several applications. This scheme\nrelies on low-rank matrix completion to estimate the annihilation relations\nfrom the measurements. The main challenge with this strategy is the high\ncomputational complexity of matrix completion. We introduce a deep learning\n(DL) approach to significantly reduce the computational complexity.\nSpecifically, we use a convolutional neural network (CNN)-based filterbank that\nis trained to estimate the annihilation relations from imperfect (under-sampled\nand noisy) k-space measurements of Magnetic Resonance Imaging (MRI). The main\nreason for the computational efficiency is the pre-learning of the parameters\nof the non-linear CNN from exemplar data, compared to SLR schemes that learn\nthe linear filterbank parameters from the dataset itself. Experimental\ncomparisons show that the proposed scheme can enable calibration-less parallel\nMRI; it can offer performance similar to SLR schemes while reducing the runtime\nby around three orders of magnitude. Unlike pre-calibrated and self-calibrated\napproaches, the proposed uncalibrated approach is insensitive to motion errors\nand affords higher acceleration. The proposed scheme also incorporates image\ndomain priors that are complementary, thus significantly improving the\nperformance over that of SLR schemes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 04:05:52 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:01:14 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 18:59:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Pramanik", "Aniket", ""], ["Aggarwal", "Hemant", ""], ["Jacob", "Mathews", ""]]}, {"id": "1912.03440", "submitter": "Yongshun Gong", "authors": "Yongshun Gong, Zhibin Li, Jian Zhang, Wei Liu and Jinfeng Yi", "title": "Potential Passenger Flow Prediction: A Novel Study for Urban\n  Transportation Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, practical applications for passenger flow prediction have brought\nmany benefits to urban transportation development. With the development of\nurbanization, a real-world demand from transportation managers is to construct\na new metro station in one city area that never planned before. Authorities are\ninterested in the picture of the future volume of commuters before constructing\na new station, and estimate how would it affect other areas. In this paper,\nthis specific problem is termed as potential passenger flow (PPF) prediction,\nwhich is a novel and important study connected with urban computing and\nintelligent transportation systems. For example, an accurate PPF predictor can\nprovide invaluable knowledge to designers, such as the advice of station scales\nand influences on other areas, etc. To address this problem, we propose a\nmulti-view localized correlation learning method. The core idea of our strategy\nis to learn the passenger flow correlations between the target areas and their\nlocalized areas with adaptive-weight. To improve the prediction accuracy, other\ndomain knowledge is involved via a multi-view learning process. We conduct\nintensive experiments to evaluate the effectiveness of our method with\nreal-world official transportation datasets. The results demonstrate that our\nmethod can achieve excellent performance compared with other available\nbaselines. Besides, our method can provide an effective solution to the\ncold-start problem in the recommender system as well, which proved by its\noutperformed experimental results.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:11:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gong", "Yongshun", ""], ["Li", "Zhibin", ""], ["Zhang", "Jian", ""], ["Liu", "Wei", ""], ["Yi", "Jinfeng", ""]]}, {"id": "1912.03452", "submitter": "Yingshi Chen", "authors": "Yingshi Chen, Jinfeng Zhu", "title": "A novel guided deep learning algorithm to design low-cost SPP films", "comments": "9 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.app-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of surface plasmon polaritons (SPP) films is an ill-posed inverse\nproblem. There are many-to-one correspondence between the structures and user\nneeds. We present a novel guided deep learning algorithm to find optimal\nsolutions (with both high accuracy and low cost). To achieve this goal, we use\nlow cost sample replacement algorithm in training process. The deep CNN would\ngradually learn better model from samples with lower cost. We have successfully\napplied this algorithm to the design of low-cost SPP films. Our model learned\nto replace precious metals with ordinary metals to reduce cost. So the the cost\nof predicted structure is much lower than standard deep CNN. And the average\nrelative error of spectrum is less than 10%. The source codes are available at\nhttps://github.com/closest-git/MetaLab.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 07:26:00 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 02:21:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Yingshi", ""], ["Zhu", "Jinfeng", ""]]}, {"id": "1912.03467", "submitter": "Mohamed Karim Belaid", "authors": "Mohamed Karim Belaid", "title": "Comparison of Neuronal Attention Models", "comments": null, "journal-ref": "Data Science Seminar, 2019, Uni Passau", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent models for image processing are using the Convolutional neural network\n(CNN) which requires a pixel per pixel analysis of the input image. This method\nworks well. However, it is time-consuming if we have large images. To increase\nthe performance, by improving the training time or the accuracy, we need a\nsize-independent method. As a solution, we can add a Neuronal Attention model\n(NAM). The power of this new approach is that it can efficiently choose several\nsmall regions from the initial image to focus on. The purpose of this paper is\nto explain and also test each of the NAM's parameters.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 09:00:18 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Belaid", "Mohamed Karim", ""]]}, {"id": "1912.03485", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Giri Narra, Zhifeng Lin, Yongqin Wang, Keshav Balasubramaniam,\n  Murali Annavaram", "title": "Privacy-Preserving Inference in Machine Learning Services Using Trusted\n  Execution Environments", "comments": "13 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Origami, which provides privacy-preserving inference for\nlarge deep neural network (DNN) models through a combination of enclave\nexecution, cryptographic blinding, interspersed with accelerator-based\ncomputation. Origami partitions the ML model into multiple partitions. The\nfirst partition receives the encrypted user input within an SGX enclave. The\nenclave decrypts the input and then applies cryptographic blinding to the input\ndata and the model parameters. Cryptographic blinding is a technique that adds\nnoise to obfuscate data. Origami sends the obfuscated data for computation to\nan untrusted GPU/CPU. The blinding and de-blinding factors are kept private by\nthe SGX enclave, thereby preventing any adversary from denoising the data, when\nthe computation is offloaded to a GPU/CPU. The computed output is returned to\nthe enclave, which decodes the computation on noisy data using the unblinding\nfactors privately stored within SGX. This process may be repeated for each DNN\nlayer, as has been done in prior work Slalom.\n  However, the overhead of blinding and unblinding the data is a limiting\nfactor to scalability. Origami relies on the empirical observation that the\nfeature maps after the first several layers can not be used, even by a powerful\nconditional GAN adversary to reconstruct input. Hence, Origami dynamically\nswitches to executing the rest of the DNN layers directly on an accelerator\nwithout needing any further cryptographic blinding intervention to preserve\nprivacy. We empirically demonstrate that using Origami, a conditional GAN\nadversary, even with an unlimited inference budget, cannot reconstruct the\ninput. We implement and demonstrate the performance gains of Origami using the\nVGG-16 and VGG-19 models. Compared to running the entire VGG-19 model within\nSGX, Origami inference improves the performance of private inference from 11x\nwhile using Slalom to 15.1x.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 10:27:33 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Narra", "Krishna Giri", ""], ["Lin", "Zhifeng", ""], ["Wang", "Yongqin", ""], ["Balasubramaniam", "Keshav", ""], ["Annavaram", "Murali", ""]]}, {"id": "1912.03488", "submitter": "Bhanu Garg Mr.", "authors": "Bhanu Garg and Naresh Manwani", "title": "Robust Deep Ordinal Regression Under Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-world data is often susceptible to label noise, which might\nconstrict the effectiveness of the existing state of the art algorithms for\nordinal regression. Existing works on ordinal regression do not take label\nnoise into account. We propose a theoretically grounded approach for class\nconditional label noise in ordinal regression problems. We present a deep\nlearning implementation of two commonly used loss functions for ordinal\nregression that is both - 1) robust to label noise, and 2) rank consistent for\na good ranking rule. We verify these properties of the algorithm empirically\nand show robustness to label noise on real data and rank consistency. To the\nbest of our knowledge, this is the first approach for robust ordinal regression\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 10:39:45 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:47:59 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Garg", "Bhanu", ""], ["Manwani", "Naresh", ""]]}, {"id": "1912.03500", "submitter": "V\\'it Musil", "authors": "Michal Rol\\'inek, V\\'it Musil, Anselm Paulus, Marin Vlastelica,\n  Claudio Michaelis, and Georg Martius", "title": "Optimizing Rank-based Metrics with Blackbox Differentiation", "comments": "CVPR 2020 conference paper (oral). The first two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank-based metrics are some of the most widely used criteria for performance\nevaluation of computer vision models. Despite years of effort, direct\noptimization for these metrics remains a challenge due to their\nnon-differentiable and non-decomposable nature. We present an efficient,\ntheoretically sound, and general method for differentiating rank-based metrics\nwith mini-batch gradient descent. In addition, we address optimization\ninstability and sparsity of the supervision signal that both arise from using\nrank-based metrics as optimization targets. Resulting losses based on recall\nand Average Precision are applied to image retrieval and object detection\ntasks. We obtain performance that is competitive with state-of-the-art on\nstandard image retrieval datasets and consistently improve performance of near\nstate-of-the-art object detectors. The code is available at\nhttps://github.com/martius-lab/blackbox-backprop\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 13:21:54 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:25:53 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Rol\u00ednek", "Michal", ""], ["Musil", "V\u00edt", ""], ["Paulus", "Anselm", ""], ["Vlastelica", "Marin", ""], ["Michaelis", "Claudio", ""], ["Martius", "Georg", ""]]}, {"id": "1912.03513", "submitter": "Warren Powell", "authors": "Warren B Powell", "title": "From Reinforcement Learning to Optimal Control: A unified framework for\n  sequential decisions", "comments": "47 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are over 15 distinct communities that work in the general area of\nsequential decisions and information, often referred to as decisions under\nuncertainty or stochastic optimization. We focus on two of the most important\nfields: stochastic optimal control, with its roots in deterministic optimal\ncontrol, and reinforcement learning, with its roots in Markov decision\nprocesses. Building on prior work, we describe a unified framework that covers\nall 15 different communities, and note the strong parallels with the modeling\nframework of stochastic optimal control. By contrast, we make the case that the\nmodeling framework of reinforcement learning, inherited from discrete Markov\ndecision processes, is quite limited. Our framework (and that of stochastic\ncontrol) is based on the core problem of optimizing over policies. We describe\nfour classes of policies that we claim are universal, and show that each of\nthese two fields have, in their own way, evolved to include examples of each of\nthese four classes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 14:50:37 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:20:17 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Powell", "Warren B", ""]]}, {"id": "1912.03517", "submitter": "Jean Tarbouriech", "authors": "Jean Tarbouriech, Evrard Garcelon, Michal Valko, Matteo Pirotta,\n  Alessandro Lazaric", "title": "No-Regret Exploration in Goal-Oriented Reinforcement Learning", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML 2020)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular reinforcement learning problems (e.g., navigation in a maze,\nsome Atari games, mountain car) are instances of the episodic setting under its\nstochastic shortest path (SSP) formulation, where an agent has to achieve a\ngoal state while minimizing the cumulative cost. Despite the popularity of this\nsetting, the exploration-exploitation dilemma has been sparsely studied in\ngeneral SSP problems, with most of the theoretical literature focusing on\ndifferent problems (i.e., fixed-horizon and infinite-horizon) or making the\nrestrictive loop-free SSP assumption (i.e., no state can be visited twice\nduring an episode). In this paper, we study the general SSP problem with no\nassumption on its dynamics (some policies may actually never reach the goal).\nWe introduce UC-SSP, the first no-regret algorithm in this setting, and prove a\nregret bound scaling as $\\displaystyle \\widetilde{\\mathcal{O}}( D S \\sqrt{ A D\nK})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions,\npositive costs and SSP-diameter $D$, defined as the smallest expected hitting\ntime from any starting state to the goal. We achieve this result by crafting a\nnovel stopping rule, such that UC-SSP may interrupt the current policy if it is\ntaking too long to achieve the goal and switch to alternative policies that are\ndesigned to rapidly terminate the episode.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 15:19:22 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 18:23:10 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 11:46:12 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Tarbouriech", "Jean", ""], ["Garcelon", "Evrard", ""], ["Valko", "Michal", ""], ["Pirotta", "Matteo", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "1912.03549", "submitter": "Juho Timonen", "authors": "Juho Timonen, Henrik Mannerstr\\\"om, Aki Vehtari and Harri\n  L\\\"ahdesm\\\"aki", "title": "lgpr: An interpretable nonparametric method for inferring covariate\n  effects from longitudinal data", "comments": "Contains main manuscript and supplementary material. Tables S1-S3 are\n  in ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal study designs are indispensable for studying disease\nprogression. Inferring covariate effects from longitudinal data, however,\nrequires interpretable methods that can model complicated covariance structures\nand detect nonlinear effects of both categorical and continuous covariates, as\nwell as their interactions. Detecting disease effects is hindered by the fact\nthat they often occur rapidly near the disease initiation time, and this time\npoint cannot be exactly observed. An additional challenge is that the effect\nmagnitude can be heterogeneous over the subjects. We present lgpr, a widely\napplicable and interpretable method for nonparametric analysis of longitudinal\ndata using additive Gaussian processes. We demonstrate that it outperforms\nprevious approaches in identifying the relevant categorical and continuous\ncovariates in various settings. Furthermore, it implements important novel\nfeatures, including the ability to account for the heterogeneity of covariate\neffects, their temporal uncertainty, and appropriate observation models for\ndifferent types of biomedical data. The lgpr tool is implemented as a\ncomprehensive and user-friendly R-package. lgpr is available at\njtimonen.github.io/lgpr-usage with documentation, tutorials, test data, and\ncode for reproducing the experiments of this paper.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 19:36:33 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 10:36:10 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Timonen", "Juho", ""], ["Mannerstr\u00f6m", "Henrik", ""], ["Vehtari", "Aki", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]]}, {"id": "1912.03558", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Igor Borovikov, Hongyuan Zha", "title": "Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill\n  Discovery", "comments": "Published at International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human players in professional team sports achieve high level coordination by\ndynamically choosing complementary skills and executing primitive actions to\nperform these skills. As a step toward creating intelligent agents with this\ncapability for fully cooperative multi-agent settings, we propose a two-level\nhierarchical multi-agent reinforcement learning (MARL) algorithm with\nunsupervised skill discovery. Agents learn useful and distinct skills at the\nlow level via independent Q-learning, while they learn to select complementary\nlatent skill variables at the high level via centralized multi-agent training\nwith an extrinsic team reward. The set of low-level skills emerges from an\nintrinsic reward that solely promotes the decodability of latent skill\nvariables from the trajectory of a low-level skill, without the need for\nhand-crafted rewards for each skill. For scalable decentralized execution, each\nagent independently chooses latent skill variables and primitive actions based\non local observations. Our overall method enables the use of general\ncooperative MARL algorithms for training high level policies and single-agent\nRL for training low level skills. Experiments on a stochastic high dimensional\nteam game show the emergence of useful skills and cooperative team play. The\ninterpretability of the learned skills show the promise of the proposed method\nfor achieving human-AI cooperation in team sports games.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 20:41:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:30:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 03:00:47 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yang", "Jiachen", ""], ["Borovikov", "Igor", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1912.03573", "submitter": "Lixiang Zhang", "authors": "Lixiang Zhang, Lin Lin, Jia Li", "title": "Deep Variable-Block Chain with Adaptive Variable Selection", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architectures of deep neural networks (DNN) rely heavily on the\nunderlying grid structure of variables, for instance, the lattice of pixels in\nan image. For general high dimensional data with variables not associated with\na grid, the multi-layer perceptron and deep brief network are often used.\nHowever, it is frequently observed that those networks do not perform\ncompetitively and they are not helpful for identifying important variables. In\nthis paper, we propose a framework that imposes on blocks of variables a chain\nstructure obtained by step-wise greedy search so that the DNN architecture can\nleverage the constructed grid. We call this new neural network Deep\nVariable-Block Chain (DVC). Because the variable blocks are used for\nclassification in a sequential manner, we further develop the capacity of\nselecting variables adaptively according to a number of regions trained by a\ndecision tree. Our experiments show that DVC outperforms other generic DNNs and\nother strong classifiers. Moreover, DVC can achieve high accuracy at much\nreduced dimensionality and sometimes reveals drastically different sets of\nrelevant variables for different regions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 23:02:02 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Lixiang", ""], ["Lin", "Lin", ""], ["Li", "Jia", ""]]}, {"id": "1912.03579", "submitter": "Ricky T. Q. Chen", "authors": "Ricky T. Q. Chen and David Duvenaud", "title": "Neural Networks with Cheap Differential Operators", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradients of neural networks can be computed efficiently for any\narchitecture, but some applications require differential operators with higher\ntime complexity. We describe a family of restricted neural network\narchitectures that allow efficient computation of a family of differential\noperators involving dimension-wise derivatives, used in cases such as computing\nthe divergence. Our proposed architecture has a Jacobian matrix composed of\ndiagonal and hollow (non-diagonal) components. We can then modify the backward\ncomputation graph to extract dimension-wise derivatives efficiently with\nautomatic differentiation. We demonstrate these cheap differential operators\nfor solving root-finding subproblems in implicit ODE solvers, exact density\nevaluation for continuous normalizing flows, and evaluating the Fokker--Planck\nequation for training stochastic differential equation models.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 00:08:50 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Ricky T. Q.", ""], ["Duvenaud", "David", ""]]}, {"id": "1912.03582", "submitter": "Vatsal Sharan", "authors": "Parikshit Gopalan, Vatsal Sharan, Udi Wieder", "title": "PIDForest: Anomaly Detection via Partial Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting anomalies in a large dataset. We propose\na framework called Partial Identification which captures the intuition that\nanomalies are easy to distinguish from the overwhelming majority of points by\nrelatively few attribute values. Formalizing this intuition, we propose a\ngeometric anomaly measure for a point that we call PIDScore, which measures the\nminimum density of data points over all subcubes containing the point. We\npresent PIDForest: a random forest based algorithm that finds anomalies based\non this definition. We show that it performs favorably in comparison to several\npopular anomaly detection methods, across a broad range of benchmarks.\nPIDForest also provides a succinct explanation for why a point is labelled\nanomalous, by providing a set of features and ranges for them which are\nrelatively uncommon in the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 00:43:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Sharan", "Vatsal", ""], ["Wieder", "Udi", ""]]}, {"id": "1912.03589", "submitter": "Xiao Lu", "authors": "Guangxia Lia, Yulong Shena, Peilin Zhaob, Xiao Lu, Jia Liu, Yangyang\n  Liu, Steven C. H. Hoi", "title": "Detecting Cyberattacks in Industrial Control Systems Using Online\n  Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial control systems are critical to the operation of industrial\nfacilities, especially for critical infrastructures, such as refineries, power\ngrids, and transportation systems. Similar to other information systems, a\nsignificant threat to industrial control systems is the attack from\ncyberspace---the offensive maneuvers launched by \"anonymous\" in the digital\nworld that target computer-based assets with the goal of compromising a\nsystem's functions or probing for information. Owing to the importance of\nindustrial control systems, and the possibly devastating consequences of being\nattacked, significant endeavors have been attempted to secure industrial\ncontrol systems from cyberattacks. Among them are intrusion detection systems\nthat serve as the first line of defense by monitoring and reporting potentially\nmalicious activities. Classical machine-learning-based intrusion detection\nmethods usually generate prediction models by learning modest-sized training\nsamples all at once. Such approach is not always applicable to industrial\ncontrol systems, as industrial control systems must process continuous control\ncommands with limited computational resources in a nonstop way. To satisfy such\nrequirements, we propose using online learning to learn prediction models from\nthe controlling data stream. We introduce several state-of-the-art online\nlearning algorithms categorically, and illustrate their efficacies on two\ntypically used testbeds---power system and gas pipeline. Further, we explore a\nnew cost-sensitive online learning algorithm to solve the class-imbalance\nproblem that is pervasive in industrial intrusion detection systems. Our\nexperimental results indicate that the proposed algorithm can achieve an\noverall improvement in the detection rate of cyberattacks in industrial control\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:29:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lia", "Guangxia", ""], ["Shena", "Yulong", ""], ["Zhaob", "Peilin", ""], ["Lu", "Xiao", ""], ["Liu", "Jia", ""], ["Liu", "Yangyang", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1912.03606", "submitter": "John Zech", "authors": "John R. Zech, Jessica Zosa Forde, Michael L. Littman", "title": "Individual predictions matter: Assessing the effect of data ordering in\n  training fine-tuned CNNs for medical imaging", "comments": "J.Z. and J.F. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reproduced the results of CheXNet with fixed hyperparameters and 50\ndifferent random seeds to identify 14 finding in chest radiographs (x-rays).\nBecause CheXNet fine-tunes a pre-trained DenseNet, the random seed affects the\nordering of the batches of training data but not the initialized model weights.\nWe found substantial variability in predictions for the same radiograph across\nmodel runs (mean ln[(maximum probability)/(minimum probability)] 2.45,\ncoefficient of variation 0.543). This individual radiograph-level variability\nwas not fully reflected in the variability of AUC on a large test set.\nAveraging predictions from 10 models reduced variability by nearly 70% (mean\ncoefficient of variation from 0.543 to 0.169, t-test 15.96, p-value < 0.0001).\nWe encourage researchers to be aware of the potential variability of CNNs and\nensemble predictions from multiple models to minimize the effect this\nvariability may have on the care of individual patients when these models are\ndeployed clinically.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 03:18:18 GMT"}], "update_date": "2019-12-28", "authors_parsed": [["Zech", "John R.", ""], ["Forde", "Jessica Zosa", ""], ["Littman", "Michael L.", ""]]}, {"id": "1912.03609", "submitter": "Yi Xiang Marcus Tan", "authors": "Yi Xiang Marcus Tan, Yuval Elovici, Alexander Binder", "title": "Exploring the Back Alleys: Analysing The Robustness of Alternative\n  Neural Network Architectures against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate to what extent alternative variants of Artificial Neural\nNetworks (ANNs) are susceptible to adversarial attacks. We analyse the\nadversarial robustness of conventional, stochastic ANNs and Spiking Neural\nNetworks (SNNs) in the raw image space, across three different datasets. Our\nexperiments reveal that stochastic ANN variants are almost equally as\nsusceptible as conventional ANNs when faced with simple iterative\ngradient-based attacks in the white-box setting. However we observe, that in\nblack-box settings, stochastic ANNs are more robust than conventional ANNs,\nwhen faced with boundary attacks, transferability and surrogate attacks.\nConsequently, we propose improved attacks and defence mechanisms for stochastic\nANNs in black-box settings. When performing surrogate-based black-box attacks,\none can employ stochastic models as surrogates to observe higher attack success\non both stochastic and deterministic targets. This success can be further\nimproved with our proposed Variance Mimicking (VM) surrogate training method,\nagainst stochastic targets. Finally, adopting a defender's perspective, we\ninvestigate the plausibility of employing stochastic switching of model\nmixtures as a viable hardening mechanism. We observe that such a scheme does\nprovide a partial hardening.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 03:47:06 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 08:15:59 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 06:41:17 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Tan", "Yi Xiang Marcus", ""], ["Elovici", "Yuval", ""], ["Binder", "Alexander", ""]]}, {"id": "1912.03618", "submitter": "Aman Sinha", "authors": "Justin Norden, Matthew O'Kelly, Aman Sinha", "title": "Efficient Black-box Assessment of Autonomous Vehicle Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While autonomous vehicle (AV) technology has shown substantial progress, we\nstill lack tools for rigorous and scalable testing. Real-world testing, the\n$\\textit{de-facto}$ evaluation method, is dangerous to the public. Moreover,\ndue to the rare nature of failures, billions of miles of driving are needed to\nstatistically validate performance claims. Thus, the industry has largely\nturned to simulation to evaluate AV systems. However, having a simulation stack\nalone is not a solution. A simulation testing framework needs to prioritize\nwhich scenarios to run, learn how the chosen scenarios provide coverage of\nfailure modes, and rank failure scenarios in order of importance. We implement\na simulation testing framework that evaluates an entire modern AV system as a\nblack box. This framework estimates the probability of accidents under a base\ndistribution governing standard traffic behavior. In order to accelerate\nrare-event probability evaluation, we efficiently learn to identify and rank\nfailure scenarios via adaptive importance-sampling methods. Using this\nframework, we conduct the first independent evaluation of a full-stack\ncommercial AV system, Comma AI's OpenPilot.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 05:12:17 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 03:56:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Norden", "Justin", ""], ["O'Kelly", "Matthew", ""], ["Sinha", "Aman", ""]]}, {"id": "1912.03624", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Sunabha Chatterjee, Piyush Rai", "title": "Bayesian Structure Adaptation for Continual Learning", "comments": "17 pages, 16 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual Learning is a learning paradigm where learning systems are trained\nwith sequential or streaming tasks. Two notable directions among the recent\nadvances in continual learning with neural networks are ($i$) variational Bayes\nbased regularization by learning priors from previous tasks, and, ($ii$)\nlearning the structure of deep networks to adapt to new tasks. So far, these\ntwo approaches have been orthogonal. We present a novel Bayesian approach to\ncontinual learning based on learning the structure of deep neural networks,\naddressing the shortcomings of both these approaches. The proposed model learns\nthe deep structure for each task by learning which weights to be used, and\nsupports inter-task transfer through the overlapping of different sparse\nsubsets of weights learned by different tasks. Experimental results on\nsupervised and unsupervised benchmarks shows that our model performs comparably\nor better than recent advances in continual learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 06:40:44 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:34:34 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Kumar", "Abhishek", ""], ["Chatterjee", "Sunabha", ""], ["Rai", "Piyush", ""]]}, {"id": "1912.03652", "submitter": "Johannes Schneider", "authors": "Johannes Schneider", "title": "Human-to-AI Coach: Improving Human Inputs to AI Systems", "comments": null, "journal-ref": "Symposium on Intelligent Data Analysis 2020, Konstanz", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans increasingly interact with Artificial intelligence(AI) systems. AI\nsystems are optimized for objectives such as minimum computation or minimum\nerror rate in recognizing and interpreting inputs from humans. In contrast,\ninputs created by humans are often treated as a given. We investigate how\ninputs of humans can be altered to reduce misinterpretation by the AI system\nand to improve efficiency of input generation for the human while altered\ninputs should remain as similar as possible to the original inputs. These\nobjectives result in trade-offs that are analyzed for a deep learning system\nclassifying handwritten digits. To create examples that serve as demonstrations\nfor humans to improve, we develop a model based on a conditional convolutional\nautoencoder (CCAE). Our quantitative and qualitative evaluation shows that in\nmany occasions the generated proposals lead to lower error rates, require less\neffort to create and differ only modestly from the original samples.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 10:43:43 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 21:33:52 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Schneider", "Johannes", ""]]}, {"id": "1912.03662", "submitter": "Duyeol Lee", "authors": "Duyeol Lee, Kai Zhang, and Michael R. Kosorok", "title": "The Binary Expansion Randomized Ensemble Test (BERET)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the binary expansion testing framework was introduced to test the\nindependence of two continuous random variables by utilizing symmetry\nstatistics that are complete sufficient statistics for dependence. We develop a\nnew test based on an ensemble approach that uses the sum of squared symmetry\nstatistics and distance correlation. Simulation studies suggest that this\nmethod improves the power while preserving the clear interpretation of the\nbinary expansion testing. We extend this method to tests of independence of\nrandom vectors in arbitrary dimension. Through random projections, the proposed\nbinary expansion randomized ensemble test transforms the multivariate\nindependence testing problem into a univariate problem. Simulation studies and\ndata example analyses show that the proposed method provides relatively robust\nperformance compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:54:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:25:20 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 23:37:36 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 20:08:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Duyeol", ""], ["Zhang", "Kai", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.03668", "submitter": "Haihui Pan", "authors": "Zhifang Liao, Haihui Pan, Qi Zeng, Xiaoping Fan, Yan Zhang, Song Yu", "title": "Short-term Load Forecasting with Dense Average Network", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important part of the power system, power load forecasting directly\naffects the national economy. The data shows that improving the load\nforecasting accuracy by 0.01% can save millions of dollars for the power\nindustry. Therefore, improving the accuracy of power load forecasting has\nalways been the pursuing goals for a power system. Based on this goal, this\npaper proposes a novel connection, the dense average connection, in which the\noutputs of all preceding layers are averaged as the input of the next layer in\na feed-forward fashion. Based on dense average connection , we construct the\ndense average network for power load forecasting. The predictions of the\nproposed model for two public datasets are better than those of existing\nmethods. On this basis, we use the ensemble method to further improve the\naccuracy of the model. To verify the reliability of the model predictions, the\nrobustness is analyzed and verified by adding input disturbances. The\nexperimental results show that the proposed model is effective and robust for\npower load forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 12:19:07 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 11:14:22 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 03:51:18 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liao", "Zhifang", ""], ["Pan", "Haihui", ""], ["Zeng", "Qi", ""], ["Fan", "Xiaoping", ""], ["Zhang", "Yan", ""], ["Yu", "Song", ""]]}, {"id": "1912.03673", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Kira Maag, Robin Chan, Fabian H\\\"uger, Peter\n  Schlicht, Hanno Gottschalk", "title": "Detection of False Positive and False Negative Samples in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods have outperformed other methods in\nimage recognition. This has fostered imagination of potential application of\ndeep learning technology including safety relevant applications like the\ninterpretation of medical images or autonomous driving. The passage from\nassistance of a human decision maker to ever more automated systems however\nincreases the need to properly handle the failure modes of deep learning\nmodules. In this contribution, we review a set of techniques for the\nself-monitoring of machine-learning algorithms based on uncertainty\nquantification. In particular, we apply this to the task of semantic\nsegmentation, where the machine learning algorithm decomposes an image\naccording to semantic categories. We discuss false positive and false negative\nerror modes at instance-level and review techniques for the detection of such\nerrors that have been recently proposed by the authors. We also give an outlook\non future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 13:04:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Rottmann", "Matthias", ""], ["Maag", "Kira", ""], ["Chan", "Robin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1912.03699", "submitter": "Ying Jin", "authors": "Ying Jin, Ximei Wang, Mingsheng Long, Jianmin Wang", "title": "Minimum Class Confusion for Versatile Domain Adaptation", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a variety of Domain Adaptation (DA) scenarios subject to label sets\nand domain configurations, including closed-set and partial-set DA, as well as\nmulti-source and multi-target DA. It is notable that existing DA methods are\ngenerally designed only for a specific scenario, and may underperform for\nscenarios they are not tailored to. To this end, this paper studies Versatile\nDomain Adaptation (VDA), where one method can handle several different DA\nscenarios without any modification. Towards this goal, a more general inductive\nbias other than the domain alignment should be explored. We delve into a\nmissing piece of existing methods: class confusion, the tendency that a\nclassifier confuses the predictions between the correct and ambiguous classes\nfor target examples, which is common in different DA scenarios. We uncover that\nreducing such pairwise class confusion leads to significant transfer gains.\nWith this insight, we propose a general loss function: Minimum Class Confusion\n(MCC). It can be characterized as (1) a non-adversarial DA method without\nexplicitly deploying domain alignment, enjoying faster convergence speed; (2) a\nversatile approach that can handle four existing scenarios: Closed-Set,\nPartial-Set, Multi-Source, and Multi-Target DA, outperforming the\nstate-of-the-art methods in these scenarios, especially on one of the largest\nand hardest datasets to date (7.3% on DomainNet). Its versatility is further\njustified by two scenarios proposed in this paper: Multi-Source Partial DA and\nMulti-Target Partial DA. In addition, it can also be used as a general\nregularizer that is orthogonal and complementary to a variety of existing DA\nmethods, accelerating convergence and pushing these readily competitive methods\nto stronger ones. Code is available at\nhttps://github.com/thuml/Versatile-Domain-Adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:31:14 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 12:41:22 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 15:49:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jin", "Ying", ""], ["Wang", "Ximei", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""]]}, {"id": "1912.03700", "submitter": "Dibyendu Das", "authors": "Dibyendu Das, Shahid Asghar Ahmad, Kumar Venkataramanan", "title": "Deep Learning-based Hybrid Graph-Coloring Algorithm for Register\n  Allocation", "comments": "11 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Register allocation, which is a crucial phase of a good optimizing compiler,\nrelies on graph coloring. Hence, an efficient graph coloring algorithm is of\nparamount importance. In this work we try to learn a good heuristic for\ncoloring interference graphs that are used in the register allocation phase. We\naim to handle moderate sized interference graphs which have 100 nodes or less.\nFor such graphs we can get the optimal allocation of colors to the nodes. Such\noptimal coloring is then used to train our Deep Learning network which is based\non several layers of LSTM that output a color for each node of the graph.\nHowever, the current network may allocate the same color to the nodes connected\nby an edge resulting in an invalid coloring of the interference graph. Since it\nis difficult to encode constraints in an LSTM to avoid invalid coloring, we\naugment our deep learning network with a color correction phase that runs after\nthe colors have been allocated by the network. Thus, our algorithm is hybrid in\nnature consisting of a mix of a deep learning algorithm followed by a more\ntraditional correction phase. We have trained our network using several\nthousand random graphs of varying sparsity. On application of our hybrid\nalgorithm to various popular graphs found in literature we see that our\nalgorithm does very well when compared to the optimal coloring of these graphs.\nWe have also run our algorithm against LLVMs popular greedy register allocator\nfor several SPEC CPU 2017 benchmarks and notice that the hybrid algorithm\nperforms on par or better than such a well-tuned allocator for most of these\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:36:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Das", "Dibyendu", ""], ["Ahmad", "Shahid Asghar", ""], ["Venkataramanan", "Kumar", ""]]}, {"id": "1912.03702", "submitter": "Yi Zhong", "authors": "Yi Zhong, Xueyu Chen, Yu Zhao, Xiaoming Chen, Tingfang Gao, Zuquan\n  Weng", "title": "Graph-augmented Convolutional Networks on Drug-Drug Interactions\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end model to predict drug-drug interactions (DDIs) by\nemploying graph-augmented convolutional networks. And this is implemented by\ncombining graph CNN with an attentive pooling network to extract structural\nrelations between drug pairs and make DDI predictions. The experiment results\nsuggest a desirable performance achieving ROC at 0.988, F1-score at 0.956, and\nAUPR at 0.986. Besides, the model can tell how the two DDI drugs interact\nstructurally by varying colored atoms. And this may be helpful for drug design\nduring drug discovery.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:43:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhong", "Yi", ""], ["Chen", "Xueyu", ""], ["Zhao", "Yu", ""], ["Chen", "Xiaoming", ""], ["Gao", "Tingfang", ""], ["Weng", "Zuquan", ""]]}, {"id": "1912.03703", "submitter": "Bhagya Hettige", "authors": "Bhagya Hettige, Yuan-Fang Li, Weiqing Wang, Suong Le and Wray Buntine", "title": "$\\mathtt{MedGraph:}$ Structural and Temporal Representation Learning of\n  Electronic Medical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic medical record (EMR) data contains historical sequences of visits\nof patients, and each visit contains rich information, such as patient\ndemographics, hospital utilisation and medical codes, including diagnosis,\nprocedure and medication codes. Most existing EMR embedding methods capture\nvisit-code associations by constructing input visit representations as binary\nvectors with a static vocabulary of medical codes. With this limited\nrepresentation, they fail in encapsulating rich attribute information of visits\n(demographics and utilisation information) and/or codes (e.g., medical code\ndescriptions). Furthermore, current work considers visits of the same patient\nas discrete-time events and ignores time gaps between them. However, the time\ngaps between visits depict dynamics of the patient's medical history inducing\nvarying influences on future visits. To address these limitations, we present\n$\\mathtt{MedGraph}$, a supervised EMR embedding method that captures two types\nof information: (1) the visit-code associations in an attributed bipartite\ngraph, and (2) the temporal sequencing of visits through a point process.\n$\\mathtt{MedGraph}$ produces Gaussian embeddings for visits and codes to model\nthe uncertainty. We evaluate the performance of $\\mathtt{MedGraph}$ through an\nextensive experimental study and show that $\\mathtt{MedGraph}$ outperforms\nstate-of-the-art EMR embedding methods in several medical risk prediction\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:43:52 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 01:15:43 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 02:06:31 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Hettige", "Bhagya", ""], ["Li", "Yuan-Fang", ""], ["Wang", "Weiqing", ""], ["Le", "Suong", ""], ["Buntine", "Wray", ""]]}, {"id": "1912.03758", "submitter": "Catharina Elisabeth Graafland", "authors": "Catharina Graafland, Jos\\'e M. Guti\\'errez, Juan M. L\\'opez, Diego\n  Paz\\'o, Miguel A. Rodr\\'iguez", "title": "The Probabilistic Backbone of Data-Driven Complex Networks: An example\n  in Climate", "comments": null, "journal-ref": "Sci Rep 10, 11484 (2020)", "doi": "10.1038/s41598-020-67970-y", "report-no": null, "categories": "physics.data-an physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Networks (CNs) inherently suffer from redundant information in\ntheir network topology. Bayesian Networks (BNs), on the other hand, include\nonly non-redundant information (from a probabilistic perspective) resulting in\na sparse topology from which generalizable physical features can be extracted.\nWe advocate the use of BNs to construct data-driven complex networks as they\ncan be regarded as the probabilistic backbone of the underlying complex system.\nResults are illustrated at the hand of a global climate dataset.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 20:54:08 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:11:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Graafland", "Catharina", ""], ["Guti\u00e9rrez", "Jos\u00e9 M.", ""], ["L\u00f3pez", "Juan M.", ""], ["Paz\u00f3", "Diego", ""], ["Rodr\u00edguez", "Miguel A.", ""]]}, {"id": "1912.03760", "submitter": "Radu Tudor Ionescu", "authors": "Cezara Benegui, Radu Tudor Ionescu", "title": "Convolutional Neural Networks for User Identificationbased on Motion\n  Sensors Represented as Image", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a deep learning approach for smartphone user\nidentification based on analyzing motion signals recorded by the accelerometer\nand the gyroscope, during a single tap gesture performed by the user on the\nscreen. We transform the discrete 3-axis signals from the motion sensors into a\ngray-scale image representation which is provided as input to a convolutional\nneural network (CNN) that is pre-trained for multi-class user classification.\nIn the pre-training stage, we benefit from different users and multiple samples\nper user. After pre-training, we use our CNN as feature extractor, generating\nan embedding associated to each single tap on the screen. The resulting\nembeddings are used to train a Support Vector Machines (SVM) model in a\nfew-shot user identification setting, i.e. requiring only 20 taps on the screen\nduring the registration phase. We compare our identification system based on\nCNN features with two baseline systems, one that employs handcrafted features\nand another that employs recurrent neural network (RNN) features. All systems\nare based on the same classifier, namely SVM. To pre-train the CNN and the RNN\nmodels for multi-class user classification, we use a different set of users\nthan the set used for few-shot user identification, ensuring a realistic\nscenario. The empirical results demonstrate that our CNN model yields a top\naccuracy of 89.75% in multi-class user classification and a top accuracy of\n96.72% in few-shot user identification. In conclusion, we believe that our\nsystem is ready for practical use, having a better generalization capacity than\nboth baselines.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 21:04:43 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 14:05:12 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Benegui", "Cezara", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1912.03761", "submitter": "Weicheng Zhu", "authors": "Weicheng Zhu, Narges Razavian", "title": "Variationally Regularized Graph-based Representation Learning for\n  Electronic Health Records", "comments": null, "journal-ref": null, "doi": "10.1145/3450439.3451855", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records (EHR) are high-dimensional data with implicit\nconnections among thousands of medical concepts. These connections, for\ninstance, the co-occurrence of diseases and lab-disease correlations can be\ninformative when only a subset of these variables is documented by the\nclinician. A feasible approach to improving the representation learning of EHR\ndata is to associate relevant medical concepts and utilize these connections.\nExisting medical ontologies can be the reference for EHR structures, but they\nplace numerous constraints on the data source. Recent progress on graph neural\nnetworks (GNN) enables end-to-end learning of topological structures for\nnon-grid or non-sequential data. However, there are problems to be addressed on\nhow to learn the medical graph adaptively and how to understand the effect of\nthe medical graph on representation learning. In this paper, we propose a\nvariationally regularized encoder-decoder graph network that achieves more\nrobustness in graph structure learning by regularizing node representations.\nOur model outperforms the existing graph and non-graph based methods in various\nEHR predictive tasks based on both public data and real-world clinical data.\nBesides the improvements in empirical experiment performances, we provide an\ninterpretation of the effect of variational regularization compared to standard\ngraph neural network, using singular value analysis.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 21:06:38 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 04:11:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhu", "Weicheng", ""], ["Razavian", "Narges", ""]]}, {"id": "1912.03769", "submitter": "Arunkumar Bagavathi", "authors": "Arunkumar Bagavathi, Siddharth Krishnan, Sanjay Subrahmanyan, and S.\n  L. Narasimhan", "title": "ragamAI: A Network Based Recommender System to Arrange a Indian\n  Classical Music Concert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  South Indian classical music (Carnatic music) is best consumed through live\nconcerts. A carnatic recital requires meticulous planning accounting for\nseveral parameters like the performers' repertoire, composition variety,\nmusical versatility, thematic structure, the recital's arrangement, etc. to\nensure that the audience have a comprehensive listening experience. In this\nwork, we present ragamAI a novel machine learning framework that utilizes the\ntonic nuances and musical structures in the carnatic music to generate a\nconcert recital that melodically captures the entire range in an octave.\nUtilizing the underlying idea of playlist and session-based recommender models,\nthe proposed model studies the mathematical structure present in past concerts\nand recommends relevant items for the playlist/concert. ragamAI ensembles\nrecommendations given by multiple models to learn user idea and past preference\nof sequences in concerts to extract recommendations. Our experiments on a vast\ncollection of concert show that our model performs 25%-50% better than baseline\nmodels. ragamAI's applications are two-fold. 1) it will assist musicians to\ncustomize their performance with the necessary variety required to sustain the\ninterest of the audience for the entirety of the concert 2) it will generate\ncarefully curated lists of south Indian classical music so that the listener\ncan discover the wide range of melody that the musical system can offer.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 21:39:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Bagavathi", "Arunkumar", ""], ["Krishnan", "Siddharth", ""], ["Subrahmanyan", "Sanjay", ""], ["Narasimhan", "S. L.", ""]]}, {"id": "1912.03771", "submitter": "Anton Osokin", "authors": "Irina Saparina and Anton Osokin", "title": "Cost-Sensitive Training for Autoregressive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training autoregressive models to better predict under the test metric,\ninstead of maximizing the likelihood, has been reported to be beneficial in\nseveral use cases but brings additional complications, which prevent wider\nadoption. In this paper, we follow the learning-to-search approach (Daum\\'e III\net al., 2009; Leblond et al., 2018) and investigate its several components.\nFirst, we propose a way to construct a reference policy based on an alignment\nbetween the model output and ground truth. Our reference policy is optimal when\napplied to the Kendall-tau distance between permutations (appear in the task of\nword ordering) and helps when working with the METEOR score for machine\ntranslation. Second, we observe that the learning-to-search approach benefits\nfrom choosing the costs related to the test metrics. Finally, we study the\neffect of different learning objectives and find that the standard KL loss only\nlearns several high-probability tokens and can be replaced with ranking\nobjectives that target these tokens explicitly.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 21:57:56 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Saparina", "Irina", ""], ["Osokin", "Anton", ""]]}, {"id": "1912.03781", "submitter": "Pierfrancesco Alaimo Di Loro", "authors": "Giovanna Tagliaferri, Daria Scacciatelli, Pierfrancesco Alaimo Di Loro", "title": "VAT tax gap prediction: a 2-steps Gradient Boosting approach", "comments": "27 pages, 4 figures, 8 tables Presented at NTTS 2019 conference Under\n  review at another peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tax evasion is the illegal evasion of taxes by individuals, corporations, and\ntrusts. The revenue loss from tax avoidance can undermine the effectiveness and\nequity of the government policies. A standard measure of tax evasion is the tax\ngap, that can be estimated as the difference between the total amounts of tax\ntheoretically collectable and the total amounts of tax actually collected in a\ngiven period. This paper presents an original contribution to bottom-up\napproach, based on results from fiscal audits, through the use of Machine\nLearning. The major disadvantage of bottom-up approaches is represented by\nselection bias when audited taxpayers are not randomly selected, as in the case\nof audits performed by the Italian Revenue Agency. Our proposal, based on a\n2-steps Gradient Boosting model, produces a robust tax gap estimate and, embeds\na solution to correct for the selection bias which do not require any\nassumptions on the underlying data distribution. The 2-steps Gradient Boosting\napproach is used to estimate the Italian Value-added tax (VAT) gap on\nindividual firms on the basis of fiscal and administrative data income tax\nreturns gathered from Tax Administration Data Base, for the fiscal year 2011.\nThe proposed method significantly boost the performance in predicting with\nrespect to the classical parametric approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:16:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 21:29:07 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 23:06:27 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tagliaferri", "Giovanna", ""], ["Scacciatelli", "Daria", ""], ["Di Loro", "Pierfrancesco Alaimo", ""]]}, {"id": "1912.03784", "submitter": "Tamara Fernandez", "authors": "Tamara Fernandez, Arthur Gretton, David Rindt, Dino Sejdinovic", "title": "A kernel log-rank test of independence for right-censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general non-parametric independence test between\nright-censored survival times and covariates, which may be multivariate. Our\ntest statistic has a dual interpretation, first in terms of the supremum of a\npotentially infinite collection of weight-indexed log-rank tests, with weight\nfunctions belonging to a reproducing kernel Hilbert space (RKHS) of functions;\nand second, as the norm of the difference of embeddings of certain finite\nmeasures into the RKHS, similar to the Hilbert-Schmidt Independence Criterion\n(HSIC) test-statistic. We study the asymptotic properties of the test, finding\nsufficient conditions to ensure our test correctly rejects the null hypothesis\nunder any alternative. The test statistic can be computed straightforwardly,\nand the rejection threshold is obtained via an asymptotically consistent Wild\nBootstrap procedure. Extensive simulations demonstrate that our testing\nprocedure generally performs better than competing approaches in detecting\ncomplex non-linear dependence.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:29:40 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 16:23:57 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Fernandez", "Tamara", ""], ["Gretton", "Arthur", ""], ["Rindt", "David", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1912.03785", "submitter": "Jerome Friedman", "authors": "Jerome H. Friedman", "title": "Contrast Trees and Distribution Boosting", "comments": "18 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often machine learning methods are applied and results reported in cases\nwhere there is little to no information concerning accuracy of the output.\nSimply because a computer program returns a result does not insure its\nvalidity. If decisions are to be made based on such results it is important to\nhave some notion of their veracity. Contrast trees represent a new approach for\nassessing the accuracy of many types of machine learning estimates that are not\namenable to standard (cross) validation methods. In situations where\ninaccuracies are detected boosted contrast trees can often improve performance.\nA special case, distribution boosting, provides an assumption free method for\nestimating the full probability distribution of an outcome variable given any\nset of joint input predictor variable values.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:30:25 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Friedman", "Jerome H.", ""]]}, {"id": "1912.03787", "submitter": "Austin Dill", "authors": "Austin Dill, Chun-Liang Li, Songwei Ge, Eunsu Kang", "title": "Getting Topology and Point Cloud Generation to Mesh", "comments": "Sets & Partitions Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we explore the idea that effective generative models for point\nclouds under the autoencoding framework must acknowledge the relationship\nbetween a continuous surface, a discretized mesh, and a set of points sampled\nfrom the surface. This view motivates a generative model that works by\nprogressively deforming a uniform sphere until it approximates the goal point\ncloud. We review the underlying concepts leading to this conclusion from\ncomputer graphics and topology in differential geometry, and model the\ngeneration process as deformation via deep neural network parameterization.\nFinally, we show that this view of the problem produces a model that can\ngenerate quality meshes efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:36:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Dill", "Austin", ""], ["Li", "Chun-Liang", ""], ["Ge", "Songwei", ""], ["Kang", "Eunsu", ""]]}, {"id": "1912.03789", "submitter": "Rishu Garg", "authors": "Saumil Maheshwari, Rohit Verma, Anupam Shukla, Ritu Tiwari and Rishu\n  Garg", "title": "Feature Engineering Combined with 1 D Convolutional Neural Network for\n  Improved Mortality Prediction", "comments": "Being a short term project, this paper is not exhaustive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensive care units (ICUs) are responsible for generating a wealth of\nuseful data in the form of Electronic Health Record (EHR). This data allows for\nthe development of a prediction tool with perfect knowledge backing. We aimed\nto build a mortality prediction model on 2012 Physionet Challenge mortality\nprediction database of 4000 patients admitted in ICU. The challenges in the\ndataset, such as high dimensionality, imbalanced distribution, and missing\nvalues were tackled with analytical methods and tools via feature engineering\nand new variable construction. The objective of the research is to utilize the\nrelations among the clinical variables and construct new variables which would\nestablish the effectiveness of 1-Dimensional Convolutional Neural Network (1- D\nCNN) with constructed features. Its performance with the traditional machine\nlearning algorithms like XGBoost classifier, Support Vector Machine (SVM),\nK-Neighbours Classifier (K-NN), and Random Forest Classifier (RF) is compared\nfor Area Under Curve (AUC). The investigation reveals the best AUC of 0.848\nusing 1-D CNN model.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:52:58 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 10:27:23 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Maheshwari", "Saumil", ""], ["Verma", "Rohit", ""], ["Shukla", "Anupam", ""], ["Tiwari", "Ritu", ""], ["Garg", "Rishu", ""]]}, {"id": "1912.03798", "submitter": "Rishu Garg", "authors": "Rishu Garg, Saumil Maheshwari, Anupam Shukla", "title": "Decision Support System for Detection and Classification of Skin Cancer\n  using CNN", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin Cancer is one of the most deathful of all the cancers. It is bound to\nspread to different parts of the body on the off chance that it is not analyzed\nand treated at the beginning time. It is mostly because of the abnormal growth\nof skin cells, often develops when the body is exposed to sunlight. The\nDetection Furthermore, the characterization of skin malignant growth in the\nbeginning time is a costly and challenging procedure. It is classified where it\ndevelops and its cell type. High Precision and recall are required for the\nclassification of lesions. The paper aims to use MNIST HAM-10000 dataset\ncontaining dermoscopy images. The objective is to propose a system that detects\nskin cancer and classifies it in different classes by using the Convolution\nNeural Network. The diagnosing methodology uses Image processing and deep\nlearning model. The dermoscopy image of skin cancer taken, undergone various\ntechniques to remove the noise and picture resolution. The image count is also\nincreased by using various image augmentation techniques. In the end, the\nTransfer Learning method is used to increase the classification accuracy of the\nimages further. Our CNN model gave a weighted average Precision of 0.88, a\nweighted Recall average of 0.74, and a weighted f1-score of 0.77. The transfer\nlearning approach applied using ResNet model yielded an accuracy of 90.51%\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 00:49:24 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Garg", "Rishu", ""], ["Maheshwari", "Saumil", ""], ["Shukla", "Anupam", ""]]}, {"id": "1912.03802", "submitter": "Candice Schumann", "authors": "Candice Schumann, Zhi Lang, Nicholas Mattei, John P. Dickerson", "title": "Group Fairness in Bandit Arm Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel formulation of group fairness in the contextual\nmulti-armed bandit (CMAB) setting. In the CMAB setting a sequential decision\nmaker must at each time step choose an arm to pull from a finite set of arms\nafter observing some context for each of the potential arm pulls. In our model\narms are partitioned into two or more sensitive groups based on some protected\nfeature (e.g., age, race, or socio-economic status). Despite the fact that\nthere may be differences in expected payout between the groups, we may wish to\nensure some form of fairness between picking arms from the various groups. In\nthis work we explore two definitions of fairness: equal group probability,\nwherein the probability of pulling an arm from any of the protected groups is\nthe same; and proportional parity, wherein the probability of choosing an arm\nfrom a particular group is proportional to the size of that group. We provide a\nnovel algorithm that can accommodate these notions of fairness for an arbitrary\nnumber of groups, and provide bounds on the regret for our algorithm. We then\nvalidate our algorithm using synthetic data as well as two real-world datasets\nfor intervention settings wherein we want to allocate resources fairly across\nprotected groups.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:02:35 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 17:26:41 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Schumann", "Candice", ""], ["Lang", "Zhi", ""], ["Mattei", "Nicholas", ""], ["Dickerson", "John P.", ""]]}, {"id": "1912.03805", "submitter": "Scott Sisson", "authors": "Tom Whitaker and Boris Beranger and Scott A. Sisson", "title": "Logistic regression models for aggregated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models are a popular and effective method to predict the\nprobability of categorical response data. However inference for these models\ncan become computationally prohibitive for large datasets. Here we adapt ideas\nfrom symbolic data analysis to summarise the collection of predictor variables\ninto histogram form, and perform inference on this summary dataset. We develop\nideas based on composite likelihoods to derive an efficient one-versus-rest\napproximate composite likelihood model for histogram-based random variables,\nconstructed from low-dimensional marginal histograms obtained from the full\nhistogram. We demonstrate that this procedure can achieve comparable\nclassification rates compared to the standard full data multinomial analysis\nand against state-of-the-art subsampling algorithms for logistic regression,\nbut at a substantially lower computational cost. Performance is explored\nthrough simulated examples, and analyses of large supersymmetry and satellite\ncrop classification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:15:49 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 07:46:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Whitaker", "Tom", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1912.03820", "submitter": "Mingzhang Yin", "authors": "Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, Chelsea\n  Finn", "title": "Meta-Learning without Memorization", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn new concepts with small amounts of data is a critical\naspect of intelligence that has proven challenging for deep learning methods.\nMeta-learning has emerged as a promising technique for leveraging data from\nprevious tasks to enable efficient learning of new tasks. However, most\nmeta-learning algorithms implicitly require that the meta-training tasks be\nmutually-exclusive, such that no single model can solve all of the tasks at\nonce. For example, when creating tasks for few-shot image classification, prior\nwork uses a per-task random assignment of image classes to N-way classification\nlabels. If this is not done, the meta-learner can ignore the task training data\nand learn a single model that performs all of the meta-training tasks\nzero-shot, but does not adapt effectively to new image classes. This\nrequirement means that the user must take great care in designing the tasks,\nfor example by shuffling labels or removing task identifying information from\nthe inputs. In some domains, this makes meta-learning entirely inapplicable. In\nthis paper, we address this challenge by designing a meta-regularization\nobjective using information theory that places precedence on data-driven\nadaptation. This causes the meta-learner to decide what must be learned from\nthe task training data and what should be inferred from the task testing input.\nBy doing so, our algorithm can successfully use data from\nnon-mutually-exclusive tasks to efficiently adapt to novel tasks. We\ndemonstrate its applicability to both contextual and gradient-based\nmeta-learning algorithms, and apply it in practical settings where applying\nstandard meta-learning has been difficult. Our approach substantially\noutperforms standard meta-learning algorithms in these settings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:30:46 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 19:49:41 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 22:33:53 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yin", "Mingzhang", ""], ["Tucker", "George", ""], ["Zhou", "Mingyuan", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1912.03821", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Zhuoran Yang, Tamer Ba\\c{s}ar", "title": "Decentralized Multi-Agent Reinforcement Learning with Networked Agents:\n  Recent Advances", "comments": "This is a invited submission to a Special Issue of the Journal of\n  Frontiers of Information Technology & Electronic Engineering (FITEE). Most of\n  the contents are based on the Sec. 4 in our recent overview arXiv:1911.10635,\n  with focus on the setting of decentralized MARL with networked agents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) has long been a significant and\neverlasting research topic in both machine learning and control. With the\nrecent development of (single-agent) deep RL, there is a resurgence of\ninterests in developing new MARL algorithms, especially those that are backed\nby theoretical analysis. In this paper, we review some recent advances a\nsub-area of this topic: decentralized MARL with networked agents. Specifically,\nmultiple agents perform sequential decision-making in a common environment,\nwithout the coordination of any central controller. Instead, the agents are\nallowed to exchange information with their neighbors over a communication\nnetwork. Such a setting finds broad applications in the control and operation\nof robots, unmanned vehicles, mobile sensor networks, and smart grid. This\nreview is built upon several our research endeavors in this direction, together\nwith some progresses made by other researchers along the line. We hope this\nreview to inspire the devotion of more research efforts to this exciting yet\nchallenging area.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:33:57 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Yang", "Zhuoran", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1912.03845", "submitter": "Giorgio Giannone", "authors": "Giorgio Giannone, Saeed Saremi, Jonathan Masci, Christian Osendorfer", "title": "No Representation without Transformation", "comments": "Preprint. Accepted at BDL and PGR workshops at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the framework of variational autoencoders to represent\ntransformations explicitly in the latent space. In the family of hierarchical\ngraphical models that emerges, the latent space is populated by higher order\nobjects that are inferred jointly with the latent representations they act on.\nTo explicitly demonstrate the effect of these higher order objects, we show\nthat the inferred latent transformations reflect interpretable properties in\nthe observation space. Furthermore, the model is structured in such a way that\nin the absence of transformations, we can run inference and obtain generative\ncapabilities comparable with standard variational autoencoders. Finally,\nutilizing the trained encoder, we outperform the baselines by a wide margin on\na challenging out-of-distribution classification task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 04:46:06 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 15:18:39 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Giannone", "Giorgio", ""], ["Saremi", "Saeed", ""], ["Masci", "Jonathan", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1912.03891", "submitter": "Petros Maragos", "authors": "Petros Maragos and Emmanouil Theodosis", "title": "Tropical Geometry and Piecewise-Linear Approximation of Curves and\n  Surfaces on Weighted Lattices", "comments": "39 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.RA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical Geometry and Mathematical Morphology share the same max-plus and\nmin-plus semiring arithmetic and matrix algebra. In this chapter we summarize\nsome of their main ideas and common (geometric and algebraic) structure,\ngeneralize and extend both of them using weighted lattices and a max-$\\star$\nalgebra with an arbitrary binary operation $\\star$ that distributes over max,\nand outline applications to geometry, machine learning, and optimization.\nFurther, we generalize tropical geometrical objects using weighted lattices.\nFinally, we provide the optimal solution of max-$\\star$ equations using\nmorphological adjunctions that are projections on weighted lattices, and apply\nit to optimal piecewise-linear regression for fitting max-$\\star$ tropical\ncurves and surfaces to arbitrary data that constitute polygonal or polyhedral\nshape approximations. This also includes an efficient algorithm for solving the\nconvex regression problem of data fitting with max-affine functions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:06:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Maragos", "Petros", ""], ["Theodosis", "Emmanouil", ""]]}, {"id": "1912.03896", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Riyasat Ohib, Sergey Plis, Vamsi Potluru", "title": "Grouped sparse projection", "comments": "21 pages, 12 figures; affiliation corrected, grant added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As evident from deep learning, very large models bring improvements in\ntraining dynamics and representation power. Yet, smaller models have benefits\nof energy efficiency and interpretability. To get the benefits from both ends\nof the spectrum we often encourage sparsity in the model. Unfortunately, most\nexisting approaches do not have a controllable way to request a desired value\nof sparsity in an interpretable parameter. In this paper, we design a new\nsparse projection method for a set of vectors in order to achieve a desired\naverage level of sparsity which is measured using the ratio of the $\\ell_1$ and\n$\\ell_2$ norms. Most existing methods project each vector individuality trying\nto achieve a target sparsity, hence the user has to choose a sparsity level for\neach vector (e.g., impose that all vectors have the same sparsity). Instead, we\nproject all vectors together to achieve an average target sparsity, where the\nsparsity levels of the vectors is automatically tuned. We also propose a\ngeneralization of this projection using a new notion of weighted sparsity\nmeasured using the ratio of a weighted $\\ell_1$ and the $\\ell_2$ norms. These\nprojections can be used in particular to sparsify the columns of a matrix,\nwhich we use to compute sparse nonnegative matrix factorization and to learn\nsparse deep networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:24:29 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 09:18:13 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Gillis", "Nicolas", ""], ["Ohib", "Riyasat", ""], ["Plis", "Sergey", ""], ["Potluru", "Vamsi", ""]]}, {"id": "1912.03905", "submitter": "Prabhat Nagarajan", "authors": "Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, Takahiro Ishikawa", "title": "ChainerRL: A Deep Reinforcement Learning Library", "comments": "Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research 22(77) (2021) 1-14;", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce ChainerRL, an open-source deep reinforcement\nlearning (DRL) library built using Python and the Chainer deep learning\nframework. ChainerRL implements a comprehensive set of DRL algorithms and\ntechniques drawn from state-of-the-art research in the field. To foster\nreproducible research, and for instructional purposes, ChainerRL provides\nscripts that closely replicate the original papers' experimental settings and\nreproduce published benchmark results for several algorithms. Lastly, ChainerRL\noffers a visualization tool that enables the qualitative inspection of trained\nagents. The ChainerRL source code can be found on GitHub:\nhttps://github.com/chainer/chainerrl.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:59:15 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:24:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fujita", "Yasuhiro", ""], ["Nagarajan", "Prabhat", ""], ["Kataoka", "Toshiki", ""], ["Ishikawa", "Takahiro", ""]]}, {"id": "1912.03915", "submitter": "Eduardo Hugo Sanchez", "authors": "Eduardo Hugo Sanchez (IRIT), Mathieu Serrurier (IRIT), Mathias Ortner", "title": "Learning Disentangled Representations via Mutual Information Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of learning disentangled\nrepresentations. Given a pair of images sharing some attributes, we aim to\ncreate a low-dimensional representation which is split into two parts: a shared\nrepresentation that captures the common information between the images and an\nexclusive representation that contains the specific information of each image.\nTo address this issue, we propose a model based on mutual information\nestimation without relying on image reconstruction or image generation. Mutual\ninformation maximization is performed to capture the attributes of data in the\nshared and exclusive representations while we minimize the mutual information\nbetween the shared and exclusive representation to enforce representation\ndisentanglement. We show that these representations are useful to perform\ndownstream tasks such as image classification and image retrieval based on the\nshared or exclusive component. Moreover, classification results show that our\nmodel outperforms the state-of-the-art model based on VAE/GAN approaches in\nrepresentation disentanglement.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:31:08 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sanchez", "Eduardo Hugo", "", "IRIT"], ["Serrurier", "Mathieu", "", "IRIT"], ["Ortner", "Mathias", ""]]}, {"id": "1912.03959", "submitter": "Eli (Omid) David", "authors": "Itay Mosafi, Eli David, Nathan S. Netanyahu", "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite\n  Unlabeled Data", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), pages\n  1-8, Budapest, Hungary, July 2019", "doi": "10.1109/IJCNN.2019.8851798", "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As state-of-the-art deep neural networks are deployed at the core of more\nadvanced Al-based products and services, the incentive for copying them (i.e.,\ntheir intellectual properties) by rival adversaries is expected to increase\nconsiderably over time. The best way to extract or steal knowledge from such\nnetworks is by querying them using a large dataset of random samples and\nrecording their output, followed by training a student network to mimic these\noutputs, without making any assumption about the original networks. The most\neffective way to protect against such a mimicking attack is to provide only the\nclassification result, without confidence values associated with the softmax\nlayer.In this paper, we present a novel method for generating composite images\nfor attacking a mentor neural network using a student model. Our method assumes\nno information regarding the mentor's training dataset, architecture, or\nweights. Further assuming no information regarding the mentor's softmax output\nvalues, our method successfully mimics the given neural network and steals all\nof its knowledge. We also demonstrate that our student network (which copies\nthe mentor) is impervious to watermarking protection methods, and thus would\nnot be detected as a stolen model.Our results imply, essentially, that all\ncurrent neural networks are vulnerable to mimicking attacks, even if they do\nnot divulge anything but the most basic required output, and that the student\nmodel which mimics them cannot be easily detected and singled out as a stolen\ncopy using currently available techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 10:57:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mosafi", "Itay", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1912.03960", "submitter": "Ankit Sharma", "authors": "Ankit Sharma, Garima Gupta, Ranjitha Prasad, Arnab Chatterjee,\n  Lovekesh Vig, Gautam Shroff", "title": "MetaCI: Meta-Learning for Causal Inference in a Heterogeneous Population", "comments": "10 pages, 4 figures, Accepted in CausalML Workshop - NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing inference on data obtained through observational studies is\nbecoming extremely relevant due to the widespread availability of data in\nfields such as healthcare, education, retail, etc. Furthermore, this data is\naccrued from multiple homogeneous subgroups of a heterogeneous population, and\nhence, generalizing the inference mechanism over such data is essential. We\npropose the MetaCI framework with the goal of answering counterfactual\nquestions in the context of causal inference (CI), where the factual\nobservations are obtained from several homogeneous subgroups. While the CI\nnetwork is designed to generalize from factual to counterfactual distribution\nin order to tackle covariate shift, MetaCI employs the meta-learning paradigm\nto tackle the shift in data distributions between training and test phase due\nto the presence of heterogeneity in the population, and due to drifts in the\ntarget distribution, also known as concept shift. We benchmark the performance\nof the MetaCI algorithm using the mean absolute percentage error over the\naverage treatment effect as the metric, and demonstrate that meta\ninitialization has significant gains compared to randomly initialized networks,\nand other methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:01:09 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 05:40:02 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 05:06:05 GMT"}, {"version": "v4", "created": "Fri, 1 May 2020 05:15:55 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2020 11:02:08 GMT"}, {"version": "v6", "created": "Wed, 17 Feb 2021 15:19:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Sharma", "Ankit", ""], ["Gupta", "Garima", ""], ["Prasad", "Ranjitha", ""], ["Chatterjee", "Arnab", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "1912.03978", "submitter": "Tan Nguyen", "authors": "Tan M. Nguyen, Animesh Garg, Richard G. Baraniuk, Anima Anandkumar", "title": "InfoCNF: An Efficient Conditional Continuous Normalizing Flow with\n  Adaptive Solvers", "comments": "17 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous Normalizing Flows (CNFs) have emerged as promising deep generative\nmodels for a wide range of tasks thanks to their invertibility and exact\nlikelihood estimation. However, conditioning CNFs on signals of interest for\nconditional image generation and downstream predictive tasks is inefficient due\nto the high-dimensional latent code generated by the model, which needs to be\nof the same size as the input data. In this paper, we propose InfoCNF, an\nefficient conditional CNF that partitions the latent space into a\nclass-specific supervised code and an unsupervised code that shared among all\nclasses for efficient use of labeled information. Since the partitioning\nstrategy (slightly) increases the number of function evaluations (NFEs),\nInfoCNF also employs gating networks to learn the error tolerances of its\nordinary differential equation (ODE) solvers for better speed and performance.\nWe show empirically that InfoCNF improves the test accuracy over the baseline\nwhile yielding comparable likelihood scores and reducing the NFEs on CIFAR10.\nFurthermore, applying the same partitioning strategy in InfoCNF on time-series\ndata helps improve extrapolation performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:37:22 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nguyen", "Tan M.", ""], ["Garg", "Animesh", ""], ["Baraniuk", "Richard G.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1912.03984", "submitter": "Shouvik Mani", "authors": "Shouvik Mani, Mehdi Maasoumy, Sina Pakazad, Henrik Ohlsson", "title": "Expert-guided Regularization via Distance Metric Learning", "comments": null, "journal-ref": "Learning with Rich Experience (LIRE) Workshop, NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional prediction is a challenging problem setting for traditional\nstatistical models. Although regularization improves model performance in high\ndimensions, it does not sufficiently leverage knowledge on feature importances\nheld by domain experts. As an alternative to standard regularization\ntechniques, we propose Distance Metric Learning Regularization (DMLreg), an\napproach for eliciting prior knowledge from domain experts and integrating that\nknowledge into a regularized linear model. First, we learn a Mahalanobis\ndistance metric between observations from pairwise similarity comparisons\nprovided by an expert. Then, we use the learned distance metric to place prior\ndistributions on coefficients in a linear model. Through experimental results\non a simulated high-dimensional prediction problem, we show that DMLreg leads\nto improvements in model performance when the domain expert is knowledgeable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:05:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mani", "Shouvik", ""], ["Maasoumy", "Mehdi", ""], ["Pakazad", "Sina", ""], ["Ohlsson", "Henrik", ""]]}, {"id": "1912.04002", "submitter": "J. Fernando Hernandez-Garcia", "authors": "J. Fernando Hernandez-Garcia and Richard S. Sutton", "title": "Learning Sparse Representations Incrementally in Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations have been shown to be useful in deep reinforcement\nlearning for mitigating catastrophic interference and improving the performance\nof agents in terms of cumulative reward. Previous results were based on a two\nstep process were the representation was learned offline and the action-value\nfunction was learned online afterwards. In this paper, we investigate if it is\npossible to learn a sparse representation and the action-value function\nsimultaneously and incrementally. We investigate this question by employing\nseveral regularization techniques and observing how they affect sparsity of the\nrepresentation learned by a DQN agent in two different benchmark domains. Our\nresults show that with appropriate regularization it is possible to increase\nthe sparsity of the representations learned by DQN agents. Moreover, we found\nthat learning sparse representations also resulted in improved performance in\nterms of cumulative reward. Finally, we found that the performance of the\nagents that learned a sparse representation was more robust to the size of the\nexperience replay buffer. This last finding supports the long standing\nhypothesis that the overlap in representations learned by deep neural networks\nis the leading cause of catastrophic interference.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:41:17 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hernandez-Garcia", "J. Fernando", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1912.04009", "submitter": "Alexandre Miot", "authors": "Alexandre Miot and Gilles Drigout", "title": "An empirical study of neural networks for trend detection in time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting structure in noisy time series is a difficult task. One intuitive\nfeature is the notion of trend. From theoretical hints and using simulated time\nseries, we empirically investigate the efficiency of standard recurrent neural\nnetworks (RNNs) to detect trends. We show the overall superiority and\nversatility of certain standard RNNs structures over various other estimators.\nThese RNNs could be used as basic blocks to build more complex time series\ntrend estimators.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:01:58 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 17:34:37 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Miot", "Alexandre", ""], ["Drigout", "Gilles", ""]]}, {"id": "1912.04022", "submitter": "J\\\"org Schl\\\"otterer", "authors": "Christian Reiser and J\\\"org Schl\\\"otterer and Michael Granitzer", "title": "Parallel Total Variation Distance Estimation with Neural Networks for\n  Merging Over-Clusterings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the initial situation where a dataset has been over-partitioned\ninto $k$ clusters and seek a domain independent way to merge those initial\nclusters. We identify the total variation distance (TVD) as suitable for this\ngoal. By exploiting the relation of the TVD to the Bayes accuracy we show how\nneural networks can be used to estimate TVDs between all pairs of clusters in\nparallel. Crucially, the needed memory space is decreased by reducing the\nrequired number of output neurons from $k^2$ to $k$. On realistically obtained\nover-clusterings of ImageNet subsets it is demonstrated that our TVD estimates\nlead to better merge decisions than those obtained by relying on\nstate-of-the-art unsupervised representations. Further the generality of the\napproach is verified by evaluating it on a a point cloud dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:25:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Reiser", "Christian", ""], ["Schl\u00f6tterer", "J\u00f6rg", ""], ["Granitzer", "Michael", ""]]}, {"id": "1912.04042", "submitter": "John Duchi", "authors": "Hilal Asi and John Duchi and Omid Javidbakht", "title": "Element Level Differential Privacy: The Right Granularity of Privacy", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy (DP) provides strong guarantees on the risk of\ncompromising a user's data in statistical learning applications, though these\nstrong protections make learning challenging and may be too stringent for some\nuse cases. To address this, we propose element level differential privacy,\nwhich extends differential privacy to provide protection against leaking\ninformation about any particular \"element\" a user has, allowing better utility\nand more robust results than classical DP. By carefully choosing these\n\"elements,\" it is possible to provide privacy protections at a desired\ngranularity. We provide definitions, associated privacy guarantees, and\nanalysis to identify the tradeoffs with the new definition; we also develop\nseveral private estimation and learning methodologies, providing careful\nexamples for item frequency and M-estimation (empirical risk minimization) with\nconcomitant privacy and utility analysis. We complement our theoretical and\nmethodological advances with several real-world applications, estimating\nhistograms and fitting several large-scale prediction models, including deep\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 23:05:54 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Asi", "Hilal", ""], ["Duchi", "John", ""], ["Javidbakht", "Omid", ""]]}, {"id": "1912.04067", "submitter": "Andreas Krug", "authors": "Andreas Krug, Sebastian Stober", "title": "Visualizing Deep Neural Networks for Speech Recognition with Learned\n  Topographic Filter Maps", "comments": "Accepted for 2019 ACL Workshop BlackboxNLP: Analyzing and\n  Interpreting Neural Networks for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uninformative ordering of artificial neurons in Deep Neural Networks\ncomplicates visualizing activations in deeper layers. This is one reason why\nthe internal structure of such models is very unintuitive. In neuroscience,\nactivity of real brains can be visualized by highlighting active regions.\nInspired by those techniques, we train a convolutional speech recognition\nmodel, where filters are arranged in a 2D grid and neighboring filters are\nsimilar to each other. We show, how those topographic filter maps visualize\nartificial neuron activations more intuitively. Moreover, we investigate,\nwhether this causes phoneme-responsive neurons to be grouped in certain regions\nof the topographic map.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 10:31:29 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Krug", "Andreas", ""], ["Stober", "Sebastian", ""]]}, {"id": "1912.04075", "submitter": "Gabrielle Ras", "authors": "Gabri\\\"elle Ras, Luca Ambrogioni, Umut G\\\"u\\c{c}l\\\"u, Marcel A. J. van\n  Gerven", "title": "Temporal Factorization of 3D Convolutional Kernels", "comments": "8 pages, 3 figures, Proceedings of BNAIC/BENELEARN 2019 conference", "journal-ref": "Proceedings of the 31st Benelux Conference on Artificial\n  Intelligence (BNAIC 2019) and the 28th Belgian Dutch Conference on Machine\n  Learning (Benelearn 2019), Brussels, Belgium, November 6-8, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D convolutional neural networks are difficult to train because they are\nparameter-expensive and data-hungry. To solve these problems we propose a\nsimple technique for learning 3D convolutional kernels efficiently requiring\nless training data. We achieve this by factorizing the 3D kernel along the\ntemporal dimension, reducing the number of parameters and making training from\ndata more efficient. Additionally we introduce a novel dataset called\nVideo-MNIST to demonstrate the performance of our method. Our method\nsignificantly outperforms the conventional 3D convolution in the low data\nregime (1 to 5 videos per class). Finally, our model achieves competitive\nresults in the high data regime (>10 videos per class) using up to 45% fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:21:00 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Ras", "Gabri\u00eblle", ""], ["Ambrogioni", "Luca", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1912.04106", "submitter": "Stavros Vologiannidis", "authors": "Polychronis Charitidis, Stavros Doropoulos, Stavros Vologiannidis,\n  Ioannis Papastergiou, Sophia Karakeva", "title": "Towards countering hate speech against journalists on social media", "comments": null, "journal-ref": null, "doi": "10.1016/j.osnem.2020.100071", "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The damaging effects of hate speech on social media are evident during the\nlast few years, and several organizations, researchers and social media\nplatforms tried to harness them in various ways. Despite these efforts, social\nmedia users are still affected by hate speech. The problem is even more\napparent to social groups that promote public discourse, such as journalists.\nIn this work, we focus on countering hate speech that is targeted to\njournalistic social media accounts. To accomplish this, a group of journalists\nassembled a definition of hate speech, taking into account the journalistic\npoint of view and the types of hate speech that are usually targeted against\njournalists. We then compile a large pool of tweets referring to\njournalism-related accounts in multiple languages. In order to annotate the\npool of unlabeled tweets according to the definition, we follow a concise\nannotation strategy that involves active learning annotation stages. The\noutcome of this paper is a novel, publicly available collection of Twitter\ndatasets in five different languages. Additionally, we experiment with\nstate-of-the-art deep learning architectures for hate speech detection and use\nour annotated datasets to train and evaluate them. Finally, we propose an\nensemble detection model that outperforms all individual models.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:51:23 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:55:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Charitidis", "Polychronis", ""], ["Doropoulos", "Stavros", ""], ["Vologiannidis", "Stavros", ""], ["Papastergiou", "Ioannis", ""], ["Karakeva", "Sophia", ""]]}, {"id": "1912.04107", "submitter": "Radu Tudor Ionescu", "authors": "S\\'ebastien D\\'ejean, Radu Tudor Ionescu, Josiane Mothe, Md Zia Ullah", "title": "Forward and Backward Feature Selection for Query Performance Prediction", "comments": "Accepted at SAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of query performance prediction (QPP) is to automatically estimate\nthe effectiveness of a search result for any given query, without relevance\njudgements. Post-retrieval features have been shown to be more effective for\nthis task while being more expensive to compute than pre-retrieval features.\nCombining multiple post-retrieval features is even more effective, but\nstate-of-the-art QPP methods are impossible to interpret because of the\nblack-box nature of the employed machine learning models. However,\ninterpretation is useful for understanding the predictive model and providing\nmore answers about its behavior. Moreover, combining many post-retrieval\nfeatures is not applicable to real-world cases, since the query running time is\nof utter importance. In this paper, we investigate a new framework for feature\nselection in which the trained model explains well the prediction. We introduce\na step-wise (forward and backward) model selection approach where different\nsubsets of query features are used to fit different models from which the\nsystem selects the best one. We evaluate our approach on four TREC collections\nusing standard QPP features. We also develop two QPP features to address the\nissue of query-drift in the query feedback setting. We found that: (1) our\nmodel based on a limited number of selected features is as good as more complex\nmodels for QPP and better than non-selective models; (2) our model is more\nefficient than complex models during inference time since it requires fewer\nfeatures; (3) the predictive model is readable and understandable; and (4) one\nof our new QPP features is consistently selected across different collections,\nproving its usefulness.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:19:16 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["D\u00e9jean", "S\u00e9bastien", ""], ["Ionescu", "Radu Tudor", ""], ["Mothe", "Josiane", ""], ["Ullah", "Md Zia", ""]]}, {"id": "1912.04108", "submitter": "Liang Zhao", "authors": "Liang Zhao, Yang Wang, Daxiang Dong, Hao Tian", "title": "Learning to Recommend via Meta Parameter Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to solve an important problem in recommendation --\nuser cold start, based on meta leaning method. Previous meta learning\napproaches finetune all parameters for each new user, which is both computing\nand storage expensive. In contrast, we divide model parameters into fixed and\nadaptive parts and develop a two-stage meta learning algorithm to learn them\nseparately. The fixed part, capturing user invariant features, is shared by all\nusers and is learned during offline meta learning stage. The adaptive part,\ncapturing user specific features, is learned during online meta learning stage.\nBy decoupling user invariant parameters from user dependent parameters, the\nproposed approach is more efficient and storage cheaper than previous methods.\nIt also has potential to deal with catastrophic forgetting while continually\nadapting for streaming coming users.\n  Experiments on production data demonstrates that the proposed method\nconverges faster and to a better performance than baseline methods.\nMeta-training without online meta model finetuning increases the AUC from\n72.24% to 74.72% (2.48% absolute improvement). Online meta training achieves a\nfurther gain of 2.46\\% absolute improvement comparing with offline meta\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:58:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhao", "Liang", ""], ["Wang", "Yang", ""], ["Dong", "Daxiang", ""], ["Tian", "Hao", ""]]}, {"id": "1912.04109", "submitter": "Yangjun Xu", "authors": "Liang Chen and Yangjun Xu and Fenfang Xie and Min Huang and Zibin\n  Zheng", "title": "Data Poisoning Attacks on Neighborhood-based Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, collaborative filtering recommender systems have been widely\ndeployed in many commercial companies to make profit. Neighbourhood-based\ncollaborative filtering is common and effective. To date, despite its\neffectiveness, there has been little effort to explore their robustness and the\nimpact of data poisoning attacks on their performance. Can the\nneighbourhood-based recommender systems be easily fooled? To this end, we shed\nlight on the robustness of neighbourhood-based recommender systems and propose\na novel data poisoning attack framework encoding the purpose of attack and\nconstraint against them. We firstly illustrate how to calculate the optimal\ndata poisoning attack, namely UNAttack. We inject a few well-designed fake\nusers into the recommender systems such that target items will be recommended\nto as many normal users as possible. Extensive experiments are conducted on\nthree real-world datasets to validate the effectiveness and the transferability\nof our proposed method. Besides, some interesting phenomenons can be found. For\nexample, 1) neighbourhood-based recommender systems with Euclidean\nDistance-based similarity have strong robustness. 2) the fake users can be\ntransferred to attack the state-of-the-art collaborative filtering recommender\nsystems such as Neural Collaborative Filtering and Bayesian Personalized\nRanking Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:34:58 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Liang", ""], ["Xu", "Yangjun", ""], ["Xie", "Fenfang", ""], ["Huang", "Min", ""], ["Zheng", "Zibin", ""]]}, {"id": "1912.04116", "submitter": "Cailey Kerley", "authors": "Cailey I. Kerley, Kurt G. Schilling, Justin Blaber, Beth Miller, Allen\n  Newton, Adam W. Anderson, Bennett A. Landman, and Tonia S. Rex", "title": "MRI correlates of chronic symptoms in mild traumatic brain injury", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Veterans with mild traumatic brain injury (mTBI) have reported auditory and\nvisual dysfunction that persists beyond the acute incident. The etiology behind\nthese symptoms is difficult to characterize with current clinical imaging.\nThese functional deficits may be caused by shear injury or micro-bleeds, which\ncan be detected with special imaging modalities. We explore these hypotheses in\na pilot study of multi-parametric MRI. We extract over 1,000 imaging and\nclinical metrics and project them to a low-dimensional space, where we can\ndiscriminate between healthy controls and patients with mTBI. We also show\ncorrelations between the metric representations and patient symptoms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:30:51 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:46:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kerley", "Cailey I.", ""], ["Schilling", "Kurt G.", ""], ["Blaber", "Justin", ""], ["Miller", "Beth", ""], ["Newton", "Allen", ""], ["Anderson", "Adam W.", ""], ["Landman", "Bennett A.", ""], ["Rex", "Tonia S.", ""]]}, {"id": "1912.04132", "submitter": "Kostadin Cvejoski", "authors": "Kostadin Cvejoski, Ramses J. Sanchez, Bogdan Georgiev, Jannis\n  Schuecker, Christian Bauckhage, Cesar Ojeda", "title": "Recurrent Point Processes for Dynamic Review Models", "comments": "Presented at the AAAI 2020 Workshop on Interactive and Conversational\n  Recommendation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in recommender system research has shown the importance of\nincluding temporal representations to improve interpretability and performance.\nHere, we incorporate temporal representations in continuous time via recurrent\npoint process for a dynamical model of reviews. Our goal is to characterize how\nchanges in perception, user interest and seasonal effects affect review text.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:42:01 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:15:31 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Cvejoski", "Kostadin", ""], ["Sanchez", "Ramses J.", ""], ["Georgiev", "Bogdan", ""], ["Schuecker", "Jannis", ""], ["Bauckhage", "Christian", ""], ["Ojeda", "Cesar", ""]]}, {"id": "1912.04136", "submitter": "Akshay Krishnamurthy", "authors": "Yining Wang, Ruosong Wang, Simon S. Du, Akshay Krishnamurthy", "title": "Optimism in Reinforcement Learning with Generalized Linear Function\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new provably efficient algorithm for episodic reinforcement\nlearning with generalized linear function approximation. We analyze the\nalgorithm under a new expressivity assumption that we call \"optimistic\nclosure,\" which is strictly weaker than assumptions from prior analyses for the\nlinear setting. With optimistic closure, we prove that our algorithm enjoys a\nregret bound of $\\tilde{O}(\\sqrt{d^3 T})$ where $d$ is the dimensionality of\nthe state-action features and $T$ is the number of episodes. This is the first\nstatistically and computationally efficient algorithm for reinforcement\nlearning with generalized linear functions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:47:27 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Ruosong", ""], ["Du", "Simon S.", ""], ["Krishnamurthy", "Akshay", ""]]}, {"id": "1912.04138", "submitter": "Adi Szeskin", "authors": "Adi Szeskin, Lev Faivishevsky, Ashwin K Muppalla, Amitai Armon and Tom\n  Hope", "title": "A Weak Supervision Approach to Detecting Visual Anomalies for Automated\n  Testing of Graphics Units", "comments": "Accepted to NeurIPS 2019 Machine Learning for Systems Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning system for testing graphics units by detecting\nnovel visual corruptions in videos. Unlike previous work in which manual\ntagging was required to collect labeled training data, our weak supervision\nmethod is fully automatic and needs no human labelling. This is achieved by\nreproducing driver bugs that increase the probability of generating\ncorruptions, and by making use of ideas and methods from the Multiple Instance\nLearning (MIL) setting. In our experiments, we significantly outperform\nunsupervised methods such as GAN-based models and discover novel corruptions\nundetected by baselines, while adhering to strict requirements on accuracy and\nefficiency of our real-time system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:48:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Szeskin", "Adi", ""], ["Faivishevsky", "Lev", ""], ["Muppalla", "Ashwin K", ""], ["Armon", "Amitai", ""], ["Hope", "Tom", ""]]}, {"id": "1912.04154", "submitter": "Yingzhou Li", "authors": "Zhongshu Xu, Yingzhou Li, Xiuyuan Cheng", "title": "Butterfly-Net2: Simplified Butterfly-Net and Fourier Transform\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured CNN designed using the prior information of problems potentially\nimproves efficiency over conventional CNNs in various tasks in solving PDEs and\ninverse problems in signal processing. This paper introduces BNet2, a\nsimplified Butterfly-Net and inline with the conventional CNN. Moreover, a\nFourier transform initialization is proposed for both BNet2 and CNN with\nguaranteed approximation power to represent the Fourier transform operator.\nExperimentally, BNet2 and the Fourier transform initialization strategy are\ntested on various tasks, including approximating Fourier transform operator,\nend-to-end solvers of linear and nonlinear PDEs, and denoising and deblurring\nof 1D signals. On all tasks, under the same initialization, BNet2 achieves\nsimilar accuracy as CNN but has fewer parameters. And Fourier transform\ninitialized BNet2 and CNN consistently improve the training and testing\naccuracy over the randomly initialized CNN.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:25:32 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 19:54:13 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 17:54:29 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Xu", "Zhongshu", ""], ["Li", "Yingzhou", ""], ["Cheng", "Xiuyuan", ""]]}, {"id": "1912.04174", "submitter": "Harry Clifford MSci DPhil", "authors": "Geoffroy Dubourg-Felonneau, Omar Darwish, Christopher Parsons, Dami\n  Rebergen, John W Cassidy, Nirmesh Patel, Harry W Clifford", "title": "Deep Bayesian Recurrent Neural Networks for Somatic Variant Calling in\n  Cancer", "comments": "Bayesian Deep Learning Workshop at NeurIPS 2019. arXiv admin note:\n  text overlap with arXiv:1912.02065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging field of precision oncology relies on the accurate pinpointing\nof alterations in the molecular profile of a tumor to provide personalized\ntargeted treatments. Current methodologies in the field commonly include the\napplication of next generation sequencing technologies to a tumor sample,\nfollowed by the identification of mutations in the DNA known as somatic\nvariants. The differentiation of these variants from sequencing error poses a\nclassic classification problem, which has traditionally been approached with\nBayesian statistics, and more recently with supervised machine learning methods\nsuch as neural networks. Although these methods provide greater accuracy,\nclassic neural networks lack the ability to indicate the confidence of a\nvariant call. In this paper, we explore the performance of deep Bayesian neural\nnetworks on next generation sequencing data, and their ability to give\nprobability estimates for somatic variant calls. In addition to demonstrating\nsimilar performance in comparison to standard neural networks, we show that the\nresultant output probabilities make these better suited to the disparate and\nhighly-variable sequencing data-sets these models are likely to encounter in\nthe real world. We aim to deliver algorithms to oncologists for which model\ncertainty better reflects accuracy, for improved clinical application. By\nmoving away from point estimates to reliable confidence intervals, we expect\nthe resultant clinical and treatment decisions to be more robust and more\ninformed by the underlying reality of the tumor molecular profile.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:01:15 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Dubourg-Felonneau", "Geoffroy", ""], ["Darwish", "Omar", ""], ["Parsons", "Christopher", ""], ["Rebergen", "Dami", ""], ["Cassidy", "John W", ""], ["Patel", "Nirmesh", ""], ["Clifford", "Harry W", ""]]}, {"id": "1912.04194", "submitter": "Ali Sharifara", "authors": "Razieh Tavakoli, Ali Sharifara, Mohammad Najafi", "title": "Prediction of Sewer Pipe Deterioration Using Random Forest\n  Classification", "comments": "This submission has been removed by arXiv administrators due to\n  copyright infringement and inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wastewater infrastructure systems deteriorate over time due to a combination\nof physical and chemical factors. Failure of this significant infrastructure\ncould affect important social, environmental, and economic impacts.\nFurthermore, recognizing the optimized timeline for inspection of sewer\npipelines are challenging tasks for the utility managers and other authorities.\nRegular examination of sewer networks is not cost-effective due to limited time\nand high cost of assessment technologies and a large inventory of pipes. To\navoid such obstacles, various researchers endeavored to improve infrastructure\ncondition assessment methodologies to maintain sewer pipe systems at the\ndesired condition. Sewer condition prediction models are developed to provide a\nframework to forecast the future condition of pipes to schedule inspection\nfrequencies. The main goal of this study is to develop a predictive model for\nwastewater pipes using random forest classification. Predictive models can\neffectively predict sewer pipe condition and can increase the certainty level\nof the predictive results and decrease uncertainty in the current condition of\nwastewater pipes. The developed random forest classification model has achieved\na stratified test set false negative rate, the false positive rate, and an\nexcellent area under the ROC curve of 0.81 in a case study application for the\nCity of LA, California. An area under the ROC curve > 0.80 indicates the\ndeveloped model is an \"excellent\" choice for predicting the condition of\nindividual pipes in a sewer network. The deterioration models can be used in\nthe industry to improve the inspection timeline and maintenance planning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 17:17:43 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Tavakoli", "Razieh", ""], ["Sharifara", "Ali", ""], ["Najafi", "Mohammad", ""]]}, {"id": "1912.04201", "submitter": "Aaron Havens", "authors": "Aaron Havens, Yi Ouyang, Prabhat Nagarajan, Yasuhiro Fujita", "title": "Learning Latent State Spaces for Planning through Reward Prediction", "comments": "Deep RL Workshop, Neurips 2019, Vancouver", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning methods typically learn models for\nhigh-dimensional state spaces by aiming to reconstruct and predict the original\nobservations. However, drawing inspiration from model-free reinforcement\nlearning, we propose learning a latent dynamics model directly from rewards. In\nthis work, we introduce a model-based planning framework which learns a latent\nreward prediction model and then plans in the latent state-space. The latent\nrepresentation is learned exclusively from multi-step reward prediction which\nwe show to be the only necessary information for successful planning. With this\nframework, we are able to benefit from the concise model-free representation,\nwhile still enjoying the data-efficiency of model-based algorithms. We\ndemonstrate our framework in multi-pendulum and multi-cheetah environments\nwhere several pendulums or cheetahs are shown to the agent but only one of\nwhich produces rewards. In these environments, it is important for the agent to\nconstruct a concise latent representation to filter out irrelevant\nobservations. We find that our method can successfully learn an accurate latent\nreward prediction model in the presence of the irrelevant information while\nexisting model-based methods fail. Planning in the learned latent state-space\nshows strong performance and high sample efficiency over model-free and\nmodel-based baselines.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 17:32:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Havens", "Aaron", ""], ["Ouyang", "Yi", ""], ["Nagarajan", "Prabhat", ""], ["Fujita", "Yasuhiro", ""]]}, {"id": "1912.04211", "submitter": "ANtoine Marot", "authors": "Antoine Marot, Benjamin Donnot, Camilo Romero, Luca Veyrin-Forrer,\n  Marvin Lerousseau, Balthazar Donon, Isabelle Guyon", "title": "Learning to run a power network challenge for training topology\n  controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For power grid operations, a large body of research focuses on using\ngeneration redispatching, load shedding or demand side management\nflexibilities. However, a less costly and potentially more flexible option\nwould be grid topology reconfiguration, as already partially exploited by\nCoreso (European RSC) and RTE (French TSO) operations. Beyond previous work on\nbranch switching, bus reconfigurations are a broader class of action and could\nprovide some substantial benefits to route electricity and optimize the grid\ncapacity to keep it within safety margins. Because of its non-linear and\ncombinatorial nature, no existing optimal power flow solver can yet tackle this\nproblem. We here propose a new framework to learn topology controllers through\nimitation and reinforcement learning. We present the design and the results of\nthe first \"Learning to Run a Power Network\" challenge released with this\nframework. We finally develop a method providing performance upper-bounds\n(oracle), which highlights remaining unsolved challenges and suggests future\ndirections of improvement.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:35:57 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Marot", "Antoine", ""], ["Donnot", "Benjamin", ""], ["Romero", "Camilo", ""], ["Veyrin-Forrer", "Luca", ""], ["Lerousseau", "Marvin", ""], ["Donon", "Balthazar", ""], ["Guyon", "Isabelle", ""]]}, {"id": "1912.04212", "submitter": "Hwan Goh", "authors": "Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh", "title": "Solving Bayesian Inverse Problems via Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the field of machine learning has made phenomenal progress\nin the pursuit of simulating real-world data generation processes. One notable\nexample of such success is the variational autoencoder (VAE). In this work,\nwith a small shift in perspective, we leverage and adapt VAEs for a different\npurpose: uncertainty quantification in scientific inverse problems. We\nintroduce UQ-VAE: a flexible, adaptive, hybrid data/model-informed framework\nfor training neural networks capable of rapid modelling of the posterior\ndistribution representing the unknown parameter of interest. Specifically, from\ndivergence-based variational inference, our framework is derived such that most\nof the information usually present in scientific inverse problems is fully\nutilized in the training procedure. Additionally, this framework includes an\nadjustable hyperparameter that allows selection of the notion of distance\nbetween the posterior model and the target distribution. This introduces more\nflexibility in controlling how optimization directs the learning of the\nposterior model. Further, this framework possesses an inherent adaptive\noptimization property that emerges through the learning of the posterior\nuncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 16:33:32 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 02:09:13 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 20:44:37 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 16:51:16 GMT"}, {"version": "v5", "created": "Sat, 12 Dec 2020 02:39:05 GMT"}, {"version": "v6", "created": "Sun, 28 Feb 2021 22:44:52 GMT"}, {"version": "v7", "created": "Sun, 7 Mar 2021 06:42:51 GMT"}, {"version": "v8", "created": "Wed, 5 May 2021 02:22:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Goh", "Hwan", ""], ["Sheriffdeen", "Sheroze", ""], ["Wittmer", "Jonathan", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1912.04216", "submitter": "Ilya Kavalerov", "authors": "Ilya Kavalerov, Wojciech Czaja, Rama Chellappa", "title": "cGANs with Multi-Hinge Loss", "comments": "Accepted to Winter Conference on Applications of Computer Vision\n  (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to incorporate class conditional information into\nthe critic of GANs via a multi-class generalization of the commonly used Hinge\nloss that is compatible with both supervised and semi-supervised settings. We\nstudy the compromise between training a state of the art generator and an\naccurate classifier simultaneously, and propose a way to use our algorithm to\nmeasure the degree to which a generator and critic are class conditional. We\nshow the trade-off between a generator-critic pair respecting class\nconditioning inputs and generating the highest quality images. With our\nmulti-hinge loss modification we are able to improve Inception Scores and\nFrechet Inception Distance on the Imagenet dataset. We make our tensorflow code\navailable at https://github.com/ilyakava/gan.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 17:51:50 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 21:01:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kavalerov", "Ilya", ""], ["Czaja", "Wojciech", ""], ["Chellappa", "Rama", ""]]}, {"id": "1912.04218", "submitter": "Hideaki Hayashi D.Eng.", "authors": "Hideaki Hayashi, Taro Shibanoki and Toshio Tsuji", "title": "A Neural Network Based on the Johnson $S_\\mathrm{U}$ Translation System\n  and Related Application to Electromyogram Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromyogram (EMG) classification is a key technique in EMG-based control\nsystems. The existing EMG classification methods do not consider the\ncharacteristics of EMG features that the distribution has skewness and\nkurtosis, causing drawbacks such as the requirement of hyperparameter tuning.\nIn this paper, we propose a neural network based on the Johnson $S_\\mathrm{U}$\ntranslation system that is capable of representing distributions with skewness\nand kurtosis. The Johnson system is a normalizing translation that transforms\nnon-normal data to a normal distribution, thereby enabling the representation\nof a wide range of distributions. In this study, a discriminative model based\non the multivariate Johnson $S_\\mathrm{U}$ translation system is transformed\ninto a linear combination of coefficients and input vectors using\nlog-linearization. This is then incorporated into a neural network structure,\nthereby allowing the calculation of the posterior probability of the input\nvectors for each class and the determination of model parameters as weight\ncoefficients of the network. The uniqueness of convergence of the network\nlearning is theoretically guaranteed. In the experiments, the suitability of\nthe proposed network for distributions including skewness and kurtosis is\nevaluated using artificially generated data. Its applicability for real\nbiological data is also evaluated via an EMG classification experiment. The\nresults show that the proposed network achieves high classification performance\nwithout the need for hyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 10:28:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hayashi", "Hideaki", ""], ["Shibanoki", "Taro", ""], ["Tsuji", "Toshio", ""]]}, {"id": "1912.04228", "submitter": "Casey Meehan", "authors": "Casey Meehan, Kamalika Chaudhuri", "title": "Location Trace Privacy Under Conditional Priors", "comments": "Included in NeurIPS 2019 PriML workshop\n  https://priml-workshop.github.io/priml2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing meaningful privacy to users of location based services is\nparticularly challenging when multiple locations are revealed in a short period\nof time. This is primarily due to the tremendous degree of dependence that can\nbe anticipated between points. We propose a R\\'enyi differentially private\nframework for bounding expected privacy loss for conditionally dependent data.\nAdditionally, we demonstrate an algorithm for achieving this privacy under\nGaussian process conditional priors. This framework both exemplifies why\nconditionally dependent data is so challenging to protect and offers a strategy\nfor preserving privacy to within a fixed radius for every user location in a\ntrace.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:12:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Meehan", "Casey", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1912.04232", "submitter": "Samuel Schoenholz", "authors": "Samuel S. Schoenholz and Ekin D. Cubuk", "title": "JAX, M.D.: A Framework for Differentiable Physics", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (2020)", "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce JAX MD, a software package for performing differentiable physics\nsimulations with a focus on molecular dynamics. JAX MD includes a number of\nphysics simulation environments, as well as interaction potentials and neural\nnetworks that can be integrated into these environments without writing any\nadditional code. Since the simulations themselves are differentiable functions,\nentire trajectories can be differentiated to perform meta-optimization. These\nfeatures are built on primitive operations, such as spatial partitioning, that\nallow simulations to scale to hundreds-of-thousands of particles on a single\nGPU. These primitives are flexible enough that they can be used to scale up\nworkloads outside of molecular dynamics. We present several examples that\nhighlight the features of JAX MD including: integration of graph neural\nnetworks into traditional simulations, meta-optimization through minimization\nof particle packings, and a multi-agent flocking simulation. JAX MD is\navailable at www.github.com/google/jax-md.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:19:06 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 08:09:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Schoenholz", "Samuel S.", ""], ["Cubuk", "Ekin D.", ""]]}, {"id": "1912.04242", "submitter": "Jacobo Roa-Vicens", "authors": "Jacobo Roa-Vicens, Yuanbo Wang, Virgile Mison, Yarin Gal, Ricardo\n  Silva", "title": "Adversarial recovery of agent rewards from latent spaces of the limit\n  order book", "comments": "Published as a workshop paper on NeurIPS 2019 Workshop on Robust AI\n  in Financial Services. 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning has proved its ability to explain state-action\ntrajectories of expert agents by recovering their underlying reward functions\nin increasingly challenging environments. Recent advances in adversarial\nlearning have allowed extending inverse RL to applications with non-stationary\nenvironment dynamics unknown to the agents, arbitrary structures of reward\nfunctions and improved handling of the ambiguities inherent to the ill-posed\nnature of inverse RL. This is particularly relevant in real time applications\non stochastic environments involving risk, like volatile financial markets.\nMoreover, recent work on simulation of complex environments enable learning\nalgorithms to engage with real market data through simulations of its latent\nspace representations, avoiding a costly exploration of the original\nenvironment. In this paper, we explore whether adversarial inverse RL\nalgorithms can be adapted and trained within such latent space simulations from\nreal market data, while maintaining their ability to recover agent rewards\nrobust to variations in the underlying dynamics, and transfer them to new\nregimes of the original environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:32:12 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Roa-Vicens", "Jacobo", ""], ["Wang", "Yuanbo", ""], ["Mison", "Virgile", ""], ["Gal", "Yarin", ""], ["Silva", "Ricardo", ""]]}, {"id": "1912.04261", "submitter": "Jonas Ismael Liechti", "authors": "Jonas I. Liechti and Sebastian Bonhoeffer", "title": "A time resolved clustering method revealing longterm structures and\n  their short-term internal dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last decades have not only been characterized by an explosive growth of\ndata, but also an increasing appreciation of data as a valuable resource. Their\nvalue comes with the ability to extract meaningful patterns that are of\neconomic, societal or scientific relevance. A particular challenge is the\nidentification of patterns across time, including those that might only become\napparent when the temporal dimension is taken into account. Here, we present a\nnovel method that aims to achieve this by detecting dynamic clusters, i.e.\nstructural elements that can be present over prolonged durations. It is based\non an adaptive identification of majority overlaps between groups at different\ntime points and accommodates the transient decompositions in otherwise\npersistent dynamic clusters. Our method enables the detection of persistent\nstructural elements with internal dynamics and can be applied to any\nclassifiable data, ranging from social contact networks to arbitrary sets of\ntime stamped feature vectors. It represents a unique tool to study systems with\nnon-trivial temporal dynamics and has a broad applicability to scientific,\nsocietal and economic data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:54:54 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 20:38:46 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Liechti", "Jonas I.", ""], ["Bonhoeffer", "Sebastian", ""]]}, {"id": "1912.04265", "submitter": "Jeffrey Negrea", "authors": "Jeffrey Negrea, Gintare Karolina Dziugaite, Daniel M. Roy", "title": "In Defense of Uniform Convergence: Generalization via derandomization\n  with an application to interpolating predictors", "comments": "14 pages before references and appendices. 35 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study the generalization error of a learned predictor $\\hat h$\nin terms of that of a surrogate (potentially randomized) predictor that is\ncoupled to $\\hat h$ and designed to trade empirical risk for control of\ngeneralization error. In the case where $\\hat h$ interpolates the data, it is\ninteresting to consider theoretical surrogate classifiers that are partially\nderandomized or rerandomized, e.g., fit to the training data but with modified\nlabel noise. We also show that replacing $\\hat h$ by its conditional\ndistribution with respect to an arbitrary $\\sigma$-field is a convenient way to\nderandomize. We study two examples, inspired by the work of Nagarajan and\nKolter (2019) and Bartlett et al. (2019), where the learned classifier $\\hat h$\ninterpolates the training data with high probability, has small risk, and, yet,\ndoes not belong to a nonrandom class with a tight uniform bound on two-sided\ngeneralization error. At the same time, we bound the risk of $\\hat h$ in terms\nof surrogates constructed by conditioning and denoising, respectively, and\nshown to belong to nonrandom classes with uniformly small generalization error.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:57:41 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 18:55:56 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Negrea", "Jeffrey", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1912.04278", "submitter": "Huidong Xie", "authors": "Huidong Xie, Hongming Shan, Wenxiang Cong, Chi Liu, Xiaohua Zhang,\n  Shaohua Liu, Ruola Ning, Ge Wang", "title": "Deep Efficient End-to-end Reconstruction (DEER) Network for Few-view\n  Breast CT Image Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3033795", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast CT provides image volumes with isotropic resolution in high contrast,\nenabling detection of small calcification (down to a few hundred microns in\nsize) and subtle density differences. Since breast is sensitive to x-ray\nradiation, dose reduction of breast CT is an important topic, and for this\npurpose, few-view scanning is a main approach. In this article, we propose a\nDeep Efficient End-to-end Reconstruction (DEER) network for few-view breast CT\nimage reconstruction. The major merits of our network include high dose\nefficiency, excellent image quality, and low model complexity. By the design,\nthe proposed network can learn the reconstruction process with as few as O(N)\nparameters, where N is the side length of an image to be reconstructed, which\nrepresents orders of magnitude improvements relative to the state-of-the-art\ndeep-learning-based reconstruction methods that map raw data to tomographic\nimages directly. Also, validated on a cone-beam breast CT dataset prepared by\nKoning Corporation on a commercial scanner, our method demonstrates a\ncompetitive performance over the state-of-the-art reconstruction networks in\nterms of image quality. The source code of this paper is available at:\nhttps://github.com/HuidongXie/DEER.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:44:24 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 11:35:10 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 21:00:05 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Xie", "Huidong", ""], ["Shan", "Hongming", ""], ["Cong", "Wenxiang", ""], ["Liu", "Chi", ""], ["Zhang", "Xiaohua", ""], ["Liu", "Shaohua", ""], ["Ning", "Ruola", ""], ["Wang", "Ge", ""]]}, {"id": "1912.04345", "submitter": "Carl Poelking", "authors": "Carl Poelking, Yehia Amar, Alexei Lapkin, Lucy Colwell", "title": "Noisy, sparse, nonlinear: Navigating the Bermuda Triangle of physical\n  inference with deep filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the microscopic interactions that determine molecular reactivity\nposes a challenge across the physical sciences. Even a basic understanding of\nthe underlying reaction mechanisms can substantially accelerate materials and\ncompound design, including the development of new catalysts or drugs. Given the\ndifficulties routinely faced by both experimental and theoretical\ninvestigations that aim to improve our mechanistic understanding of a reaction,\nrecent advances have focused on data-driven routes to derive structure-property\nrelationships directly from high-throughput screens. However, even these\nhigh-quality, high-volume data are noisy, sparse and biased -- placing them in\na regime where machine-learning is extremely challenging. Here we show that a\nstatistical approach based on deep filtering of nonlinear feature networks\nresults in physicochemical models that are more robust, transparent and\ngeneralize better than standard machine-learning architectures. Using diligent\ndescriptor design and data post-processing, we exemplify the approach using\nboth literature and fresh data on asymmetric catalytic hydrogenation,\nPalladium-catalyzed cross-coupling reactions, and drug-drug synergy. We\nillustrate how the sparse models uncovered by the filtering help us formulate\nphysicochemical reaction ``pharmacophores'', investigate experimental bias and\nderive strategies for mechanism detection and classification.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:57:07 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Poelking", "Carl", ""], ["Amar", "Yehia", ""], ["Lapkin", "Alexei", ""], ["Colwell", "Lucy", ""]]}, {"id": "1912.04370", "submitter": "Aparna Balagopalan", "authors": "Aparna Balagopalan, Jekaterina Novikova, Matthew B. A. McDermott, Bret\n  Nestor, Tristan Naumann, Marzyeh Ghassemi", "title": "Cross-Language Aphasia Detection using Optimal Transport Domain\n  Adaptation", "comments": "Accepted to ML4H at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-language speech datasets are scarce and often have small sample sizes\nin the medical domain. Robust transfer of linguistic features across languages\ncould improve rates of early diagnosis and therapy for speakers of low-resource\nlanguages when detecting health conditions from speech. We utilize\nout-of-domain, unpaired, single-speaker, healthy speech data for training\nmultiple Optimal Transport (OT) domain adaptation systems. We learn mappings\nfrom other languages to English and detect aphasia from linguistic\ncharacteristics of speech, and show that OT domain adaptation improves aphasia\ndetection over unilingual baselines for French (6% increased F1) and Mandarin\n(5% increased F1). Further, we show that adding aphasic data to the domain\nadaptation system significantly increases performance for both French and\nMandarin, increasing the F1 scores further (10% and 8% increase in F1 scores\nfor French and Mandarin, respectively, over unilingual baselines).\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:48:54 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""], ["McDermott", "Matthew B. A.", ""], ["Nestor", "Bret", ""], ["Naumann", "Tristan", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1912.04378", "submitter": "Ioannis Panageas", "authors": "Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and\n  Xiao Wang", "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the representational power of Deep Neural Networks (DNNs) and\nhow their structural properties (e.g., depth, width, type of activation unit)\naffect the functions they can compute, has been an important yet challenging\nquestion in deep learning and approximation theory. In a seminal paper,\nTelgarsky highlighted the benefits of depth by presenting a family of functions\n(based on simple triangular waves) for which DNNs achieve zero classification\nerror, whereas shallow networks with fewer than exponentially many nodes incur\nconstant error. Even though Telgarsky's work reveals the limitations of shallow\nneural networks, it does not inform us on why these functions are difficult to\nrepresent and in fact he states it as a tantalizing open question to\ncharacterize those functions that cannot be well-approximated by smaller\ndepths.\n  In this work, we point to a new connection between DNNs expressivity and\nSharkovsky's Theorem from dynamical systems, that enables us to characterize\nthe depth-width trade-offs of ReLU networks for representing functions based on\nthe presence of generalized notion of fixed points, called periodic points (a\nfixed point is a point of period 1). Motivated by our observation that the\ntriangle waves used in Telgarsky's work contain points of period 3 - a period\nthat is special in that it implies chaotic behavior based on the celebrated\nresult by Li-Yorke - we proceed to give general lower bounds for the width\nneeded to represent periodic functions as a function of the depth. Technically,\nthe crux of our approach is based on an eigenvalue analysis of the dynamical\nsystem associated with such functions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 21:11:02 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Chatziafratis", "Vaggos", ""], ["Nagarajan", "Sai Ganesh", ""], ["Panageas", "Ioannis", ""], ["Wang", "Xiao", ""]]}, {"id": "1912.04379", "submitter": "Jonathan Baxter", "authors": "Douglas Aberdeen and Jonathan Baxter", "title": "General Matrix-Matrix Multiplication Using SIMD features of the PIII", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.05181", "journal-ref": "Euro-Par '00 Proceedings from the 6th International Euro-Par\n  Conference on Parallel Processing (2000) Pages 980-983", "doi": "10.1007/3-540-44520-X_138", "report-no": null, "categories": "cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised matrix-matrix multiplication forms the kernel of many\nmathematical algorithms. A faster matrix-matrix multiply immediately benefits\nthese algorithms. In this paper we implement efficient matrix multiplication\nfor large matrices using the floating point Intel Pentium SIMD (Single\nInstruction Multiple Data) architecture. A description of the issues and our\nsolution is presented, paying attention to all levels of the memory hierarchy.\nOur results demonstrate an average performance of 2.09 times faster than the\nleading public domain matrix-matrix multiply routines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:45:56 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Aberdeen", "Douglas", ""], ["Baxter", "Jonathan", ""]]}, {"id": "1912.04381", "submitter": "Dolly Agarwal", "authors": "Dolly Agarwal, Jayant Gupchup, Nishant Baghel", "title": "A Dataset for measuring reading levels in India at scale", "comments": "5 pages, 3 figures, 3 Tables, Paper accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CY cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One out of four children in India are leaving grade eight without basic\nreading skills. Measuring the reading levels in a vast country like India poses\nsignificant hurdles. Recent advances in machine learning opens up the\npossibility of automating this task. However, the datasets of children's speech\nare not only rare but are primarily in English. To solve this assessment\nproblem and advance deep learning research in regional Indian languages, we\npresent the ASER dataset of children in the age group of 6-14. The dataset\nconsists of 5,301 subjects generating 81,330 labeled audio clips in Hindi,\nMarathi and English. These labels represent expert opinions on the child's\nability to read at a specified level. Using this dataset, we built a simple\nASR-based classifier. Early results indicate that we can achieve a prediction\naccuracy of 86% for the English language. Considering the ASER survey spans\nhalf a million subjects, this dataset can grow to those scales.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:06:22 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:57:21 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Agarwal", "Dolly", ""], ["Gupchup", "Jayant", ""], ["Baghel", "Nishant", ""]]}, {"id": "1912.04391", "submitter": "Harrison Nguyen", "authors": "Harrison Nguyen, Simon Luo, Fabio Ramos", "title": "Semi-supervised Learning Approach to Generate Neuroimaging Modalities\n  with Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) of the brain can come in the form of\ndifferent modalities such as T1-weighted and Fluid Attenuated Inversion\nRecovery (FLAIR) which has been used to investigate a wide range of\nneurological disorders. Current state-of-the-art models for brain tissue\nsegmentation and disease classification require multiple modalities for\ntraining and inference. However, the acquisition of all of these modalities are\nexpensive, time-consuming, inconvenient and the required modalities are often\nnot available. As a result, these datasets contain large amounts of\n\\emph{unpaired} data, where examples in the dataset do not contain all\nmodalities. On the other hand, there is smaller fraction of examples that\ncontain all modalities (\\emph{paired} data) and furthermore each modality is\nhigh dimensional when compared to number of datapoints. In this work, we\ndevelop a method to address these issues with semi-supervised learning in\ntranslating between two neuroimaging modalities. Our proposed model,\nSemi-Supervised Adversarial CycleGAN (SSA-CGAN), uses an adversarial loss to\nlearn from \\emph{unpaired} data points, cycle loss to enforce consistent\nreconstructions of the mappings and another adversarial loss to take advantage\nof \\emph{paired} data points. Our experiments demonstrate that our proposed\nframework produces an improvement in reconstruction error and reduced variance\nfor the pairwise translation of multiple modalities and is more robust to\nthermal noise when compared to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 21:53:27 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Nguyen", "Harrison", ""], ["Luo", "Simon", ""], ["Ramos", "Fabio", ""]]}, {"id": "1912.04408", "submitter": "Monimoy Bujarbaruah", "authors": "Monimoy Bujarbaruah, Charlott Vallon", "title": "Exploiting Model Sparsity in Adaptive MPC: A Compressed Sensing\n  Viewpoint", "comments": "Both authors contributed equally. arXiv admin note: text overlap with\n  arXiv:1804.09790", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an Adaptive Stochastic Model Predictive Control (MPC)\nstrategy for stable linear time-invariant systems in the presence of bounded\ndisturbances. We consider multi-input, multi-output systems that can be\nexpressed by a Finite Impulse Response (FIR) model. The parameters of the FIR\nmodel corresponding to each output are unknown but assumed sparse. We estimate\nthese parameters using the Recursive Least Squares algorithm. The estimates are\nthen improved using set-based bounds obtained by solving the Basis Pursuit\nDenoising [1] problem. Our approach is able to handle hard input constraints\nand probabilistic output constraints. Using tools from distributionally robust\noptimization, we reformulate the probabilistic output constraints as tractable\nconvex second-order cone constraints, which enables us to pose our MPC design\ntask as a convex optimization problem. The efficacy of the developed algorithm\nis highlighted with a thorough numerical example, where we demonstrate\nperformance gain over the counterpart algorithm of [2], which does not utilize\nthe sparsity information of the system impulse response parameters during\ncontrol design.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 22:34:45 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Bujarbaruah", "Monimoy", ""], ["Vallon", "Charlott", ""]]}, {"id": "1912.04427", "submitter": "Pedro Savarese", "authors": "Pedro Savarese and Hugo Silva and Michael Maire", "title": "Winning the Lottery with Continuous Sparsification", "comments": "Published as a conference paper at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for efficient, sparse deep neural network models is most\nprominently performed by pruning: training a dense, overparameterized network\nand removing parameters, usually via following a manually-crafted heuristic.\nAdditionally, the recent Lottery Ticket Hypothesis conjectures that, for a\ntypically-sized neural network, it is possible to find small sub-networks\nwhich, when trained from scratch on a comparable budget, match the performance\nof the original dense counterpart. We revisit fundamental aspects of pruning\nalgorithms, pointing out missing ingredients in previous approaches, and\ndevelop a method, Continuous Sparsification, which searches for sparse networks\nbased on a novel approximation of an intractable $\\ell_0$ regularization. We\ncompare against dominant heuristic-based methods on pruning as well as ticket\nsearch -- finding sparse subnetworks that can be successfully re-trained from\nan early iterate. Empirical results show that we surpass the state-of-the-art\nfor both objectives, across models and datasets, including VGG trained on\nCIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new\nstandard for pruning, Continuous Sparsification also offers fast parallel\nticket search, opening doors to new applications of the Lottery Ticket\nHypothesis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 00:30:34 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 00:55:36 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 23:58:27 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 11:53:22 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Savarese", "Pedro", ""], ["Silva", "Hugo", ""], ["Maire", "Michael", ""]]}, {"id": "1912.04439", "submitter": "Joonas J\\\"alk\\\"o", "authors": "Joonas J\\\"alk\\\"o, Eemil Lagerspetz, Jari Haukka, Sasu Tarkoma, Antti\n  Honkela, Samuel Kaski", "title": "Privacy-preserving data sharing via probabilistic modelling", "comments": null, "journal-ref": null, "doi": "10.1016/j.patter.2021.100271", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential privacy allows quantifying privacy loss resulting from accessing\nsensitive personal data. Repeated accesses to underlying data incur increasing\nloss. Releasing data as privacy-preserving synthetic data would avoid this\nlimitation, but would leave open the problem of designing what kind of\nsynthetic data. We propose formulating the problem of private data release\nthrough probabilistic modelling. This approach transforms the problem of\ndesigning the synthetic data into choosing a model for the data, allowing also\nincluding prior knowledge, which improves the quality of the synthetic data. We\ndemonstrate empirically, in an epidemiological study, that statistical\ndiscoveries can be reliably reproduced from the synthetic data. We expect the\nmethod to have broad use in creating high-quality anonymized data twins of key\ndata sets for research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:21:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:09:43 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 07:39:45 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 09:26:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["J\u00e4lk\u00f6", "Joonas", ""], ["Lagerspetz", "Eemil", ""], ["Haukka", "Jari", ""], ["Tarkoma", "Sasu", ""], ["Honkela", "Antti", ""], ["Kaski", "Samuel", ""]]}, {"id": "1912.04472", "submitter": "Daniel Brown", "authors": "Daniel S. Brown and Scott Niekum", "title": "Deep Bayesian Reward Learning from Preferences", "comments": "Workshop on Safety and Robustness in Decision Making at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inverse reinforcement learning (IRL) methods are ideal for safe\nimitation learning, as they allow a learning agent to reason about reward\nuncertainty and the safety of a learned policy. However, Bayesian IRL is\ncomputationally intractable for high-dimensional problems because each sample\nfrom the posterior requires solving an entire Markov Decision Process (MDP).\nWhile there exist non-Bayesian deep IRL methods, these methods typically infer\npoint estimates of reward functions, precluding rigorous safety and uncertainty\nanalysis. We propose Bayesian Reward Extrapolation (B-REX), a highly efficient,\npreference-based Bayesian reward learning algorithm that scales to\nhigh-dimensional, visual control tasks. Our approach uses successor feature\nrepresentations and preferences over demonstrations to efficiently generate\nsamples from the posterior distribution over the demonstrator's reward function\nwithout requiring an MDP solver. Using samples from the posterior, we\ndemonstrate how to calculate high-confidence bounds on policy performance in\nthe imitation learning setting, in which the ground-truth reward function is\nunknown. We evaluate our proposed approach on the task of learning to play\nAtari games via imitation learning from pixel inputs, with no access to the\ngame score. We demonstrate that B-REX learns imitation policies that are\ncompetitive with a state-of-the-art deep imitation learning method that only\nlearns a point estimate of the reward function. Furthermore, we demonstrate\nthat samples from the posterior generated via B-REX can be used to compute\nhigh-confidence performance bounds for a variety of evaluation policies. We\nshow that high-confidence performance bounds are useful for accurately ranking\ndifferent evaluation policies when the reward function is unknown. We also\ndemonstrate that high-confidence performance bounds may be useful for detecting\nreward hacking.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:29:51 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Brown", "Daniel S.", ""], ["Niekum", "Scott", ""]]}, {"id": "1912.04508", "submitter": "Mohammed Amer", "authors": "Mohammed Amer, Tom\\'as Maul", "title": "Reducing Catastrophic Forgetting in Modular Neural Networks by Dynamic\n  Information Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning is a very important step toward realizing robust autonomous\nartificial agents. Neural networks are the main engine of deep learning, which\nis the current state-of-the-art technique in formulating adaptive artificial\nintelligent systems. However, neural networks suffer from catastrophic\nforgetting when stressed with the challenge of continual learning. We\ninvestigate how to exploit modular topology in neural networks in order to\ndynamically balance the information load between different modules by routing\ninputs based on the information content in each module so that information\ninterference is minimized. Our dynamic information balancing (DIB) technique\nadapts a reinforcement learning technique to guide the routing of different\ninputs based on a reward signal derived from a measure of the information load\nin each module. Our empirical results show that DIB combined with elastic\nweight consolidation (EWC) regularization outperforms models with similar\ncapacity and EWC regularization across different task formulations and\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 05:41:44 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Amer", "Mohammed", ""], ["Maul", "Tom\u00e1s", ""]]}, {"id": "1912.04511", "submitter": "Quanquan Gu", "authors": "Pan Xu and Quanquan Gu", "title": "A Finite-Time Analysis of Q-Learning with Neural Network Function\n  Approximation", "comments": "22 pages, 1 table. This version simplifies the proof and improves the\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning with neural network function approximation (neural Q-learning for\nshort) is among the most prevalent deep reinforcement learning algorithms.\nDespite its empirical success, the non-asymptotic convergence rate of neural\nQ-learning remains virtually unknown. In this paper, we present a finite-time\nanalysis of a neural Q-learning algorithm, where the data are generated from a\nMarkov decision process and the action-value function is approximated by a deep\nReLU neural network. We prove that neural Q-learning finds the optimal policy\nwith $O(1/\\sqrt{T})$ convergence rate if the neural function approximator is\nsufficiently overparameterized, where $T$ is the number of iterations. To our\nbest knowledge, our result is the first finite-time analysis of neural\nQ-learning under non-i.i.d. data assumption.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 05:52:32 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 21:31:07 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1912.04521", "submitter": "Yige Zhang", "authors": "Yige Zhang, Aaron Yi Ding, Jorg Ott, Mingxuan Yuan, Jia Zeng, Kun\n  Zhang, Weixiong Rao", "title": "Transfer Learning-Based Outdoor Position Recovery with Telco Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telecommunication (Telco) outdoor position recovery aims to localize outdoor\nmobile devices by leveraging measurement report (MR) data. Unfortunately, Telco\nposition recovery requires sufficient amount of MR samples across different\nareas and suffers from high data collection cost. For an area with scarce MR\nsamples, it is hard to achieve good accuracy. In this paper, by leveraging the\nrecently developed transfer learning techniques, we design a novel Telco\nposition recovery framework, called TLoc, to transfer good models in the\ncarefully selected source domains (those fine-grained small subareas) to a\ntarget one which originally suffers from poor localization accuracy.\nSpecifically, TLoc introduces three dedicated components: 1) a new coordinate\nspace to divide an area of interest into smaller domains, 2) a similarity\nmeasurement to select best source domains, and 3) an adaptation of an existing\ntransfer learning approach. To the best of our knowledge, TLoc is the first\nframework that demonstrates the efficacy of applying transfer learning in the\nTelco outdoor position recovery. To exemplify, on the 2G GSM and 4G LTE MR\ndatasets in Shanghai, TLoc outperforms a nontransfer approach by 27.58% and\n26.12% less median errors, and further leads to 47.77% and 49.22% less median\nerrors than a recent fingerprinting approach NBL.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:09:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhang", "Yige", ""], ["Ding", "Aaron Yi", ""], ["Ott", "Jorg", ""], ["Yuan", "Mingxuan", ""], ["Zeng", "Jia", ""], ["Zhang", "Kun", ""], ["Rao", "Weixiong", ""]]}, {"id": "1912.04527", "submitter": "Francesca Baldini", "authors": "Francesca Baldini, Animashree Anandkumar, and Richard M. Murray", "title": "Learning Pose Estimation for UAV Autonomous Navigation andLanding Using\n  Visual-Inertial Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new learning approach for autonomous navigation\nand landing of an Unmanned-Aerial-Vehicle (UAV). We develop a multimodal fusion\nof deep neural architectures for visual-inertial odometry. We train the model\nin an end-to-end fashion to estimate the current vehicle pose from streams of\nvisual and inertial measurements. We first evaluate the accuracy of our\nestimation by comparing the prediction of the model to traditional algorithms\non the publicly available EuRoC MAV dataset. The results illustrate a $25 \\%$\nimprovement in estimation accuracy over the baseline. Finally, we integrate the\narchitecture in the closed-loop flight control system of Airsim - a plugin\nsimulator for Unreal Engine - and we provide simulation results for autonomous\nnavigation and landing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:37:30 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 11:18:55 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Baldini", "Francesca", ""], ["Anandkumar", "Animashree", ""], ["Murray", "Richard M.", ""]]}, {"id": "1912.04530", "submitter": "Kan Li PhD", "authors": "Kan Li and Jose C. Principe", "title": "No-Trick (Treat) Kernel Adaptive Filtering using Deterministic Features", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods form a powerful, versatile, and theoretically-grounded\nunifying framework to solve nonlinear problems in signal processing and machine\nlearning. The standard approach relies on the kernel trick to perform pairwise\nevaluations of a kernel function, which leads to scalability issues for large\ndatasets due to its linear and superlinear growth with respect to the training\ndata. A popular approach to tackle this problem, known as random Fourier\nfeatures (RFFs), samples from a distribution to obtain the data-independent\nbasis of a higher finite-dimensional feature space, where its dot product\napproximates the kernel function. Recently, deterministic, rather than random\nconstruction has been shown to outperform RFFs, by approximating the kernel in\nthe frequency domain using Gaussian quadrature. In this paper, we view the dot\nproduct of these explicit mappings not as an approximation, but as an\nequivalent positive-definite kernel that induces a new finite-dimensional\nreproducing kernel Hilbert space (RKHS). This opens the door to no-trick (NT)\nonline kernel adaptive filtering (KAF) that is scalable and robust. Random\nfeatures are prone to large variances in performance, especially for smaller\ndimensions. Here, we focus on deterministic feature-map construction based on\npolynomial-exact solutions and show their superiority over random\nconstructions. Without loss of generality, we apply this approach to classical\nadaptive filtering algorithms and validate the methodology to show that\ndeterministic features are faster to generate and outperform state-of-the-art\nkernel methods based on random Fourier features.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:39:59 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Li", "Kan", ""], ["Principe", "Jose C.", ""]]}, {"id": "1912.04533", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski, Feynman Liang and Michael W. Mahoney", "title": "Exact expressions for double descent and implicit regularization via\n  surrogate random design", "comments": "Minor typo corrections and clarifications; moved the proofs into the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double descent refers to the phase transition that is exhibited by the\ngeneralization error of unregularized learning models when varying the ratio\nbetween the number of parameters and the number of training samples. The recent\nsuccess of highly over-parameterized machine learning models such as deep\nneural networks has motivated a theoretical analysis of the double descent\nphenomenon in classical models such as linear regression which can also\ngeneralize well in the over-parameterized regime. We provide the first exact\nnon-asymptotic expressions for double descent of the minimum norm linear\nestimator. Our approach involves constructing a special determinantal point\nprocess which we call surrogate random design, to replace the standard i.i.d.\ndesign of the training sample. This surrogate design admits exact expressions\nfor the mean squared error of the estimator while preserving the key properties\nof the standard design. We also establish an exact implicit regularization\nresult for over-parameterized training samples. In particular, we show that,\nfor the surrogate design, the implicit bias of the unregularized minimum norm\nestimator precisely corresponds to solving a ridge-regularized least squares\nproblem on the population distribution. In our analysis we introduce a new\nmathematical tool of independent interest: the class of random matrices for\nwhich determinant commutes with expectation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:49:46 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:36:41 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 17:03:42 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Liang", "Feynman", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1912.04549", "submitter": "Ibrahim Yilmaz", "authors": "Ibrahim Yilmaz and Rahat Masum", "title": "Expansion of Cyber Attack Data From Unbalanced Datasets Using Generative\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques help to understand patterns of a dataset to\ncreate a defense mechanism against cyber attacks. However, it is difficult to\nconstruct a theoretical model due to the imbalances in the dataset for\ndiscriminating attacks from the overall dataset. Multilayer Perceptron (MLP)\ntechnique will provide improvement in accuracy and increase the performance of\ndetecting the attack and benign data from a balanced dataset. We have worked on\nthe UGR'16 dataset publicly available for this work. Data wrangling has been\ndone due to prepare test set from in the original set. We fed the neural\nnetwork classifier larger input to the neural network in an increasing manner\n(i.e. 10000, 50000, 1 million) to see the distribution of features over the\naccuracy. We have implemented a GAN model that can produce samples of different\nattack labels (e.g. blacklist, anomaly spam, ssh scan). We have been able to\ngenerate as many samples as necessary based on the data sample we have taken\nfrom the UGR'16. We have tested the accuracy of our model with the imbalance\ndataset initially and then with the increasing the attack samples and found\nimprovement of classification performance for the latter.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 07:33:17 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Yilmaz", "Ibrahim", ""], ["Masum", "Rahat", ""]]}, {"id": "1912.04556", "submitter": "Ahmad ALAbadleh", "authors": "Ahmad Abadleh", "title": "Accurate Entrance Position Detection Based on Wi-Fi and GPS Signals\n  Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at detecting an accurate position of the main entrance of the\nbuildings. The proposed approach relies on the fact that the GPS signals drop\nsignificantly when the user enters a building. Moreover, as most of the public\nbuildings provide Wi-Fi services, the Wi-Fi received signal strength (RSS) can\nbe utilized in order to detect the entrance of the buildings. The rationale\nbehind this paper is that the GPS signals decrease as the user gets close to\nthe main entrance and the Wi-Fi signal increases as the user approaches the\nmain entrance. Several real experiments have been conducted in order to\nguarantee the feasibility of the proposed approach. The experiment results have\nshown an interesting result and the accuracy of the whole system was one meter\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:02:19 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Abadleh", "Ahmad", ""]]}, {"id": "1912.04629", "submitter": "Thomas Berrett", "authors": "Thomas Berrett and Cristina Butucea", "title": "Classification under local differential privacy", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the binary classification problem in a setup that preserves the\nprivacy of the original sample. We provide a privacy mechanism that is locally\ndifferentially private and then construct a classifier based on the private\nsample that is universally consistent in Euclidean spaces. Under stronger\nassumptions, we establish the minimax rates of convergence of the excess risk\nand see that they are slower than in the case when the original sample is\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:37:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Berrett", "Thomas", ""], ["Butucea", "Cristina", ""]]}, {"id": "1912.04635", "submitter": "Alessandro Betti", "authors": "Alessandro Betti and Marco Gori", "title": "Backprop Diffusion is Biologically Plausible", "comments": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1907.05106", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Backpropagation algorithm relies on the abstraction of using a neural\nmodel that gets rid of the notion of time, since the input is mapped\ninstantaneously to the output. In this paper, we claim that this abstraction of\nignoring time, along with the abrupt input changes that occur when feeding the\ntraining set, are in fact the reasons why, in some papers, Backprop biological\nplausibility is regarded as an arguable issue. We show that as soon as a deep\nfeedforward network operates with neurons with time-delayed response, the\nbackprop weight update turns out to be the basic equation of a biologically\nplausible diffusion process based on forward-backward waves. We also show that\nsuch a process very well approximates the gradient for inputs that are not too\nfast with respect to the depth of the network. These remarks somewhat disclose\nthe diffusion process behind the backprop equation and leads us to interpret\nthe corresponding algorithm as a degeneration of a more general diffusion\nprocess that takes place also in neural networks with cyclic connections.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:50:15 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 10:04:48 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""]]}, {"id": "1912.04684", "submitter": "Martin Klauco", "authors": "Karol Ki\\v{s}, Martin Klau\\v{c}o", "title": "Neural Network Based Explicit MPC for Chemical Reactor Control", "comments": "Preprint submitted to Acta Chimica Slovaca, ISSN: 1339-3065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show the implementation of deep neural networks applied in\nprocess control. In our approach, we based the training of the neural network\non model predictive control. Model predictive control is popular for its\nability to be tuned by the weighting matrices and by the fact that it respects\nthe constraints. We present the neural network that can approximate the\nbehavior of the MPC in the way of mimicking the control input trajectory while\nthe constraints on states and control input remain unimpaired of the value of\nthe weighting matrices. This approach is demonstrated in a simulation case\nstudy involving a continuous stirred tank reactor, where multi-component\nchemical reaction takes place.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 13:44:48 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Ki\u0161", "Karol", ""], ["Klau\u010do", "Martin", ""]]}, {"id": "1912.04690", "submitter": "Angshul Majumdar Dr.", "authors": "Vanika Singhal and Angshul Majumdar", "title": "Reconstructing Multi-echo Magnetic Resonance Images via Structured Deep\n  Dictionary Learning", "comments": "Final version accepted at Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-echo magnetic resonance (MR) images are acquired by changing the echo\ntimes (for T2 weighted) or relaxation times (for T1 weighted) of scans. The\nresulting (multi-echo) images are usually used for quantitative MR imaging.\nAcquiring MR images is a slow process and acquiring multi scans of the same\ncross section for multi-echo imaging is even slower. In order to accelerate the\nscan, compressed sensing (CS) based techniques have been advocating partial\nK-space (Fourier domain) scans; the resulting images are reconstructed via\nstructured CS algorithms. In recent times, it has been shown that instead of\nusing off-the-shelf CS, better results can be obtained by adaptive\nreconstruction algorithms based on structured dictionary learning. In this\nwork, we show that the reconstruction results can be further improved by using\nstructured deep dictionaries. Experimental results on real datasets show that\nby using our proposed technique the scan-time can be cut by half compared to\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:01:17 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Singhal", "Vanika", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.04695", "submitter": "Zhuo Wang", "authors": "Zhuo Wang, Wei Zhang, Ning Liu, Jianyong Wang", "title": "Transparent Classification with Multilayer Logical Perceptrons and\n  Random Binarization", "comments": "AAAI-20 (oral presentation); source codes added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with transparent inner structure and high classification performance\nare required to reduce potential risk and provide trust for users in domains\nlike health care, finance, security, etc. However, existing models are hard to\nsimultaneously satisfy the above two properties. In this paper, we propose a\nnew hierarchical rule-based model for classification tasks, named Concept Rule\nSets (CRS), which has both a strong expressive ability and a transparent inner\nstructure. To address the challenge of efficiently learning the\nnon-differentiable CRS model, we propose a novel neural network architecture,\nMultilayer Logical Perceptron (MLLP), which is a continuous version of CRS.\nUsing MLLP and the Random Binarization (RB) method we proposed, we can search\nthe discrete solution of CRS in continuous space using gradient descent and\nensure the discrete CRS acts almost the same as the corresponding continuous\nMLLP. Experiments on 12 public data sets show that CRS outperforms the\nstate-of-the-art approaches and the complexity of the learned CRS is close to\nthe simple decision tree. Source code is available at\nhttps://github.com/12wang3/mllp.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:13:09 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 07:45:01 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Zhuo", ""], ["Zhang", "Wei", ""], ["Liu", "Ning", ""], ["Wang", "Jianyong", ""]]}, {"id": "1912.04734", "submitter": "Angshul Majumdar Dr.", "authors": "Jyoti Maggu, Angshul Majumdar and Emilie Chouzenoux", "title": "Transformed Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering assumes that the data is sepa-rable into separate\nsubspaces. Such a simple as-sumption, does not always hold. We assume that,\neven if the raw data is not separable into subspac-es, one can learn a\nrepresentation (transform coef-ficients) such that the learnt representation is\nsep-arable into subspaces. To achieve the intended goal, we embed subspace\nclustering techniques (locally linear manifold clustering, sparse sub-space\nclustering and low rank representation) into transform learning. The entire\nformulation is jointly learnt; giving rise to a new class of meth-ods called\ntransformed subspace clustering (TSC). In order to account for non-linearity,\nker-nelized extensions of TSC are also proposed. To test the performance of the\nproposed techniques, benchmarking is performed on image clustering and document\nclustering datasets. Comparison with state-of-the-art clustering techniques\nshows that our formulation improves upon them.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:57:14 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Maggu", "Jyoti", ""], ["Majumdar", "Angshul", ""], ["Chouzenoux", "Emilie", ""]]}, {"id": "1912.04738", "submitter": "Hanyuan Hang", "authors": "Hanyuan Hang, Zhouchen Lin, Xiaoyu Liu, Hongwei Wen", "title": "Histogram Transform Ensembles for Large-scale Regression", "comments": "arXiv admin note: text overlap with arXiv:1911.11581", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for large-scale regression problems named\nhistogram transform ensembles (HTE), composed of random rotations, stretchings,\nand translations. First of all, we investigate the theoretical properties of\nHTE when the regression function lies in the H\\\"{o}lder space $C^{k,\\alpha}$,\n$k \\in \\mathbb{N}_0$, $\\alpha \\in (0,1]$. In the case that $k=0, 1$, we adopt\nthe constant regressors and develop the na\\\"{i}ve histogram transforms (NHT).\nWithin the space $C^{0,\\alpha}$, although almost optimal convergence rates can\nbe derived for both single and ensemble NHT, we fail to show the benefits of\nensembles over single estimators theoretically. In contrast, in the subspace\n$C^{1,\\alpha}$, we prove that if $d \\geq 2(1+\\alpha)/\\alpha$, the lower bound\nof the convergence rates for single NHT turns out to be worse than the upper\nbound of the convergence rates for ensemble NHT. In the other case when $k \\geq\n2$, the NHT may no longer be appropriate in predicting smoother regression\nfunctions. Instead, we apply kernel histogram transforms (KHT) equipped with\nsmoother regressors such as support vector machines (SVMs), and it turns out\nthat both single and ensemble KHT enjoy almost optimal convergence rates. Then\nwe validate the above theoretical results by numerical experiments. On the one\nhand, simulations are conducted to elucidate that ensemble NHT outperform\nsingle NHT. On the other hand, the effects of bin sizes on accuracy of both NHT\nand KHT also accord with theoretical analysis. Last but not least, in the\nreal-data experiments, comparisons between the ensemble KHT, equipped with\nadaptive histogram transforms, and other state-of-the-art large-scale\nregression estimators verify the effectiveness and accuracy of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 16:39:02 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hang", "Hanyuan", ""], ["Lin", "Zhouchen", ""], ["Liu", "Xiaoyu", ""], ["Wen", "Hongwei", ""]]}, {"id": "1912.04747", "submitter": "Amir Farzad", "authors": "Amir Farzad and T. Aaron Gulliver", "title": "Oversampling Log Messages Using a Sequence Generative Adversarial\n  Network for Anomaly Detection and Classification", "comments": "14 pages, 4 figures, 2 tables", "journal-ref": "International Conference on Artificial Intelligence and Machine\n  Learning, 10 (2020), 163-175", "doi": "10.5121/csit.2020.100515", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with imbalanced data is one of the main challenges in machine/deep\nlearning algorithms for classification. This issue is more important with log\nmessage data as it is typically very imbalanced and negative logs are rare. In\nthis paper, a model is proposed to generate text log messages using a SeqGAN\nnetwork. Then features are extracted using an Autoencoder and anomaly detection\nis done using a GRU network. The proposed model is evaluated with two\nimbalanced log data sets, namely BGL and Openstack. Results are presented which\nshow that oversampling and balancing data increases the accuracy of anomaly\ndetection and classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:00:52 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 05:54:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Farzad", "Amir", ""], ["Gulliver", "T. Aaron", ""]]}, {"id": "1912.04754", "submitter": "Angshul Majumdar Dr.", "authors": "Aanchal Mongia, Neha Jhamb, Emilie Chouzenoux and Angshul Majumdar", "title": "Deep Latent Factor Model for Collaborative Filtering", "comments": "This is an initial draft of the accepted paper at Elsevier Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models have been used widely in collaborative filtering based\nrecommender systems. In recent years, deep learning has been successful in\nsolving a wide variety of machine learning problems. Motivated by the success\nof deep learning, we propose a deeper version of latent factor model.\nExperiments on benchmark datasets shows that our proposed technique\nsignificantly outperforms all state-of-the-art collaborative filtering\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:16:06 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mongia", "Aanchal", ""], ["Jhamb", "Neha", ""], ["Chouzenoux", "Emilie", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.04783", "submitter": "Xavier Boix", "authors": "Stephen Casper, Xavier Boix, Vanessa D'Amario, Ling Guo, Martin\n  Schrimpf, Kasper Vinken, Gabriel Kreiman", "title": "Frivolous Units: Wider Networks Are Not Really That Wide", "comments": null, "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A remarkable characteristic of overparameterized deep neural networks (DNNs)\nis that their accuracy does not degrade when the network's width is increased.\nRecent evidence suggests that developing compressible representations is key\nfor adjusting the complexity of large networks to the learning task at hand.\nHowever, these compressible representations are poorly understood. A promising\nstrand of research inspired from biology is understanding representations at\nthe unit level as it offers a more granular and intuitive interpretation of the\nneural mechanisms. In order to better understand what facilitates increases in\nwidth without decreases in accuracy, we ask: Are there mechanisms at the unit\nlevel by which networks control their effective complexity as their width is\nincreased? If so, how do these depend on the architecture, dataset, and\ntraining parameters? We identify two distinct types of \"frivolous\" units that\nproliferate when the network's width is increased: prunable units which can be\ndropped out of the network without significant change to the output and\nredundant units whose activities can be expressed as a linear combination of\nothers. These units imply complexity constraints as the function the network\nrepresents could be expressed by a network without them. We also identify how\nthe development of these units can be influenced by architecture and a number\nof training factors. Together, these results help to explain why the accuracy\nof DNNs does not degrade when width is increased and highlight the importance\nof frivolous units toward understanding implicit regularization in DNNs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:53:45 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 19:41:16 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 16:20:23 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 02:56:07 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 23:42:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Casper", "Stephen", ""], ["Boix", "Xavier", ""], ["D'Amario", "Vanessa", ""], ["Guo", "Ling", ""], ["Schrimpf", "Martin", ""], ["Vinken", "Kasper", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1912.04792", "submitter": "Chen Liu", "authors": "Chen Liu, Mathieu Salzmann, Sabine S\\\"usstrunk", "title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training certifiable neural networks enables one to obtain models with\nrobustness guarantees against adversarial attacks. In this work, we introduce a\nframework to bound the adversary-free region in the neighborhood of the input\ndata by a polyhedral envelope, which yields finer-grained certified robustness.\nWe further introduce polyhedral envelope regularization (PER) to encourage\nlarger polyhedral envelopes and thus improve the provable robustness of the\nmodels. We demonstrate the flexibility and effectiveness of our framework on\nstandard benchmarks; it applies to networks of different architectures and\ngeneral activation functions. Compared with the state-of-the-art methods, PER\nhas very little computational overhead and better robustness guarantees without\nover-regularizing the model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:05:20 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 20:46:25 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Liu", "Chen", ""], ["Salzmann", "Mathieu", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1912.04825", "submitter": "Samuel Kim", "authors": "Samuel Kim, Peter Y. Lu, Srijon Mukherjee, Michael Gilbert, Li Jing,\n  Vladimir \\v{C}eperi\\'c, and Marin Solja\\v{c}i\\'c", "title": "Integration of Neural Network-Based Symbolic Regression in Deep Learning\n  for Scientific Discovery", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression is a powerful technique that can discover analytical\nequations that describe data, which can lead to explainable models and\ngeneralizability outside of the training data set. In contrast, neural networks\nhave achieved amazing levels of accuracy on image recognition and natural\nlanguage processing tasks, but are often seen as black-box models that are\ndifficult to interpret and typically extrapolate poorly. Here we use a neural\nnetwork-based architecture for symbolic regression called the Equation Learner\n(EQL) network and integrate it with other deep learning architectures such that\nthe whole system can be trained end-to-end through backpropagation. To\ndemonstrate the power of such systems, we study their performance on several\nsubstantially different tasks. First, we show that the neural network can\nperform symbolic regression and learn the form of several functions. Next, we\npresent an MNIST arithmetic task where a separate part of the neural network\nextracts the digits. Finally, we demonstrate prediction of dynamical systems\nwhere an unknown parameter is extracted through an encoder. We find that the\nEQL-based architecture can extrapolate quite well outside of the training data\nset compared to a standard neural network-based architecture, paving the way\nfor deep learning to be applied in scientific exploration and discovery.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:07:52 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 18:40:43 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kim", "Samuel", ""], ["Lu", "Peter Y.", ""], ["Mukherjee", "Srijon", ""], ["Gilbert", "Michael", ""], ["Jing", "Li", ""], ["\u010ceperi\u0107", "Vladimir", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "1912.04832", "submitter": "Lukas Pfannschmidt", "authors": "Lukas Pfannschmidt, Jonathan Jakob, Fabian Hinder, Michael Biehl,\n  Peter Tino, Barbara Hammer", "title": "Feature Relevance Determination for Ordinal Regression in the Context of\n  Feature Redundancies and Privileged Information", "comments": "Preprint accepted at Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.12.133", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in machine learning technologies have led to increasingly powerful\nmodels in particular in the context of big data. Yet, many application\nscenarios demand for robustly interpretable models rather than optimum model\naccuracy; as an example, this is the case if potential biomarkers or causal\nfactors should be discovered based on a set of given measurements. In this\ncontribution, we focus on feature selection paradigms, which enable us to\nuncover relevant factors of a given regularity based on a sparse model. We\nfocus on the important specific setting of linear ordinal regression, i.e.\\\ndata have to be ranked into one of a finite number of ordered categories by a\nlinear projection. Unlike previous work, we consider the case that features are\npotentially redundant, such that no unique minimum set of relevant features\nexists. We aim for an identification of all strongly and all weakly relevant\nfeatures as well as their type of relevance (strong or weak); we achieve this\ngoal by determining feature relevance bounds, which correspond to the minimum\nand maximum feature relevance, respectively, if searched over all equivalent\nmodels. In addition, we discuss how this setting enables us to substitute some\nof the features, e.g.\\ due to their semantics, and how to extend the framework\nof feature relevance intervals to the setting of privileged information, i.e.\\\npotentially relevant information is available for training purposes only, but\ncannot be used for the prediction itself.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:20:18 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Pfannschmidt", "Lukas", ""], ["Jakob", "Jonathan", ""], ["Hinder", "Fabian", ""], ["Biehl", "Michael", ""], ["Tino", "Peter", ""], ["Hammer", "Barbara", ""]]}, {"id": "1912.04838", "submitter": "Pei Sun", "authors": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard,\n  Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin\n  Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev,\n  Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang\n  Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov", "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research community has increasing interest in autonomous driving\nresearch, despite the resource intensity of obtaining representative real world\ndata. Existing self-driving datasets are limited in the scale and variation of\nthe environments they capture, even though generalization within and between\noperating regions is crucial to the overall viability of the technology. In an\neffort to help align the research community's contributions with real-world\nself-driving problems, we introduce a new large scale, high quality, diverse\ndataset. Our new dataset consists of 1150 scenes that each span 20 seconds,\nconsisting of well synchronized and calibrated high quality LiDAR and camera\ndata captured across a range of urban and suburban geographies. It is 15x more\ndiverse than the largest camera+LiDAR dataset available based on our proposed\ndiversity metric. We exhaustively annotated this data with 2D (camera image)\nand 3D (LiDAR) bounding boxes, with consistent identifiers across frames.\nFinally, we provide strong baselines for 2D as well as 3D detection and\ntracking tasks. We further study the effects of dataset size and generalization\nacross geographies on 3D detection methods. Find data, code and more up-to-date\ninformation at http://www.waymo.com/open.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:28:55 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 17:51:21 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 19:21:41 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 00:42:38 GMT"}, {"version": "v5", "created": "Wed, 18 Dec 2019 01:41:49 GMT"}, {"version": "v6", "created": "Mon, 30 Mar 2020 03:22:30 GMT"}, {"version": "v7", "created": "Tue, 12 May 2020 23:28:05 GMT"}], "update_date": "2020-05-24", "authors_parsed": [["Sun", "Pei", ""], ["Kretzschmar", "Henrik", ""], ["Dotiwalla", "Xerxes", ""], ["Chouard", "Aurelien", ""], ["Patnaik", "Vijaysai", ""], ["Tsui", "Paul", ""], ["Guo", "James", ""], ["Zhou", "Yin", ""], ["Chai", "Yuning", ""], ["Caine", "Benjamin", ""], ["Vasudevan", "Vijay", ""], ["Han", "Wei", ""], ["Ngiam", "Jiquan", ""], ["Zhao", "Hang", ""], ["Timofeev", "Aleksei", ""], ["Ettinger", "Scott", ""], ["Krivokon", "Maxim", ""], ["Gao", "Amy", ""], ["Joshi", "Aditya", ""], ["Zhao", "Sheng", ""], ["Cheng", "Shuyang", ""], ["Zhang", "Yu", ""], ["Shlens", "Jonathon", ""], ["Chen", "Zhifeng", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1912.04845", "submitter": "Stefan Oehmcke", "authors": "Vinnie Ko, Stefan Oehmcke, Fabian Gieseke", "title": "Magnitude and Uncertainty Pruning Criterion for Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have achieved dramatic improvements in recent years and\ndepict the state-of-the-art methods for many real-world tasks nowadays. One\ndrawback is, however, that many of these models are overparameterized, which\nmakes them both computationally and memory intensive. Furthermore,\noverparameterization can also lead to undesired overfitting side-effects.\nInspired by recently proposed magnitude-based pruning schemes and the Wald test\nfrom the field of statistics, we introduce a novel magnitude and uncertainty\n(M&U) pruning criterion that helps to lessen such shortcomings. One important\nadvantage of our M&U pruning criterion is that it is scale-invariant, a\nphenomenon that the magnitude-based pruning criterion suffers from. In\naddition, we present a ``pseudo bootstrap'' scheme, which can efficiently\nestimate the uncertainty of the weights by using their update information\nduring training. Our experimental evaluation, which is based on various neural\nnetwork architectures and datasets, shows that our new criterion leads to more\ncompressed models compared to models that are solely based on magnitude-based\npruning criteria, with, at the same time, less loss in predictive power.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:35:23 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Ko", "Vinnie", ""], ["Oehmcke", "Stefan", ""], ["Gieseke", "Fabian", ""]]}, {"id": "1912.04862", "submitter": "Mamikon Gulian", "authors": "Eric C. Cyr, Mamikon A. Gulian, Ravi G. Patel, Mauro Perego, and\n  Nathaniel A. Trask", "title": "Robust Training and Initialization of Deep Neural Networks: An Adaptive\n  Basis Viewpoint", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the gap between theoretical optimal approximation rates of deep\nneural networks (DNNs) and the accuracy realized in practice, we seek to\nimprove the training of DNNs. The adoption of an adaptive basis viewpoint of\nDNNs leads to novel initializations and a hybrid least squares/gradient descent\noptimizer. We provide analysis of these techniques and illustrate via numerical\nexamples dramatic increases in accuracy and convergence rate for benchmarks\ncharacterizing scientific applications where DNNs are currently used, including\nregression problems and physics-informed neural networks for the solution of\npartial differential equations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 18:04:03 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Cyr", "Eric C.", ""], ["Gulian", "Mamikon A.", ""], ["Patel", "Ravi G.", ""], ["Perego", "Mauro", ""], ["Trask", "Nathaniel A.", ""]]}, {"id": "1912.04871", "submitter": "Brenden Petersen", "authors": "Brenden K. Petersen, Mikel Landajuela Larma, T. Nathan Mundhenk,\n  Claudio P. Santiago, Soo K. Kim, Joanne T. Kim", "title": "Deep symbolic regression: Recovering mathematical expressions from data\n  via risk-seeking policy gradients", "comments": "Published at International Conference on Learning Representations,\n  2021", "journal-ref": "International Conference on Learning Representations, 2021", "doi": null, "report-no": "LLNL-CONF-790457", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the underlying mathematical expressions describing a dataset is a\ncore challenge for artificial intelligence. This is the problem of\n$\\textit{symbolic regression}$. Despite recent advances in training neural\nnetworks to solve complex tasks, deep learning approaches to symbolic\nregression are underexplored. We propose a framework that leverages deep\nlearning for symbolic regression via a simple idea: use a large model to search\nthe space of small models. Specifically, we use a recurrent neural network to\nemit a distribution over tractable mathematical expressions and employ a novel\nrisk-seeking policy gradient to train the network to generate better-fitting\nexpressions. Our algorithm outperforms several baseline methods (including\nEureqa, the gold standard for symbolic regression) in its ability to exactly\nrecover symbolic expressions on a series of benchmark problems, both with and\nwithout added noise. More broadly, our contributions include a framework that\ncan be applied to optimize hierarchical, variable-length objects under a\nblack-box performance metric, with the ability to incorporate constraints in\nsitu, and a risk-seeking policy gradient formulation that optimizes for\nbest-case performance instead of expected performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 18:25:48 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 17:16:24 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 04:00:32 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 22:29:16 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Petersen", "Brenden K.", ""], ["Larma", "Mikel Landajuela", ""], ["Mundhenk", "T. Nathan", ""], ["Santiago", "Claudio P.", ""], ["Kim", "Soo K.", ""], ["Kim", "Joanne T.", ""]]}, {"id": "1912.04884", "submitter": "Benjie Wang", "authors": "Benjie Wang, Stefan Webb, Tom Rainforth", "title": "Statistically Robust Neural Network Classification", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been much interest in quantifying the robustness of neural\nnetwork classifiers through adversarial risk metrics. However, for problems\nwhere test-time corruptions occur in a probabilistic manner, rather than being\ngenerated by an explicit adversary, adversarial metrics typically do not\nprovide an accurate or reliable indicator of robustness. To address this, we\nintroduce a statistically robust risk (SRR) framework which measures robustness\nin expectation over both network inputs and a corruption distribution. Unlike\nmany adversarial risk metrics, which typically require separate applications on\na point-by-point basis, the SRR can easily be directly estimated for an entire\nnetwork and used as a training objective in a stochastic gradient scheme.\nFurthermore, we show both theoretically and empirically that it can scale to\nhigher-dimensional networks by providing superior generalization performance\ncompared with comparable adversarial risks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 18:47:35 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 01:30:46 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Benjie", ""], ["Webb", "Stefan", ""], ["Rainforth", "Tom", ""]]}, {"id": "1912.04946", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch", "title": "Frequentist Consistency of Generalized Variational Inference", "comments": "31 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Frequentist consistency properties of the posterior\ndistributions constructed via Generalized Variational Inference (GVI). A number\nof generic and novel strategies are given for proving consistency, relying on\nthe theory of $\\Gamma$-convergence. Specifically, this paper shows that under\nminimal regularity conditions, the sequence of GVI posteriors is consistent and\ncollapses to a point mass at the population-optimal parameter value as the\nnumber of observations goes to infinity. The results extend to the latent\nvariable case without additional assumptions and hold under misspecification.\nLastly, the paper explains how to apply the results to a selection of GVI\nposteriors with especially popular variational families. For example,\nconsistency is established for GVI methods using the mean field normal\nvariational family, normal mixtures, Gaussian process variational families as\nwell as neural networks indexing a normal (mixture) distribution.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:38:35 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Knoblauch", "Jeremias", ""]]}, {"id": "1912.04958", "submitter": "Samuli Laine", "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko\n  Lehtinen, Timo Aila", "title": "Analyzing and Improving the Image Quality of StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:44:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:21:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Karras", "Tero", ""], ["Laine", "Samuli", ""], ["Aittala", "Miika", ""], ["Hellsten", "Janne", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "1912.04961", "submitter": "Sai Prabhakar Pandi Selvaraj", "authors": "Sai P. Selvaraj, Sandeep Konam", "title": "Medication Regimen Extraction From Medical Conversations", "comments": "Proceedings of International Workshop on Health Intelligence\n  (W3PHIAI) of the 34th AAAI Conference on Artificial Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relevant information from medical conversations and providing it\nto doctors and patients might help in addressing doctor burnout and patient\nforgetfulness. In this paper, we focus on extracting the Medication Regimen\n(dosage and frequency for medications) discussed in a medical conversation. We\nframe the problem as a Question Answering (QA) task and perform comparative\nanalysis over: a QA approach, a new combined QA and Information Extraction\napproach, and other baselines. We use a small corpus of 6,692 annotated\ndoctor-patient conversations for the task. Clinical conversation corpora are\ncostly to create, difficult to handle (because of data privacy concerns), and\nthus scarce. We address this data scarcity challenge through data augmentation\nmethods, using publicly available embeddings and pretrain part of the network\non a related task (summarization) to improve the model's performance. Compared\nto the baseline, our best-performing models improve the dosage and frequency\nextractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94,\nrespectively. Using our best-performing model, we present the first fully\nautomated system that can extract Medication Regimen tags from spontaneous\ndoctor-patient conversations with about $\\approx$71% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:18:39 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 04:03:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 18:04:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Selvaraj", "Sai P.", ""], ["Konam", "Sandeep", ""]]}, {"id": "1912.04968", "submitter": "David Ahmedt-Aristizabal", "authors": "David Ahmedt-Aristizabal, Tharindu Fernando, Simon Denman, Lars\n  Petersson, Matthew J. Aburn, Clinton Fookes", "title": "Neural Memory Networks for Seizure Type Classification", "comments": "Proceedings of the IEEE International Conference of Engineering in\n  Medicine and Biology Society. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of seizure type is a key step in the clinical process for\nevaluating an individual who presents with seizures. It determines the course\nof clinical diagnosis and treatment, and its impact stretches beyond the\nclinical domain to epilepsy research and the development of novel therapies.\nAutomated identification of seizure type may facilitate understanding of the\ndisease, and seizure detection and prediction has been the focus of recent\nresearch that has sought to exploit the benefits of machine learning and deep\nlearning architectures. Nevertheless, there is not yet a definitive solution\nfor automating the classification of seizure type, a task that must currently\nbe performed by an expert epileptologist. Inspired by recent advances in neural\nmemory networks (NMNs), we introduce a novel approach for the classification of\nseizure type using electrophysiological data. We first explore the performance\nof traditional deep learning techniques which use convolutional and recurrent\nneural networks, and enhance these architectures by using external memory\nmodules with trainable neural plasticity. We show that our model achieves a\nstate-of-the-art weighted F1 score of 0.945 for seizure type classification on\nthe TUH EEG Seizure Corpus with the IBM TUSZ preprocessed data. This work\nhighlights the potential of neural memory networks to support the field of\nepilepsy research, along with biomedical research and signal analysis more\nbroadly.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:27:40 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 02:04:44 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ahmedt-Aristizabal", "David", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Petersson", "Lars", ""], ["Aburn", "Matthew J.", ""], ["Fookes", "Clinton", ""]]}, {"id": "1912.04973", "submitter": "Debasmit Das", "authors": "Debasmit Das and C. S. George Lee", "title": "A Two-Stage Approach to Few-Shot Learning for Image Recognition", "comments": "To Appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2959254", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-layer neural network structure for few-shot image\nrecognition of novel categories. The proposed multi-layer neural network\narchitecture encodes transferable knowledge extracted from a large annotated\ndataset of base categories. This architecture is then applied to novel\ncategories containing only a few samples. The transfer of knowledge is carried\nout at the feature-extraction and the classification levels distributed across\nthe two training stages. In the first-training stage, we introduce the relative\nfeature to capture the structure of the data as well as obtain a\nlow-dimensional discriminative space. Secondly, we account for the variable\nvariance of different categories by using a network to predict the variance of\neach class. Classification is then performed by computing the Mahalanobis\ndistance to the mean-class representation in contrast to previous approaches\nthat used the Euclidean distance. In the second-training stage, a\ncategory-agnostic mapping is learned from the mean-sample representation to its\ncorresponding class-prototype representation. This is because the mean-sample\nrepresentation may not accurately represent the novel category prototype.\nFinally, we evaluate the proposed network structure on four standard few-shot\nimage recognition datasets, where our proposed few-shot learning system\nproduces competitive performance compared to previous work. We also extensively\nstudied and analyzed the contribution of each component of our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:45:35 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Das", "Debasmit", ""], ["Lee", "C. S. George", ""]]}, {"id": "1912.04977", "submitter": "Peter Kairouz", "authors": "Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\\'elien Bellet,\n  Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham\n  Cormode, Rachel Cummings, Rafael G.L. D'Oliveira, Hubert Eichner, Salim El\n  Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\\`a Gasc\\'on, Badih\n  Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie\n  He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,\n  Gauri Joshi, Mikhail Khodak, Jakub Kone\\v{c}n\\'y, Aleksandra Korolova,\n  Farinaz Koushanfar, Sanmi Koyejo, Tancr\\`ede Lepoint, Yang Liu, Prateek\n  Mittal, Mehryar Mohri, Richard Nock, Ayfer \\\"Ozg\\\"ur, Rasmus Pagh, Mariana\n  Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song,\n  Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\\`er,\n  Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu,\n  Han Yu, Sen Zhao", "title": "Advances and Open Problems in Federated Learning", "comments": "Published in Foundations and Trends in Machine Learning Vol 4 Issue\n  1. See: https://www.nowpublishers.com/article/Details/MAL-083", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a machine learning setting where many clients\n(e.g. mobile devices or whole organizations) collaboratively train a model\nunder the orchestration of a central server (e.g. service provider), while\nkeeping the training data decentralized. FL embodies the principles of focused\ndata collection and minimization, and can mitigate many of the systemic privacy\nrisks and costs resulting from traditional, centralized machine learning and\ndata science approaches. Motivated by the explosive growth in FL research, this\npaper discusses recent advances and presents an extensive collection of open\nproblems and challenges.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:55:41 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 06:20:24 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 03:03:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kairouz", "Peter", ""], ["McMahan", "H. Brendan", ""], ["Avent", "Brendan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Bennis", "Mehdi", ""], ["Bhagoji", "Arjun Nitin", ""], ["Bonawitz", "Kallista", ""], ["Charles", "Zachary", ""], ["Cormode", "Graham", ""], ["Cummings", "Rachel", ""], ["D'Oliveira", "Rafael G. L.", ""], ["Eichner", "Hubert", ""], ["Rouayheb", "Salim El", ""], ["Evans", "David", ""], ["Gardner", "Josh", ""], ["Garrett", "Zachary", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Ghazi", "Badih", ""], ["Gibbons", "Phillip B.", ""], ["Gruteser", "Marco", ""], ["Harchaoui", "Zaid", ""], ["He", "Chaoyang", ""], ["He", "Lie", ""], ["Huo", "Zhouyuan", ""], ["Hutchinson", "Ben", ""], ["Hsu", "Justin", ""], ["Jaggi", "Martin", ""], ["Javidi", "Tara", ""], ["Joshi", "Gauri", ""], ["Khodak", "Mikhail", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Korolova", "Aleksandra", ""], ["Koushanfar", "Farinaz", ""], ["Koyejo", "Sanmi", ""], ["Lepoint", "Tancr\u00e8de", ""], ["Liu", "Yang", ""], ["Mittal", "Prateek", ""], ["Mohri", "Mehryar", ""], ["Nock", "Richard", ""], ["\u00d6zg\u00fcr", "Ayfer", ""], ["Pagh", "Rasmus", ""], ["Raykova", "Mariana", ""], ["Qi", "Hang", ""], ["Ramage", "Daniel", ""], ["Raskar", "Ramesh", ""], ["Song", "Dawn", ""], ["Song", "Weikang", ""], ["Stich", "Sebastian U.", ""], ["Sun", "Ziteng", ""], ["Suresh", "Ananda Theertha", ""], ["Tram\u00e8r", "Florian", ""], ["Vepakomma", "Praneeth", ""], ["Wang", "Jianyu", ""], ["Xiong", "Li", ""], ["Xu", "Zheng", ""], ["Yang", "Qiang", ""], ["Yu", "Felix X.", ""], ["Yu", "Han", ""], ["Zhao", "Sen", ""]]}, {"id": "1912.04981", "submitter": "Tobias Uelwer", "authors": "Tobias Uelwer, Alexander Oberstra{\\ss}, Stefan Harmeling", "title": "Phase Retrieval Using Conditional Generative Adversarial Networks", "comments": "Accepted at the 25th International Conference on Pattern Recognition\n  2020 (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the application of conditional generative\nadversarial networks to solve various phase retrieval problems. We show that\nincluding knowledge of the measurement process at training time leads to an\noptimization at test time that is more robust to initialization than existing\napproaches involving generative models. In addition, conditioning the generator\nnetwork on the measurements enables us to achieve much more detailed results.\nWe empirically demonstrate that these advantages provide meaningful solutions\nto the Fourier and the compressive phase retrieval problem and that our method\noutperforms well-established projection-based methods as well as existing\nmethods that are based on neural networks. Like other deep learning methods,\nour approach is very robust to noise and can therefore be very useful for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 21:03:59 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 07:37:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Uelwer", "Tobias", ""], ["Oberstra\u00df", "Alexander", ""], ["Harmeling", "Stefan", ""]]}, {"id": "1912.04994", "submitter": "Changlong Wu", "authors": "Changlong Wu, Narayana Prasad Santhanam", "title": "Almost Uniform Sampling From Neural Networks", "comments": "Submitted to 54th Annual Conference on Information Sciences and\n  Systems (CISS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a length $n$ sample from $\\mathbb{R}^d$ and a neural network with a\nfixed architecture with $W$ weights, $k$ neurons, linear threshold activation\nfunctions, and binary outputs on each neuron, we study the problem of uniformly\nsampling from all possible labelings on the sample corresponding to different\nchoices of weights. We provide an algorithm that runs in time polynomial both\nin $n$ and $W$ such that any labeling appears with probability at least\n$\\left(\\frac{W}{2ekn}\\right)^W$ for $W<n$. For a single neuron, we also provide\na random walk based algorithm that samples exactly uniformly.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 21:40:34 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wu", "Changlong", ""], ["Santhanam", "Narayana Prasad", ""]]}, {"id": "1912.05007", "submitter": "Alexander Ziller", "authors": "Alexander Ziller, Julius Hansjakob, Vitalii Rusinov, Daniel Z\\\"ugner,\n  Peter Vogel, Stephan G\\\"unnemann", "title": "Oktoberfest Food Dataset", "comments": "Dataset publication of Oktoberfest Food Dataset. 4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We release a realistic, diverse, and challenging dataset for object detection\non images. The data was recorded at a beer tent in Germany and consists of 15\ndifferent categories of food and drink items. We created more than 2,500 object\nannotations by hand for 1,110 images captured by a video camera above the\ncheckout. We further make available the remaining 600GB of (unlabeled) data\ncontaining days of footage. Additionally, we provide our trained models as a\nbenchmark. Possible applications include automated checkout systems which could\nsignificantly speed up the process.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:28:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ziller", "Alexander", ""], ["Hansjakob", "Julius", ""], ["Rusinov", "Vitalii", ""], ["Z\u00fcgner", "Daniel", ""], ["Vogel", "Peter", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1912.05021", "submitter": "Xiao Yang", "authors": "Xiao Yang, Fangyun Wei, Hongyang Zhang, Jun Zhu", "title": "Design and Interpretation of Universal Adversarial Patches in Face\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider universal adversarial patches for faces -- small visual elements\nwhose addition to a face image reliably destroys the performance of face\ndetectors. Unlike previous work that mostly focused on the algorithmic design\nof adversarial examples in terms of improving the success rate as an attacker,\nin this work we show an interpretation of such patches that can prevent the\nstate-of-the-art face detectors from detecting the real faces. We investigate a\nphenomenon: patches designed to suppress real face detection appear face-like.\nThis phenomenon holds generally across different initialization, locations,\nscales of patches, backbones, and state-of-the-art face detection frameworks.\nWe propose new optimization-based approaches to automatic design of universal\nadversarial patches for varying goals of the attack, including scenarios in\nwhich true positives are suppressed without introducing false positives. Our\nproposed algorithms perform well on real-world datasets, deceiving\nstate-of-the-art face detectors in terms of multiple precision/recall metrics\nand transferability.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 12:43:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:00:41 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 09:37:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yang", "Xiao", ""], ["Wei", "Fangyun", ""], ["Zhang", "Hongyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1912.05026", "submitter": "Stefan Oehmcke", "authors": "Stefan Oehmcke, Christoffer Thrys{\\o}e, Andreas Borgstad, Marcos\n  Antonio Vaz Salles, Martin Brandt, Fabian Gieseke", "title": "Detecting Hardly Visible Roads in Low-Resolution Satellite Time Series\n  Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amounts of satellite data have been gathered over time, holding the\npotential to unveil a spatiotemporal chronicle of the surface of Earth. These\ndata allow scientists to investigate various important issues, such as land use\nchanges, on a global scale. However, not all land-use phenomena are equally\nvisible on satellite imagery. In particular, the creation of an inventory of\nthe planet's road infrastructure remains a challenge, despite being crucial to\nanalyze urbanization patterns and their impact. Towards this end, this work\nadvances data-driven approaches for the automatic identification of roads based\non open satellite data. Given the typical resolutions of these historical\nsatellite data, we observe that there is inherent variation in the visibility\nof different road types. Based on this observation, we propose two deep\nlearning frameworks that extend state-of-the-art deep learning methods by\nformalizing road detection as an ordinal classification task. In contrast to\nrelated schemes, one of the two models also resorts to satellite time series\ndata that are potentially affected by missing data and cloud occlusion. Taking\nthese time series data into account eliminates the need to manually curate\ndatasets of high-quality image tiles, substantially simplifying the application\nof such models on a global scale. We evaluate our approaches on a dataset that\nis based on Sentinel~2 satellite imagery and OpenStreetMap vector data. Our\nresults indicate that the proposed models can successfully identify large and\nmedium-sized roads. We also discuss opportunities and challenges related to the\ndetection of roads and other infrastructure on a global scale.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:40:43 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Oehmcke", "Stefan", ""], ["Thrys\u00f8e", "Christoffer", ""], ["Borgstad", "Andreas", ""], ["Salles", "Marcos Antonio Vaz", ""], ["Brandt", "Martin", ""], ["Gieseke", "Fabian", ""]]}, {"id": "1912.05029", "submitter": "Luca Erculiani Mr", "authors": "Luca Erculiani and Fausto Giunchiglia and Andrea Passerini", "title": "Continual egocentric object recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework capable of tackilng the problem of continual object\nrecognition in a setting which resembles that under whichhumans see and learn.\nThis setting has a set of unique characteristics:it assumes an egocentric\npoint-of-view bound to the needs of a singleperson, which implies a relatively\nlow diversity of data and a coldstart with no data; it requires to operate in\nan open world, where newobjects can be encounteredat any time; supervision is\nscarce and hasto be solicited to the user, and completelyunsupervised\nrecognitionof new objects should be possible. Note that this setting differs\nfromthe one addressed in the open world recognition literature, where\nsupervised feedback is always requested to be able to incorporate newobjects.\nWe propose a first solution to this problem in the form ofa memory-based\nincremental framework that is capable of storinginformation of each and any\nobject it encounters, while using the supervision of the user to learn to\ndiscriminate between known and unknown objects. Our approach is based on four\nmain features: the useof time and space persistence (i.e., the appearance of\nobjects changesrelatively slowly), the use of similarity as the main driving\nprinciplefor object recognition and novelty detection, the progressive\nintroduction of new objects in a developmental fashion and the\nselectiveelicitation of user feedback in an online active learning fashion.\nExperimental results show the feasibility of open world, generic\nobjectrecognition, the ability to recognize, memorize and re-identify\nnewobjects even in complete absence of user supervision, and the utilityof\npersistence and incrementality in boosting performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 12:10:59 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 15:22:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Erculiani", "Luca", ""], ["Giunchiglia", "Fausto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1912.05031", "submitter": "Abraham Nunes", "authors": "Abraham Nunes, Martin Alda, Timothy Bardouille, and Thomas Trappenberg", "title": "Representational R\\'enyi heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A discrete system's heterogeneity is measured by the R\\'enyi heterogeneity\nfamily of indices (also known as Hill numbers or Hannah--Kay indices), whose\nunits are {the numbers equivalent}. Unfortunately, numbers equivalent\nheterogeneity measures for non-categorical data require {a priori} (A)\ncategorical partitioning and (B) pairwise distance measurement on the\nobservable data space, thereby precluding application to problems with\nill-defined categories or where semantically relevant features must be learned\nas abstractions from some data. We thus introduce representational R\\'enyi\nheterogeneity (RRH), which transforms an observable domain onto a latent space\nupon which the R\\'enyi heterogeneity is both tractable and semantically\nrelevant. This method requires neither {a priori} binning nor definition of a\ndistance function on the observable space. We show that RRH can generalize\nexisting biodiversity and economic equality indices. Compared with existing\nindices on a beta-mixture distribution, we show that RRH responds more\nappropriately to changes in mixture component separation and weighting.\nFinally, we demonstrate the measurement of RRH in a set of natural images, with\nrespect to abstract representations learned by a deep neural network. The RRH\napproach will further enable heterogeneity measurement in disciplines whose\ndata do not easily conform to the assumptions of existing indices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:22:54 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 16:42:12 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 15:09:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nunes", "Abraham", ""], ["Alda", "Martin", ""], ["Bardouille", "Timothy", ""], ["Trappenberg", "Thomas", ""]]}, {"id": "1912.05032", "submitter": "Ilya Kostrikov", "authors": "Ilya Kostrikov, Ofir Nachum, Jonathan Tompson", "title": "Imitation Learning via Off-Policy Distribution Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing imitation learning from expert demonstrations, distribution\nmatching is a popular approach, in which one alternates between estimating\ndistribution ratios and then using these ratios as rewards in a standard\nreinforcement learning (RL) algorithm. Traditionally, estimation of the\ndistribution ratio requires on-policy data, which has caused previous work to\neither be exorbitantly data-inefficient or alter the original objective in a\nmanner that can drastically change its optimum. In this work, we show how the\noriginal distribution ratio estimation objective may be transformed in a\nprincipled manner to yield a completely off-policy objective. In addition to\nthe data-efficiency that this provides, we are able to show that this objective\nalso renders the use of a separate RL optimization unnecessary.Rather, an\nimitation policy may be learned directly from this objective without the use of\nexplicit rewards. We call the resulting algorithm ValueDICE and evaluate it on\na suite of popular imitation learning benchmarks, finding that it can achieve\nstate-of-the-art sample efficiency and performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:31:09 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Nachum", "Ofir", ""], ["Tompson", "Jonathan", ""]]}, {"id": "1912.05034", "submitter": "Joonas P\\\"a\\\"akk\\\"onen", "authors": "Joonas P\\\"a\\\"akk\\\"onen", "title": "Fenton-Wilkinson Order Statistics and German Tanks: A Case Study of an\n  Orienteering Relay Race", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal regression falls between discrete-valued classification and\ncontinuous-valued regression. Ordinal target variables can be associated with\nranked random variables. These random variables are known as order statistics\nand they are closely related to ordinal regression. However, the challenge of\nusing order statistics for ordinal regression prediction is finding a suitable\nparent distribution. In this work, we provide a case study of a real-world\norienteering relay race by viewing it as a random process. For this process, we\nshow that accurate order statistical ordinal regression predictions of final\nteam rankings, or places, can be obtained by assuming a lognormal distribution\nof individual leg times. Moreover, we apply Fenton-Wilkinson approximations to\nintermediate changeover times alongside an estimator for the total number of\nteams as in the notorious German tank problem. The purpose of this work is, in\npart, to spark interest in studying the applicability of order statistics in\nordinal regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:37:55 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["P\u00e4\u00e4kk\u00f6nen", "Joonas", ""]]}, {"id": "1912.05045", "submitter": "James Bagrow", "authors": "Abigail Hotaling and James P. Bagrow", "title": "Efficient crowdsourcing of crowd-generated microtasks", "comments": "12 pages, 5 figures", "journal-ref": "PLoS ONE 15(12): e0244245, 2020", "doi": "10.1371/journal.pone.0244245", "report-no": null, "categories": "cs.HC cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing members of the crowd to propose novel microtasks for one another is\nan effective way to combine the efficiencies of traditional microtask work with\nthe inventiveness and hypothesis generation potential of human workers.\nHowever, microtask proposal leads to a growing set of tasks that may overwhelm\nlimited crowdsourcer resources. Crowdsourcers can employ methods to utilize\ntheir resources efficiently, but algorithmic approaches to efficient\ncrowdsourcing generally require a fixed task set of known size. In this paper,\nwe introduce *cost forecasting* as a means for a crowdsourcer to use efficient\ncrowdsourcing algorithms with a growing set of microtasks. Cost forecasting\nallows the crowdsourcer to decide between eliciting new tasks from the crowd or\nreceiving responses to existing tasks based on whether or not new tasks will\ncost less to complete than existing tasks, efficiently balancing resources as\ncrowdsourcing occurs. Experiments with real and synthetic crowdsourcing data\nshow that cost forecasting leads to improved accuracy. Accuracy and efficiency\ngains for crowd-generated microtasks hold the promise to further leverage the\ncreativity and wisdom of the crowd, with applications such as generating more\ninformative and diverse training data for machine learning applications and\nimproving the performance of user-generated content and question-answering\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 23:23:54 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 19:24:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hotaling", "Abigail", ""], ["Bagrow", "James P.", ""]]}, {"id": "1912.05066", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Ronghuo Zheng, Yuezhang Li, Katia Sycara", "title": "Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in\n  Microblog Feeds", "comments": "9 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis of microblog feeds has attracted considerable interest in\nrecent times. Most of the current work focuses on tweet sentiment\nclassification. But not much work has been done to explore how reliable the\nopinions of the mass (crowd wisdom) in social network microblogs such as\ntwitter are in predicting outcomes of certain events such as election debates.\nIn this work, we investigate whether crowd wisdom is useful in predicting such\noutcomes and whether their opinions are influenced by the experts in the field.\nWe work in the domain of multi-label classification to perform sentiment\nclassification of tweets and obtain the opinion of the crowd. This learnt\nsentiment is then used to predict outcomes of events such as: US Presidential\nDebate winners, Grammy Award winners, Super Bowl Winners. We find that in most\nof the cases, the wisdom of the crowd does indeed match with that of the\nexperts, and in cases where they don't (particularly in the case of debates),\nwe see that the crowd's opinion is actually influenced by that of the experts.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 00:30:24 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Zheng", "Ronghuo", ""], ["Li", "Yuezhang", ""], ["Sycara", "Katia", ""]]}, {"id": "1912.05075", "submitter": "Mike Wu", "authors": "Mike Wu, Noah Goodman", "title": "Multimodal Generative Models for Compositional Representation Learning", "comments": "24 pages content; 7 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks become more adept at traditional tasks, many of the\nmost exciting new challenges concern multimodality---observations that combine\ndiverse types, such as image and text. In this paper, we introduce a family of\nmultimodal deep generative models derived from variational bounds on the\nevidence (data marginal likelihood). As part of our derivation we find that\nmany previous multimodal variational autoencoders used objectives that do not\ncorrectly bound the joint marginal likelihood across modalities. We further\ngeneralize our objective to work with several types of deep generative model\n(VAE, GAN, and flow-based), and allow use of different model types for\ndifferent modalities. We benchmark our models across many image, label, and\ntext datasets, and find that our multimodal VAEs excel with and without weak\nsupervision. Additional improvements come from use of GAN image models with VAE\nlanguage models. Finally, we investigate the effect of language on learned\nimage representations through a variety of downstream tasks, such as\ncompositionally, bounding box prediction, and visual relation prediction. We\nfind evidence that these image representations are more abstract and\ncompositional than equivalent representations learned from only visual data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 01:43:56 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wu", "Mike", ""], ["Goodman", "Noah", ""]]}, {"id": "1912.05078", "submitter": "E Zhenqian", "authors": "E Zhenqian and Gao Weiguo", "title": "An Improving Framework of regularization for Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have achieved remarkable success relying on the\ndeveloping high computation capability of GPUs and large-scale datasets with\nincreasing network depth and width in image recognition, object detection and\nmany other applications. However, due to the expensive computation and\nintensive memory, researchers have concentrated on designing compression\nmethods in recent years. In this paper, we briefly summarize the existing\nadvanced techniques that are useful in model compression at first. After that,\nwe give a detailed description on group lasso regularization and its variants.\nMore importantly, we propose an improving framework of partial regularization\nbased on the relationship between neurons and connections of adjacent layers.\nIt is reasonable and feasible with the help of permutation property of neural\nnetwork . Experiment results show that partial regularization methods brings\nimprovements such as higher classification accuracy in both training and\ntesting stages on multiple datasets. Since our regularizers contain the\ncomputation of less parameters, it shows competitive performances in terms of\nthe total running time of experiments. Finally, we analysed the results and\ndraw a conclusion that the optimal network structure must exist and depend on\nthe input data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 01:59:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 06:44:41 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhenqian", "E", ""], ["Weiguo", "Gao", ""]]}, {"id": "1912.05081", "submitter": "Ziwei Li", "authors": "Ziwei Li and Sai Ravela", "title": "Neural Networks as Geometric Chaotic Maps", "comments": "in IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2021.3087497", "report-no": null, "categories": "cs.LG math.DS nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of artificial neural networks as models of chaotic dynamics has been\nrapidly expanding. Still, a theoretical understanding of how neural networks\nlearn chaos is lacking. Here, we employ a geometric perspective to show that\nneural networks can efficiently model chaotic dynamics by becoming structurally\nchaotic themselves. We first confirm neural network's efficiency in emulating\nchaos by showing that a parsimonious neural network trained only on few data\npoints can reconstruct strange attractors, extrapolate outside training data\nboundaries, and accurately predict local divergence rates. We then posit that\nthe trained network's map comprises sequential geometric stretching, rotation,\nand compression operations. These geometric operations indicate topological\nmixing and chaos, explaining why neural networks are naturally suitable to\nemulate chaotic dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:00:53 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 20:12:05 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:29:30 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 04:40:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Ziwei", ""], ["Ravela", "Sai", ""]]}, {"id": "1912.05100", "submitter": "Kacper Sokol", "authors": "Kacper Sokol and Peter Flach", "title": "Explainability Fact Sheets: A Framework for Systematic Assessment of\n  Explainable Approaches", "comments": "Conference on Fairness, Accountability, and Transparency (FAT* '20),\n  January 27-30, 2020, Barcelona, Spain", "journal-ref": null, "doi": "10.1145/3351095.3372870", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations in Machine Learning come in many forms, but a consensus\nregarding their desired properties is yet to emerge. In this paper we introduce\na taxonomy and a set of descriptors that can be used to characterise and\nsystematically assess explainable systems along five key dimensions:\nfunctional, operational, usability, safety and validation. In order to design a\ncomprehensive and representative taxonomy and associated descriptors we\nsurveyed the eXplainable Artificial Intelligence literature, extracting the\ncriteria and desiderata that other authors have proposed or implicitly used in\ntheir research. The survey includes papers introducing new explainability\nalgorithms to see what criteria are used to guide their development and how\nthese algorithms are evaluated, as well as papers proposing such criteria from\nboth computer science and social science perspectives. This novel framework\nallows to systematically compare and contrast explainability approaches, not\njust to better understand their capabilities but also to identify discrepancies\nbetween their theoretical qualities and properties of their implementations. We\ndeveloped an operationalisation of the framework in the form of Explainability\nFact Sheets, which enable researchers and practitioners alike to quickly grasp\ncapabilities and limitations of a particular explainable method. When used as a\nWork Sheet, our taxonomy can guide the development of new explainability\napproaches by aiding in their critical evaluation along the five proposed\ndimensions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:21:23 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Sokol", "Kacper", ""], ["Flach", "Peter", ""]]}, {"id": "1912.05104", "submitter": "Riashat Islam", "authors": "Riashat Islam, Raihan Seraj, Pierre-Luc Bacon, Doina Precup", "title": "Entropy Regularization with Discounted Future State Distribution in\n  Policy Gradient Methods", "comments": "In Submission; Appeared at NeurIPS 2019 Optimization Foundations of\n  Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The policy gradient theorem is defined based on an objective with respect to\nthe initial distribution over states. In the discounted case, this results in\npolicies that are optimal for one distribution over initial states, but may not\nbe uniformly optimal for others, no matter where the agent starts from.\nFurthermore, to obtain unbiased gradient estimates, the starting point of the\npolicy gradient estimator requires sampling states from a normalized discounted\nweighting of states. However, the difficulty of estimating the normalized\ndiscounted weighting of states, or the stationary state distribution, is quite\nwell-known. Additionally, the large sample complexity of policy gradient\nmethods is often attributed to insufficient exploration, and to remedy this, it\nis often assumed that the restart distribution provides sufficient exploration\nin these algorithms. In this work, we propose exploration in policy gradient\nmethods based on maximizing entropy of the discounted future state\ndistribution. The key contribution of our work includes providing a practically\nfeasible algorithm to estimate the normalized discounted weighting of states,\ni.e, the \\textit{discounted future state distribution}. We propose that\nexploration can be achieved by entropy regularization with the discounted state\ndistribution in policy gradients, where a metric for maximal coverage of the\nstate space can be based on the entropy of the induced state distribution. The\nproposed approach can be considered as a three time-scale algorithm and under\nsome mild technical conditions, we prove its convergence to a locally optimal\npolicy. Experimentally, we demonstrate usefulness of regularization with the\ndiscounted future state distribution in terms of increased state space coverage\nand faster learning on a range of complex tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:40:46 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Islam", "Riashat", ""], ["Seraj", "Raihan", ""], ["Bacon", "Pierre-Luc", ""], ["Precup", "Doina", ""]]}, {"id": "1912.05109", "submitter": "Riashat Islam", "authors": "Riashat Islam, Raihan Seraj, Samin Yeasar Arnob, Doina Precup", "title": "Doubly Robust Off-Policy Actor-Critic Algorithms for Reinforcement\n  Learning", "comments": "In Submission; Appeared at NeurIPS 2019 Workshop on Safety and\n  Robustness in Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy critic evaluation in several variants of\nvalue-based off-policy actor-critic algorithms. Off-policy actor-critic\nalgorithms require an off-policy critic evaluation step, to estimate the value\nof the new policy after every policy gradient update. Despite enormous success\nof off-policy policy gradients on control tasks, existing general methods\nsuffer from high variance and instability, partly because the policy\nimprovement depends on gradient of the estimated value function. In this work,\nwe present a new way of off-policy policy evaluation in actor-critic, based on\nthe doubly robust estimators. We extend the doubly robust estimator from\noff-policy policy evaluation (OPE) to actor-critic algorithms that consist of a\nreward estimator performance model. We find that doubly robust estimation of\nthe critic can significantly improve performance in continuous control tasks.\nFurthermore, in cases where the reward function is stochastic that can lead to\nhigh variance, doubly robust critic estimation can improve performance under\ncorrupted, stochastic reward signals, indicating its usefulness for robust and\nsafe reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 04:21:47 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Islam", "Riashat", ""], ["Seraj", "Raihan", ""], ["Arnob", "Samin Yeasar", ""], ["Precup", "Doina", ""]]}, {"id": "1912.05122", "submitter": "Jiezhu Cheng", "authors": "Jiezhu Cheng, Kaizhu Huang, Zibin Zheng", "title": "Towards Better Forecasting by Fusing Near and Distant Future Visions", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series forecasting is an important yet challenging problem\nin machine learning. Most existing approaches only forecast the series value of\none future moment, ignoring the interactions between predictions of future\nmoments with different temporal distance. Such a deficiency probably prevents\nthe model from getting enough information about the future, thus limiting the\nforecasting accuracy. To address this problem, we propose Multi-Level Construal\nNeural Network (MLCNN), a novel multi-task deep learning framework. Inspired by\nthe Construal Level Theory of psychology, this model aims to improve the\npredictive performance by fusing forecasting information (i.e., future visions)\nof different future time. We first use the Convolution Neural Network to\nextract multi-level abstract representations of the raw data for near and\ndistant future predictions. We then model the interplay between multiple\npredictive tasks and fuse their future visions through a modified\nEncoder-Decoder architecture. Finally, we combine traditional Autoregression\nmodel with the neural network to solve the scale insensitive problem.\nExperiments on three real-world datasets show that our method achieves\nstatistically significant improvements compared to the most state-of-the-art\nbaseline methods, with average 4.59% reduction on RMSE metric and average 6.87%\nreduction on MAE metric.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:32:24 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Cheng", "Jiezhu", ""], ["Huang", "Kaizhu", ""], ["Zheng", "Zibin", ""]]}, {"id": "1912.05127", "submitter": "Weishun Zhong", "authors": "Harshvardhan Sikka, Weishun Zhong, Jun Yin, Cengiz Pehlevan", "title": "A Closer Look at Disentangling in $\\beta$-VAE", "comments": "Presented at the 53rd Asilomar Conference on Signals, Systems, and\n  Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data analysis tasks, it is beneficial to learn representations where\neach dimension is statistically independent and thus disentangled from the\nothers. If data generating factors are also statistically independent,\ndisentangled representations can be formed by Bayesian inference of latent\nvariables. We examine a generalization of the Variational Autoencoder (VAE),\n$\\beta$-VAE, for learning such representations using variational inference.\n$\\beta$-VAE enforces conditional independence of its bottleneck neurons\ncontrolled by its hyperparameter $\\beta$. This condition is in general not\ncompatible with the statistical independence of latents. By providing\nanalytical and numerical arguments, we show that this incompatibility leads to\na non-monotonic inference performance in $\\beta$-VAE with a finite optimal\n$\\beta$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:51:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Sikka", "Harshvardhan", ""], ["Zhong", "Weishun", ""], ["Yin", "Jun", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "1912.05128", "submitter": "Riashat Islam", "authors": "Riashat Islam, Zafarali Ahmed, Doina Precup", "title": "Marginalized State Distribution Entropy Regularization in Policy\n  Optimization", "comments": "In Submission; Appeared at NeurIPS 2019 Deep Reinforcement Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy regularization is used to get improved optimization performance in\nreinforcement learning tasks. A common form of regularization is to maximize\npolicy entropy to avoid premature convergence and lead to more stochastic\npolicies for exploration through action space. However, this does not ensure\nexploration in the state space. In this work, we instead consider the\ndistribution of discounted weighting of states, and propose to maximize the\nentropy of a lower bound approximation to the weighting of a state, based on\nlatent space state representation. We propose entropy regularization based on\nthe marginal state distribution, to encourage the policy to have a more uniform\ndistribution over the state space for exploration. Our approach based on\nmarginal state distribution achieves superior state space coverage on complex\ngridworld domains, that translate into empirical gains in sparse reward 3D maze\nnavigation and continuous control domains compared to entropy regularization\nwith stochastic policies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:55:32 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Islam", "Riashat", ""], ["Ahmed", "Zafarali", ""], ["Precup", "Doina", ""]]}, {"id": "1912.05137", "submitter": "Yaniv Blumenfeld", "authors": "Yaniv Blumenfeld, Dar Gilboa, Daniel Soudry", "title": "Is Feature Diversity Necessary in Neural Network Initialization?", "comments": "This paper has been substantially modified, updated, and expanded\n  with additional content (arXiv:2007.01038). To avoid confusion, we are\n  withdrawing the old version of this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard practice in training neural networks involves initializing the\nweights in an independent fashion. The results of recent work suggest that\nfeature \"diversity\" at initialization plays an important role in training the\nnetwork. However, other initialization schemes with reduced feature diversity\nhave also been shown to be viable. In this work, we conduct a series of\nexperiments aimed at elucidating the importance of feature diversity at\ninitialization. We show that a complete lack of diversity is harmful to\ntraining, but its effects can be counteracted by a relatively small addition of\nnoise - even the noise in standard non-deterministic GPU computations is\nsufficient. Furthermore, we construct a deep convolutional network with\nidentical features at initialization and almost all of the weights initialized\nat 0 that can be trained to reach accuracy matching its standard-initialized\ncounterpart.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:36:46 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 04:01:03 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 05:30:32 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Blumenfeld", "Yaniv", ""], ["Gilboa", "Dar", ""], ["Soudry", "Daniel", ""]]}, {"id": "1912.05153", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter L. Bartlett, Michael\n  I. Jordan", "title": "Sampling for Bayesian Mixture Models: MCMC with Polynomial-Time Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from the power posterior distribution in\nBayesian Gaussian mixture models, a robust version of the classical posterior.\nThis power posterior is known to be non-log-concave and multi-modal, which\nleads to exponential mixing times for some standard MCMC algorithms. We\nintroduce and study the Reflected Metropolis-Hastings Random Walk (RMRW)\nalgorithm for sampling. For symmetric two-component Gaussian mixtures, we prove\nthat its mixing time is bounded as $d^{1.5}(d + \\Vert \\theta_{0}\n\\Vert^2)^{4.5}$ as long as the sample size $n$ is of the order $d (d + \\Vert\n\\theta_{0} \\Vert^2)$. Notably, this result requires no conditions on the\nseparation of the two means. En route to proving this bound, we establish some\nnew results of possible independent interest that allow for combining\nPoincar\\'{e} inequalities for conditional and marginal densities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:48:49 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mou", "Wenlong", ""], ["Ho", "Nhat", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1912.05159", "submitter": "Huibing Wang", "authors": "Guangqi Jiang, Huibing Wang, Jinjia Peng, Dongyan Chen, Xianping Fu", "title": "Graph-based Multi-view Binary Learning for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing techniques, also known as binary code learning, have recently gained\nincreasing attention in large-scale data analysis and storage. Generally, most\nexisting hash clustering methods are single-view ones, which lack complete\nstructure or complementary information from multiple views. For cluster tasks,\nabundant prior researches mainly focus on learning discrete hash code while few\nworks take original data structure into consideration. To address these\nproblems, we propose a novel binary code algorithm for clustering, which adopts\ngraph embedding to preserve the original data structure, called (Graph-based\nMulti-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding\nthe information of multiple views into a compact binary code, which explores\ncomplementary information from multiple views. In particular, in order to\nmaintain the graph-based structure of the original data, we adopt a Laplacian\nmatrix to preserve the local linear relationship of the data and map it to the\nHamming space. Considering different views have distinctive contributions to\nthe final clustering results, GMBL adopts a strategy of automatically assign\nweights for each view to better guide the clustering. Finally, An alternating\niterative optimization method is adopted to optimize discrete binary codes\ndirectly instead of relaxing the binary constraint in two steps. Experiments on\nfive public datasets demonstrate the superiority of our proposed method\ncompared with previous approaches in terms of clustering performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:04:56 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Jiang", "Guangqi", ""], ["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Chen", "Dongyan", ""], ["Fu", "Xianping", ""]]}, {"id": "1912.05170", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy", "title": "Image Classification with Deep Learning in the Presence of Noisy Labels:\n  A Survey", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2021.106771", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image classification systems recently made a giant leap with the advancement\nof deep neural networks. However, these systems require an excessive amount of\nlabeled data to be adequately trained. Gathering a correctly annotated dataset\nis not always feasible due to several factors, such as the expensiveness of the\nlabeling process or difficulty of correctly classifying data, even for the\nexperts. Because of these practical challenges, label noise is a common problem\nin real-world datasets, and numerous methods to train deep neural networks with\nlabel noise are proposed in the literature. Although deep neural networks are\nknown to be relatively robust to label noise, their tendency to overfit data\nmakes them vulnerable to memorizing even random noise. Therefore, it is crucial\nto consider the existence of label noise and develop counter algorithms to fade\naway its adverse effects to train deep neural networks efficiently. Even though\nan extensive survey of machine learning techniques under label noise exists,\nthe literature lacks a comprehensive survey of methodologies centered\nexplicitly around deep learning in the presence of noisy labels. This paper\naims to present these algorithms while categorizing them into one of the two\nsubgroups: noise model based and noise model free methods. Algorithms in the\nfirst group aim to estimate the noise structure and use this information to\navoid the adverse effects of noisy labels. Differently, methods in the second\ngroup try to come up with inherently noise robust algorithms by using\napproaches like robust losses, regularizers or other learning paradigms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:26:57 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 09:56:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 08:50:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""]]}, {"id": "1912.05179", "submitter": "Yermek Kapushev", "authors": "Yermek Kapushev, Ivan Oseledets, Evgeny Burnaev", "title": "Tensor Completion via Gaussian Process Based Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the tensor completion problem representing the\nsolution in the tensor train (TT) format. It is assumed that tensor is\nhigh-dimensional, and tensor values are generated by an unknown smooth\nfunction. The assumption allows us to develop an efficient initialization\nscheme based on Gaussian Process Regression and TT-cross approximation\ntechnique. The proposed approach can be used in conjunction with any\noptimization algorithm that is usually utilized in tensor completion problems.\nWe empirically justify that in this case the reconstruction error improves\ncompared to the tensor completion with random initialization. As an additional\nbenefit, our technique automatically selects rank thanks to using the TT-cross\napproximation technique.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:52:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 19:17:56 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 12:18:07 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Kapushev", "Yermek", ""], ["Oseledets", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1912.05184", "submitter": "Amir Abdi", "authors": "Amir H. Abdi, Purang Abolmaesumi, Sidney Fels", "title": "Variational Learning with Disentanglement-PyTorch", "comments": "Disentanglement Challenge - 33rd Conference on Neural Information\n  Processing Systems (NeurIPS) - NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of disentangled representations is an open problem in\nmachine learning. The Disentanglement-PyTorch library is developed to\nfacilitate research, implementation, and testing of new variational algorithms.\nIn this modular library, neural architectures, dimensionality of the latent\nspace, and the training algorithms are fully decoupled, allowing for\nindependent and consistent experiments across variational methods. The library\nhandles the training scheduling, logging, and visualizations of reconstructions\nand latent space traversals. It also evaluates the encodings based on various\ndisentanglement metrics. The library, so far, includes implementations of the\nfollowing unsupervised algorithms VAE, Beta-VAE, Factor-VAE, DIP-I-VAE,\nDIP-II-VAE, Info-VAE, and Beta-TCVAE, as well as conditional approaches such as\nCVAE and IFCVAE. The library is compatible with the Disentanglement Challenge\nof NeurIPS 2019, hosted on AICrowd, and achieved the 3rd rank in both the first\nand second stages of the challenge.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:02:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Abdi", "Amir H.", ""], ["Abolmaesumi", "Purang", ""], ["Fels", "Sidney", ""]]}, {"id": "1912.05198", "submitter": "Angshul Majumdar Dr.", "authors": "Megha Gupta and Angshul Majumdar", "title": "Recurrent Transform Learning", "comments": "A slightly different version has been accepted at Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to improve the accuracy of building demand\nforecasting. This is a more challenging task than grid level forecasting. For\nthe said purpose, we develop a new technique called recurrent transform\nlearning (RTL). Two versions are proposed. The first one (RTL) is unsupervised;\nthis is used as a feature extraction tool that is further fed into a regression\nmodel. The second formulation embeds regression into the RTL framework leading\nto regressing recurrent transform learning (R2TL). Forecasting experiments have\nbeen carried out on three popular publicly available datasets. Both of our\nproposed techniques yield results superior to the state-of-the-art like long\nshort term memory network, echo state network and sparse coding regression.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:29:57 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Gupta", "Megha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.05238", "submitter": "Patrick Schramowski", "authors": "Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin\n  Rothkopf and Kristian Kersting", "title": "BERT has a Moral Compass: Improvements of ethical and moral values of\n  machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing machines to choose whether to kill humans would be devastating for\nworld peace and security. But how do we equip machines with the ability to\nlearn ethical or even moral choices? Jentzsch et al.(2019) showed that applying\nmachine learning to human texts can extract deontological ethical reasoning\nabout \"right\" and \"wrong\" conduct by calculating a moral bias score on a\nsentence level using sentence embeddings. The machine learned that it is\nobjectionable to kill living beings, but it is fine to kill time; It is\nessential to eat, yet one might not eat dirt; it is important to spread\ninformation, yet one should not spread misinformation. However, the evaluated\nmoral bias was restricted to simple actions -- one verb -- and a ranking of\nactions with surrounding context. Recently BERT ---and variants such as RoBERTa\nand SBERT--- has set a new state-of-the-art performance for a wide range of NLP\ntasks. But has BERT also a better moral compass? In this paper, we discuss and\nshow that this is indeed the case. Thus, recent improvements of language\nrepresentations also improve the representation of the underlying ethical and\nmoral values of the machine. We argue that through an advanced semantic\nrepresentation of text, BERT allows one to get better insights of moral and\nethical values implicitly represented in text. This enables the Moral Choice\nMachine (MCM) to extract more accurate imprints of moral choices and ethical\nvalues.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:27:06 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schramowski", "Patrick", ""], ["Turan", "Cigdem", ""], ["Jentzsch", "Sophie", ""], ["Rothkopf", "Constantin", ""], ["Kersting", "Kristian", ""]]}, {"id": "1912.05283", "submitter": "Nicolas Michael M\\\"uller", "authors": "Nicolas Michael M\\\"uller, Karla Markert", "title": "Identifying Mislabeled Instances in Classification Datasets", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN),\n  Budapest, Hungary, 2019", "doi": "10.1109/IJCNN.2019.8851920", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for supervised machine learning is labeled training data,\nwhich is created by annotating unlabeled data with the appropriate class.\nBecause this process can in many cases not be done by machines, labeling needs\nto be performed by human domain experts. This process tends to be expensive\nboth in time and money, and is prone to errors. Additionally, reviewing an\nentire labeled dataset manually is often prohibitively costly, so many real\nworld datasets contain mislabeled instances.\n  To address this issue, we present in this paper a non-parametric end-to-end\npipeline to find mislabeled instances in numerical, image and natural language\ndatasets. We evaluate our system quantitatively by adding a small number of\nlabel noise to 29 datasets, and show that we find mislabeled instances with an\naverage precision of more than 0.84 when reviewing our system's top 1\\%\nrecommendation. We then apply our system to publicly available datasets and\nfind mislabeled instances in CIFAR-100, Fashion-MNIST, and others. Finally, we\npublish the code and an applicable implementation of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 13:18:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["M\u00fcller", "Nicolas Michael", ""], ["Markert", "Karla", ""]]}, {"id": "1912.05288", "submitter": "Sungbin Choi", "authors": "Sungbin Choi", "title": "Traffic map prediction using UNet based deep convolutional neural\n  network", "comments": "NeuralIPS 2019 Traffic4cast Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our UNet based deep convolutional neural network\napproach on the Traffic4cast challenge 2019. Challenges task is to predict\nfuture traffic flow volume, heading and speed on high resolution whole city\nmap. We used UNet based deep convolutional neural network to train predictive\nmodel for the short term traffic forecast. On each convolution block, layers\nare densely connected with subsequent layers like a DenseNet. Trained and\nevaluated on the real world data set collected from three distinct cities in\nthe world, our method achieved best performance in this challenge.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 00:25:44 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Choi", "Sungbin", ""]]}, {"id": "1912.05308", "submitter": "Mehrdad Valipour", "authors": "Mehrdad Valipour, En-Shiun Annie Lee, Jaime R. Jamacaro, and Carolina\n  Bessega", "title": "Unsupervised Transfer Learning via BERT Neuron Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advancements in language representation models such as BERT have led\nto a rapid improvement in numerous natural language processing tasks. However,\nlanguage models usually consist of a few hundred million trainable parameters\nwith embedding space distributed across multiple layers, thus making them\nchallenging to be fine-tuned for a specific task or to be transferred to a new\ndomain. To determine whether there are task-specific neurons that can be\nexploited for unsupervised transfer learning, we introduce a method for\nselecting the most important neurons to solve a specific classification task.\nThis algorithm is further extended to multi-source transfer learning by\ncomputing the importance of neurons for several single-source transfer learning\nscenarios between different subsets of data sources. Besides, a task-specific\nfingerprint for each data source is obtained based on the percentage of the\nselected neurons in each layer. We perform extensive experiments in\nunsupervised transfer learning for sentiment analysis, natural language\ninference and sentence similarity, and compare our results with the existing\nliterature and baselines. Significantly, we found that the source and target\ndata sources with higher degrees of similarity between their task-specific\nfingerprints demonstrate a better transferability property. We conclude that\nour method can lead to better performance using just a few hundred\ntask-specific and interpretable neurons.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:08:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Valipour", "Mehrdad", ""], ["Lee", "En-Shiun Annie", ""], ["Jamacaro", "Jaime R.", ""], ["Bessega", "Carolina", ""]]}, {"id": "1912.05375", "submitter": "Vaishakhi Mayya", "authors": "Vaishakhi Mayya, Galen Reeves", "title": "Mutual Information in Community Detection with Covariate Information and\n  Correlated Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of community detection when there is covariate\ninformation about the node labels and one observes multiple correlated\nnetworks. We provide an asymptotic upper bound on the per-node mutual\ninformation as well as a heuristic analysis of a multivariate performance\nmeasure called the MMSE matrix. These results show that the combined effects of\nseemingly very different types of information can be characterized explicitly\nin terms of formulas involving low-dimensional estimation problems in additive\nGaussian noise. Our analysis is supported by numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:10:30 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mayya", "Vaishakhi", ""], ["Reeves", "Galen", ""]]}, {"id": "1912.05391", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Minoru Kuribayashi, Junichi Yamagishi, Isao Echizen", "title": "Detecting and Correcting Adversarial Images Using Image Processing\n  Operations", "comments": "Fixing incorrect results by removing the CNN detector part", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved excellent performance on several\ntasks and have been widely applied in both academia and industry. However, DNNs\nare vulnerable to adversarial machine learning attacks, in which noise is added\nto the input to change the network output. We have devised an\nimage-processing-based method to detect adversarial images based on our\nobservation that adversarial noise is reduced after applying these operations\nwhile the normal images almost remain unaffected. In addition to detection,\nthis method can be used to restore the adversarial images' original labels,\nwhich is crucial to restoring the normal functionalities of DNN-based systems.\nTesting using an adversarial machine learning database we created for\ngenerating several types of attack using images from the ImageNet Large Scale\nVisual Recognition Challenge database demonstrated the efficiency of our\nproposed method for both detection and correction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:32:45 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 11:05:01 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Kuribayashi", "Minoru", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1912.05421", "submitter": "David Demeter", "authors": "David Demeter and Doug Downey", "title": "Just Add Functions: A Neural-Symbolic Language Model", "comments": "Preprint of paper accepted for AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network language models (NNLMs) have achieved ever-improving accuracy\ndue to more sophisticated architectures and increasing amounts of training\ndata. However, the inductive bias of these models (formed by the distributional\nhypothesis of language), while ideally suited to modeling most running text,\nresults in key limitations for today's models. In particular, the models often\nstruggle to learn certain spatial, temporal, or quantitative relationships,\nwhich are commonplace in text and are second-nature for human readers. Yet, in\nmany cases, these relationships can be encoded with simple mathematical or\nlogical expressions. How can we augment today's neural models with such\nencodings?\n  In this paper, we propose a general methodology to enhance the inductive bias\nof NNLMs by incorporating simple functions into a neural architecture to form a\nhierarchical neural-symbolic language model (NSLM). These functions explicitly\nencode symbolic deterministic relationships to form probability distributions\nover words. We explore the effectiveness of this approach on numbers and\ngeographic locations, and show that NSLMs significantly reduce perplexity in\nsmall-corpus language modeling, and that the performance improvement persists\nfor rare tokens even on much larger corpora. The approach is simple and\ngeneral, and we discuss how it can be applied to other word classes beyond\nnumbers and geography.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:27:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Demeter", "David", ""], ["Downey", "Doug", ""]]}, {"id": "1912.05440", "submitter": "Andrew Simpson", "authors": "Shuyang Du, Haoli Guo, Andrew Simpson", "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition", "comments": "9 pages 13 figures. Paper originally from CS231n (Stanford) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles have expanded dramatically over the last few years.\nUdacity has release a dataset containing, among other data, a set of images\nwith the steering angle captured during driving. The Udacity challenge aimed to\npredict steering angle based on only the provided images. We explore two\ndifferent models to perform high quality prediction of steering angles based on\nimages using different deep learning techniques including Transfer Learning, 3D\nCNN, LSTM and ResNet. If the Udacity challenge was still ongoing, both of our\nmodels would have placed in the top ten of all entries.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:44:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Du", "Shuyang", ""], ["Guo", "Haoli", ""], ["Simpson", "Andrew", ""]]}, {"id": "1912.05449", "submitter": "Minjie Wang", "authors": "Minjie Wang, Genevera I. Allen", "title": "Integrative Generalized Convex Clustering Optimization and Feature\n  Selection for Mixed Multi-View Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixed multi-view data, multiple sets of diverse features are measured on\nthe same set of samples. By integrating all available data sources, we seek to\ndiscover common group structure among the samples that may be hidden in\nindividualistic cluster analyses of a single data-view. While several\ntechniques for such integrative clustering have been explored, we propose and\ndevelop a convex formalization that will inherit the strong statistical,\nmathematical and empirical properties of increasingly popular convex clustering\nmethods. Specifically, our Integrative Generalized Convex Clustering\nOptimization (iGecco) method employs different convex distances, losses, or\ndivergences for each of the different data views with a joint convex fusion\npenalty that leads to common groups. Additionally, integrating mixed multi-view\ndata is often challenging when each data source is high-dimensional. To perform\nfeature selection in such scenarios, we develop an adaptive shifted group-lasso\npenalty that selects features by shrinking them towards their loss-specific\ncenters. Our so-called iGecco+ approach selects features from each data-view\nthat are best for determining the groups, often leading to improved integrative\nclustering. To fit our model, we develop a new type of generalized multi-block\nADMM algorithm using sub-problem approximations that more efficiently fits our\nmodel for big data sets. Through a series of numerical experiments and real\ndata examples on text mining and genomics, we show that iGecco+ achieves\nsuperior empirical performance for high-dimensional mixed multi-view data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:51:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Minjie", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1912.05458", "submitter": "Mohsen Ghassemi Parsa", "authors": "Mohsen Ghassemi Parsa, Hadi Zare, Mehdi Ghatee", "title": "Unsupervised Feature Selection based on Adaptive Similarity Learning and\n  Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection methods have an important role on the readability of data\nand the reduction of complexity of learning algorithms. In recent years, a\nvariety of efforts are investigated on feature selection problems based on\nunsupervised viewpoint due to the laborious labeling task on large datasets. In\nthis paper, we propose a novel approach on unsupervised feature selection\ninitiated from the subspace clustering to preserve the similarities by\nrepresentation learning of low dimensional subspaces among the samples. A\nself-expressive model is employed to implicitly learn the cluster similarities\nin an adaptive manner. The proposed method not only maintains the sample\nsimilarities through subspace clustering, but it also captures the\ndiscriminative information based on a regularized regression model. In line\nwith the convergence analysis of the proposed method, the experimental results\non benchmark datasets demonstrate the effectiveness of our approach as compared\nwith the state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:10:48 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Parsa", "Mohsen Ghassemi", ""], ["Zare", "Hadi", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "1912.05459", "submitter": "Christian Etmann", "authors": "Christian Etmann, Maximilian Schmidt, Jens Behrmann, Tobias Boskamp,\n  Lena Hauberg-Lotte, Annette Peter, Rita Casadonte, J\\\"org Kriegsmann, Peter\n  Maass", "title": "Deep Relevance Regularization: Interpretable and Robust Tumor Typing of\n  Imaging Mass Spectrometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently been established as a viable classification\nmethod for imaging mass spectrometry data for tumor typing. For\nmulti-laboratory scenarios however, certain confounding factors may strongly\nimpede their performance. In this work, we introduce Deep Relevance\nRegularization, a method of restricting what the neural network can focus on\nduring classification, in order to improve the classification performance. We\ndemonstrate how Deep Relevance Regularization robustifies neural networks\nagainst confounding factors on a challenging inter-lab dataset consisting of\nbreast and ovarian carcinoma. We further show that this makes the relevance map\n-- a way of visualizing the discriminative parts of the mass spectrum --\nsparser, thereby making the classifier easier to interpret\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:45:55 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Etmann", "Christian", ""], ["Schmidt", "Maximilian", ""], ["Behrmann", "Jens", ""], ["Boskamp", "Tobias", ""], ["Hauberg-Lotte", "Lena", ""], ["Peter", "Annette", ""], ["Casadonte", "Rita", ""], ["Kriegsmann", "J\u00f6rg", ""], ["Maass", "Peter", ""]]}, {"id": "1912.05475", "submitter": "David \\v{S}i\\v{s}ka", "authors": "Jean-Fran\\c{c}ois Jabir and David \\v{S}i\\v{s}ka and {\\L}ukasz Szpruch", "title": "Mean-Field Neural ODEs via Relaxed Optimal Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for the analysis of deep neural networks and neural\nODE models that are trained with stochastic gradient algorithms. We do that by\nidentifying the connections between control theory, deep learning and theory of\nstatistical sampling. We derive Pontryagin's optimality principle and study the\ncorresponding gradient flow in the form of Mean-Field Langevin dynamics (MFLD)\nfor solving relaxed data-driven control problems. Subsequently, we study\nuniform-in-time propagation of chaos of time-discretised MFLD. We derive\nexplicit convergence rate in terms of the learning rate, the number of\nparticles/model parameters and the number of iterations of the gradient\nalgorithm. In addition, we study the error arising when using a finite training\ndata set and thus provide quantitive bounds on the generalisation error.\nCrucially, the obtained rates are dimension-independent. This is possible by\nexploiting the regularity of the model with respect to the measure over the\nparameter space.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:13:06 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 17:13:44 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 20:11:15 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jabir", "Jean-Fran\u00e7ois", ""], ["\u0160i\u0161ka", "David", ""], ["Szpruch", "\u0141ukasz", ""]]}, {"id": "1912.05480", "submitter": "Kerstin Hammernik", "authors": "Jo Schlemper, Chen Qin, Jinming Duan, Ronald M. Summers, Kerstin\n  Hammernik", "title": "$\\Sigma$-net: Ensembled Iterative Deep Neural Networks for Accelerated\n  Parallel MR Image Reconstruction", "comments": "fastMRI challenge submission (team: holykspace)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an ensembled $\\Sigma$-net for fast parallel MR imaging, including\nparallel coil networks, which perform implicit coil weighting, and sensitivity\nnetworks, involving explicit sensitivity maps. The networks in $\\Sigma$-net are\ntrained in a supervised way, including content and GAN losses, and with various\nways of data consistency, i.e., proximal mappings, gradient descent and\nvariable splitting. A semi-supervised finetuning scheme allows us to adapt to\nthe k-space data at test time, which, however, decreases the quantitative\nmetrics, although generating the visually most textured and sharp images. For\nthis challenge, we focused on robust and high SSIM scores, which we achieved by\nensembling all models to a $\\Sigma$-net.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:23:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schlemper", "Jo", ""], ["Qin", "Chen", ""], ["Duan", "Jinming", ""], ["Summers", "Ronald M.", ""], ["Hammernik", "Kerstin", ""]]}, {"id": "1912.05509", "submitter": "Elsa Cazelles", "authors": "Elsa Cazelles, Arnaud Robert, Felipe Tobar", "title": "The Wasserstein-Fourier Distance for Stationary Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Wasserstein-Fourier (WF) distance to measure the\n(dis)similarity between time series by quantifying the displacement of their\nenergy across frequencies. The WF distance operates by calculating the\nWasserstein distance between the (normalised) power spectral densities (NPSD)\nof time series. Yet this rationale has been considered in the past, we fill a\ngap in the open literature providing a formal introduction of this distance,\ntogether with its main properties from the joint perspective of Fourier\nanalysis and optimal transport. As the main aim of this work is to validate WF\nas a general-purpose metric for time series, we illustrate its applicability on\nthree broad contexts. First, we rely on WF to implement a PCA-like\ndimensionality reduction for NPSDs which allows for meaningful visualisation\nand pattern recognition applications. Second, we show that the geometry induced\nby WF on the space of NPSDs admits a geodesic interpolant between time series,\nthus enabling data augmentation on the spectral domain, by averaging the\ndynamic content of two signals. Third, we implement WF for time series\nclassification using parametric/non-parametric classifiers and compare it to\nother classical metrics. Supported on theoretical results, as well as synthetic\nillustrations and experiments on real-world data, this work establishes WF as a\nmeaningful and capable resource pertinent to general distance-based\napplications of time series.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:17:12 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:24:24 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cazelles", "Elsa", ""], ["Robert", "Arnaud", ""], ["Tobar", "Felipe", ""]]}, {"id": "1912.05510", "submitter": "Glen Berseth", "authors": "Glen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea\n  Finn, Dinesh Jayaraman, Sergey Levine", "title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable\n  Environments", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every living organism struggles against disruptive environmental forces to\ncarve out and maintain an orderly niche. We propose that such a struggle to\nachieve and preserve order might offer a principle for the emergence of useful\nbehaviors in artificial agents. We formalize this idea into an unsupervised\nreinforcement learning method called surprise minimizing reinforcement learning\n(SMiRL). SMiRL alternates between learning a density model to evaluate the\nsurprise of a stimulus, and improving the policy to seek more predictable\nstimuli. The policy seeks out stable and repeatable situations that counteract\nthe environment's prevailing sources of entropy. This might include avoiding\nother hostile agents, or finding a stable, balanced pose for a bipedal robot in\nthe face of disturbance forces. We demonstrate that our surprise minimizing\nagents can successfully play Tetris, Doom, control a humanoid to avoid falls,\nand navigate to escape enemies in a maze without any task-specific reward\nsupervision. We further show that SMiRL can be used together with standard task\nrewards to accelerate reward-driven learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:19:11 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 20:10:36 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 00:44:48 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 02:58:22 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Berseth", "Glen", ""], ["Geng", "Daniel", ""], ["Devin", "Coline", ""], ["Rhinehart", "Nicholas", ""], ["Finn", "Chelsea", ""], ["Jayaraman", "Dinesh", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.05537", "submitter": "Kristy Choi", "authors": "Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse\n  Engel", "title": "Encoding Musical Style with Transformer Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning high-level controls over the global\nstructure of generated sequences, particularly in the context of symbolic music\ngeneration with complex language models. In this work, we present the\nTransformer autoencoder, which aggregates encodings of the input data across\ntime to obtain a global representation of style from a given performance. We\nshow it is possible to combine this global representation with other temporally\ndistributed embeddings, enabling improved control over the separate aspects of\nperformance style and melody. Empirically, we demonstrate the effectiveness of\nour method on various music generation tasks on the MAESTRO dataset and a\nYouTube dataset with 10,000+ hours of piano performances, where we achieve\nimprovements in terms of log-likelihood and mean listening scores as compared\nto baselines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:51:44 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:00:40 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Choi", "Kristy", ""], ["Hawthorne", "Curtis", ""], ["Simon", "Ian", ""], ["Dinculescu", "Monica", ""], ["Engel", "Jesse", ""]]}, {"id": "1912.05539", "submitter": "Shahin Khobahi", "authors": "Shahin Khobahi, Arindam Bose, Mojtaba Soltanalian", "title": "Deep One-bit Compressive Autoencoding", "comments": "This work have been submitted to the IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, May 2020.\n  arXiv admin note: substantial text overlap with arXiv:1911.12410", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized mathematical models play a central role in understanding and\ndesign of complex information systems. However, they often cannot take into\naccount the intricate interactions innate to such systems. On the contrary,\npurely data-driven approaches do not need explicit mathematical models for data\ngeneration and have a wider applicability at the cost of interpretability. In\nthis paper, we consider the design of a one-bit compressive autoencoder, and\npropose a novel hybrid model-based and data-driven methodology that allows us\nto not only design the sensing matrix for one-bit data acquisition, but also\nallows for learning the latent-parameters of an iterative optimization\nalgorithm specifically designed for the problem of one-bit sparse signal\nrecovery. Our results demonstrate a significant improvement compared to\nstate-of-the-art model-based algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 21:58:54 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Khobahi", "Shahin", ""], ["Bose", "Arindam", ""], ["Soltanalian", "Mojtaba", ""]]}, {"id": "1912.05541", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Information-Theoretic Performance Limitations of Feedback Control:\n  Underlying Entropic Laws and Generic $\\mathcal{L}_{p}$ Bounds", "comments": "arXiv admin note: text overlap with arXiv:1912.02628", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.IT cs.LG cs.RO cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize information theory to study the fundamental\nperformance limitations of generic feedback systems, where both the controller\nand the plant may be any causal functions/mappings while the disturbance can be\nwith any distributions. More specifically, we obtain fundamental\n$\\mathcal{L}_p$ bounds on the control error, which are shown to be completely\ncharacterized by the conditional entropy of the disturbance, based upon the\nentropic laws that are inherent in any feedback systems. We also discuss the\ngenerality and implications (in, e.g., fundamental limits of learning-based\ncontrol) of the obtained bounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:41:07 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:07:04 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 16:28:18 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 21:58:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1912.05571", "submitter": "Iman Tabrizian", "authors": "Saeedeh Parsaeefard, Iman Tabrizian, Alberto Leon Garcia", "title": "Representation of Federated Learning via Worst-Case Robust Optimization\n  Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed learning approach where a set of\nend-user devices participate in the learning process by acting on their\nisolated local data sets. Here, we process local data sets of users where\nworst-case optimization theory is used to reformulate the FL problem where the\nimpact of local data sets in training phase is considered as an uncertain\nfunction bounded in a closed uncertainty region. This representation allows us\nto compare the performance of FL with its centralized counterpart, and to\nreplace the uncertain function with a concept of protection functions leading\nto more tractable formulation. The latter supports applying a regularization\nfactor in each user cost function in FL to reach a better performance. We\nevaluated our model using the MNIST data set versus the protection function\nparameters, e.g., regularization factors.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:02:49 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Parsaeefard", "Saeedeh", ""], ["Tabrizian", "Iman", ""], ["Garcia", "Alberto Leon", ""]]}, {"id": "1912.05617", "submitter": "Jaechang Lim", "authors": "Seung Hwan Hong, Jaechang Lim, Seongok Ryu, and Woo Youn Kim", "title": "Molecular Generative Model Based On Adversarially Regularized\n  Autoencoder", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are attracting great attention as a new promising\napproach for molecular design. All models reported so far are based on either\nvariational autoencoder (VAE) or generative adversarial network (GAN). Here we\npropose a new type model based on an adversarially regularized autoencoder\n(ARAE). It basically uses latent variables like VAE, but the distribution of\nthe latent variables is obtained by adversarial training like in GAN. The\nlatter is intended to avoid both inappropriate approximation of posterior\ndistribution in VAE and difficulty in handling discrete variables in GAN. Our\nbenchmark study showed that ARAE indeed outperformed conventional models in\nterms of validity, uniqueness, and novelty per generated molecule. We also\ndemonstrated successful conditional generation of drug-like molecules with ARAE\nfor both cases of single and multiple properties control. As a potential\nreal-world application, we could generate EGFR inhibitors sharing the scaffolds\nof known active molecules while satisfying drug-like conditions simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 04:23:15 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Hong", "Seung Hwan", ""], ["Lim", "Jaechang", ""], ["Ryu", "Seongok", ""], ["Kim", "Woo Youn", ""]]}, {"id": "1912.05625", "submitter": "Seonwoo Min", "authors": "Seonwoo Min, Seunghyun Park, Siwon Kim, Hyun-Soo Choi, Sungroh Yoon", "title": "Pre-Training of Deep Bidirectional Protein Sequence Representations with\n  Structural Information", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Bridging the exponentially growing gap between the number of\nunlabeled and labeled proteins, a couple of works have adopted semi-supervised\nlearning for protein sequence modeling. They pre-train a model with a\nsubstantial amount of unlabeled data and transfer the learned representations\nto various downstream tasks. Nonetheless, the current pre-training methods\nmostly rely on a language modeling task and often show limited performances.\nTherefore, a complementary protein-specific task for pre-training is necessary\nto better capture the information contained within unlabeled protein sequences.\n  Results: In this paper, we introduce a novel pre-training scheme called PLUS,\nwhich stands for Protein sequence representations Learned Using Structural\ninformation. PLUS consists of masked language modeling and a complementary\nprotein-specific pre-training task, namely same family prediction. PLUS can be\nused to pre-train various model architectures. In this work, we mainly use PLUS\nto pre-train a recurrent neural network (RNN) and refer to the resulting model\nas PLUS-RNN. It advances state-of-the-art pre-training methods on six out of\nseven tasks, i.e., (1) three protein(-pair)-level classification, (2) two\nprotein-level regression, and (3) two amino-acid-level classification tasks.\nFurthermore, we present results from our ablation studies and interpretation\nanalyses to better understand the strengths of PLUS-RNN.\n  Availability: The codes and pre-trained models are available at\nhttps://github.com/mswzeus/PLUS/\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:12:10 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 09:06:30 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 03:58:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Min", "Seonwoo", ""], ["Park", "Seunghyun", ""], ["Kim", "Siwon", ""], ["Choi", "Hyun-Soo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1912.05629", "submitter": "Raffaello Camoriano", "authors": "Raffaello Camoriano", "title": "Large-scale Kernel Methods and Applications to Lifelong Robot Learning", "comments": "Ph. D. Thesis for the Doctoral Course in Bioengineering and Robotics\n  (Curriculum in Humanoid Robotics) at Universit\\`a degli Studi di Genova, in\n  collaboration with Istituto Italiano di Tecnologia. Advisors: Prof. Giorgio\n  Metta and Prof. Lorenzo Rosasco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size and richness of available datasets grow larger, the opportunities\nfor solving increasingly challenging problems with algorithms learning directly\nfrom data grow at the same pace. Consequently, the capability of learning\nalgorithms to work with large amounts of data has become a crucial scientific\nand technological challenge for their practical applicability. Hence, it is no\nsurprise that large-scale learning is currently drawing plenty of research\neffort in the machine learning research community. In this thesis, we focus on\nkernel methods, a theoretically sound and effective class of learning\nalgorithms yielding nonparametric estimators. Kernel methods, in their\nclassical formulations, are accurate and efficient on datasets of limited size,\nbut do not scale up in a cost-effective manner. Recent research has shown that\napproximate learning algorithms, for instance random subsampling methods like\nNystr\\\"om and random features, with time-memory-accuracy trade-off mechanisms\nare more scalable alternatives. In this thesis, we provide analyses of the\ngeneralization properties and computational requirements of several types of\nsuch approximation schemes. In particular, we expose the tight relationship\nbetween statistics and computations, with the goal of tailoring the accuracy of\nthe learning process to the available computational resources. Our results are\nsupported by experimental evidence on large-scale datasets and numerical\nsimulations. We also study how large-scale learning can be applied to enable\naccurate, efficient, and reactive lifelong learning for robotics. In\nparticular, we propose algorithms allowing robots to learn continuously from\nexperience and adapt to changes in their operational environment. The proposed\nmethods are validated on the iCub humanoid robot in addition to other\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:08:35 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Camoriano", "Raffaello", ""]]}, {"id": "1912.05634", "submitter": "Orestis Loukas", "authors": "Orestis Loukas", "title": "Self-regularizing restricted Boltzmann machines", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on the grand-canonical extension of the ordinary restricted\nBoltzmann machine, we suggest an energy-based model for feature extraction that\nuses a layer of hidden units with varying size. By an appropriate choice of the\nchemical potential and given a sufficiently large number of hidden resources\nthe generative model is able to efficiently deduce the optimal number of hidden\nunits required to learn the target data with exceedingly small generalization\nerror. The formal simplicity of the grand-canonical ensemble combined with a\nrapidly converging ansatz in mean-field theory enable us to recycle\nwell-established numerical algothhtims during training, like contrastive\ndivergence, with only minor changes. As a proof of principle and to demonstrate\nthe novel features of grand-canonical Boltzmann machines, we train our\ngenerative models on data from the Ising theory and MNIST.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:00:09 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Loukas", "Orestis", ""]]}, {"id": "1912.05638", "submitter": "Kadir G\\\"um\\\"us", "authors": "Kadir G\\\"um\\\"us, Alex Alvarado, Bin Chen, Christian H\\\"ager, Erik\n  Agrell", "title": "End-to-End Learning of Geometrical Shaping Maximizing Generalized Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GMI-based end-to-end learning is shown to be highly nonconvex. We apply\ngradient descent initialized with Gray-labeled APSK constellations directly to\nthe constellation coordinates. State-of-the-art constellations in 2D and 4D are\nfound providing reach increases up to 26\\% w.r.t. to QAM.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:25:46 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["G\u00fcm\u00fcs", "Kadir", ""], ["Alvarado", "Alex", ""], ["Chen", "Bin", ""], ["H\u00e4ger", "Christian", ""], ["Agrell", "Erik", ""]]}, {"id": "1912.05651", "submitter": "Erik Daxberger", "authors": "Erik Daxberger, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution\n  Detection", "comments": "21 pages, extended version with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their successes, deep neural networks may make unreliable predictions\nwhen faced with test data drawn from a distribution different to that of the\ntraining data, constituting a major problem for AI safety. While this has\nrecently motivated the development of methods to detect such\nout-of-distribution (OoD) inputs, a robust solution is still lacking. We\npropose a new probabilistic, unsupervised approach to this problem based on a\nBayesian variational autoencoder model, which estimates a full posterior\ndistribution over the decoder parameters using stochastic gradient Markov chain\nMonte Carlo, instead of fitting a point estimate. We describe how\ninformation-theoretic measures based on this posterior can then be used to\ndetect OoD inputs both in input space and in the model's latent space. We\nempirically demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:37:54 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 21:26:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 07:07:28 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Daxberger", "Erik", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1912.05652", "submitter": "Siddharth Reddy", "authors": "Siddharth Reddy, Anca D. Dragan, Sergey Levine, Shane Legg, Jan Leike", "title": "Learning Human Objectives by Evaluating Hypothetical Behavior", "comments": "Published at International Conference on Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to align agent behavior with a user's objectives in a reinforcement\nlearning setting with unknown dynamics, an unknown reward function, and unknown\nunsafe states. The user knows the rewards and unsafe states, but querying the\nuser is expensive. To address this challenge, we propose an algorithm that\nsafely and interactively learns a model of the user's reward function. We start\nwith a generative model of initial states and a forward dynamics model trained\non off-policy data. Our method uses these models to synthesize hypothetical\nbehaviors, asks the user to label the behaviors with rewards, and trains a\nneural network to predict the rewards. The key idea is to actively synthesize\nthe hypothetical behaviors from scratch by maximizing tractable proxies for the\nvalue of information, without interacting with the environment. We call this\nmethod reward query synthesis via trajectory optimization (ReQueST). We\nevaluate ReQueST with simulated users on a state-based 2D navigation task and\nthe image-based Car Racing video game. The results show that ReQueST\nsignificantly outperforms prior methods in learning reward models that transfer\nto new environments with different initial state distributions. Moreover,\nReQueST safely trains the reward model to detect unsafe states, and corrects\nreward hacking before deploying the agent.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:25:48 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 22:26:35 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Reddy", "Siddharth", ""], ["Dragan", "Anca D.", ""], ["Levine", "Sergey", ""], ["Legg", "Shane", ""], ["Leike", "Jan", ""]]}, {"id": "1912.05663", "submitter": "Stephanie Chan", "authors": "Stephanie C.Y. Chan, Samuel Fishman, John Canny, Anoop Korattikara,\n  Sergio Guadarrama", "title": "Measuring the Reliability of Reinforcement Learning Algorithms", "comments": "Accepted for publication at ICLR 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lack of reliability is a well-known issue for reinforcement learning (RL)\nalgorithms. This problem has gained increasing attention in recent years, and\nefforts to improve it have grown substantially. To aid RL researchers and\nproduction users with the evaluation and improvement of reliability, we propose\na set of metrics that quantitatively measure different aspects of reliability.\nIn this work, we focus on variability and risk, both during training and after\nlearning (on a fixed policy). We designed these metrics to be general-purpose,\nand we also designed complementary statistical tests to enable rigorous\ncomparisons on these metrics. In this paper, we first describe the desired\nproperties of the metrics and their design, the aspects of reliability that\nthey measure, and their applicability to different scenarios. We then describe\nthe statistical tests and make additional practical recommendations for\nreporting results. The metrics and accompanying statistical tools have been\nmade available as an open-source library at\nhttps://github.com/google-research/rl-reliability-metrics. We apply our metrics\nto a set of common RL algorithms and environments, compare them, and analyze\nthe results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:50:33 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 19:23:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chan", "Stephanie C. Y.", ""], ["Fishman", "Samuel", ""], ["Canny", "John", ""], ["Korattikara", "Anoop", ""], ["Guadarrama", "Sergio", ""]]}, {"id": "1912.05671", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael\n  Carbin", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis", "comments": "Published in ICML 2020. This submission subsumes arXiv:1903.01611\n  (\"Stabilizing the Lottery Ticket Hypothesis\" and \"The Lottery Ticket\n  Hypothesis at Scale\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether a neural network optimizes to the same, linearly connected\nminimum under different samples of SGD noise (e.g., random data order and\naugmentation). We find that standard vision models become stable to SGD noise\nin this way early in training. From then on, the outcome of optimization is\ndetermined to a linearly connected region. We use this technique to study\niterative magnitude pruning (IMP), the procedure used by work on the lottery\nticket hypothesis to identify subnetworks that could have trained in isolation\nto full accuracy. We find that these subnetworks only reach full accuracy when\nthey are stable to SGD noise, which either occurs at initialization for\nsmall-scale settings (MNIST) or early in training for large-scale settings\n(ResNet-50 and Inception-v3 on ImageNet).\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:22:21 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 20:39:29 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 19:36:46 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 20:31:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Frankle", "Jonathan", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Carbin", "Michael", ""]]}, {"id": "1912.05686", "submitter": "Daniel T Chang", "authors": "Daniel T. Chang", "title": "Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are full of hyperparameters, which are set manually\nbefore the learning process can start. To find the best configuration for these\nhyperparameters in such a high dimensional space, with time-consuming and\nexpensive model training / validation, is not a trivial challenge. Bayesian\noptimization is a powerful tool for the joint optimization of hyperparameters,\nefficiently trading off exploration and exploitation of the hyperparameter\nspace. In this paper, we discuss Bayesian hyperparameter optimization,\nincluding hyperparameter optimization, Bayesian optimization, and Gaussian\nprocesses. We also review BoTorch, GPyTorch and Ax, the new open-source\nframeworks that we use for Bayesian optimization, Gaussian process inference\nand adaptive experimentation, respectively. For experimentation, we apply\nBayesian hyperparameter optimization, for optimizing group weights, to weighted\ngroup pooling, which couples unsupervised tiered graph autoencoders learning\nand supervised graph prediction learning for molecular graphs. We find that Ax,\nBoTorch and GPyTorch together provide a simple-to-use but powerful framework\nfor Bayesian hyperparameter optimization, using Ax's high-level API that\nconstructs and runs a full optimization loop and returns the best\nhyperparameter configuration.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:11:40 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 14:22:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chang", "Daniel T.", ""]]}, {"id": "1912.05687", "submitter": "Omid Bazgir", "authors": "Omid Bazgir, Ruibo Zhang, Saugato Rahman Dhruba, Raziur Rahman,\n  Souparno Ghosh, Ranadip Pal", "title": "REFINED (REpresentation of Features as Images with NEighborhood\n  Dependencies): A novel feature representation for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-18197-y", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning with Convolutional Neural Networks has shown great promise in\nvarious areas of image-based classification and enhancement but is often\nunsuitable for predictive modeling involving non-image based features or\nfeatures without spatial correlations. We present a novel approach for\nrepresentation of high dimensional feature vector in a compact image form,\ntermed REFINED (REpresentation of Features as Images with NEighborhood\nDependencies), that is conducible for convolutional neural network based deep\nlearning. We consider the correlations between features to generate a compact\nrepresentation of the features in the form of a two-dimensional image using\nminimization of pairwise distances similar to multi-dimensional scaling. We\nhypothesize that this approach enables embedded feature selection and\nintegrated with Convolutional Neural Network based Deep Learning can produce\nmore accurate predictions as compared to Artificial Neural Networks, Random\nForests and Support Vector Regression. We illustrate the superior predictive\nperformance of the proposed representation, as compared to existing approaches,\nusing synthetic datasets, cell line efficacy prediction based on drug chemical\ndescriptors for NCI60 dataset and drug sensitivity prediction based on\ntranscriptomic data and chemical descriptors using GDSC dataset. Results\nillustrated on both synthetic and biological datasets shows the higher\nprediction accuracy of the proposed framework as compared to existing\nmethodologies while maintaining desirable properties in terms of bias and\nfeature extraction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:18:05 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:26:15 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Bazgir", "Omid", ""], ["Zhang", "Ruibo", ""], ["Dhruba", "Saugato Rahman", ""], ["Rahman", "Raziur", ""], ["Ghosh", "Souparno", ""], ["Pal", "Ranadip", ""]]}, {"id": "1912.05693", "submitter": "Hao Yan", "authors": "Ziyue Li, Nurettin Dorukhan Sergin, Hao Yan, Chen Zhang, Fugee Tsung", "title": "Tensor Completion for Weakly-dependent Data on Graph for Metro Passenger\n  Flow Prediction", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor decomposition and completion have attracted significant\ninterest from academia given the ubiquity of tensor data. However, the low-rank\nstructure is a global property, which will not be fulfilled when the data\npresents complex and weak dependencies given specific graph structures. One\nparticular application that motivates this study is the spatiotemporal data\nanalysis. As shown in the preliminary study, weakly dependencies can worsen the\nlow-rank tensor completion performance. In this paper, we propose a novel\nlow-rank CANDECOMP / PARAFAC (CP) tensor decomposition and completion framework\nby introducing the $L_{1}$-norm penalty and Graph Laplacian penalty to model\nthe weakly dependency on graph. We further propose an efficient optimization\nalgorithm based on the Block Coordinate Descent for efficient estimation. A\ncase study based on the metro passenger flow data in Hong Kong is conducted to\ndemonstrate improved performance over the regular tensor completion methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:29:25 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Li", "Ziyue", ""], ["Sergin", "Nurettin Dorukhan", ""], ["Yan", "Hao", ""], ["Zhang", "Chen", ""], ["Tsung", "Fugee", ""]]}, {"id": "1912.05695", "submitter": "Baekjin Kim", "authors": "Baekjin Kim, Ambuj Tewari", "title": "Randomized Exploration for Non-Stationary Stochastic Linear Bandits", "comments": "In Proceedings of the 36th Annual Conference on Uncertainty in\n  Artificial Intelligence, 2020. (UAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two perturbation approaches to overcome conservatism that\noptimism based algorithms chronically suffer from in practice. The first\napproach replaces optimism with a simple randomization when using confidence\nsets. The second one adds random perturbations to its current estimate before\nmaximizing the expected reward. For non-stationary linear bandits, where each\naction is associated with a $d$-dimensional feature and the unknown parameter\nis time-varying with total variation $B_T$, we propose two randomized\nalgorithms, Discounted Randomized LinUCB (D-RandLinUCB) and Discounted Linear\nThompson Sampling (D-LinTS) via the two perturbation approaches. We highlight\nthe statistical optimality versus computational efficiency trade-off between\nthem in that the former asymptotically achieves the optimal dynamic regret\n$\\tilde{\\mathcal{O}}(d ^{2/3}B_T^{1/3} T^{2/3})$, but the latter is\noracle-efficient with an extra logarithmic factor in the number of arms\ncompared to minimax-optimal dynamic regret. In a simulation study, both\nalgorithms show outstanding performance in tackling conservatism issue that\nDiscounted LinUCB struggles with.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 23:34:12 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 00:33:57 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 18:40:14 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 17:04:55 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Kim", "Baekjin", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1912.05723", "submitter": "Karthikeyan K", "authors": "Karthikeyan K, Shubham Kumar Bharti, Piyush Rai", "title": "On the relationship between multitask neural networks and multitask\n  Gaussian Processes", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the effectiveness of multitask deep neural network (MTDNN), there is\na limited theoretical understanding on how the information is shared across\ndifferent tasks in MTDNN. In this work, we establish a formal connection\nbetween MTDNN with infinitely-wide hidden layers and multitask Gaussian Process\n(GP). We derive multitask GP kernels corresponding to both single-layer and\ndeep multitask Bayesian neural networks (MTBNN) and show that information among\ndifferent tasks is shared primarily due to correlation across last layer\nweights of MTBNN and shared hyper-parameters, which is contrary to the popular\nhypothesis that information is shared because of shared intermediate layer\nweights. Our construction enables using multitask GP to perform efficient\nBayesian inference for the equivalent MTDNN with infinitely-wide hidden layers.\nPrior work on the connection between deep neural networks and GP for single\ntask settings can be seen as special cases of our construction. We also present\nan adaptive multitask neural network architecture that corresponds to a\nmultitask GP with more flexible kernels, such as Linear Model of\nCoregionalization (LMC) and Cross-Coregionalization (CC) kernels. We provide\nexperimental results to further illustrate these ideas on synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 01:51:35 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["K", "Karthikeyan", ""], ["Bharti", "Shubham Kumar", ""], ["Rai", "Piyush", ""]]}, {"id": "1912.05753", "submitter": "Ramtin Hosseini", "authors": "Ramtin Hosseini, Neda Hassanpour, Li-Ping Liu and Soha Hassoun", "title": "Pathway-Activity Likelihood Analysis and Metabolite Annotation for\n  Untargeted Metabolomics using Probabilistic Modeling", "comments": "For more details, please visit my homepage at:\n  https://www.eecs.tufts.edu/~ramtin/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Untargeted metabolomics comprehensively characterizes small\nmolecules and elucidates activities of biochemical pathways within a biological\nsample. Despite computational advances, interpreting collected measurements and\ndetermining their biological role remains a challenge. Results: To interpret\nmeasurements, we present an inference-based approach, termed Probabilistic\nmodeling for Untargeted Metabolomics Analysis (PUMA). Our approach captures\nmeasurements and known information about the sample under study in a generative\nmodel and uses stochastic sampling to compute posterior probability\ndistributions. PUMA predicts the likelihood of pathways being active, and then\nderives a probabilistic annotation, which assigns chemical identities to the\nmeasurements. PUMA is validated on synthetic datasets. When applied to test\ncases, the resulting pathway activities are biologically meaningful and\ndistinctly different from those obtained using statistical pathway enrichment\ntechniques. Annotation results are in agreement to those obtained using other\ntools that utilize additional information in the form of spectral signatures.\nImportantly, PUMA annotates many additional measurements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 03:26:37 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:32:32 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hosseini", "Ramtin", ""], ["Hassanpour", "Neda", ""], ["Liu", "Li-Ping", ""], ["Hassoun", "Soha", ""]]}, {"id": "1912.05779", "submitter": "Melanie F. Pradier", "authors": "Beau Coker, Melanie F. Pradier, Finale Doshi-Velez", "title": "Towards Expressive Priors for Bayesian Neural Networks: Poisson Process\n  Radial Basis Function Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian neural networks have many appealing characteristics, current\npriors do not easily allow users to specify basic properties such as expected\nlengthscale or amplitude variance. In this work, we introduce Poisson Process\nRadial Basis Function Networks, a novel prior that is able to encode amplitude\nstationarity and input-dependent lengthscale. We prove that our novel\nformulation allows for a decoupled specification of these properties, and that\nthe estimated regression function is consistent as the number of observations\ntends to infinity. We demonstrate its behavior on synthetic and real examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 05:37:46 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Coker", "Beau", ""], ["Pradier", "Melanie F.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1912.05796", "submitter": "Haoyu Yang", "authors": "Haoyu Yang, Wen Chen, Piyush Pathak, Frank Gennari, Ya-Chieh Lai, Bei\n  Yu", "title": "Automatic Layout Generation with Applications in Machine Learning Engine\n  Evaluation", "comments": "6 pages, submitted to 1st ACM/IEEE Workshop on Machine Learning for\n  CAD (MLCAD) for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based lithography hotspot detection has been deeply studied\nrecently, from varies feature extraction techniques to efficient learning\nmodels. It has been observed that such machine learning-based frameworks are\nproviding satisfactory metal layer hotspot prediction results on known public\nmetal layer benchmarks. In this work, we seek to evaluate how these machine\nlearning-based hotspot detectors generalize to complicated patterns. We first\nintroduce a automatic layout generation tool that can synthesize varies layout\npatterns given a set of design rules. The tool currently supports both metal\nlayer and via layer generation. As a case study, we conduct hotspot detection\non the generated via layer layouts with representative machine learning-based\nhotspot detectors, which shows that continuous study on model robustness and\ngenerality is necessary to prototype and integrate the learning engines in DFM\nflows. The source code of the layout generation tool will be available at\nhttps://github. com/phdyang007/layout-generation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 06:52:12 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yang", "Haoyu", ""], ["Chen", "Wen", ""], ["Pathak", "Piyush", ""], ["Gennari", "Frank", ""], ["Lai", "Ya-Chieh", ""], ["Yu", "Bei", ""]]}, {"id": "1912.05810", "submitter": "Owen Thomas", "authors": "Owen Thomas, Jukka Corander", "title": "Diagnosing model misspecification and performing generalized Bayes'\n  updates via probabilistic classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model misspecification is a long-standing enigma of the Bayesian inference\nframework as posteriors tend to get overly concentrated on ill-informed\nparameter values towards the large sample limit. Tempering of the likelihood\nhas been established as a safer way to do updates from prior to posterior in\nthe presence of model misspecification. At one extreme tempering can ignore the\ndata altogether and at the other extreme it provides the standard Bayes' update\nwhen no misspecification is assumed to be present. However, it is an open issue\nhow to best recognize misspecification and choose a suitable level of tempering\nwithout access to the true generating model. Here we show how probabilistic\nclassifiers can be employed to resolve this issue. By training a probabilistic\nclassifier to discriminate between simulated and observed data provides an\nestimate of the ratio between the model likelihood and the likelihood of the\ndata under the unobserved true generative process, within the discriminatory\nabilities of the classifier. The expectation of the logarithm of a ratio with\nrespect to the data generating process gives an estimation of the negative\nKullback-Leibler divergence between the statistical generative model and the\ntrue generative distribution. Using a set of canonical examples we show that\nthis divergence provides a useful misspecification diagnostic, a model\ncomparison tool, and a method to inform a generalised Bayesian update in the\npresence of misspecification for likelihood-based models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 07:45:21 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Thomas", "Owen", ""], ["Corander", "Jukka", ""]]}, {"id": "1912.05827", "submitter": "Giyoung Jeon", "authors": "Giyoung Jeon, Haedong Jeong and Jaesik Choi", "title": "An Efficient Explorative Sampling Considering the Generative Boundaries\n  of Deep Generative Neural Networks", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative neural networks (DGNNs) have achieved realistic and\nhigh-quality data generation. In particular, the adversarial training scheme\nhas been applied to many DGNNs and has exhibited powerful performance. Despite\nof recent advances in generative networks, identifying the image generation\nmechanism still remains challenging. In this paper, we present an explorative\nsampling algorithm to analyze generation mechanism of DGNNs. Our method\nefficiently obtains samples with identical attributes from a query image in a\nperspective of the trained model. We define generative boundaries which\ndetermine the activation of nodes in the internal layer and probe inside the\nmodel with this information. To handle a large number of boundaries, we obtain\nthe essential set of boundaries using optimization. By gathering samples within\nthe region surrounded by generative boundaries, we can empirically reveal the\ncharacteristics of the internal layers of DGNNs. We also demonstrate that our\nalgorithm can find more homogeneous, the model specific samples compared to the\nvariations of {\\epsilon}-based sampling method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:27:46 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Jeon", "Giyoung", ""], ["Jeong", "Haedong", ""], ["Choi", "Jaesik", ""]]}, {"id": "1912.05830", "submitter": "Zhuoran Yang", "authors": "Qi Cai, Zhuoran Yang, Chi Jin, Zhaoran Wang", "title": "Provably Efficient Exploration in Policy Optimization", "comments": "We have fixed a technical issue in the first version of this paper.\n  We remark the technical assumption of the linear MDP in this version of the\n  paper is different from that in the first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While policy-based reinforcement learning (RL) achieves tremendous successes\nin practice, it is significantly less understood in theory, especially compared\nwith value-based RL. In particular, it remains elusive how to design a provably\nefficient policy optimization algorithm that incorporates exploration. To\nbridge such a gap, this paper proposes an Optimistic variant of the Proximal\nPolicy Optimization algorithm (OPPO), which follows an ``optimistic version''\nof the policy gradient direction. This paper proves that, in the problem of\nepisodic Markov decision process with linear function approximation, unknown\ntransition, and adversarial reward with full-information feedback, OPPO\nachieves $\\tilde{O}(\\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature\ndimension, $H$ is the episode horizon, and $T$ is the total number of steps. To\nthe best of our knowledge, OPPO is the first provably efficient policy\noptimization algorithm that explores.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:40:02 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 17:38:42 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 17:38:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cai", "Qi", ""], ["Yang", "Zhuoran", ""], ["Jin", "Chi", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1912.05833", "submitter": "Triantafyllos Kefalas", "authors": "Triantafyllos Kefalas, Konstantinos Vougioukas, Yannis Panagakis,\n  Stavros Petridis, Jean Kossaifi, Maja Pantic", "title": "Speech-driven facial animation using polynomial fusion of features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-driven facial animation involves using a speech signal to generate\nrealistic videos of talking faces. Recent deep learning approaches to facial\nsynthesis rely on extracting low-dimensional representations and concatenating\nthem, followed by a decoding step of the concatenated vector. This accounts for\nonly first-order interactions of the features and ignores higher-order\ninteractions. In this paper we propose a polynomial fusion layer that models\nthe joint representation of the encodings by a higher-order polynomial, with\nthe parameters modelled by a tensor decomposition. We demonstrate the\nsuitability of this approach through experiments on generated videos evaluated\non a range of metrics on video quality, audiovisual synchronisation and\ngeneration of blinks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:46:57 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:36:16 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kefalas", "Triantafyllos", ""], ["Vougioukas", "Konstantinos", ""], ["Panagakis", "Yannis", ""], ["Petridis", "Stavros", ""], ["Kossaifi", "Jean", ""], ["Pantic", "Maja", ""]]}, {"id": "1912.05879", "submitter": "Johan Medrano", "authors": "Johan Medrano, Fuchun Joseph Lin", "title": "Enabling Machine Learning Across Heterogeneous Sensor Networks with\n  Graph Autoencoders", "comments": null, "journal-ref": "Chatzigiannakis I., De Ruyter B., Mavrommati I. (eds) Ambient\n  Intelligence. AmI 2019. Lecture Notes in Computer Science, vol 11912.\n  Springer, Cham", "doi": "10.1007/978-3-030-34255-5_11", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has been applied to enable many life-assisting\nappli-cations, such as abnormality detection and emdergency request for the\nsoli-tary elderly. However, in most cases machine learning algorithms depend on\nthe layout of the target Internet of Things (IoT) sensor network. Hence, to\ndeploy an application across Heterogeneous Sensor Networks (HSNs), i.e. sensor\nnetworks with different sensors type or layouts, it is required to repeat the\nprocess of data collection and ML algorithm training. In this paper, we\nintroduce a novel framework leveraging deep learning for graphs to enable using\nthe same activity recognition system across HSNs deployed in differ-ent smart\nhomes. Using our framework, we were able to transfer activity classifiers\ntrained with activity labels on a source HSN to a target HSN, reaching about\n75% of the baseline accuracy on the target HSN without us-ing target activity\nlabels. Moreover, our model can quickly adapt to unseen sensor layouts, which\nmakes it highly suitable for the gradual deployment of real-world ML-based\napplications. In addition, we show that our framework is resilient to\nsuboptimal graph representations of HSNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 11:14:12 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Medrano", "Johan", ""], ["Lin", "Fuchun Joseph", ""]]}, {"id": "1912.05901", "submitter": "Llu\\'is Antoni Jim\\'enez Rugama", "authors": "Giuseppe Nuti and Llu\\'is Antoni Jim\\'enez Rugama and Kaspar Thommen", "title": "Adaptive Bayesian Reticulum", "comments": "23 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks and Decision Trees: two popular techniques for supervised\nlearning that are seemingly disconnected in their formulation and optimization\nmethod, have recently been combined in a single construct. The connection\npivots on assembling an artificial Neural Network with nodes that allow for a\ngate-like function to mimic a tree split, optimized using the standard approach\nof recursively applying the chain rule to update its parameters. Yet two main\nchallenges have impeded wide use of this hybrid approach: (a) the inability of\nglobal gradient ascent techniques to optimize hierarchical parameters (as\nintroduced by the gate function); and (b) the construction of the tree\nstructure, which has relied on standard decision tree algorithms to learn the\nnetwork topology or incrementally (and heuristically) searching the space at\nrandom. Here we propose a probabilistic construct that exploits the idea of a\nnode's unexplained potential (the total error channeled through the node) in\norder to decide where to expand further, mimicking the standard tree\nconstruction in a Neural Network setting, alongside a modified gradient ascent\nthat first locally optimizes an expanded node before a global optimization. The\nprobabilistic approach allows us to evaluate each new split as a ratio of\nlikelihoods that balances the statistical improvement in explaining the\nevidence against the additional model complexity --- thus providing a natural\nstopping condition. The result is a novel classification and regression\ntechnique that leverages the strength of both: a tree-structure that grows\nnaturally and is simple to interpret with the plasticity of Neural Networks\nthat allow for soft margins and slanted boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 12:54:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 13:36:40 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 13:16:09 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Nuti", "Giuseppe", ""], ["Rugama", "Llu\u00eds Antoni Jim\u00e9nez", ""], ["Thommen", "Kaspar", ""]]}, {"id": "1912.05903", "submitter": "Weikaixin Kong", "authors": "Weikaixin Kong, Xinyu Tu, Zhengwei Xie and Zhuo Huang", "title": "Prediction and optimization of NaV1.7 inhibitors based on machine\n  learning methods", "comments": "The evaluation of the model in the results section of this article is\n  not comprehensive enough.We will carry out further work. The article needs to\n  be polished. There are certain disadvantages to the molecular optimization\n  method. The discussion part is not deep enough, so withdraw is needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We used machine learning methods to predict NaV1.7 inhibitors and found the\nmodel RF-CDK that performed best on the imbalanced dataset. Using the RF-CDK\nmodel for screening drugs, we got effective compounds K1. We use the cell patch\nclamp method to verify K1. However, because the model evaluation method in this\narticle is not comprehensive enough, there is still a lot of research work to\nbe performed, such as comparison with other existing methods.\n  The target protein has multiple active sites and requires our further\nresearch. We need more detailed models to consider this biological process and\ncompare it with the current results, which is an error in this article.\n  So we want to withdraw this article.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:56:03 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 10:01:40 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kong", "Weikaixin", ""], ["Tu", "Xinyu", ""], ["Xie", "Zhengwei", ""], ["Huang", "Zhuo", ""]]}, {"id": "1912.05905", "submitter": "Radu Alexandru Rosu", "authors": "Radu Alexandru Rosu, Peer Sch\\\"utt, Jan Quenzel, Sven Behnke", "title": "LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. However, applying the same\nmethods on 3D data still poses challenges due to the heavy memory requirements\nand the lack of structured data. Here, we propose LatticeNet, a novel approach\nfor 3D semantic segmentation, which takes as input raw point clouds. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on various datasets where our method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 13:01:36 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 15:29:58 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 16:33:59 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rosu", "Radu Alexandru", ""], ["Sch\u00fctt", "Peer", ""], ["Quenzel", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "1912.05910", "submitter": "Tianfan Fu", "authors": "Tianfan Fu, Cao Xiao, Jimeng Sun", "title": "CORE: Automatic Molecule Optimization Using Copy & Refine Strategy", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecule optimization is about generating molecule $Y$ with more desirable\nproperties based on an input molecule $X$. The state-of-the-art approaches\npartition the molecules into a large set of substructures $S$ and grow the new\nmolecule structure by iteratively predicting which substructure from $S$ to\nadd. However, since the set of available substructures $S$ is large, such an\niterative prediction task is often inaccurate especially for substructures that\nare infrequent in the training data. To address this challenge, we propose a\nnew generating strategy called \"Copy & Refine\" (CORE), where at each step the\ngenerator first decides whether to copy an existing substructure from input $X$\nor to generate a new substructure, then the most promising substructure will be\nadded to the new molecule. Combining together with scaffolding tree generation\nand adversarial training, CORE can significantly improve several latest\nmolecule optimization methods in various measures including drug likeness\n(QED), dopamine receptor (DRD2) and penalized LogP. We tested CORE and\nbaselines using the ZINC database and CORE obtained up to 11% and 21%\nrelatively improvement over the baselines on success rate on the complete test\nset and the subset with infrequent substructures, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 23:02:31 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Fu", "Tianfan", ""], ["Xiao", "Cao", ""], ["Sun", "Jimeng", ""]]}, {"id": "1912.05911", "submitter": "Robin Marc Schmidt", "authors": "Robin M. Schmidt", "title": "Recurrent Neural Networks (RNNs): A gentle Introduction and Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art solutions in the areas of \"Language Modelling & Generating\nText\", \"Speech Recognition\", \"Generating Image Descriptions\" or \"Video Tagging\"\nhave been using Recurrent Neural Networks as the foundation for their\napproaches. Understanding the underlying concepts is therefore of tremendous\nimportance if we want to keep up with recent or upcoming publications in those\nareas. In this work we give a short overview over some of the most important\nconcepts in the realm of Recurrent Neural Networks which enables readers to\neasily understand the fundamentals such as but not limited to \"Backpropagation\nthrough Time\" or \"Long Short-Term Memory Units\" as well as some of the more\nrecent advances like the \"Attention Mechanism\" or \"Pointer Networks\". We also\ngive recommendations for further reading regarding more complex topics where it\nis necessary.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 06:36:13 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Schmidt", "Robin M.", ""]]}, {"id": "1912.05915", "submitter": "Jonathan Baxter", "authors": "Jonathan Baxter", "title": "Some observations concerning Off Training Set (OTS) error", "comments": "Technical Report, Australian National University, August 1999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A form of generalisation error known as Off Training Set (OTS) error was\nrecently introduced in [Wolpert, 1996b], along with a theorem showing that\nsmall training set error does not guarantee small OTS error, unless assumptions\nare made about the target function. Here it is shown that the applicability of\nthis theorem is limited to models in which the distribution generating training\ndata has no overlap with the distribution generating test data. It is argued\nthat such a scenario is of limited relevance to machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:01:10 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Baxter", "Jonathan", ""]]}, {"id": "1912.05920", "submitter": "Xuewen Yao", "authors": "Xuewen Yao, Dong He, Tiancheng Jing, Kaya de Barbaro", "title": "Measuring Mother-Infant Emotions By Audio Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested in developmental psychology literature that the\ncommunication of affect between mothers and their infants correlates with the\nsocioemotional and cognitive development of infants. In this study, we obtained\nday-long audio recordings of 10 mother-infant pairs in order to study their\naffect communication in speech with a focus on mother's speech. In order to\nbuild a model for speech emotion detection, we used the Ryerson Audio-Visual\nDatabase of Emotional Speech and Song (RAVDESS) and trained a Convolutional\nNeural Nets model which is able to classify 6 different emotions at 70%\naccuracy. We applied our model to mother's speech and found the dominant\nemotions were angry and sad, which were not true. Based on our own\nobservations, we concluded that emotional speech databases made with the help\nof actors cannot generalize well to real-life settings, suggesting an active\nlearning or unsupervised approach in the future.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:49:35 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yao", "Xuewen", ""], ["He", "Dong", ""], ["Jing", "Tiancheng", ""], ["de Barbaro", "Kaya", ""]]}, {"id": "1912.05944", "submitter": "Yifan Gao", "authors": "Yifan Gao, Vicente A. Gonzalez, Tak Wing Yiu, and Guillermo\n  Cabrera-Guerrerod", "title": "Non-linearity identification for construction workers'\n  personality-safety behaviour predictive relationship using neural network and\n  linear regression modelling", "comments": "The manuscript is currently undergoing a major revision as some\n  contents in its current form are not scientifically rigorous and can be\n  misleading to potential readers. Thus, we apply for withdrawal of the\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of workers' safety behaviour can help identify vulnerable\nworkers who intend to undertake unsafe behaviours and be useful in the design\nof management practices to minimise the occurrence of accidents. The latest\nliterature has evidenced that there is within-population diversity that leads\npeople's intended safety behaviours in the workplace, which are found to vary\namong individuals as a function of their personality traits. In this study, an\ninnovative forecasting model, which employs neural network algorithms, is\ndeveloped to numerically simulate the predictive relationship between\nconstruction workers' personality traits and their intended safety behaviour.\nThe data-driven nature of neural network enabled a reliable estimate of the\nrelationship, which allowed this research to find that a nonlinear effect\nexists in the relationship. This research has practical implications. The\nneural network developed is shown to have highly satisfactory prediction\naccuracy and is thereby potentially useful for assisting project\ndecision-makers to assess how prone workers are to carry out unsafe behaviours\nin the workplace.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:51:56 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 02:06:27 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 13:05:32 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Gao", "Yifan", ""], ["Gonzalez", "Vicente A.", ""], ["Yiu", "Tak Wing", ""], ["Cabrera-Guerrerod", "Guillermo", ""]]}, {"id": "1912.05945", "submitter": "Behzad Asadi", "authors": "Behzad Asadi, Vijay Varadharajan", "title": "Towards a Robust Classifier: An MDL-Based Method for Generating\n  Adversarial Examples", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.03751", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of adversarial examples in machine learning where an\nadversary tries to misguide a classifier by making functionality-preserving\nmodifications to original samples. We assume a black-box scenario where the\nadversary has access to only the feature set, and the final hard-decision\noutput of the classifier. We propose a method to generate adversarial examples\nusing the minimum description length (MDL) principle. Our final aim is to\nimprove the robustness of the classifier by considering generated examples in\nrebuilding the classifier. We evaluate our method for the application of static\nmalware detection in portable executable (PE) files. We consider API calls of\nPE files as their distinguishing features where the feature vector is a binary\nvector representing the presence-absence of API calls. In our method, we first\ncreate a dataset of benign samples by querying the target classifier. We next\nconstruct a code table of frequent patterns for the compression of this dataset\nusing the MDL principle. We finally generate an adversarial example\ncorresponding to a malware sample by selecting and adding a pattern from the\nbenign code table to the malware sample. The selected pattern is the one that\nminimizes the length of the compressed adversarial example given the code\ntable. This modification preserves the functionalities of the original malware\nsample as all original API calls are kept, and only some new API calls are\nadded. Considering a neural network, we show that the evasion rate is 78.24\npercent for adversarial examples compared to 8.16 percent for original malware\nsamples. This shows the effectiveness of our method in generating examples that\nneed to be considered in rebuilding the classifier.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:42:41 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Asadi", "Behzad", ""], ["Varadharajan", "Vijay", ""]]}, {"id": "1912.05977", "submitter": "Menghan Wang", "authors": "Menghan Wang, Kun Zhang, Gulin Li, Keping Yang, Luo Si", "title": "Tracing the Propagation Path: A Flow Perspective of Representation\n  Learning on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have gained significant developments in\nrepresentation learning on graphs. However, current GCNs suffer from two common\nchallenges: 1) GCNs are only effective with shallow structures; stacking\nmultiple GCN layers will lead to over-smoothing. 2) GCNs do not scale well with\nlarge, dense graphs due to the recursive neighborhood expansion. We generalize\nthe propagation strategies of current GCNs as a \\emph{\"Sink$\\to$Source\"} mode,\nwhich seems to be an underlying cause of the two challenges. To address these\nissues intrinsically, in this paper, we study the information propagation\nmechanism in a \\emph{\"Source$\\to$Sink\"} mode. We introduce a new concept\n\"information flow path\" that explicitly defines where information originates\nand how it diffuses. Then a novel framework, namely Flow Graph Network\n(FlowGN), is proposed to learn node representations. FlowGN is computationally\nefficient and flexible in propagation strategies. Moreover, FlowGN decouples\nthe layer structure from the information propagation process, removing the\ninterior constraint of applying deep structures in traditional GCNs. Further\nexperiments on public datasets demonstrate the superiority of FlowGN against\nstate-of-the-art GCNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:21:58 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Wang", "Menghan", ""], ["Zhang", "Kun", ""], ["Li", "Gulin", ""], ["Yang", "Keping", ""], ["Si", "Luo", ""]]}, {"id": "1912.06015", "submitter": "Andre Manoel", "authors": "Gaspar Rochette, Andre Manoel, Eric W. Tramel", "title": "Efficient Per-Example Gradient Computations in Convolutional Neural\n  Networks", "comments": null, "journal-ref": "Theory and Practice of Differential Privacy (TPDP) workshop at CCS\n  2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks leverage GPUs to perform massively-parallel\ncomputations over batches of many training examples efficiently. However, for\ncertain tasks, one may be interested in performing per-example computations,\nfor instance using per-example gradients to evaluate a quantity of interest\nunique to each example. One notable application comes from the field of\ndifferential privacy, where per-example gradients must be norm-bounded in order\nto limit the impact of each example on the aggregated batch gradient. In this\nwork, we discuss how per-example gradients can be efficiently computed in\nconvolutional neural networks (CNNs). We compare existing strategies by\nperforming a few steps of differentially-private training on CNNs of varying\nsizes. We also introduce a new strategy for per-example gradient calculation,\nwhich is shown to be advantageous depending on the model architecture and how\nthe model is trained. This is a first step in making differentially-private\ntraining of CNNs practical.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:10:14 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Rochette", "Gaspar", ""], ["Manoel", "Andre", ""], ["Tramel", "Eric W.", ""]]}, {"id": "1912.06036", "submitter": "Pranay Sharma", "authors": "Pranay Sharma, Swatantra Kafle, Prashant Khanduri, Saikiran Bulusu,\n  Ketan Rajawat, and Pramod K. Varshney", "title": "Parallel Restarted SPIDER -- Communication Efficient Distributed\n  Nonconvex Optimization with Optimal Computation Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed algorithm for stochastic smooth,\nnon-convex optimization. We assume a worker-server architecture where $N$\nnodes, each having $n$ (potentially infinite) number of samples, collaborate\nwith the help of a central server to perform the optimization task. The global\nobjective is to minimize the average of local cost functions available at\nindividual nodes. The proposed approach is a non-trivial extension of the\npopular parallel-restarted SGD algorithm, incorporating the optimal\nvariance-reduction based SPIDER gradient estimator into it. We prove\nconvergence of our algorithm to a first-order stationary solution. The proposed\napproach achieves the best known communication complexity $O(\\epsilon^{-1})$\nalong with the optimal computation complexity. For finite-sum problems (finite\n$n$), we achieve the optimal computation (IFO) complexity\n$O(\\sqrt{Nn}\\epsilon^{-1})$. For online problems ($n$ unknown or infinite), we\nachieve the optimal IFO complexity $O(\\epsilon^{-3/2})$. In both the cases, we\nmaintain the linear speedup achieved by existing methods. This is a massive\nimprovement over the $O(\\epsilon^{-2})$ IFO complexity of the existing\napproaches. Additionally, our algorithm is general enough to allow\nnon-identical distributions of data across workers, as in the recently proposed\nfederated learning paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:36:22 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 06:03:32 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sharma", "Pranay", ""], ["Kafle", "Swatantra", ""], ["Khanduri", "Prashant", ""], ["Bulusu", "Saikiran", ""], ["Rajawat", "Ketan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1912.06058", "submitter": "George Dasoulas", "authors": "George Dasoulas, Ludovic Dos Santos, Kevin Scaman, Aladin Virmaux", "title": "Coloring graph neural networks for node disambiguation", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that a simple coloring scheme can improve, both\ntheoretically and empirically, the expressive power of Message Passing Neural\nNetworks(MPNNs). More specifically, we introduce a graph neural network called\nColored Local Iterative Procedure (CLIP) that uses colors to disambiguate\nidentical node attributes, and show that this representation is a universal\napproximator of continuous functions on graphs with node attributes. Our method\nrelies on separability , a key topological characteristic that allows to extend\nwell-chosen neural networks into universal representations. Finally, we show\nexperimentally that CLIP is capable of capturing structural characteristics\nthat traditional MPNNs fail to distinguish,while being state-of-the-art on\nbenchmark graph classification datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:06:47 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Dasoulas", "George", ""], ["Santos", "Ludovic Dos", ""], ["Scaman", "Kevin", ""], ["Virmaux", "Aladin", ""]]}, {"id": "1912.06059", "submitter": "Petro Liashchynskyi", "authors": "Petro Liashchynskyi and Pavlo Liashchynskyi", "title": "Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS", "comments": "11 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare the three most popular algorithms for\nhyperparameter optimization (Grid Search, Random Search, and Genetic Algorithm)\nand attempt to use them for neural architecture search (NAS). We use these\nalgorithms for building a convolutional neural network (search architecture).\nExperimental results on CIFAR-10 dataset further demonstrate the performance\ndifference between compared algorithms. The comparison results are based on the\nexecution time of the above algorithms and accuracy of the proposed models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:07:20 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Liashchynskyi", "Petro", ""], ["Liashchynskyi", "Pavlo", ""]]}, {"id": "1912.06060", "submitter": "Xiaofei Shi", "authors": "Xiaofei Shi, David P. Woodruff", "title": "Sublinear Time Numerical Linear Algebra for Structured Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to solve a number of problems in numerical linear algebra, such\nas least squares regression, $\\ell_p$-regression for any $p \\geq 1$, low rank\napproximation, and kernel regression, in time $T(A) \\poly(\\log(nd))$, where for\na given input matrix $A \\in \\mathbb{R}^{n \\times d}$, $T(A)$ is the time needed\nto compute $A\\cdot y$ for an arbitrary vector $y \\in \\mathbb{R}^d$. Since $T(A)\n\\leq O(\\nnz(A))$, where $\\nnz(A)$ denotes the number of non-zero entries of\n$A$, the time is no worse, up to polylogarithmic factors, as all of the recent\nadvances for such problems that run in input-sparsity time. However, for many\napplications, $T(A)$ can be much smaller than $\\nnz(A)$, yielding significantly\nsublinear time algorithms. For example, in the overconstrained\n$(1+\\epsilon)$-approximate polynomial interpolation problem, $A$ is a\nVandermonde matrix and $T(A) = O(n \\log n)$; in this case our running time is\n$n \\cdot \\poly(\\log n) + \\poly(d/\\epsilon)$ and we recover the results of\n\\cite{avron2013sketching} as a special case. For overconstrained\nautoregression, which is a common problem arising in dynamical systems, $T(A) =\nO(n \\log n)$, and we immediately obtain $n \\cdot \\poly(\\log n) +\n\\poly(d/\\epsilon)$ time. For kernel autoregression, we significantly improve\nthe running time of prior algorithms for general kernels. For the important\ncase of autoregression with the polynomial kernel and arbitrary target vector\n$b\\in\\mathbb{R}^n$, we obtain even faster algorithms. Our algorithms show that,\nperhaps surprisingly, most of these optimization problems do not require much\nmore time than that of a polylogarithmic number of matrix-vector\nmultiplications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:13:51 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Shi", "Xiaofei", ""], ["Woodruff", "David P.", ""]]}, {"id": "1912.06073", "submitter": "He Jia", "authors": "He Jia, Uro\\v{s} Seljak", "title": "Normalizing Constant Estimation with Gaussianized Bridge Sampling", "comments": "Accepted by AABI 2019 Proceedings", "journal-ref": "Proceedings of The 2nd Symposium on Advances in Approximate\n  Bayesian Inference, PMLR 118:1-14, 2020", "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing constant (also called partition function, Bayesian evidence, or\nmarginal likelihood) is one of the central goals of Bayesian inference, yet\nmost of the existing methods are both expensive and inaccurate. Here we develop\na new approach, starting from posterior samples obtained with a standard Markov\nChain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to\nobtain an analytic density estimator from these samples, followed by Optimal\nBridge Sampling (OBS) to obtain the normalizing constant. We compare our method\nwhich we call Gaussianized Bridge Sampling (GBS) to existing methods such as\nNested Sampling (NS) and Annealed Importance Sampling (AIS) on several\nexamples, showing our method is both significantly faster and substantially\nmore accurate than these methods, and comes with a reliable error estimation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:50:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Jia", "He", ""], ["Seljak", "Uro\u0161", ""]]}, {"id": "1912.06074", "submitter": "Yifan Wu", "authors": "Fan Yang, Liu Leqi, Yifan Wu, Zachary C. Lipton, Pradeep Ravikumar,\n  William W. Cohen, Tom Mitchell", "title": "Game Design for Eliciting Distinguishable Behavior", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to inferring latent psychological traits from human behavior is\nkey to developing personalized human-interacting machine learning systems.\nApproaches to infer such traits range from surveys to manually-constructed\nexperiments and games. However, these traditional games are limited because\nthey are typically designed based on heuristics. In this paper, we formulate\nthe task of designing \\emph{behavior diagnostic games} that elicit\ndistinguishable behavior as a mutual information maximization problem, which\ncan be solved by optimizing a variational lower bound. Our framework is\ninstantiated by using prospect theory to model varying player traits, and\nMarkov Decision Processes to parameterize the games. We validate our approach\nempirically, showing that our designed games can successfully distinguish among\nplayers with different traits, outperforming manually-designed ones by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:50:43 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yang", "Fan", ""], ["Leqi", "Liu", ""], ["Wu", "Yifan", ""], ["Lipton", "Zachary C.", ""], ["Ravikumar", "Pradeep", ""], ["Cohen", "William W.", ""], ["Mitchell", "Tom", ""]]}, {"id": "1912.06075", "submitter": "Felix Denzinger", "authors": "Felix Denzinger, Michael Wels, Nishant Ravikumar, Katharina\n  Breininger, Anika Reidelsh\\\"ofer, Joachim Eckert, Michael S\\\"uhling, Axel\n  Schmermund, and Andreas Maier", "title": "Coronary Artery Plaque Characterization from CCTA Scans using Deep\n  Learning and Radiomics", "comments": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention. Springer, Cham, 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32251-9_65", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing coronary artery plaque segments in coronary CT angiography scans is\nan important task to improve patient management and clinical outcomes, as it\ncan help to decide whether invasive investigation and treatment are necessary.\nIn this work, we present three machine learning approaches capable of\nperforming this task. The first approach is based on radiomics, where a plaque\nsegmentation is used to calculate various shape-, intensity- and texture-based\nfeatures under different image transformations. A second approach is based on\ndeep learning and relies on centerline extraction as sole prerequisite. In the\nthird approach, we fuse the deep learning approach with radiomic features. On\nour data the methods reached similar scores as simulated fractional flow\nreserve (FFR) measurements, which - in contrast to our methods - requires an\nexact segmentation of the whole coronary tree and often time-consuming manual\ninteraction. In literature, the performance of simulated FFR reaches an AUC\nbetween 0.79-0.93 predicting an abnormal invasive FFR that demands\nrevascularization. The radiomics approach achieves an AUC of 0.86, the deep\nlearning approach 0.84 and the combined method 0.88 for predicting the\nrevascularization decision directly. While all three proposed methods can be\ndetermined within seconds, the FFR simulation typically takes several minutes.\nProvided representative training data in sufficient quantities, we believe that\nthe presented methods can be used to create systems for fully automatic\nnon-invasive risk assessment for a variety of adverse cardiac events.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:51:09 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 11:34:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Denzinger", "Felix", ""], ["Wels", "Michael", ""], ["Ravikumar", "Nishant", ""], ["Breininger", "Katharina", ""], ["Reidelsh\u00f6fer", "Anika", ""], ["Eckert", "Joachim", ""], ["S\u00fchling", "Michael", ""], ["Schmermund", "Axel", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.06085", "submitter": "Francesco De Lellis", "authors": "Francesco De Lellis, Fabrizia Auletta, Giovanni Russo, Piero De Lellis\n  and Mario di Bernardo", "title": "Control-Tutored Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a control-tutored reinforcement learning (CTRL) algorithm. The\nidea is to enhance tabular learning algorithms so as to improve the exploration\nof the state-space, and substantially reduce learning times by leveraging some\nlimited knowledge of the plant encoded into a tutoring model-based control\nstrategy. We illustrate the benefits of our novel approach and its\neffectiveness by using the problem of controlling one or more agents to herd\nand contain within a goal region a set of target free-roving agents in the\nplane.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:14:15 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["De Lellis", "Francesco", ""], ["Auletta", "Fabrizia", ""], ["Russo", "Giovanni", ""], ["De Lellis", "Piero", ""], ["di Bernardo", "Mario", ""]]}, {"id": "1912.06088", "submitter": "Dibya Ghosh", "authors": "Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin,\n  Benjamin Eysenbach, Sergey Levine", "title": "Learning to Reach Goals via Iterated Supervised Learning", "comments": "First two authors contributed equally. Code available at\n  https://github.com/dibyaghosh/gcsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current reinforcement learning (RL) algorithms can be brittle and difficult\nto use, especially when learning goal-reaching behaviors from sparse rewards.\nAlthough supervised imitation learning provides a simple and stable\nalternative, it requires access to demonstrations from a human supervisor. In\nthis paper, we study RL algorithms that use imitation learning to acquire goal\nreaching policies from scratch, without the need for expert demonstrations or a\nvalue function. In lieu of demonstrations, we leverage the property that any\ntrajectory is a successful demonstration for reaching the final state in that\nsame trajectory. We propose a simple algorithm in which an agent continually\nrelabels and imitates the trajectories it generates to progressively learn\ngoal-reaching behaviors from scratch. Each iteration, the agent collects new\ntrajectories using the latest policy, and maximizes the likelihood of the\nactions along these trajectories under the goal that was actually reached, so\nas to improve the policy. We formally show that this iterated supervised\nlearning procedure optimizes a bound on the RL objective, derive performance\nbounds of the learned policy, and empirically demonstrate improved\ngoal-reaching performance and robustness over current RL algorithms in several\nbenchmark tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:26:47 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 01:42:38 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 17:22:46 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 19:49:10 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ghosh", "Dibya", ""], ["Gupta", "Abhishek", ""], ["Reddy", "Ashwin", ""], ["Fu", "Justin", ""], ["Devin", "Coline", ""], ["Eysenbach", "Benjamin", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.06111", "submitter": "Weihao Kong", "authors": "Weihao Kong and Gregory Valiant and Emma Brunskill", "title": "Sublinear Optimal Policy Value Estimation in Contextual Bandits", "comments": "Extended to the mixture of Gaussians setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the expected reward of the optimal policy\nin the stochastic disjoint linear bandit setting. We prove that for certain\nsettings it is possible to obtain an accurate estimate of the optimal policy\nvalue even with a number of samples that is sublinear in the number that would\nbe required to \\emph{find} a policy that realizes a value close to this optima.\nWe establish nearly matching information theoretic lower bounds, showing that\nour algorithm achieves near optimal estimation error. Finally, we demonstrate\nthe effectiveness of our algorithm on joke recommendation and cancer inhibition\ndosage selection problems using real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:20:11 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:43:40 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kong", "Weihao", ""], ["Valiant", "Gregory", ""], ["Brunskill", "Emma", ""]]}, {"id": "1912.06137", "submitter": "Thierry Denoeux", "authors": "Thierry Denoeux", "title": "Calibrated model-based evidential clustering using bootstrapping", "comments": null, "journal-ref": "Information Sciences, Vol. 528, pages 17-45, 2020", "doi": "10.1016/j.ins.2020.04.014", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidential clustering is an approach to clustering in which\ncluster-membership uncertainty is represented by a collection of\nDempster-Shafer mass functions forming an evidential partition. In this paper,\nwe propose to construct these mass functions by bootstrapping finite mixture\nmodels. In the first step, we compute bootstrap percentile confidence intervals\nfor all pairwise probabilities (the probabilities for any two objects to belong\nto the same class). We then construct an evidential partition such that the\npairwise belief and plausibility degrees approximate the bounds of the\nconfidence intervals. This evidential partition is calibrated, in the sense\nthat the pairwise belief-plausibility intervals contain the true probabilities\n\"most of the time\", i.e., with a probability close to the defined confidence\nlevel. This frequentist property is verified by simulation, and the practical\napplicability of the method is demonstrated using several real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:40:39 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:07:19 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Denoeux", "Thierry", ""]]}, {"id": "1912.06166", "submitter": "Inioluwa Deborah Raji", "authors": "Inioluwa Deborah Raji, Jingying Yang", "title": "ABOUT ML: Annotation and Benchmarking on Understanding and Transparency\n  of Machine Learning Lifecycles", "comments": "Presented at Human-Centric Machine Learning workshop at Neural\n  Information Processing Systems conference 2019; equal contribution from\n  authors, Jingying Yang is the current program lead for the ABOUT ML project\n  at Partnership on AI, more details can be found about the project at\n  https://www.partnershiponai.org/about-ml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the \"Annotation and Benchmarking on Understanding and Transparency\nof Machine Learning Lifecycles\" (ABOUT ML) project as an initiative to\noperationalize ML transparency and work towards a standard ML documentation\npractice. We make the case for the project's relevance and effectiveness in\nconsolidating disparate efforts across a variety of stakeholders, as well as\nbringing in the perspectives of currently missing voices that will be valuable\nin shaping future conversations. We describe the details of the initiative and\nthe gaps we hope this project will help address.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:23:04 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 03:51:50 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 04:10:52 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Raji", "Inioluwa Deborah", ""], ["Yang", "Jingying", ""]]}, {"id": "1912.06174", "submitter": "Marta Skreta", "authors": "Marta Skreta, Aryan Arbabi, Jixuan Wang, Michael Brudno", "title": "Training without training data: Improving the generalizability of\n  automated medical abbreviation disambiguation", "comments": "NeurIPS Machine Learning for Healthcare 2019 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abbreviation disambiguation is important for automated clinical note\nprocessing due to the frequent use of abbreviations in clinical settings.\nCurrent models for automated abbreviation disambiguation are restricted by the\nscarcity and imbalance of labeled training data, decreasing their\ngeneralizability to orthogonal sources. In this work we propose a novel data\naugmentation technique that utilizes information from related medical concepts,\nwhich improves our model's ability to generalize. Furthermore, we show that\nincorporating the global context information within the whole medical note (in\naddition to the traditional local context window), can significantly improve\nthe model's representation for abbreviations. We train our model on a public\ndataset (MIMIC III) and test its performance on datasets from different sources\n(CASI, i2b2). Together, these two techniques boost the accuracy of abbreviation\ndisambiguation by almost 14% on the CASI dataset and 4% on i2b2.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:32:41 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Skreta", "Marta", ""], ["Arbabi", "Aryan", ""], ["Wang", "Jixuan", ""], ["Brudno", "Michael", ""]]}, {"id": "1912.06190", "submitter": "Andrzej Banburski", "authors": "Tomaso Poggio, Gil Kur, Andrzej Banburski", "title": "Double descent in the condition number", "comments": "Removed parts relating to kernel regression to streamline the\n  presentation, fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In solving a system of $n$ linear equations in $d$ variables $Ax=b$, the\ncondition number of the $n,d$ matrix $A$ measures how much errors in the data\n$b$ affect the solution $x$. Estimates of this type are important in many\ninverse problems. An example is machine learning where the key task is to\nestimate an underlying function from a set of measurements at random points in\na high dimensional space and where low sensitivity to error in the data is a\nrequirement for good predictive performance. Here we discuss the simple\nobservation, which is known but surprisingly little quoted (see Theorem 4.2 in\n\\cite{Brgisser:2013:CGN:2526261}): when the columns of $A$ are random vectors,\nthe condition number of $A$ is highest if $d=n$, that is when the inverse of\n$A$ exists. An overdetermined system ($n>d$) as well as an underdetermined\nsystem ($n<d$), for which the pseudoinverse must be used instead of the\ninverse, typically have significantly better, that is lower, condition numbers.\nThus the condition number of $A$ plotted as function of $d$ shows a double\ndescent behavior with a peak at $d=n$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:16:11 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 14:46:25 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 04:29:37 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Poggio", "Tomaso", ""], ["Kur", "Gil", ""], ["Banburski", "Andrzej", ""]]}, {"id": "1912.06200", "submitter": "Christoph Klemenjak", "authors": "Christoph Klemenjak, Anthony Faustine, Stephen Makonin, Wilfried\n  Elmenreich", "title": "On Metrics to Assess the Transferability of Machine Learning Models in\n  Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the performance of load disaggregation algorithms it is common\npractise to train a candidate algorithm on data from one or multiple households\nand subsequently apply cross-validation by evaluating the classification and\nenergy estimation performance on unseen portions of the dataset derived from\nthe same households. With an emerging discussion of transferability in\nNon-Intrusive Load Monitoring (NILM), there is a need for domain-specific\nmetrics to assess the performance of NILM algorithms on new test scenarios\nbeing unseen buildings. In this paper, we discuss several metrics to assess the\ngeneralisation ability of NILM algorithms. These metrics target different\naspects of performance evaluation in NILM and are meant to complement the\ntraditional performance evaluation approach. We demonstrate how our metrics can\nbe utilised to evaluate NILM algorithms by means of two case studies. We\nconduct our studies on several energy consumption datasets and take into\nconsideration five state-of-the-art as well as four baseline NILM solutions.\nFinally, we formulate research challenges for future work.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:43:06 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Klemenjak", "Christoph", ""], ["Faustine", "Anthony", ""], ["Makonin", "Stephen", ""], ["Elmenreich", "Wilfried", ""]]}, {"id": "1912.06248", "submitter": "Sayandev Mukherjee", "authors": "Sayandev Mukherjee", "title": "General Information Bottleneck Objectives and their Applications to\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view the Information Bottleneck Principle (IBP: Tishby et al., 1999;\nSchwartz-Ziv and Tishby, 2017) and Predictive Information Bottleneck Principle\n(PIBP: Still et al., 2007; Alemi, 2019) as special cases of a family of general\ninformation bottleneck objectives (IBOs). Each IBO corresponds to a particular\nconstrained optimization problem where the constraints apply to: (a) the mutual\ninformation between the training data and the learned model parameters or\nextracted representation of the data, and (b) the mutual information between\nthe learned model parameters or extracted representation of the data and the\ntest data (if any). The heuristics behind the IBP and PIBP are shown to yield\ndifferent constraints in the corresponding constrained optimization problem\nformulations. We show how other heuristics lead to a new IBO, different from\nboth the IBP and PIBP, and use the techniques from (Alemi, 2019) to derive and\noptimize a variational upper bound on the new IBO.\n  We then apply the theory of general IBOs to resolve the seeming contradiction\nbetween, on the one hand, the recommendations of IBP and PIBP to maximize the\nmutual information between the model parameters and test data, and on the\nother, recent information-theoretic results (see Xu and Raginsky, 2017)\nsuggesting that this mutual information should be minimized. The key insight is\nthat the heuristics (and thus the constraints in the constrained optimization\nproblems) of IBP and PIBP are not applicable to the scenario analyzed by (Xu\nand Raginsky, 2017) because the latter makes the additional assumption that the\nparameters of the trained model have been selected to minimize the empirical\nloss function. Aided by this insight, we formulate a new IBO that accounts for\nthis property of the parameters of the trained model, and derive and optimize a\nvariational bound on this IBO.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 22:46:58 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 22:07:33 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mukherjee", "Sayandev", ""]]}, {"id": "1912.06269", "submitter": "Alexander Dowling", "authors": "Elvis A. Eugene and Xian Gao and Alexander W. Dowling", "title": "Learning and Optimization with Bayesian Hybrid Models", "comments": "Submitted to 2020 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hybrid models fuse physics-based insights with machine learning\nconstructs to correct for systematic bias. In this paper, we compare Bayesian\nhybrid models against physics-based glass-box and Gaussian process black-box\nsurrogate models. We consider ballistic firing as an illustrative case study\nfor a Bayesian decision-making workflow. First, Bayesian calibration is\nperformed to estimate model parameters. We then use the posterior distribution\nfrom Bayesian analysis to compute optimal firing conditions to hit a target via\na single-stage stochastic program. The case study demonstrates the ability of\nBayesian hybrid models to overcome systematic bias from missing physics with\nless data than the pure machine learning approach. Ultimately, we argue\nBayesian hybrid models are an emerging paradigm for data-informed\ndecision-making under parametric and epistemic uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:59:41 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Eugene", "Elvis A.", ""], ["Gao", "Xian", ""], ["Dowling", "Alexander W.", ""]]}, {"id": "1912.06290", "submitter": "Sean Hendryx", "authors": "Sean M. Hendryx, Andrew B. Leach, Paul D. Hein, Clayton T. Morrison", "title": "Meta-Learning Initializations for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend first-order model agnostic meta-learning algorithms (including\nFOMAML and Reptile) to image segmentation, present a novel neural network\narchitecture built for fast learning which we call EfficientLab, and leverage a\nformal definition of the test error of meta-learning algorithms to decrease\nerror on out of distribution tasks. We show state of the art results on the\nFSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian\noptimization to infer the optimal test-time adaptation routine hyperparameters.\nWe also construct a small benchmark dataset, FP-k, for the empirical study of\nhow meta-learning systems perform in both few- and many-shot settings. On the\nFP-k dataset, we show that meta-learned initializations provide value for\ncanonical few-shot image segmentation but their performance is quickly matched\nby conventional transfer learning with performance being equal beyond 10\nlabeled examples. Our code, meta-learned model, and the FP-k dataset are\navailable at https://github.com/ml4ai/mliis .\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:58:36 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 06:44:55 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 20:52:49 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 23:33:07 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Hendryx", "Sean M.", ""], ["Leach", "Andrew B.", ""], ["Hein", "Paul D.", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1912.06292", "submitter": "Aur\\'elien Bibaut", "authors": "Aur\\'elien F. Bibaut, Ivana Malenica, Nikos Vlassis, Mark J. van der\n  Laan", "title": "More Efficient Off-Policy Evaluation through Regularized Targeted\n  Learning", "comments": "We are uploading the full paper with the appendix as of 12/12/2019,\n  as we noticed that, unlike the main text, the appendix has not been made\n  available on PMLR's website. The version of the appendix in this document is\n  the same that we have been sending by email since June 2019 to readers who\n  solicited it", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:654-663, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation (OPE) in Reinforcement Learning\n(RL), where the aim is to estimate the performance of a new policy given\nhistorical data that may have been generated by a different policy, or\npolicies. In particular, we introduce a novel doubly-robust estimator for the\nOPE problem in RL, based on the Targeted Maximum Likelihood Estimation\nprinciple from the statistical causal inference literature. We also introduce\nseveral variance reduction techniques that lead to impressive performance gains\nin off-policy evaluation. We show empirically that our estimator uniformly wins\nover existing off-policy evaluation methods across multiple RL environments and\nvarious levels of model misspecification. Finally, we further the existing\ntheoretical analysis of estimators for the RL off-policy estimation problem by\nshowing their $O_P(1/\\sqrt{n})$ rate of convergence and characterizing their\nasymptotic distribution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 02:04:22 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Bibaut", "Aur\u00e9lien F.", ""], ["Malenica", "Ivana", ""], ["Vlassis", "Nikos", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1912.06307", "submitter": "Andrii Babii", "authors": "Andrii Babii and Eric Ghysels and Jonas Striaukas", "title": "High-Dimensional Granger Causality Tests with an Application to VIX and\n  News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Granger causality testing for high-dimensional time series using\nregularized regressions. To perform proper inference, we rely on\nheteroskedasticity and autocorrelation consistent (HAC) estimation of the\nasymptotic variance and develop the inferential theory in the high-dimensional\nsetting. To recognize the time series data structures we focus on the\nsparse-group LASSO estimator, which includes the LASSO and the group LASSO as\nspecial cases. We establish the debiased central limit theorem for low\ndimensional groups of regression coefficients and study the HAC estimator of\nthe long-run variance based on the sparse-group LASSO residuals. This leads to\nvalid time series inference for individual regression coefficients as well as\ngroups, including Granger causality tests. The treatment relies on a new\nFuk-Nagaev inequality for a class of $\\tau$-mixing processes with heavier than\nGaussian tails, which is of independent interest. In an empirical application,\nwe study the Granger causal relationship between the VIX and financial news.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:20:51 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 20:11:21 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:51:44 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 15:48:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Babii", "Andrii", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "1912.06342", "submitter": "Runxiong Wu", "authors": "Runxiong Wu and Xin Chen", "title": "MM Algorithms for Distance Covariance based Sufficient Dimension\n  Reduction and Sufficient Variable Selection", "comments": "26 pages, 4 figures", "journal-ref": "Computational Statistics & Data Analysis, 2021", "doi": "10.1016/j.csda.2020.107089", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction (SDR) using distance covariance (DCOV) was\nrecently proposed as an approach to dimension-reduction problems. Compared with\nother SDR methods, it is model-free without estimating link function and does\nnot require any particular distributions on predictors (see Sheng and Yin,\n2013, 2016). However, the DCOV-based SDR method involves optimizing a nonsmooth\nand nonconvex objective function over the Stiefel manifold. To tackle the\nnumerical challenge, we novelly formulate the original objective function\nequivalently into a DC (Difference of Convex functions) program and construct\nan iterative algorithm based on the majorization-minimization (MM) principle.\nAt each step of the MM algorithm, we inexactly solve the quadratic subproblem\non the Stiefel manifold by taking one iteration of Riemannian Newton's method.\nThe algorithm can also be readily extended to sufficient variable selection\n(SVS) using distance covariance. We establish the convergence property of the\nproposed algorithm under some regularity conditions. Simulation studies show\nour algorithm drastically improves the computation efficiency and is robust\nacross various settings compared with the existing method. Supplemental\nmaterials for this article are available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:08:15 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 08:50:08 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Runxiong", ""], ["Chen", "Xin", ""]]}, {"id": "1912.06366", "submitter": "Shi Dong", "authors": "Shi Dong, Benjamin Van Roy, Zhengyuan Zhou", "title": "Provably Efficient Reinforcement Learning with Aggregated States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish that an optimistic variant of Q-learning applied to a\nfixed-horizon episodic Markov decision process with an aggregated state\nrepresentation incurs regret $\\tilde{\\mathcal{O}}(\\sqrt{H^5 M K} + \\epsilon\nHK)$, where $H$ is the horizon, $M$ is the number of aggregate states, $K$ is\nthe number of episodes, and $\\epsilon$ is the largest difference between any\npair of optimal state-action values associated with a common aggregate state.\nNotably, this regret bound does not depend on the number of states or actions\nand indicates that asymptotic per-period regret is no greater than $\\epsilon$,\nindependent of horizon. To our knowledge, this is the first such result that\napplies to reinforcement learning with nontrivial value function approximation\nwithout any restrictions on transition probabilities.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 09:10:18 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 06:05:39 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Dong", "Shi", ""], ["Van Roy", "Benjamin", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "1912.06384", "submitter": "Francesco Concas", "authors": "Francesco Concas, Julien Mineraud, Eemil Lagerspetz, Samu Varjonen,\n  Xiaoli Liu, Kai Puolam\\\"aki, Petteri Nurmi, Sasu Tarkoma", "title": "Low-Cost Outdoor Air Quality Monitoring and Sensor Calibration: A Survey\n  and Critical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of air pollution and the problems associated with it are\nfueling deployments of air quality monitoring stations worldwide. The most\ncommon approach for air quality monitoring is to rely on environmental\nmonitoring stations, which unfortunately are very expensive both to acquire and\nto maintain. Hence environmental monitoring stations are typically sparsely\ndeployed, resulting in limited spatial resolution for measurements. Recently,\nlow-cost air quality sensors have emerged as an alternative that can improve\nthe granularity of monitoring. The use of low-cost air quality sensors,\nhowever, presents several challenges: they suffer from cross-sensitivities\nbetween different ambient pollutants; they can be affected by external factors,\nsuch as traffic, weather changes, and human behavior; and their accuracy\ndegrades over time. Periodic re-calibration can improve the accuracy of\nlow-cost sensors, particularly with machine-learning-based calibration, which\nhas shown great promise due to its capability to calibrate sensors in-field. In\nthis article, we survey the rapidly growing research landscape of low-cost\nsensor technologies for air quality monitoring and their calibration using\nmachine learning techniques. We also identify open research challenges and\npresent directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:07:10 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 10:00:14 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 08:17:48 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 17:39:21 GMT"}, {"version": "v5", "created": "Fri, 8 Jan 2021 15:12:30 GMT"}, {"version": "v6", "created": "Mon, 25 Jan 2021 13:41:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Concas", "Francesco", ""], ["Mineraud", "Julien", ""], ["Lagerspetz", "Eemil", ""], ["Varjonen", "Samu", ""], ["Liu", "Xiaoli", ""], ["Puolam\u00e4ki", "Kai", ""], ["Nurmi", "Petteri", ""], ["Tarkoma", "Sasu", ""]]}, {"id": "1912.06385", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Feroz Karim, Junaid Javed Qureshi, Adnan Omer Abuassba,\n  Mohammad Farhad Bulbul", "title": "Seizure Prediction Using Bidirectional LSTM", "comments": "CyberDI 2019, Cyberspace Data and Intelligence, and Cyber-Living,\n  Syndrome, and Health pp 349-356", "journal-ref": null, "doi": "10.1007/978-981-15-1922-2_25", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximately, 50 million people in the world are affected by epilepsy. For\npatients, the anti-epileptic drugs are not always useful and these drugs may\nhave undesired side effects on a patient's health. If the seizure is predicted\nthe patients will have enough time to take preventive measures. The purpose of\nthis work is to investigate the application of bidirectional LSTM for seizure\nprediction. In this paper, we trained EEG data from canines on a double\nBidirectional LSTM layer followed by a fully connected layer. The data was\nprovided in the form of a Kaggle competition by American Epilepsy Society. The\nmain task was to classify the interictal and preictal EEG clips. Using this\nmodel, we obtained an AUC of 0.84 on the test dataset. Which shows that our\nclassifier's performance is above chance level on unseen data. The comparison\nwith the previous work shows that the use of bidirectional LSTM networks can\nachieve significantly better results than SVM and GRU networks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:08:45 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ali", "Hazrat", ""], ["Karim", "Feroz", ""], ["Qureshi", "Junaid Javed", ""], ["Abuassba", "Adnan Omer", ""], ["Bulbul", "Mohammad Farhad", ""]]}, {"id": "1912.06407", "submitter": "Pedro Delicado", "authors": "Pedro Delicado and Daniel Pe\\~na", "title": "Understanding complex predictive models with Ghost Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for assigning a relevance measure to each explanatory\nvariable in a complex predictive model. We assume that we have a training set\nto fit the model and a test set to check the out of sample performance. First,\nthe individual relevance of each variable is computed by comparing the\npredictions in the test set, given by the model that includes all the variables\nwith those of another model in which the variable of interest is substituted by\nits ghost variable, defined as the prediction of this variable by using the\nrest of explanatory variables. Second, we check the joint effects among the\nvariables by using the eigenvalues of a relevance matrix that is the covariance\nmatrix of the vectors of individual effects. It is shown that in simple models,\nas linear or additive models, the proposed measures are related to standard\nmeasures of significance of the variables and in neural networks models (and in\nother algorithmic prediction models) the procedure provides information about\nthe joint and individual effects of the variables that is not usually available\nby other methods. The procedure is illustrated with simulated examples and the\nanalysis of a large real data set.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:06:12 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 17:25:06 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Delicado", "Pedro", ""], ["Pe\u00f1a", "Daniel", ""]]}, {"id": "1912.06409", "submitter": "Amir Nazemi", "authors": "Amir Nazemi, Paul Fieguth", "title": "Potential adversarial samples for white-box attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks can be highly vulnerable to small\nperturbations of their inputs, potentially a major issue or limitation on\nsystem robustness when using deep networks as classifiers. In this paper we\npropose a low-cost method to explore marginal sample data near trained\nclassifier decision boundaries, thus identifying potential adversarial samples.\nBy finding such adversarial samples it is possible to reduce the search space\nof adversarial attack algorithms while keeping a reasonable successful\nperturbation rate. In our developed strategy, the potential adversarial samples\nrepresent only 61% of the test data, but in fact cover more than 82% of the\nadversarial samples produced by iFGSM and 92% of the adversarial samples\nsuccessfully perturbed by DeepFool on CIFAR10.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:09:35 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Nazemi", "Amir", ""], ["Fieguth", "Paul", ""]]}, {"id": "1912.06417", "submitter": "Felix Denzinger", "authors": "Felix Denzinger, Michael Wels, Katharina Breininger, Anika\n  Reidelsh\\\"ofer, Joachim Eckert, Michael S\\\"uhling, Axel Schmermund, Andreas\n  Maier", "title": "Deep Learning Algorithms for Coronary Artery Plaque Characterisation\n  from CCTA Scans", "comments": "Accepted at BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing coronary artery plaque segments with respect to their functional\nsignificance and therefore their influence to patient management in a\nnon-invasive setup is an important subject of current research. In this work we\ncompare and improve three deep learning algorithms for this task: A 3D\nrecurrent convolutional neural network (RCNN), a 2D multi-view ensemble\napproach based on texture analysis, and a newly proposed 2.5D approach. Current\nstate of the art methods utilising fluid dynamics based fractional flow reserve\n(FFR) simulation reach an AUC of up to 0.93 for the task of predicting an\nabnormal invasive FFR value. For the comparable task of predicting\nrevascularisation decision, we are able to improve the performance in terms of\nAUC of both existing approaches with the proposed modifications, specifically\nfrom 0.80 to 0.90 for the 3D-RCNN, and from 0.85 to 0.90 for the multi-view\ntexture-based ensemble. The newly proposed 2.5D approach achieves comparable\nresults with an AUC of 0.90.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:27:17 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Denzinger", "Felix", ""], ["Wels", "Michael", ""], ["Breininger", "Katharina", ""], ["Reidelsh\u00f6fer", "Anika", ""], ["Eckert", "Joachim", ""], ["S\u00fchling", "Michael", ""], ["Schmermund", "Axel", ""], ["Maier", "Andreas", ""]]}, {"id": "1912.06444", "submitter": "Zhao Zhang", "authors": "Yan Zhang, Zhao Zhang, Zheng Zhang, Mingbo Zhao, Li Zhang, Zhengjun\n  Zha, Meng Wang", "title": "Deep Self-representative Concept Factorization Network for\n  Representation Learning", "comments": "Accepted by SDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the unsupervised deep representation learning\nissue and technically propose a novel framework called Deep Self-representative\nConcept Factorization Network (DSCF-Net), for clustering deep features. To\nimprove the representation and clustering abilities, DSCF-Net explicitly\nconsiders discovering hidden deep semantic features, enhancing the robustness\nproper-ties of the deep factorization to noise and preserving the local\nman-ifold structures of deep features. Specifically, DSCF-Net seamlessly\nintegrates the robust deep concept factorization, deep self-expressive\nrepresentation and adaptive locality preserving feature learning into a unified\nframework. To discover hidden deep repre-sentations, DSCF-Net designs a\nhierarchical factorization architec-ture using multiple layers of linear\ntransformations, where the hierarchical representation is performed by\nformulating the prob-lem as optimizing the basis concepts in each layer to\nimprove the representation indirectly. DSCF-Net also improves the robustness by\nsubspace recovery for sparse error correction firstly and then performs the\ndeep factorization in the recovered visual subspace. To obtain\nlocality-preserving representations, we also present an adaptive deep\nself-representative weighting strategy by using the coefficient matrix as the\nadaptive reconstruction weights to keep the locality of representations.\nExtensive comparison results with several other related models show that\nDSCF-Net delivers state-of-the-art performance on several public databases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:50:01 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 06:58:03 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 09:46:01 GMT"}, {"version": "v4", "created": "Sun, 29 Dec 2019 14:16:12 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhang", "Yan", ""], ["Zhang", "Zhao", ""], ["Zhang", "Zheng", ""], ["Zhao", "Mingbo", ""], ["Zhang", "Li", ""], ["Zha", "Zhengjun", ""], ["Wang", "Meng", ""]]}, {"id": "1912.06472", "submitter": "Thomas Carroll", "authors": "Thomas L. Carroll", "title": "Dimension of Reservoir Computers", "comments": "submitted to Chaos", "journal-ref": "Chaos vol. 30 issue 1 013102 2020", "doi": "10.1063/1.5128898", "report-no": null, "categories": "nlin.AO cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A reservoir computer is a complex dynamical system, often created by coupling\nnonlinear nodes in a network. The nodes are all driven by a common driving\nsignal. In this work, three dimension estimation methods, false nearest\nneighbor, covariance and Kaplan-Yorke dimensions, are used to estimate the\ndimension of the reservoir dynamical system. It is shown that the signals in\nthe reservoir system exist on a relatively low dimensional surface. Changing\nthe spectral radius of the reservoir network can increase the fractal dimension\nof the reservoir signals, leading to an increase in testing error.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:14:08 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Carroll", "Thomas L.", ""]]}, {"id": "1912.06500", "submitter": "Amitay Eldar", "authors": "Amitay Eldar, Boris Landa, Yoel Shkolnisky", "title": "KLT Picker: Particle Picking Using Data-Driven Optimal Templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle picking is currently a critical step in the cryo-EM single particle\nreconstruction pipeline. Despite extensive work on this problem, for many data\nsets it is still challenging, especially for low SNR micrographs. We present\nthe KLT (Karhunen Loeve Transform) picker, which is fully automatic and\nrequires as an input only the approximated particle size. In particular, it\ndoes not require any manual picking. Our method is designed especially to\nhandle low SNR micrographs. It is based on learning a set of optimal templates\nthrough the use of multi-variate statistical analysis via the Karhunen Loeve\nTransform. We evaluate the KLT picker on publicly available data sets and\npresent high-quality results with minimal manual effort.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 10:41:46 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Eldar", "Amitay", ""], ["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1912.06508", "submitter": "Ching-Pei Lee", "authors": "Ching-pei Lee, Cong Han Lim, Stephen J. Wright", "title": "A Distributed Quasi-Newton Algorithm for Primal and Dual Regularized\n  Empirical Risk Minimization", "comments": "arXiv admin note: text overlap with arXiv:1803.01370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a communication- and computation-efficient distributed\noptimization algorithm using second-order information for solving empirical\nrisk minimization (ERM) problems with a nonsmooth regularization term. Our\nalgorithm is applicable to both the primal and the dual ERM problem. Current\nsecond-order and quasi-Newton methods for this problem either do not work well\nin the distributed setting or work only for specific regularizers. Our\nalgorithm uses successive quadratic approximations of the smooth part, and we\ndescribe how to maintain an approximation of the (generalized) Hessian and\nsolve subproblems efficiently in a distributed manner. When applied to the\ndistributed dual ERM problem, unlike state of the art that takes only the\nblock-diagonal part of the Hessian, our approach is able to utilize global\ncurvature information and is thus magnitudes faster. The proposed method enjoys\nglobal linear convergence for a broad range of non-strongly convex problems\nthat includes the most commonly used ERMs, thus requiring lower communication\ncomplexity. It also converges on non-convex problems, so has the potential to\nbe used on applications such as deep learning. Computational results\ndemonstrate that our method significantly improves on communication cost and\nrunning time over the current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:25:37 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Lee", "Ching-pei", ""], ["Lim", "Cong Han", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1912.06552", "submitter": "Daniel Heestermans Svendsen", "authors": "Daniel Heestermans Svendsen, Luca Martino, Gustau Camps-Valls", "title": "Active emulation of computer codes with Gaussian processes --\n  Application to remote sensing", "comments": "Keywords: Active learning; Gaussian process; Emulation; Design of\n  experiments; Computer code; Remote sensing; Radiative transfer model", "journal-ref": "Pattern Recognition (2019): 107103", "doi": "10.1016/j.patcog.2019.107103", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fields of science and engineering rely on running simulations with\ncomplex and computationally expensive models to understand the involved\nprocesses in the system of interest. Nevertheless, the high cost involved\nhamper reliable and exhaustive simulations. Very often such codes incorporate\nheuristics that ironically make them less tractable and transparent. This paper\nintroduces an active learning methodology for adaptively constructing surrogate\nmodels, i.e. emulators, of such costly computer codes in a multi-output\nsetting. The proposed technique is sequential and adaptive, and is based on the\noptimization of a suitable acquisition function. It aims to achieve accurate\napproximations, model tractability, as well as compact and expressive simulated\ndatasets. In order to achieve this, the proposed Active Multi-Output Gaussian\nProcess Emulator (AMOGAPE) combines the predictive capacity of Gaussian\nProcesses (GPs) with the design of an acquisition function that favors sampling\nin low density and fluctuating regions of the approximation functions.\nComparing different acquisition functions, we illustrate the promising\nperformance of the method for the construction of emulators with toy examples,\nas well as for a widely used remote sensing transfer code.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 15:16:13 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Svendsen", "Daniel Heestermans", ""], ["Martino", "Luca", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1912.06570", "submitter": "Eli Chien", "authors": "Eli Chien, Antonia Maria Tulino, Jaime Llorca", "title": "Active learning in the geometric block model", "comments": "The conference version will appear in AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric block model is a recently proposed generative model for random\ngraphs that is able to capture the inherent geometric properties of many\ncommunity detection problems, providing more accurate characterizations of\npractical community structures compared with the popular stochastic block\nmodel. Galhotra et al. recently proposed a motif-counting algorithm for\nunsupervised community detection in the geometric block model that is proved to\nbe near-optimal. They also characterized the regimes of the model parameters\nfor which the proposed algorithm can achieve exact recovery. In this work, we\ninitiate the study of active learning in the geometric block model. That is, we\nare interested in the problem of exactly recovering the community structure of\nrandom graphs following the geometric block model under arbitrary model\nparameters, by possibly querying the labels of a limited number of chosen\nnodes. We propose two active learning algorithms that combine the idea of\nmotif-counting with two different label query policies. Our main contribution\nis to show that sampling the labels of a vanishingly small fraction of nodes\n(sub-linear in the total number of nodes) is sufficient to achieve exact\nrecovery in the regimes under which the state-of-the-art unsupervised method\nfails. We validate the superior performance of our algorithms via numerical\nsimulations on both real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:41:50 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Chien", "Eli", ""], ["Tulino", "Antonia Maria", ""], ["Llorca", "Jaime", ""]]}, {"id": "1912.06667", "submitter": "Naim Rashid", "authors": "Naim U. Rashid, Daniel J. Luckett, Jingxiang Chen, Michael T. Lawson,\n  Longshaokan Wang, Yunshu Zhang, Eric B. Laber, Yufeng Liu, Jen Jen Yeh,\n  Donglin Zeng, Michael R. Kosorok", "title": "High dimensional precision medicine from patient-derived xenografts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of human cancer often results in significant heterogeneity in\nresponse to treatment. Precision medicine offers potential to improve patient\noutcomes by leveraging this heterogeneity. Individualized treatment rules\n(ITRs) formalize precision medicine as maps from the patient covariate space\ninto the space of allowable treatments. The optimal ITR is that which maximizes\nthe mean of a clinical outcome in a population of interest. Patient-derived\nxenograft (PDX) studies permit the evaluation of multiple treatments within a\nsingle tumor and thus are ideally suited for estimating optimal ITRs. PDX data\nare characterized by correlated outcomes, a high-dimensional feature space, and\na large number of treatments. Existing methods for estimating optimal ITRs do\nnot take advantage of the unique structure of PDX data or handle the associated\nchallenges well. In this paper, we explore machine learning methods for\nestimating optimal ITRs from PDX data. We analyze data from a large PDX study\nto identify biomarkers that are informative for developing personalized\ntreatment recommendations in multiple cancers. We estimate optimal ITRs using\nregression-based approaches such as Q-learning and direct search methods such\nas outcome weighted learning. Finally, we implement a superlearner approach to\ncombine a set of estimated ITRs and show that the resulting ITR performs better\nthan any of the input ITRs, mitigating uncertainty regarding user choice of any\nparticular ITR estimation methodology. Our results indicate that PDX data are a\nvaluable resource for developing individualized treatment strategies in\noncology.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:17:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Rashid", "Naim U.", ""], ["Luckett", "Daniel J.", ""], ["Chen", "Jingxiang", ""], ["Lawson", "Michael T.", ""], ["Wang", "Longshaokan", ""], ["Zhang", "Yunshu", ""], ["Laber", "Eric B.", ""], ["Liu", "Yufeng", ""], ["Yeh", "Jen Jen", ""], ["Zeng", "Donglin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.06675", "submitter": "Gilmer Valdes", "authors": "Gilmer Valdes, Yannet Interian, Efstathios D. Gennatas Mark J. Van der\n  Laan", "title": "Conditional Super Learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the Conditional Super Learner (CSL), an algorithm\nwhich selects the best model candidate from a library conditional on the\ncovariates. The CSL expands the idea of using cross-validation to select the\nbest model and merges it with meta learning. Here we propose a specific\nalgorithm that finds a local minimum to the problem posed, proof that it\nconverges at a rate faster than $O_p(n^{-1/4})$ and offers extensive empirical\nevidence that it is an excellent candidate to substitute stacking or for the\nanalysis of Hierarchical problems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:44:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Valdes", "Gilmer", ""], ["Interian", "Yannet", ""], ["Van der Laan", "Efstathios D. Gennatas Mark J.", ""]]}, {"id": "1912.06680", "submitter": "Filip Wolski", "authors": "OpenAI: Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\n  Przemys{\\l}aw D\\k{e}biak, Christy Dennison, David Farhi, Quirin Fischer,\n  Shariq Hashme, Chris Hesse, Rafal J\\'ozefowicz, Scott Gray, Catherine Olsson,\n  Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim\n  Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever,\n  Jie Tang, Filip Wolski, Susan Zhang", "title": "Dota 2 with Large Scale Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On April 13th, 2019, OpenAI Five became the first AI system to defeat the\nworld champions at an esports game. The game of Dota 2 presents novel\nchallenges for AI systems such as long time horizons, imperfect information,\nand complex, continuous state-action spaces, all challenges which will become\nincreasingly central to more capable AI systems. OpenAI Five leveraged existing\nreinforcement learning techniques, scaled to learn from batches of\napproximately 2 million frames every 2 seconds. We developed a distributed\ntraining system and tools for continual training which allowed us to train\nOpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG),\nOpenAI Five demonstrates that self-play reinforcement learning can achieve\nsuperhuman performance on a difficult task.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:56:40 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["OpenAI", "", ""], [":", "", ""], ["Berner", "Christopher", ""], ["Brockman", "Greg", ""], ["Chan", "Brooke", ""], ["Cheung", "Vicki", ""], ["D\u0119biak", "Przemys\u0142aw", ""], ["Dennison", "Christy", ""], ["Farhi", "David", ""], ["Fischer", "Quirin", ""], ["Hashme", "Shariq", ""], ["Hesse", "Chris", ""], ["J\u00f3zefowicz", "Rafal", ""], ["Gray", "Scott", ""], ["Olsson", "Catherine", ""], ["Pachocki", "Jakub", ""], ["Petrov", "Michael", ""], ["Pinto", "Henrique P. d. O.", ""], ["Raiman", "Jonathan", ""], ["Salimans", "Tim", ""], ["Schlatter", "Jeremy", ""], ["Schneider", "Jonas", ""], ["Sidor", "Szymon", ""], ["Sutskever", "Ilya", ""], ["Tang", "Jie", ""], ["Wolski", "Filip", ""], ["Zhang", "Susan", ""]]}, {"id": "1912.06688", "submitter": "Joerg Zimmermann", "authors": "Kristina Enes, Hassan Errami, Moritz Wolter, Tim Krake, Bernhard\n  Eberhardt, Andreas Weber, J\\\"org Zimmermann", "title": "Unsupervised and Generic Short-Term Anticipation of Human Body Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various neural network based methods are capable of anticipating human body\nmotions from data for a short period of time. What these methods lack are the\ninterpretability and explainability of the network and its results. We propose\nto use Dynamic Mode Decomposition with delays to represent and anticipate human\nbody motions. Exploring the influence of the number of delays on the\nreconstruction and prediction of various motion classes, we show that the\nanticipation errors in our results are comparable or even better for very short\nanticipation times ($<0.4$ sec) to a recurrent neural network based method. We\nperceive our method as a first step towards the interpretability of the results\nby representing human body motions as linear combinations of ``factors''. In\naddition, compared to the neural network based methods large training times are\nnot needed. Actually, our methods do not even regress to any other motions than\nthe one to be anticipated and hence is of a generic nature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:13:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Enes", "Kristina", ""], ["Errami", "Hassan", ""], ["Wolter", "Moritz", ""], ["Krake", "Tim", ""], ["Eberhardt", "Bernhard", ""], ["Weber", "Andreas", ""], ["Zimmermann", "J\u00f6rg", ""]]}, {"id": "1912.06689", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "Data-driven confidence bands for distributed nonparametric regression", "comments": "COLT2020 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process Regression and Kernel Ridge Regression are popular\nnonparametric regression approaches. Unfortunately, they suffer from high\ncomputational complexity rendering them inapplicable to the modern massive\ndatasets. To that end a number of approximations have been suggested, some of\nthem allowing for a distributed implementation. One of them is the divide and\nconquer approach, splitting the data into a number of partitions, obtaining the\nlocal estimates and finally averaging them. In this paper we suggest a novel\ncomputationally efficient fully data-driven algorithm, quantifying uncertainty\nof this method, yielding frequentist $L_2$-confidence bands. We rigorously\ndemonstrate validity of the algorithm. Another contribution of the paper is a\nminimax-optimal high-probability bound for the averaged estimator,\ncomplementing and generalizing the known risk bounds.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:13:55 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:17:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "1912.06708", "submitter": "Mogens Graf Plessen", "authors": "Mogens Graf Plessen", "title": "A posteriori Trading-inspired Model-free Time Series Segmentation", "comments": "9 pages, double column, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of multivariate time series segmentation this paper\nproposes a method inspired by a posteriori optimal trading. After a\nnormalization step time series are treated channel-wise as surrogate stock\nprices that can be traded optimally a posteriori in a virtual portfolio holding\neither stock or cash. Linear transaction costs are interpreted as\nhyperparameters for noise filtering. Resulting trading signals as well as\nresulting trading signals obtained on the reversed time series are used for\nunsupervised labeling, before a consensus over channels is reached that\ndetermines segmentation time instants. The method is model-free such that no\nmodel prescriptions for segments are made. Benefits of proposed approach\ninclude simplicity, computational efficiency and adaptability to a wide range\nof different shapes of time series. Performance is demonstrated on synthetic\nand real-world data, including a large-scale dataset comprising a multivariate\ntime series of dimension 1000 and length 2709. Proposed method is compared to a\npopular model-based bottom-up approach fitting piecewise affine models and to a\nrecent model-based top-down approach fitting Gaussian models, and found to be\nconsistently faster while producing more intuitive results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 06:14:03 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Plessen", "Mogens Graf", ""]]}, {"id": "1912.06723", "submitter": "Daniel Karl I. Weidele", "authors": "Daniel Karl I. Weidele, Justin D. Weisz, Eno Oduor, Michael Muller,\n  Josh Andres, Alexander Gray, Dakuo Wang", "title": "AutoAIViz: Opening the Blackbox of Automated Artificial Intelligence\n  with Conditional Parallel Coordinates", "comments": "5 pages, 1 figure, IUI2020", "journal-ref": null, "doi": "10.1145/3377325.3377538", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) can now automate the algorithm selection,\nfeature engineering, and hyperparameter tuning steps in a machine learning\nworkflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve\ndata scientists from the tedious manual work. However, today's AutoAI systems\noften present only limited to no information about the process of how they\nselect and generate model results. Thus, users often do not understand the\nprocess, neither do they trust the outputs. In this short paper, we provide a\nfirst user evaluation by 10 data scientists of an experimental system,\nAutoAIViz, that aims to visualize AutoAI's model generation process. We find\nthat the proposed system helps users to complete the data science tasks, and\nincreases their understanding, toward the goal of increasing trust in the\nAutoAI system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 21:53:01 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 16:32:18 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 15:51:23 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Weidele", "Daniel Karl I.", ""], ["Weisz", "Justin D.", ""], ["Oduor", "Eno", ""], ["Muller", "Michael", ""], ["Andres", "Josh", ""], ["Gray", "Alexander", ""], ["Wang", "Dakuo", ""]]}, {"id": "1912.06732", "submitter": "Tim De Ryck", "authors": "Tim De Ryck, Siddhartha Mishra, Deep Ray", "title": "On the approximation of rough functions with deep neural networks", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": "ETH - SAM report 2020-07", "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and the ENO procedure are both efficient frameworks for\napproximating rough functions. We prove that at any order, the ENO\ninterpolation procedure can be cast as a deep ReLU neural network. This\nsurprising fact enables the transfer of several desirable properties of the ENO\nprocedure to deep neural networks, including its high-order accuracy at\napproximating Lipschitz functions. Numerical tests for the resulting neural\nnetworks show excellent performance for approximating solutions of nonlinear\nconservation laws and at data compression.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:48:36 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 06:27:30 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["De Ryck", "Tim", ""], ["Mishra", "Siddhartha", ""], ["Ray", "Deep", ""]]}, {"id": "1912.06733", "submitter": "Daniel Peterson", "authors": "Daniel Peterson, Pallika Kanani, Virendra J. Marathe", "title": "Private Federated Learning with Domain Adaptation", "comments": "Presented at the Workshop on Federated Learning for Data Privacy and\n  Confidentiality (in Conjunction with NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a distributed machine learning (ML) paradigm that\nenables multiple parties to jointly re-train a shared model without sharing\ntheir data with any other parties, offering advantages in both scale and\nprivacy. We propose a framework to augment this collaborative model-building\nwith per-user domain adaptation. We show that this technique improves model\naccuracy for all users, using both real and synthetic data, and that this\nimprovement is much more pronounced when differential privacy bounds are\nimposed on the FL model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:48:43 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Peterson", "Daniel", ""], ["Kanani", "Pallika", ""], ["Marathe", "Virendra J.", ""]]}, {"id": "1912.06744", "submitter": "Leonardo Banchi", "authors": "Laura Gentini, Alessandro Cuccoli, Stefano Pirandola, Paola Verrucchi,\n  Leonardo Banchi", "title": "Noise-Resilient Variational Hybrid Quantum-Classical Optimization", "comments": null, "journal-ref": "Phys. Rev. A 102, 052414 (2020)", "doi": "10.1103/PhysRevA.102.052414", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational hybrid quantum-classical optimization represents one of the most\npromising avenue to show the advantage of nowadays noisy intermediate-scale\nquantum computers in solving hard problems, such as finding the minimum-energy\nstate of a Hamiltonian or solving some machine-learning tasks. In these devices\nnoise is unavoidable and impossible to error-correct, yet its role in the\noptimization process is not well understood, especially from the theoretical\nviewpoint. Here we consider a minimization problem with respect to a\nvariational state, iteratively obtained via a parametric quantum circuit,\ntaking into account both the role of noise and the stochastic nature of quantum\nmeasurement outcomes. We show that the accuracy of the result obtained for a\nfixed number of iterations is bounded by a quantity related to the Quantum\nFisher Information of the variational state. Using this bound, we study the\nconvergence property of the quantum approximate optimization algorithm under\nrealistic noise models, showing the robustness of the algorithm against\ndifferent noise strengths.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 23:31:48 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 07:51:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Gentini", "Laura", ""], ["Cuccoli", "Alessandro", ""], ["Pirandola", "Stefano", ""], ["Verrucchi", "Paola", ""], ["Banchi", "Leonardo", ""]]}, {"id": "1912.06752", "submitter": "Jeremy Morton", "authors": "Jeremy Morton and Freddie D. Witherden and Mykel J. Kochenderfer", "title": "Parameter-Conditioned Sequential Generative Modeling of Fluid Flows", "comments": "29 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost associated with simulating fluid flows can make it\ninfeasible to run many simulations across multiple flow conditions. Building\nupon concepts from generative modeling, we introduce a new method for learning\nneural network models capable of performing efficient parameterized simulations\nof fluid flows. Evaluated on their ability to simulate both two-dimensional and\nthree-dimensional fluid flows, trained models are shown to capture local and\nglobal properties of the flow fields at a wide array of flow conditions.\nFurthermore, flow simulations generated by the trained models are shown to be\norders of magnitude faster than the corresponding computational fluid dynamics\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 00:16:53 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Morton", "Jeremy", ""], ["Witherden", "Freddie D.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1912.06760", "submitter": "John Moberg", "authors": "John Moberg, Lennart Svensson, Juliano Pinto, Henk Wymeersch", "title": "Bayesian Linear Regression on Deep Representations", "comments": "4th workshop on Bayesian Deep Learning (NeurIPS 2019), Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple approach to obtaining uncertainty-aware neural networks for\nregression is to do Bayesian linear regression (BLR) on the representation from\nthe last hidden layer. Recent work [Riquelme et al., 2018, Azizzadenesheli et\nal., 2018] indicates that the method is promising, though it has been limited\nto homoscedastic noise. In this paper, we propose a novel variation that\nenables the method to flexibly model heteroscedastic noise. The method is\nbenchmarked against two prominent alternative methods on a set of standard\ndatasets, and finally evaluated as an uncertainty-aware model in model-based\nreinforcement learning. Our experiments indicate that the method is competitive\nwith standard ensembling, and ensembles of BLR outperforms the methods we\ncompared to.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 01:03:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Moberg", "John", ""], ["Svensson", "Lennart", ""], ["Pinto", "Juliano", ""], ["Wymeersch", "Henk", ""]]}, {"id": "1912.06761", "submitter": "Yannet Interian", "authors": "Miguel Romero and Yannet Interian and Timothy Solberg and Gilmer\n  Valdes", "title": "Targeted transfer learning to improve performance in small medical\n  physics datasets", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14507", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of Machine Learning has produced significant advances in many\nfields. For image-based tasks, however, the use of deep learning remains\nchallenging in small datasets. In this article, we review, evaluate and compare\nthe current state-of-the-art techniques in training neural networks to\nelucidate which techniques work best for small datasets. We further propose a\npath forward for the improvement of model accuracy in medical imaging\napplications. We observed best results from one cycle training, discriminative\nlearning rates with gradual freezing and parameter modification after transfer\nlearning. We also established that when datasets are small, transfer learning\nplays an important role beyond parameter initialization by reusing previously\nlearned features. Surprisingly we observed that there is little advantage in\nusing pre-trained networks in images from another part of the body compared to\nImagenet. On the contrary, if images from the same part of the body are\navailable then transfer learning can produce a significant improvement in\nperformance with as little as 50 images in the training data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 01:05:10 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 19:54:09 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 04:36:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Romero", "Miguel", ""], ["Interian", "Yannet", ""], ["Solberg", "Timothy", ""], ["Valdes", "Gilmer", ""]]}, {"id": "1912.06767", "submitter": "Likang Wu", "authors": "Likang Wu, Zhi Li, Hongke Zhao, Zhen Pan, Qi Liu, Enhong Chen", "title": "Estimating Early Fundraising Performance of Innovations via Graph-based\n  Market Environment Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well begun is half done. In the crowdfunding market, the early fundraising\nperformance of the project is a concerned issue for both creators and\nplatforms. However, estimating the early fundraising performance before the\nproject published is very challenging and still under-explored. To that end, in\nthis paper, we present a focused study on this important problem in a market\nmodeling view. Specifically, we propose a Graph-based Market Environment model\n(GME) for estimating the early fundraising performance of the target project by\nexploiting the market environment. In addition, we discriminatively model the\nmarket competition and market evolution by designing two graph-based neural\nnetwork architectures and incorporating them into the joint optimization stage.\nFinally, we conduct extensive experiments on the real-world crowdfunding data\ncollected from Indiegogo.com. The experimental results clearly demonstrate the\neffectiveness of our proposed model for modeling and estimating the early\nfundraising performance of the target project.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 02:11:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wu", "Likang", ""], ["Li", "Zhi", ""], ["Zhao", "Hongke", ""], ["Pan", "Zhen", ""], ["Liu", "Qi", ""], ["Chen", "Enhong", ""]]}, {"id": "1912.06779", "submitter": "Hanson Wang", "authors": "Hanson Wang, Zehui Wang, Yuanyuan Ma", "title": "Predictive Precompute with Recurrent Neural Networks", "comments": null, "journal-ref": "Proceedings of the 3rd MLSys Conference, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both mobile and web applications, speeding up user interface response\ntimes can often lead to significant improvements in user engagement. A common\ntechnique to improve responsiveness is to precompute data ahead of time for\nspecific activities. However, simply precomputing data for all user and\nactivity combinations is prohibitive at scale due to both network constraints\nand server-side computational costs. It is therefore important to accurately\npredict per-user application usage in order to minimize wasted precomputation\n(\"predictive precompute\"). In this paper, we describe the novel application of\nrecurrent neural networks (RNNs) for predictive precompute. We compare their\nperformance with traditional machine learning models, and share findings from\ntheir large-scale production use at Facebook. We demonstrate that RNN models\nimprove prediction accuracy, eliminate most feature engineering steps, and\nreduce the computational cost of serving predictions by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 03:40:38 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 04:40:47 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wang", "Hanson", ""], ["Wang", "Zehui", ""], ["Ma", "Yuanyuan", ""]]}, {"id": "1912.06803", "submitter": "Puja Sahu", "authors": "Puja Sahu and Nandyala Hemachandra", "title": "Optimal PAC-Bayesian Posteriors for Stochastic Classifiers and their use\n  for Choice of SVM Regularization Parameter", "comments": "56 pages, 6 Figures, ACML 2019 conference paper with supplementary\n  material", "journal-ref": "Proceedings of The Eleventh Asian Conference on Machine Learning,\n  in PMLR 101:268-283 (2019)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PAC-Bayesian set up involves a stochastic classifier characterized by a\nposterior distribution on a classifier set, offers a high probability bound on\nits averaged true risk and is robust to the training sample used. For a given\nposterior, this bound captures the trade off between averaged empirical risk\nand KL-divergence based model complexity term. Our goal is to identify an\noptimal posterior with the least PAC-Bayesian bound. We consider a finite\nclassifier set and 5 distance functions: KL-divergence, its Pinsker's and a\nsixth degree polynomial approximations; linear and squared distances. Linear\ndistance based model results in a convex optimization problem. We obtain closed\nform expression for its optimal posterior. For uniform prior, this posterior\nhas full support with weights negative-exponentially proportional to number of\nmisclassifications. Squared distance and Pinsker's approximation bounds are\npossibly quasi-convex and are observed to have single local minimum. We derive\nfixed point equations (FPEs) using partial KKT system with strict positivity\nconstraints. This obviates the combinatorial search for subset support of the\noptimal posterior. For uniform prior, exponential search on a full-dimensional\nsimplex can be limited to an ordered subset of classifiers with increasing\nempirical risk values. These FPEs converge rapidly to a stationary point, even\nfor a large classifier set when a solver fails. We apply these approaches to\nSVMs generated using a finite set of SVM regularization parameter values on 9\nUCI datasets. These posteriors yield stochastic SVM classifiers with tight\nbounds. KL-divergence based bound is the tightest, but is computationally\nexpensive due to non-convexity and multiple calls to a root finding algorithm.\nOptimal posteriors for all 5 distance functions have lowest 10% test error\nvalues on most datasets, with linear distance being the easiest to obtain.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:21:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sahu", "Puja", ""], ["Hemachandra", "Nandyala", ""]]}, {"id": "1912.06844", "submitter": "Mihai Suteu", "authors": "Mihai Suteu, Yike Guo", "title": "Regularizing Deep Multi-Task Networks using Orthogonal Gradients", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a promising approach towards multi-task learning\nbecause of their capability to leverage knowledge across domains and learn\ngeneral purpose representations. Nevertheless, they can fail to live up to\nthese promises as tasks often compete for a model's limited resources,\npotentially leading to lower overall performance. In this work we tackle the\nissue of interfering tasks through a comprehensive analysis of their training,\nderived from looking at the interaction between gradients within their shared\nparameters. Our empirical results show that well-performing models have low\nvariance in the angles between task gradients and that popular regularization\nmethods implicitly reduce this measure. Based on this observation, we propose a\nnovel gradient regularization term that minimizes task interference by\nenforcing near orthogonal gradients. Updating the shared parameters using this\nproperty encourages task specific decoders to optimize different parts of the\nfeature extractor, thus reducing competition. We evaluate our method with\nclassification and regression tasks on the multiDigitMNIST, NYUv2 and SUN RGB-D\ndatasets where we obtain competitive results.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 13:35:32 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Suteu", "Mihai", ""], ["Guo", "Yike", ""]]}, {"id": "1912.06845", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer", "title": "Mixing Time Estimation in Ergodic Markov Chains from a Single Trajectory\n  with Contraction Methods", "comments": "Accepted for presentation at ALT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixing time $t_{\\mathsf{mix}}$ of an ergodic Markov chain measures the\nrate of convergence towards its stationary distribution $\\boldsymbol{\\pi}$. We\nconsider the problem of estimating $t_{\\mathsf{mix}}$ from one single\ntrajectory of $m$ observations $(X_1, . . . , X_m)$, in the case where the\ntransition kernel $\\boldsymbol{M}$ is unknown, a research program started by\nHsu et al. [2015]. The community has so far focused primarily on leveraging\nspectral methods to estimate the relaxation time $t_{\\mathsf{rel}}$ of a\nreversible Markov chain as a proxy for $t_{\\mathsf{mix}}$. Although these\ntechniques have recently been extended to tackle non-reversible chains, this\ngeneral setting remains much less understood. Our new approach based on\ncontraction methods is the first that aims at directly estimating\n$t_{\\mathsf{mix}}$ up to multiplicative small universal constants instead of\n$t_{\\mathsf{rel}}$. It does so by introducing a generalized version of\nDobrushin's contraction coefficient $\\kappa_{\\mathsf{gen}}$, which is shown to\ncontrol the mixing time regardless of reversibility. We subsequently design\nfully data-dependent high confidence intervals around $\\kappa_{\\mathsf{gen}}$\nthat generally yield better convergence guarantees and are more practical than\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 13:38:02 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wolfer", "Geoffrey", ""]]}, {"id": "1912.06875", "submitter": "Yuwei Luo", "authors": "Yuwei Luo, Zhuoran Yang, Zhaoran Wang, Mladen Kolar", "title": "Natural Actor-Critic Converges Globally for Hierarchical Linear\n  Quadratic Regulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning has been successfully applied to a number\nof challenging problems. Despite these empirical successes, theoretical\nunderstanding of different algorithms is lacking, primarily due to the curse of\ndimensionality caused by the exponential growth of the state-action space with\nthe number of agents. We study a fundamental problem of multi-agent linear\nquadratic regulator in a setting where the agents are partially exchangeable.\nIn this setting, we develop a hierarchical actor-critic algorithm, whose\ncomputational complexity is independent of the total number of agents, and\nprove its global linear convergence to the optimal policy. As linear quadratic\nregulators are often used to approximate general dynamic systems, this paper\nprovided an important step towards better understanding of general hierarchical\nmean-field multi-agent reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:26:42 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Luo", "Yuwei", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Kolar", "Mladen", ""]]}, {"id": "1912.06876", "submitter": "Nicolas Garneau", "authors": "Nicolas Garneau, Jean-Samuel Leboeuf, Yuval Pinter and Luc Lamontagne", "title": "Attending Form and Context to Generate Specialized\n  Out-of-VocabularyWords Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new contextual-compositional neural network layer that handles\nout-of-vocabulary (OOV) words in natural language processing (NLP) tagging\ntasks. This layer consists of a model that attends to both the character\nsequence and the context in which the OOV words appear. We show that our model\nlearns to generate task-specific \\textit{and} sentence-dependent OOV word\nrepresentations without the need for pre-training on an embedding table, unlike\nprevious attempts. We insert our layer in the state-of-the-art tagging model of\n\\citet{plank2016multilingual} and thoroughly evaluate its contribution on 23\ndifferent languages on the task of jointly tagging part-of-speech and\nmorphosyntactic attributes. Our OOV handling method successfully improves\nperformances of this model on every language but one to achieve a new\nstate-of-the-art on the Universal Dependencies Dataset 1.4.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:27:57 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Garneau", "Nicolas", ""], ["Leboeuf", "Jean-Samuel", ""], ["Pinter", "Yuval", ""], ["Lamontagne", "Luc", ""]]}, {"id": "1912.06879", "submitter": "Tom Van Steenkiste", "authors": "Tom Van Steenkiste, Dirk Deschrijver, Tom Dhaene", "title": "Sensor Fusion using Backward Shortcut Connections for Sleep Apnea\n  Detection in Multi-Modal Data", "comments": "Paper presented at ML4H (Machine Learning for Health) workshop at\n  NeurIPS 2019. https://ml4health.github.io/2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a common respiratory disorder characterized by breathing\npauses during the night. Consequences of untreated sleep apnea can be severe.\nStill, many people remain undiagnosed due to shortages of hospital beds and\ntrained sleep technicians. To assist in the diagnosis process, automated\ndetection methods are being developed. Recent works have demonstrated that deep\nlearning models can extract useful information from raw respiratory data and\nthat such models can be used as a robust sleep apnea detector. However, trained\nsleep technicians take into account multiple sensor signals when annotating\nsleep recordings instead of relying on a single respiratory estimate. To\nimprove the predictive performance and reliability of the models, early and\nlate sensor fusion methods are explored in this work. In addition, a novel late\nsensor fusion method is proposed which uses backward shortcut connections to\nimprove the learning of the first stages of the models. The performance of\nthese fusion methods is analyzed using CNN as well as LSTM deep learning\nbase-models. The results demonstrate a significant and consistent improvement\nin predictive performance over the single sensor methods and over the other\nexplored sensor fusion methods, by using the proposed sensor fusion method with\nbackward shortcut connections.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:55:34 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:47:03 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Van Steenkiste", "Tom", ""], ["Deschrijver", "Dirk", ""], ["Dhaene", "Tom", ""]]}, {"id": "1912.06883", "submitter": "Reuben Binns Dr", "authors": "Reuben Binns", "title": "On the Apparent Conflict Between Individual and Group Fairness", "comments": "Conference on Fairness, Accountability, and Transparency (FAT* '20),\n  January 27--30, 2020, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A distinction has been drawn in fair machine learning research between\n`group' and `individual' fairness measures. Many technical research papers\nassume that both are important, but conflicting, and propose ways to minimise\nthe trade-offs between these measures. This paper argues that this apparent\nconflict is based on a misconception. It draws on theoretical discussions from\nwithin the fair machine learning research, and from political and legal\nphilosophy, to argue that individual and group fairness are not fundamentally\nin conflict. First, it outlines accounts of egalitarian fairness which\nencompass plausible motivations for both group and individual fairness, thereby\nsuggesting that there need be no conflict in principle. Second, it considers\nthe concept of individual justice, from legal philosophy and jurisprudence\nwhich seems similar but actually contradicts the notion of individual fairness\nas proposed in the fair machine learning literature. The conclusion is that the\napparent conflict between individual and group fairness is more of an artifact\nof the blunt application of fairness measures, rather than a matter of\nconflicting principles. In practice, this conflict may be resolved by a nuanced\nconsideration of the sources of `unfairness' in a particular deployment\ncontext, and the carefully justified application of measures to mitigate it.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 17:13:15 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Binns", "Reuben", ""]]}, {"id": "1912.06910", "submitter": "Tom Schaul", "authors": "Tom Schaul, Diana Borsa, David Ding, David Szepesvari, Georg\n  Ostrovski, Will Dabney, Simon Osindero", "title": "Adapting Behaviour for Learning Progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining what experience to generate to best facilitate learning (i.e.\nexploration) is one of the distinguishing features and open challenges in\nreinforcement learning. The advent of distributed agents that interact with\nparallel instances of the environment has enabled larger scales and greater\nflexibility, but has not removed the need to tune exploration to the task,\nbecause the ideal data for the learning algorithm necessarily depends on its\nprocess of learning. We propose to dynamically adapt the data generation by\nusing a non-stationary multi-armed bandit to optimize a proxy of the learning\nprogress. The data distribution is controlled by modulating multiple parameters\nof the policy (such as stochasticity, consistency or optimism) without\nsignificant overhead. The adaptation speed of the bandit can be increased by\nexploiting the factored modulation structure. We demonstrate on a suite of\nAtari 2600 games how this unified approach produces results comparable to\nper-task tuning at a fraction of the cost.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 19:34:47 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Schaul", "Tom", ""], ["Borsa", "Diana", ""], ["Ding", "David", ""], ["Szepesvari", "David", ""], ["Ostrovski", "Georg", ""], ["Dabney", "Will", ""], ["Osindero", "Simon", ""]]}, {"id": "1912.06969", "submitter": "Aditya Cowsik", "authors": "Aditya Cowsik, John W. Clark", "title": "Breast Cancer Diagnosis by Higher-Order Probabilistic Perceptrons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-layer neural network model that systematically includes correlations\namong input variables to arbitrary order and is designed to implement Bayes\ninference has been adapted to classify breast cancer tumors as malignant or\nbenign, assigning a probability for either outcome. The inputs to the network\nrepresent measured characteristics of cell nuclei imaged in Fine Needle\nAspiration biopsies. The present machine-learning approach to diagnosis (known\nas HOPP, for higher-order probabilistic perceptron) is tested on the\nmuch-studied, open-access Breast Cancer Wisconsin (Diagnosis) Data Set of\nWolberg et al. This set lists, for each tumor, measured physical parameters of\nthe cell nuclei of each sample. The HOPP model can identify the key factors --\ninput features and their combinations -- most relevant for reliable diagnosis.\nHOPP networks were trained on 90\\% of the examples in the Wisconsin database,\nand tested on the remaining 10\\%. Referred to ensembles of 300 networks,\nselected randomly for cross-validation, accuracy of classification for the test\nsets of up to 97\\% was readily achieved, with standard deviation around 2\\%,\ntogether with average Matthews correlation coefficients reaching 0.94\nindicating excellent predictive performance. Demonstrably, the HOPP is capable\nof matching the predictive power attained by other advanced machine-learning\nalgorithms applied to this much-studied database, over several decades.\nAnalysis shows that in this special problem, which is almost linearly\nseparable, the effects of irreducible correlations among the measured features\nof the Wisconsin database are of relatively minor importance, as the Naive\nBayes approximation can itself yield predictive accuracy approaching 95\\%. The\nadvantages of the HOPP algorithm will be more clearly revealed in application\nto more challenging machine-learning problems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 03:00:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Cowsik", "Aditya", ""], ["Clark", "John W.", ""]]}, {"id": "1912.06977", "submitter": "Steve Yadlowsky", "authors": "Steve Yadlowsky, Fabio Pellegrini, Federica Lionetto, Stefan Braune,\n  and Lu Tian", "title": "Estimation and Validation of Ratio-based Conditional Average Treatment\n  Effects Using Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sample sizes in randomized clinical trials are large enough to estimate\nthe average treatment effect well, they are often insufficient for estimation\nof treatment-covariate interactions critical to studying data-driven precision\nmedicine. Observational data from real world practice may play an important\nrole in alleviating this problem. One common approach in trials is to predict\nthe outcome of interest with separate regression models in each treatment arm,\nand estimate the treatment effect based on the contrast of the predictions.\nUnfortunately, this simple approach may induce spurious treatment-covariate\ninteraction in observational studies when the regression model is misspecified.\nMotivated by the need of modeling the number of relapses in multiple sclerosis\npatients, where the ratio of relapse rates is a natural choice of the treatment\neffect, we propose to estimate the conditional average treatment effect (CATE)\nas the ratio of expected potential outcomes, and derive a doubly robust\nestimator of this CATE in a semiparametric model of treatment-covariate\ninteractions. We also provide a validation procedure to check the quality of\nthe estimator on an independent sample. We conduct simulations to demonstrate\nthe finite sample performance of the proposed methods, and illustrate their\nadvantages on real data by examining the treatment effect of dimethyl fumarate\ncompared to teriflunomide in multiple sclerosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:13:56 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 00:48:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yadlowsky", "Steve", ""], ["Pellegrini", "Fabio", ""], ["Lionetto", "Federica", ""], ["Braune", "Stefan", ""], ["Tian", "Lu", ""]]}, {"id": "1912.06987", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Lei Wu", "title": "The Generalization Error of the Minimum-norm Solutions for\n  Over-parameterized Neural Networks", "comments": "Published version", "journal-ref": "Pure and Applied Functional Analysis, Volume 5, Number 6,\n  1145-1460, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization properties of minimum-norm solutions for three\nover-parametrized machine learning models including the random feature model,\nthe two-layer neural network model and the residual network model. We proved\nthat for all three models, the generalization error for the minimum-norm\nsolution is comparable to the Monte Carlo rate, up to some logarithmic terms,\nas long as the models are sufficiently over-parametrized.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 06:05:14 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 15:16:12 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wu", "Lei", ""]]}, {"id": "1912.06989", "submitter": "Jicong Fan", "authors": "Jicong Fan, Yuqian Zhang, Madeleine Udell", "title": "Polynomial Matrix Completion for Missing Data Imputation and\n  Transductive Learning", "comments": "Accepted by AAAI 2020. The supplementary material is at\n  https://github.com/jicongfan/Supplementary-material-of-conference-papers/blob/master/supp_PMC_AAAI2020.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops new methods to recover the missing entries of a high-rank\nor even full-rank matrix when the intrinsic dimension of the data is low\ncompared to the ambient dimension. Specifically, we assume that the columns of\na matrix are generated by polynomials acting on a low-dimensional intrinsic\nvariable, and wish to recover the missing entries under this assumption. We\nshow that we can identify the complete matrix of minimum intrinsic dimension by\nminimizing the rank of the matrix in a high dimensional feature space. We\ndevelop a new formulation of the resulting problem using the kernel trick\ntogether with a new relaxation of the rank objective, and propose an efficient\noptimization method. We also show how to use our methods to complete data drawn\nfrom multiple nonlinear manifolds. Comparative studies on synthetic data,\nsubspace clustering with missing data, motion capture data recovery, and\ntransductive learning verify the superiority of our methods over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 06:39:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Fan", "Jicong", ""], ["Zhang", "Yuqian", ""], ["Udell", "Madeleine", ""]]}, {"id": "1912.06991", "submitter": "Amir Bahador Parsa", "authors": "Amir Bahador Parsa, Rishabh Singh Chauhan, Homa Taghipour, Sybil\n  Derrible, Abolfazl Mohammadian", "title": "Applying Deep Learning to Detect Traffic Accidents in Real Time Using\n  Spatiotemporal Sequential Data", "comments": "13 pages, 4 figures,2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accident detection is a vital part of traffic safety. Many road users suffer\nfrom traffic accidents, as well as their consequences such as delay,\ncongestion, air pollution, and so on. In this study, we utilize two advanced\ndeep learning techniques, Long Short-Term Memory (LSTM) and Gated Recurrent\nUnits (GRUs), to detect traffic accidents in Chicago. These two techniques are\nselected because they are known to perform well with sequential data (i.e.,\ntime series). The full dataset consists of 241 accident and 6,038 non-accident\ncases selected from Chicago expressway, and it includes traffic spatiotemporal\ndata, weather condition data, and congestion status data. Moreover, because the\ndataset is imbalanced (i.e., the dataset contains many more non-accident cases\nthan accident cases), Synthetic Minority Over-sampling Technique (SMOTE) is\nemployed. Overall, the two models perform significantly well, both with an Area\nUnder Curve (AUC) of 0.85. Nonetheless, the GRU model is observed to perform\nslightly better than LSTM model with respect to detection rate. The performance\nof both models is similar in terms of false alarm rate.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 06:49:43 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 20:45:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Parsa", "Amir Bahador", ""], ["Chauhan", "Rishabh Singh", ""], ["Taghipour", "Homa", ""], ["Derrible", "Sybil", ""], ["Mohammadian", "Abolfazl", ""]]}, {"id": "1912.07018", "submitter": "Adarsh Kappiyath", "authors": "Silpa V S, Adarsh K, Sumitra S, Raju K George", "title": "Disentanglement based Active Learning", "comments": "Accepted to NewinML workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Disentanglement based Active Learning (DAL), a new active learning\ntechnique based on query synthesis which leverages the concept of\ndisentanglement. Instead of requesting labels from the human oracle, our method\nautomatically labels majority of the datapoints, thus drastically reducing the\nhuman labelling budget in active learning. The proposed method uses Information\nMaximizing Generative Adversarial Nets (InfoGAN) to achieve the task where the\nactive learner provides a feedback on the generation of InfoGAN based on which\ndecision is taken about the datapoints to be queried. Results on two benchmark\ndatasets demonstrate that DAL is able to achieve nearly fully supervised\naccuracy with fairly less labelling budget compared to existing active learning\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 10:48:06 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["S", "Silpa V", ""], ["K", "Adarsh", ""], ["S", "Sumitra", ""], ["George", "Raju K", ""]]}, {"id": "1912.07048", "submitter": "Alexander Korotin", "authors": "Alexander Korotin, Vladimir V'yugin, Evgeny Burnaev", "title": "Integral Mixability: a Tool for Efficient Online Aggregation of\n  Functional and Probabilistic Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the setting of the online prediction with expert\nadvice to function-valued forecasts. At each step of the online game several\nexperts predict a function, and the learner has to efficiently aggregate these\nfunctional forecasts into a single forecast. We adapt basic mixable (and\nexponentially concave) loss functions to compare functional predictions and\nprove that these adaptations are also mixable (exp-concave). We call this\nphenomena integral mixability (exp-concavity). As an application of our main\nresult, we prove that various loss functions used for probabilistic forecasting\nare mixable (exp-concave). The considered losses include Sliced Continuous\nRanking Probability Score, Energy-Based Distance, Optimal Transport Costs &\nSliced Wasserstein-2 distance, Beta-2 & Kullback-Leibler divergences,\nCharacteristic function and Maximum Mean Discrepancies.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:25:33 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:04:34 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 11:52:20 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Korotin", "Alexander", ""], ["V'yugin", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1912.07123", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rinc\\'on, Valeria Muro, Carlos D'Giano, Jorge\n  Prendes, Hadj Batatia", "title": "A novel spike-and-wave automatic detection in EEG signals", "comments": "8 pages, 3 figures", "journal-ref": "Computers (MDPI AG) 2020", "doi": "10.3390/computers9040085", "report-no": "computers9040085", "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-and-wave discharge (SWD) pattern classification in\nelectroencephalography (EEG) signals is a key problem in signal processing. It\nis particularly important to develop a SWD automatic detection method in\nlong-term EEG recordings since the task of marking the patters manually is time\nconsuming, difficult and error-prone. This paper presents a new detection\nmethod with a low computational complexity that can be easily trained if\nstandard medical protocols are respected. The detection procedure is as\nfollows: First, each EEG signal is divided into several time segments and for\neach time segment, the Morlet 1-D decomposition is applied. Then three\nparameters are extracted from the wavelet coefficients of each segment: scale\n(using a generalized Gaussian statistical model), variance and median. This is\nfollowed by a k-nearest neighbors (k-NN) classifier to detect the\nspike-and-wave pattern in each EEG channel from these three parameters. A total\nof 106 spike-and-wave and 106 non-spike-and-wave were used for training, while\n69 new annotated EEG segments from six subjects were used for classification.\nIn these circumstances, the proposed methodology achieved 100% accuracy. These\nresults generate new research opportunities for the underlying causes of the\nso-called absence epilepsy in long-term EEG recordings.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:42:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Quintero-Rinc\u00f3n", "Antonio", ""], ["Muro", "Valeria", ""], ["D'Giano", "Carlos", ""], ["Prendes", "Jorge", ""], ["Batatia", "Hadj", ""]]}, {"id": "1912.07127", "submitter": "Amirhossein Kiani", "authors": "Amirhossein Kiani, Chris Wang, Angela Xu", "title": "Sepsis World Model: A MIMIC-based OpenAI Gym \"World Model\" Simulator for\n  Sepsis Treatment", "comments": "This project was done as a class project for CS221 at Stanford\n  University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a life-threatening condition caused by the body's response to an\ninfection. In order to treat patients with sepsis, physicians must control\nvarying dosages of various antibiotics, fluids, and vasopressors based on a\nlarge number of variables in an emergency setting. In this project we employ a\n\"world model\" methodology to create a simulator that aims to predict the next\nstate of a patient given a current state and treatment action. In doing so, we\nhope our simulator learns from a latent and less noisy representation of the\nEHR data. Using historical sepsis patient records from the MIMIC dataset, our\nmethod creates an OpenAI Gym simulator that leverages a Variational\nAuto-Encoder and a Mixture Density Network combined with a RNN (MDN-RNN) to\nmodel the trajectory of any sepsis patient in the hospital. To reduce the\neffects of noise, we sample from a generated distribution of next steps during\nsimulation and have the option of introducing uncertainty into our simulator by\ncontrolling the \"temperature\" variable. It is worth noting that we do not have\naccess to the ground truth for the best policy because we can only evaluate\nlearned policies by real-world experimentation or expert feedback. Instead, we\naim to study our simulator model's performance by evaluating the similarity\nbetween our environment's rollouts with the real EHR data and assessing its\nviability for learning a realistic policy for sepsis treatment using Deep\nQ-Learning.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:50:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kiani", "Amirhossein", ""], ["Wang", "Chris", ""], ["Xu", "Angela", ""]]}, {"id": "1912.07146", "submitter": "Dmitriy Drusvyatskiy", "authors": "Damek Davis, Dmitriy Drusvyatskiy", "title": "Proximal methods avoid active strict saddles of weakly convex functions", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a geometrically transparent strict saddle property for nonsmooth\nfunctions. This property guarantees that simple proximal algorithms on weakly\nconvex problems converge only to local minimizers, when randomly initialized.\nWe argue that the strict saddle property may be a realistic assumption in\napplications, since it provably holds for generic semi-algebraic optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 00:56:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 19:25:08 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Davis", "Damek", ""], ["Drusvyatskiy", "Dmitriy", ""]]}, {"id": "1912.07160", "submitter": "Sizhe Chen", "authors": "Sizhe Chen, Xiaolin Huang, Zhengbao He, Chengjin Sun", "title": "DAmageNet: A Universal Adversarial Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well known that deep neural networks (DNNs) are vulnerable to\nadversarial attack. Adversarial samples are similar to the clean ones, but are\nable to cheat the attacked DNN to produce incorrect predictions in high\nconfidence. But most of the existing adversarial attacks have high success rate\nonly when the information of the attacked DNN is well-known or could be\nestimated by massive queries. A promising way is to generate adversarial\nsamples with high transferability. By this way, we generate 96020 transferable\nadversarial samples from original ones in ImageNet. The average difference,\nmeasured by root means squared deviation, is only around 3.8 on average.\nHowever, the adversarial samples are misclassified by various models with an\nerror rate up to 90\\%. Since the images are generated independently with the\nattacked DNNs, this is essentially zero-query adversarial attack. We call the\ndataset \\emph{DAmageNet}, which is the first universal adversarial dataset that\nbeats many models trained in ImageNet. By finding the drawbacks, DAmageNet\ncould serve as a benchmark to study and improve robustness of DNNs. DAmageNet\ncould be downloaded in http://www.pami.sjtu.edu.cn/Show/56/122.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:11:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chen", "Sizhe", ""], ["Huang", "Xiaolin", ""], ["He", "Zhengbao", ""], ["Sun", "Chengjin", ""]]}, {"id": "1912.07209", "submitter": "Mohammad Majdi", "authors": "Mohammad S Majdi (1), Mahesh B Keerthivasan (2 and 5), Brian K Rutt\n  (3), Natalie M Zahr (4), Jeffrey J Rodriguez (1), Manojkumar Saranathan (1\n  and 2) ((1) Department of Electrical and Computer Engineering, University of\n  Arizona, (2) Department of Medical Imaging, University of Arizona, (3)\n  Department of Radiology, Stanford University, (4) Department of Psychiatry\n  and Behavioral Sciences, Stanford University, (5) Siemens Healthcare USA)", "title": "Automated Thalamic Nuclei Segmentation Using Multi-Planar Cascaded\n  Convolutional Neural Networks", "comments": "Submitted to Magnetic Resonance Imaging. 34 pages, 6 figures , 2\n  tables, 1 supporting figures, 2 supporting tables. Magnetic Resonance Imaging\n  (2020)", "journal-ref": null, "doi": "10.1016/j.mri.2020.08.005", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cascaded multi-planar scheme with a modified residual U-Net architecture\nwas used to segment thalamic nuclei on conventional and white-matter-nulled\n(WMn) magnetization prepared rapid gradient echo (MPRAGE) data. A single\nnetwork was optimized to work with images from healthy controls and patients\nwith multiple sclerosis (MS) and essential tremor (ET), acquired at both 3T and\n7T field strengths. Dice similarity coefficient and volume similarity index\n(VSI) were used to evaluate performance. Clinical utility was demonstrated by\napplying this method to study the effect of MS on thalamic nuclei atrophy.\nSegmentation of each thalamus into twelve nuclei was achieved in under a\nminute. For 7T WMn-MPRAGE, the proposed method outperforms current\nstate-of-the-art on patients with ET with statistically significant\nimprovements in Dice for five nuclei (increase in the range of 0.05-0.18) and\nVSI for four nuclei (increase in the range of 0.05-0.19), while performing\ncomparably for healthy and MS subjects. Dice and VSI achieved using 7T\nWMn-MPRAGE data are comparable to those using 3T WMn-MPRAGE data. For\nconventional MPRAGE, the proposed method shows a statistically significant Dice\nimprovement in the range of 0.14-0.63 over FreeSurfer for all nuclei and\ndisease types. Effect of noise on network performance shows robustness to\nimages with SNR as low as half the baseline SNR. Atrophy of four thalamic\nnuclei and whole thalamus was observed for MS patients compared to healthy\ncontrol subjects, after controlling for the effect of parallel imaging,\nintracranial volume, gender, and age (p<0.004). The proposed segmentation\nmethod is fast, accurate, performs well across disease types and field\nstrengths, and shows great potential for improving our understanding of\nthalamic nuclei involvement in neurological diseases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:57:33 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 21:06:00 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Majdi", "Mohammad S", "", "2 and 5"], ["Keerthivasan", "Mahesh B", "", "2 and 5"], ["Rutt", "Brian K", "", "1\n  and 2"], ["Zahr", "Natalie M", "", "1\n  and 2"], ["Rodriguez", "Jeffrey J", "", "1\n  and 2"], ["Saranathan", "Manojkumar", "", "1\n  and 2"]]}, {"id": "1912.07211", "submitter": "Yukun Zhang", "authors": "Yukun Zhang, Longsheng Zhou", "title": "Fairness Assessment for Artificial Intelligence in Financial Industry", "comments": "Robust AI in FS 2019 : NeurIPS 2019 Workshop on Robust AI in\n  Financial Services: Data, Fairness, Explainability, Trustworthiness, and\n  Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) is an important driving force for the\ndevelopment and transformation of the financial industry. However, with the\nfast-evolving AI technology and application, unintentional bias, insufficient\nmodel validation, immature contingency plan and other underestimated threats\nmay expose the company to operational and reputational risks. In this paper, we\nfocus on fairness evaluation, one of the key components of AI Governance,\nthrough a quantitative lens. Statistical methods are reviewed for imbalanced\ndata treatment and bias mitigation. These methods and fairness evaluation\nmetrics are then applied to a credit card default payment example.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 06:09:39 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhang", "Yukun", ""], ["Zhou", "Longsheng", ""]]}, {"id": "1912.07242", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran", "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:28:26 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Nakkiran", "Preetum", ""]]}, {"id": "1912.07248", "submitter": "Chenping Hou", "authors": "Hong Tao and Chenping Hou and Yuhua Qian and Jubo Zhu and Dongyun Yi", "title": "Latent Complete Row Space Recovery for Multi-view Subspace Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3010631", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view subspace clustering has been applied to applications such as image\nprocessing and video surveillance, and has attracted increasing attention. Most\nexisting methods learn view-specific self-representation matrices, and\nconstruct a combined affinity matrix from multiple views. The affinity\nconstruction process is time-consuming, and the combined affinity matrix is not\nguaranteed to reflect the whole true subspace structure. To overcome these\nissues, the Latent Complete Row Space Recovery (LCRSR) method is proposed.\nConcretely, LCRSR is based on the assumption that the multi-view observations\nare generated from an underlying latent representation, which is further\nassumed to collect the authentic samples drawn exactly from multiple subspaces.\nLCRSR is able to recover the row space of the latent representation, which not\nonly carries complete information from multiple views but also determines the\nsubspace membership under certain conditions. LCRSR does not involve the graph\nconstruction procedure and is solved with an efficient and convergent\nalgorithm, thereby being more scalable to large-scale datasets. The\neffectiveness and efficiency of LCRSR are validated by clustering various kinds\nof multi-view data and illustrated in the background subtraction task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:46:03 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Tao", "Hong", ""], ["Hou", "Chenping", ""], ["Qian", "Yuhua", ""], ["Zhu", "Jubo", ""], ["Yi", "Dongyun", ""]]}, {"id": "1912.07254", "submitter": "Haoyu Yang", "authors": "Haoyu Yang, Wei Zhong, Yuzhe Ma, Hao Geng, Ran Chen, Wanli Chen, Bei\n  Yu", "title": "VLSI Mask Optimization: From Shallow To Deep Learning", "comments": "6 pages; accepted by 25th Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VLSI mask optimization is one of the most critical stages in\nmanufacturability aware design, which is costly due to the complicated mask\noptimization and lithography simulation. Recent researches have shown prominent\nadvantages of machine learning techniques dealing with complicated and big data\nproblems, which bring potential of dedicated machine learning solution for DFM\nproblems and facilitate the VLSI design cycle. In this paper, we focus on a\nheterogeneous OPC framework that assists mask layout optimization. Preliminary\nresults show the efficiency and effectiveness of proposed frameworks that have\nthe potential to be alternatives to existing EDA solutions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 09:11:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yang", "Haoyu", ""], ["Zhong", "Wei", ""], ["Ma", "Yuzhe", ""], ["Geng", "Hao", ""], ["Chen", "Ran", ""], ["Chen", "Wanli", ""], ["Yu", "Bei", ""]]}, {"id": "1912.07360", "submitter": "Angshul Majumdar Dr.", "authors": "Shikha Singh and Angshul Majumdar", "title": "Non-intrusive Load Monitoring via Multi-label Sparse Representation\n  based Classification", "comments": "The final version has been accepted at IEEE Transactions on Smartgrid", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work follows the approach of multi-label classification for\nnon-intrusive load monitoring (NILM). We modify the popular sparse\nrepresentation based classification (SRC) approach (developed for single label\nclassification) to solve multi-label classification problems. Results on\nbenchmark REDD and Pecan Street dataset shows significant improvement over\nstate-of-the-art techniques with small volume of training data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:15:32 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Singh", "Shikha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.07366", "submitter": "Piyush Pandita", "authors": "Piyush Pandita, Nimish Awalgaonkar, Ilias Bilionis and Jitesh Panchal", "title": "Learning Arbitrary Quantities of Interest from Expensive Black-Box\n  Functions through Bayesian Sequential Optimal Design", "comments": "58 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating arbitrary quantities of interest (QoIs) that are non-linear\noperators of complex, expensive-to-evaluate, black-box functions is a\nchallenging problem due to missing domain knowledge and finite budgets.\nBayesian optimal design of experiments (BODE) is a family of methods that\nidentify an optimal design of experiments (DOE) under different contexts, using\nonly in a limited number of function evaluations. Under BODE methods,\nsequential design of experiments (SDOE) accomplishes this task by selecting an\noptimal sequence of experiments while using data-driven probabilistic surrogate\nmodels instead of the expensive black-box function. Probabilistic predictions\nfrom the surrogate model are used to define an information acquisition function\n(IAF) which quantifies the marginal value contributed or the expected\ninformation gained by a hypothetical experiment. The next experiment is\nselected by maximizing the IAF. A generally applicable IAF is the expected\ninformation gain (EIG) about a QoI as captured by the expectation of the\nKullback-Leibler divergence between the predictive distribution of the QoI\nafter doing a hypothetical experiment and the current predictive distribution\nabout the same QoI. We model the underlying information source as a\nfully-Bayesian, non-stationary Gaussian process (FBNSGP), and derive an\napproximation of the information gain of a hypothetical experiment about an\narbitrary QoI conditional on the hyper-parameters The EIG about the same QoI is\nestimated by sample averages to integrate over the posterior of the\nhyper-parameters and the potential experimental outcomes. We demonstrate the\nperformance of our method in four numerical examples and a practical\nengineering problem of steel wire manufacturing. The method is compared to two\nclassic SDOE methods: random sampling and uncertainty sampling.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:55:07 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pandita", "Piyush", ""], ["Awalgaonkar", "Nimish", ""], ["Bilionis", "Ilias", ""], ["Panchal", "Jitesh", ""]]}, {"id": "1912.07367", "submitter": "Haolin Fei", "authors": "Haolin Fei and Xiaofeng Wu and Chunbo Luo", "title": "A Model-driven and Data-driven Fusion Framework for Accurate Air Quality\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air quality is closely related to public health. Health issues such as\ncardiovascular diseases and respiratory diseases, may have connection with long\nexposure to highly polluted environment. Therefore, accurate air quality\nforecasts are extremely important to those who are vulnerable. To estimate the\nvariation of several air pollution concentrations, previous researchers used\nvarious approaches, such as the Community Multiscale Air Quality model (CMAQ)\nor neural networks. Although CMAQ model considers a coverage of the historic\nair pollution data and meteorological variables, extra bias is introduced due\nto additional adjustment. In this paper, a combination of model-based strategy\nand data-driven method namely the physical-temporal collection(PTC) model is\nproposed, aiming to fix the systematic error that traditional models deliver.\nIn the data-driven part, the first components are the temporal pattern and the\nweather pattern to measure important features that contribute to the prediction\nperformance. The less relevant input variables will be removed to eliminate\nnegative weights in network training. Then, we deploy a long-short-term-memory\n(LSTM) to fetch the preliminary results, which will be further corrected by a\nneural network (NN) involving the meteorological index as well as other\npollutants concentrations. The data-set we applied for forecasting is from\nJanuary 1st, 2016 to December 31st, 2016. According to the results, our PTC\nachieves an excellent performance compared with the baseline model (CMAQ\nprediction, GRU, DNN and etc.). This joint model-based data-driven method for\nair quality prediction can be easily deployed on stations without extra\nadjustment, providing results with high-time-resolution information for\nvulnerable members to prevent heavy air pollution ahead.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:24:11 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Fei", "Haolin", ""], ["Wu", "Xiaofeng", ""], ["Luo", "Chunbo", ""]]}, {"id": "1912.07418", "submitter": "Huajun Wang", "authors": "Huajun Wang, Yuanhai Shao, Shenglong Zhou, Ce Zhang and Naihua Xiu", "title": "Support Vector Machine Classifier via $L_{0/1}$ Soft-Margin Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) has attracted great attentions for the last two\ndecades due to its extensive applications, and thus numerous optimization\nmodels have been proposed. To distinguish all of them, in this paper, we\nintroduce a new model equipped with an $L_{0/1}$ soft-margin loss (dubbed as\n$L_{0/1}$-SVM) which well captures the nature of the binary classification.\nMany of the existing convex/non-convex soft-margin losses can be viewed as a\nsurrogate of the $L_{0/1}$ soft-margin loss. Despite the discrete nature of\n$L_{0/1}$, we manage to establish the existence of global minimizer of the new\nmodel as well as revealing the relationship among its minimizers and\nKKT/P-stationary points. These theoretical properties allow us to take\nadvantage of the alternating direction method of multipliers. In addition, the\n$L_{0/1}$-support vector operator is introduced as a filter to prevent outliers\nfrom being support vectors during the training process. Hence, the method is\nexpected to be relatively robust. Finally, numerical experiments demonstrate\nthat our proposed method generates better performance in terms of much shorter\ncomputational time with much fewer number of support vectors when against with\nsome other leading methods in areas of SVM. When the data size gets bigger, its\nadvantage becomes more evident.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:42:30 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 13:36:15 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 03:30:28 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 14:30:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Huajun", ""], ["Shao", "Yuanhai", ""], ["Zhou", "Shenglong", ""], ["Zhang", "Ce", ""], ["Xiu", "Naihua", ""]]}, {"id": "1912.07435", "submitter": "Benjamin Lu", "authors": "Benjamin Lu and Johanna Hardin", "title": "A Unified Framework for Random Forest Prediction Error Estimation", "comments": "41 pages, 8 figures, 6 tables", "journal-ref": "Journal of Machine Learning Research, 22(8):1-41, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a unified framework for random forest prediction error\nestimation based on a novel estimator of the conditional prediction error\ndistribution function. Our framework enables simple plug-in estimation of key\nprediction uncertainty metrics, including conditional mean squared prediction\nerrors, conditional biases, and conditional quantiles, for random forests and\nmany variants. Our approach is especially well-adapted for prediction interval\nestimation; we show via simulations that our proposed prediction intervals are\ncompetitive with, and in some settings outperform, existing methods. To\nestablish theoretical grounding for our framework, we prove pointwise uniform\nconsistency of a more stringent version of our estimator of the conditional\nprediction error distribution function. The estimators introduced here are\nimplemented in the R package forestError.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:11:15 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 19:22:49 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 06:32:06 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 22:15:52 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 22:05:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lu", "Benjamin", ""], ["Hardin", "Johanna", ""]]}, {"id": "1912.07441", "submitter": "Joeran Beel", "authors": "Nicholas Bonello, Joeran Beel, Seamus Lawless, Jeremy Debattista", "title": "Multi-stream Data Analytics for Enhanced Performance Prediction in\n  Fantasy Football", "comments": null, "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science. 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fantasy Premier League (FPL) performance predictors tend to base their\nalgorithms purely on historical statistical data. The main problems with this\napproach is that external factors such as injuries, managerial decisions and\nother tournament match statistics can never be factored into the final\npredictions. In this paper, we present a new method for predicting future\nplayer performances by automatically incorporating human feedback into our\nmodel. Through statistical data analysis such as previous performances,\nupcoming fixture difficulty ratings, betting market analysis, opinions of the\ngeneral-public and experts alike via social media and web articles, we can\nimprove our understanding of who is likely to perform well in upcoming matches.\nWhen tested on the English Premier League 2018/19 season, the model\noutperformed regular statistical predictors by over 300 points, an average of\n11 points per week, ranking within the top 0.5% of players rank 30,000 out of\nover 6.5 million players.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:24:30 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bonello", "Nicholas", ""], ["Beel", "Joeran", ""], ["Lawless", "Seamus", ""], ["Debattista", "Jeremy", ""]]}, {"id": "1912.07443", "submitter": "Reza Khodayi-Mehr", "authors": "Reza Khodayi-Mehr and Michael M. Zavlanos", "title": "VarNet: Variational Neural Networks for the Solution of Partial\n  Differential Equations", "comments": "15 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.comp-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new model-based unsupervised learning method,\ncalled VarNet, for the solution of partial differential equations (PDEs) using\ndeep neural networks (NNs). Particularly, we propose a novel loss function that\nrelies on the variational (integral) form of PDEs as apposed to their\ndifferential form which is commonly used in the literature. Our loss function\nis discretization-free, highly parallelizable, and more effective in capturing\nthe solution of PDEs since it employs lower-order derivatives and trains over\nmeasure non-zero regions of space-time. Given this loss function, we also\npropose an approach to optimally select the space-time samples, used to train\nthe NN, that is based on the feedback provided from the PDE residual. The\nmodels obtained using VarNet are smooth and do not require interpolation. They\nare also easily differentiable and can directly be used for control and\noptimization of PDEs. Finally, VarNet can straight-forwardly incorporate\nparametric PDE models making it a natural tool for model order reduction (MOR)\nof PDEs. We demonstrate the performance of our method through extensive\nnumerical experiments for the advection-diffusion PDE as an important\ncase-study.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:30:48 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Khodayi-Mehr", "Reza", ""], ["Zavlanos", "Michael M.", ""]]}, {"id": "1912.07458", "submitter": "Kanil Patel", "authors": "Kanil Patel, William Beluch, Dan Zhang, Michael Pfeiffer, Bin Yang", "title": "On-manifold Adversarial Data Augmentation Improves Uncertainty\n  Calibration", "comments": "Accepted for oral at International Conference on Pattern Recognition,\n  ICPR 2020. Nominated (top 4) for Best Industry Related Paper Award (BIRPA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimates help to identify ambiguous, novel, or anomalous inputs,\nbut the reliable quantification of uncertainty has proven to be challenging for\nmodern deep networks. In order to improve uncertainty estimation, we propose\nOn-Manifold Adversarial Data Augmentation or OMADA, which specifically attempts\nto generate the most challenging examples by following an on-manifold\nadversarial attack path in the latent space of an autoencoder-based generative\nmodel that closely approximates decision boundaries between two or more\nclasses. On a variety of datasets as well as on multiple diverse network\narchitectures, OMADA consistently yields more accurate and better calibrated\nclassifiers than baseline models, and outperforms competing approaches such as\nMixup, as well as achieving similar performance to (at times better than)\npost-processing calibration methods such as temperature scaling. Variants of\nOMADA can employ different sampling schemes for ambiguous on-manifold examples\nbased on the entropy of their estimated soft labels, which exhibit specific\nstrengths for generalization, calibration of predicted uncertainty, or\ndetection of out-of-distribution inputs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:43:03 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 22:12:20 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 13:38:50 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 14:18:49 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 22:32:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Patel", "Kanil", ""], ["Beluch", "William", ""], ["Zhang", "Dan", ""], ["Pfeiffer", "Michael", ""], ["Yang", "Bin", ""]]}, {"id": "1912.07464", "submitter": "Shao-Bo Lin", "authors": "Charles K. Chui, Shao-Bo Lin, Bo Zhang, and Ding-Xuan Zhou", "title": "Realization of spatial sparseness by deep ReLU nets with massive data", "comments": "15pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great success of deep learning poses urgent challenges for understanding\nits working mechanism and rationality. The depth, structure, and massive size\nof the data are recognized to be three key ingredients for deep learning. Most\nof the recent theoretical studies for deep learning focus on the necessity and\nadvantages of depth and structures of neural networks. In this paper, we aim at\nrigorous verification of the importance of massive data in embodying the\nout-performance of deep learning. To approximate and learn spatially sparse and\nsmooth functions, we establish a novel sampling theorem in learning theory to\nshow the necessity of massive data. We then prove that implementing the\nclassical empirical risk minimization on some deep nets facilitates in\nrealization of the optimal learning rates derived in the sampling theorem. This\nperhaps explains why deep learning performs so well in the era of big data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:55:51 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chui", "Charles K.", ""], ["Lin", "Shao-Bo", ""], ["Zhang", "Bo", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1912.07519", "submitter": "Angshul Majumdar Dr.", "authors": "Janki Mehta and Angshul Majumdar", "title": "RODEO: Robust DE-aliasing autoencOder for Real-time Medical Image\n  Reconstruction", "comments": "Final paper accepted at Pattern Recognition. arXiv admin note: text\n  overlap with arXiv:1503.06383", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of real-time dynamic medical MRI and X\nRay CT image reconstruction from parsimonious samples Fourier frequency space\nfor MRI and sinogram tomographic projections for CT. Today the de facto\nstandard for such reconstruction is compressed sensing. CS produces high\nquality images (with minimal perceptual loss, but such reconstructions are time\nconsuming, requiring solving a complex optimization problem. In this work we\npropose to learn the reconstruction from training samples using an autoencoder.\nOur work is based on the universal function approximation capacity of neural\nnetworks. The training time for the autoencoder is large, but is offline and\nhence does not affect performance during operation. During testing or\noperation, our method requires only a few matrix vector products and hence is\nsignificantly faster than CS based methods. In fact, it is fast enough for\nreal-time reconstruction the images are reconstructed as fast as they are\nacquired with only slight degradation of image quality. However, in order to\nmake the autoencoder suitable for our problem, we depart from the standard\nEuclidean norm cost function of autoencoders and use a robust l1-norm instead.\nThe ensuing problem is solved using the Split Bregman method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:16:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Mehta", "Janki", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.07544", "submitter": "John Winder", "authors": "John Winder, Stephanie Milani, Matthew Landen, Erebus Oh, Shane Parr,\n  Shawn Squire, Marie desJardins, Cynthia Matuszek", "title": "Planning with Abstract Learned Models While Learning Transferable\n  Subtasks", "comments": "Accepted at AAAI-20, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm for model-based hierarchical reinforcement learning\nto acquire self-contained transition and reward models suitable for\nprobabilistic planning at multiple levels of abstraction. We call this\nframework Planning with Abstract Learned Models (PALM). By representing\nsubtasks symbolically using a new formal structure, the lifted abstract Markov\ndecision process (L-AMDP), PALM learns models that are independent and modular.\nThrough our experiments, we show how PALM integrates planning and execution,\nfacilitating a rapid and efficient learning of abstract, hierarchical models.\nWe also demonstrate the increased potential for learned models to be\ntransferred to new and related tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:47:57 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 15:09:33 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Winder", "John", ""], ["Milani", "Stephanie", ""], ["Landen", "Matthew", ""], ["Oh", "Erebus", ""], ["Parr", "Shane", ""], ["Squire", "Shawn", ""], ["desJardins", "Marie", ""], ["Matuszek", "Cynthia", ""]]}, {"id": "1912.07546", "submitter": "Prateek Raj Srivastava", "authors": "Prateek R. Srivastava, Purnamrita Sarkar, Grani A. Hanasusanto", "title": "A Robust Spectral Clustering Algorithm for Sub-Gaussian Mixture Models\n  with Outliers", "comments": "54 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering datasets in the presence of arbitrary\noutliers. Traditional clustering algorithms such as k-means and spectral\nclustering are known to perform poorly for datasets contaminated with even a\nsmall number of outliers. In this paper, we develop a provably robust spectral\nclustering algorithm that applies a simple rounding scheme to denoise a\nGaussian kernel matrix built from the data points and uses vanilla spectral\nclustering to recover the cluster labels of data points. We analyze the\nperformance of our algorithm under the assumption that the \"good\" data points\nare generated from a mixture of sub-gaussians (we term these \"inliers\"), while\nthe outlier points can come from any arbitrary probability distribution. For\nthis general class of models, we show that the misclassification error decays\nat an exponential rate in the signal-to-noise ratio, provided the number of\noutliers is a small fraction of the inlier points. Surprisingly, this derived\nerror bound matches with the best-known bound for semidefinite programs (SDPs)\nunder the same setting without outliers. We conduct extensive experiments on a\nvariety of simulated and real-world datasets to demonstrate that our algorithm\nis less sensitive to outliers compared to other state-of-the-art algorithms\nproposed in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:48:58 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 00:47:37 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 08:32:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Srivastava", "Prateek R.", ""], ["Sarkar", "Purnamrita", ""], ["Hanasusanto", "Grani A.", ""]]}, {"id": "1912.07557", "submitter": "Dan Schmidt", "authors": "Dan Schmidt, Nick Moran, Jonathan S. Rosenfeld, Jonathan Rosenthal,\n  Jonathan Yedidia", "title": "Self-Play Learning Without a Reward Metric", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AlphaZero algorithm for the learning of strategy games via self-play,\nwhich has produced superhuman ability in the games of Go, chess, and shogi,\nuses a quantitative reward function for game outcomes, requiring the users of\nthe algorithm to explicitly balance different components of the reward against\neach other, such as the game winner and margin of victory. We present a\nmodification to the AlphaZero algorithm that requires only a total ordering\nover game outcomes, obviating the need to perform any quantitative balancing of\nreward components. We demonstrate that this system learns optimal play in a\ncomparable amount of time to AlphaZero on a sample game.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:11:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Schmidt", "Dan", ""], ["Moran", "Nick", ""], ["Rosenfeld", "Jonathan S.", ""], ["Rosenthal", "Jonathan", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1912.07559", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Simon Osindero, Razvan Pascanu, Max\n  Jaderberg", "title": "A Deep Neural Network's Loss Surface Contains Every Low-dimensional\n  Pattern", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work \"Loss Landscape Sightseeing with Multi-Point Optimization\"\n(Skorokhodov and Burtsev, 2019) demonstrated that one can empirically find\narbitrary 2D binary patterns inside loss surfaces of popular neural networks.\nIn this paper we prove that: (i) this is a general property of deep universal\napproximators; and (ii) this property holds for arbitrary smooth patterns, for\nother dimensionalities, for every dataset, and any neural network that is\nsufficiently deep and wide. Our analysis predicts not only the existence of all\nsuch low-dimensional patterns, but also two other properties that were observed\nempirically: (i) that it is easy to find these patterns; and (ii) that they\ntransfer to other data-sets (e.g. a test-set).\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:15:18 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 17:40:38 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Osindero", "Simon", ""], ["Pascanu", "Razvan", ""], ["Jaderberg", "Max", ""]]}, {"id": "1912.07561", "submitter": "Grzegorz G{\\l}uch", "authors": "Grzegorz G{\\l}uch, R\\\"udiger Urbanke", "title": "Constructing a provably adversarially-robust classifier from a high\n  accuracy one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning models with very high accuracy have been shown to be\nvulnerable to small, adversarially chosen perturbations of the input. Given\nblack-box access to a high-accuracy classifier $f$, we show how to construct a\nnew classifier $g$ that has high accuracy and is also robust to adversarial\n$\\ell_2$-bounded perturbations. Our algorithm builds upon the framework of\n\\textit{randomized smoothing} that has been recently shown to outperform all\nprevious defenses against $\\ell_2$-bounded adversaries. Using techniques like\nrandom partitions and doubling dimension, we are able to bound the adversarial\nerror of $g$ in terms of the optimum error. In this paper we focus on our\nconceptual contribution, but we do present two examples to illustrate our\nframework. We will argue that, under some assumptions, our bounds are optimal\nfor these cases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:19:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Urbanke", "R\u00fcdiger", ""]]}, {"id": "1912.07578", "submitter": "Ali Shojaie", "authors": "Lina Lin, Mathias Drton and Ali Shojaie", "title": "Statistical significance in high-dimensional linear mixed models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the development of an inferential framework for\nhigh-dimensional linear mixed effect models. These are suitable models, for\ninstance, when we have $n$ repeated measurements for $M$ subjects. We consider\na scenario where the number of fixed effects $p$ is large (and may be larger\nthan $M$), but the number of random effects $q$ is small. Our framework is\ninspired by a recent line of work that proposes de-biasing penalized estimators\nto perform inference for high-dimensional linear models with fixed effects\nonly. In particular, we demonstrate how to correct a `naive' ridge estimator in\nextension of work by B\\\"uhlmann (2013) to build asymptotically valid confidence\nintervals for mixed effect models. We validate our theoretical results with\nnumerical experiments, in which we show our method outperforms those that fail\nto account for correlation induced by the random effects. For a practical\ndemonstration we consider a riboflavin production dataset that exhibits group\nstructure, and show that conclusions drawn using our method are consistent with\nthose obtained on a similar dataset without group structure.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:47:09 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lin", "Lina", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1912.07589", "submitter": "Paul Bertens", "authors": "Paul Bertens, Seong-Whan Lee", "title": "Network of Evolvable Neural Units: Evolving to Learn at a Synaptic Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep Neural Networks have seen great success in recent years through\nvarious changes in overall architectures and optimization strategies, their\nfundamental underlying design remains largely unchanged. Computational\nneuroscience on the other hand provides more biologically realistic models of\nneural processing mechanisms, but they are still high level abstractions of the\nactual experimentally observed behaviour. Here a model is proposed that bridges\nNeuroscience, Machine Learning and Evolutionary Algorithms to evolve individual\nsoma and synaptic compartment models of neurons in a scalable manner. Instead\nof attempting to manually derive models for all the observed complexity and\ndiversity in neural processing, we propose an Evolvable Neural Unit (ENU) that\ncan approximate the function of each individual neuron and synapse. We\ndemonstrate that this type of unit can be evolved to mimic Integrate-And-Fire\nneurons and synaptic Spike-Timing-Dependent Plasticity. Additionally, by\nconstructing a new type of neural network where each synapse and neuron is such\nan evolvable neural unit, we show it is possible to evolve an agent capable of\nlearning to solve a T-maze environment task. This network independently\ndiscovers spiking dynamics and reinforcement type learning rules, opening up a\nnew path towards biologically inspired artificial intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:57:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bertens", "Paul", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "1912.07602", "submitter": "Elvis Cui", "authors": "Elvis Cui, Heather Zhou", "title": "Projection pursuit with applications to scRNA sequencing data", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the limitations of PCA as a dimension reduction\ntechnique and study its extension, projection pursuit (PP), which is a broad\nclass of linear dimension reduction methods. We first discuss the relevant\nconcepts and theorems and then apply PCA and PP (with negative standardized\nShannon's entropy as the projection index) on single cell RNA sequencing data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:57:03 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Cui", "Elvis", ""], ["Zhou", "Heather", ""]]}, {"id": "1912.07618", "submitter": "Arjun Gupta", "authors": "Arjun Gupta, E. A. Huerta, Zhizhen Zhao and Issam Moussa", "title": "Deep Learning for Cardiologist-level Myocardial Infarction Detection in\n  Electrocardiograms", "comments": "Accepted to the European Medical and Biological Engineering\n  Conference (EMBEC) 2020", "journal-ref": null, "doi": "10.1007/978-3-030-64610-3_40", "report-no": null, "categories": "cs.LG eess.SP physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial infarction is the leading cause of death worldwide. In this paper,\nwe design domain-inspired neural network models to detect myocardial\ninfarction. First, we study the contribution of various leads. This systematic\nanalysis, first of its kind in the literature, indicates that out of 15 ECG\nleads, data from the v6, vz, and ii leads are critical to correctly identify\nmyocardial infarction. Second, we use this finding and adapt the ConvNetQuake\nneural network model--originally designed to identify earthquakes--to attain\nstate-of-the-art classification results for myocardial infarction, achieving\n$99.43\\%$ classification accuracy on a record-wise split, and $97.83\\%$\nclassification accuracy on a patient-wise split. These two results represent\ncardiologist-level performance level for myocardial infarction detection after\nfeeding only 10 seconds of raw ECG data into our model. Third, we show that our\nmulti-ECG-channel neural network achieves cardiologist-level performance\nwithout the need of any kind of manual feature extraction or data\npre-processing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:00:03 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 05:07:12 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 16:59:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gupta", "Arjun", ""], ["Huerta", "E. A.", ""], ["Zhao", "Zhizhen", ""], ["Moussa", "Issam", ""]]}, {"id": "1912.07629", "submitter": "Sitan Chen", "authors": "Sitan Chen, Jerry Li, Zhao Song", "title": "Learning Mixtures of Linear Regressions in Subexponential Time via\n  Fourier Moments", "comments": "83 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a mixture of linear regressions (MLRs).\nAn MLR is specified by $k$ nonnegative mixing weights $p_1, \\ldots, p_k$\nsumming to $1$, and $k$ unknown regressors $w_1,...,w_k\\in\\mathbb{R}^d$. A\nsample from the MLR is drawn by sampling $i$ with probability $p_i$, then\noutputting $(x, y)$ where $y = \\langle x, w_i \\rangle + \\eta$, where\n$\\eta\\sim\\mathcal{N}(0,\\varsigma^2)$ for noise rate $\\varsigma$. Mixtures of\nlinear regressions are a popular generative model and have been studied\nextensively in machine learning and theoretical computer science. However, all\nprevious algorithms for learning the parameters of an MLR require running time\nand sample complexity scaling exponentially with $k$.\n  In this paper, we give the first algorithm for learning an MLR that runs in\ntime which is sub-exponential in $k$. Specifically, we give an algorithm which\nruns in time $\\widetilde{O}(d)\\cdot\\exp(\\widetilde{O}(\\sqrt{k}))$ and outputs\nthe parameters of the MLR to high accuracy, even in the presence of nontrivial\nregression noise. We demonstrate a new method that we call \"Fourier moment\ndescent\" which uses univariate density estimation and low-degree moments of the\nFourier transform of suitable univariate projections of the MLR to iteratively\nrefine our estimate of the parameters. To the best of our knowledge, these\ntechniques have never been used in the context of high dimensional distribution\nlearning, and may be of independent interest. We also show that our techniques\ncan be used to give a sub-exponential time algorithm for learning mixtures of\nhyperplanes, a natural hard instance of the subspace clustering problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:00:19 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Jerry", ""], ["Song", "Zhao", ""]]}, {"id": "1912.07650", "submitter": "Alexander Hayes", "authors": "Alexander L. Hayes and Mayukh Das and Phillip Odom and Sriraam\n  Natarajan", "title": "User Friendly Automatic Construction of Background Knowledge: Mode\n  Construction from ER Diagrams", "comments": "8 pages. Published in Proceedings of the Knowledge Capture\n  Conference, 2017", "journal-ref": "Proceedings of the Knowledge Capture Conference (2017) 30:1-30:8", "doi": "10.1145/3148011.3148027", "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the key advantages of Inductive Logic Programming systems is the\nability of the domain experts to provide background knowledge as modes that\nallow for efficient search through the space of hypotheses. However, there is\nan inherent assumption that this expert should also be an ILP expert to provide\neffective modes. We relax this assumption by designing a graphical user\ninterface that allows the domain expert to interact with the system using\nEntity Relationship diagrams. These interactions are used to construct modes\nfor the learning system. We evaluate our algorithm on a probabilistic logic\nlearning system where we demonstrate that the user is able to construct\neffective background knowledge on par with the expert-encoded knowledge on five\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:30:57 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Hayes", "Alexander L.", ""], ["Das", "Mayukh", ""], ["Odom", "Phillip", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1912.07651", "submitter": "Arash Vahdat", "authors": "Arash Vahdat, Arun Mallya, Ming-Yu Liu, Jan Kautz", "title": "UNAS: Differentiable Architecture Search Meets Reinforcement Learning", "comments": "Accepted to CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) aims to discover network architectures with\ndesired properties such as high accuracy or low latency. Recently,\ndifferentiable NAS (DNAS) has demonstrated promising results while maintaining\na search cost orders of magnitude lower than reinforcement learning (RL) based\nNAS. However, DNAS models can only optimize differentiable loss functions in\nsearch, and they require an accurate differentiable approximation of\nnon-differentiable criteria. In this work, we present UNAS, a unified framework\nfor NAS, that encapsulates recent DNAS and RL-based approaches under one\nframework. Our framework brings the best of both worlds, and it enables us to\nsearch for architectures with both differentiable and non-differentiable\ncriteria in one unified framework while maintaining a low search cost. Further,\nwe introduce a new objective function for search based on the generalization\ngap that prevents the selection of architectures prone to overfitting. We\npresent extensive experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets\nand we perform search in two fundamentally different search spaces. We show\nthat UNAS obtains the state-of-the-art average accuracy on all three datasets\nwhen compared to the architectures searched in the DARTS space. Moreover, we\nshow that UNAS can find an efficient and accurate architecture in the\nProxylessNAS search space, that outperforms existing MobileNetV2 based\narchitectures. The source code is available at https://github.com/NVlabs/unas .\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:31:39 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 21:48:42 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Vahdat", "Arash", ""], ["Mallya", "Arun", ""], ["Liu", "Ming-Yu", ""], ["Kautz", "Jan", ""]]}, {"id": "1912.07661", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Arunachalam Narayanaswamy, Samuel Yang, Anton\n  Geraschenko, Scott Lipnick, Nina Makhortova, James Hawrot, Christine Marques,\n  Joao Pereira, Michael Brenner, Lee Rubin, Brian Wainger, Marc Berndl", "title": "It's easy to fool yourself: Case studies on identifying bias and\n  confounding in bio-medical datasets", "comments": "Accepted at Neurips 2019 LMRL workshop -- extended abstract track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confounding variables are a well known source of nuisance in biomedical\nstudies. They present an even greater challenge when we combine them with\nblack-box machine learning techniques that operate on raw data. This work\npresents two case studies. In one, we discovered biases arising from systematic\nerrors in the data generation process. In the other, we found a spurious source\nof signal unrelated to the prediction task at hand. In both cases, our\nprediction models performed well but under careful examination hidden\nconfounders and biases were revealed. These are cautionary tales on the limits\nof using machine learning techniques on raw data from scientific experiments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:45:25 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 18:46:58 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Narayanaswamy", "Arunachalam", ""], ["Yang", "Samuel", ""], ["Geraschenko", "Anton", ""], ["Lipnick", "Scott", ""], ["Makhortova", "Nina", ""], ["Hawrot", "James", ""], ["Marques", "Christine", ""], ["Pereira", "Joao", ""], ["Brenner", "Michael", ""], ["Rubin", "Lee", ""], ["Wainger", "Brian", ""], ["Berndl", "Marc", ""]]}, {"id": "1912.07662", "submitter": "Alessio Pagani Dr", "authors": "Alessio Pagani, Abhinav Mehrotra and Mirco Musolesi", "title": "Graph Input Representations for Machine Learning Applications in Urban\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and learning the characteristics of network paths has been of\nparticular interest for decades and has led to several successful applications.\nSuch analysis becomes challenging for urban networks as their size and\ncomplexity are significantly higher compared to other networks. The\nstate-of-the-art machine learning (ML) techniques allow us to detect hidden\npatterns and, thus, infer the features associated with them. However, very\nlittle is known about the impact on the performance of such predictive models\nby the use of different input representations. In this paper, we design and\nevaluate six different graph input representations (i.e., representations of\nthe network paths), by considering the network's topological and temporal\ncharacteristics, for being used as inputs for machine learning models to learn\nthe behavior of urban networks paths. The representations are validated and\nthen tested with a real-world taxi journeys dataset predicting the tips using a\nroad network of New York. Our results demonstrate that the input\nrepresentations that use temporal information help the model to achieve the\nhighest accuracy (RMSE of 1.42$).\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:28:00 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Pagani", "Alessio", ""], ["Mehrotra", "Abhinav", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1912.07663", "submitter": "Haoxing Lin", "authors": "Haoxing Lin, Weijia Jia, Yiping Sun, Yongjian You", "title": "Spatial-Temporal Self-Attention Network for Flow Prediction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow prediction (e.g., crowd flow, traffic flow) with features of\nspatial-temporal is increasingly investigated in AI research field. It is very\nchallenging due to the complicated spatial dependencies between different\nlocations and dynamic temporal dependencies among different time intervals.\nAlthough measurements of both dependencies are employed, existing methods\nsuffer from the following two problems. First, the temporal dependencies are\nmeasured either uniformly or bias against long-term dependencies, which\noverlooks the distinctive impacts of short-term and long-term temporal\ndependencies. Second, the existing methods capture spatial and temporal\ndependencies independently, which wrongly assumes that the correlations between\nthese dependencies are weak and ignores the complicated mutual influences\nbetween them. To address these issues, we propose a Spatial-Temporal\nSelf-Attention Network (ST-SAN). As the path-length of attending long-term\ndependency is shorter in the self-attention mechanism, the vanishing of\nlong-term temporal dependencies is prevented. In addition, since our model\nrelies solely on attention mechanisms, the spatial and temporal dependencies\ncan be simultaneously measured. Experimental results on real-world data\ndemonstrate that, in comparison with state-of-the-art methods, our model\nreduces the root mean square errors by 9% in inflow prediction and 4% in\noutflow prediction on Taxi-NYC data, which is very significant compared to the\nprevious improvement.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:42:35 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 02:02:09 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Lin", "Haoxing", ""], ["Jia", "Weijia", ""], ["Sun", "Yiping", ""], ["You", "Yongjian", ""]]}, {"id": "1912.07685", "submitter": "Benedikt Boecking", "authors": "Benedikt Boecking and Artur Dubrawski", "title": "Pairwise Feedback for Data Programming", "comments": "Presented at the NeurIPS 2019 workshop on Learning with Rich\n  Experience: Integration of Learning Paradigms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of the labeling process and the attainable quality of labels\nhave become limiting factors for many applications of machine learning. The\nprogrammatic creation of labeled datasets via the synthesis of noisy heuristics\nprovides a promising avenue to address this problem. We propose to improve\nmodeling of latent class variables in the programmatic creation of labeled\ndatasets by incorporating pairwise feedback into the process. We discuss the\nease with which such pairwise feedback can be obtained or generated in many\napplication domains. Our experiments show that even a small number of sources\nof pairwise feedback can substantially improve the quality of the posterior\nestimate of the latent class variable.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:24:45 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Boecking", "Benedikt", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1912.07721", "submitter": "David DeFazio", "authors": "David DeFazio and Arti Ramesh", "title": "Adversarial Model Extraction on Graph Neural Networks", "comments": "AAAI Workshop on Deep Learning on Graphs: Methodologies and\n  Applications (DLGMA), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the advent of deep neural networks came various methods of\nexploitation, such as fooling the classifier or contaminating its training\ndata. Another such attack is known as model extraction, where provided API\naccess to some black box neural network, the adversary extracts the underlying\nmodel. This is done by querying the model in such a way that the underlying\nneural network provides enough information to the adversary to be\nreconstructed. While several works have achieved impressive results with neural\nnetwork extraction in the propositional domain, this problem has not yet been\nconsidered over the relational domain, where data samples are no longer\nconsidered to be independent and identically distributed (iid). Graph Neural\nNetworks (GNNs) are a popular deep learning framework to perform machine\nlearning tasks over relational data. In this work, we formalize an instance of\nGNN extraction, present a solution with preliminary results, and discuss our\nassumptions and future directions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 21:54:21 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["DeFazio", "David", ""], ["Ramesh", "Arti", ""]]}, {"id": "1912.07729", "submitter": "Charlie Frogner", "authors": "Charlie Frogner, Sebastian Claici, Edward Chien, Justin Solomon", "title": "Incorporating Unlabeled Data into Distributionally Robust Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a robust alternative to empirical risk minimization called\ndistributionally robust learning (DRL), in which one learns to perform against\nan adversary who can choose the data distribution from a specified set of\ndistributions. We illustrate a problem with current DRL formulations, which\nrely on an overly broad definition of allowed distributions for the adversary,\nleading to learned classifiers that are unable to predict with any confidence.\nWe propose a solution that incorporates unlabeled data into the DRL problem to\nfurther constrain the adversary. We show that this new formulation is tractable\nfor stochastic gradient-based optimization and yields a computable guarantee on\nthe future performance of the learned classifier, analogous to -- but tighter\nthan -- guarantees from conventional DRL. We examine the performance of this\nnew formulation on 14 real datasets and find that it often yields effective\nclassifiers with nontrivial performance guarantees in situations where\nconventional DRL produces neither. Inspired by these results, we extend our DRL\nformulation to active learning with a novel, distributionally-robust version of\nthe standard model-change heuristic. Our active learning algorithm often\nachieves superior learning performance to the original heuristic on real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:13:19 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 01:55:20 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Frogner", "Charlie", ""], ["Claici", "Sebastian", ""], ["Chien", "Edward", ""], ["Solomon", "Justin", ""]]}, {"id": "1912.07730", "submitter": "Gautam Krishna", "authors": "Gautam Krishna, Mason Carnahan, Co Tran, Ahmed H Tewfik", "title": "Continuous Speech Recognition using EEG and Video", "comments": "On preparation for submission to EUSIPCO 2020. arXiv admin note: text\n  overlap with arXiv:1911.11610, arXiv:1911.04261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate whether electroencephalography (EEG) features\ncan be used to improve the performance of continuous visual speech recognition\nsystems. We implemented a connectionist temporal classification (CTC) based\nend-to-end automatic speech recognition (ASR) model for performing recognition.\nOur results demonstrate that EEG features are helpful in enhancing the\nperformance of continuous visual speech recognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:16:19 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 04:34:45 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 04:01:48 GMT"}, {"version": "v4", "created": "Tue, 24 Dec 2019 06:39:00 GMT"}, {"version": "v5", "created": "Fri, 27 Dec 2019 22:47:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Krishna", "Gautam", ""], ["Carnahan", "Mason", ""], ["Tran", "Co", ""], ["Tewfik", "Ahmed H", ""]]}, {"id": "1912.07737", "submitter": "Djordje Slijepcevic", "authors": "Djordje Slijepcevic, Fabian Horst, Sebastian Lapuschkin, Anna-Maria\n  Raberger, Matthias Zeppelzauer, Wojciech Samek, Christian Breiteneder,\n  Wolfgang I. Sch\\\"ollhorn, Brian Horsak", "title": "On the Explanation of Machine Learning Predictions in Clinical Gait\n  Analysis", "comments": "37 pages, 7 figures, 2 tables, 24 supplementary figures, 1\n  supplementary table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) is increasingly used to support decision-making in the\nhealthcare sector. While ML approaches provide promising results with regard to\ntheir classification performance, most share a central limitation, namely their\nblack-box character. Motivated by the interest to understand the functioning of\nML models, methods from the field of Explainable Artificial Intelligence (XAI)\nhave recently become important. This article investigates the usefulness of XAI\nmethods in clinical gait classification. For this purpose, predictions of\nstate-of-the-art classification methods are explained with an established XAI\nmethod, i.e., Layer-wise Relevance Propagation (LRP). We propose to evaluate\nthe obtained explanations with two complementary approaches: a statistical\nanalysis of the underlying data using Statistical Parametric Mapping and a\nqualitative evaluation by a clinical expert. A gait dataset comprising ground\nreaction force measurements from 132 patients with different lower-body gait\ndisorders and 62 healthy controls is utilized. We investigate several gait\nclassification tasks, employ multiple classification methods, and analyze the\nimpact of data normalization and different signal components for classification\nperformance and explanation quality. Our experiments show that explanations\nobtained by LRP exhibit promising statistical properties concerning inter-class\ndiscriminativity and are also in line with clinically relevant biomechanical\ngait characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 22:34:06 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 12:20:32 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Slijepcevic", "Djordje", ""], ["Horst", "Fabian", ""], ["Lapuschkin", "Sebastian", ""], ["Raberger", "Anna-Maria", ""], ["Zeppelzauer", "Matthias", ""], ["Samek", "Wojciech", ""], ["Breiteneder", "Christian", ""], ["Sch\u00f6llhorn", "Wolfgang I.", ""], ["Horsak", "Brian", ""]]}, {"id": "1912.07748", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo\n  Bremer", "title": "MimicGAN: Robust Projection onto Image Manifolds with Corruption\n  Mimicking", "comments": "International Journal on Computer Vision's (IJCV) Special Issue on\n  GANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, Generative Adversarial Networks (GANs) have\ndramatically advanced our ability to represent and parameterize\nhigh-dimensional, non-linear image manifolds. As a result, they have been\nwidely adopted across a variety of applications, ranging from challenging\ninverse problems like image completion, to problems such as anomaly detection\nand adversarial defense. A recurring theme in many of these applications is the\nnotion of projecting an image observation onto the manifold that is inferred by\nthe generator. In this context, Projected Gradient Descent (PGD) has been the\nmost popular approach, which essentially optimizes for a latent vector that\nminimizes the discrepancy between a generated image and the given observation.\nHowever, PGD is a brittle optimization technique that fails to identify the\nright projection (or latent vector) when the observation is corrupted, or\nperturbed even by a small amount. Such corruptions are common in the real\nworld, for example images in the wild come with unknown crops, rotations,\nmissing pixels, or other kinds of non-linear distributional shifts which break\ncurrent encoding methods, rendering downstream applications unusable. To\naddress this, we propose corruption mimicking -- a new robust projection\ntechnique, that utilizes a surrogate network to approximate the unknown\ncorruption directly at test time, without the need for additional supervision\nor data augmentation. The proposed method is significantly more robust than PGD\nand other competing methods under a wide variety of corruptions, thereby\nenabling a more effective use of GANs in real-world applications. More\nimportantly, we show that our approach produces state-of-the-art performance in\nseveral GAN-based applications -- anomaly detection, domain adaptation, and\nadversarial defense, that benefit from an accurate projection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:14:56 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 19:18:41 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 17:21:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Kailkhura", "Bhavya", ""], ["Bremer", "Timo", ""]]}, {"id": "1912.07756", "submitter": "Gianluca Maguolo", "authors": "Loris Nanni, Gianluca Maguolo, Michelangelo Paci", "title": "Data augmentation approaches for improving animal audio classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present ensembles of classifiers for automated animal audio\nclassification, exploiting different data augmentation techniques for training\nConvolutional Neural Networks (CNNs). The specific animal audio classification\nproblems are i) birds and ii) cat sounds, whose datasets are freely available.\nWe train five different CNNs on the original datasets and on their versions\naugmented by four augmentation protocols, working on the raw audio signals or\ntheir representations as spectrograms. We compared our best approaches with the\nstate of the art, showing that we obtain the best recognition rate on the same\ndatasets, without ad hoc parameter optimization. Our study shows that different\nCNNs can be trained for the purpose of animal audio classification and that\ntheir fusion works better than the stand-alone classifiers. To the best of our\nknowledge this is the largest study on data augmentation for CNNs in animal\naudio classification audio datasets using the same set of classifiers and\nparameters. Our MATLAB code is available at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:30:42 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 20:19:52 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""], ["Paci", "Michelangelo", ""]]}, {"id": "1912.07768", "submitter": "Felipe Petroski Such", "authors": "Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O. Stanley,\n  Jeff Clune", "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by\n  Learning to Generate Synthetic Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the intriguing question of whether we can create\nlearning algorithms that automatically generate training data, learning\nenvironments, and curricula in order to help AI agents rapidly learn. We show\nthat such algorithms are possible via Generative Teaching Networks (GTNs), a\ngeneral approach that is, in theory, applicable to supervised, unsupervised,\nand reinforcement learning, although our experiments only focus on the\nsupervised case. GTNs are deep neural networks that generate data and/or\ntraining environments that a learner (e.g. a freshly initialized neural\nnetwork) trains on for a few SGD steps before being tested on a target task. We\nthen differentiate through the entire learning process via meta-gradients to\nupdate the GTN parameters to improve performance on the target task. GTNs have\nthe beneficial property that they can theoretically generate any type of data\nor training environment, making their potential impact large. This paper\nintroduces GTNs, discusses their potential, and showcases that they can\nsubstantially accelerate learning. We also demonstrate a practical and exciting\napplication of GTNs: accelerating the evaluation of candidate architectures for\nneural architecture search (NAS), which is rate-limited by such evaluations,\nenabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art,\nfinding higher performing architectures when controlling for the search\nproposal mechanism. GTN-NAS also is competitive with the overall state of the\nart approaches, which achieve top performance while using orders of magnitude\nless computation than typical NAS methods. Speculating forward, GTNs may\nrepresent a first step toward the ambitious goal of algorithms that generate\ntheir own training data and, in doing so, open a variety of interesting new\nresearch questions and directions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 00:57:50 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Such", "Felipe Petroski", ""], ["Rawal", "Aditya", ""], ["Lehman", "Joel", ""], ["Stanley", "Kenneth O.", ""], ["Clune", "Jeff", ""]]}, {"id": "1912.07776", "submitter": "Zeyu Deng", "authors": "Zeyu Deng, Lihui Wang, Zixiang Kuai, Qijian Chen, Xinyu Cheng, Feng\n  Yang, Jie Yang, Yuemin Zhu", "title": "CNN-Based Invertible Wavelet Scattering for the Investigation of\n  Diffusion Properties of the In Vivo Human Heart in Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vivo diffusion tensor imaging (DTI) is a promising technique to\ninvestigate noninvasively the fiber structures of the in vivo human heart.\nHowever, signal loss due to motions remains a persistent problem in in vivo\ncardiac DTI. We propose a novel motion-compensation method for investigating in\nvivo myocardium structures in DTI with free-breathing acquisitions. The method\nis based on an invertible Wavelet Scattering achieved by means of Convolutional\nNeural Network (WSCNN). It consists of first extracting translation-invariant\nwavelet scattering features from DW images acquired at different trigger delays\nand then mapping the fused scattering features into motion-compensated spatial\nDW images by performing an inverse wavelet scattering transform achieved using\nCNN. The results on both simulated and acquired in vivo cardiac DW images\nshowed that the proposed WSCNN method effectively compensates for\nmotion-induced signal loss and produces in vivo cardiac DW images with better\nquality and more coherent fiber structures with respect to existing methods,\nwhich makes it an interesting method for measuring correctly the diffusion\nproperties of the in vivo human heart in DTI under free breathing.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:26:06 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Deng", "Zeyu", ""], ["Wang", "Lihui", ""], ["Kuai", "Zixiang", ""], ["Chen", "Qijian", ""], ["Cheng", "Xinyu", ""], ["Yang", "Feng", ""], ["Yang", "Jie", ""], ["Zhu", "Yuemin", ""]]}, {"id": "1912.07800", "submitter": "Ethan Chi", "authors": "Bowen Jing, Ethan A. Chi, Jillian Tang", "title": "SGVAE: Sequential Graph Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models of graphs are well-known, but many existing models are\nlimited in scalability and expressivity. We present a novel sequential\ngraphical variational autoencoder operating directly on graphical\nrepresentations of data. In our model, the encoding and decoding of a graph as\nis framed as a sequential deconstruction and construction process,\nrespectively, enabling the the learning of a latent space. Experiments on a\ncycle dataset show promise, but highlight the need for a relaxation of the\ndistribution over node permutations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 03:19:47 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jing", "Bowen", ""], ["Chi", "Ethan A.", ""], ["Tang", "Jillian", ""]]}, {"id": "1912.07812", "submitter": "Guangyi Zhang", "authors": "Guangyi Zhang and Ali Etemad", "title": "Capsule Attention for Multimodal EEG-EOG Representation Learning with\n  Application to Driver Vigilance Estimation", "comments": "Accepted by IEEE Transactions on Neural Systems and Rehabilitation\n  Engineering", "journal-ref": null, "doi": "10.1109/TNSRE.2021.3089594", "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver vigilance estimation is an important task for transportation safety.\nWearable and portable brain-computer interface devices provide a powerful means\nfor real-time monitoring of the vigilance level of drivers to help with\navoiding distracted or impaired driving. In this paper, we propose a novel\nmultimodal architecture for in-vehicle vigilance estimation from\nElectroencephalogram and Electrooculogram. To enable the system to focus on the\nmost salient parts of the learned multimodal representations, we propose an\narchitecture composed of a capsule attention mechanism following a deep Long\nShort-Term Memory (LSTM) network. Our model learns hierarchical dependencies in\nthe data through the LSTM and capsule feature representation layers. To better\nexplore the discriminative ability of the learned representations, we study the\neffect of the proposed capsule attention mechanism including the number of\ndynamic routing iterations as well as other parameters. Experiments show the\nrobustness of our method by outperforming other solutions and baseline\ntechniques, setting a new state-of-the-art. We then provide an analysis on\ndifferent frequency bands and brain regions to evaluate their suitability for\ndriver vigilance estimation. Lastly, an analysis on the role of capsule\nattention, multimodality, and robustness to noise is performed, highlighting\nthe advantages of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 04:20:08 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 19:59:07 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 16:31:51 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 16:14:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Guangyi", ""], ["Etemad", "Ali", ""]]}, {"id": "1912.07814", "submitter": "Fahimeh Bahmaninezhad", "authors": "Fahimeh Bahmaninezhad, Shi-Xiong Zhang, Yong Xu, Meng Yu, John H.L.\n  Hansen, Dong Yu", "title": "A Unified Framework for Speech Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech separation refers to extracting each individual speech source in a\ngiven mixed signal. Recent advancements in speech separation and ongoing\nresearch in this area, have made these approaches as promising techniques for\npre-processing of naturalistic audio streams. After incorporating deep learning\ntechniques into speech separation, performance on these systems is improving\nfaster. The initial solutions introduced for deep learning based speech\nseparation analyzed the speech signals into time-frequency domain with STFT;\nand then encoded mixed signals were fed into a deep neural network based\nseparator. Most recently, new methods are introduced to separate waveform of\nthe mixed signal directly without analyzing them using STFT. Here, we introduce\na unified framework to include both spectrogram and waveform separations into a\nsingle structure, while being only different in the kernel function used to\nencode and decode the data; where, both can achieve competitive performance.\nThis new framework provides flexibility; in addition, depending on the\ncharacteristics of the data, or limitations of the memory and latency can set\nthe hyper-parameters to flow in a pipeline of the framework which fits the task\nproperly. We extend single-channel speech separation into multi-channel\nframework with end-to-end training of the network while optimizing the speech\nseparation criterion (i.e., Si-SNR) directly. We emphasize on how tied kernel\nfunctions for calculating spatial features, encoder, and decoder in\nmulti-channel framework can be effective. We simulate spatialized reverberate\ndata for both WSJ0 and LibriSpeech corpora here, and while these two sets of\ndata are different in the matter of size and duration, the effect of capturing\nshorter and longer dependencies of previous/+future samples are studied in\ndetail. We report SDR, Si-SNR and PESQ to evaluate the performance of developed\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 04:21:03 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Bahmaninezhad", "Fahimeh", ""], ["Zhang", "Shi-Xiong", ""], ["Xu", "Yong", ""], ["Yu", "Meng", ""], ["Hansen", "John H. L.", ""], ["Yu", "Dong", ""]]}, {"id": "1912.07819", "submitter": "Jiantao Wu", "authors": "JT Wu and L.Wang", "title": "Angular Learning: Toward Discriminative Embedded Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The margin-based softmax loss functions greatly enhance intra-class\ncompactness and perform well on the tasks of face recognition and object\nclassification. Outperformance, however, depends on the careful hyperparameter\nselection. Moreover, the hard angle restriction also increases the risk of\noverfitting. In this paper, angular loss suggested by maximizing the angular\ngradient to promote intra-class compactness avoids overfitting. Besides, our\nmethod has only one adjustable constant for intra-class compactness control. We\ndefine three metrics to measure inter-class separability and intra-class\ncompactness. In experiments, we test our method, as well as other methods, on\nmany well-known datasets. Experimental results reveal that our method has the\nsuperiority of accuracy improvement, discriminative information, and\ntime-consumption.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:08:20 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Wu", "JT", ""], ["Wang", "L.", ""]]}, {"id": "1912.07820", "submitter": "Sainyam Galhotra Mr", "authors": "Sandhya Saisubramanian, Sainyam Galhotra and Shlomo Zilberstein", "title": "Balancing the Tradeoff Between Clustering Value and Interpretability", "comments": "Accepted at AIES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph clustering groups entities -- the vertices of a graph -- based on their\nsimilarity, typically using a complex distance function over a large number of\nfeatures. Successful integration of clustering approaches in automated\ndecision-support systems hinges on the interpretability of the resulting\nclusters. This paper addresses the problem of generating interpretable\nclusters, given features of interest that signify interpretability to an\nend-user, by optimizing interpretability in addition to common clustering\nobjectives. We propose a $\\beta$-interpretable clustering algorithm that\nensures that at least $\\beta$ fraction of nodes in each cluster share the same\nfeature value. The tunable parameter $\\beta$ is user-specified. We also present\na more efficient algorithm for scenarios with $\\beta\\!=\\!1$ and analyze the\ntheoretical guarantees of the two algorithms. Finally, we empirically\ndemonstrate the benefits of our approaches in generating interpretable clusters\nusing four real-world datasets. The interpretability of the clusters is\ncomplemented by generating simple explanations denoting the feature values of\nthe nodes in the clusters, using frequent pattern mining.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:08:34 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 03:24:54 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 00:17:28 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Saisubramanian", "Sandhya", ""], ["Galhotra", "Sainyam", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1912.07832", "submitter": "Yu Chen", "authors": "Yu Chen, Lingfei Wu and Mohammed J. Zaki", "title": "Deep Iterative and Adaptive Learning for Graph Neural Networks", "comments": "6 pages. Accepted at the AAAI 2020 Workshop on Deep Learning on\n  Graphs: Methodologies and Applications (AAAI DLGMA 2020). Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end graph learning framework, namely Deep\nIterative and Adaptive Learning for Graph Neural Networks (DIAL-GNN), for\njointly learning the graph structure and graph embeddings simultaneously. We\nfirst cast the graph structure learning problem as a similarity metric learning\nproblem and leverage an adapted graph regularization for controlling\nsmoothness, connectivity and sparsity of the generated graph. We further\npropose a novel iterative method for searching for a hidden graph structure\nthat augments the initial graph structure. Our iterative method dynamically\nstops when the learned graph structure approaches close enough to the optimal\ngraph. Our extensive experiments demonstrate that the proposed DIAL-GNN model\ncan consistently outperform or match state-of-the-art baselines in terms of\nboth downstream task performance and computational time. The proposed approach\ncan cope with both transductive learning and inductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 06:02:59 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chen", "Yu", ""], ["Wu", "Lingfei", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1912.07882", "submitter": "Yiming Gu", "authors": "Donsuk Lee, Yiming Gu, Jerrick Hoang, Micol Marchetti-Bowick", "title": "Joint Interaction and Trajectory Prediction for Autonomous Driving using\n  Graph Neural Networks", "comments": "Accepted in Machine Learning for Autonomous Driving NeurIPS 2019\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to predict the future motion of vehicles in a traffic\nscene by explicitly modeling their pairwise interactions. Specifically, we\npropose a graph neural network that jointly predicts the discrete interaction\nmodes and 5-second future trajectories for all agents in the scene. Our model\ninfers an interaction graph whose nodes are agents and whose edges capture the\nlong-term interaction intents among the agents. In order to train the model to\nrecognize known modes of interaction, we introduce an auto-labeling function to\ngenerate ground truth interaction labels. Using a large-scale real-world\ndriving dataset, we demonstrate that jointly predicting the trajectories along\nwith the explicit interaction types leads to significantly lower trajectory\nerror than baseline methods. Finally, we show through simulation studies that\nthe learned interaction modes are semantically meaningful.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 09:06:31 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Lee", "Donsuk", ""], ["Gu", "Yiming", ""], ["Hoang", "Jerrick", ""], ["Marchetti-Bowick", "Micol", ""]]}, {"id": "1912.07902", "submitter": "Yanan Li", "authors": "Yanan Li, Shusen Yang, Xuebin Ren, Cong Zhao", "title": "Asynchronous Federated Learning with Differential Privacy for Edge\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has been showing as a promising approach in paving the\nlast mile of artificial intelligence, due to its great potential of solving the\ndata isolation problem in large scale machine learning. Particularly, with\nconsideration of the heterogeneity in practical edge computing systems,\nasynchronous edge-cloud collaboration based federated learning can further\nimprove the learning efficiency by significantly reducing the straggler effect.\nDespite no raw data sharing, the open architecture and extensive collaborations\nof asynchronous federated learning (AFL) still give some malicious participants\ngreat opportunities to infer other parties' training data, thus leading to\nserious concerns of privacy. To achieve a rigorous privacy guarantee with high\nutility, we investigate to secure asynchronous edge-cloud collaborative\nfederated learning with differential privacy, focusing on the impacts of\ndifferential privacy on model convergence of AFL. Formally, we give the first\nanalysis on the model convergence of AFL under DP and propose a multi-stage\nadjustable private algorithm (MAPA) to improve the trade-off between model\nutility and privacy by dynamically adjusting both the noise scale and the\nlearning rate. Through extensive simulations and real-world experiments with an\nedge-could testbed, we demonstrate that MAPA significantly improves both the\nmodel accuracy and convergence speed with sufficient privacy guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 09:49:38 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Li", "Yanan", ""], ["Yang", "Shusen", ""], ["Ren", "Xuebin", ""], ["Zhao", "Cong", ""]]}, {"id": "1912.07911", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Jing Chen, Haonan Sun, Keyang Xu", "title": "A Heterogeneous Graphical Model to Understand User-Level Sentiments in\n  Social Media", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Media has seen a tremendous growth in the last decade and is\ncontinuing to grow at a rapid pace. With such adoption, it is increasingly\nbecoming a rich source of data for opinion mining and sentiment analysis. The\ndetection and analysis of sentiment in social media is thus a valuable topic\nand attracts a lot of research efforts. Most of the earlier efforts focus on\nsupervised learning approaches to solve this problem, which require expensive\nhuman annotations and therefore limits their practical use. In our work, we\npropose a semi-supervised approach to predict user-level sentiments for\nspecific topics. We define and utilize a heterogeneous graph built from the\nsocial networks of the users with the knowledge that connected users in social\nnetworks typically share similar sentiments. Compared with the previous works,\nwe have several novelties: (1) we incorporate the influences/authoritativeness\nof the users into the model, 2) we include comment-based and like-based\nuser-user links to the graph, 3) we superimpose multiple heterogeneous graphs\ninto one thereby allowing multiple types of links to exist between two users.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:29:26 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Chen", "Jing", ""], ["Sun", "Haonan", ""], ["Xu", "Keyang", ""]]}, {"id": "1912.07913", "submitter": "Anthony Nouy", "authors": "Erwan Grelier and Anthony Nouy and R\\'egis Lebrun", "title": "Learning high-dimensional probability distributions using tree tensor\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the estimation of a high-dimensional probability\ndistribution from i.i.d. samples of the distribution using model classes of\nfunctions in tree-based tensor formats, a particular case of tensor networks\nassociated with a dimension partition tree. The distribution is assumed to\nadmit a density with respect to a product measure, possibly discrete for\nhandling the case of discrete random variables.\n  After discussing the representation of classical model classes in tree-based\ntensor formats, we present learning algorithms based on empirical risk\nminimization using a $L^2$ contrast.\n  These algorithms exploit the multilinear parametrization of the formats to\nrecast the nonlinear minimization problem into a sequence of empirical risk\nminimization problems with linear models. A suitable parametrization of the\ntensor in tree-based tensor format allows to obtain a linear model with\northogonal bases, so that each problem admits an explicit expression of the\nsolution and cross-validation risk estimates. These estimations of the risk\nenable the model selection, for instance when exploiting sparsity in the\ncoefficients of the representation.\n  A strategy for the adaptation of the tensor format (dimension tree and\ntree-based ranks) is provided, which allows to discover and exploit some\nspecific structures of high-dimensional probability distributions such as\nindependence or conditional independence.\n  We illustrate the performances of the proposed algorithms for the\napproximation of classical probabilistic models (such as Gaussian distribution,\ngraphical models, Markov chain).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:31:16 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 14:39:59 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 17:21:11 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Grelier", "Erwan", ""], ["Nouy", "Anthony", ""], ["Lebrun", "R\u00e9gis", ""]]}, {"id": "1912.07938", "submitter": "Galit Shmueli", "authors": "Travis Greene and Galit Shmueli", "title": "How Personal is Machine Learning Personalization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though used extensively, the concept and process of machine learning (ML)\npersonalization have generally received little attention from academics,\npractitioners, and the general public. We describe the ML approach as relying\non the metaphor of the person as a feature vector and contrast this with\nhumanistic views of the person. In light of the recent calls by the IEEE to\nconsider the effects of ML on human well-being, we ask whether ML\npersonalization can be reconciled with these humanistic views of the person,\nwhich highlight the importance of moral and social identity. As human behavior\nincreasingly becomes digitized, analyzed, and predicted, to what extent do our\nsubsequent decisions about what to choose, buy, or do, made both by us and\nothers, reflect who we are as persons? This paper first explicates the term\npersonalization by considering ML personalization and highlights its relation\nto humanistic conceptions of the person, then proposes several dimensions for\nevaluating the degree of personalization of ML personalized scores. By doing\nso, we hope to contribute to current debate on the issues of algorithmic bias,\ntransparency, and fairness in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:37:19 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 03:39:46 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Greene", "Travis", ""], ["Shmueli", "Galit", ""]]}, {"id": "1912.07942", "submitter": "Santiago Zanella-Beguelin", "authors": "Marc Brockschmidt, Boris K\\\"opf, Olga Ohrimenko, Andrew Paverd, Victor\n  R\\\"uhle, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B\\'eguelin", "title": "Analyzing Information Leakage of Updates to Natural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---differential score and\ndifferential rank---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:46:08 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 13:28:16 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 21:41:35 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Brockschmidt", "Marc", ""], ["K\u00f6pf", "Boris", ""], ["Ohrimenko", "Olga", ""], ["Paverd", "Andrew", ""], ["R\u00fchle", "Victor", ""], ["Tople", "Shruti", ""], ["Wutschitz", "Lukas", ""], ["Zanella-B\u00e9guelin", "Santiago", ""]]}, {"id": "1912.07946", "submitter": "Giuseppe Antonio Di Luna", "authors": "Fiorella Artuso, Giuseppe Antonio Di Luna, Luca Massarelli and\n  Leonardo Querzoni", "title": "In Nomine Function: Naming Functions in Stripped Binaries with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of automatically naming pieces of\nassembly code. Where by naming we mean assigning to an assembly function a\nstring of words that would likely be assigned by a human reverse engineer. We\nformally and precisely define the framework in which our investigation takes\nplace. That is we define the problem, we provide reasonable justifications for\nthe choices that we made for the design of training and the tests. We performed\nan analysis on a large real-world corpora constituted by nearly 9 millions of\nfunctions taken from more than 22k softwares. In such framework we test\nbaselines coming from the field of Natural Language Processing (e.g., Seq2Seq\nnetworks and Transformer). Interestingly, our evaluation shows promising\nresults beating the state-of-the-art and reaching good performance. We\ninvestigate the applicability of tine-tuning (i.e., taking a model already\ntrained on a large generic corpora and retraining it for a specific task). Such\ntechnique is popular and well-known in the NLP field. Our results confirm that\nfine-tuning is effective even when neural networks are applied to binaries. We\nshow that a model, pre-trained on the aforementioned corpora, when fine-tuned\nhas higher performances on specific domains (such as predicting names in system\nutilites, malware, etc).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:59:41 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 16:17:59 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 09:31:56 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Artuso", "Fiorella", ""], ["Di Luna", "Giuseppe Antonio", ""], ["Massarelli", "Luca", ""], ["Querzoni", "Leonardo", ""]]}, {"id": "1912.07991", "submitter": "Yatin Dandi", "authors": "Yatin Dandi, Aniket Das, Soumye Singhal, Vinay P. Namboodiri, Piyush\n  Rai", "title": "Jointly Trained Image and Video Generation using Residual Vectors", "comments": "Accepted in 2020 Winter Conference on Applications of Computer Vision\n  (WACV '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a modeling technique for jointly training image and\nvideo generation models by simultaneously learning to map latent variables with\na fixed prior onto real images and interpolate over images to generate videos.\nThe proposed approach models the variations in representations using residual\nvectors encoding the change at each time step over a summary vector for the\nentire video. We utilize the technique to jointly train an image generation\nmodel with a fixed prior along with a video generation model lacking\nconstraints such as disentanglement. The joint training enables the image\ngenerator to exploit temporal information while the video generation model\nlearns to flexibly share information across frames. Moreover, experimental\nresults verify our approach's compatibility with pre-training on videos or\nimages and training on datasets containing a mixture of both. A comprehensive\nset of quantitative and qualitative evaluations reveal the improvements in\nsample quality and diversity over both video generation and image generation\nbaselines. We further demonstrate the technique's capabilities of exploiting\nsimilarity in features across frames by applying it to a model based on\ndecomposing the video into motion and content. The proposed model allows minor\nvariations in content across frames while maintaining the temporal dependence\nthrough latent vectors encoding the pose or motion features.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:14:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Dandi", "Yatin", ""], ["Das", "Aniket", ""], ["Singhal", "Soumye", ""], ["Namboodiri", "Vinay P.", ""], ["Rai", "Piyush", ""]]}, {"id": "1912.07999", "submitter": "No\\\"elie Cherrier", "authors": "No\\\"elie Cherrier, Maxime Defurne, Jean-Philippe Poli and Franck\n  Sabati\\'e", "title": "Embedded Constrained Feature Construction for High-Energy Physics Data\n  Classification", "comments": "Accepted at the NeurIPS 2019 workshop on Machine Learning for the\n  Physical Sciences (https://ml4physicalsciences.github.io)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before any publication, data analysis of high-energy physics experiments must\nbe validated. This validation is granted only if a perfect understanding of the\ndata and the analysis process is demonstrated. Therefore, physicists prefer\nusing transparent machine learning algorithms whose performances highly rely on\nthe suitability of the provided input features. To transform the feature space,\nfeature construction aims at automatically generating new relevant features.\nWhereas most of previous works in this area perform the feature construction\nprior to the model training, we propose here a general framework to embed a\nfeature construction technique adapted to the constraints of high-energy\nphysics in the induction of tree-based models. Experiments on two high-energy\nphysics datasets confirm that a significant gain is obtained on the\nclassification scores, while limiting the number of built features. Since the\nfeatures are built to be interpretable, the whole model is transparent and\nreadable.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:30:11 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Cherrier", "No\u00eblie", ""], ["Defurne", "Maxime", ""], ["Poli", "Jean-Philippe", ""], ["Sabati\u00e9", "Franck", ""]]}, {"id": "1912.08001", "submitter": "No\\\"elie Cherrier", "authors": "Marouen Baalouch, Maxime Defurne, Jean-Philippe Poli and No\\\"elie\n  Cherrier", "title": "Sim-to-Real Domain Adaptation For High Energy Physics", "comments": "Accepted at the NeurIPS 2019 workshop on Machine Learning for the\n  Physical Sciences (https://ml4physicalsciences.github.io)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle physics or High Energy Physics (HEP) studies the elementary\nconstituents of matter and their interactions with each other. Machine Learning\n(ML) has played an important role in HEP analysis and has proven extremely\nsuccessful in this area. Usually, the ML algorithms are trained on numerical\nsimulations of the experimental setup and then applied to the real experimental\ndata. However, any discrepancy between the simulation and real data may lead to\ndramatic consequences concerning the performances of the algorithm on real\ndata. In this paper, we present an application of domain adaptation using a\nDomain Adversarial Neural Network trained on public HEP data. We demonstrate\nthe success of this approach to achieve sim-to-real transfer and ensure the\nconsistency of the ML algorithms performances on real and simulated HEP\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:37:32 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Baalouch", "Marouen", ""], ["Defurne", "Maxime", ""], ["Poli", "Jean-Philippe", ""], ["Cherrier", "No\u00eblie", ""]]}, {"id": "1912.08041", "submitter": "Xavier Amatriain", "authors": "Anitha Kannan, Jason Alan Fries, Eric Kramer, Jen Jen Chen, Nigam\n  Shah, Xavier Amatriain", "title": "The accuracy vs. coverage trade-off in patient-facing diagnosis models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A third of adults in America use the Internet to diagnose medical concerns,\nand online symptom checkers are increasingly part of this process. These tools\nare powered by diagnosis models similar to clinical decision support systems,\nwith the primary difference being the coverage of symptoms and diagnoses. To be\nuseful to patients and physicians, these models must have high accuracy while\ncovering a meaningful space of symptoms and diagnoses. To the best of our\nknowledge, this paper is the first in studying the trade-off between the\ncoverage of the model and its performance for diagnosis. To this end, we learn\ndiagnosis models with different coverage from EHR data. We find a 1\\% drop in\ntop-3 accuracy for every 10 diseases added to the coverage. We also observe\nthat complexity for these models does not affect performance, with linear\nmodels performing as well as neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:27:18 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Kannan", "Anitha", ""], ["Fries", "Jason Alan", ""], ["Kramer", "Eric", ""], ["Chen", "Jen Jen", ""], ["Shah", "Nigam", ""], ["Amatriain", "Xavier", ""]]}, {"id": "1912.08055", "submitter": "Heramb Nemlekar", "authors": "Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar\n  and Stefanos Nikolaidis", "title": "Fair Contextual Multi-Armed Bandits: Theory and Experiments", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an AI system interacts with multiple users, it frequently needs to make\nallocation decisions. For instance, a virtual agent decides whom to pay\nattention to in a group setting, or a factory robot selects a worker to deliver\na part. Demonstrating fairness in decision making is essential for such systems\nto be broadly accepted. We introduce a Multi-Armed Bandit algorithm with\nfairness constraints, where fairness is defined as a minimum rate that a task\nor a resource is assigned to a user. The proposed algorithm uses contextual\ninformation about the users and the task and makes no assumptions on how the\nlosses capturing the performance of different users are generated. We provide\ntheoretical guarantees of performance and empirical results from simulation and\nan online user study. The results highlight the benefit of accounting for\ncontexts in fair decision making, especially when users perform better at some\ncontexts and worse at others.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:25:34 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chen", "Yifang", ""], ["Cuellar", "Alex", ""], ["Luo", "Haipeng", ""], ["Modi", "Jignesh", ""], ["Nemlekar", "Heramb", ""], ["Nikolaidis", "Stefanos", ""]]}, {"id": "1912.08103", "submitter": "Rodrigo A. Gonz\\'alez", "authors": "Rodrigo A. Gonz\\'alez and Cristian R. Rojas", "title": "A Finite-Sample Deviation Bound for Stable Autoregressive Processes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study non-asymptotic deviation bounds of the least squares\nestimator in Gaussian AR($n$) processes. By relying on martingale concentration\ninequalities and a tail-bound for $\\chi^2$ distributed variables, we provide a\nconcentration bound for the sample covariance matrix of the process output.\nWith this, we present a problem-dependent finite-time bound on the deviation\nprobability of any fixed linear combination of the estimated parameters of the\nAR$(n)$ process. We discuss extensions and limitations of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:55:54 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 12:06:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Gonz\u00e1lez", "Rodrigo A.", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "1912.08111", "submitter": "Geunseob Oh", "authors": "Geunseob Oh, Jean-Sebastien Valois", "title": "HCNAF: Hyper-Conditioned Neural Autoregressive Flow and its Application\n  for Probabilistic Occupancy Map Forecasting", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Hyper-Conditioned Neural Autoregressive Flow (HCNAF); a powerful\nuniversal distribution approximator designed to model arbitrarily complex\nconditional probability density functions. HCNAF consists of a neural-net based\nconditional autoregressive flow (AF) and a hyper-network that can take large\nconditions in non-autoregressive fashion and outputs the network parameters of\nthe AF. Like other flow models, HCNAF performs exact likelihood inference. We\nconduct a number of density estimation tasks on toy experiments and MNIST to\ndemonstrate the effectiveness and attributes of HCNAF, including its\ngeneralization capability over unseen conditions and expressivity. Finally, we\nshow that HCNAF scales up to complex high-dimensional prediction problems of\nthe magnitude of self-driving and that HCNAF yields a state-of-the-art\nperformance in a public self-driving dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:07:55 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 03:43:07 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 03:58:24 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Oh", "Geunseob", ""], ["Valois", "Jean-Sebastien", ""]]}, {"id": "1912.08113", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Brian K.\n  Spears", "title": "Improved Surrogates in Inertial Confinement Fusion with Manifold and\n  Cycle Consistencies", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become very popular in surrogate modeling because of\ntheir ability to characterize arbitrary, high dimensional functions in a data\ndriven fashion. This paper advocates for the training of surrogates that are\nconsistent with the physical manifold -- i.e., predictions are always\nphysically meaningful, and are cyclically consistent -- i.e., when the\npredictions of the surrogate, when passed through an independently trained\ninverse model give back the original input parameters. We find that these two\nconsistencies lead to surrogates that are superior in terms of predictive\nperformance, more resilient to sampling artifacts, and tend to be more data\nefficient. Using Inertial Confinement Fusion (ICF) as a test bed problem, we\nmodel a 1D semi-analytic numerical simulator and demonstrate the effectiveness\nof our approach. Code and data are available at\nhttps://github.com/rushilanirudh/macc/\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:14:43 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Bremer", "Peer-Timo", ""], ["Spears", "Brian K.", ""]]}, {"id": "1912.08124", "submitter": "Luca Manneschi", "authors": "Luca Manneschi, Andrew C. Lin, Eleni Vasilaki", "title": "SpaRCe: Improved Learning of Reservoir Computing Systems through Sparse\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Sparse\" neural networks, in which relatively few neurons or connections are\nactive, are common in both machine learning and neuroscience. Whereas in\nmachine learning, \"sparsity\" is related to a penalty term that leads to some\nconnecting weights becoming small or zero, in biological brains, sparsity is\noften created when high spiking thresholds prevent neuronal activity. Here we\nintroduce sparsity into a reservoir computing network via neuron-specific\nlearnable thresholds of activity, allowing neurons with low thresholds to\ncontribute to decision-making but suppressing information from neurons with\nhigh thresholds. This approach, which we term \"SpaRCe\", optimises the sparsity\nlevel of the reservoir without affecting the reservoir dynamics. The read-out\nweights and the thresholds are learned by an on-line gradient rule that\nminimises an error function on the outputs of the network. Threshold learning\noccurs by the balance of two opposing forces: reducing inter-neuronal\ncorrelations in the reservoir by deactivating redundant neurons, while\nincreasing the activity of neurons participating in correct decisions. We test\nSpaRCe on classification problems and find that threshold learning improves\nperformance compared to standard reservoir computing. SpaRCe alleviates the\nproblem of catastrophic forgetting, a problem most evident in standard echo\nstate networks and recurrent neural networks in general, due to increasing the\nnumber of task-specialised neurons that are included in the network decisions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:05:26 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 17:51:11 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 16:25:07 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 05:26:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Manneschi", "Luca", ""], ["Lin", "Andrew C.", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1912.08136", "submitter": "Yan Luo", "authors": "Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, and Qi Zhao", "title": "Direction Concentration Learning: Enhancing Congruency in Machine\n  Learning", "comments": "This is a preprint and the formal version has been published in TPAMI", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2019.2963387", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the well-known challenges in computer vision tasks is the visual\ndiversity of images, which could result in an agreement or disagreement between\nthe learned knowledge and the visual content exhibited by the current\nobservation. In this work, we first define such an agreement in a concepts\nlearning process as congruency. Formally, given a particular task and\nsufficiently large dataset, the congruency issue occurs in the learning process\nwhereby the task-specific semantics in the training data are highly varying. We\npropose a Direction Concentration Learning (DCL) method to improve congruency\nin the learning process, where enhancing congruency influences the convergence\npath to be less circuitous. The experimental results show that the proposed DCL\nmethod generalizes to state-of-the-art models and optimizers, as well as\nimproves the performances of saliency prediction task, continual learning task,\nand classification task. Moreover, it helps mitigate the catastrophic\nforgetting problem in the continual learning task. The code is publicly\navailable at https://github.com/luoyan407/congruency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 16:58:04 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 01:28:05 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Luo", "Yan", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan S.", ""], ["Zhao", "Qi", ""]]}, {"id": "1912.08140", "submitter": "Yashaswi Verma", "authors": "Yashaswi Verma", "title": "An Embarrassingly Simple Baseline for eXtreme Multi-label Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of eXtreme Multi-label Learning (XML) is to design and learn a model\nthat can automatically annotate a given data point with the most relevant\nsubset of labels from an extremely large label set. Recently, many techniques\nhave been proposed for XML that achieve reasonable performance on benchmark\ndatasets. Motivated by the complexities of these methods and their subsequent\ntraining requirements, in this paper we propose a simple baseline technique for\nthis task. Precisely, we present a global feature embedding technique for XML\nthat can easily scale to very large datasets containing millions of data points\nin very high-dimensional feature space, irrespective of number of samples and\nlabels. Next we show how an ensemble of such global embeddings can be used to\nachieve further boost in prediction accuracies with only linear increase in\ntraining and prediction time. During testing, we assign the labels using a\nweighted k-nearest neighbour classifier in the embedding space. Experiments\nreveal that though conceptually simple, this technique achieves quite\ncompetitive results, and has training time of less than one minute using a\nsingle CPU core with 15.6 GB RAM even for large-scale datasets such as\nAmazon-3M.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:11:17 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Verma", "Yashaswi", ""]]}, {"id": "1912.08165", "submitter": "Julien Mairal", "authors": "Julien Mairal", "title": "Cyanure: An Open-Source Toolbox for Empirical Risk Minimization for\n  Python, C++, and soon more", "comments": "http://julien.mairal.org/cyanure/welcome.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyanure is an open-source C++ software package with a Python interface. The\ngoal of Cyanure is to provide state-of-the-art solvers for learning linear\nmodels, based on stochastic variance-reduced stochastic optimization with\nacceleration mechanisms. Cyanure can handle a large variety of loss functions\n(logistic, square, squared hinge, multinomial logistic) and regularization\nfunctions (l_2, l_1, elastic-net, fused Lasso, multi-task group Lasso). It\nprovides a simple Python API, which is very close to that of scikit-learn,\nwhich should be extended to other languages such as R or Matlab in a near\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:04:31 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 08:07:05 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Mairal", "Julien", ""]]}, {"id": "1912.08180", "submitter": "Arindam Bose", "authors": "Shahin Khobahi and Arindam Bose and Mojtaba Soltanalian", "title": "Deep Radar Waveform Design for Efficient Automotive Radar Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radar systems, unimodular (or constant-modulus) waveform design plays an\nimportant role in achieving better clutter/interference rejection, as well as a\nmore accurate estimation of the target parameters. The design of such sequences\nhas been studied widely in the last few decades, with most design algorithms\nrequiring sophisticated a priori knowledge of environmental parameters which\nmay be difficult to obtain in real-time scenarios. In this paper, we propose a\nnovel hybrid model-driven and data-driven architecture that adapts to the ever\nchanging environment and allows for adaptive unimodular waveform design. In\nparticular, the approach lays the groundwork for developing extremely low-cost\nwaveform design and processing frameworks for radar systems deployed in\nautonomous vehicles. The proposed model-based deep architecture imitates a\nwell-known unimodular signal design algorithm in its structure, and can quickly\ninfer statistical information from the environment using the observed data. Our\nnumerical experiments portray the advantages of using the proposed method for\nefficient radar waveform design in time-varying environments.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:39:39 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:23:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Khobahi", "Shahin", ""], ["Bose", "Arindam", ""], ["Soltanalian", "Mojtaba", ""]]}, {"id": "1912.08198", "submitter": "Alexander Hayes", "authors": "Alexander L. Hayes", "title": "srlearn: A Python Library for Gradient-Boosted Statistical Relational\n  Models", "comments": "Ninth International Workshop on Statistical Relational AI (StarAI\n  2020). Software online at https://github.com/hayesall/srlearn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present srlearn, a Python library for boosted statistical relational\nmodels. We adapt the scikit-learn interface to this setting and provide\nexamples for how this can be used to express learning and inference problems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 20:46:32 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Hayes", "Alexander L.", ""]]}, {"id": "1912.08202", "submitter": "Hwiyoung Lee", "authors": "Hwiyoung Lee, Vic Patrangenaru", "title": "Extrinsic Kernel Ridge Regression Classifier for Planar Kendall Shape\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have had great success in Statistics and Machine Learning.\nDespite their growing popularity, however, less effort has been drawn towards\ndeveloping kernel based classification methods on Riemannian manifolds due to\ndifficulty in dealing with non-Euclidean geometry. In this paper, motivated by\nthe extrinsic framework of manifold-valued data analysis, we propose a new\npositive definite kernel on planar Kendall shape space $\\Sigma_2^k$, called\nextrinsic Veronese Whitney Gaussian kernel. We show that our approach can be\nextended to develop Gaussian kernels on any embedded manifold. Furthermore,\nkernel ridge regression classifier (KRRC) is implemented to address the shape\nclassification problem on $\\Sigma_2^k$, and their promising performances are\nillustrated through the real data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:33:58 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 06:12:27 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Lee", "Hwiyoung", ""], ["Patrangenaru", "Vic", ""]]}, {"id": "1912.08278", "submitter": "Andrea Mari", "authors": "Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, Nathan\n  Killoran", "title": "Transfer learning in hybrid classical-quantum neural networks", "comments": "Accepted in Quantum. Code available at:\n  https://github.com/XanaduAI/quantum-transfer-learning", "journal-ref": "Quantum 4, 340 (2020)", "doi": "10.22331/q-2020-10-09-340", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extend the concept of transfer learning, widely applied in modern machine\nlearning algorithms, to the emerging context of hybrid neural networks composed\nof classical and quantum elements. We propose different implementations of\nhybrid transfer learning, but we focus mainly on the paradigm in which a\npre-trained classical network is modified and augmented by a final variational\nquantum circuit. This approach is particularly attractive in the current era of\nintermediate-scale quantum technology since it allows to optimally pre-process\nhigh dimensional data (e.g., images) with any state-of-the-art classical\nnetwork and to embed a select set of highly informative features into a quantum\nprocessor. We present several proof-of-concept examples of the convenient\napplication of quantum transfer learning for image recognition and quantum\nstate classification. We use the cross-platform software library PennyLane to\nexperimentally test a high-resolution image classifier with two different\nquantum computers, respectively provided by IBM and Rigetti.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:24:54 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 09:35:45 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mari", "Andrea", ""], ["Bromley", "Thomas R.", ""], ["Izaac", "Josh", ""], ["Schuld", "Maria", ""], ["Killoran", "Nathan", ""]]}, {"id": "1912.08281", "submitter": "Kimmo K\\\"arkk\\\"ainen", "authors": "Kimmo K\\\"arkk\\\"ainen, Mohammad Kachuee, Orpaz Goldstein, Majid\n  Sarrafzadeh", "title": "Cost-Sensitive Feature-Value Acquisition Using Feature Relevance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world machine learning problems, feature values are not readily\navailable. To make predictions, some of the missing features have to be\nacquired, which can incur a cost in money, computational time, or human time,\ndepending on the problem domain. This leads us to the problem of choosing which\nfeatures to use at the prediction time. The chosen features should increase the\nprediction accuracy for a low cost, but determining which features will do that\nis challenging. The choice should take into account the previously acquired\nfeature values as well as the feature costs. This paper proposes a novel\napproach to address this problem. The proposed approach chooses the most useful\nfeatures adaptively based on how relevant they are for the prediction task as\nwell as what the corresponding feature costs are. Our approach uses a generic\nneural network architecture, which is suitable for a wide range of problems. We\nevaluate our approach on three cost-sensitive datasets, including Yahoo!\nLearning to Rank Competition dataset as well as two health datasets. We show\nthat our approach achieves high accuracy with a lower cost than the current\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:34:36 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 01:32:21 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Kimmo", ""], ["Kachuee", "Mohammad", ""], ["Goldstein", "Orpaz", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1912.08286", "submitter": "Brady Neal", "authors": "Brady Neal", "title": "On the Bias-Variance Tradeoff: Textbooks Need an Update", "comments": "MSc Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this thesis is to point out that the bias-variance tradeoff\nis not always true (e.g. in neural networks). We advocate for this lack of\nuniversality to be acknowledged in textbooks and taught in introductory courses\nthat cover the tradeoff. We first review the history of the bias-variance\ntradeoff, its prevalence in textbooks, and some of the main claims made about\nthe bias-variance tradeoff. Through extensive experiments and analysis, we show\na lack of a bias-variance tradeoff in neural networks when increasing network\nwidth. Our findings seem to contradict the claims of the landmark work by Geman\net al. (1992). Motivated by this contradiction, we revisit the experimental\nmeasurements in Geman et al. (1992). We discuss that there was never strong\nevidence for a tradeoff in neural networks when varying the number of\nparameters. We observe a similar phenomenon beyond supervised learning, with a\nset of deep reinforcement learning experiments. We argue that textbook and\nlecture revisions are in order to convey this nuanced modern understanding of\nthe bias-variance tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:50:59 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Neal", "Brady", ""]]}, {"id": "1912.08294", "submitter": "Elizaveta Rebrova", "authors": "M. A. Iwen, D. Needell, E. Rebrova, and A. Zare", "title": "Lower Memory Oblivious (Tensor) Subspace Embeddings with Fewer Random\n  Bits: Modewise Methods for Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new general modewise Johnson-Lindenstrauss (JL) subspace\nembeddings are proposed that are both considerably faster to generate and\neasier to store than traditional JL embeddings when working with extremely\nlarge vectors and/or tensors.\n  Corresponding embedding results are then proven for two different types of\nlow-dimensional (tensor) subspaces. The first of these new subspace embedding\nresults produces improved space complexity bounds for embeddings of rank-$r$\ntensors whose CP decompositions are contained in the span of a fixed (but\nunknown) set of $r$ rank-one basis tensors. In the traditional vector setting\nthis first result yields new and very general near-optimal oblivious subspace\nembedding constructions that require fewer random bits to generate than\nstandard JL embeddings when embedding subspaces of $\\mathbb{C}^N$ spanned by\nbasis vectors with special Kronecker structure. The second result proven herein\nprovides new fast JL embeddings of arbitrary $r$-dimensional subspaces\n$\\mathcal{S} \\subset \\mathbb{C}^N$ which also require fewer random bits (and so\nare easier to store - i.e., require less space) than standard fast JL embedding\nmethods in order to achieve small $\\epsilon$-distortions. These new oblivious\nsubspace embedding results work by $(i)$ effectively folding any given vector\nin $\\mathcal{S}$ into a (not necessarily low-rank) tensor, and then $(ii)$\nembedding the resulting tensor into $\\mathbb{C}^m$ for $m \\leq C r \\log^c(N) /\n\\epsilon^2$.\n  Applications related to compression and fast compressed least squares\nsolution methods are also considered, including those used for fitting low-rank\nCP decompositions, and the proposed JL embedding results are shown to work well\nnumerically in both settings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 22:18:29 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 19:13:00 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Iwen", "M. A.", ""], ["Needell", "D.", ""], ["Rebrova", "E.", ""], ["Zare", "A.", ""]]}, {"id": "1912.08311", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Bhargav Srinivasa Desikan", "title": "Kernel-Based Ensemble Learning in Python", "comments": "11 pages", "journal-ref": "Information 2020, 11(2)", "doi": "10.3390/info11020063", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new supervised learning algorithm, for classification and\nregression problems where two or more preliminary predictors are available. We\nintroduce \\texttt{KernelCobra}, a non-linear learning strategy for combining an\narbitrary number of initial predictors. \\texttt{KernelCobra} builds on the\nCOBRA algorithm introduced by \\citet{biau2016cobra}, which combined estimators\nbased on a notion of proximity of predictions on the training data. While the\nCOBRA algorithm used a binary threshold to declare which training data were\nclose and to be used, we generalize this idea by using a kernel to better\nencapsulate the proximity information. Such a smoothing kernel provides more\nrepresentative weights to each of the training points which are used to build\nthe aggregate and final predictor, and \\texttt{KernelCobra} systematically\noutperforms the COBRA algorithm. While COBRA is intended for regression,\n\\texttt{KernelCobra} deals with classification and regression.\n\\texttt{KernelCobra} is included as part of the open source Python package\n\\texttt{Pycobra} (0.2.4 and onward), introduced by \\citet{guedj2018pycobra}.\nNumerical experiments assess the performance (in terms of pure prediction and\ncomputational complexity) of \\texttt{KernelCobra} on real-life and synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:23:00 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Guedj", "Benjamin", ""], ["Desikan", "Bhargav Srinivasa", ""]]}, {"id": "1912.08335", "submitter": "Andres Masegosa R", "authors": "Andres R. Masegosa", "title": "Learning under Model Misspecification: Applications to Variational and\n  Ensemble methods", "comments": "Camera-Ready Version. NeurIPS 2020. Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually any model we use in machine learning to make predictions does not\nperfectly represent reality. So, most of the learning happens under model\nmisspecification. In this work, we present a novel analysis of the\ngeneralization performance of Bayesian model averaging under model\nmisspecification and i.i.d. data using a new family of second-order PAC-Bayes\nbounds. This analysis shows, in simple and intuitive terms, that Bayesian model\naveraging provides suboptimal generalization performance when the model is\nmisspecified. In consequence, we provide strong theoretical arguments showing\nthat Bayesian methods are not optimal for learning predictive models, unless\nthe model class is perfectly specified. Using novel second-order PAC-Bayes\nbounds, we derive a new family of Bayesian-like algorithms, which can be\nimplemented as variational and ensemble methods. The output of these algorithms\nis a new posterior distribution, different from the Bayesian posterior, which\ninduces a posterior predictive distribution with better generalization\nperformance. Experiments with Bayesian neural networks illustrate these\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:38:58 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 09:29:53 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 12:29:11 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 11:10:41 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 10:08:28 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Masegosa", "Andres R.", ""]]}, {"id": "1912.08342", "submitter": "Orlando Romero", "authors": "Orlando Romero, Mouhacine Benosman", "title": "Finite-Time Convergence of Continuous-Time Optimization Algorithms via\n  Differential Inclusions", "comments": "Presented at workshop \"Beyond First Order Methods in Machine\n  Learning\" of NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two discontinuous dynamical systems in continuous\ntime with guaranteed prescribed finite-time local convergence to strict local\nminima of a given cost function. Our approach consists of exploiting a\nLyapunov-based differential inequality for differential inclusions, which leads\nto finite-time stability and thus finite-time convergence with a provable bound\non the settling time. In particular, for exact solutions to the aforementioned\ndifferential inequality, the settling-time bound is also exact, thus achieving\nprescribed finite-time convergence. We thus construct a class of discontinuous\ndynamical systems, of second order with respect to the cost function, that\nserve as continuous-time optimization algorithms with finite-time convergence\nand prescribed convergence time. Finally, we illustrate our results on the\nRosenbrock function.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 02:18:04 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Romero", "Orlando", ""], ["Benosman", "Mouhacine", ""]]}, {"id": "1912.08348", "submitter": "Farzana Nasrin", "authors": "Farzana Nasrin, Christopher Oballe, David L. Boothe, and Vasileios\n  Maroulas", "title": "Bayesian Topological Learning for Brain State Classification", "comments": "7 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of human brain states through electroencephalograph (EEG)\nsignals is a crucial step in human-machine communications. However, classifying\nand analyzing EEG signals are challenging due to their noisy, nonlinear and\nnonstationary nature. Current methodologies for analyzing these signals often\nfall short because they have several regularity assumptions baked in. This work\nprovides an effective, flexible and noise-resilient scheme to analyze EEG by\nextracting pertinent information while abiding by the 3N (noisy, nonlinear and\nnonstationary) nature of data. We implement a topological tool, namely\npersistent homology, that tracks the evolution of topological features over\ntime intervals and incorporates individual's expectations as prior knowledge by\nmeans of a Bayesian framework to compute posterior distributions. Relying on\nthese posterior distributions, we apply Bayes factor classification to noisy\nEEG measurements. The performance of this Bayesian classification scheme is\nthen compared with other existing methods for EEG signals.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 02:36:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nasrin", "Farzana", ""], ["Oballe", "Christopher", ""], ["Boothe", "David L.", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1912.08350", "submitter": "Makena Low", "authors": "Makena Low, Priyanka Raina", "title": "Automating Vitiligo Skin Lesion Segmentation Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several skin conditions such as vitiligo, accurate segmentation of\nlesions from skin images is the primary measure of disease progression and\nseverity. Existing methods for vitiligo lesion segmentation require manual\nintervention. Unfortunately, manual segmentation is time and labor-intensive,\nas well as irreproducible between physicians. We introduce a convolutional\nneural network (CNN) that quickly and robustly performs vitiligo skin lesion\nsegmentation. Our CNN has a U-Net architecture with a modified contracting\npath. We use the CNN to generate an initial segmentation of the lesion, then\nrefine it by running the watershed algorithm on high-confidence pixels. We\ntrain the network on 247 images with a variety of lesion sizes, complexity, and\nanatomical sites. The network with our modifications noticeably outperforms the\nstate-of-the-art U-Net, with a Jaccard Index (JI) score of 73.6% (compared to\n36.7%). Moreover, our method requires only a few seconds for segmentation, in\ncontrast with the previously proposed semi-autonomous watershed approach, which\nrequires 2-29 minutes per image.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:15:44 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Low", "Makena", ""], ["Raina", "Priyanka", ""]]}, {"id": "1912.08394", "submitter": "Andreas W. Kempa-Liehr", "authors": "Andreas W. Kempa-Liehr and Jonty Oram and Andrew Wong and Mark Finch\n  and Thor Besier", "title": "Feature engineering workflow for activity recognition from synchronized\n  inertial measurement units", "comments": "Multi-Sensor for Action and Gesture Recognition (MAGR), ACPR 2019\n  Workshop, Auckland, New Zealand", "journal-ref": null, "doi": "10.1007/978-981-15-3651-9_20", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous availability of wearable sensors is responsible for driving\nthe Internet-of-Things but is also making an impact on sport sciences and\nprecision medicine. While human activity recognition from smartphone data or\nother types of inertial measurement units (IMU) has evolved to one of the most\nprominent daily life examples of machine learning, the underlying process of\ntime-series feature engineering still seems to be time-consuming. This lengthy\nprocess inhibits the development of IMU-based machine learning applications in\nsport science and precision medicine. This contribution discusses a feature\nengineering workflow, which automates the extraction of time-series feature on\nbased on the FRESH algorithm (FeatuRe Extraction based on Scalable Hypothesis\ntests) to identify statistically significant features from synchronized IMU\nsensors (IMeasureU Ltd, NZ). The feature engineering workflow has five main\nsteps: time-series engineering, automated time-series feature extraction,\noptimized feature extraction, fitting of a specialized classifier, and\ndeployment of optimized machine learning pipeline. The workflow is discussed\nfor the case of a user-specific running-walking classification, and the\ngeneralization to a multi-user multi-activity classification is demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:51:42 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kempa-Liehr", "Andreas W.", ""], ["Oram", "Jonty", ""], ["Wong", "Andrew", ""], ["Finch", "Mark", ""], ["Besier", "Thor", ""]]}, {"id": "1912.08400", "submitter": "Yu Xia", "authors": "Jiawei Long and Yu Xia", "title": "Cluster Analysis of High-Dimensional scRNA Sequencing Data", "comments": "9 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ongoing developments and innovations in single-cell RNA sequencing\nmethods, advancements in sequencing performance could empower significant\ndiscoveries as well as new emerging possibilities to address biological and\nmedical investigations. In the study, we will be using the dataset collected by\nthe authors of Systematic comparative analysis of single cell RNA-sequencing\nmethods. The dataset consists of single-cell and single nucleus profiling from\nthree types of samples - cell lines, peripheral blood mononuclear cells, and\nbrain tissue, which offers 36 libraries in six separate experiments in a single\ncenter. Our quantitative comparison aims to identify unique characteristics\nassociated with different single-cell sequencing methods, especially among\nlow-throughput sequencing methods and high-throughput sequencing methods. Our\nprocedures also incorporate evaluations of every method's capacity for\nrecovering known biological information in the samples through clustering\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 06:05:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Long", "Jiawei", ""], ["Xia", "Yu", ""]]}, {"id": "1912.08409", "submitter": "Burc Gokden", "authors": "Burc Gokden", "title": "CoulGAT: An Experiment on Interpretability of Graph Attention Networks", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention mechanism inspired from definition of screened\nCoulomb potential. This attention mechanism was used to interpret the Graph\nAttention (GAT) model layers and training dataset by using a flexible and\nscalable framework (CoulGAT) developed for this purpose. Using CoulGAT, a\nforest of plain and resnet models were trained and characterized using this\nattention mechanism against CHAMPS dataset. The learnable variables of the\nattention mechanism are used to extract node-node and node-feature interactions\nto define an empirical standard model for the graph structure and hidden layer.\nThis representation of graph and hidden layers can be used as a tool to compare\ndifferent models, optimize hidden layers and extract a compact definition of\ngraph structure of the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 06:46:39 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Gokden", "Burc", ""]]}, {"id": "1912.08416", "submitter": "Sebastian Ober", "authors": "Sebastian W. Ober, Carl Edward Rasmussen", "title": "Benchmarking the Neural Linear Model for Regression", "comments": "Advances in Approximate Bayesian Inference (AABI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural linear model is a simple adaptive Bayesian linear regression\nmethod that has recently been used in a number of problems ranging from\nBayesian optimization to reinforcement learning. Despite its apparent successes\nin these settings, to the best of our knowledge there has been no systematic\nexploration of its capabilities on simple regression tasks. In this work we\ncharacterize these on the UCI datasets, a popular benchmark for Bayesian\nregression models, as well as on the recently introduced UCI \"gap\" datasets,\nwhich are better tests of out-of-distribution uncertainty. We demonstrate that\nthe neural linear model is a simple method that shows generally good\nperformance on these tasks, but at the cost of requiring good hyperparameter\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 07:23:57 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Ober", "Sebastian W.", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1912.08421", "submitter": "Shuang Zhang", "authors": "Shuang Zhang, Liyao Xiang, Congcong Li, Yixuan Wang, Quanshi Zhang,\n  Wei Wang and Bo Li", "title": "Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile\n  Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered by machine learning services in the cloud, numerous learning-driven\nmobile applications are gaining popularity in the market. As deep learning\ntasks are mostly computation-intensive, it has become a trend to process raw\ndata on devices and send the deep neural network (DNN) features to the cloud,\nwhere the features are further processed to return final results. However,\nthere is always unexpected leakage with the release of features, with which an\nadversary could infer a significant amount of information about the original\ndata. We propose a privacy-preserving reinforcement learning framework on top\nof the mobile cloud infrastructure from the perspective of DNN structures. The\nframework aims to learn a policy to modify the base DNNs to prevent information\nleakage while maintaining high inference accuracy. The policy can also be\nreadily transferred to large-size DNNs to speed up learning. Extensive\nevaluations on a variety of DNNs have shown that our framework can successfully\nfind privacy-preserving DNN structures to defend different privacy attacks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 07:42:57 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 11:55:03 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Shuang", ""], ["Xiang", "Liyao", ""], ["Li", "Congcong", ""], ["Wang", "Yixuan", ""], ["Zhang", "Quanshi", ""], ["Wang", "Wei", ""], ["Li", "Bo", ""]]}, {"id": "1912.08434", "submitter": "Javier Felip Leon", "authors": "Javier Felip and Nilesh Ahuja and Omesh Tickoo", "title": "Tree pyramidal adaptive importance sampling", "comments": "20 pages + 13 pages of additional result plots and evaluation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Tree-Pyramidal Adaptive Importance Sampling (TP-AIS), a\nnovel iterated sampling method that outperforms state-of-the-art approaches\nlike deterministic mixture population Monte Carlo (DM-PMC), mixture population\nMonte Carlo (M-PMC) and layered adaptive importance sampling (LAIS). TP-AIS\niteratively builds a proposal distribution parameterized by a tree pyramid,\nwhere each tree leaf spans a subspace that represents its importance density.\nAfter each new sample operation, a set of tree leaves are subdivided for\nimproving the approximation of the proposal distribution to the target density.\nUnlike the rest of the methods in the literature, TP-AIS is parameter free and\nrequires no tuning to achieve its best performance. We evaluate TP-AIS with\ndifferent complexity randomized target probability density functions (PDF) and\nalso analyze its application to different dimensions. The results are compared\nto state-of-the-art iterative importance sampling approaches and other baseline\nMCMC approaches using Normalized Effective Sample Size (N-ESS), Jensen-Shannon\nDivergence, and time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:05:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 18:55:24 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Felip", "Javier", ""], ["Ahuja", "Nilesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1912.08444", "submitter": "Yichuan Charlie Tang", "authors": "Lionel Blond\\'e, Yichuan Charlie Tang, Jian Zhang, Russ Webb", "title": "Relational Mimic for Visual Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new method for imitation learning from video\ndemonstrations. Our method, Relational Mimic (RM), improves on previous visual\nimitation learning methods by combining generative adversarial networks and\nrelational learning. RM is flexible and can be used in conjunction with other\nrecent advances in generative adversarial imitation learning to better address\nthe need for more robust and sample-efficient approaches. In addition, we\nintroduce a new neural network architecture that improves upon the previous\nstate-of-the-art in reinforcement learning and illustrate how increasing the\nrelational reasoning capabilities of the agent enables the latter to achieve\nincreasingly higher performance in a challenging locomotion task with pixel\ninputs. Finally, we study the effects and contributions of relational learning\nin policy evaluation, policy improvement and reward learning through ablation\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:19:39 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Blond\u00e9", "Lionel", ""], ["Tang", "Yichuan Charlie", ""], ["Zhang", "Jian", ""], ["Webb", "Russ", ""]]}, {"id": "1912.08465", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Augusto Santos, Ali H. Sayed", "title": "Graph Learning Under Partial Observability", "comments": "to appear in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization, inference and learning tasks can be accomplished\nefficiently by means of decentralized processing algorithms where the network\ntopology (i.e., the graph) plays a critical role in enabling the interactions\namong neighboring nodes. There is a large body of literature examining the\neffect of the graph structure on the performance of decentralized processing\nstrategies. In this article, we examine the inverse problem and consider the\nreverse question: How much information does observing the behavior at the nodes\nof a graph convey about the underlying topology? For large-scale networks, the\ndifficulty in addressing such inverse problems is compounded by the fact that\nusually only a limited fraction of the nodes can be probed, giving rise to a\nsecond important question: Despite the presence of unobserved nodes, can\npartial observations still be sufficient to discover the graph linking the\nprobed nodes? The article surveys recent advances on this challenging learning\nproblem and related questions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:10:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:35:58 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 15:03:52 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Matta", "Vincenzo", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1912.08484", "submitter": "Philipp Marquetand", "authors": "Julia Westermayr, Felix A. Faber, Anders S. Christensen, O. Anatole\n  von Lilienfeld, Philipp Marquetand", "title": "Neural networks and kernel ridge regression for excited states dynamics\n  of CH$_2$NH$_2^+$: From single-state to multi-state representations and\n  multi-property machine learning models", "comments": null, "journal-ref": null, "doi": "10.1088/2632-2153/ab88d0", "report-no": null, "categories": "physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Excited-state dynamics simulations are a powerful tool to investigate\nphoto-induced reactions of molecules and materials and provide complementary\ninformation to experiments. Since the applicability of these simulation\ntechniques is limited by the costs of the underlying electronic structure\ncalculations, we develop and assess different machine learning models for this\ntask. The machine learning models are trained on {\\emph ab initio} calculations\nfor excited electronic states, using the methylenimmonium cation\n(CH$_2$NH$_2^+$) as a model system. For the prediction of excited-state\nproperties, multiple outputs are desirable, which is straightforward with\nneural networks but less explored with kernel ridge regression. We overcome\nthis challenge for kernel ridge regression in the case of energy predictions by\nencoding the electronic states explicitly in the inputs, in addition to the\nmolecular representation. We adopt this strategy also for our neural networks\nfor comparison. Such a state encoding enables not only kernel ridge regression\nwith multiple outputs but leads also to more accurate machine learning models\nfor state-specific properties. An important goal for excited-state machine\nlearning models is their use in dynamics simulations, which needs not only\nstate-specific information but also couplings, i.e., properties involving pairs\nof states. Accordingly, we investigate the performance of different models for\nsuch coupling elements. Furthermore, we explore how combining all properties in\na single neural network affects the accuracy. As an ultimate test for our\nmachine learning models, we carry out excited-state dynamics simulations based\non the predicted energies, forces and couplings and, thus, show the scopes and\npossibilities of machine learning for the treatment of electronically excited\nstates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:41:59 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Westermayr", "Julia", ""], ["Faber", "Felix A.", ""], ["Christensen", "Anders S.", ""], ["von Lilienfeld", "O. Anatole", ""], ["Marquetand", "Philipp", ""]]}, {"id": "1912.08495", "submitter": "Yatao A. Bian", "authors": "Yatao An Bian", "title": "Provable Non-Convex Optimization and Algorithm Validation via\n  Submodularity", "comments": "PhD thesis of Yatao (An) Bian; It is about continuous submodular\n  optimization and algorithm validation", "journal-ref": null, "doi": "10.3929/ethz-b-000386316", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodularity is one of the most well-studied properties of problem classes\nin combinatorial optimization and many applications of machine learning and\ndata mining, with strong implications for guaranteed optimization. In this\nthesis, we investigate the role of submodularity in provable non-convex\noptimization and validation of algorithms. A profound understanding which\nclasses of functions can be tractably optimized remains a central challenge for\nnon-convex optimization. By advancing the notion of submodularity to continuous\ndomains (termed \"continuous submodularity\"), we characterize a class of\ngenerally non-convex and non-concave functions -- continuous submodular\nfunctions, and derive algorithms for approximately maximizing them with strong\napproximation guarantees. Meanwhile, continuous submodularity captures a wide\nspectrum of applications, ranging from revenue maximization with general\nmarketing strategies, MAP inference for DPPs to mean field inference for\nprobabilistic log-submodular models, which renders it as a valuable domain\nknowledge in optimizing this class of objectives. Validation of algorithms is\nan information-theoretic framework to investigate the robustness of algorithms\nto fluctuations in the input/observations and their generalization ability. We\ninvestigate various algorithms for one of the paradigmatic unconstrained\nsubmodular maximization problem: MaxCut. Due to submodularity of the MaxCut\nobjective, we are able to present efficient approaches to calculate the\nalgorithmic information content of MaxCut algorithms. The results provide\ninsights into the robustness of different algorithmic techniques for MaxCut.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 10:13:38 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Bian", "Yatao An", ""]]}, {"id": "1912.08517", "submitter": "Marc Dymetman", "authors": "Tetiana Parshakova and Jean-Marc Andreoli and Marc Dymetman", "title": "Distributional Reinforcement Learning for Energy-Based Sequential Models", "comments": "OptRL workshop (Optimization Foundations for Reinforcement Learning)\n  at Neurips 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Autoregressive Models (GAMs) are a recent proposal [Parshakova et al.,\nCoNLL 2019] for exploiting global properties of sequences for data-efficient\nlearning of seq2seq models. In the first phase of training, an Energy-Based\nmodel (EBM) over sequences is derived. This EBM has high representational\npower, but is unnormalized and cannot be directly exploited for sampling. To\naddress this issue [Parshakova et al., CoNLL 2019] proposes a distillation\ntechnique, which can only be applied under limited conditions. By relating this\nproblem to Policy Gradient techniques in RL, but in a \\emph{distributional}\nrather than \\emph{optimization} perspective, we propose a general approach\napplicable to any sequential EBM. Its effectiveness is illustrated on GAM-based\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:05:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Parshakova", "Tetiana", ""], ["Andreoli", "Jean-Marc", ""], ["Dymetman", "Marc", ""]]}, {"id": "1912.08521", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Sadegh Aliakbarian, Fatemeh Sadat Saleh, Lars Petersson, Stephen\n  Gould, Mathieu Salzmann", "title": "Contextually Plausible and Diverse 3D Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of diverse 3D human motion prediction, that is,\nforecasting multiple plausible future 3D poses given a sequence of observed 3D\nposes. In this context, a popular approach consists of using a Conditional\nVariational Autoencoder (CVAE). However, existing approaches that do so either\nfail to capture the diversity in human motion, or generate diverse but\nsemantically implausible continuations of the observed motion. In this paper,\nwe address both of these problems by developing a new variational framework\nthat accounts for both diversity and context of the generated future motion. To\nthis end, and in contrast to existing approaches, we condition the sampling of\nthe latent variable that acts as source of diversity on the representation of\nthe past observation, thus encouraging it to carry relevant information. Our\nexperiments demonstrate that our approach yields motions not only of higher\nquality while retaining diversity, but also that preserve the contextual\ninformation contained in the observed 3D pose sequence.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:13:44 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 13:16:24 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 01:29:31 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2020 08:59:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Aliakbarian", "Sadegh", ""], ["Saleh", "Fatemeh Sadat", ""], ["Petersson", "Lars", ""], ["Gould", "Stephen", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1912.08526", "submitter": "Anastasia Borovykh", "authors": "Anastasia Borovykh", "title": "Analytic expressions for the output evolution of a deep neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology based on a Taylor expansion of the network\noutput for obtaining analytical expressions for the expected value of the\nnetwork weights and output under stochastic training. Using these analytical\nexpressions the effects of the hyperparameters and the noise variance of the\noptimization algorithm on the performance of the deep neural network are\nstudied. In the early phases of training with a small noise coefficient, the\noutput is equivalent to a linear model. In this case the network can generalize\nbetter due to the noise preventing the output from fully converging on the\ntrain data, however the noise does not result in any explicit regularization.\nIn the later training stages, when higher order approximations are required,\nthe impact of the noise becomes more significant, i.e. in a model which is\nnon-linear in the weights noise can regularize the output function resulting in\nbetter generalization as witnessed by its influence on the weight Hessian, a\ncommonly used metric for generalization capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 11:18:18 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Borovykh", "Anastasia", ""]]}, {"id": "1912.08581", "submitter": "H{\\aa}vard Kvamme", "authors": "H{\\aa}vard Kvamme and {\\O}rnulf Borgan", "title": "The Brier Score under Administrative Censoring: Problems and Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Brier score is commonly used for evaluating probability predictions. In\nsurvival analysis, with right-censored observations of the event times, this\nscore can be weighted by the inverse probability of censoring (IPCW) to retain\nits original interpretation. It is common practice to estimate the censoring\ndistribution with the Kaplan-Meier estimator, even though it assumes that the\ncensoring distribution is independent of the covariates. This paper discusses\nthe general impact of the censoring estimates on the Brier score and shows that\nthe estimation of the censoring distribution can be problematic. In particular,\nwhen the censoring times can be identified from the covariates, the IPCW score\nis no longer valid. For administratively censored data, where the potential\ncensoring times are known for all individuals, we propose an alternative\nversion of the Brier score. This administrative Brier score does not require\nestimation of the censoring distribution and is valid even if the censoring\ntimes can be identified from the covariates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:13:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kvamme", "H\u00e5vard", ""], ["Borgan", "\u00d8rnulf", ""]]}, {"id": "1912.08616", "submitter": "Anton Akusok", "authors": "Anton Akusok and Emil Eirola", "title": "Comparison of Classification Methods for Very High-Dimensional Data in\n  Sparse Random Projection Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The big data trend has inspired feature-driven learning tasks, which cannot\nbe handled by conventional machine learning models. Unstructured data produces\nvery large binary matrices with millions of columns when converted to vector\nform. However, such data is often sparse, and hence can be manageable through\nthe use of sparse random projections.\n  This work studies efficient non-iterative and iterative methods suitable for\nsuch data, evaluating the results on two representative machine learning tasks\nwith millions of samples and features. An efficient Jaccard kernel is\nintroduced as an alternative to the sparse random projection. Findings indicate\nthat non-iterative methods can find larger, more accurate models than iterative\nmethods in different application scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:05:54 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Akusok", "Anton", ""], ["Eirola", "Emil", ""]]}, {"id": "1912.08637", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil, Sheetal Kalyani", "title": "Generalized Residual Ratio Thresholding", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous orthogonal matching pursuit (SOMP) and block OMP (BOMP) are two\nwidely used techniques for sparse support recovery in multiple measurement\nvector (MMV) and block sparse (BS) models respectively. For optimal\nperformance, both SOMP and BOMP require \\textit{a priori} knowledge of signal\nsparsity or noise variance. However, sparsity and noise variance are\nunavailable in most practical applications. This letter presents a novel\ntechnique called generalized residual ratio thresholding (GRRT) for operating\nSOMP and BOMP without the \\textit{a priori} knowledge of signal sparsity and\nnoise variance and derive finite sample and finite signal to noise ratio (SNR)\nguarantees for exact support recovery. Numerical simulations indicate that GRRT\nperforms similar to BOMP and SOMP with \\textit{a priori} knowledge of signal\nand noise statistics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:39:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 06:18:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1912.08638", "submitter": "Anton Akusok", "authors": "Anton Akusok, Emil Eirola, Yoan Miche, Ian Oliver, Kaj-Mikael Bj\\\"ork,\n  Andrey Gritsenko, Stephen Baek and Amaury Lendasse", "title": "Incremental ELMVIS for unsupervised learning", "comments": null, "journal-ref": "Proceedings of ELM-2016 (pp. 183-193). Springer, Cham", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An incremental version of the ELMVIS+ method is proposed in this paper. It\niteratively selects a few best fitting data samples from a large pool, and adds\nthem to the model. The method keeps high speed of ELMVIS+ while allowing for\nmuch larger possible sample pools due to lower memory requirements. The\nextension is useful for reaching a better local optimum with greedy\noptimization of ELMVIS, and the data structure can be specified in\nsemi-supervised optimization. The major new application of incremental ELMVIS\nis not to visualization, but to a general dataset processing. The method is\ncapable of learning dependencies from non-organized unsupervised data -- either\nreconstructing a shuffled dataset, or learning dependencies in complex\nhigh-dimensional space. The results are interesting and promising, although\nthere is space for improvements.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:41:05 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Akusok", "Anton", ""], ["Eirola", "Emil", ""], ["Miche", "Yoan", ""], ["Oliver", "Ian", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Gritsenko", "Andrey", ""], ["Baek", "Stephen", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.08755", "submitter": "Marc-Andre Schulz", "authors": "Marc-Andre Schulz, Matt Chapman-Rounds, Manisha Verma, Danilo Bzdok,\n  Konstantinos Georgatzis", "title": "Clusters in Explanation Space: Inferring disease subtypes from model\n  explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of disease subtypes and corresponding biomarkers can\nsubstantially improve clinical diagnosis and treatment selection. Discovering\nthese subtypes in noisy, high dimensional biomedical data is often impossible\nfor humans and challenging for machines. We introduce a new approach to\nfacilitate the discovery of disease subtypes: Instead of analyzing the original\ndata, we train a diagnostic classifier (healthy vs. diseased) and extract\ninstance-wise explanations for the classifier's decisions. The distribution of\ninstances in the explanation space of our diagnostic classifier amplifies the\ndifferent reasons for belonging to the same class - resulting in a\nrepresentation that is uniquely useful for discovering latent subtypes. We\ncompare our ability to recover subtypes via cluster analysis on model\nexplanations to classical cluster analysis on the original data. In multiple\ndatasets with known ground-truth subclasses, most compellingly on UK Biobank\nbrain imaging data and transcriptome data from the Cancer Genome Atlas, we show\nthat cluster analysis on model explanations substantially outperforms the\nclassical approach. While we believe clustering in explanation space to be\nparticularly valuable for inferring disease subtypes, the method is more\ngeneral and applicable to any kind of sub-type identification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:39:56 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 23:05:20 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Schulz", "Marc-Andre", ""], ["Chapman-Rounds", "Matt", ""], ["Verma", "Manisha", ""], ["Bzdok", "Danilo", ""], ["Georgatzis", "Konstantinos", ""]]}, {"id": "1912.08756", "submitter": "Jing Li", "authors": "Soroosh Khoram, Stephen J Wright, Jing Li", "title": "Interleaved Composite Quantization for High-Dimensional Similarity\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search retrieves the nearest neighbors of a query vector from a\ndataset of high-dimensional vectors. As the size of the dataset grows, the cost\nof performing the distance computations needed to implement a query can become\nprohibitive. A method often used to reduce this computational cost is\nquantization of the vector space and location-based encoding of the dataset\nvectors. These encodings can be used during query processing to find\napproximate nearest neighbors of the query point quickly. Search speed can be\nimproved by using shorter codes, but shorter codes have higher quantization\nerror, leading to degraded precision. In this work, we propose the Interleaved\nComposite Quantization (ICQ) which achieves fast similarity search without\nusing shorter codes. In ICQ, a small subset of the code is used to approximate\nthe distances, with complete codes being used only when necessary. Our method\neffectively reduces both code length and quantization error. Furthermore, ICQ\nis compatible with several recently proposed techniques for reducing\nquantization error and can be used in conjunction with these other techniques\nto improve results. We confirm these claims and show strong empirical\nperformance of ICQ using several synthetic and real-word datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:40:56 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 01:41:50 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Khoram", "Soroosh", ""], ["Wright", "Stephen J", ""], ["Li", "Jing", ""]]}, {"id": "1912.08765", "submitter": "Muhammed Talo", "authors": "Muhammed Talo", "title": "An Automated Deep Learning Approach for Bacterial Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automated recognition and classification of bacteria species from microscopic\nimages have significant importance in clinical microbiology. Bacteria\nclassification is usually carried out manually by biologists using different\nshapes and morphologic characteristics of bacteria species. The manual taxonomy\nof bacteria types from microscopy images is time-consuming and a challenging\ntask for even experienced biologists. In this study, an automated deep learning\nbased classification approach has been proposed to classify bacterial images\ninto different categories. The ResNet-50 pre-trained CNN architecture has been\nused to classify digital bacteria images into 33 categories. The transfer\nlearning technique was employed to accelerate the training process of the\nnetwork and improve the classification performance of the network. The proposed\nmethod achieved an average classification accuracy of 99.2%. The experimental\nresults demonstrate that the proposed technique surpasses state-of-the-art\nmethods in the literature and can be used for any type of bacteria\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:38:31 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Talo", "Muhammed", ""]]}, {"id": "1912.08766", "submitter": "Varun Nair", "authors": "Varun Nair, Javier Fuentes Alonso, Tony Beltramelli", "title": "RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms", "comments": "Code available at https://github.com/uizard-technologies/realmix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) algorithms have shown great potential in\ntraining regimes when access to labeled data is scarce but access to unlabeled\ndata is plentiful. However, our experiments illustrate several shortcomings\nthat prior SSL algorithms suffer from. In particular, poor performance when\nunlabeled and labeled data distributions differ. To address these observations,\nwe develop RealMix, which achieves state-of-the-art results on standard\nbenchmark datasets across different labeled and unlabeled set sizes while\novercoming the aforementioned challenges. Notably, RealMix achieves an error\nrate of 9.79% on CIFAR10 with 250 labels and is the only SSL method tested able\nto surpass baseline performance when there is significant mismatch in the\nlabeled and unlabeled data distributions. RealMix demonstrates how SSL can be\nused in real world situations with limited access to both data and compute and\nguides further research in SSL with practical applicability in mind.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:03:28 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nair", "Varun", ""], ["Alonso", "Javier Fuentes", ""], ["Beltramelli", "Tony", ""]]}, {"id": "1912.08771", "submitter": "Nick Johnston", "authors": "Nick Johnston, Elad Eban, Ariel Gordon, Johannes Ball\\'e", "title": "Computationally Efficient Neural Image Compression", "comments": "In submission to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression using neural networks have reached or exceeded non-neural\nmethods (such as JPEG, WebP, BPG). While these networks are state of the art in\nratedistortion performance, computational feasibility of these models remains a\nchallenge. We apply automatic network optimization techniques to reduce the\ncomputational complexity of a popular architecture used in neural image\ncompression, analyze the decoder complexity in execution runtime and explore\nthe trade-offs between two distortion metrics, rate-distortion performance and\nrun-time performance to design and research more computationally efficient\nneural image compression. We find that our method decreases the decoder\nrun-time requirements by over 50% for a stateof-the-art neural architecture.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:09:20 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Johnston", "Nick", ""], ["Eban", "Elad", ""], ["Gordon", "Ariel", ""], ["Ball\u00e9", "Johannes", ""]]}, {"id": "1912.08776", "submitter": "Byungsoo Kim", "authors": "Simon Biland, Vinicius C. Azevedo, Byungsoo Kim and Barbara\n  Solenthaler", "title": "Frequency-Aware Reconstruction of Fluid Simulations with Generative\n  Networks", "comments": "Submitted to Eurographics2020", "journal-ref": "Eurographics 2020 - Short Papers", "doi": "10.2312/egs.20201019", "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks were recently employed to fully reconstruct\nfluid simulation data from a set of reduced parameters. However, since\n(de-)convolutions traditionally trained with supervised L1-loss functions do\nnot discriminate between low and high frequencies in the data, the error is not\nminimized efficiently for higher bands. This directly correlates with the\nquality of the perceived results, since missing high frequency details are\neasily noticeable. In this paper, we analyze the reconstruction quality of\ngenerative networks and present a frequency-aware loss function that is able to\nfocus on specific bands of the dataset during training time. We show that our\napproach improves reconstruction quality of fluid simulation data in\nmid-frequency bands, yielding perceptually better results while requiring\ncomparable training time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:13:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Biland", "Simon", ""], ["Azevedo", "Vinicius C.", ""], ["Kim", "Byungsoo", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1912.08791", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov", "title": "Forecasting significant stock price changes using neural networks", "comments": null, "journal-ref": "Neural Computing and Applications (2020)", "doi": "10.1007/s00521-020-04942-3", "report-no": null, "categories": "q-fin.TR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock price prediction is a rich research topic that has attracted interest\nfrom various areas of science. The recent success of machine learning in speech\nand image recognition has prompted researchers to apply these methods to asset\nprice prediction. The majority of literature has been devoted to predicting\neither the actual asset price or the direction of price movement. In this\npaper, we study a hitherto little explored question of predicting significant\nchanges in stock price based on previous changes using machine learning\nalgorithms. We are particularly interested in the performance of neural network\nclassifiers in the given context. To this end, we construct and test three\nneural network models including multi-layer perceptron, convolutional net, and\nlong short term memory net. As benchmark models we use random forest and\nrelative strength index methods. The models are tested using 10-year daily\nstock price data of four major US public companies. Test results show that\npredicting significant changes in stock price can be accomplished with a high\ndegree of accuracy. In particular, we obtain substantially better results than\nsimilar studies that forecast the direction of price change.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:48:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kamalov", "Firuz", ""]]}, {"id": "1912.08792", "submitter": "Jing Li", "authors": "Soroosh Khoram, Jing Li", "title": "TOCO: A Framework for Compressing Neural Network Models Based on\n  Tolerance Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network compression methods have enabled deploying large models on\nemerging edge devices with little cost, by adapting already-trained models to\nthe constraints of these devices. The rapid development of AI-capable edge\ndevices with limited computation and storage requires streamlined methodologies\nthat can efficiently satisfy the constraints of different devices. In contrast,\nexisting methods often rely on heuristic and manual adjustments to maintain\naccuracy, support only coarse compression policies, or target specific device\nconstraints that limit their applicability. We address these limitations by\nproposing the TOlerance-based COmpression (TOCO) framework. TOCO uses an\nin-depth analysis of the model, to maintain the accuracy, in an active learning\nsystem. The results of the analysis are tolerances that can be used to perform\ncompression in a fine-grained manner. Finally, by decoupling compression from\nthe tolerance analysis, TOCO allows flexibility to changes in the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:45:47 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 01:40:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Khoram", "Soroosh", ""], ["Li", "Jing", ""]]}, {"id": "1912.08795", "submitter": "Hongxu Yin", "authors": "Hongxu Yin, Pavlo Molchanov, Zhizhong Li, Jose M. Alvarez, Arun\n  Mallya, Derek Hoiem, Niraj K. Jha and Jan Kautz", "title": "Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepInversion, a new method for synthesizing images from the\nimage distribution used to train a deep neural network. We 'invert' a trained\nnetwork (teacher) to synthesize class-conditional input images starting from\nrandom noise, without using any additional information about the training\ndataset. Keeping the teacher fixed, our method optimizes the input while\nregularizing the distribution of intermediate feature maps using information\nstored in the batch normalization layers of the teacher. Further, we improve\nthe diversity of synthesized images using Adaptive DeepInversion, which\nmaximizes the Jensen-Shannon divergence between the teacher and student network\nlogits. The resulting synthesized images from networks trained on the CIFAR-10\nand ImageNet datasets demonstrate high fidelity and degree of realism, and help\nenable a new breed of data-free applications - ones that do not require any\nreal images or labeled data. We demonstrate the applicability of our proposed\nmethod to three tasks of immense practical importance -- (i) data-free network\npruning, (ii) data-free knowledge transfer, and (iii) data-free continual\nlearning. Code is available at https://github.com/NVlabs/DeepInversion\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:50:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:30:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yin", "Hongxu", ""], ["Molchanov", "Pavlo", ""], ["Li", "Zhizhong", ""], ["Alvarez", "Jose M.", ""], ["Mallya", "Arun", ""], ["Hoiem", "Derek", ""], ["Jha", "Niraj K.", ""], ["Kautz", "Jan", ""]]}, {"id": "1912.08808", "submitter": "Artem Lutov", "authors": "Artem Lutov, Dingqi Yang and Philippe Cudr\\'e-Mauroux", "title": "Bridging the Gap between Community and Node Representations: Graph\n  Embedding via Community Detection", "comments": "IEEE BigData'19, Special Session on Information Granulation in Data\n  Science and Scalable Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph embedding has become a key component of many data mining and analysis\nsystems. Current graph embedding approaches either sample a large number of\nnode pairs from a graph to learn node embeddings via stochastic optimization or\nfactorize a high-order proximity/adjacency matrix of the graph via\ncomputationally expensive matrix factorization techniques. These approaches\ntypically require significant resources for the learning process and rely on\nmultiple parameters, which limits their applicability in practice. Moreover,\nmost of the existing graph embedding techniques operate effectively in one\nspecific metric space only (e.g., the one produced with cosine similarity), do\nnot preserve higher-order structural features of the input graph and cannot\nautomatically determine a meaningful number of embedding dimensions. Typically,\nthe produced embeddings are not easily interpretable, which complicates further\nanalyses and limits their applicability. To address these issues, we propose\nDAOR, a highly efficient and parameter-free graph embedding technique producing\nmetric space-robust, compact and interpretable embeddings without any manual\ntuning. Compared to a dozen state-of-the-art graph embedding algorithms, DAOR\nyields competitive results on both node classification (which benefits form\nhigh-order proximity) and link prediction (which relies on low-order proximity\nmostly). Unlike existing techniques, however, DAOR does not require any\nparameter tuning and improves the embeddings generation speed by several orders\nof magnitude. Our approach has hence the ambition to greatly simplify and speed\nup data analysis tasks involving graph representation learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:14:48 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Lutov", "Artem", ""], ["Yang", "Dingqi", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1912.08809", "submitter": "Joy Bose", "authors": "Joy Bose", "title": "Field Label Prediction for Autofill in Web Browsers", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic form fill is an important productivity related feature present in\nmajor web browsers, which predicts the field labels of a web form and\nautomatically fills values in a new form based on the values previously filled\nfor the same field in other forms. This feature increases the convenience and\nefficiency of users who have to fill similar information in fields in multiple\nforms. In this paper we describe a machine learning solution for predicting the\nform field labels, implemented as a web service using Azure ML Studio.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 22:55:06 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bose", "Joy", ""]]}, {"id": "1912.08843", "submitter": "Taehee Lee", "authors": "Taehee Lee and Charles E. Lawrence", "title": "Heteroscedastic Gaussian Process Regression on the Alkenone over Sea\n  Surface Temperatures", "comments": "This article has been submitted to \"Dec 2019, Proceedings of the 9th\n  International Workshop on Climate Informatics: CI 2019. NCAR Technical Note\n  NCAR/TN-561+PROC\"", "journal-ref": null, "doi": "10.5065/y82j-f154", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To restore the historical sea surface temperatures (SSTs) better, it is\nimportant to construct a good calibration model for the associated proxies. In\nthis paper, we introduce a new model for alkenone (${\\rm{U}}_{37}^{\\rm{K}'}$)\nbased on the heteroscedastic Gaussian process (GP) regression method. Our\nnonparametric approach not only deals with the variable pattern of noises over\nSSTs but also contains a Bayesian method of classifying potential outliers.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:20:33 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lee", "Taehee", ""], ["Lawrence", "Charles E.", ""]]}, {"id": "1912.08860", "submitter": "Emmanuel Kahembwe", "authors": "Emmanuel Kahembwe, Subramanian Ramamoorthy", "title": "Lower Dimensional Kernels for Video Discriminators", "comments": null, "journal-ref": "Neural.Networks 132 (2020) 506-520", "doi": "10.1016/j.neunet.2020.09.016", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an analysis of the discriminators used in Generative\nAdversarial Networks (GANs) for Video. We show that unconstrained video\ndiscriminator architectures induce a loss surface with high curvature which\nmake optimisation difficult. We also show that this curvature becomes more\nextreme as the maximal kernel dimension of video discriminators increases. With\nthese observations in hand, we propose a family of efficient Lower-Dimensional\nVideo Discriminators for GANs (LDVD GANs). The proposed family of\ndiscriminators improve the performance of video GAN models they are applied to\nand demonstrate good performance on complex and diverse datasets such as\nUCF-101. In particular, we show that they can double the performance of\nTemporal-GANs and provide for state-of-the-art performance on a single GPU.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:54:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kahembwe", "Emmanuel", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1912.08864", "submitter": "Sheroze Sheriffdeen", "authors": "Sheroze Sheriffdeen, Jean C. Ragusa, Jim E. Morel, Marvin L. Adams,\n  Tan Bui-Thanh", "title": "Accelerating PDE-constrained Inverse Solutions with Deep Learning and\n  Reduced Order Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems are pervasive mathematical methods in inferring knowledge\nfrom observational and experimental data by leveraging simulations and models.\nUnlike direct inference methods, inverse problem approaches typically require\nmany forward model solves usually governed by Partial Differential Equations\n(PDEs). This a crucial bottleneck in determining the feasibility of such\nmethods. While machine learning (ML) methods, such as deep neural networks\n(DNNs), can be employed to learn nonlinear forward models, designing a network\narchitecture that preserves accuracy while generalizing to new parameter\nregimes is a daunting task. Furthermore, due to the computation-expensive\nnature of forward models, state-of-the-art black-box ML methods would require\nan unrealistic amount of work in order to obtain an accurate surrogate model.\nOn the other hand, standard Reduced-Order Models (ROMs) accurately capture\nsupposedly important physics of the forward model in the reduced subspaces, but\notherwise could be inaccurate elsewhere. In this paper, we propose to enlarge\nthe validity of ROMs and hence improve the accuracy outside the reduced\nsubspaces by incorporating a data-driven ML technique. In particular, we focus\non a goal-oriented approach that substantially improves the accuracy of reduced\nmodels by learning the error between the forward model and the ROM outputs.\nOnce an ML-enhanced ROM is constructed it can accelerate the performance of\nsolving many-query problems in parametrized forward and inverse problems.\nNumerical results for inverse problems governed by elliptic PDEs and\nparametrized neutron transport equations will be presented to support our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:45:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Sheriffdeen", "Sheroze", ""], ["Ragusa", "Jean C.", ""], ["Morel", "Jim E.", ""], ["Adams", "Marvin L.", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1912.08865", "submitter": "Zetong Qi", "authors": "Zetong Qi, T.J. Wilder", "title": "Adversarial VC-dimension and Sample Complexity of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks during the testing phase of neural networks pose a\nchallenge for the deployment of neural networks in security critical settings.\nThese attacks can be performed by adding noise that is imperceptible to humans\non top of the original data. By doing so, an attacker can create an adversarial\nsample, which will cause neural networks to misclassify. In this paper, we seek\nto understand the theoretical limits of what can be learned by neural networks\nin the presence of an adversary. We first defined the hypothesis space of a\nneural network, and showed the relationship between the growth number of the\nentire neural network and the growth number of each neuron. Combine that with\nthe adversarial Vapnik-Chervonenkis(VC)-dimension of halfspace classifiers, we\nconcluded the adversarial VC-dimension of the neural networks with sign\nactivation functions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:10:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Qi", "Zetong", ""], ["Wilder", "T. J.", ""]]}, {"id": "1912.08866", "submitter": "James Harrison", "authors": "James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone", "title": "Continuous Meta-Learning without Tasks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:10:40 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 00:14:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Harrison", "James", ""], ["Sharma", "Apoorva", ""], ["Finn", "Chelsea", ""], ["Pavone", "Marco", ""]]}, {"id": "1912.08868", "submitter": "Rashid Mehdiyev Dr", "authors": "Rashid Mehdiyev, Jean Nava, Karan Sodhi, Saurav Acharya, Annie Ibrahim\n  Rana", "title": "Topic subject creation using unsupervised learning for topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the use of Non-Negative Matrix Factorization (NMF) and Latent\nDirichlet Allocation (LDA) algorithms to perform topic mining and labelling\napplied to retail customer communications in attempt to characterize the\nsubject of customers inquiries. In this paper we compare both algorithms in the\ntopic mining performance and propose methods to assign topic subject labels in\nan automated way.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:11:03 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Mehdiyev", "Rashid", ""], ["Nava", "Jean", ""], ["Sodhi", "Karan", ""], ["Acharya", "Saurav", ""], ["Rana", "Annie Ibrahim", ""]]}, {"id": "1912.08869", "submitter": "Neil Dhir", "authors": "Mathias Edman and Neil Dhir", "title": "Boltzmann Exploration Expectation-Maximisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general method for fitting finite mixture models (FMM). Learning\nin a mixture model consists of finding the most likely cluster assignment for\neach data-point, as well as finding the parameters of the clusters themselves.\nIn many mixture models, this is difficult with current learning methods, where\nthe most common approach is to employ monotone learning algorithms e.g. the\nconventional expectation-maximisation algorithm. While effective, the success\nof any monotone algorithm is crucially dependant on good parameter\ninitialisation, where a common choice is $K$-means initialisation, commonly\nemployed for Gaussian mixture models.\n  For other types of mixture models, the path to good initialisation parameters\nis often unclear and may require a problem-specific solution. To this end, we\npropose a general heuristic learning algorithm that utilises Boltzmann\nexploration to assign each observation to a specific base distribution within\nthe mixture model, which we call Boltzmann exploration expectation-maximisation\n(BEEM). With BEEM, hard assignments allow straight forward parameter learning\nfor each base distribution by conditioning only on its assigned observations.\nConsequently, it can be applied to mixtures of any base distribution where\nsingle component parameter learning is tractable. The stochastic learning\nprocedure is able to escape local optima and is thus insensitive to parameter\ninitialisation. We show competitive performance on a number of synthetic\nbenchmark cases as well as on real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:15:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Edman", "Mathias", ""], ["Dhir", "Neil", ""]]}, {"id": "1912.08881", "submitter": "Sebastian Lapuschkin", "authors": "Seul-Ki Yeom, Philipp Seegerer, Sebastian Lapuschkin, Alexander\n  Binder, Simon Wiedemann, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning", "comments": "25 pages + 5 supplementary pages, 13 figures, 6 tables", "journal-ref": "Pattern Recognition, Volume 115, pp.107899, 2021", "doi": "10.1016/j.patcog.2021.107899", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of convolutional neural networks (CNNs) in various applications\nis accompanied by a significant increase in computation and parameter storage\ncosts. Recent efforts to reduce these overheads involve pruning and compressing\nthe weights of various layers while at the same time aiming to not sacrifice\nperformance. In this paper, we propose a novel criterion for CNN pruning\ninspired by neural network interpretability: The most relevant units, i.e.\nweights or filters, are automatically found using their relevance scores\nobtained from concepts of explainable AI (XAI). By exploring this idea, we\nconnect the lines of interpretability and model compression research. We show\nthat our proposed method can efficiently prune CNN models in transfer-learning\nsetups in which networks pre-trained on large corpora are adapted to\nspecialized tasks. The method is evaluated on a broad range of computer vision\ndatasets. Notably, our novel criterion is not only competitive or better\ncompared to state-of-the-art pruning criteria when successive retraining is\nperformed, but clearly outperforms these previous criteria in the\nresource-constrained application scenario in which the data of the task to be\ntransferred to is very scarce and one chooses to refrain from fine-tuning. Our\nmethod is able to compress the model iteratively while maintaining or even\nimproving accuracy. At the same time, it has a computational cost in the order\nof gradient computation and is comparatively simple to apply without the need\nfor tuning hyperparameters for pruning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:42:30 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 14:43:54 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 11:30:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yeom", "Seul-Ki", ""], ["Seegerer", "Philipp", ""], ["Lapuschkin", "Sebastian", ""], ["Binder", "Alexander", ""], ["Wiedemann", "Simon", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1912.08910", "submitter": "Nutta Homdee", "authors": "Nutta Homdee, Mehdi Boukhechba, Yixue W. Feng, Natalie Kramer, John\n  Lach, Laura E. Barnes", "title": "Enabling Smartphone-based Estimation of Heart Rate", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous, ubiquitous monitoring through wearable sensors has the potential\nto collect useful information about users' context. Heart rate is an important\nphysiologic measure used in a wide variety of applications, such as fitness\ntracking and health monitoring. However, wearable sensors that monitor heart\nrate, such as smartwatches and electrocardiogram (ECG) patches, can have gaps\nin their data streams because of technical issues (e.g., bad wireless channels,\nbattery depletion, etc.) or user-related reasons (e.g. motion artifacts, user\ncompliance, etc.). The ability to use other available sensor data (e.g.,\nsmartphone data) to estimate missing heart rate readings is useful to cope with\nany such gaps, thus improving data quality and continuity. In this paper, we\ntest the feasibility of estimating raw heart rate using smartphone sensor data.\nUsing data generated by 12 participants in a one-week study period, we were\nable to build both personalized and generalized models using regression, SVM,\nand random forest algorithms. All three algorithms outperformed the baseline\nmoving-average interpolation method for both personalized and generalized\nsettings. Moreover, our findings suggest that personalized models outperformed\nthe generalized models, which speaks to the importance of considering personal\nphysiology, behavior, and life style in the estimation of heart rate. The\npromising results provide preliminary evidence of the feasibility of combining\nsmartphone sensor data with wearable sensor data for continuous heart rate\nmonitoring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:00:19 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Homdee", "Nutta", ""], ["Boukhechba", "Mehdi", ""], ["Feng", "Yixue W.", ""], ["Kramer", "Natalie", ""], ["Lach", "John", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1912.08914", "submitter": "Istv\\'an Ketyk\\'o", "authors": "Istv\\'an Ketyk\\'o and Ferenc Kov\\'acs", "title": "On the Metrics and Adaptation Methods for Domain Divergences of\n  sEMG-based Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new metric to measure domain divergence and a new domain\nadaptation method for time-series classification. The metric belongs to the\nclass of probability distributions-based metrics, is transductive, and does not\nassume the presence of source data samples. The 2-stage method utilizes an\nimproved autoregressive, RNN-based architecture with deep/non-linear\ntransformation. We assess our metric and the performance of our model in the\ncontext of sEMG/EMG-based gesture recognition under inter-session and\ninter-subject domain shifts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:12:53 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Ketyk\u00f3", "Istv\u00e1n", ""], ["Kov\u00e1cs", "Ferenc", ""]]}, {"id": "1912.08920", "submitter": "Sakshi Udeshi", "authors": "Sakshi Udeshi, Xingbin Jiang, Sudipta Chattopadhyay", "title": "Callisto: Entropy based test generation and data quality assessment for\n  Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine Learning (ML) has seen massive progress in the last decade and as a\nresult, there is a pressing need for validating ML-based systems. To this end,\nwe propose, design and evaluate CALLISTO - a novel test generation and data\nquality assessment framework. To the best of our knowledge, CALLISTO is the\nfirst blackbox framework to leverage the uncertainty in the prediction and\nsystematically generate new test cases for ML classifiers. Our evaluation of\nCALLISTO on four real world data sets reveals thousands of errors. We also show\nthat leveraging the uncertainty in prediction can increase the number of\nerroneous test cases up to a factor of 20, as compared to when no such\nknowledge is used for testing.\n  CALLISTO has the capability to detect low quality data in the datasets that\nmay contain mislabelled data. We conduct and present an extensive user study to\nvalidate the results of CALLISTO on identifying low quality data from four\nstate-of-the-art real world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:20:18 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Udeshi", "Sakshi", ""], ["Jiang", "Xingbin", ""], ["Chattopadhyay", "Sudipta", ""]]}, {"id": "1912.08921", "submitter": "Diego Pinheiro", "authors": "Diego Pinheiro, Ryan Hartman, Erick Romero, Ronaldo Menezes, Martin\n  Cadeiras", "title": "Network-Based Delineation of Health Service Areas: A Comparative\n  Analysis of Community Detection Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Health Service Area (HSA) is a group of geographic regions served by\nsimilar health care facilities. The delineation of HSAs plays a pivotal role in\nthe characterization of health care services available in an area, enabling a\nbetter planning and regulation of health care services. Though Dartmouth HSAs\nhave been the standard delineation for decades, previous work has recently\nshown an improved HSA delineation using a network-based approach, in which HSAs\nare the communities extracted by the Louvain algorithm in hospital-patient\ndischarge networks. Given the existent heterogeneity of communities extracted\nby different community detection algorithms, a comparative analysis of\ncommunity detection algorithms for optimal HSA delineation is lacking. In this\nwork, we compared HSA delineations produced by community detection algorithms\nusing a large-scale dataset containing different types of hospital-patient\ndischarges spanning a 7-year period in US. Our results replicated the\nheterogeneity among community detection algorithms found in previous works, the\nimproved HSA delineation obtained by a network-based, and suggested that\nInfomap may be a more suitable community detection for HSA delineation since it\nfinds a high number of HSAs with high localization index and a low network\nconductance.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 18:35:23 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Pinheiro", "Diego", ""], ["Hartman", "Ryan", ""], ["Romero", "Erick", ""], ["Menezes", "Ronaldo", ""], ["Cadeiras", "Martin", ""]]}, {"id": "1912.08922", "submitter": "Nuno Moniz", "authors": "Nuno Moniz", "title": "Real-time 2019 Portuguese Parliament Election Results Dataset", "comments": "Dataset Descriptor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a data set describing the evolution of results in the\nPortuguese Parliamentary Elections of October 6$^{th}$ 2019. The data spans a\ntime interval of 4 hours and 25 minutes, in intervals of 5 minutes, concerning\nthe results of the 27 parties involved in the electoral event. The data set is\ntailored for predictive modelling tasks, mostly focused on numerical\nforecasting tasks. Regardless, it allows for other tasks such as ordinal\nregression or learn-to-rank.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:52:31 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Moniz", "Nuno", ""]]}, {"id": "1912.08926", "submitter": "Sardar Hamidian", "authors": "Sardar Hamidian and Mona T Diab", "title": "Rumor Detection and Classification for Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the pervasiveness of online media data as a source of information\nverifying the validity of this information is becoming even more important yet\nquite challenging. Rumors spread a large quantity of misinformation on\nmicroblogs. In this study we address two common issues within the context of\nmicroblog social media. First we detect rumors as a type of misinformation\npropagation and next we go beyond detection to perform the task of rumor\nclassification. WE explore the problem using a standard data set. We devise\nnovel features and study their impact on the task. We experiment with various\nlevels of preprocessing as a precursor of the classification as well as\ngrouping of features. We achieve and f-measure of over 0.82 in RDC task in\nmixed rumors data set and 84 percent in a single rumor data set using a\ntwo-step classification approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:55:55 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Hamidian", "Sardar", ""], ["Diab", "Mona T", ""]]}, {"id": "1912.08927", "submitter": "Peiyuan Sun", "authors": "Peiyuan Sun", "title": "Hyperbolic Multiplex Network Embedding with Maps of Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on network embedding in hyperbolic space have proven\nsuccessful in several applications. However, nodes in real world networks tend\nto interact through several distinct channels. Simple aggregation or ignorance\nof this multiplexity will lead to misleading results. On the other hand, there\nexists redundant information between different interaction patterns between\nnodes. Recent research reveals the analogy between the community structure and\nthe hyperbolic coordinate. To learn each node's effective embedding\nrepresentation while reducing the redundancy of multiplex network, we then\npropose a unified framework combing multiplex network hyperbolic embedding and\nmultiplex community detection. The intuitive rationale is that high order node\nembedding approach is expected to alleviate the observed network's sparse and\nnoisy structure which will benefit the community detection task. On the\ncontrary, the improved community structure will also guide the node embedding\ntask. To incorporate the common features between channels while preserving\nunique features, a random walk approach which traversing in latent multiplex\nhyperbolic space is proposed to detect the community across channels and bridge\nthe connection between node embedding and community detection. The proposed\nframework is evaluated on several network tasks using different real world\ndataset. The results demonstrates that our framework is effective and\nefficiency compared with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 10:39:08 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 17:23:06 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sun", "Peiyuan", ""]]}, {"id": "1912.08949", "submitter": "Chaopeng Shen", "authors": "Dapeng Feng, Kuai Fang, and Chaopeng Shen", "title": "Enhancing streamflow forecast and extracting insights using long-short\n  term memory networks with data integration at continental scales", "comments": null, "journal-ref": "Water Resources Research, 2020", "doi": "10.1029/2019WR026793", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent observations with varied schedules and types (moving average,\nsnapshot, or regularly spaced) can help to improve streamflow forecasts, but it\nis challenging to integrate them effectively. Based on a long short-term memory\n(LSTM) streamflow model, we tested multiple versions of a flexible procedure we\ncall data integration (DI) to leverage recent discharge measurements to improve\nforecasts. DI accepts lagged inputs either directly or through a convolutional\nneural network (CNN) unit. DI ubiquitously elevated streamflow forecast\nperformance to unseen levels, reaching a record continental-scale median\nNash-Sutcliffe Efficiency coefficient value of 0.86. Integrating moving-average\ndischarge, discharge from the last few days, or even average discharge from the\nprevious calendar month could all improve daily forecasts. Directly using\nlagged observations as inputs was comparable in performance to using the CNN\nunit. Importantly, we obtained valuable insights regarding hydrologic processes\nimpacting LSTM and DI performance. Before applying DI, the base LSTM model\nworked well in mountainous or snow-dominated regions, but less well in regions\nwith low discharge volumes (due to either low precipitation or high\nprecipitation-energy synchronicity) and large inter-annual storage variability.\nDI was most beneficial in regions with high flow autocorrelation: it greatly\nreduced baseflow bias in groundwater-dominated western basins and also improved\npeak prediction for basins with dynamical surface water storage, such as the\nPrairie Potholes or Great Lakes regions. However, even DI cannot elevate\nhigh-aridity basins with one-day flash peaks. Despite this limitation, there is\nmuch promise for a deep-learning-based forecast paradigm due to its\nperformance, automation, efficiency, and flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:44:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 19:22:43 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 12:26:30 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Feng", "Dapeng", ""], ["Fang", "Kuai", ""], ["Shen", "Chaopeng", ""]]}, {"id": "1912.08957", "submitter": "Ruoyu Sun", "authors": "Ruoyu Sun", "title": "Optimization for deep learning: theory and algorithms", "comments": "38 pages of main body; 5 pages of appendix; 12 pages of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When and why can a neural network be successfully trained? This article\nprovides an overview of optimization algorithms and theory for training neural\nnetworks. First, we discuss the issue of gradient explosion/vanishing and the\nmore general issue of undesirable spectrum, and then discuss practical\nsolutions including careful initialization and normalization methods. Second,\nwe review generic optimization methods used in training neural networks, such\nas SGD, adaptive gradient methods and distributed methods, and theoretical\nresults for these algorithms. Third, we review existing research on the global\nissues of neural network training, including results on bad local minima, mode\nconnectivity, lottery ticket hypothesis and infinite-width analysis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 00:23:18 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Sun", "Ruoyu", ""]]}, {"id": "1912.08974", "submitter": "Stefanie G\\\"unther", "authors": "Eric C. Cyr and Stefanie G\\\"unther and Jacob B. Schroder", "title": "Multilevel Initialization for Layer-Parallel Deep Neural Network\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates multilevel initialization strategies for training\nvery deep neural networks with a layer-parallel multigrid solver. The scheme is\nbased on the continuous interpretation of the training problem as a problem of\noptimal control, in which neural networks are represented as discretizations of\ntime-dependent ordinary differential equations. A key goal is to develop a\nmethod able to intelligently initialize the network parameters for the very\ndeep networks enabled by scalable layer-parallel training. To do this, we apply\na refinement strategy across the time domain, that is equivalent to refining in\nthe layer dimension. The resulting refinements create deep networks, with good\ninitializations for the network parameters coming from the coarser trained\nnetworks. We investigate the effectiveness of such multilevel \"nested\niteration\" strategies for network training, showing supporting numerical\nevidence of reduced run time for equivalent accuracy. In addition, we study\nwhether the initialization strategies provide a regularizing effect on the\noverall training process and reduce sensitivity to hyperparameters and\nrandomness in initial network parameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:28:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Cyr", "Eric C.", ""], ["G\u00fcnther", "Stefanie", ""], ["Schroder", "Jacob B.", ""]]}, {"id": "1912.08986", "submitter": "Nicholas Roberts", "authors": "Nicholas Roberts, Dian Ang Yap, Vinay Uday Prabhu", "title": "Deep Connectomics Networks: Neural Network Architectures Inspired by\n  Neuronal Networks", "comments": "Presented at the Real Neurons & Hidden Units Workshop, 33rd\n  Conference on Neural Information ProcessingSystems (NeurIPS 2019), Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between inter-neuronal network topology and cognition has been\nstudied deeply by connectomics researchers and network scientists, which is\ncrucial towards understanding the remarkable efficacy of biological neural\nnetworks. Curiously, the deep learning revolution that revived neural networks\nhas not paid much attention to topological aspects. The architectures of deep\nneural networks (DNNs) do not resemble their biological counterparts in the\ntopological sense. We bridge this gap by presenting initial results of Deep\nConnectomics Networks (DCNs) as DNNs with topologies inspired by real-world\nneuronal networks. We show high classification accuracy obtained by DCNs whose\narchitecture was inspired by the biological neuronal networks of C. Elegans and\nthe mouse visual cortex.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:59:20 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Roberts", "Nicholas", ""], ["Yap", "Dian Ang", ""], ["Prabhu", "Vinay Uday", ""]]}, {"id": "1912.08987", "submitter": "Nicholas Roberts", "authors": "Nicholas Roberts, Vinay Uday Prabhu, Matthew McAteer", "title": "Model Weight Theft With Just Noise Inputs: The Curious Case of the\n  Petulant Attacker", "comments": "Presented at the Security and Privacy of Machine Learning Workshop,\n  36th International Conference on Machine Learning (ICML 2019), Long Beach,\n  California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the scenarios under which an attacker can claim that\n'Noise and access to the softmax layer of the model is all you need' to steal\nthe weights of a convolutional neural network whose architecture is already\nknown. We were able to achieve 96% test accuracy using the stolen MNIST model\nand 82% accuracy using the stolen KMNIST model learned using only i.i.d.\nBernoulli noise inputs. We posit that this theft-susceptibility of the weights\nis indicative of the complexity of the dataset and propose a new metric that\ncaptures the same. The goal of this dissemination is to not just showcase how\nfar knowing the architecture can take you in terms of model stealing, but to\nalso draw attention to this rather idiosyncratic weight learnability aspects of\nCNNs spurred by i.i.d. noise input. We also disseminate some initial results\nobtained with using the Ising probability distribution in lieu of the i.i.d.\nBernoulli distribution.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:59:59 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Roberts", "Nicholas", ""], ["Prabhu", "Vinay Uday", ""], ["McAteer", "Matthew", ""]]}, {"id": "1912.08993", "submitter": "Bai Jiang", "authors": "Bai Jiang, Qiang Sun", "title": "Bayesian high-dimensional linear regression with generic spike-and-slab\n  priors", "comments": "17 pages for main file, 13 pages for appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-and-slab priors are popular Bayesian solutions for high-dimensional\nlinear regression problems. Previous theoretical studies on spike-and-slab\nmethods focus on specific prior formulations and use prior-dependent conditions\nand analyses, and thus can not be generalized directly. In this paper, we\npropose a class of generic spike-and-slab priors and develop a unified\nframework to rigorously assess their theoretical properties. Technically, we\nprovide general conditions under which generic spike-and-slab priors can\nachieve the nearly-optimal posterior contraction rate and the model selection\nconsistency. Our results include those of Narisetty and He (2014) and Castillo\net al. (2015) as special cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 02:42:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:03:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Jiang", "Bai", ""], ["Sun", "Qiang", ""]]}, {"id": "1912.09002", "submitter": "Eduardo Mendes", "authors": "Ricardo P. Masini and Marcelo C. Medeiros and Eduardo F. Mendes", "title": "Regularized Estimation of High-Dimensional Vector AutoRegressions with\n  Weakly Dependent Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable advance in understanding the properties of sparse\nregularization procedures in high-dimensional models. In time series context,\nit is mostly restricted to Gaussian autoregressions or mixing sequences. We\nstudy oracle properties of LASSO estimation of weakly sparse\nvector-autoregressive models with heavy tailed, weakly dependent innovations\nwith virtually no assumption on the conditional heteroskedasticity. In contrast\nto current literature, our innovation process satisfy an $L^1$ mixingale type\ncondition on the centered conditional covariance matrices. This condition\ncovers $L^1$-NED sequences and strong ($\\alpha$-) mixing sequences as\nparticular examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:19:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 15:56:40 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 01:17:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""], ["Mendes", "Eduardo F.", ""]]}, {"id": "1912.09007", "submitter": "Pedro Sequeira", "authors": "Pedro Sequeira and Melinda Gervasio", "title": "Interestingness Elements for Explainable Reinforcement Learning:\n  Understanding Agents' Capabilities and Limitations", "comments": "To appear in: Artificial Intelligence", "journal-ref": null, "doi": "10.1016/j.artint.2020.103367", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an explainable reinforcement learning (XRL) framework that\nanalyzes an agent's history of interaction with the environment to extract\ninterestingness elements that help explain its behavior. The framework relies\non data readily available from standard RL algorithms, augmented with data that\ncan easily be collected by the agent while learning. We describe how to create\nvisual summaries of an agent's behavior in the form of short video-clips\nhighlighting key interaction moments, based on the proposed elements. We also\nreport on a user study where we evaluated the ability of humans to correctly\nperceive the aptitude of agents with different characteristics, including their\ncapabilities and limitations, given visual summaries automatically generated by\nour framework. The results show that the diversity of aspects captured by the\ndifferent interestingness elements is crucial to help humans correctly\nunderstand an agent's strengths and limitations in performing a task, and\ndetermine when it might need adjustments to improve its performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:46:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 03:25:14 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sequeira", "Pedro", ""], ["Gervasio", "Melinda", ""]]}, {"id": "1912.09009", "submitter": "Ravdeep Pasricha", "authors": "Ravdeep Pasricha, Ekta Gujral and Evangelos E. Papalexakis", "title": "Adaptive Granularity in Tensors: A Quest for Interpretable Structure", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collected at very frequent intervals is usually extremely sparse and has\nno structure that is exploitable by modern tensor decomposition algorithms.\nThus the utility of such tensors is low, in terms of the amount of\ninterpretable and exploitable structure that one can extract from them. In this\npaper, we introduce the problem of finding a tensor of adaptive aggregated\ngranularity that can be decomposed to reveal meaningful latent concepts\n(structures) from datasets that, in their original form, are not amenable to\ntensor analysis. Such datasets fall under the broad category of sparse point\nprocesses that evolve over space and/or time. To the best of our knowledge,\nthis is the first work that explores adaptive granularity aggregation in\ntensors. Furthermore, we formally define the problem and discuss what different\ndefinitions of \"good structure\" can be in practice, and show that optimal\nsolution is of prohibitive combinatorial complexity. Subsequently, we propose\nan efficient and effective greedy algorithm which follows a number of intuitive\ndecision criteria that locally maximize the \"goodness of structure\", resulting\nin high-quality tensors. We evaluate our method on both semi-synthetic data\nwhere ground truth is known and real datasets for which we do not have any\nground truth. In both cases, our proposed method constructs tensors that have\nvery high structure quality. Finally, our proposed method is able to discover\ndifferent natural resolutions of a multi-aspect dataset, which can lead to\nmulti-resolution analysis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 04:07:42 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Pasricha", "Ravdeep", ""], ["Gujral", "Ekta", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1912.09015", "submitter": "Dongmyung Shin", "authors": "Dongmyung Shin, Sooyeon Ji, Doohee Lee, Jieun Lee, Se-Hong Oh, and\n  Jongho Lee", "title": "Deep Reinforcement Learning Designed Shinnar-Le Roux RF Pulse using\n  Root-Flipping: DeepRF_SLR", "comments": "Accepted at IEEE transactions on Medical Imaging\n  (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174664)", "journal-ref": null, "doi": "10.1109/TMI.2020.3018508", "report-no": null, "categories": "cs.LG cs.AI eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach of applying deep reinforcement learning to an RF pulse\ndesign is introduced. This method, which is referred to as DeepRF_SLR, is\ndesigned to minimize the peak amplitude or, equivalently, minimize the pulse\nduration of a multiband refocusing pulse generated by the Shinar Le-Roux (SLR)\nalgorithm. In the method, the root pattern of SLR polynomial, which determines\nthe RF pulse shape, is optimized by iterative applications of deep\nreinforcement learning and greedy tree search. When tested for the designs of\nthe multiband factors of three and seven RFs, DeepRF_SLR demonstrated improved\nperformance compared to conventional methods, generating shorter duration RF\npulses in shorter computational time. In the experiments, the RF pulse from\nDeepRF_SLR produced a slice profile similar to the minimum-phase SLR RF pulse\nand the profiles matched to that of the computer simulation. Our approach\nsuggests a new way of designing an RF by applying a machine learning algorithm,\ndemonstrating a machine-designed MRI sequence.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 04:51:57 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:20:04 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 08:46:31 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Shin", "Dongmyung", ""], ["Ji", "Sooyeon", ""], ["Lee", "Doohee", ""], ["Lee", "Jieun", ""], ["Oh", "Se-Hong", ""], ["Lee", "Jongho", ""]]}, {"id": "1912.09026", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage and Randy Paffenroth", "title": "Bounded Manifold Completion", "comments": "12 pages, 7 figures, submitted to Pattern Recognition Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dimensionality reduction or, equivalently, the approximation of\nhigh-dimensional data using a low-dimensional nonlinear manifold is an active\narea of research. In this paper, we will present a thematically different\napproach to detect the existence of a low-dimensional manifold of a given\ndimension that lies within a set of bounds derived from a given point cloud. A\nmatrix representing the appropriately defined distances on a low-dimensional\nmanifold is low-rank, and our method is based on current techniques for\nrecovering a partially observed matrix from a small set of fully observed\nentries that can be implemented as a low-rank Matrix Completion (MC) problem.\nMC methods are currently used to solve challenging real-world problems, such as\nimage inpainting and recommender systems, and we leverage extent efficient\noptimization techniques that use a nuclear norm convex relaxation as a\nsurrogate for non-convex and discontinuous rank minimization. Our proposed\nmethod provides several advantages over current nonlinear dimensionality\nreduction techniques, with the two most important being theoretical guarantees\non the detection of low-dimensional embeddings and robustness to non-uniformity\nin the sampling of the manifold. We validate the performance of this approach\nusing both a theoretical analysis as well as synthetic and real-world benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 05:42:21 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""]]}, {"id": "1912.09033", "submitter": "Zhongjie Yu", "authors": "Zhongjie Yu, Lin Chen, Zhongwei Cheng, Jiebo Luo", "title": "TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot\n  Learning", "comments": "Accepted at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successful application of deep learning to many visual recognition tasks\nrelies heavily on the availability of a large amount of labeled data which is\nusually expensive to obtain. The few-shot learning problem has attracted\nincreasing attention from researchers for building a robust model upon only a\nfew labeled samples. Most existing works tackle this problem under the\nmeta-learning framework by mimicking the few-shot learning task with an\nepisodic training strategy. In this paper, we propose a new transfer-learning\nframework for semi-supervised few-shot learning to fully utilize the auxiliary\ninformation from labeled base-class data and unlabeled novel-class data. The\nframework consists of three components: 1) pre-training a feature extractor on\nbase-class data; 2) using the feature extractor to initialize the classifier\nweights for the novel classes; and 3) further updating the model with a\nsemi-supervised learning method. Under the proposed framework, we develop a\nnovel method for semi-supervised few-shot learning called TransMatch by\ninstantiating the three components with Imprinting and MixMatch. Extensive\nexperiments on two popular benchmark datasets for few-shot learning,\nCUB-200-2011 and miniImageNet, demonstrate that our proposed method can\neffectively utilize the auxiliary information from labeled base-class data and\nunlabeled novel-class data to significantly improve the accuracy of few-shot\nlearning task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 06:50:45 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 19:25:18 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Yu", "Zhongjie", ""], ["Chen", "Lin", ""], ["Cheng", "Zhongwei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.09040", "submitter": "Zichen Zhang", "authors": "Zichen Zhang, Qingfeng Lan, Lei Ding, Yue Wang, Negar Hassanpour,\n  Russell Greiner", "title": "Reducing Selection Bias in Counterfactual Reasoning for Individual\n  Treatment Effects Estimation", "comments": "NeurIPS 2019 Workshop on \"Do the right thing\": machine learning and\n  causal inference for improved decision making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual reasoning is an important paradigm applicable in many fields,\nsuch as healthcare, economics, and education. In this work, we propose a novel\nmethod to address the issue of \\textit{selection bias}. We learn two groups of\nlatent random variables, where one group corresponds to variables that only\ncause selection bias, and the other group is relevant for outcome prediction.\nThey are learned by an auto-encoder where an additional regularized loss based\non Pearson Correlation Coefficient (PCC) encourages the de-correlation between\nthe two groups of random variables. This allows for explicitly alleviating\nselection bias by only keeping the latent variables that are relevant for\nestimating individual treatment effects. Experimental results on a synthetic\ntoy dataset and a benchmark dataset show that our algorithm is able to achieve\nstate-of-the-art performance and improve the result of its counterpart that\ndoes not explicitly model the selection bias.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 07:10:00 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhang", "Zichen", ""], ["Lan", "Qingfeng", ""], ["Ding", "Lei", ""], ["Wang", "Yue", ""], ["Hassanpour", "Negar", ""], ["Greiner", "Russell", ""]]}, {"id": "1912.09068", "submitter": "Diego Granziol", "authors": "Diego Granziol, Robin Ru, Stefan Zohren, Xiaowen Dong, Michael\n  Osborne, Stephen Roberts", "title": "A Maximum Entropy approach to Massive Graph Spectra", "comments": "12 pages. 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph spectral techniques for measuring graph similarity, or for learning the\ncluster number, require kernel smoothing. The choice of kernel function and\nbandwidth are typically chosen in an ad-hoc manner and heavily affect the\nresulting output. We prove that kernel smoothing biases the moments of the\nspectral density. We propose an information theoretically optimal approach to\nlearn a smooth graph spectral density, which fully respects the moment\ninformation. Our method's computational cost is linear in the number of edges,\nand hence can be applied to large networks, with millions of nodes. We apply\nour method to the problems to graph similarity and cluster number learning,\nwhere we outperform comparable iterative spectral approaches on synthetic and\nreal graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 08:48:48 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Granziol", "Diego", ""], ["Ru", "Robin", ""], ["Zohren", "Stefan", ""], ["Dong", "Xiaowen", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""]]}, {"id": "1912.09083", "submitter": "Anton Akusok", "authors": "Anton Akusok, Kaj-Mikael Bj\\\"ork, Leonardo Espinosa Leal, Yoan Miche,\n  Renjie Hu and Amaury Lendasse", "title": "Spiking Networks for Improved Cognitive Abilities of Edge Computing\n  Devices", "comments": null, "journal-ref": "Proceedings of the 12th ACM International Conference on PErvasive\n  Technologies Related to Assistive Environments (PETRA '19). ACM, New York,\n  NY, USA, 307-308. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This concept paper highlights a recently opened opportunity for large scale\nanalytical algorithms to be trained directly on edge devices. Such approach is\na response to the arising need of processing data generated by natural person\n(a human being), also known as personal data. Spiking Neural networks are the\ncore method behind it: suitable for a low latency energy-constrained hardware,\nenabling local training or re-training, while not taking advantage of\nscalability available in the Cloud.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:36:00 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Leal", "Leonardo Espinosa", ""], ["Miche", "Yoan", ""], ["Hu", "Renjie", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.09086", "submitter": "Alexis Bellot", "authors": "Alexis Bellot, Mihaela van der Schaar", "title": "A Bayesian Approach to Modelling Longitudinal Data in Electronic Health\n  Records", "comments": "Presented as an abstract at the Machine Learning for Health Workshop,\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing electronic health records (EHR) poses significant challenges\nbecause often few samples are available describing a patient's health and, when\navailable, their information content is highly diverse. The problem we consider\nis how to integrate sparsely sampled longitudinal data, missing measurements\ninformative of the underlying health status and fixed demographic information\nto produce estimated survival distributions updated through a patient's follow\nup. We propose a nonparametric probabilistic model that generates survival\ntrajectories from an ensemble of Bayesian trees that learns variable\ninteractions over time without specifying beforehand the longitudinal process.\nWe show performance improvements on Primary Biliary Cirrhosis patient data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:42:27 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bellot", "Alexis", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1912.09087", "submitter": "Anton Akusok", "authors": "Anton Akusok, Emil Eirola, Kaj-Mikael Bj\\\"ork and Amaury Lendasse", "title": "Extreme Learning Tree", "comments": null, "journal-ref": "Cao J., Vong C., Miche Y., Lendasse A. (eds) Proceedings of\n  ELM-2017. ELM 2017. Proceedings in Adaptation, Learning and Optimization, vol\n  10. Springer, Cham", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new variant of a decision tree, called an Extreme\nLearning Tree. It consists of an extremely random tree with non-linear data\ntransformation, and a linear observer that provides predictions based on the\nleaf index where the data samples fall. The proposed method outperforms linear\nmodels on a benchmark dataset, and may be a building block for a future variant\nof Random Forest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:43:11 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Eirola", "Emil", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.09090", "submitter": "Anton Akusok", "authors": "Anton Akusok, Yoan Miche, Kaj-Mikael Bj\\\"ork and Amaury Lendasse", "title": "Per-sample Prediction Intervals for Extreme Learning Machines", "comments": null, "journal-ref": "Int. J. Mach. Learn. & Cyber. (2019) 10: 991", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction intervals in supervised Machine Learning bound the region where\nthe true outputs of new samples may fall. They are necessary in the task of\nseparating reliable predictions of a trained model from near random guesses,\nminimizing the rate of False Positives, and other problem-specific tasks in\napplied Machine Learning. Many real problems have heteroscedastic stochastic\noutputs, which explains the need of input-dependent prediction intervals.\n  This paper proposes to estimate the input-dependent prediction intervals by a\nseparate Extreme Learning Machine model, using variance of its predictions as a\ncorrection term accounting for the model uncertainty. The variance is estimated\nfrom the model's linear output layer with a weighted Jackknife method. The\nmethodology is very fast, robust to heteroscedastic outputs, and handles both\nextremely large datasets and insufficient amount of training data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:50:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Miche", "Yoan", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.09091", "submitter": "Haifeng Li", "authors": "Jian Peng, Bo Tang, Hao Jiang, Zhuo Li, Yinjie Lei, Tao Lin, Haifeng\n  Li", "title": "Overcoming Long-term Catastrophic Forgetting through Adversarial Neural\n  Pruning and Synaptic Consolidation", "comments": "14 pages, 11 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": "10.1109/TNNLS.2021.3056201", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks face the well-known problem of catastrophic\nforgetting. What's worse, the degradation of previously learned skills becomes\nmore severe as the task sequence increases, known as the long-term catastrophic\nforgetting. It is due to two facts: first, as the model learns more tasks, the\nintersection of the low-error parameter subspace satisfying for these tasks\nbecomes smaller or even does not exist; second, when the model learns a new\ntask, the cumulative error keeps increasing as the model tries to protect the\nparameter configuration of previous tasks from interference. Inspired by the\nmemory consolidation mechanism in mammalian brains with synaptic plasticity, we\npropose a confrontation mechanism in which Adversarial Neural Pruning and\nsynaptic Consolidation (ANPyC) is used to overcome the long-term catastrophic\nforgetting issue. The neural pruning acts as long-term depression to prune\ntask-irrelevant parameters, while the novel synaptic consolidation acts as\nlong-term potentiation to strengthen task-relevant parameters. During the\ntraining, this confrontation achieves a balance in that only crucial parameters\nremain, and non-significant parameters are freed to learn subsequent tasks.\nANPyC avoids forgetting important information and makes the model efficient to\nlearn a large number of tasks. Specifically, the neural pruning iteratively\nrelaxes the current task's parameter conditions to expand the common parameter\nsubspace of the task; the synaptic consolidation strategy, which consists of a\nstructure-aware parameter-importance measurement and an element-wise parameter\nupdating strategy, decreases the cumulative error when learning new tasks. The\nfull source code is available at https://github.com/GeoX-Lab/ANPyC.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:51:54 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 08:42:07 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 12:41:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Peng", "Jian", ""], ["Tang", "Bo", ""], ["Jiang", "Hao", ""], ["Li", "Zhuo", ""], ["Lei", "Yinjie", ""], ["Lin", "Tao", ""], ["Li", "Haifeng", ""]]}, {"id": "1912.09092", "submitter": "Remy Kusters", "authors": "Gert-Jan Both, Remy Kusters", "title": "Temporal Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing and interpreting time-dependent stochastic data requires accurate\nand robust density estimation. In this paper we extend the concept of\nnormalizing flows to so-called temporal Normalizing Flows (tNFs) to estimate\ntime dependent distributions, leveraging the full spatio-temporal information\npresent in the dataset. Our approach is unsupervised, does not require an\na-priori characteristic scale and can accurately estimate multi-scale\ndistributions of vastly different length scales. We illustrate tNFs on sparse\ndatasets of Brownian and chemotactic walkers, showing that the inclusion of\ntemporal information enhances density estimation. Finally, we speculate how\ntNFs can be applied to fit and discover the continuous PDE underlying a\nstochastic process.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:52:13 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Both", "Gert-Jan", ""], ["Kusters", "Remy", ""]]}, {"id": "1912.09094", "submitter": "Anton Akusok", "authors": "Anton Akusok, Mirka Saarela, Tommi K\\\"arkk\\\"ainen, Kaj-Mikael Bj\\\"ork\n  and Amaury Lendasse", "title": "Mislabel Detection of Finnish Publication Ranks", "comments": null, "journal-ref": "International Conference on Extreme Learning Machine 2017 Oct 4\n  (pp. 240-248). Springer, Cham", "doi": null, "report-no": null, "categories": "cs.LG cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes to analyze a data set of Finnish ranks of academic\npublication channels with Extreme Learning Machine (ELM). The purpose is to\nintroduce and test recently proposed ELM-based mislabel detection approach with\na rich set of features characterizing a publication channel. We will compare\nthe architecture, accuracy, and, especially, the set of detected mislabels of\nthe ELM-based approach to the corresponding reference results on the reference\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:56:50 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Saarela", "Mirka", ""], ["K\u00e4rkk\u00e4inen", "Tommi", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.09132", "submitter": "Wei Huang", "authors": "Wei Huang, Richard Yi Da Xu, Weitao Du, Yutian Zeng, Yunce Zhao", "title": "Mean field theory for deep dropout networks: digging up gradient\n  backpropagation deeply", "comments": "20 pages, 7 figures", "journal-ref": "24th European Conference on Artificial Intelligence - ECAI 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the mean field theory has been applied to the study of\nneural networks and has achieved a great deal of success. The theory has been\napplied to various neural network structures, including CNNs, RNNs, Residual\nnetworks, and Batch normalization. Inevitably, recent work has also covered the\nuse of dropout. The mean field theory shows that the existence of depth scales\nthat limit the maximum depth of signal propagation and gradient\nbackpropagation. However, the gradient backpropagation is derived under the\ngradient independence assumption that weights used during feed forward are\ndrawn independently from the ones used in backpropagation. This is not how\nneural networks are trained in a real setting. Instead, the same weights used\nin a feed-forward step needs to be carried over to its corresponding\nbackpropagation. Using this realistic condition, we perform theoretical\ncomputation on linear dropout networks and a series of experiments on dropout\nnetworks. Our empirical results show an interesting phenomenon that the length\ngradients can backpropagate for a single input and a pair of inputs are\ngoverned by the same depth scale. Besides, we study the relationship between\nvariance and mean of statistical metrics of the gradient and shown an emergence\nof universality. Finally, we investigate the maximum trainable length for deep\ndropout networks through a series of experiments using MNIST and CIFAR10 and\nprovide a more precise empirical formula that describes the trainable length\nthan original work.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:33:34 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 06:50:24 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 08:16:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Huang", "Wei", ""], ["Da Xu", "Richard Yi", ""], ["Du", "Weitao", ""], ["Zeng", "Yutian", ""], ["Zhao", "Yunce", ""]]}, {"id": "1912.09140", "submitter": "Eyal Shulman", "authors": "Eyal Shulman, Lior Wolf", "title": "Meta Decision Trees for Explainable Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of building explainable recommendation systems that are\nbased on a per-user decision tree, with decision rules that are based on single\nattribute values. We build the trees by applying learned regression functions\nto obtain the decision rules as well as the values at the leaf nodes. The\nregression functions receive as input the embedding of the user's training set,\nas well as the embedding of the samples that arrive at the current node. The\nembedding and the regressors are learned end-to-end with a loss that encourages\nthe decision rules to be sparse. By applying our method, we obtain a\ncollaborative filtering solution that provides a direct explanation to every\nrating it provides. With regards to accuracy, it is competitive with other\nalgorithms. However, as expected, explainability comes at a cost and the\naccuracy is typically slightly lower than the state of the art result reported\nin the literature.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:45:01 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Shulman", "Eyal", ""], ["Wolf", "Lior", ""]]}, {"id": "1912.09147", "submitter": "Enmei Tu", "authors": "Xiao Han, Zihao Wang, Enmei Tu, Gunnam Suryanarayana, Jie Yang", "title": "Semi-Supervised Deep Learning Using Improved Unsupervised Discriminant\n  Projection", "comments": "1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning demands a huge amount of well-labeled data to train the network\nparameters. How to use the least amount of labeled data to obtain the desired\nclassification accuracy is of great practical significance, because for many\nreal-world applications (such as medical diagnosis), it is difficult to obtain\nso many labeled samples. In this paper, modify the unsupervised discriminant\nprojection algorithm from dimension reduction and apply it as a regularization\nterm to propose a new semi-supervised deep learning algorithm, which is able to\nutilize both the local and nonlocal distribution of abundant unlabeled samples\nto improve classification performance. Experiments show that given dozens of\nlabeled samples, the proposed algorithm can train a deep network to attain\nsatisfactory classification results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:55:12 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Han", "Xiao", ""], ["Wang", "Zihao", ""], ["Tu", "Enmei", ""], ["Suryanarayana", "Gunnam", ""], ["Yang", "Jie", ""]]}, {"id": "1912.09251", "submitter": "Khe Chai Sim", "authors": "Khe Chai Sim, Fran\\c{c}oise Beaufays, Arnaud Benard, Dhruv Guliani,\n  Andreas Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif\n  Johnson, Giovanni Motta, Lillian Zhou", "title": "Personalization of End-to-end Speech Recognition On Mobile Devices For\n  Named Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of several techniques to personalize end-to-end\nspeech models and improve the recognition of proper names relevant to the user.\nThese techniques differ in the amounts of user effort required to provide\nsupervision, and are evaluated on how they impact speech recognition\nperformance. We propose using keyword-dependent precision and recall metrics to\nmeasure vocabulary acquisition performance. We evaluate the algorithms on a\ndataset that we designed to contain names of persons that are difficult to\nrecognize. Therefore, the baseline recall rate for proper names in this dataset\nis very low: 2.4%. A data synthesis approach we developed brings it to 48.6%,\nwith no need for speech input from the user. With speech input, if the user\ncorrects only the names, the name recall rate improves to 64.4%. If the user\ncorrects all the recognition errors, we achieve the best recall of 73.5%. To\neliminate the need to upload user data and store personalized models on a\nserver, we focus on performing the entire personalization workflow on a mobile\ndevice.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:18:53 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Sim", "Khe Chai", ""], ["Beaufays", "Fran\u00e7oise", ""], ["Benard", "Arnaud", ""], ["Guliani", "Dhruv", ""], ["Kabel", "Andreas", ""], ["Khare", "Nikhil", ""], ["Lucassen", "Tamar", ""], ["Zadrazil", "Petr", ""], ["Zhang", "Harry", ""], ["Johnson", "Leif", ""], ["Motta", "Giovanni", ""], ["Zhou", "Lillian", ""]]}, {"id": "1912.09254", "submitter": "Jeroen Zegers", "authors": "Jeroen Zegers, Hugo Van hamme", "title": "CNN-LSTM models for Multi-Speaker Source Separation using Bayesian Hyper\n  Parameter Optimization", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there have been many deep learning approaches towards the\nmulti-speaker source separation problem. Most use Long Short-Term Memory -\nRecurrent Neural Networks (LSTM-RNN) or Convolutional Neural Networks (CNN) to\nmodel the sequential behavior of speech. In this paper we propose a novel\nnetwork for source separation using an encoder-decoder CNN and LSTM in\nparallel. Hyper parameters have to be chosen for both parts of the network and\nthey are potentially mutually dependent. Since hyper parameter grid search has\na high computational burden, random search is often preferred. However, when\nsampling a new point in the hyper parameter space, it can potentially be very\nclose to a previously evaluated point and thus give little additional\ninformation. Furthermore, random sampling is as likely to sample in a promising\narea as in an hyper space area dominated with poor performing models.\nTherefore, we use a Bayesian hyper parameter optimization technique and find\nthat the parallel CNN-LSTM outperforms the LSTM-only and CNN-only model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:04:34 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zegers", "Jeroen", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1912.09261", "submitter": "Jeroen Zegers", "authors": "Pieter Appeltans, Jeroen Zegers, Hugo Van hamme", "title": "Practical applicability of deep neural networks for overlapping speaker\n  separation", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the applicability in realistic scenarios of two deep\nlearning based solutions to the overlapping speaker separation problem.\nFirstly, we present experiments that show that these methods are applicable for\na broad range of languages. Further experimentation indicates limited\nperformance loss for untrained languages, when these have common features with\nthe trained language(s). Secondly, it investigates how the methods deal with\nrealistic background noise and proposes some modifications to better cope with\nthese disturbances. The deep learning methods that will be examined are deep\nclustering and deep attractor networks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:13:36 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Appeltans", "Pieter", ""], ["Zegers", "Jeroen", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1912.09287", "submitter": "Minh Vu", "authors": "Minh H. Vu and Guus Grimbergen and Tufve Nyholm and Tommy L\\\"ofstedt", "title": "Evaluation of Multi-Slice Inputs to Convolutional Neural Networks for\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14391", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When using Convolutional Neural Networks (CNNs) for segmentation of organs\nand lesions in medical images, the conventional approach is to work with inputs\nand outputs either as single slice (2D) or whole volumes (3D). One common\nalternative, in this study denoted as pseudo-3D, is to use a stack of adjacent\nslices as input and produce a prediction for at least the central slice. This\napproach gives the network the possibility to capture 3D spatial information,\nwith only a minor additional computational cost. In this study, we\nsystematically evaluate the segmentation performance and computational costs of\nthis pseudo-3D approach as a function of the number of input slices, and\ncompare the results to conventional end-to-end 2D and 3D CNNs. The standard\npseudo-3D method regards the neighboring slices as multiple input image\nchannels. We additionally evaluate a simple approach where the input stack is a\nvolumetric input that is repeatably convolved in 3D to obtain a 2D feature map.\nThis 2D map is in turn fed into a standard 2D network. We conducted experiments\nusing two different CNN backbone architectures and on five diverse data sets\ncovering different anatomical regions, imaging modalities, and segmentation\ntasks. We found that while both pseudo-3D methods can process a large number of\nslices at once and still be computationally much more efficient than fully 3D\nCNNs, a significant improvement over a regular 2D CNN was only observed for one\nof the five data sets. An analysis of the structural properties of the\nsegmentation masks revealed no relations to the segmentation performance with\nrespect to the number of input slices. The conclusion is therefore that in the\ngeneral case, multi-slice inputs appear to not significantly improve\nsegmentation results over using 2D or 3D CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:26:16 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 19:31:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Vu", "Minh H.", ""], ["Grimbergen", "Guus", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "1912.09301", "submitter": "Caifa Zhou", "authors": "Caifa Zhou", "title": "Feature-wise change detection and robust indoor positioning using\n  RANSAC-like approach", "comments": "36 pages, 20 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprinting-based positioning, one of the promising indoor positioning\nsolutions, has been broadly explored owing to the pervasiveness of sensor-rich\nmobile devices, the prosperity of opportunistically measurable\nlocation-relevant signals and the progress of data-driven algorithms. One\ncritical challenge is to controland improve the quality of the reference\nfingerprint map (RFM), which is built at the offline stage and applied for\nonline positioning. The key concept concerningthe quality control of the RFM is\nupdating the RFM according to the newly measured data. Though varies methods\nhave been proposed for adapting the RFM, they approach the problem by\nintroducing extra-positioning schemes (e.g. PDR orUGV) and directly adjust the\nRFM without distinguishing whether critical changes have occurred. This paper\naims at proposing an extra-positioning-free solution by making full use of the\nredundancy of measurable features. Loosely inspired by random sampling\nconsensus (RANSAC), arbitrarily sampled subset of features from the online\nmeasurement are used for generating multi-resamples, which areused for\nestimating the intermediate locations. In the way of resampling, it can\nmitigate the impact of the changed features on positioning and enables to\nretrieve accurate location estimation. The users location is robustly computed\nby identifying the candidate locations from these intermediate ones using\nmodified Jaccardindex (MJI) and the feature-wise change belief is calculated\naccording to the world model of the RFM and the estimated variability of\nfeatures. In order to validate our proposed approach, two levels of\nexperimental analysis have been carried out. On the simulated dataset, the\naverage change detection accuracy is about 90%. Meanwhile, the improvement of\npositioning accuracy within 2 m is about 20% by dropping out the features that\nare detected as changed when performing positioning comparing to that of using\nall measured features for location estimation. On the long-term collected\ndataset, the average change detection accuracy is about 85%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:46:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhou", "Caifa", ""]]}, {"id": "1912.09303", "submitter": "Simon Msika", "authors": "Simon Msika, Alejandro Quintero, Foutse Khomh", "title": "SIGMA : Strengthening IDS with GAN and Metaheuristics Attacks", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Intrusion Detection System (IDS) is a key cybersecurity tool for network\nadministrators as it identifies malicious traffic and cyberattacks. With the\nrecent successes of machine learning techniques such as deep learning, more and\nmore IDS are now using machine learning algorithms to detect attacks faster.\nHowever, these systems lack robustness when facing previously unseen types of\nattacks. With the increasing number of new attacks, especially against Internet\nof Things devices, having a robust IDS able to spot unusual and new attacks\nbecomes necessary.\n  This work explores the possibility of leveraging generative adversarial\nmodels to improve the robustness of machine learning based IDS. More\nspecifically, we propose a new method named SIGMA, that leverages adversarial\nexamples to strengthen IDS against new types of attacks. Using Generative\nAdversarial Networks (GAN) and metaheuristics, SIGMA %Our method consists in\ngenerates adversarial examples, iteratively, and uses it to retrain a machine\nlearning-based IDS, until a convergence of the detection rate (i.e. until the\ndetection system is not improving anymore). A round of improvement consists of\na generative phase, in which we use GANs and metaheuristics to generate\ninstances ; an evaluation phase in which we calculate the detection rate of\nthose newly generated attacks ; and a training phase, in which we train the IDS\nwith those attacks. We have evaluated the SIGMA method for four standard\nmachine learning classification algorithms acting as IDS, with a combination of\nGAN and a hybrid local-search and genetic algorithm, to generate new datasets\nof attacks. Our results show that SIGMA can successfully generate adversarial\nattacks against different machine learning based IDS. Also, using SIGMA, we can\nimprove the performance of an IDS to up to 100\\% after as little as two rounds\nof improvement.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:35:38 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Msika", "Simon", ""], ["Quintero", "Alejandro", ""], ["Khomh", "Foutse", ""]]}, {"id": "1912.09306", "submitter": "Balint Daroczy", "authors": "B\\'alint Dar\\'oczy and Rita Aleksziev and Andr\\'as Bencz\\'ur", "title": "Tangent Space Separability in Feedforward Neural Networks", "comments": "10 pages; accepted at Workshop \"Beyond First-Order Optimization\n  Methods in Machine Learning\", 33rd Conference on Neural Information\n  Processing Systems (NeurIPS 2019). arXiv admin note: substantial text overlap\n  with arXiv:1807.06630", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical neural networks are exponentially more efficient than their\ncorresponding \"shallow\" counterpart with the same expressive power, but involve\nhuge number of parameters and require tedious amounts of training. By\napproximating the tangent subspace, we suggest a sparse representation that\nenables switching to shallow networks, GradNet after a very early training\nstage. Our experiments show that the proposed approximation of the metric\nimproves and sometimes even surpasses the achievable performance of the\noriginal network significantly even after a few epochs of training the original\nfeedforward network.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:04:02 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Dar\u00f3czy", "B\u00e1lint", ""], ["Aleksziev", "Rita", ""], ["Bencz\u00far", "Andr\u00e1s", ""]]}, {"id": "1912.09322", "submitter": "Sergio Gast\\'on Burdisso", "authors": "Sergio G. Burdisso, Marcelo Errecalde, Manuel Montes-y-G\\'omez", "title": "PySS3: A Python package implementing a novel text classifier with\n  visualization tools for Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently introduced text classifier, called SS3, has obtained\nstate-of-the-art performance on the CLEF's eRisk tasks. SS3 was created to deal\nwith risk detection over text streams and, therefore, not only supports\nincremental training and classification but also can visually explain its\nrationale. However, little attention has been paid to the potential use of SS3\nas a general classifier. We believe this could be due to the unavailability of\nan open-source implementation of SS3. In this work, we introduce PySS3, a\npackage that implements SS3 and also comes with visualization tools that allow\nresearchers to deploy robust, explainable, and trusty machine learning models\nfor text classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:01:41 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 00:31:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Burdisso", "Sergio G.", ""], ["Errecalde", "Marcelo", ""], ["Montes-y-G\u00f3mez", "Manuel", ""]]}, {"id": "1912.09323", "submitter": "Artem Ryzhikov", "authors": "Artem Ryzhikov, Maxim Borisyak, Andrey Ustyuzhanin, Denis Derkach", "title": "Normalizing flows for deep anomaly detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection for complex data is a challenging task from the perspective\nof machine learning. In this work, weconsider cases with missing certain kinds\nof anomalies in the training dataset, while significant statistics for the\nnormal class isavailable. For such scenarios, conventional supervised methods\nmight suffer from the class imbalance, while unsupervised methodstend to ignore\ndifficult anomalous examples. We extend the idea of the supervised\nclassification approach for class-imbalanceddatasets by exploiting normalizing\nflows for proper Bayesian inference of the posterior probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:03:11 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ryzhikov", "Artem", ""], ["Borisyak", "Maxim", ""], ["Ustyuzhanin", "Andrey", ""], ["Derkach", "Denis", ""]]}, {"id": "1912.09356", "submitter": "Nathan Laubeuf", "authors": "Bram-Ernst Verhoef, Nathan Laubeuf, Stefan Cosemans, Peter Debacker,\n  Ioannis Papistas, Arindam Mallik, Diederik Verkest", "title": "FQ-Conv: Fully Quantized Convolution for Efficient and Accurate\n  Inference", "comments": "12 pages, 4 Figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) can be made hardware-efficient by reducing the\nnumerical precision of the weights and activations of the network and by\nimproving the network's resilience to noise. However, this gain in efficiency\noften comes at the cost of significantly reduced accuracy. In this paper, we\npresent a novel approach to quantizing convolutional neural network. The\nresulting networks perform all computations in low-precision, without requiring\nhigher-precision BN and nonlinearities, while still being highly accurate. To\nachieve this result, we employ a novel quantization technique that learns to\noptimally quantize the weights and activations of the network during training.\nAdditionally, to enhance training convergence we use a new training technique,\ncalled gradual quantization. We leverage the nonlinear and normalizing behavior\nof our quantization function to effectively remove the higher-precision\nnonlinearities and BN from the network. The resulting convolutional layers are\nfully quantized to low precision, from input to output, ideal for neural\nnetwork accelerators on the edge. We demonstrate the potential of this approach\non different datasets and networks, showing that ternary-weight CNNs with\nlow-precision in- and outputs perform virtually on par with their\nfull-precision equivalents. Finally, we analyze the influence of noise on the\nweights, activations and convolution outputs (multiply-accumulate, MAC) and\npropose a strategy to improve network performance under noisy conditions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:39:45 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Verhoef", "Bram-Ernst", ""], ["Laubeuf", "Nathan", ""], ["Cosemans", "Stefan", ""], ["Debacker", "Peter", ""], ["Papistas", "Ioannis", ""], ["Mallik", "Arindam", ""], ["Verkest", "Diederik", ""]]}, {"id": "1912.09363", "submitter": "Bryan Lim", "authors": "Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister", "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-horizon forecasting problems often contain a complex mix of inputs --\nincluding static (i.e. time-invariant) covariates, known future inputs, and\nother exogenous time series that are only observed historically -- without any\nprior information on how they interact with the target. While several deep\nlearning models have been proposed for multi-step prediction, they typically\ncomprise black-box models which do not account for the full range of inputs\npresent in common scenarios. In this paper, we introduce the Temporal Fusion\nTransformer (TFT) -- a novel attention-based architecture which combines\nhigh-performance multi-horizon forecasting with interpretable insights into\ntemporal dynamics. To learn temporal relationships at different scales, the TFT\nutilizes recurrent layers for local processing and interpretable self-attention\nlayers for learning long-term dependencies. The TFT also uses specialized\ncomponents for the judicious selection of relevant features and a series of\ngating layers to suppress unnecessary components, enabling high performance in\na wide range of regimes. On a variety of real-world datasets, we demonstrate\nsignificant performance improvements over existing benchmarks, and showcase\nthree practical interpretability use-cases of TFT.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:45:40 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 12:45:45 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 14:17:28 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lim", "Bryan", ""], ["Arik", "Sercan O.", ""], ["Loeff", "Nicolas", ""], ["Pfister", "Tomas", ""]]}, {"id": "1912.09379", "submitter": "Benedikt Pf\\\"ulb", "authors": "Alexander Gepperth and Benedikt Pf\\\"ulb", "title": "Gradient-based training of Gaussian Mixture Models for High-Dimensional\n  Streaming Data", "comments": "17 pages, 4 figures, preprint Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for efficiently training Gaussian Mixture Model (GMM)\nby Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional\nstreaming data. Our training scheme does not require data-driven parameter\ninitialization (e.g., k-means) and can thus be trained based on a random\ninitialization. Furthermore, the approach allows mini-batch sizes as low as 1,\nwhich are typical for streaming-data settings. Major problems in such settings\nare undesirable local optima during early training phases and numerical\ninstabilities due to high data dimensionalities. We introduce an adaptive\nannealing procedure to address the first problem, whereas numerical\ninstabilities are eliminated by using an exponential-free approximation to the\nstandard GMM log-likelihood. Experiments on a variety of visual and non-visual\nbenchmarks show that our SGD approach can be trained completely without, for\ninstance, k-means based centroid initialization. It also compares favorably to\nan online variant of Expectation-Maximization (EM) - stochastic EM (sEM), which\nit outperforms by a large margin for very high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:35:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 15:32:05 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 16:25:30 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gepperth", "Alexander", ""], ["Pf\u00fclb", "Benedikt", ""]]}, {"id": "1912.09380", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Gabriel Gagnon-Turcotte, Angkoon Phinyomark,\n  Kyrre Glette, Erik Scheme, Fran\\c{c}ois Laviolette, and Benoit Gosselin", "title": "A Transferable Adaptive Domain Adversarial Neural Network for Virtual\n  Reality Augmented EMG-Based Gesture Recognition", "comments": "10 Pages. The last three authors shared senior authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of electromyography-based (EMG) gesture recognition,\ndisparities exist between the offline accuracy reported in the literature and\nthe real-time usability of a classifier. This gap mainly stems from two\nfactors: 1) The absence of a controller, making the data collected dissimilar\nto actual control. 2) The difficulty of including the four main dynamic factors\n(gesture intensity, limb position, electrode shift, and transient changes in\nthe signal), as including their permutations drastically increases the amount\nof data to be recorded. Contrarily, online datasets are limited to the exact\nEMG-based controller used to record them, necessitating the recording of a new\ndataset for each control method or variant to be tested. Consequently, this\npaper proposes a new type of dataset to serve as an intermediate between\noffline and online datasets, by recording the data using a real-time\nexperimental protocol. The protocol, performed in virtual reality, includes the\nfour main dynamic factors and uses an EMG-independent controller to guide\nmovements. This EMG-independent feedback ensures that the user is in-the-loop\nduring recording, while enabling the resulting dynamic dataset to be used as an\nEMG-based benchmark. The dataset is comprised of 20 able-bodied participants\ncompleting three to four sessions over a period of 14 to 21 days. The ability\nof the dynamic dataset to serve as a benchmark is leveraged to evaluate the\nimpact of different recalibration techniques for long-term (across-day) gesture\nrecognition, including a novel algorithm, named TADANN. TADANN consistently and\nsignificantly (p<0.05) outperforms using fine-tuning as the recalibration\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:41:56 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 14:55:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Gagnon-Turcotte", "Gabriel", ""], ["Phinyomark", "Angkoon", ""], ["Glette", "Kyrre", ""], ["Scheme", "Erik", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Gosselin", "Benoit", ""]]}, {"id": "1912.09382", "submitter": "Aur\\'elien Decelle", "authors": "Giancarlo Fissore, Aur\\'elien Decelle, Cyril Furtlehner, Yufei Han", "title": "Robust Multi-Output Learning with Highly Incomplete Data via Restricted\n  Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard multi-output classification scenario, both features and labels\nof training data are partially observed. This challenging issue is widely\nwitnessed due to sensor or database failures, crowd-sourcing and noisy\ncommunication channels in industrial data analytic services. Classic methods\nfor handling multi-output classification with incomplete supervision\ninformation usually decompose the problem into an imputation stage that\nreconstructs the missing training information, and a learning stage that builds\na classifier based on the imputed training set. These methods fail to fully\nleverage the dependencies between features and labels. In order to take full\nadvantage of these dependencies we consider a purely probabilistic setting in\nwhich the features imputation and multi-label classification problems are\njointly solved. Indeed, we show that a simple Restricted Boltzmann Machine can\nbe trained with an adapted algorithm based on mean-field equations to\nefficiently solve problems of inductive and transductive learning in which both\nfeatures and labels are missing at random. The effectiveness of the approach is\ndemonstrated empirically on various datasets, with particular focus on a\nreal-world Internet-of-Things security dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:03:27 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Fissore", "Giancarlo", ""], ["Decelle", "Aur\u00e9lien", ""], ["Furtlehner", "Cyril", ""], ["Han", "Yufei", ""]]}, {"id": "1912.09395", "submitter": "Andreas Kofler", "authors": "Andreas Kofler, Markus Haltmeier, Tobias Schaeffter, Marc\n  Kachelrie{\\ss}, Marc Dewey, Christian Wald and Christoph Kolbitsch", "title": "Neural Networks-based Regularization for Large-Scale Medical Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/ab990e", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a generalized Deep Learning-based approach for\nsolving ill-posed large-scale inverse problems occuring in medical image\nreconstruction. Recently, Deep Learning methods using iterative neural networks\nand cascaded neural networks have been reported to achieve state-of-the-art\nresults with respect to various quantitative quality measures as PSNR, NRMSE\nand SSIM across different imaging modalities. However, the fact that these\napproaches employ the forward and adjoint operators repeatedly in the network\narchitecture requires the network to process the whole images or volumes at\nonce, which for some applications is computationally infeasible. In this work,\nwe follow a different reconstruction strategy by decoupling the regularization\nof the solution from ensuring consistency with the measured data. The\nregularization is given in the form of an image prior obtained by the output of\na previously trained neural network which is used in a Tikhonov regularization\nframework. By doing so, more complex and sophisticated network architectures\ncan be used for the removal of the artefacts or noise than it is usually the\ncase in iterative networks. Due to the large scale of the considered problems\nand the resulting computational complexity of the employed networks, the priors\nare obtained by processing the images or volumes as patches or slices. We\nevaluated the method for the cases of 3D cone-beam low dose CT and undersampled\n2D radial cine MRI and compared it to a total variation-minimization-based\nreconstruction algorithm as well as to a method with regularization based on\nlearned overcomplete dictionaries. The proposed method outperformed all the\nreported methods with respect to all chosen quantitative measures and further\naccelerates the regularization step in the reconstruction by several orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:15:27 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:52:58 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kofler", "Andreas", ""], ["Haltmeier", "Markus", ""], ["Schaeffter", "Tobias", ""], ["Kachelrie\u00df", "Marc", ""], ["Dewey", "Marc", ""], ["Wald", "Christian", ""], ["Kolbitsch", "Christoph", ""]]}, {"id": "1912.09399", "submitter": "Julian Zilly", "authors": "Julian Zilly, Lorenz Hetzel, Andrea Censi, Emilio Frazzoli", "title": "Quantifying the effect of representations on task complexity", "comments": "Workshop paper at Information Theory and Machine Learning Workshop at\n  NeurIPS'19. 13 pages (8 pages + 2 bibliography + 3 appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the influence of input data representations on learning\ncomplexity. For learning, we posit that each model implicitly uses a candidate\nmodel distribution for unexplained variations in the data, its noise model. If\nthe model distribution is not well aligned to the true distribution, then even\nrelevant variations will be treated as noise. Crucially however, the alignment\nof model and true distribution can be changed, albeit implicitly, by changing\ndata representations. \"Better\" representations can better align the model to\nthe true distribution, making it easier to approximate the input-output\nrelationship in the data without discarding useful data variations. To quantify\nthis alignment effect of data representations on the difficulty of a learning\ntask, we make use of an existing task complexity score and show its connection\nto the representation-dependent information coding length of the input.\nEmpirically we extract the necessary statistics from a linear regression\napproximation and show that these are sufficient to predict relative learning\nperformance outcomes of different data representations and neural network types\nobtained when utilizing an extensive neural network architecture search. We\nconclude that to ensure better learning outcomes, representations may need to\nbe tailored to both task and model to align with the implicit distribution of\nmodel and task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:19:14 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Zilly", "Julian", ""], ["Hetzel", "Lorenz", ""], ["Censi", "Andrea", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1912.09423", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Smon Hessner, Yao-Chong Lim, Louis-Phlippe Morency", "title": "Pseudo-Encoded Stochastic Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior inference in directed graphical models is commonly done using a\nprobabilistic encoder (a.k.a inference model) conditioned on the input. Often\nthis inference model is trained jointly with the probabilistic decoder (a.k.a\ngenerator model). If probabilistic encoder encounters complexities during\ntraining (e.g. suboptimal complxity or parameterization), then learning reaches\na suboptimal objective; a phenomena commonly called inference suboptimality. In\nVariational Inference (VI), optimizing the ELBo using Stochastic Variational\nInference (SVI) can eliminate the inference suboptimality (as demonstrated in\nthis paper), however, this solution comes at a substantial computational cost\nwhen inference needs to be done on new data points. Essentially, a long\nsequential chain of gradient updates is required to fully optimize approximate\nposteriors. In this paper, we present an approach called Pseudo-Encoded\nStochastic Variational Inference (PE-SVI), to reduce the inference complexity\nof SVI during test time. Our approach relies on finding a suitable initial\nstart point for gradient operations, which naturally reduces the required\ngradient steps. Furthermore, this initialization allows for adopting larger\nstep sizes (compared to random initialization used in SVI), which further\nreduces the inference time complexity. PE-SVI reaches the same ELBo objective\nas SVI using less than one percent of required steps, on average.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:50:18 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Zadeh", "Amir", ""], ["Hessner", "Smon", ""], ["Lim", "Yao-Chong", ""], ["Morency", "Louis-Phlippe", ""]]}, {"id": "1912.09426", "submitter": "Johann Baumgartner", "authors": "Johann Baumgartner (1), Katharina Gruber (1), Sofia Simoes (2),\n  Yves-Marie Saint-Drenan (3), Johannes Schmidt (1) ((1) University of Natural\n  Resources and Life Sciences, Vienna, (2) NOVA University Lisbon, (3) MINES\n  ParisTech)", "title": "Machine learning models show similar performance to Renewables.ninja for\n  generation of long-term wind power time series even without location\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Driven by climatic processes, wind power generation is inherently variable.\nLong-term simulated wind power time series are therefore an essential component\nfor understanding the temporal availability of wind power and its integration\ninto future renewable energy systems. In the recent past, mainly power curve\nbased models such as Renewables.ninja (RN) have been used for deriving\nsynthetic time series for wind power generation despite their need for accurate\nlocation information as well as for bias correction, and their insufficient\nreplication of extreme events and short-term power ramps. We assess how time\nseries generated by machine learning models (MLM) compare to RN in terms of\ntheir ability to replicate the characteristics of observed nationally\naggregated wind power generation for Germany. Hence, we apply neural networks\nto one MERRA2 reanalysis wind speed input dataset with no location information\nand one with basic location information. The resulting time series and the RN\ntime series are compared with actual generation. Both MLM time series feature\nequal or even better time series quality than RN depending on the\ncharacteristics considered. We conclude that MLM models can, even when reducing\ninformation on turbine locations and turbine types, produce time series of at\nleast equal quality to RN.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:59:49 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Baumgartner", "Johann", ""], ["Gruber", "Katharina", ""], ["Simoes", "Sofia", ""], ["Saint-Drenan", "Yves-Marie", ""], ["Schmidt", "Johannes", ""]]}, {"id": "1912.09428", "submitter": "Mrinmoy Sarkar", "authors": "Dhiman Chowdhury and Mrinmoy Sarkar", "title": "Location Forensics Analysis Using ENF Sequences Extracted from Power and\n  Audio Recordings", "comments": "5 pages, 5 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Electrical network frequency (ENF) is the signature of a power distribution\ngrid which represents the nominal frequency (50 or 60 Hz) of a power system\nnetwork. Due to load variations in a power grid, ENF sequences experience\nfluctuations. These ENF variations are inherently located in a multimedia\nsignal which is recorded close to the grid or directly from the mains power\nline. Therefore, a multimedia recording can be localized by analyzing the ENF\nsequences of that signal in absence of the concurrent power signal. In this\npaper, a novel approach to analyze location forensics using ENF sequences\nextracted from a number of power and audio recordings is proposed. The digital\nrecordings are collected from different grid locations around the world.\nPotential feature components are determined from the ENF sequences. Then, a\nmulti-class support vector machine (SVM) classification model is developed to\nvalidate the location authenticity of the recordings. The performance\nassessments affirm the efficacy of the presented work.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:15:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Chowdhury", "Dhiman", ""], ["Sarkar", "Mrinmoy", ""]]}, {"id": "1912.09445", "submitter": "S. Mohammad Mirbagheri", "authors": "S. Mohammad Mirbagheri, Howard J. Hamilton", "title": "FIBS: A Generic Framework for Classifying Interval-based Temporal\n  Sequences", "comments": "In: Big Data Analytics and Knowledge Discovery. DaWaK 2020. Springer,\n  Cham", "journal-ref": "22nd International Conference on Big Data Analytics and Knowledge\n  Discovery (DaWaK), Bratislava, Slovakia, September 14-17, 2020. Springer,\n  Cham", "doi": "10.1007/978-3-030-59065-9_24", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of classifying interval-based temporal sequences\n(IBTSs). Since common classification algorithms cannot be directly applied to\nIBTSs, the main challenge is to define a set of features that effectively\nrepresents the data such that classifiers can be applied. Most prior work\nutilizes frequent pattern mining to define a feature set based on discovered\npatterns. However, frequent pattern mining is computationally expensive and\noften discovers many irrelevant patterns. To address this shortcoming, we\npropose the FIBS framework for classifying IBTSs. FIBS extracts features\nrelevant to classification from IBTSs based on relative frequency and temporal\nrelations. To avoid selecting irrelevant features, a filter-based selection\nstrategy is incorporated into FIBS. Our empirical evaluation on eight\nreal-world datasets demonstrates the effectiveness of our methods in practice.\nThe results provide evidence that FIBS effectively represents IBTSs for\nclassification algorithms, which contributes to similar or significantly better\naccuracy compared to state-of-the-art competitors. It also suggests that the\nfeature selection strategy is beneficial to FIBS's performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 18:23:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 18:21:47 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Mirbagheri", "S. Mohammad", ""], ["Hamilton", "Howard J.", ""]]}, {"id": "1912.09484", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias and Warren B. Powell", "title": "Zeroth-order Stochastic Compositional Algorithms for Risk-Aware Learning", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SP eess.SY math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Free-MESSAGEp, the first zeroth-order algorithm for convex\nmean-semideviation-based risk-aware learning, which is also the first\nthree-level zeroth-order compositional stochastic optimization algorithm,\nwhatsoever. Using a non-trivial extension of Nesterov's classical results on\nGaussian smoothing, we develop the Free-MESSAGEp algorithm from first\nprinciples, and show that it essentially solves a smoothed surrogate to the\noriginal problem, the former being a uniform approximation of the latter, in a\nuseful, convenient sense. We then present a complete analysis of the\nFree-MESSAGEp algorithm, which establishes convergence in a user-tunable\nneighborhood of the optimal solutions of the original problem, as well as\nexplicit convergence rates for both convex and strongly convex costs.\nOrderwise, and for fixed problem parameters, our results demonstrate no\nsacrifice in convergence speed compared to existing first-order methods, while\nstriking a certain balance among the condition of the problem, its\ndimensionality, as well as the accuracy of the obtained results, naturally\nextending previous results in zeroth-order risk-neutral learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:04:15 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1912.09508", "submitter": "Zhe Liu", "authors": "Zhe Liu, Fuchun Peng", "title": "Statistical Testing on ASR Performance via Blockwise Bootstrap", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common question being raised in automatic speech recognition (ASR)\nevaluations is how reliable is an observed word error rate (WER) improvement\ncomparing two ASR systems, where statistical hypothesis testing and confidence\ninterval (CI) can be utilized to tell whether this improvement is real or only\ndue to random chance. The bootstrap resampling method has been popular for such\nsignificance analysis which is intuitive and easy to use. However, this method\nfails in dealing with dependent data, which is prevalent in speech world - for\nexample, ASR performance on utterances from the same speaker could be\ncorrelated. In this paper we present blockwise bootstrap approach - by dividing\nevaluation utterances into nonoverlapping blocks, this method resamples these\nblocks instead of original data. We show that the resulting variance estimator\nof absolute WER difference between two ASR systems is consistent under mild\nconditions. We also demonstrate the validity of blockwise bootstrap method on\nboth synthetic and real-world speech data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:20:09 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 22:51:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Liu", "Zhe", ""], ["Peng", "Fuchun", ""]]}, {"id": "1912.09522", "submitter": "Siqi Liu", "authors": "Siqi Liu and Milos Hauskrecht", "title": "Event Outlier Detection in Continuous Time", "comments": "ICML 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time event sequences represent discrete events occurring in\ncontinuous time. Such sequences arise frequently in real-life. Usually we\nexpect the sequences to follow some regular pattern over time. However,\nsometimes these patterns may be interrupted by unexpected absence or\noccurrences of events. Identification of these unexpected cases can be very\nimportant as they may point to abnormal situations that need human attention.\nIn this work, we study and develop methods for detecting outliers in\ncontinuous-time event sequences, including unexpected absence and unexpected\noccurrences of events. Since the patterns that event sequences tend to follow\nmay change in different contexts, we develop outlier detection methods based on\npoint processes that can take context information into account. Our methods are\nbased on Bayesian decision theory and hypothesis testing with theoretical\nguarantees. To test the performance of the methods, we conduct experiments on\nboth synthetic data and real-world clinical data and show the effectiveness of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:53:22 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 17:27:53 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 15:50:02 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Siqi", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1912.09533", "submitter": "Jeet Mohapatra", "authors": "Jeet Mohapatra, Tsui-Wei (Lily) Weng, Pin-Yu Chen, Sijia Liu and Luca\n  Daniel", "title": "Towards Verifying Robustness of Neural Networks Against Semantic\n  Perturbations", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying robustness of neural networks given a specified threat model is a\nfundamental yet challenging task. While current verification methods mainly\nfocus on the $\\ell_p$-norm threat model of the input instances, robustness\nverification against semantic adversarial attacks inducing large $\\ell_p$-norm\nperturbations, such as color shifting and lighting adjustment, are beyond their\ncapacity. To bridge this gap, we propose \\textit{Semantify-NN}, a\nmodel-agnostic and generic robustness verification approach against semantic\nperturbations for neural networks. By simply inserting our proposed\n\\textit{semantic perturbation layers} (SP-layers) to the input layer of any\ngiven model, \\textit{Semantify-NN} is model-agnostic, and any $\\ell_p$-norm\nbased verification tools can be used to verify the model robustness against\nsemantic perturbations. We illustrate the principles of designing the SP-layers\nand provide examples including semantic perturbations to image classification\nin the space of hue, saturation, lightness, brightness, contrast and rotation,\nrespectively. In addition, an efficient refinement technique is proposed to\nfurther significantly improve the semantic certificate. Experiments on various\nnetwork architectures and different datasets demonstrate the superior\nverification performance of \\textit{Semantify-NN} over $\\ell_p$-norm-based\nverification frameworks that naively convert semantic perturbation to\n$\\ell_p$-norm. The results show that \\textit{Semantify-NN} can support\nrobustness verification against a wide range of semantic perturbations.\n  Code available https://github.com/JeetMo/Semantify-NN\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:21:03 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 17:54:53 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Mohapatra", "Jeet", "", "Lily"], ["Tsui-Wei", "", "", "Lily"], ["Weng", "", ""], ["Chen", "Pin-Yu", ""], ["Liu", "Sijia", ""], ["Daniel", "Luca", ""]]}, {"id": "1912.09536", "submitter": "Subru Krishnan", "authors": "Fotis Psallidas, Yiwen Zhu, Bojan Karlas, Matteo Interlandi, Avrilia\n  Floratou, Konstantinos Karanasos, Wentao Wu, Ce Zhang, Subru Krishnan, Carlo\n  Curino, Markus Weimer", "title": "Data Science through the looking glass and what we found there", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent success of machine learning (ML) has led to an explosive growth\nboth in terms of new systems and algorithms built in industry and academia, and\nnew applications built by an ever-growing community of data science (DS)\npractitioners. This quickly shifting panorama of technologies and applications\nis challenging for builders and practitioners alike to follow. In this paper,\nwe set out to capture this panorama through a wide-angle lens, by performing\nthe largest analysis of DS projects to date, focusing on questions that can\nhelp determine investments on either side. Specifically, we download and\nanalyze: (a) over 6M Python notebooks publicly available on GITHUB, (b) over 2M\nenterprise DS pipelines developed within COMPANYX, and (c) the source code and\nmetadata of over 900 releases from 12 important DS libraries. The analysis we\nperform ranges from coarse-grained statistical characterizations to analysis of\nlibrary imports, pipelines, and comparative studies across datasets and time.\nWe report a large number of measurements for our readers to interpret, and dare\nto draw a few (actionable, yet subjective) conclusions on (a) what systems\nbuilders should focus on to better serve practitioners, and (b) what\ntechnologies should practitioners bet on given current trends. We plan to\nautomate this analysis and release associated tools and results periodically.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:29:44 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Psallidas", "Fotis", ""], ["Zhu", "Yiwen", ""], ["Karlas", "Bojan", ""], ["Interlandi", "Matteo", ""], ["Floratou", "Avrilia", ""], ["Karanasos", "Konstantinos", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""], ["Krishnan", "Subru", ""], ["Curino", "Carlo", ""], ["Weimer", "Markus", ""]]}, {"id": "1912.09575", "submitter": "Mustafa Coskun", "authors": "Mustafa Coskun, Burcu Bakir Gungor, Mehmet Koyuturk", "title": "Expanding Label Sets for Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, Graph Convolutional Networks (GCNs) and their variants have\nbeen widely utilized in learning tasks that involve graphs. These tasks include\nrecommendation systems, node classification, among many others. In node\nclassification problem, the input is a graph in which the edges represent the\nassociation between pairs of nodes, multi-dimensional feature vectors are\nassociated with the nodes, and some of the nodes in the graph have known\nlabels. The objective is to predict the labels of the nodes that are not\nlabeled, using the nodes features, in conjunction with graph topology. While\nGCNs have been successfully applied to this problem, the caveats that they\ninherit from traditional deep learning models pose significant challenges to\nbroad utilization of GCNs in node classification. One such caveat is that\ntraining a GCN requires a large number of labeled training instances, which is\noften not the case in realistic settings. To remedy this requirement,\nstate-of-the-art methods leverage network diffusion-based approaches to\npropagate labels across the network before training GCNs. However, these\napproaches ignore the tendency of the network diffusion methods in biasing\nproximity with centrality, resulting in the propagation of labels to the nodes\nthat are well-connected in the graph. To address this problem, here we present\nan alternate approach to extrapolating node labels in GCNs in the following\nthree steps: (i) clustering of the network to identify communities, (ii) use of\nnetwork diffusion algorithms to quantify the proximity of each node to the\ncommunities, thereby obtaining a low-dimensional topological profile for each\nnode, (iii) comparing these topological profiles to identify nodes that are\nmost similar to the labeled nodes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:10:16 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Coskun", "Mustafa", ""], ["Gungor", "Burcu Bakir", ""], ["Koyuturk", "Mehmet", ""]]}, {"id": "1912.09588", "submitter": "Andres Potapczynski", "authors": "Andres Potapczynski, Gabriel Loaiza-Ganem, John P. Cunningham", "title": "Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax", "comments": "Accepted at NeurIPS 2020", "journal-ref": "Published: NeurIPS 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gumbel-Softmax is a continuous distribution over the simplex that is\noften used as a relaxation of discrete distributions. Because it can be readily\ninterpreted and easily reparameterized, it enjoys widespread use. We propose a\nmodular and more flexible family of reparameterizable distributions where\nGaussian noise is transformed into a one-hot approximation through an\ninvertible function. This invertible function is composed of a modified softmax\nand can incorporate diverse transformations that serve different specific\npurposes. For example, the stick-breaking procedure allows us to extend the\nreparameterization trick to distributions with countably infinite support, thus\nenabling the use of our distribution along nonparametric models, or normalizing\nflows let us increase the flexibility of the distribution. Our construction\nenjoys theoretical advantages over the Gumbel-Softmax, such as closed form KL,\nand significantly outperforms it in a variety of experiments. Our code is\navailable at https://github.com/cunningham-lab/igr.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 23:11:39 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 19:35:17 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 23:07:40 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 18:26:19 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Potapczynski", "Andres", ""], ["Loaiza-Ganem", "Gabriel", ""], ["Cunningham", "John P.", ""]]}, {"id": "1912.09592", "submitter": "Ihsan Ullah", "authors": "Ihsan Ullah, Mario Manzo, Mitul Shah, Michael Madden", "title": "Graph Convolutional Networks: analysis, improvements and results", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era of neural networks and big data, higher dimensional data\nis processed for automation of different application areas. Graphs represent a\ncomplex data organization in which dependencies between more than one object or\nactivity occur. Due to the high dimensionality, this data creates challenges\nfor machine learning algorithms. Graph convolutional networks were introduced\nto utilize the convolutional models concepts that shows good results. In this\ncontext, we enhanced two of the existing Graph convolutional network models by\nproposing four enhancements. These changes includes: hyper parameters\noptimization, convex combination of activation functions, topological\ninformation enrichment through clustering coefficients measure, and structural\nredesign of the network through addition of dense layers. We present extensive\nresults on four state-of-art benchmark datasets. The performance is notable not\nonly in terms of lesser computational cost compared to competitors, but also\nachieved competitive results for three of the datasets and state-of-the-art for\nthe fourth dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 23:56:18 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ullah", "Ihsan", ""], ["Manzo", "Mario", ""], ["Shah", "Mitul", ""], ["Madden", "Michael", ""]]}, {"id": "1912.09593", "submitter": "Wei Huang", "authors": "Wei Huang, Richard Yi Da Xu", "title": "Gaussian Process Latent Variable Model Factorization for Context-aware\n  Recommender Systems", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommender systems (CARS) have gained increasing attention due\nto their ability to utilize contextual information. Compared to traditional\nrecommender systems, CARS are, in general, able to generate more accurate\nrecommendations. Latent factors approach accounts for a large proportion of\nCARS. Recently, a non-linear Gaussian Process (GP) based factorization method\nwas proven to outperform the state-of-the-art methods in CARS. Despite its\neffectiveness, GP model-based methods can suffer from over-fitting and may not\nbe able to determine the impact of each context automatically. In order to\naddress such shortcomings, we propose a Gaussian Process Latent Variable Model\nFactorization (GPLVMF) method, where we apply an appropriate prior to the\noriginal GP model. Our work is primarily inspired by the Gaussian Process\nLatent Variable Model (GPLVM), which was a non-linear dimensionality reduction\nmethod. As a result, we improve the performance on the real datasets\nsignificantly as well as capturing the importance of each context. In addition\nto the general advantages, our method provides two main contributions regarding\nrecommender system settings: (1) addressing the influence of bias by setting a\nnon-zero mean function, and (2) utilizing real-valued contexts by fixing the\nlatent space with real values.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 23:57:55 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Huang", "Wei", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1912.09595", "submitter": "Johan Samir Obando Ceron", "authors": "Johan S. Obando-Ceron, Victor Romero Cano, Walter Mayor Toro", "title": "Exploiting the potential of deep reinforcement learning for\n  classification tasks in high-dimensional and unstructured data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for efficiently learning feature selection\npolicies which use less features to reach a high classification precision on\nlarge unstructured data. It uses a Deep Convolutional Autoencoder (DCAE) for\nlearning compact feature spaces, in combination with recently-proposed\nReinforcement Learning (RL) algorithms as Double DQN and Retrace.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:27:34 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Obando-Ceron", "Johan S.", ""], ["Cano", "Victor Romero", ""], ["Toro", "Walter Mayor", ""]]}, {"id": "1912.09600", "submitter": "Mohammad Kachuee Mr.", "authors": "Mohammad Kachuee, Sajad Darabi, Shayan Fazeli, Majid Sarrafzadeh", "title": "Group-Connected Multilayer Perceptron Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning in domains such as image, voice, and\ngraphs, there has been little progress in deep representation learning for\ndomains without a known structure between features. For instance, a tabular\ndataset of different demographic and clinical factors where the feature\ninteractions are not given as a prior. In this paper, we propose\nGroup-Connected Multilayer Perceptron (GMLP) networks to enable deep\nrepresentation learning in these domains. GMLP is based on the idea of learning\nexpressive feature combinations (groups) and exploiting them to reduce the\nnetwork complexity by defining local group-wise operations. During the training\nphase, GMLP learns a sparse feature grouping matrix using temperature annealing\nsoftmax with an added entropy loss term to encourage the sparsity. Furthermore,\nan architecture is suggested which resembles binary trees, where group-wise\noperations are followed by pooling operations to combine information; reducing\nthe number of groups as the network grows in depth. To evaluate the proposed\nmethod, we conducted experiments on different real-world datasets covering\nvarious application areas. Additionally, we provide visualizations on MNIST and\nsynthesized data. According to the results, GMLP is able to successfully learn\nand exploit expressive feature combinations and achieve state-of-the-art\nclassification performance on different datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:49:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:22:39 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 16:56:55 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kachuee", "Mohammad", ""], ["Darabi", "Sajad", ""], ["Fazeli", "Shayan", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1912.09614", "submitter": "Alireza Abdoli", "authors": "Sara Alaee, Alireza Abdoli, Christian Shelton, Amy C. Murillo, Alec C.\n  Gerry, Eamonn Keogh", "title": "Features or Shape? Tackling the False Dichotomy of Time Series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification is an important task in its own right, and it is\noften a precursor to further downstream analytics. To date, virtually all works\nin the literature have used either shape-based classification using a distance\nmeasure or feature-based classification after finding some suitable features\nfor the domain. It seems to be underappreciated that in many datasets it is the\ncase that some classes are best discriminated with features, while others are\nbest discriminated with shape. Thus, making the shape vs. feature choice will\ncondemn us to poor results, at least for some classes. In this work, we propose\na new model for classifying time series that allows the use of both shape and\nfeature-based measures, when warranted. Our algorithm automatically decides\nwhich approach is best for which class, and at query time chooses which\nclassifier to trust the most. We evaluate our idea on real world datasets and\ndemonstrate that our ideas produce statistically significant improvement in\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 02:24:41 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Alaee", "Sara", ""], ["Abdoli", "Alireza", ""], ["Shelton", "Christian", ""], ["Murillo", "Amy C.", ""], ["Gerry", "Alec C.", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1912.09621", "submitter": "Barath Narayanan Narayanan", "authors": "Barath Narayanan Narayanan, Manawaduge Supun De Silva, Russell C.\n  Hardie, Nathan K. Kueterman, Redha Ali", "title": "Understanding Deep Neural Network Predictions for Medical Imaging\n  Applications", "comments": "20 pages, 28 Figures and 9 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided detection has been a research area attracting great interest\nin the past decade. Machine learning algorithms have been utilized extensively\nfor this application as they provide a valuable second opinion to the doctors.\nDespite several machine learning models being available for medical imaging\napplications, not many have been implemented in the real-world due to the\nuninterpretable nature of the decisions made by the network. In this paper, we\ninvestigate the results provided by deep neural networks for the detection of\nmalaria, diabetic retinopathy, brain tumor, and tuberculosis in different\nimaging modalities. We visualize the class activation mappings for all the\napplications in order to enhance the understanding of these networks. This type\nof visualization, along with the corresponding network performance metrics,\nwould aid the data science experts in better understanding of their models as\nwell as assisting doctors in their decision-making process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 02:57:05 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Narayanan", "Barath Narayanan", ""], ["De Silva", "Manawaduge Supun", ""], ["Hardie", "Russell C.", ""], ["Kueterman", "Nathan K.", ""], ["Ali", "Redha", ""]]}, {"id": "1912.09624", "submitter": "Can Chen", "authors": "Can Chen and Indika Rajapakse", "title": "Tensor Entropy for Uniform Hypergraphs", "comments": "12 pages, 11 figures, 1 table, IEEE Transactions on Network Science\n  and Engineering, accepted to appear", "journal-ref": null, "doi": "10.1109/TNSE.2020.3002963", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop the notion of entropy for uniform hypergraphs via\ntensor theory. We employ the probability distribution of the generalized\nsingular values, calculated from the higher-order singular value decomposition\nof the Laplacian tensors, to fit into the Shannon entropy formula. We show that\nthis tensor entropy is an extension of von Neumann entropy for graphs. In\naddition, we establish results on the lower and upper bounds of the entropy and\ndemonstrate that it is a measure of regularity for uniform hypergraphs in\nsimulated and experimental data. We exploit the tensor train decomposition in\ncomputing the proposed tensor entropy efficiently. Finally, we introduce the\nnotion of robustness for uniform hypergraphs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:26:45 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 21:18:57 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 18:38:48 GMT"}, {"version": "v4", "created": "Sun, 14 Jun 2020 18:59:37 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Chen", "Can", ""], ["Rajapakse", "Indika", ""]]}, {"id": "1912.09630", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Mandar Pitale and Vasu Singh and Zhangyang Wang", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "comments": "Accepted at Safe AI workshop at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:47:28 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Mohseni", "Sina", ""], ["Pitale", "Mandar", ""], ["Singh", "Vasu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1912.09656", "submitter": "Diego Granziol", "authors": "Diego Granziol, Xingchen Wan, Timur Garipov", "title": "Deep Curvature Suite", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MLRG Deep Curvature suite, a PyTorch-based, open-source package\nfor analysis and visualisation of neural network curvature and loss landscape.\nDespite of providing rich information into properties of neural network and\nuseful for a various designed tasks, curvature information is still not made\nsufficient use for various reasons, and our method aims to bridge this gap. We\npresent a primer, including its main practical desiderata and common\nmisconceptions, of \\textit{Lanczos algorithm}, the theoretical backbone of our\npackage, and present a series of examples based on synthetic toy examples and\nrealistic modern neural networks tested on CIFAR datasets, and show the\nsuperiority of our package against existing competing approaches for the\nsimilar purposes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 06:37:40 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:36:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Granziol", "Diego", ""], ["Wan", "Xingchen", ""], ["Garipov", "Timur", ""]]}, {"id": "1912.09705", "submitter": "Guodong Shi", "authors": "Deming Yuan and Alexandre Proutiere and Guodong Shi", "title": "Distributed Online Optimization with Long-Term Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed online convex optimization problems, where the\ndistributed system consists of various computing units connected through a\ntime-varying communication graph. In each time step, each computing unit\nselects a constrained vector, experiences a loss equal to an arbitrary convex\nfunction evaluated at this vector, and may communicate to its neighbors in the\ngraph. The objective is to minimize the system-wide loss accumulated over time.\nWe propose a decentralized algorithm with regret and cumulative constraint\nviolation in $\\mathcal{O}(T^{\\max\\{c,1-c\\} })$ and $\\mathcal{O}(T^{1-c/2})$,\nrespectively, for any $c\\in (0,1)$, where $T$ is the time horizon. When the\nloss functions are strongly convex, we establish improved regret and constraint\nviolation upper bounds in $\\mathcal{O}(\\log(T))$ and\n$\\mathcal{O}(\\sqrt{T\\log(T)})$. These regret scalings match those obtained by\nstate-of-the-art algorithms and fundamental limits in the corresponding\ncentralized online optimization problem (for both convex and strongly convex\nloss functions). In the case of bandit feedback, the proposed algorithms\nachieve a regret and constraint violation in $\\mathcal{O}(T^{\\max\\{c,1-c/3 \\}\n})$ and $\\mathcal{O}(T^{1-c/2})$ for any $c\\in (0,1)$. We numerically\nillustrate the performance of our algorithms for the particular case of\ndistributed online regularized linear regression problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:07:28 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Yuan", "Deming", ""], ["Proutiere", "Alexandre", ""], ["Shi", "Guodong", ""]]}, {"id": "1912.09713", "submitter": "Marc van Zee", "authors": "Daniel Keysers, Nathanael Sch\\\"arli, Nathan Scales, Hylke Buisman,\n  Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz\n  Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, Olivier\n  Bousquet", "title": "Measuring Compositional Generalization: A Comprehensive Method on\n  Realistic Data", "comments": "Accepted for publication at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning methods exhibit limited compositional\ngeneralization. At the same time, there is a lack of realistic benchmarks that\ncomprehensively measure this ability, which makes it challenging to find and\nevaluate improvements. We introduce a novel method to systematically construct\nsuch benchmarks by maximizing compound divergence while guaranteeing a small\natom divergence between train and test sets, and we quantitatively compare this\nmethod to other approaches for creating compositional generalization\nbenchmarks. We present a large and realistic natural language question\nanswering dataset that is constructed according to this method, and we use it\nto analyze the compositional generalization ability of three machine learning\narchitectures. We find that they fail to generalize compositionally and that\nthere is a surprisingly strong negative correlation between compound divergence\nand accuracy. We also demonstrate how our method can be used to create new\ncompositionality benchmarks on top of the existing SCAN dataset, which confirms\nthese findings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:32:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 06:38:53 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Keysers", "Daniel", ""], ["Sch\u00e4rli", "Nathanael", ""], ["Scales", "Nathan", ""], ["Buisman", "Hylke", ""], ["Furrer", "Daniel", ""], ["Kashubin", "Sergii", ""], ["Momchev", "Nikola", ""], ["Sinopalnikov", "Danila", ""], ["Stafiniak", "Lukasz", ""], ["Tihon", "Tibor", ""], ["Tsarkov", "Dmitry", ""], ["Wang", "Xiao", ""], ["van Zee", "Marc", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1912.09722", "submitter": "Shujie Han", "authors": "Shujie Han, Jun Wu, Erci Xu, Cheng He, Patrick P. C. Lee, Yi Qiang,\n  Qixing Zheng, Tao Huang, Zixi Huang, Rui Li", "title": "Robust Data Preprocessing for Machine-Learning-Based Disk Failure\n  Prediction in Cloud Production Environments", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide proactive fault tolerance for modern cloud data centers, extensive\nstudies have proposed machine learning (ML) approaches to predict imminent disk\nfailures for early remedy and evaluated their approaches directly on public\ndatasets (e.g., Backblaze SMART logs). However, in real-world production\nenvironments, the data quality is imperfect (e.g., inaccurate labeling, missing\ndata samples, and complex failure types), thereby degrading the prediction\naccuracy. We present RODMAN, a robust data preprocessing pipeline that refines\ndata samples before feeding them into ML models. We start with a large-scale\ntrace-driven study of over three million disks from Alibaba Cloud's data\ncenters, and motivate the practical challenges in ML-based disk failure\nprediction. We then design RODMAN with three data preprocessing echniques,\nnamely failure-type filtering, spline-based data filling, and automated\npre-failure backtracking, that are applicable for general ML models. Evaluation\non both the Alibaba and Backblaze datasets shows that RODMAN improves the\nprediction accuracy compared to without data preprocessing under various\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:43:25 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Han", "Shujie", ""], ["Wu", "Jun", ""], ["Xu", "Erci", ""], ["He", "Cheng", ""], ["Lee", "Patrick P. C.", ""], ["Qiang", "Yi", ""], ["Zheng", "Qixing", ""], ["Huang", "Tao", ""], ["Huang", "Zixi", ""], ["Li", "Rui", ""]]}, {"id": "1912.09733", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin", "title": "An adaptive simulated annealing EM algorithm for inference on\n  non-homogeneous hidden Markov models", "comments": "8 pages, 6 figures, 4 tables. Accepted version of the article\n  published in AIIPCC 2019", "journal-ref": null, "doi": "10.1145/3371425.3371641", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-homogeneous hidden Markov models (NHHMM) are a subclass of dependent\nmixture models used for semi-supervised learning, where both transition\nprobabilities between the latent states and mean parameter of the probability\ndistribution of the responses (for a given state) depend on the set of $p$\ncovariates. A priori we do not know which (and how) covariates influence the\ntransition probabilities and the mean parameters. This induces a complex\ncombinatorial optimization problem for model selection with $4^p$ potential\nconfigurations. To address the problem, in this article we propose an adaptive\n(A) simulated annealing (SA) expectation maximization (EM) algorithm (ASA-EM)\nfor joint optimization of models and their parameters with respect to a\ncriterion of interest.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:03:53 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Hubin", "Aliaksandr", ""]]}, {"id": "1912.09784", "submitter": "Chongxuan Li", "authors": "Chongxuan Li, Kun Xu, Jiashuo Liu, Jun Zhu, Bo Zhang", "title": "Triple Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified game-theoretical framework to perform classification and\nconditional image generation given limited supervision. It is formulated as a\nthree-player minimax game consisting of a generator, a classifier and a\ndiscriminator, and therefore is referred to as Triple Generative Adversarial\nNetwork (Triple-GAN). The generator and the classifier characterize the\nconditional distributions between images and labels to perform conditional\ngeneration and classification, respectively. The discriminator solely focuses\non identifying fake image-label pairs. Under a nonparametric assumption, we\nprove the unique equilibrium of the game is that the distributions\ncharacterized by the generator and the classifier converge to the data\ndistribution. As a byproduct of the three-player mechanism, Triple-GAN is\nflexible to incorporate different semi-supervised classifiers and GAN\narchitectures. We evaluate Triple-GAN in two challenging settings, namely,\nsemi-supervised learning and the extreme low data regime. In both settings,\nTriple-GAN can achieve excellent classification results and generate meaningful\nsamples in a specific class simultaneously. In particular, using a commonly\nadopted 13-layer CNN classifier, Triple-GAN outperforms extensive\nsemi-supervised learning methods substantially on more than 10 benchmarks no\nmatter data augmentation is applied or not.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:17:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 10:09:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Li", "Chongxuan", ""], ["Xu", "Kun", ""], ["Liu", "Jiashuo", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1912.09789", "submitter": "Jan S. Rellermeyer", "authors": "Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen\n  Kloppenburg, Tim Verbelen and Jan S. Rellermeyer", "title": "A Survey on Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for artificial intelligence has grown significantly over the last\ndecade and this growth has been fueled by advances in machine learning\ntechniques and the ability to leverage hardware acceleration. However, in order\nto increase the quality of predictions and render machine learning solutions\nfeasible for more complex applications, a substantial amount of training data\nis required. Although small machine learning models can be trained with modest\namounts of data, the input for training larger models such as neural networks\ngrows exponentially with the number of parameters. Since the demand for\nprocessing training data has outpaced the increase in computation power of\ncomputing machinery, there is a need for distributing the machine learning\nworkload across multiple machines, and turning the centralized into a\ndistributed system. These distributed systems present new challenges, first and\nforemost the efficient parallelization of the training process and the creation\nof a coherent model. This article provides an extensive overview of the current\nstate-of-the-art in the field by outlining the challenges and opportunities of\ndistributed machine learning over conventional (centralized) machine learning,\ndiscussing the techniques used for distributed machine learning, and providing\nan overview of the systems that are available.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:24:25 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Verbraeken", "Joost", ""], ["Wolting", "Matthijs", ""], ["Katzy", "Jonathan", ""], ["Kloppenburg", "Jeroen", ""], ["Verbelen", "Tim", ""], ["Rellermeyer", "Jan S.", ""]]}, {"id": "1912.09802", "submitter": "Markus Nagel", "authors": "Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen\n  Blankevoort, Max Welling", "title": "Taxonomy and Evaluation of Structured Compression of Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks in many real-world applications is\nleading to new challenges in building more efficient architectures. One\neffective way of making networks more efficient is neural network compression.\nWe provide an overview of existing neural network compression methods that can\nbe used to make neural networks more efficient by changing the architecture of\nthe network. First, we introduce a new way to categorize all published\ncompression methods, based on the amount of data and compute needed to make the\nmethods work in practice. These are three 'levels of compression solutions'.\nSecond, we provide a taxonomy of tensor factorization based and probabilistic\ncompression methods. Finally, we perform an extensive evaluation of different\ncompression techniques from the literature for models trained on ImageNet. We\nshow that SVD and probabilistic compression or pruning methods are\ncomplementary and give the best results of all the considered methods. We also\nprovide practical ways to combine them.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:11:44 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kuzmin", "Andrey", ""], ["Nagel", "Markus", ""], ["Pitre", "Saurabh", ""], ["Pendyam", "Sandeep", ""], ["Blankevoort", "Tijmen", ""], ["Welling", "Max", ""]]}, {"id": "1912.09817", "submitter": "Marharyta Aleksandrova", "authors": "Marharyta Aleksandrova and Oleg Chertov", "title": "SCR-Apriori for Mining `Sets of Contrasting Rules'", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient algorithm for mining novel `Set of\nContrasting Rules'-pattern (SCR-pattern), which consists of several association\nrules. This pattern is of high interest due to the guaranteed quality of the\nrules forming it and its ability to discover useful knowledge. However,\nSCR-pattern has no efficient mining algorithm. We propose SCR-Apriori\nalgorithm, which results in the same set of SCR-patterns as the\nstate-of-the-art approache, but is less computationally expensive. We also show\nexperimentally that by incorporating the knowledge about the pattern structure\ninto Apriori algorithm, SCR-Apriori can significantly prune the search space of\nfrequent itemsets to be analysed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:36:10 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Aleksandrova", "Marharyta", ""], ["Chertov", "Oleg", ""]]}, {"id": "1912.09818", "submitter": "Leon Sixt", "authors": "Leon Sixt, Maximilian Granz, Tim Landgraf", "title": "When Explanations Lie: Why Many Modified BP Attributions Fail", "comments": "Published in ICML 2020, Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods aim to explain a neural network's prediction by\nhighlighting the most relevant image areas. A popular approach is to\nbackpropagate (BP) a custom relevance score using modified rules, rather than\nthe gradient. We analyze an extensive set of modified BP methods: Deep Taylor\nDecomposition, Layer-wise Relevance Propagation (LRP), Excitation BP,\nPatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find\nempirically that the explanations of all mentioned methods, except for\nDeepLIFT, are independent of the parameters of later layers. We provide\ntheoretical insights for this surprising behavior and also analyze why DeepLIFT\ndoes not suffer from this limitation. Empirically, we measure how information\nof later layers is ignored by using our new metric, cosine similarity\nconvergence (CSC). The paper provides a framework to assess the faithfulness of\nnew and existing modified BP methods theoretically and empirically. For code\nsee: https://github.com/berleon/when-explanations-lie\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 13:46:31 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 07:00:17 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 17:45:40 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 16:39:10 GMT"}, {"version": "v5", "created": "Thu, 7 May 2020 19:24:58 GMT"}, {"version": "v6", "created": "Thu, 13 Aug 2020 14:12:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sixt", "Leon", ""], ["Granz", "Maximilian", ""], ["Landgraf", "Tim", ""]]}, {"id": "1912.09831", "submitter": "Gabrielle Ras", "authors": "Gabri\\\"elle Ras, Ron Dotsch, Luca Ambrogioni, Umut G\\\"u\\c{c}l\\\"u,\n  Marcel A. J. van Gerven", "title": "Background Hardly Matters: Understanding Personality Attribution in Deep\n  Residual Networks", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceived personality traits attributed to an individual do not have to\ncorrespond to their actual personality traits and may be determined in part by\nthe context in which one encounters a person. These apparent traits determine,\nto a large extent, how other people will behave towards them. Deep neural\nnetworks are increasingly being used to perform automated personality\nattribution (e.g., job interviews). It is important that we understand the\ndriving factors behind the predictions, in humans and in deep neural networks.\nThis paper explicitly studies the effect of the image background on apparent\npersonality prediction while addressing two important confounds present in\nexisting literature; overlapping data splits and including facial information\nin the background. Surprisingly, we found no evidence that background\ninformation improves model predictions for apparent personality traits. In\nfact, when background is explicitly added to the input, a decrease in\nperformance was measured across all models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:11:29 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ras", "Gabri\u00eblle", ""], ["Dotsch", "Ron", ""], ["Ambrogioni", "Luca", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1912.09848", "submitter": "Yuri G. Gordienko", "authors": "Peng Gang, Wei Zeng, Yuri Gordienko, Oleksandr Rokovyi, Oleg Alienin,\n  and Sergii Stirenko", "title": "Prediction of Physical Load Level by Machine Learning Analysis of Heart\n  Activity after Exercises", "comments": "6 pages, 8 figures, 3 tables; preprint of paper for 2019 IEEE\n  Symposium Series on Computational Intelligence (SSCI), December 6-9 2019,\n  Xiamen, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of energy expenditure in real life is of great importance for\nmonitoring the current physical state of people, especially in work, sport,\nelderly care, health care, and everyday life even. This work reports about\napplication of some machine learning methods (linear regression, linear\ndiscriminant analysis, k-nearest neighbors, decision tree, random forest,\nGaussian naive Bayes, support-vector machine) for monitoring energy\nexpenditures in athletes. The classification problem was to predict the known\nlevel of the in-exercise loads (in three categories by calories) by the heart\nrate activity features measured during the short period of time (1 minute only)\nafter training, i.e by features of the post-exercise load. The results obtained\nshown that the post-exercise heart activity features preserve the information\nof the in-exercise training loads and allow us to predict their actual\nin-exercise levels. The best performance can be obtained by the random forest\nclassifier with all 8 heart rate features (micro-averaged area under curve\nvalue AUCmicro = 0.87 and macro-averaged one AUCmacro = 0.88) and the k-nearest\nneighbors classifier with 4 most important heart rate features (AUCmicro = 0.91\nand AUCmacro = 0.89). The limitations and perspectives of the ML methods used\nare outlined, and some practical advices are proposed as to their improvement\nand implementation for the better prediction of in-exercise energy\nexpenditures.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:35:49 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Gang", "Peng", ""], ["Zeng", "Wei", ""], ["Gordienko", "Yuri", ""], ["Rokovyi", "Oleksandr", ""], ["Alienin", "Oleg", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1912.09855", "submitter": "Maximilian Bachl", "authors": "Alexander Hartl, Maximilian Bachl, Joachim Fabini, Tanja Zseby", "title": "Explainability and Adversarial Robustness for RNNs", "comments": "Accepted at IEEE BigDataService 2020", "journal-ref": "2020 IEEE Sixth International Conference on Big Data Computing\n  Service and Applications (BigDataService)", "doi": "10.1109/BigDataService49289.2020.00030", "report-no": null, "categories": "cs.LG cs.CR cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNNs) yield attractive properties for constructing\nIntrusion Detection Systems (IDSs) for network data. With the rise of\nubiquitous Machine Learning (ML) systems, malicious actors have been catching\nup quickly to find new ways to exploit ML vulnerabilities for profit. Recently\ndeveloped adversarial ML techniques focus on computer vision and their\napplicability to network traffic is not straightforward: Network packets expose\nfewer features than an image, are sequential and impose several constraints on\ntheir features.\n  We show that despite these completely different characteristics, adversarial\nsamples can be generated reliably for RNNs. To understand a classifier's\npotential for misclassification, we extend existing explainability techniques\nand propose new ones, suitable particularly for sequential data. Applying them\nshows that already the first packets of a communication flow are of crucial\nimportance and are likely to be targeted by attackers. Feature importance\nmethods show that even relatively unimportant features can be effectively\nabused to generate adversarial samples. Since traditional evaluation metrics\nsuch as accuracy are not sufficient for quantifying the adversarial threat, we\npropose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a\ncommon notion of adversarial robustness, and show that an adversarial training\nprocedure can significantly and successfully reduce the attack surface.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:47:09 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 13:23:07 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Hartl", "Alexander", ""], ["Bachl", "Maximilian", ""], ["Fabini", "Joachim", ""], ["Zseby", "Tanja", ""]]}, {"id": "1912.09859", "submitter": "Dixing Xu", "authors": "Dixing Xu, Mengyao Zheng, Linshan Jiang, Chaojie Gu, Rui Tan and Peng\n  Cheng", "title": "Lightweight and Unobtrusive Data Obfuscation at IoT Edge for Remote\n  Inference", "comments": "This paper has been accepted by IEEE Internet of Things Journal,\n  Special Issue on Artificial Intelligence Powered Edge Computing for Internet\n  of Things", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executing deep neural networks for inference on the server-class or cloud\nbackend based on data generated at the edge of Internet of Things is desirable\ndue primarily to the limited compute power of edge devices and the need to\nprotect the confidentiality of the inference neural networks. However, such a\nremote inference scheme incurs concerns regarding the privacy of the inference\ndata transmitted by the edge devices to the curious backend. This paper\npresents a lightweight and unobtrusive approach to obfuscate the inference data\nat the edge devices. It is lightweight in that the edge device only needs to\nexecute a small-scale neural network; it is unobtrusive in that the edge device\ndoes not need to indicate whether obfuscation is applied. Extensive evaluation\nby three case studies of free spoken digit recognition, handwritten digit\nrecognition, and American sign language recognition shows that our approach\neffectively protects the confidentiality of the raw forms of the inference data\nwhile effectively preserving the backend's inference accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:58:19 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 18:10:10 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 04:10:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Xu", "Dixing", ""], ["Zheng", "Mengyao", ""], ["Jiang", "Linshan", ""], ["Gu", "Chaojie", ""], ["Tan", "Rui", ""], ["Cheng", "Peng", ""]]}, {"id": "1912.09867", "submitter": "William L Hamilton", "authors": "Avishek Joey Bose, Ankit Jain, Piero Molino, and William L. Hamilton", "title": "Meta-Graph: Few Shot Link Prediction via Meta Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of few shot link prediction on graphs. The goal is to\nlearn from a distribution over graphs so that a model is able to quickly infer\nmissing edges in a new graph after a small amount of training. We show that\ncurrent link prediction methods are generally ill-equipped to handle this task.\nThey cannot effectively transfer learned knowledge from one graph to another\nand are unable to effectively learn from sparse samples of edges. To address\nthis challenge, we introduce a new gradient-based meta learning framework,\nMeta-Graph. Our framework leverages higher-order gradients along with a learned\ngraph signature function that conditionally generates a graph neural network\ninitialization. Using a novel set of few shot link prediction benchmarks, we\nshow that Meta-Graph can learn to quickly adapt to a new graph using only a\nsmall sample of true edges, enabling not only fast adaptation but also improved\nresults at convergence.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:09:50 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 21:03:29 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bose", "Avishek Joey", ""], ["Jain", "Ankit", ""], ["Molino", "Piero", ""], ["Hamilton", "William L.", ""]]}, {"id": "1912.09893", "submitter": "Federico Errica", "authors": "Federico Errica, Marco Podda, Davide Bacciu, Alessio Micheli", "title": "A Fair Comparison of Graph Neural Networks for Graph Classification", "comments": "Proceedings of the International Conference on Learning\n  Representations (ICLR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental reproducibility and replicability are critical topics in machine\nlearning. Authors have often raised concerns about their lack in scientific\npublications to improve the quality of the field. Recently, the graph\nrepresentation learning field has attracted the attention of a wide research\ncommunity, which resulted in a large stream of works. As such, several Graph\nNeural Network models have been developed to effectively tackle graph\nclassification. However, experimental procedures often lack rigorousness and\nare hardly reproducible. Motivated by this, we provide an overview of common\npractices that should be avoided to fairly compare with the state of the art.\nTo counter this troubling trend, we ran more than 47000 experiments in a\ncontrolled and uniform framework to re-evaluate five popular models across nine\ncommon benchmarks. Moreover, by comparing GNNs with structure-agnostic\nbaselines we provide convincing evidence that, on some datasets, structural\ninformation has not been exploited yet. We believe that this work can\ncontribute to the development of the graph learning field, by providing a much\nneeded grounding for rigorous evaluations of graph classification models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:40:50 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 13:49:46 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Errica", "Federico", ""], ["Podda", "Marco", ""], ["Bacciu", "Davide", ""], ["Micheli", "Alessio", ""]]}, {"id": "1912.09899", "submitter": "Jinyuan Jia", "authors": "Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong", "title": "Certified Robustness for Top-k Predictions against Adversarial\n  Perturbations via Randomized Smoothing", "comments": "ICLR 2020, code is available at this:\n  https://github.com/jjy1994/Certify_Topk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that classifiers are vulnerable to adversarial\nperturbations. To defend against adversarial perturbations, various certified\nrobustness results have been derived. However, existing certified robustnesses\nare limited to top-1 predictions. In many real-world applications, top-$k$\npredictions are more relevant. In this work, we aim to derive certified\nrobustness for top-$k$ predictions. In particular, our certified robustness is\nbased on randomized smoothing, which turns any classifier to a new classifier\nvia adding noise to an input example. We adopt randomized smoothing because it\nis scalable to large-scale neural networks and applicable to any classifier. We\nderive a tight robustness in $\\ell_2$ norm for top-$k$ predictions when using\nrandomized smoothing with Gaussian noise. We find that generalizing the\ncertified robustness from top-1 to top-$k$ predictions faces significant\ntechnical challenges. We also empirically evaluate our method on CIFAR10 and\nImageNet. For example, our method can obtain an ImageNet classifier with a\ncertified top-5 accuracy of 62.8\\% when the $\\ell_2$-norms of the adversarial\nperturbations are less than 0.5 (=127/255). Our code is publicly available at:\n\\url{https://github.com/jjy1994/Certify_Topk}.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:54:51 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Jia", "Jinyuan", ""], ["Cao", "Xiaoyu", ""], ["Wang", "Binghui", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "1912.09902", "submitter": "Molly O'Brien", "authors": "Molly O'Brien, William Goble, Greg Hager, Julia Bukowski", "title": "Dependable Neural Networks for Safety Critical Tasks", "comments": "8 pages, 4 figures. Accepted to AAAI EDSMLS Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks are being integrated into safety critical systems, e.g.,\nperception systems for autonomous vehicles, which require trained networks to\nperform safely in novel scenarios. It is challenging to verify neural networks\nbecause their decisions are not explainable, they cannot be exhaustively\ntested, and finite test samples cannot capture the variation across all\noperating conditions. Existing work seeks to train models robust to new\nscenarios via domain adaptation, style transfer, or few-shot learning. But\nthese techniques fail to predict how a trained model will perform when the\noperating conditions differ from the testing conditions. We propose a metric,\nMachine Learning (ML) Dependability, that measures the network's probability of\nsuccess in specified operating conditions which need not be the testing\nconditions. In addition, we propose the metrics Task Undependability and\nHarmful Undependability to distinguish network failures by their consequences.\nWe evaluate the performance of a Neural Network agent trained using\nReinforcement Learning in a simulated robot manipulation task. Our results\ndemonstrate that we can accurately predict the ML Dependability, Task\nUndependability, and Harmful Undependability for operating conditions that are\nsignificantly different from the testing conditions. Finally, we design a\nSafety Function, using harmful failures identified during testing, that reduces\nharmful failures, in one example, by a factor of 700 while maintaining a high\nprobability of success.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:03:10 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["O'Brien", "Molly", ""], ["Goble", "William", ""], ["Hager", "Greg", ""], ["Bukowski", "Julia", ""]]}, {"id": "1912.09926", "submitter": "Yuzheng Hu", "authors": "Yuzheng Hu and Licong Lin and Shange Tang", "title": "Second-order Information in First-order Optimization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to uncover the second-order essence of several\nfirst-order optimization methods. For Nesterov Accelerated Gradient, we\nrigorously prove that the algorithm makes use of the difference between past\nand current gradients, thus approximates the Hessian and accelerates the\ntraining. For adaptive methods, we related Adam and Adagrad to a powerful\ntechnique in computation statistics---Natural Gradient Descent. These adaptive\nmethods can in fact be treated as relaxations of NGD with only a slight\ndifference lying in the square root of the denominator in the update rules.\nSkeptical about the effect of such difference, we design a new\nalgorithm---AdaSqrt, which removes the square root in the denominator and\nscales the learning rate by sqrt(T). Surprisingly, our new algorithm is\ncomparable to various first-order methods(such as SGD and Adam) on MNIST and\neven beats Adam on CIFAR-10! This phenomenon casts doubt on the convention view\nthat the square root is crucial and training without it will lead to terrible\nperformance. As far as we have concerned, so long as the algorithm tries to\nexplore second or even higher information of the loss surface, then proper\nscaling of the learning rate alone will guarantee fast training and good\ngeneralization performance. To the best of our knowledge, this is the first\npaper that seriously considers the necessity of square root among all adaptive\nmethods. We believe that our work can shed light on the importance of\nhigher-order information and inspire the design of more powerful algorithms in\nthe future.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:36:15 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Hu", "Yuzheng", ""], ["Lin", "Licong", ""], ["Tang", "Shange", ""]]}, {"id": "1912.09953", "submitter": "James Townsend", "authors": "James Townsend, Thomas Bird, Julius Kunze, David Barber", "title": "HiLLoC: Lossless Image Compression with Hierarchical Latent Variable\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make the following striking observation: fully convolutional VAE models\ntrained on 32x32 ImageNet can generalize well, not just to 64x64 but also to\nfar larger photographs, with no changes to the model. We use this property,\napplying fully convolutional models to lossless compression, demonstrating a\nmethod to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless\ncompression to large color photographs, and achieving state of the art for\ncompression of full size ImageNet images. We release Craystack, an open source\nlibrary for convenient prototyping of lossless compression using probabilistic\nmodels, along with full implementations of all of our compression results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 17:20:38 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Townsend", "James", ""], ["Bird", "Thomas", ""], ["Kunze", "Julius", ""], ["Barber", "David", ""]]}, {"id": "1912.09989", "submitter": "Hai Shu", "authors": "Hai Shu, Zhe Qu", "title": "CDPA: Common and Distinctive Pattern Analysis between High-dimensional\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A representative model in integrative analysis of two high-dimensional\ncorrelated datasets is to decompose each data matrix into a low-rank common\nmatrix generated by latent factors shared across datasets, a low-rank\ndistinctive matrix corresponding to each dataset, and an additive noise matrix.\nExisting decomposition methods claim that their common matrices capture the\ncommon pattern of the two datasets. However, their so-called common pattern\nonly denotes the common latent factors but ignores the common pattern between\nthe two coefficient matrices of these common latent factors. We propose a new\nunsupervised learning method, called the common and distinctive pattern\nanalysis (CDPA), which appropriately defines the two types of data patterns by\nfurther incorporating the common and distinctive patterns of the coefficient\nmatrices. A consistent estimation approach is developed for high-dimensional\nsettings, and shows reasonably good finite-sample performance in simulations.\nOur simulation studies and real data analysis corroborate that the proposed\nCDPA can provide better characterization of common and distinctive patterns and\nthereby benefit data mining.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:21:19 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 07:01:37 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 16:44:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shu", "Hai", ""], ["Qu", "Zhe", ""]]}, {"id": "1912.09996", "submitter": "Konrad Czechowski", "authors": "Piotr Mi{\\l}o\\'s, {\\L}ukasz Kuci\\'nski, Konrad Czechowski, Piotr\n  Kozakowski, Maciek Klimek", "title": "Uncertainty-sensitive Learning and Planning with Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning framework for discrete environments in\nwhich an agent makes both strategic and tactical decisions. The former\nmanifests itself through the use of value function, while the latter is powered\nby a tree search planner. These tools complement each other. The planning\nmodule performs a local \\textit{what-if} analysis, which allows to avoid\ntactical pitfalls and boost backups of the value function. The value function,\nbeing global in nature, compensates for inherent locality of the planner. In\norder to further solidify this synergy, we introduce an exploration mechanism\nwith two distinctive components: uncertainty modelling and risk measurement. To\nmodel the uncertainty we use value function ensembles, and to reflect risk we\nuse propose several functionals that summarize the implied by the ensemble. We\nshow that our method performs well on hard exploration environments: Deep-sea,\ntoy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up in\nlearning and boost in performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 17:58:25 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 23:00:07 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 17:47:28 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Mi\u0142o\u015b", "Piotr", ""], ["Kuci\u0144ski", "\u0141ukasz", ""], ["Czechowski", "Konrad", ""], ["Kozakowski", "Piotr", ""], ["Klimek", "Maciek", ""]]}, {"id": "1912.10013", "submitter": "Battista Biggio", "authors": "Marco Melis and Ambra Demontis and Maura Pintor and Angelo Sotgiu and\n  Battista Biggio", "title": "secml: A Python Library for Secure and Explainable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present secml, an open-source Python library for secure and explainable\nmachine learning. It implements the most popular attacks against machine\nlearning, including not only test-time evasion attacks to generate adversarial\nexamples against deep neural networks, but also training-time poisoning attacks\nagainst support vector machines and many other algorithms. These attacks enable\nevaluating the security of learning algorithms and of the corresponding\ndefenses under both white-box and black-box threat models. To this end, secml\nprovides built-in functions to compute security evaluation curves, showing how\nquickly classification performance decreases against increasing adversarial\nperturbations of the input data. secml also includes explainability methods to\nhelp understand why adversarial attacks succeed against a given model, by\nvisualizing the most influential features and training prototypes contributing\nto each decision. It is distributed under the Apache License 2.0, and hosted at\nhttps://gitlab.com/secml/secml.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:41:37 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Melis", "Marco", ""], ["Demontis", "Ambra", ""], ["Pintor", "Maura", ""], ["Sotgiu", "Angelo", ""], ["Biggio", "Battista", ""]]}, {"id": "1912.10065", "submitter": "Ethan Weinberger", "authors": "Ethan Weinberger, Joseph Janizek, Su-In Lee", "title": "Learning Deep Attribution Priors Based On Prior Knowledge", "comments": "NeurIPS 2020 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature attribution methods, which explain an individual prediction made by a\nmodel as a sum of attributions for each input feature, are an essential tool\nfor understanding the behavior of complex deep learning models. However,\nensuring that models produce meaningful explanations, rather than ones that\nrely on noise, is not straightforward. Exacerbating this problem is the fact\nthat attribution methods do not provide insight as to why features are assigned\ntheir attribution values, leading to explanations that are difficult to\ninterpret. In real-world problems we often have sets of additional information\nfor each feature that are predictive of that feature's importance to the task\nat hand. Here, we propose the deep attribution prior (DAPr) framework to\nexploit such information to overcome the limitations of attribution methods.\nOur framework jointly learns a relationship between prior information and\nfeature importance, as well as biases models to have explanations that rely on\nfeatures predicted to be important. We find that our framework both results in\nnetworks that generalize better to out of sample data and admits new methods\nfor interpreting model behavior.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:17:07 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 22:53:20 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 23:02:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Weinberger", "Ethan", ""], ["Janizek", "Joseph", ""], ["Lee", "Su-In", ""]]}, {"id": "1912.10068", "submitter": "Sarah Dean", "authors": "Sarah Dean, Sarah Rich, Benjamin Recht", "title": "Recommendations and User Agency: The Reachability of\n  Collaboratively-Filtered Information", "comments": "appeared at FAccT '20", "journal-ref": null, "doi": "10.1145/3351095.3372866", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often rely on models which are trained to maximize\naccuracy in predicting user preferences. When the systems are deployed, these\nmodels determine the availability of content and information to different\nusers. The gap between these objectives gives rise to a potential for\nunintended consequences, contributing to phenomena such as filter bubbles and\npolarization. In this work, we consider directly the information availability\nproblem through the lens of user recourse. Using ideas of reachability, we\npropose a computationally efficient audit for top-$N$ linear recommender\nmodels. Furthermore, we describe the relationship between model complexity and\nthe effort necessary for users to exert control over their recommendations. We\nuse this insight to provide a novel perspective on the user cold-start problem.\nFinally, we demonstrate these concepts with an empirical investigation of a\nstate-of-the-art model trained on a widely used movie ratings dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:23:05 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 01:23:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dean", "Sarah", ""], ["Rich", "Sarah", ""], ["Recht", "Benjamin", ""]]}, {"id": "1912.10070", "submitter": "Jonathan Lwowski", "authors": "Isaac Corley, Jonathan Lwowski, and Justin Hoffman", "title": "Destruction of Image Steganography using Generative Adversarial Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG eess.IV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital image steganalysis, or the detection of image steganography, has been\nstudied in depth for years and is driven by Advanced Persistent Threat (APT)\ngroups', such as APT37 Reaper, utilization of steganographic techniques to\ntransmit additional malware to perform further post-exploitation activity on a\ncompromised host. However, many steganalysis algorithms are constrained to work\nwith only a subset of all possible images in the wild or are known to produce a\nhigh false positive rate. This results in blocking any suspected image being an\nunreasonable policy. A more feasible policy is to filter suspicious images\nprior to reception by the host machine. However, how does one optimally filter\nspecifically to obfuscate or remove image steganography while avoiding\ndegradation of visual image quality in the case that detection of the image was\na false positive? We propose the Deep Digital Steganography Purifier (DDSP), a\nGenerative Adversarial Network (GAN) which is optimized to destroy\nsteganographic content without compromising the perceptual quality of the\noriginal image. As verified by experimental results, our model is capable of\nproviding a high rate of destruction of steganographic image content while\nmaintaining a high visual quality in comparison to other state-of-the-art\nfiltering methods. Additionally, we test the transfer learning capability of\ngeneralizing to to obfuscate real malware payloads embedded into different\nimage file formats and types using an unseen steganographic algorithm and prove\nthat our model can in fact be deployed to provide adequate results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:23:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Corley", "Isaac", ""], ["Lwowski", "Jonathan", ""], ["Hoffman", "Justin", ""]]}, {"id": "1912.10077", "submitter": "Chulhee Yun", "authors": "Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J.\n  Reddi, Sanjiv Kumar", "title": "Are Transformers universal approximators of sequence-to-sequence\n  functions?", "comments": "23 pages, ICLR 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread adoption of Transformer models for NLP tasks, the\nexpressive power of these models is not well-understood. In this paper, we\nestablish that Transformer models are universal approximators of continuous\npermutation equivariant sequence-to-sequence functions with compact support,\nwhich is quite surprising given the amount of shared parameters in these\nmodels. Furthermore, using positional encodings, we circumvent the restriction\nof permutation equivariance, and show that Transformer models can universally\napproximate arbitrary continuous sequence-to-sequence functions on a compact\ndomain. Interestingly, our proof techniques clearly highlight the different\nroles of the self-attention and the feed-forward layers in Transformers. In\nparticular, we prove that fixed width self-attention layers can compute\ncontextual mappings of the input sequences, playing a key role in the universal\napproximation property of Transformers. Based on this insight from our\nanalysis, we consider other simpler alternatives to self-attention layers and\nempirically evaluate them.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:49:32 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 03:12:57 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Yun", "Chulhee", ""], ["Bhojanapalli", "Srinadh", ""], ["Rawat", "Ankit Singh", ""], ["Reddi", "Sashank J.", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1912.10080", "submitter": "Tiago Alves", "authors": "Tiago Alves, Alberto Laender, Adriano Veloso, Nivio Ziviani", "title": "Dynamic Prediction of ICU Mortality Risk Using Domain Adaptation", "comments": null, "journal-ref": "2018 IEEE International Conference on Big Data", "doi": "10.1109/BigData.2018.8621927", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early recognition of risky trajectories during an Intensive Care Unit (ICU)\nstay is one of the key steps towards improving patient survival. Learning\ntrajectories from physiological signals continuously measured during an ICU\nstay requires learning time-series features that are robust and discriminative\nacross diverse patient populations. Patients within different ICU populations\n(referred here as domains) vary by age, conditions and interventions. Thus,\nmortality prediction models using patient data from a particular ICU population\nmay perform suboptimally in other populations because the features used to\ntrain such models have different distributions across the groups. In this\npaper, we explore domain adaptation strategies in order to learn mortality\nprediction models that extract and transfer complex temporal features from\nmultivariate time-series ICU data. Features are extracted in a way that the\nstate of the patient in a certain time depends on the previous state. This\nenables dynamic predictions and creates a mortality risk space that describes\nthe risk of a patient at a particular time. Experiments based on cross-ICU\npopulations reveals that our model outperforms all considered baselines. Gains\nin terms of AUC range from 4% to 8% for early predictions when compared with a\nrecent state-of-the-art representative for ICU mortality prediction. In\nparticular, models for the Cardiac ICU population achieve AUC numbers as high\nas 0.88, showing excellent clinical utility for early mortality prediction.\nFinally, we present an explanation of factors contributing to the possible ICU\noutcomes, so that our models can be used to complement clinical reasoning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:59:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Alves", "Tiago", ""], ["Laender", "Alberto", ""], ["Veloso", "Adriano", ""], ["Ziviani", "Nivio", ""]]}, {"id": "1912.10087", "submitter": "Matteo Grimaldi", "authors": "Matteo Grimaldi, Valentino Peluso, Andrea Calimera", "title": "EAST: Encoding-Aware Sparse Training for Deep Memory Compression of\n  ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of Deep Convolutional Neural Networks (ConvNets) on tiny\nend-nodes with limited non-volatile memory space calls for smart compression\nstrategies capable of shrinking the footprint yet preserving predictive\naccuracy. There exist a number of strategies for this purpose, from those that\nplay with the topology of the model or the arithmetic precision, e.g. pruning\nand quantization, to those that operate a model agnostic compression, e.g.\nweight encoding. The tighter the memory constraint, the higher the probability\nthat these techniques alone cannot meet the requirement, hence more awareness\nand cooperation across different optimizations become mandatory. This work\naddresses the issue by introducing EAST, Encoding-Aware Sparse Training, a\nnovel memory-constrained training procedure that leads quantized ConvNets\ntowards deep memory compression. EAST implements an adaptive group pruning\ndesigned to maximize the compression rate of the weight encoding scheme (the\nLZ4 algorithm in this work). If compared to existing methods, EAST meets the\nmemory constraint with lower sparsity, hence ensuring higher accuracy. Results\nconducted on a state-of-the-art ConvNet (ResNet-9) deployed on a low-power\nmicrocontroller (ARM Cortex-M4) validate the proposal.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:20:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Grimaldi", "Matteo", ""], ["Peluso", "Valentino", ""], ["Calimera", "Andrea", ""]]}, {"id": "1912.10094", "submitter": "Rongjie Lai", "authors": "Stefan Schonsheck, Jie Chen, Rongjie Lai", "title": "Chart Auto-Encoders for Manifold Structured Data", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have made tremendous advances in image and signal\nrepresentation learning and generation. These models employ the full Euclidean\nspace or a bounded subset as the latent space, whose flat geometry, however, is\noften too simplistic to meaningfully reflect the manifold structure of the\ndata. In this work, we advocate the use of a multi-chart latent space for\nbetter data representation. Inspired by differential geometry, we propose a\n\\textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem\non its representation capability. We show that the training data size and the\nnetwork size scale exponentially in approximation error with an exponent\ndepending on the intrinsic dimension of the data manifold. CAE admits desirable\nmanifold properties that auto-encoders with a flat latent space fail to obey,\npredominantly proximity of data. We conduct extensive experimentation with\nsynthetic and real-life examples to demonstrate that CAE provides\nreconstruction with high fidelity, preserves proximity in the latent space, and\ngenerates new data remaining near the manifold. These experiments show that CAE\nis advantageous over existing auto-encoders and variants by preserving the\ntopology of the data manifold as well as its geometry.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:46:51 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 20:54:56 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Schonsheck", "Stefan", ""], ["Chen", "Jie", ""], ["Lai", "Rongjie", ""]]}, {"id": "1912.10095", "submitter": "Alexander Shevchenko", "authors": "Alexander Shevchenko and Marco Mondelli", "title": "Landscape Connectivity and Dropout Stability of SGD Solutions for\n  Over-parameterized Neural Networks", "comments": "Proceedings of the 37th International Conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of multilayer neural networks typically leads to a solution\nwith zero training error, yet the landscape can exhibit spurious local minima\nand the minima can be disconnected. In this paper, we shed light on this\nphenomenon: we show that the combination of stochastic gradient descent (SGD)\nand over-parameterization makes the landscape of multilayer neural networks\napproximately connected and thus more favorable to optimization. More\nspecifically, we prove that SGD solutions are connected via a piecewise linear\npath, and the increase in loss along this path vanishes as the number of\nneurons grows large. This result is a consequence of the fact that the\nparameters found by SGD are increasingly dropout stable as the network becomes\nwider. We show that, if we remove part of the neurons (and suitably rescale the\nremaining ones), the change in loss is independent of the total number of\nneurons, and it depends only on how many neurons are left. Our results exhibit\na mild dependence on the input dimension: they are dimension-free for two-layer\nnetworks and depend linearly on the dimension for multilayer networks. We\nvalidate our theoretical findings with numerical experiments for different\narchitectures and classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 20:49:52 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 13:26:37 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Shevchenko", "Alexander", ""], ["Mondelli", "Marco", ""]]}, {"id": "1912.10103", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Andrea Calimera", "title": "TentacleNet: A Pseudo-Ensemble Template for Accurate Binary\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization is an attractive strategy for implementing lightweight Deep\nConvolutional Neural Networks (CNNs). Despite the unquestionable savings\noffered, memory footprint above all, it may induce an excessive accuracy loss\nthat prevents a widespread use. This work elaborates on this aspect introducing\nTentacleNet, a new template designed to improve the predictive performance of\nbinarized CNNs via parallelization. Inspired by the ensemble learning theory,\nit consists of a compact topology that is end-to-end trainable and organized to\nminimize memory utilization. Experimental results collected over three\nrealistic benchmarks show TentacleNet fills the gap left by classical binary\nmodels, ensuring substantial memory savings w.r.t. state-of-the-art binary\nensemble methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 21:18:16 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 12:37:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Mocerino", "Luca", ""], ["Calimera", "Andrea", ""]]}, {"id": "1912.10127", "submitter": "Matthew Kollada", "authors": "Matthew Kollada, Qingzhu Gao, Monika S Mellem, Tathagata Banerjee,\n  William J Martin", "title": "A Generalizable Method for Automated Quality Control of Functional\n  Neuroimaging Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last twenty five years, advances in the collection and analysis of\nfMRI data have enabled new insights into the brain basis of human health and\ndisease. Individual behavioral variation can now be visualized at a neural\nlevel as patterns of connectivity among brain regions. Functional brain imaging\nis enhancing our understanding of clinical psychiatric disorders by revealing\nties between regional and network abnormalities and psychiatric symptoms.\nInitial success in this arena has recently motivated collection of larger\ndatasets which are needed to leverage fMRI to generate brain-based biomarkers\nto support development of precision medicines. Despite methodological advances\nand enhanced computational power, evaluating the quality of fMRI scans remains\na critical step in the analytical framework. Before analysis can be performed,\nexpert reviewers visually inspect raw scans and preprocessed derivatives to\ndetermine viability of the data. This Quality Control (QC) process is labor\nintensive, and the inability to automate at large scale has proven to be a\nlimiting factor in clinical neuroscience fMRI research. We present a novel\nmethod for automating the QC of fMRI scans. We train machine learning\nclassifiers using features derived from brain MR images to predict the\n\"quality\" of those images, based on the ground truth of an expert's opinion. We\nemphasize the importance of these classifiers' ability to generalize their\npredictions across data from different studies. To address this, we propose a\nnovel approach entitled \"FMRI preprocessing Log mining for Automated,\nGeneralizable Quality Control\" (FLAG-QC), in which features derived from mining\nruntime logs are used to train the classifier. We show that classifiers trained\non FLAG-QC features perform much better (AUC=0.79) than previously proposed\nfeature sets (AUC=0.56) when testing their ability to generalize across\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:43:52 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kollada", "Matthew", ""], ["Gao", "Qingzhu", ""], ["Mellem", "Monika S", ""], ["Banerjee", "Tathagata", ""], ["Martin", "William J", ""]]}, {"id": "1912.10136", "submitter": "Oscar Zatarain-Vera", "authors": "Oscar Zatarain-Vera", "title": "A vector-contraction inequality for Rademacher complexities using\n  $p$-stable variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Andreas Maurer in the paper \"A vector-contraction inequality for Rademacher\ncomplexities\" extended the contraction inequality for Rademacher averages to\nLipschitz functions with vector-valued domains; He did it replacing the\nRademacher variables in the bounding expression by arbitrary idd symmetric and\nsub-gaussian variables. We will see how to extend this work when we replace\nsub-gaussian variables by $p$-stable variables for $1<p<2$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 23:05:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 05:45:07 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zatarain-Vera", "Oscar", ""]]}, {"id": "1912.10156", "submitter": "Stephen Ra", "authors": "Farhan Damani, Vishnu Sresht, Stephen Ra", "title": "Black Box Recursive Translations for Molecular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms for generating molecular structures offer a\npromising new approach to drug discovery. We cast molecular optimization as a\ntranslation problem, where the goal is to map an input compound to a target\ncompound with improved biochemical properties. Remarkably, we observe that when\ngenerated molecules are iteratively fed back into the translator, molecular\ncompound attributes improve with each step. We show that this finding is\ninvariant to the choice of translation model, making this a \"black box\"\nalgorithm. We call this method Black Box Recursive Translation (BBRT), a new\ninference method for molecular property optimization. This simple, powerful\ntechnique operates strictly on the inputs and outputs of any translation model.\nWe obtain new state-of-the-art results for molecular property optimization\ntasks using our simple drop-in replacement with well-known sequence and\ngraph-based models. Our method provides a significant boost in performance\nrelative to its non-recursive peers with just a simple \"for\" loop. Further,\nBBRT is highly interpretable, allowing users to map the evolution of newly\ndiscovered compounds from known starting points.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 00:53:12 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Damani", "Farhan", ""], ["Sresht", "Vishnu", ""], ["Ra", "Stephen", ""]]}, {"id": "1912.10158", "submitter": "Qiyao Wang", "authors": "Qiyao Wang, Haiyan Wang, Chetan Gupta, Susumu Serita", "title": "Regularized Operating Envelope with Interpretability and\n  Implementability Constraints", "comments": "In the proceedings of the 2019 IEEE Big Data Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating envelope is an important concept in industrial operations. Accurate\nidentification for operating envelope can be extremely beneficial to\nstakeholders as it provides a set of operational parameters that optimizes some\nkey performance indicators (KPI) such as product quality, operational safety,\nequipment efficiency, environmental impact, etc. Given the importance,\ndata-driven approaches for computing the operating envelope are gaining\npopularity. These approaches typically use classifiers such as support vector\nmachines, to set the operating envelope by learning the boundary in the\noperational parameter spaces between the manually assigned `large KPI' and\n`small KPI' groups. One challenge to these approaches is that the assignment to\nthese groups is often ad-hoc and hence arbitrary. However, a bigger challenge\nwith these approaches is that they don't take into account two key features\nthat are needed to operationalize operating envelopes: (i) interpretability of\nthe envelope by the operator and (ii) implementability of the envelope from a\npractical standpoint. In this work, we propose a new definition for operating\nenvelope which directly targets the expected magnitude of KPI (i.e., no need to\narbitrarily bin the data instances into groups) and accounts for the\ninterpretability and the implementability. We then propose a regularized `GA +\npenalty' algorithm that outputs an envelope where the user can tradeoff between\nbias and variance. The validity of our proposed algorithm is demonstrated by\ntwo sets of simulation studies and an application to a real-world challenge in\nthe mining processes of a flotation plant.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 01:03:54 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Qiyao", ""], ["Wang", "Haiyan", ""], ["Gupta", "Chetan", ""], ["Serita", "Susumu", ""]]}, {"id": "1912.10160", "submitter": "Gaurav Kumar", "authors": "Gaurav Kumar, Rishabh Joshi, Jaspreet Singh, Promod Yenigalla", "title": "AMUSED: A Multi-Stream Vector Representation Method for Use in Natural\n  Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of building a coherent and non-monotonous conversational agent\nwith proper discourse and coverage is still an area of open research. Current\narchitectures only take care of semantic and contextual information for a given\nquery and fail to completely account for syntactic and external knowledge which\nare crucial for generating responses in a chit-chat system. To overcome this\nproblem, we propose an end to end multi-stream deep learning architecture which\nlearns unified embeddings for query-response pairs by leveraging contextual\ninformation from memory networks and syntactic information by incorporating\nGraph Convolution Networks (GCN) over their dependency parse. A stream of this\nnetwork also utilizes transfer learning by pre-training a bidirectional\ntransformer to extract semantic representation for each input sentence and\nincorporates external knowledge through the the neighborhood of the entities\nfrom a Knowledge Base (KB). We benchmark these embeddings on next sentence\nprediction task and significantly improve upon the existing techniques.\nFurthermore, we use AMUSED to represent query and responses along with its\ncontext to develop a retrieval based conversational agent which has been\nvalidated by expert linguists to have comprehensive engagement with humans.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:35:03 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kumar", "Gaurav", ""], ["Joshi", "Rishabh", ""], ["Singh", "Jaspreet", ""], ["Yenigalla", "Promod", ""]]}, {"id": "1912.10162", "submitter": "Stavros Vologiannidis", "authors": "Eleni Partalidou, Eleftherios Spyromitros-Xioufis, Stavros Doropoulos,\n  Stavros Vologiannidis, Konstantinos I. Diamantaras", "title": "Design and implementation of an open source Greek POS Tagger and Entity\n  Recognizer using spaCy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a machine learning approach to part-of-speech tagging and\nnamed entity recognition for Greek, focusing on the extraction of morphological\nfeatures and classification of tokens into a small set of classes for named\nentities. The architecture model that was used is introduced. The greek version\nof the spaCy platform was added into the source code, a feature that did not\nexist before our contribution, and was used for building the models.\nAdditionally, a part of speech tagger was trained that can detect the\nmorphology of the tokens and performs higher than the state-of-the-art results\nwhen classifying only the part of speech. For named entity recognition using\nspaCy, a model that extends the standard ENAMEX type (organization, location,\nperson) was built. Certain experiments that were conducted indicate the need\nfor flexibility in out-of-vocabulary words and there is an effort for resolving\nthis issue. Finally, the evaluation results are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 13:29:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Partalidou", "Eleni", ""], ["Spyromitros-Xioufis", "Eleftherios", ""], ["Doropoulos", "Stavros", ""], ["Vologiannidis", "Stavros", ""], ["Diamantaras", "Konstantinos I.", ""]]}, {"id": "1912.10166", "submitter": "Zeljko Kraljevic", "authors": "Zeljko Kraljevic, Daniel Bean, Aurelie Mascio, Lukasz Roguski, Amos\n  Folarin, Angus Roberts, Rebecca Bendayan, Richard Dobson", "title": "MedCAT -- Medical Concept Annotation Tool", "comments": "Preprint, 25 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical documents such as Electronic Health Records (EHRs) contain a large\namount of information in an unstructured format. The data in EHRs is a hugely\nvaluable resource documenting clinical narratives and decisions, but whilst the\ntext can be easily understood by human doctors it is challenging to use in\nresearch and clinical applications. To uncover the potential of biomedical\ndocuments we need to extract and structure the information they contain. The\ntask at hand is Named Entity Recognition and Linking (NER+L). The number of\nentities, ambiguity of words, overlapping and nesting make the biomedical area\nsignificantly more difficult than many others. To overcome these difficulties,\nwe have developed the Medical Concept Annotation Tool (MedCAT), an open-source\nunsupervised approach to NER+L. MedCAT uses unsupervised machine learning to\ndisambiguate entities. It was validated on MIMIC-III (a freely accessible\ncritical care database) and MedMentions (Biomedical papers annotated with\nmentions from the Unified Medical Language System). In case of NER+L, the\ncomparison with existing tools shows that MedCAT improves the previous best\nwith only unsupervised learning (F1=0.848 vs 0.691 for disease detection;\nF1=0.710 vs. 0.222 for general concept detection). A qualitative analysis of\nthe vector embeddings learnt by MedCAT shows that it captures latent medical\nknowledge available in EHRs (MIMIC-III). Unsupervised learning can improve the\nperformance of large scale entity extraction, but it has some limitations when\nworking with only a couple of entities and a small dataset. In that case\noptions are supervised learning or active learning, both of which are supported\nin MedCAT via the MedCATtrainer extension. Our approach can detect and link\nmillions of different biomedical concepts with state-of-the-art performance,\nwhilst being lightweight, fast and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:42:31 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kraljevic", "Zeljko", ""], ["Bean", "Daniel", ""], ["Mascio", "Aurelie", ""], ["Roguski", "Lukasz", ""], ["Folarin", "Amos", ""], ["Roberts", "Angus", ""], ["Bendayan", "Rebecca", ""], ["Dobson", "Richard", ""]]}, {"id": "1912.10168", "submitter": "Blaine Cole", "authors": "Blaine Cole", "title": "Two Way Adversarial Unsupervised Word Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word translation is a problem in machine translation that seeks to build\nmodels that recover word level correspondence between languages. Recent\napproaches to this problem have shown that word translation models can learned\nwith very small seeding dictionaries, and even without any starting\nsupervision. In this paper we propose a method to jointly find translations\nbetween a pair of languages. Not only does our method learn translations in\nboth directions but it improves accuracy of those translations over past\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:21:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cole", "Blaine", ""]]}, {"id": "1912.10170", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Andrew Collins, Joeran Beel", "title": "Na\\\"iveRole: Author-Contribution Extraction and Parsing from Biomedical\n  Manuscripts", "comments": "arXiv admin note: substantial text overlap with arXiv:1802.01174", "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the contributions of individual authors to scientific\npublications is important for assessing authors' achievements. Some biomedical\npublications have a short section that describes authors' roles and\ncontributions. It is usually written in natural language and hence author\ncontributions cannot be trivially extracted in a machine-readable format. In\nthis paper, we present 1) A statistical analysis of roles in author\ncontributions sections, and 2) Na\\\"iveRole, a novel approach to extract\nstructured authors' roles from author contribution sections. For the first\npart, we used co-clustering techniques, as well as Open Information Extraction,\nto semi-automatically discover the popular roles within a corpus of 2,000\ncontributions sections from PubMed Central. The discovered roles were used to\nautomatically build a training set for Na\\\"iveRole, our role extractor\napproach, based on Na\\\"ive Bayes. Na\\\"iveRole extracts roles with a\nmicro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the\nbest of our knowledge, the first attempt to automatically extract author roles\nfrom research papers. This paper is an extended version of a previous poster\npublished at JCDL 2018.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:37:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10189", "submitter": "Christopher J. Cueva", "authors": "Christopher J. Cueva, Peter Y. Wang, Matthew Chin, Xue-Xin Wei", "title": "Emergence of functional and structural properties of the head direction\n  system by optimization of recurrent neural networks", "comments": "International Conference on Learning Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests goal-driven training of neural networks can be used to\nmodel neural activity in the brain. While response properties of neurons in\nartificial neural networks bear similarities to those in the brain, the network\narchitectures are often constrained to be different. Here we ask if a neural\nnetwork can recover both neural representations and, if the architecture is\nunconstrained and optimized, the anatomical properties of neural circuits. We\ndemonstrate this in a system where the connectivity and the functional\norganization have been characterized, namely, the head direction circuits of\nthe rodent and fruit fly. We trained recurrent neural networks (RNNs) to\nestimate head direction through integration of angular velocity. We found that\nthe two distinct classes of neurons observed in the head direction system, the\nCompass neurons and the Shifter neurons, emerged naturally in artificial neural\nnetworks as a result of training. Furthermore, connectivity analysis and\nin-silico neurophysiology revealed structural and mechanistic similarities\nbetween artificial networks and the head direction system. Overall, our results\nshow that optimization of RNNs in a goal-driven task can recapitulate the\nstructure and function of biological circuits, suggesting that artificial\nneural networks can be used to study the brain at the level of both neural\nactivity and anatomical organization.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 03:51:58 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 09:06:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Cueva", "Christopher J.", ""], ["Wang", "Peter Y.", ""], ["Chin", "Matthew", ""], ["Wei", "Xue-Xin", ""]]}, {"id": "1912.10200", "submitter": "Rui Zhang", "authors": "Rui Zhang, Christian J. Walder, Edwin V. Bonilla, Marian-Andrei\n  Rizoiu, Lexing Xie", "title": "Quantile Propagation for Wasserstein-Approximate Gaussian Processes", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate inference techniques are the cornerstone of probabilistic methods\nbased on Gaussian process priors. Despite this, most work approximately\noptimizes standard divergence measures such as the Kullback-Leibler (KL)\ndivergence, which lack the basic desiderata for the task at hand, while chiefly\noffering merely technical convenience. We develop a new approximate inference\nmethod for Gaussian process models which overcomes the technical challenges\narising from abandoning these convenient divergences. Our method---dubbed\nQuantile Propagation (QP)---is similar to expectation propagation (EP) but\nminimizes the $L_2$ Wasserstein distance (WD) instead of the KL divergence. The\nWD exhibits all the required properties of a distance metric, while respecting\nthe geometry of the underlying sample space. We show that QP matches quantile\nfunctions rather than moments as in EP and has the same mean update but a\nsmaller variance update than EP, thereby alleviating EP's tendency to\nover-estimate posterior variances. Crucially, despite the significant\ncomplexity of dealing with the WD, QP has the same favorable locality property\nas EP, and thereby admits an efficient algorithm. Experiments on classification\nand Poisson regression show that QP outperforms both EP and variational Bayes.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:11:12 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 06:02:16 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 08:37:16 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhang", "Rui", ""], ["Walder", "Christian J.", ""], ["Bonilla", "Edwin V.", ""], ["Rizoiu", "Marian-Andrei", ""], ["Xie", "Lexing", ""]]}, {"id": "1912.10202", "submitter": "Songgaojun Deng", "authors": "Songgaojun Deng, Shusen Wang, Huzefa Rangwala, Lijing Wang, Yue Ning", "title": "Graph Message Passing with Cross-location Attentions for Long-term ILI\n  Prediction", "comments": "17 pages, 22 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting influenza-like illness (ILI) is of prime importance to\nepidemiologists and health-care providers. Early prediction of epidemic\noutbreaks plays a pivotal role in disease intervention and control. Most\nexisting work has either limited long-term prediction performance or lacks a\ncomprehensive ability to capture spatio-temporal dependencies in data. Accurate\nand early disease forecasting models would markedly improve both epidemic\nprevention and managing the onset of an epidemic. In this paper, we design a\ncross-location attention based graph neural network (Cola-GNN) for learning\ntime series embeddings and location aware attentions. We propose a graph\nmessage passing framework to combine learned feature embeddings and an\nattention matrix to model disease propagation over time. We compare the\nproposed method with state-of-the-art statistical approaches and deep learning\nmodels on real-world epidemic-related datasets from United States and Japan.\nThe proposed method shows strong predictive performance and leads to\ninterpretable results for long-term epidemic predictions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:28:05 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 03:47:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Deng", "Songgaojun", ""], ["Wang", "Shusen", ""], ["Rangwala", "Huzefa", ""], ["Wang", "Lijing", ""], ["Ning", "Yue", ""]]}, {"id": "1912.10204", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Carolyn Penstein Rose", "title": "A Machine Learning Framework for Authorship Identification From Texts", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship identification is a process in which the author of a text is\nidentified. Most known literary texts can easily be attributed to a certain\nauthor because they are, for example, signed. Yet sometimes we find unfinished\npieces of work or a whole bunch of manuscripts with a wide variety of possible\nauthors. In order to assess the importance of such a manuscript, it is vital to\nknow who wrote it. In this work, we aim to develop a machine learning framework\nto effectively determine authorship. We formulate the task as a single-label\nmulti-class text categorization problem and propose a supervised machine\nlearning framework incorporating stylometric features. This task is highly\ninterdisciplinary in that it takes advantage of machine learning, information\nretrieval, and natural language processing. We present an approach and a model\nwhich learns the differences in writing style between $50$ different authors\nand is able to predict the author of a new text with high accuracy. The\naccuracy is seen to increase significantly after introducing certain linguistic\nstylometric features along with text features.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:47:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Rose", "Carolyn Penstein", ""]]}, {"id": "1912.10206", "submitter": "James Fox", "authors": "James Fox, Sivasankaran Rajamanickam", "title": "How Robust Are Graph Neural Networks to Structural Noise?", "comments": "Accepted workshop paper at Deep Learning on Graphs: Methodologies and\n  Applications (DLGMA'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are an emerging model for learning graph\nembeddings and making predictions on graph structured data. However, robustness\nof graph neural networks is not yet well-understood. In this work, we focus on\nnode structural identity predictions, where a representative GNN model is able\nto achieve near-perfect accuracy. We also show that the same GNN model is not\nrobust to addition of structural noise, through a controlled dataset and set of\nexperiments. Finally, we show that under the right conditions, graph-augmented\ntraining is capable of significantly improving robustness to structural noise.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:56:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fox", "James", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "1912.10233", "submitter": "Deli Zhao", "authors": "Deli Zhao and Jiapeng Zhu and Bo Zhang", "title": "Latent Variables on Spheres for Autoencoders in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Auto-Encoder (VAE) has been widely applied as a fundamental\ngenerative model in machine learning. For complex samples like imagery objects\nor scenes, however, VAE suffers from the dimensional dilemma between\nreconstruction precision that needs high-dimensional latent codes and\nprobabilistic inference that favors a low-dimensional latent space. By virtue\nof high-dimensional geometry, we propose a very simple algorithm, called\nSpherical Auto-Encoder (SAE), completely different from existing VAEs to\naddress the issue. SAE is in essence the vanilla autoencoder with spherical\nnormalization on the latent space. We analyze the unique characteristics of\nrandom variables on spheres in high dimensions and argue that random variables\non spheres are agnostic to various prior distributions and data modes when the\ndimension is sufficiently high. Therefore, SAE can harness a high-dimensional\nlatent space to improve the inference precision of latent codes while maintain\nthe property of stochastic sampling from priors. The experiments on sampling\nand inference validate our theoretical analysis and the superiority of SAE.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:53:53 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 02:20:03 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Deli", ""], ["Zhu", "Jiapeng", ""], ["Zhang", "Bo", ""]]}, {"id": "1912.10251", "submitter": "Chowdhury Rahman", "authors": "Ruhul Amin, Chowdhury Rafeed Rahman, Md. Habibur Rahman Sifat, Md\n  Nazmul Khan Liton, Md. Moshiur Rahman, Swakkhar Shatabda and Sajid Ahmed", "title": "iPromoter-BnCNN: a Novel Branched CNN Based Predictor for Identifying\n  and Classifying Sigma Promoters", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btaa609", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoter is a short region of DNA which is responsible for initiating\ntranscription of specific genes. Development of computational tools for\nautomatic identification of promoters is in high demand. According to the\ndifference of functions, promoters can be of different types. Promoters may\nhave both intra and inter class variation and similarity in terms of consensus\nsequences. Accurate classification of various types of sigma promoters still\nremains a challenge. We present iPromoter-BnCNN for identification and accurate\nclassification of six types of promoters - sigma24, sigma28, sigma32, sigma38,\nsigma54, sigma70. It is a Convolutional Neural Network (CNN) based classifier\nwhich combines local features related to monomer nucleotide sequence, trimer\nnucleotide sequence, dimer structural properties and trimer structural\nproperties through the use of parallel branching. We conducted experiments on a\nbenchmark dataset and compared with two state-of-the-art tools to show our\nsupremacy on 5-fold cross-validation. Moreover, we tested our classifier on an\nindependent test dataset. Our proposed tool iPromoter-BnCNN web server is\nfreely available at http://103.109.52.8/iPromoter-BnCNN. The runnable source\ncode can be found at\nhttps://colab.research.google.com/drive/1yWWh7BXhsm8U4PODgPqlQRy23QGjF2DZ.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 11:59:38 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 06:51:47 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 01:20:09 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 20:44:32 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Amin", "Ruhul", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Sifat", "Md. Habibur Rahman", ""], ["Liton", "Md Nazmul Khan", ""], ["Rahman", "Md. Moshiur", ""], ["Shatabda", "Swakkhar", ""], ["Ahmed", "Sajid", ""]]}, {"id": "1912.10264", "submitter": "Hegler Tissot", "authors": "Matthew Wai Heng Chung and Hegler Tissot", "title": "Evaluating the Effectiveness of Margin Parameter when Learning Knowledge\n  Embedding Representation for Domain-specific Multi-relational Categorized\n  Data", "comments": null, "journal-ref": "StarAI 2020 9th International Workshop on Statistical Relational\n  AI", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning knowledge representation is an increasingly important technology\nthat supports a variety of machine learning related applications. However, the\nchoice of hyperparameters is seldom justified and usually relies on exhaustive\nsearch. Understanding the effect of hyperparameter combinations on embedding\nquality is crucial to avoid the inefficient process and enhance practicality of\nvector representation methods. We evaluate the effects of distinct values for\nthe margin parameter focused on translational embedding representation models\nfor multi-relational categorized data. We assess the margin influence regarding\nthe quality of embedding models by contrasting traditional link prediction task\naccuracy against a classification task. The findings provide evidence that\nlower values of margin are not rigorous enough to help with the learning\nprocess, whereas larger values produce much noise pushing the entities beyond\nto the surface of the hyperspace, thus requiring constant regularization.\nFinally, the correlation between link prediction and classification accuracy\nshows traditional validation protocol for embedding models is a weak metric to\nrepresent the quality of embedding representation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 13:24:08 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chung", "Matthew Wai Heng", ""], ["Tissot", "Hegler", ""]]}, {"id": "1912.10292", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Chenliang Xu, and Dingzeyu Li", "title": "Deep Audio Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are known to specialize in distilling\ncompact and robust prior from a large amount of data. We are interested in\napplying deep networks in the absence of training dataset. In this paper, we\nintroduce deep audio prior (DAP) which leverages the structure of a network and\nthe temporal information in a single audio file. Specifically, we demonstrate\nthat a randomly-initialized neural network can be used with carefully designed\naudio prior to tackle challenging audio problems such as universal blind source\nseparation, interactive audio editing, audio texture synthesis, and audio\nco-separation. To understand the robustness of the deep audio prior, we\nconstruct a benchmark dataset \\emph{Universal-150} for universal sound source\nseparation with a diverse set of sources. We show superior audio results than\nprevious work on both qualitative and quantitative evaluations. We also perform\nthorough ablation study to validate our design choices.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 16:35:54 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tian", "Yapeng", ""], ["Xu", "Chenliang", ""], ["Li", "Dingzeyu", ""]]}, {"id": "1912.10306", "submitter": "Xiong Liu", "authors": "Xiong Liu, Yu Chen, Jay Bae, Hu Li, Joseph Johnston, Todd Sanger", "title": "Predicting Heart Failure Readmission from Clinical Notes Using Deep\n  Learning", "comments": "IEEE BIBM 2019", "journal-ref": "2019 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart failure hospitalization is a severe burden on healthcare. How to\npredict and therefore prevent readmission has been a significant challenge in\noutcomes research. To address this, we propose a deep learning approach to\npredict readmission from clinical notes. Unlike conventional methods that use\nstructured data for prediction, we leverage the unstructured clinical notes to\ntrain deep learning models based on convolutional neural networks (CNN). We\nthen use the trained models to classify and predict potentially high-risk\nadmissions/patients. For evaluation, we trained CNNs using the discharge\nsummary notes in the MIMIC III database. We also trained regular machine\nlearning models based on random forest using the same datasets. The result\nshows that deep learning models outperform the regular models in prediction\ntasks. CNN method achieves a F1 score of 0.756 in general readmission\nprediction and 0.733 in 30-day readmission prediction, while random forest only\nachieves a F1 score of 0.674 and 0.656 respectively. We also propose a\nchi-square test based method to interpret key features associated with deep\nlearning predicted readmissions. It reveals clinical insights about readmission\nembedded in the clinical notes. Collectively, our method can make the human\nevaluation process more efficient and potentially facilitate the reduction of\nreadmission rates.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:49:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Xiong", ""], ["Chen", "Yu", ""], ["Bae", "Jay", ""], ["Li", "Hu", ""], ["Johnston", "Joseph", ""], ["Sanger", "Todd", ""]]}, {"id": "1912.10309", "submitter": "Graham Fyffe", "authors": "Graham Fyffe", "title": "There and Back Again: Unraveling the Variational Auto-Encoder", "comments": "20 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We prove that the evidence lower bound (ELBO) employed by variational\nauto-encoders (VAEs) admits non-trivial solutions having constant posterior\nvariances under certain mild conditions, removing the need to learn variances\nin the encoder. The proof follows from an unexpected journey through an array\nof topics: the closed form optimal decoder for Gaussian VAEs, a proof that the\ndecoder is always smooth, a proof that the ELBO at its stationary points is\nequal to the exact log evidence, and the posterior variance is merely part of a\nstochastic estimator of the decoder Hessian. The penalty incurred from using a\nconstant posterior variance is small under mild conditions, and otherwise\ndiscourages large variations in the decoder Hessian. From here we derive a\nsimplified formulation of the ELBO as an expectation over a batch, which we\ncall the Batch Information Lower Bound (BILBO). Despite the use of Gaussians,\nour analysis is broadly applicable -- it extends to any likelihood function\nthat induces a Riemannian metric. Regarding learned likelihoods, we show that\nthe ELBO is optimal in the limit as the likelihood variances approach zero,\nwhere it is equivalent to the change of variables formulation employed in\nnormalizing flow networks. Standard optimization procedures are unstable in\nthis limit, so we propose a bounded Gaussian likelihood that is invariant to\nthe scale of the data using a measure of the aggregate information in a batch,\nwhich we call Bounded Aggregate Information Sampling (BAGGINS). Combining the\ntwo formulations, we construct VAE networks with only half the outputs of\nordinary VAEs (no learned variances), yielding improved ELBO scores and scale\ninvariance in experiments. As we perform our analyses irrespective of any\nparticular network architecture, our reformulations may apply to any VAE\nimplementation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:21:22 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 22:30:22 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 07:27:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Fyffe", "Graham", ""]]}, {"id": "1912.10316", "submitter": "Abhishek Nan", "authors": "Abhishek Nan", "title": "Exploring TD error as a heuristic for $\\sigma$ selection in Q($\\sigma$,\n  $\\lambda$)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the landscape of TD algorithms, the Q($\\sigma$, $\\lambda$) algorithm is an\nalgorithm with the ability to perform a multistep backup in an online manner\nwhile also successfully unifying the concepts of sampling with using the\nexpectation across all actions for a state. $\\sigma \\in [0, 1]$ indicates the\nextent to which sampling is used. Selecting the value of {\\sigma} can be based\non characteristics of the current state rather than having a constant value or\nbeing time based. This report explores the viability of such a TD-error based\nscheme.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:53:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Nan", "Abhishek", ""]]}, {"id": "1912.10321", "submitter": "Ari Heljakka", "authors": "Ari Heljakka, Yuxin Hou, Juho Kannala, Arno Solin", "title": "Deep Automodulators", "comments": "To appear in Advances in Neural Information Processing Systems\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new category of generative autoencoders called automodulators.\nThese networks can faithfully reproduce individual real-world input images like\nregular autoencoders, but also generate a fused sample from an arbitrary\ncombination of several such images, allowing instantaneous 'style-mixing' and\nother new applications. An automodulator decouples the data flow of decoder\noperations from statistical properties thereof and uses the latent vector to\nmodulate the former by the latter, with a principled approach for mutual\ndisentanglement of decoder layers. Prior work has explored similar decoder\narchitecture with GANs, but their focus has been on random sampling. A\ncorresponding autoencoder could operate on real input images. For the first\ntime, we show how to train such a general-purpose model with sharp outputs in\nhigh resolution, using novel training techniques, demonstrated on four image\ndata sets. Besides style-mixing, we show state-of-the-art results in\nautoencoder comparison, and visual image quality nearly indistinguishable from\nstate-of-the-art GANs. We expect the automodulator variants to become a useful\nbuilding block for image applications and other data domains.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 19:16:33 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 18:01:55 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 16:54:08 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 12:58:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Heljakka", "Ari", ""], ["Hou", "Yuxin", ""], ["Kannala", "Juho", ""], ["Solin", "Arno", ""]]}, {"id": "1912.10325", "submitter": "Arghyadip Roy", "authors": "Arghyadip Roy, Vivek Borkar, Abhay Karandikar and Prasanna Chaporkar", "title": "Online Reinforcement Learning of Optimal Threshold Policies for Markov\n  Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Decision Process (MDP) problems can be solved using Dynamic\nProgramming (DP) methods which suffer from the curse of dimensionality and the\ncurse of modeling. To overcome these issues, Reinforcement Learning (RL)\nmethods are adopted in practice. In this paper, we aim to obtain the optimal\nadmission control policy in a system where different classes of customers are\npresent. Using DP techniques, we prove that it is optimal to admit the $i$ th\nclass of customers only upto a threshold $\\tau(i)$ which is a non-increasing\nfunction of $i$. Contrary to traditional RL algorithms which do not take into\naccount the structural properties of the optimal policy while learning, we\npropose a structure-aware learning algorithm which exploits the threshold\nstructure of the optimal policy. We prove the asymptotic convergence of the\nproposed algorithm to the optimal policy. Due to the reduction in the policy\nspace, the structure-aware learning algorithm provides remarkable improvements\nin storage and computational complexities over classical RL algorithms.\nSimulation results also establish the gain in the convergence rate of the\nproposed algorithm over other RL algorithms. The techniques presented in the\npaper can be applied to any general MDP problem covering various applications\nsuch as inventory management, financial planning and communication networking.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 19:46:55 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 01:17:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Roy", "Arghyadip", ""], ["Borkar", "Vivek", ""], ["Karandikar", "Abhay", ""], ["Chaporkar", "Prasanna", ""]]}, {"id": "1912.10329", "submitter": "Yanchao Sun", "authors": "Yanchao Sun and Furong Huang", "title": "Can Agents Learn by Analogy? An Inferable Model for PAC Reinforcement\n  Learning", "comments": "To be published in proceedings of AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning algorithms make decisions by building and\nutilizing a model of the environment. However, none of the existing algorithms\nattempts to infer the dynamics of any state-action pair from known state-action\npairs before meeting it for sufficient times. We propose a new model-based\nmethod called Greedy Inference Model (GIM) that infers the unknown dynamics\nfrom known dynamics based on the internal spectral properties of the\nenvironment. In other words, GIM can \"learn by analogy\". We further introduce a\nnew exploration strategy which ensures that the agent rapidly and evenly visits\nunknown state-action pairs. GIM is much more computationally efficient than\nstate-of-the-art model-based algorithms, as the number of dynamic programming\noperations is independent of the environment size. Lower sample complexity\ncould also be achieved under mild conditions compared against methods without\ninferring. Experimental results demonstrate the effectiveness and efficiency of\nGIM in a variety of real-world tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 20:09:10 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 20:43:15 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 16:04:53 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Sun", "Yanchao", ""], ["Huang", "Furong", ""]]}, {"id": "1912.10337", "submitter": "Mingyuan Zhou", "authors": "Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou", "title": "Recurrent Hierarchical Topic-Guided RNN for Language Generation", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To simultaneously capture syntax and global semantics from a text corpus, we\npropose a new larger-context recurrent neural network (RNN) based language\nmodel, which extracts recurrent hierarchical semantic structure via a dynamic\ndeep topic model to guide natural language generation. Moving beyond a\nconventional RNN-based language model that ignores long-range word dependencies\nand sentence order, the proposed model captures not only intra-sentence word\ndependencies, but also temporal transitions between sentences and\ninter-sentence topic dependencies. For inference, we develop a hybrid of\nstochastic-gradient Markov chain Monte Carlo and recurrent autoencoding\nvariational Bayes. Experimental results on a variety of real-world text corpora\ndemonstrate that the proposed model not only outperforms larger-context\nRNN-based language models, but also learns interpretable recurrent multilayer\ntopics and generates diverse sentences and paragraphs that are syntactically\ncorrect and semantically coherent.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:11:35 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 22:22:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Guo", "Dandan", ""], ["Chen", "Bo", ""], ["Lu", "Ruiying", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1912.10340", "submitter": "Jittat Fakcharoenphol", "authors": "Jittat Fakcharoenphol, Chayutpong Prompak", "title": "Bandit Multiclass Linear Classification for the Group Linear Separable\n  Case", "comments": "This work is first published in iSAI-NLP 2019, Chiang Mai, Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online multiclass linear classification under the bandit\nfeedback setting. Beygelzimer, P\\'{a}l, Sz\\\"{o}r\\'{e}nyi, Thiruvenkatachari,\nWei, and Zhang [ICML'19] considered two notions of linear separability, weak\nand strong linear separability. When examples are strongly linearly separable\nwith margin $\\gamma$, they presented an algorithm based on Multiclass\nPerceptron with mistake bound $O(K/\\gamma^2)$, where $K$ is the number of\nclasses. They employed rational kernel to deal with examples under the weakly\nlinearly separable condition, and obtained the mistake bound of $\\min(K\\cdot\n2^{\\tilde{O}(K\\log^2(1/\\gamma))},K\\cdot 2^{\\tilde{O}(\\sqrt{1/\\gamma}\\log K)})$.\nIn this paper, we refine the notion of weak linear separability to support the\nnotion of class grouping, called group weak linear separable condition. This\nsituation may arise from the fact that class structures contain inherent\ngrouping. We show that under this condition, we can also use the rational\nkernel and obtain the mistake bound of $K\\cdot 2^{\\tilde{O}(\\sqrt{1/\\gamma}\\log\nL)})$, where $L\\leq K$ represents the number of groups.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:17:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fakcharoenphol", "Jittat", ""], ["Prompak", "Chayutpong", ""]]}, {"id": "1912.10382", "submitter": "Qianxiao Li", "authors": "Qianxiao Li, Ting Lin, Zuowei Shen", "title": "Deep Learning via Dynamical Systems: An Approximation Perspective", "comments": "Revision 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on the dynamical systems approach to deep learning, where deep\nresidual networks are idealized as continuous-time dynamical systems, from the\napproximation perspective. In particular, we establish general sufficient\nconditions for universal approximation using continuous-time deep residual\nnetworks, which can also be understood as approximation theories in $L^p$ using\nflow maps of dynamical systems. In specific cases, rates of approximation in\nterms of the time horizon are also established. Overall, these results reveal\nthat composition function approximation through flow maps present a new\nparadigm in approximation theory and contributes to building a useful\nmathematical framework to investigate deep learning.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 04:19:33 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 03:21:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Qianxiao", ""], ["Lin", "Ting", ""], ["Shen", "Zuowei", ""]]}, {"id": "1912.10396", "submitter": "Kae-Wen (Kevin) Chern", "authors": "Alexandre Bouchard-C\\^ot\\'e, Kevin Chern, Davor Cubranic, Sahand\n  Hosseini, Justin Hume, Matteo Lepur, Zihui Ouyang, Giorgio Sgarbi", "title": "Blang: Bayesian declarative modelling of general data structures and\n  inference via algorithms based on distribution continua", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Bayesian inference problem where a variable of interest does not\ntake values in a Euclidean space. These \"non-standard\" data structures are in\nreality fairly common. They are frequently used in problems involving latent\ndiscrete factor models, networks, and domain specific problems such as sequence\nalignments and reconstructions, pedigrees, and phylogenies. In principle,\nBayesian inference should be particularly well-suited in such scenarios, as the\nBayesian paradigm provides a principled way to obtain confidence assessment for\nrandom variables of any type. However, much of the recent work on making\nBayesian analysis more accessible and computationally efficient has focused on\ninference in Euclidean spaces.\n  In this paper, we introduce Blang, a domain specific language and library\naimed at bridging this gap. Blang allows users to perform Bayesian analysis on\narbitrary data types while using a declarative syntax similar to BUGS. Blang is\naugmented with intuitive language additions to create data types of the user's\nchoosing. To perform inference at scale on such arbitrary state spaces, Blang\nleverages recent advances in sequential Monte Carlo and non-reversible Markov\nchain Monte Carlo methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 08:05:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 19:24:39 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Chern", "Kevin", ""], ["Cubranic", "Davor", ""], ["Hosseini", "Sahand", ""], ["Hume", "Justin", ""], ["Lepur", "Matteo", ""], ["Ouyang", "Zihui", ""], ["Sgarbi", "Giorgio", ""]]}, {"id": "1912.10398", "submitter": "L.A. Prashanth", "authors": "Ajay Kumar Pandey, Prashanth L.A. and Sanjay P. Bhat", "title": "Estimation of Spectral Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a spectral risk measure (SRM) from\ni.i.d. samples, and propose a novel method that is based on numerical\nintegration. We show that our SRM estimate concentrates exponentially, when the\nunderlying distribution has bounded support. Further, we also consider the case\nwhen the underlying distribution is either Gaussian or exponential, and derive\na concentration bound for our estimation scheme. We validate the theoretical\nfindings on a synthetic setup, and in a vehicular traffic routing application.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 08:11:42 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Pandey", "Ajay Kumar", ""], ["A.", "Prashanth L.", ""], ["Bhat", "Sanjay P.", ""]]}, {"id": "1912.10402", "submitter": "Ian Manchester", "authors": "Max Revay and Ian R. Manchester", "title": "Contracting Implicit Recurrent Neural Networks: Stable Models with\n  Improved Trainability", "comments": "Conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability of recurrent models is closely linked with trainability,\ngeneralizability and in some applications, safety. Methods that train stable\nrecurrent neural networks, however, do so at a significant cost to\nexpressibility. We propose an implicit model structure that allows for a convex\nparametrization of stable models using contraction analysis of non-linear\nsystems. Using these stability conditions we propose a new approach to model\ninitialization and then provide a number of empirical results comparing the\nperformance of our proposed model set to previous stable RNNs and vanilla RNNs.\nBy carefully controlling stability in the model, we observe a significant\nincrease in the speed of training and model performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 09:16:05 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Revay", "Max", ""], ["Manchester", "Ian R.", ""]]}, {"id": "1912.10438", "submitter": "Mohammad Saleh Mahdizadeh", "authors": "Mohammad Saleh Mahdizadeh, Behnam Bahrak", "title": "A Regression Framework for Predicting User's Next Location using Call\n  Detail Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of using cell phones and the increase in diversity of smart\nmobile devices, a massive volume of data is generated continuously in the\nprocess of using these devices. Among these data, Call Detail Records, CDR, is\nhighly remarkable. Since CDR contains both temporal and spatial labels,\nmobility analysis of CDR is one of the favorite subjects of study among the\nresearchers. The user next location prediction is one of the main problems in\nthe field of human mobility analysis. In this paper, we propose a data\nprocessing framework to predict user next location. We propose domain-specific\ndata processing strategies and design a deep neural network model which is\nbased on recurrent neurons and perform regression tasks. Using this prediction\nframework, the error of the prediction decreases from 74% to 55% in comparison\nto the worst and best performing traditional models. Methods, strategies, the\nframework and the results of this paper can be helpful in many applications\nsuch as urban planning and digital marketing.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 12:51:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mahdizadeh", "Mohammad Saleh", ""], ["Bahrak", "Behnam", ""]]}, {"id": "1912.10481", "submitter": "Angelos Filos", "authors": "Angelos Filos, Sebastian Farquhar, Aidan N. Gomez, Tim G. J. Rudner,\n  Zachary Kenton, Lewis Smith, Milad Alizadeh, Arnoud de Kroon, Yarin Gal", "title": "A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic\n  Retinopathy Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of Bayesian deep learning (BDL) methods is challenging. We often\nseek to evaluate the methods' robustness and scalability, assessing whether new\ntools give `better' uncertainty estimates than old ones. These evaluations are\nparamount for practitioners when choosing BDL tools on-top of which they build\ntheir applications. Current popular evaluations of BDL methods, such as the UCI\nexperiments, are lacking: Methods that excel with these experiments often fail\nwhen used in application such as medical or automotive, suggesting a pertinent\nneed for new benchmarks in the field. We propose a new BDL benchmark with a\ndiverse set of tasks, inspired by a real-world medical imaging application on\n\\emph{diabetic retinopathy diagnosis}. Visual inputs (512x512 RGB images of\nretinas) are considered, where model uncertainty is used for medical\npre-screening---i.e. to refer patients to an expert when model diagnosis is\nuncertain. Methods are then ranked according to metrics derived from\nexpert-domain to reflect real-world use of model uncertainty in automated\ndiagnosis. We develop multiple tasks that fall under this application,\nincluding out-of-distribution detection and robustness to distribution shift.\nWe then perform a systematic comparison of well-tuned BDL techniques on the\nvarious tasks. From our comparison we conclude that some current techniques\nwhich solve benchmarks such as UCI `overfit' their uncertainty to the\ndataset---when evaluated on our benchmark these underperform in comparison to\nsimpler baselines. The code for the benchmark, its baselines, and a simple API\nfor evaluating new BDL tools are made available at\nhttps://github.com/oatml/bdl-benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:17:14 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Filos", "Angelos", ""], ["Farquhar", "Sebastian", ""], ["Gomez", "Aidan N.", ""], ["Rudner", "Tim G. J.", ""], ["Kenton", "Zachary", ""], ["Smith", "Lewis", ""], ["Alizadeh", "Milad", ""], ["de Kroon", "Arnoud", ""], ["Gal", "Yarin", ""]]}, {"id": "1912.10490", "submitter": "Athanasios Davvetas", "authors": "Athanasios Davvetas and Iraklis A. Klampanos", "title": "Learning Improved Representations by Transferring Incomplete Evidence\n  Across Heterogeneous Tasks", "comments": "8 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring ground truth labels for unlabelled data can be a costly procedure,\nsince it often requires manual labour that is error-prone. Consequently, the\navailable amount of labelled data is increasingly reduced due to the\nlimitations of manual data labelling. It is possible to increase the amount of\nlabelled data samples by performing automated labelling or crowd-sourcing the\nannotation procedure. However, they often introduce noise or uncertainty in the\nlabelset, that leads to decreased performance of supervised deep learning\nmethods. On the other hand, weak supervision methods remain robust during noisy\nlabelsets or can be effective even with low amounts of labelled data. In this\npaper we evaluate the effectiveness of a representation learning method that\nuses external categorical evidence called \"Evidence Transfer\", against low\namount of corresponding evidence termed as incomplete evidence. Evidence\ntransfer is a robust solution against external unknown categorical evidence\nthat can introduce noise or uncertainty. In our experimental evaluation,\nevidence transfer proves to be effective and robust against different levels of\nincompleteness, for two types of incomplete evidence.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:44:24 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Davvetas", "Athanasios", ""], ["Klampanos", "Iraklis A.", ""]]}, {"id": "1912.10503", "submitter": "Jennifer Steeden Dr", "authors": "Jennifer A. Steeden, Michael Quail, Alexander Gotschy, Andreas\n  Hauptmann, Simon Arridge, Rodney Jones, Vivek Muthurangu", "title": "Rapid Whole-Heart CMR with Single Volume Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Three-dimensional, whole heart, balanced steady state free\nprecession (WH-bSSFP) sequences provide delineation of intra-cardiac and\nvascular anatomy. However, they have long acquisition times. Here, we propose\nsignificant speed ups using a deep learning single volume super resolution\nreconstruction, to recover high resolution features from rapidly acquired low\nresolution WH-bSSFP images. Methods: A 3D residual U-Net was trained using\nsynthetic data, created from a library of high-resolution WH-bSSFP images by\nsimulating 0.5 slice resolution and 0.5 phase resolution. The trained network\nwas validated with synthetic test data, as well as prospective low-resolution\ndata. Results: Synthetic low-resolution data had significantly better image\nquality after super-resolution reconstruction. Qualitative image scores showed\nsuper-resolved images had better edge sharpness, fewer residual artefacts and\nless image distortion than low-resolution images, with similar scores to\nhigh-resolution data. Quantitative image scores showed super-resolved images\nhad significantly better edge sharpness than low-resolution or high-resolution\nimages, with significantly better signal-to-noise ratio than high-resolution\ndata. Vessel diameters measurements showed over-estimation in the\nlow-resolution measurements, compared to the high-resolution data. No\nsignificant differences and no bias was found in the super-resolution\nmeasurements. Conclusion: This paper demonstrates the potential of using a\nresidual U-Net for super-resolution reconstruction of rapidly acquired\nlow-resolution whole heart bSSFP data within a clinical setting. The resulting\nnetwork can be applied very quickly, making these techniques particularly\nappealing within busy clinical workflow. Thus, we believe that this technique\nmay help speed up whole heart CMR in clinical practice.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 18:36:18 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Steeden", "Jennifer A.", ""], ["Quail", "Michael", ""], ["Gotschy", "Alexander", ""], ["Hauptmann", "Andreas", ""], ["Arridge", "Simon", ""], ["Jones", "Rodney", ""], ["Muthurangu", "Vivek", ""]]}, {"id": "1912.10536", "submitter": "Ruocheng Guo", "authors": "Ruocheng Guo and Jundong Li and Huan Liu", "title": "Counterfactual Evaluation of Treatment Assignment Functions with\n  Networked Observational Data", "comments": "10 pages, 5 figures, Accepted to SDM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual evaluation of novel treatment assignment functions (e.g.,\nadvertising algorithms and recommender systems) is one of the most crucial\ncausal inference problems for practitioners. Traditionally, randomized\ncontrolled trials (A/B tests) are performed to evaluate treatment assignment\nfunctions. However, such trials can be time-consuming, expensive, and even\nunethical in some cases. Therefore, offline counterfactual evaluation of\ntreatment assignment functions becomes a pressing issue because a massive\namount of observational data is available in today's big data era.\nCounterfactual evaluation requires handling the hidden confounders -- the\nunmeasured features which causally influence both the treatment assignment and\nthe outcome. To deal with the hidden confounders, most of the existing methods\nrely on the assumption of no hidden confounders. However, this assumption can\nbe untenable in the context of massive observational data. When such data comes\nwith network information, the later can be potentially useful to correct hidden\nconfounding bias. As such, we first formulate a novel problem, counterfactual\nevaluation of treatment assignment functions with networked observational data.\nThen, we investigate the following research questions: How can we utilize\nnetwork information in counterfactual evaluation? Can network information\nimprove the estimates in counterfactual evaluation? Toward answering these\nquestions, first, we propose a novel framework, \\emph{Counterfactual Network\nEvaluator} (CONE), which (1) learns partial representations of latent\nconfounders under the supervision of observed treatments and outcomes; and (2)\ncombines them for counterfactual evaluation. Then through extensive\nexperiments, we corroborate the effectiveness of CONE. The results imply that\nincorporating network information mitigates hidden confounding bias in\ncounterfactual evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 21:13:10 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Guo", "Ruocheng", ""], ["Li", "Jundong", ""], ["Liu", "Huan", ""]]}, {"id": "1912.10552", "submitter": "Anahita Hosseini", "authors": "Anahita Hosseini, Tyler Davis, Majid Sarrafzadeh", "title": "Hierarchical Target-Attentive Diagnosis Prediction in Heterogeneous\n  Information Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HTAD, a novel model for diagnosis prediction using Electronic\nHealth Records (EHR) represented as Heterogeneous Information Networks. Recent\nstudies on modeling EHR have shown success in automatically learning\nrepresentations of the clinical records in order to avoid the need for manual\nfeature selection. However, these representations are often learned and\naggregated without specificity for the different possible targets being\npredicted. Our model introduces a target-aware hierarchical attention mechanism\nthat allows it to learn to attend to the most important clinical records when\naggregating their representations for prediction of a diagnosis.\n  We evaluate our model using a publicly available benchmark dataset and\ndemonstrate that the use of target-aware attention significantly improves\nperformance compared to the current state of the art. Additionally, we propose\na method for incorporating non-categorical data into our predictions and\ndemonstrate that this technique leads to further performance improvements.\nLastly, we demonstrate that the predictions made by our proposed model are\neasily interpretable.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:22:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hosseini", "Anahita", ""], ["Davis", "Tyler", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1912.10558", "submitter": "Renuka Sindhgatta", "authors": "Renuka Sindhgatta, Chun Ouyang, Catarina Moreira", "title": "Exploring Interpretability for Predictive Process Analytics", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern predictive analytics underpinned by machine learning techniques has\nbecome a key enabler to the automation of data-driven decision making. In the\ncontext of business process management, predictive analytics has been applied\nto making predictions about the future state of an ongoing business process\ninstance, for example, when will the process instance complete and what will be\nthe outcome upon completion. Machine learning models can be trained on event\nlog data recording historical process execution to build the underlying\npredictive models. Multiple techniques have been proposed so far which encode\nthe information available in an event log and construct input features required\nto train a predictive model. While accuracy has been a dominant criterion in\nthe choice of various techniques, they are often applied as a black-box in\nbuilding predictive models. In this paper, we derive explanations using\ninterpretable machine learning techniques to compare and contrast the\nsuitability of multiple predictive models of high accuracy. The explanations\nallow us to gain an understanding of the underlying reasons for a prediction\nand highlight scenarios where accuracy alone may not be sufficient in assessing\nthe suitability of techniques used to encode event log data to features used by\na predictive model. Findings from this study motivate the need and importance\nto incorporate interpretability in predictive process analytics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 23:09:34 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 10:42:45 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 12:09:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sindhgatta", "Renuka", ""], ["Ouyang", "Chun", ""], ["Moreira", "Catarina", ""]]}, {"id": "1912.10559", "submitter": "Drimik Roy Chowdhury", "authors": "Drimik Roy Chowdhury, Muhammad Firmansyah Kasim", "title": "Efficient Parameter Sampling for Neural Network Construction", "comments": "Accepted for NeurIPS 2019: Machine Learning and the Physical Sciences\n  conference. Paper archived here: https://ml4physicalsciences.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-ex physics.comp-ph physics.plasm-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The customizable nature of deep learning models have allowed them to be\nsuccessful predictors in various disciplines. These models are often trained\nwith respect to thousands or millions of instances for complicated problems,\nbut the gathering of such an immense collection may be infeasible and\nexpensive. However, what often occurs is the pollution of redundant information\nfrom these instances to the deep learning models. This paper outlines an\nalgorithm that dynamically selects and appends instances to a training dataset\nfrom uncertain regions of the parameter space based on differences in\npredictions from multiple convolutional neural networks (CNNs). These CNNs are\nalso simultaneously trained on this growing dataset to construct more accurate\nand knowledgable models. The methodology presented has reduced training dataset\nsizes by almost 90% and maintained predictive power in two diagnostics of high\nenergy density physics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 23:13:50 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chowdhury", "Drimik Roy", ""], ["Kasim", "Muhammad Firmansyah", ""]]}, {"id": "1912.10577", "submitter": "Zhihan Xiong", "authors": "Tian Tan, Zhihan Xiong, Vikranth R. Dwaracherla", "title": "Parameterized Indexed Value Function for Efficient Exploration in\n  Reinforcement Learning", "comments": "17 pages, 4 figures, Proceedings of the 34th AAAI Conference on\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that quantifying uncertainty in the action-value estimates\nis crucial for efficient exploration in reinforcement learning. Ensemble\nsampling offers a relatively computationally tractable way of doing this using\nrandomized value functions. However, it still requires a huge amount of\ncomputational resources for complex problems. In this paper, we present an\nalternative, computationally efficient way to induce exploration using index\nsampling. We use an indexed value function to represent uncertainty in our\naction-value estimates. We first present an algorithm to learn parameterized\nindexed value function through a distributional version of temporal difference\nin a tabular setting and prove its regret bound. Then, in a computational point\nof view, we propose a dual-network architecture, Parameterized Indexed Networks\n(PINs), comprising one mean network and one uncertainty network to learn the\nindexed value function. Finally, we show the efficacy of PINs through\ncomputational experiments.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 01:28:53 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 18:33:52 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tan", "Tian", ""], ["Xiong", "Zhihan", ""], ["Dwaracherla", "Vikranth R.", ""]]}, {"id": "1912.10583", "submitter": "Thinh Thanh Doan", "authors": "Thinh T. Doan", "title": "Finite-Time Analysis and Restarting Scheme for Linear Two-Time-Scale\n  Stochastic Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by their broad applications in reinforcement learning, we study the\nlinear two-time-scale stochastic approximation, an iterative method using two\ndifferent step sizes for finding the solutions of a system of two equations.\nOur main focus is to characterize the finite-time complexity of this method\nunder time-varying step sizes and Markovian noise. In particular, we show that\nthe mean square errors of the variables generated by the method converge to\nzero at a sublinear rate $\\Ocal(k^{2/3})$, where $k$ is the number of\niterations. We then improve the performance of this method by considering the\nrestarting scheme, where we restart the algorithm after every predetermined\nnumber of iterations. We show that using this restarting method the complexity\nof the algorithm under time-varying step sizes is as good as the one using\nconstant step sizes, but still achieving an exact converge to the desired\nsolution. Moreover, the restarting scheme also helps to prevent the step sizes\nfrom getting too small, which is useful for the practical implementation of the\nlinear two-time-scale stochastic approximation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 02:00:55 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 20:06:59 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Doan", "Thinh T.", ""]]}, {"id": "1912.10597", "submitter": "Pedro Sandoval Segura", "authors": "Pedro Sandoval Segura, Julius Lauw, Daniel Bashir, Kinjal Shah, Sonia\n  Sehra, Dominique Macias, and George Montanez", "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine\n  Learning Algorithm Capacity", "comments": "Accepted to 12th International Conference on Agents and Artificial\n  Intelligence (ICAART 2020), 7 pages including references", "journal-ref": null, "doi": "10.5220/0009178209800986", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm performance in supervised learning is a combination of\nmemorization, generalization, and luck. By estimating how much information an\nalgorithm can memorize from a dataset, we can set a lower bound on the amount\nof performance due to other factors such as generalization and luck. With this\ngoal in mind, we introduce the Labeling Distribution Matrix (LDM) as a tool for\nestimating the capacity of learning algorithms. The method attempts to\ncharacterize the diversity of possible outputs by an algorithm for different\ntraining datasets, using this to measure algorithm flexibility and\nresponsiveness to data. We test the method on several supervised learning\nalgorithms, and find that while the results are not conclusive, the LDM does\nallow us to gain potentially valuable insight into the prediction behavior of\nalgorithms. We also introduce the Label Recorder as an additional tool for\nestimating algorithm capacity, with more promising initial results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:07:00 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 02:35:08 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Segura", "Pedro Sandoval", ""], ["Lauw", "Julius", ""], ["Bashir", "Daniel", ""], ["Shah", "Kinjal", ""], ["Sehra", "Sonia", ""], ["Macias", "Dominique", ""], ["Montanez", "George", ""]]}, {"id": "1912.10598", "submitter": "Farbod Taymouri", "authors": "Farbod Taymouri, Marcello La Rosa, Josep Carmona", "title": "Business Process Variant Analysis based on Mutual Fingerprints of Event\n  Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing business process variants using event logs is a common use case in\nprocess mining. Existing techniques for process variant analysis detect\nstatistically-significant differences between variants at the level of\nindividual entities (such as process activities) and their relationships (e.g.\ndirectly-follows relations between activities). This may lead to a\nproliferation of differences due to the low level of granularity in which such\ndifferences are captured. This paper presents a novel approach to detect\nstatistically-significant differences between variants at the level of entire\nprocess traces (i.e. sequences of directly-follows relations). The cornerstone\nof this approach is a technique to learn a directly follows graph called mutual\nfingerprint from the event logs of the two variants. A mutual fingerprint is a\nlossless encoding of a set of traces and their duration using discrete wavelet\ntransformation. This structure facilitates the understanding of statistical\ndifferences along the control-flow and performance dimensions. The approach has\nbeen evaluated using real-life event logs against two baselines. The results\nshow that at a trace level, the baselines cannot always reveal the differences\ndiscovered by our approach, or can detect spurious differences.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:13:52 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 08:55:24 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Taymouri", "Farbod", ""], ["La Rosa", "Marcello", ""], ["Carmona", "Josep", ""]]}, {"id": "1912.10600", "submitter": "Yang Guan", "authors": "Yang Guan, Shengbo Eben Li, Jingliang Duan, Jie Li, Yangang Ren, Qi\n  Sun, Bo Cheng", "title": "Direct and indirect reinforcement learning", "comments": "Published in International Journal of Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms have been successfully applied to a\nrange of challenging sequential decision making and control tasks. In this\npaper, we classify RL into direct and indirect RL according to how they seek\nthe optimal policy of the Markov decision process problem. The former solves\nthe optimal policy by directly maximizing an objective function using gradient\ndescent methods, in which the objective function is usually the expectation of\naccumulative future rewards. The latter indirectly finds the optimal policy by\nsolving the Bellman equation, which is the sufficient and necessary condition\nfrom Bellman's principle of optimality. We study policy gradient forms of\ndirect and indirect RL and show that both of them can derive the actor-critic\narchitecture and can be unified into a policy gradient with the approximate\nvalue function and the stationary state distribution, revealing the equivalence\nof direct and indirect RL. We employ a Gridworld task to verify the influence\nof different forms of policy gradient, suggesting their differences and\nrelationships experimentally. Finally, we classify current mainstream RL\nalgorithms using the direct and indirect taxonomy, together with other ones\nincluding value-based and policy-based, model-based and model-free.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:20:42 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:14:44 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guan", "Yang", ""], ["Li", "Shengbo Eben", ""], ["Duan", "Jingliang", ""], ["Li", "Jie", ""], ["Ren", "Yangang", ""], ["Sun", "Qi", ""], ["Cheng", "Bo", ""]]}, {"id": "1912.10636", "submitter": "Takashi Goda", "authors": "Takashi Goda, Kei Ishikawa", "title": "Multilevel Monte Carlo estimation of log marginal likelihood", "comments": "4 pages, no figure, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we provide an unbiased multilevel Monte Carlo estimator of\nthe log marginal likelihood and discuss its application to variational Bayes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:44:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Goda", "Takashi", ""], ["Ishikawa", "Kei", ""]]}, {"id": "1912.10648", "submitter": "Xiaobai Ma Mr.", "authors": "Xiaobai Ma, Katherine Driggs-Campbell, Zongzhang Zhang, Mykel J.\n  Kochenderfer", "title": "Monte-Carlo Tree Search for Policy Optimization", "comments": "IJCAI 2019", "journal-ref": "In Proceedings of the 28th International Joint Conference on\n  Artificial Intelligence, pp. 3116-3122. AAAI Press, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient-based methods are often used for policy optimization in deep\nreinforcement learning, despite being vulnerable to local optima and saddle\npoints. Although gradient-free methods (e.g., genetic algorithms or evolution\nstrategies) help mitigate these issues, poor initialization and local optima\nare still concerns in highly nonconvex spaces. This paper presents a method for\npolicy optimization based on Monte-Carlo tree search and gradient-free\noptimization. Our method, called Monte-Carlo tree search for policy\noptimization (MCTSPO), provides a better exploration-exploitation trade-off\nthrough the use of the upper confidence bound heuristic. We demonstrate\nimproved performance on reinforcement learning tasks with deceptive or sparse\nreward functions compared to popular gradient-based and deep genetic algorithm\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:04:24 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ma", "Xiaobai", ""], ["Driggs-Campbell", "Katherine", ""], ["Zhang", "Zongzhang", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1912.10702", "submitter": "Bin Dai", "authors": "Bin Dai, Ziyu Wang, David Wipf", "title": "The Usual Suspects? Reassessing Blame for VAE Posterior Collapse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In narrow asymptotic settings Gaussian VAE models of continuous data have\nbeen shown to possess global optima aligned with ground-truth distributions.\nEven so, it is well known that poor solutions whereby the latent posterior\ncollapses to an uninformative prior are sometimes obtained in practice.\nHowever, contrary to conventional wisdom that largely assigns blame for this\nphenomena on the undue influence of KL-divergence regularization, we will argue\nthat posterior collapse is, at least in part, a direct consequence of bad local\nminima inherent to the loss surface of deep autoencoder networks. In\nparticular, we prove that even small nonlinear perturbations of affine VAE\ndecoder models can produce such minima, and in deeper models, analogous minima\ncan force the VAE to behave like an aggressive truncation operator, provably\ndiscarding information along all latent dimensions in certain circumstances.\nRegardless, the underlying message here is not meant to undercut valuable\nexisting explanations of posterior collapse, but rather, to refine the\ndiscussion and elucidate alternative risk factors that may have been previously\nunderappreciated.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:40:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dai", "Bin", ""], ["Wang", "Ziyu", ""], ["Wipf", "David", ""]]}, {"id": "1912.10703", "submitter": "Dongqi Han", "authors": "Dongqi Han, Kenji Doya, Jun Tani", "title": "Variational Recurrent Models for Solving Partially Observable Control\n  Tasks", "comments": "Published as a conference paper at the Eighth International\n  Conference on Learning Representations (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In partially observable (PO) environments, deep reinforcement learning (RL)\nagents often suffer from unsatisfactory performance, since two problems need to\nbe tackled together: how to extract information from the raw observations to\nsolve the task, and how to improve the policy. In this study, we propose an RL\nalgorithm for solving PO tasks. Our method comprises two parts: a variational\nrecurrent model (VRM) for modeling the environment, and an RL controller that\nhas access to both the environment and the VRM. The proposed algorithm was\ntested in two types of PO robotic control tasks, those in which either\ncoordinates or velocities were not observable and those that require long-term\nmemorization. Our experiments show that the proposed algorithm achieved better\ndata efficiency and/or learned more optimal policy than other alternative\napproaches in tasks in which unobserved states cannot be inferred from raw\nobservations in a simple manner.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 09:43:16 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 05:05:00 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Han", "Dongqi", ""], ["Doya", "Kenji", ""], ["Tani", "Jun", ""]]}, {"id": "1912.10708", "submitter": "Minoru Kusaba", "authors": "Minoru Kusaba, Chang Liu, Yukinori Koyama, Kiyoyuki Terakura, Ryo\n  Yoshida", "title": "Recreation of the Periodic Table with an Unsupervised Machine Learning\n  Algorithm", "comments": "28 pages, 14 figures, complete version of this paper is available at\n  https://www.nature.com/articles/s41598-021-81850-z (Published: 26 February\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1869, the first draft of the periodic table was published by Russian\nchemist Dmitri Mendeleev. In terms of data science, his achievement can be\nviewed as a successful example of feature embedding based on human cognition:\nchemical properties of all known elements at that time were compressed onto the\ntwo-dimensional grid system for tabular display. In this study, we seek to\nanswer the question of whether machine learning can reproduce or recreate the\nperiodic table by using observed physicochemical properties of the elements. To\nachieve this goal, we developed a periodic table generator (PTG). The PTG is an\nunsupervised machine learning algorithm based on the generative topographic\nmapping (GTM), which can automate the translation of high-dimensional data into\na tabular form with varying layouts on-demand. The PTG autonomously produced\nvarious arrangements of chemical symbols, which organized a two-dimensional\narray such as Mendeleev's periodic table or three-dimensional spiral table\naccording to the underlying periodicity in the given data. We further showed\nwhat the PTG learned from the element data and how the element features, such\nas melting point and electronegativity, are compressed to the lower-dimensional\nlatent spaces.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:01:51 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 21:24:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kusaba", "Minoru", ""], ["Liu", "Chang", ""], ["Koyama", "Yukinori", ""], ["Terakura", "Kiyoyuki", ""], ["Yoshida", "Ryo", ""]]}, {"id": "1912.10729", "submitter": "Yujing Wang", "authors": "Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce Zhang, Guinan Su,\n  Xiaoyu Kou, Yunhai Tong, Mao Yang, Lidong Zhou", "title": "TextNAS: A Neural Architecture Search Space tailored for Text\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning text representation is crucial for text classification and other\nlanguage related tasks. There are a diverse set of text representation networks\nin the literature, and how to find the optimal one is a non-trivial problem.\nRecently, the emerging Neural Architecture Search (NAS) techniques have\ndemonstrated good potential to solve the problem. Nevertheless, most of the\nexisting works of NAS focus on the search algorithms and pay little attention\nto the search space. In this paper, we argue that the search space is also an\nimportant human prior to the success of NAS in different applications. Thus, we\npropose a novel search space tailored for text representation. Through\nautomatic search, the discovered network architecture outperforms\nstate-of-the-art models on various public datasets on text classification and\nnatural language inference tasks. Furthermore, some of the design principles\nfound in the automatic network agree well with human intuition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:51:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Yujing", ""], ["Yang", "Yaming", ""], ["Chen", "Yiren", ""], ["Bai", "Jing", ""], ["Zhang", "Ce", ""], ["Su", "Guinan", ""], ["Kou", "Xiaoyu", ""], ["Tong", "Yunhai", ""], ["Yang", "Mao", ""], ["Zhou", "Lidong", ""]]}, {"id": "1912.10730", "submitter": "Yingshi Chen", "authors": "Yingshi Chen, Jinfeng Zhu", "title": "An optical diffractive deep neural network with multiple\n  frequency-channels", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffractive deep neural network (DNNet) is a novel machine learning framework\non the modulation of optical transmission. Diffractive network would get\npredictions at the speed of light. It's pure passive architecture, no\nadditional power consumption. We improved the accuracy of diffractive network\nwith optical waves at different frequency. Each layers have multiple\nfrequency-channels (optical distributions at different frequency). These\nchannels are merged at the output plane to get final output. The experiment in\nthe fasion-MNIST and EMNIST datasets showed multiple frequency-channels would\nincrease the accuracy a lot. We also give detailed analysis to show the\ndifference between DNNet and MLP. The modulation process in DNNet is actually\noptical activation function. We develop an open source package ONNet. The\nsource codes are available at https://github.com/closest-git/ONNet.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:54:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "Yingshi", ""], ["Zhu", "Jinfeng", ""]]}, {"id": "1912.10742", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Bertrand Michel", "title": "Statistical analysis of Mapper for stochastic and multivariate filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reeb spaces, as well as their discretized versions called Mappers, are common\ndescriptors used in Topological Data Analysis, with plenty of applications in\nvarious fields of science, such as computational biology and data\nvisualization, among others. The stability and quantification of the rate of\nconvergence of the Mapper to the Reeb space has been studied a lot in recent\nworks [BBMW19, CO17, CMO18, MW16], focusing on the case where a scalar-valued\nfilter is used for the computation of Mapper. On the other hand, much less is\nknown in the multivariate case, when the codomain of the filter is\n$\\mathbb{R}^p$, and in the general case, when it is a general metric space $(Z,\nd_Z)$, instead of $\\mathbb{R}$. The few results that are available in this\nsetting [DMW17, MW16] can only handle continuous topological spaces and cannot\nbe used as is for finite metric spaces representing data, such as point clouds\nand distance matrices. In this article, we introduce a slight modification of\nthe usual Mapper construction and we give risk bounds for estimating the Reeb\nspace using this estimator. Our approach applies in particular to the setting\nwhere the filter function used to compute Mapper is also estimated from data,\nsuch as the eigenfunctions of PCA. Our results are given with respect to the\nGromov-Hausdorff distance, computed with specific filter-based pseudometrics\nfor Mappers and Reeb spaces defined in [DMW17]. We finally provide applications\nof this setting in statistics and machine learning for different kinds of\ntarget filters, as well as numerical experiments that demonstrate the relevance\nof our approach\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:32:07 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 11:31:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Michel", "Bertrand", ""]]}, {"id": "1912.10746", "submitter": "Hassan Eldeeb Dr.", "authors": "Hassan Eldeeb and Abdelrhman Eldallal", "title": "AutoML: Exploration v.s. Exploitation", "comments": "The paper has been rejected by EDBT conference and it needs major\n  enhancements and modifications. Therefore, it is better to be withdrawn until\n  we finish these enhancements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a machine learning (ML) pipeline in an automated way is a crucial\nand complex task as it is constrained with the available time budget and\nresources. This encouraged the research community to introduce several\nsolutions to utilize the available time and resources. A lot of work is done to\nsuggest the most promising classifiers for a given dataset using sundry of\ntechniques including meta-learning based techniques. This gives the autoML\nframework the chance to spend more time exploiting those classifiers and tuning\ntheir hyper-parameters. In this paper, we empirically study the hypothesis of\nimproving the pipeline performance by exploiting the most promising classifiers\nwithin the limited time budget. We also study the effect of increasing the time\nbudget over the pipeline performance. The empirical results across autoSKLearn,\nTPOT and ATM, show that exploiting the most promising classifiers does not\nachieve a statistically better performance than exploring the entire search\nspace. The same conclusion is also applied for long time budgets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:41:11 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 23:32:07 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Eldeeb", "Hassan", ""], ["Eldallal", "Abdelrhman", ""]]}, {"id": "1912.10754", "submitter": "Jaouad Mourtada", "authors": "Jaouad Mourtada", "title": "Exact minimax risk for linear least squares, and the lower tail of\n  sample covariance matrices", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first part of this paper is devoted to the decision-theoretic analysis of\nrandom-design linear prediction. It is known that, under boundedness\nconstraints on the response (and thus on regression coefficients), the minimax\nexcess risk scales, up to constants, as $\\sigma^2 d / n$ in dimension $d$ with\n$n$ samples and noise $\\sigma^2$. Here, we study the expected excess risk with\nrespect to the full linear class. We show that the ordinary least squares\nestimator is exactly minimax optimal in the well-specified case for every\ndistribution of covariates. Further, we express the minimax risk in terms of\nthe distribution of \\emph{statistical leverage scores} of individual samples.\nWe deduce a precise minimax lower bound of $\\sigma^2d/(n-d+1)$ for general\ncovariate distribution, which nearly matches the risk for Gaussian design. We\nthen obtain nonasymptotic upper bounds on the minimax risk for covariates that\nsatisfy a \"small ball\"-type regularity condition, which scale as\n$(1+o(1))\\sigma^2d/n$ as $d=o(n)$, both in the well-specified and misspecified\ncases.\n  Our main technical contribution is the study of the lower tail of the\nsmallest singular value of empirical covariance matrices around $0$. We\nestablish a lower bound on this lower tail, valid for any distribution in\ndimension $d \\geq 2$, together with a matching upper bound under a necessary\nregularity condition. Our proof relies on the PAC-Bayesian technique for\ncontrolling empirical processes, and extends an analysis of Oliveira devoted to\na different part of the lower tail. Equivalently, our upper bound shows that\nthe operator norm of the inverse sample covariance matrix has bounded $L^q$\nnorm up to $q \\asymp n$, and our lower bound implies that this exponent is\nunimprovable. Finally, we show that the regularity condition naturally holds\nfor independent coordinates.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:08:09 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:46:12 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mourtada", "Jaouad", ""]]}, {"id": "1912.10764", "submitter": "S\\'ebastien Henwood", "authors": "S\\'ebastien Henwood, Fran\\c{c}ois Leduc-Primeau and Yvon Savaria", "title": "Layerwise Noise Maximisation to Train Low-Energy Deep Neural Networks", "comments": "To be presented at AICAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) depend on the storage of a large number of\nparameters, which consumes an important portion of the energy used during\ninference. This paper considers the case where the energy usage of memory\nelements can be reduced at the cost of reduced reliability. A training\nalgorithm is proposed to optimize the reliability of the storage separately for\neach layer of the network, while incurring a negligible complexity overhead\ncompared to a conventional stochastic gradient descent training. For an\nexponential energy-reliability model, the proposed training approach can\ndecrease the memory energy consumption of a DNN with binary parameters by\n3.3$\\times$ at isoaccuracy, compared to a reliable implementation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:36:51 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Henwood", "S\u00e9bastien", ""], ["Leduc-Primeau", "Fran\u00e7ois", ""], ["Savaria", "Yvon", ""]]}, {"id": "1912.10773", "submitter": "Sampo Kuutti", "authors": "Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, Saber Fallah", "title": "A Survey of Deep Learning Applications to Autonomous Vehicle Control", "comments": "23 pages, 3 figures, Accepted in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a controller for autonomous vehicles capable of providing adequate\nperformance in all driving scenarios is challenging due to the highly complex\nenvironment and inability to test the system in the wide variety of scenarios\nwhich it may encounter after deployment. However, deep learning methods have\nshown great promise in not only providing excellent performance for complex and\nnon-linear control problems, but also in generalising previously learned rules\nto new scenarios. For these reasons, the use of deep learning for vehicle\ncontrol is becoming increasingly popular. Although important advancements have\nbeen achieved in this field, these works have not been fully summarised. This\npaper surveys a wide range of research works reported in the literature which\naim to control a vehicle through deep learning methods. Although there exists\noverlap between control and perception, the focus of this paper is on vehicle\ncontrol, rather than the wider perception problem which includes tasks such as\nsemantic segmentation and object detection. The paper identifies the strengths\nand limitations of available deep learning methods through comparative analysis\nand discusses the research challenges in terms of computation, architecture\nselection, goal specification, generalisation, verification and validation, as\nwell as safety. Overall, this survey brings timely and topical information to a\nrapidly evolving field relevant to intelligent transportation systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:50:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kuutti", "Sampo", ""], ["Bowden", "Richard", ""], ["Jin", "Yaochu", ""], ["Barber", "Phil", ""], ["Fallah", "Saber", ""]]}, {"id": "1912.10784", "submitter": "Jaouad Mourtada", "authors": "Jaouad Mourtada, St\\'ephane Ga\\\"iffas", "title": "An improper estimator with optimal excess risk in misspecified density\n  estimation and logistic regression", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a procedure for predictive conditional density estimation under\nlogarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator\nminimizes a new general excess risk bound for supervised statistical learning.\nOn standard examples, this bound scales as $d/n$ with $d$ the model dimension\nand $n$ the sample size, and critically remains valid under model\nmisspecification. Being an improper (out-of-model) procedure, SMP improves over\nwithin-model estimators such as the maximum likelihood estimator, whose excess\nrisk degrades under misspecification. Compared to approaches reducing to the\nsequential problem, our bounds remove suboptimal $\\log n$ factors, addressing\nan open problem from Gr\\\"unwald and Kotlowski for the considered models, and\ncan handle unbounded classes. For the Gaussian linear model, the predictions\nand risk bound of SMP are governed by leverage scores of covariates, nearly\nmatching the optimal risk in the well-specified case without conditions on the\nnoise variance or approximation error of the linear model. For logistic\nregression, SMP provides a non-Bayesian approach to calibration of\nprobabilistic predictions relying on virtual samples, and can be computed by\nsolving two logistic regressions. It achieves a non-asymptotic excess risk of\n$O ( (d + B^2R^2)/n )$, where $R$ bounds the norm of features and $B$ that of\nthe comparison parameter; by contrast, no within-model estimator can achieve\nbetter rate than $\\min( {B R}/{\\sqrt{n}}, {d e^{BR}}/{n} )$ in general. This\nprovides a computationally more efficient alternative to Bayesian approaches,\nwhich require approximate posterior sampling, thereby partly answering a\nquestion by Foster et al. (2018).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:11:54 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:04:49 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""]]}, {"id": "1912.10787", "submitter": "Austin Dill", "authors": "Austin Dill, Songwei Ge, Eunsu Kang, Chun-Liang Li, Barnabas Poczos", "title": "Learned Interpolation for 3D Generation", "comments": "Creativity and Design Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to generate novel 3D shapes with machine learning, one must allow\nfor interpolation. The typical approach for incorporating this creative process\nis to interpolate in a learned latent space so as to avoid the problem of\ngenerating unrealistic instances by exploiting the model's learned structure.\nThe process of the interpolation is supposed to form a semantically smooth\nmorphing. While this approach is sound for synthesizing realistic media such as\nlifelike portraits or new designs for everyday objects, it subjectively fails\nto directly model the unexpected, unrealistic, or creative. In this work, we\npresent a method for learning how to interpolate point clouds. By encoding\nprior knowledge about real-world objects, the intermediate forms are both\nrealistic and unlike any existing forms. We show not only how this method can\nbe used to generate \"creative\" point clouds, but how the method can also be\nleveraged to generate 3D models suitable for sculpture.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:44:33 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 20:12:32 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Dill", "Austin", ""], ["Ge", "Songwei", ""], ["Kang", "Eunsu", ""], ["Li", "Chun-Liang", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1912.10798", "submitter": "Kelly Kochanski", "authors": "Kelly Kochanski, Divya Mohan, Jenna Horrall, Barry Rountree, Ghaleb\n  Abdulla", "title": "Deep learning predictions of sand dune migration", "comments": "Workshop on Tackling climate change with machine learning at NeurIPS.\n  Vancouver, Canada, December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dry decade in the Navajo Nation has killed vegetation, dessicated soils,\nand released once-stable sand into the wind. This sand now covers one-third of\nthe Nation's land, threatening roads, gardens and hundreds of homes. Many arid\nregions have similar problems: global warming has increased dune movement\nacross farmland in Namibia and Angola, and the southwestern US. Current dune\nmodels, unfortunately, do not scale well enough to provide useful forecasts for\nthe $\\sim$5\\% of land surfaces covered by mobile sand. We test the ability of\ntwo deep learning algorithms, a GAN and a CNN, to model the motion of sand\ndunes. The models are trained on simulated data from community-standard\ncellular automaton model of sand dunes. Preliminary results show the GAN\nproducing reasonable forward predictions of dune migration at ten million times\nthe speed of the existing model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:17:42 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kochanski", "Kelly", ""], ["Mohan", "Divya", ""], ["Horrall", "Jenna", ""], ["Rountree", "Barry", ""], ["Abdulla", "Ghaleb", ""]]}, {"id": "1912.10819", "submitter": "Joeran Beel", "authors": "Conor O'Sullivan and Joeran Beel", "title": "Predicting the Outcome of Judicial Decisions made by the European Court\n  of Human Rights", "comments": null, "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, machine learning models were constructed to predict whether\njudgments made by the European Court of Human Rights (ECHR) would lead to a\nviolation of an Article in the Convention on Human Rights. The problem is\nframed as a binary classification task where a judgment can lead to a\n\"violation\" or \"non-violation\" of a particular Article. Using auto-sklearn, an\nautomated algorithm selection package, models were constructed for 12 Articles\nin the Convention. To train these models, textual features were obtained from\nthe ECHR Judgment documents using N-grams, word embeddings and paragraph\nembeddings. Additional documents, from the ECHR, were incorporated into the\nmodels through the creation of a word embedding (echr2vec) and a doc2vec model.\nThe features obtained using the echr2vec embedding provided the highest\ncross-validation accuracy for 5 of the Articles. The overall test accuracy,\nacross the 12 Articles, was 68.83%. As far as we could tell, this is the first\nestimate of the accuracy of such machine learning models using a realistic test\nset. This provides an important benchmark for future work. As a baseline, a\nsimple heuristic of always predicting the most common outcome in the past was\nused. The heuristic achieved an overall test accuracy of 86.68% which is 29.7%\nhigher than the models. Again, this was seemingly the first study that included\nsuch a heuristic with which to compare model results. The higher accuracy\nachieved by the heuristic highlights the importance of including such a\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:42:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["O'Sullivan", "Conor", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10829", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Elena Zheleva, and Tanya Y. Berger-Wolf", "title": "Variable-lag Granger Causality for Time Series Analysis", "comments": "This paper will be appeared in the proceeding of 2019 IEEE\n  International Conference on Data Science and Advanced Analytics (DSAA). The R\n  package is available at https://github.com/DarkEyes/VLTimeSeriesCausality", "journal-ref": "Proceedings of 2019 IEEE International Conference on Data Science\n  and Advanced Analytics (DSAA)", "doi": "10.1109/DSAA.2019.00016", "report-no": null, "categories": "cs.LG econ.EM q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is a fundamental technique for causal inference in time\nseries data, commonly used in the social and biological sciences. Typical\noperationalizations of Granger causality make a strong assumption that every\ntime point of the effect time series is influenced by a combination of other\ntime series with a fixed time delay. However, the assumption of the fixed time\ndelay does not hold in many applications, such as collective behavior,\nfinancial markets, and many natural phenomena. To address this issue, we\ndevelop variable-lag Granger causality, a generalization of Granger causality\nthat relaxes the assumption of the fixed time delay and allows causes to\ninfluence effects with arbitrary time delays. In addition, we propose a method\nfor inferring variable-lag Granger causality relations. We demonstrate our\napproach on an application for studying coordinated collective behavior and\nshow that it performs better than several existing methods in both simulated\nand real-world datasets. Our approach can be applied in any domain of time\nseries analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:38:48 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Zheleva", "Elena", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "1912.10832", "submitter": "Xiaoqing Yang", "authors": "Huiting Hong, Hantao Guo, Yucheng Lin, Xiaoqing Yang, Zang Li, Jieping\n  Ye", "title": "An Attention-based Graph Neural Network for Heterogeneous Structural\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on graph representation learning of heterogeneous\ninformation network (HIN), in which various types of vertices are connected by\nvarious types of relations. Most of the existing methods conducted on HIN\nrevise homogeneous graph embedding models via meta-paths to learn\nlow-dimensional vector space of HIN. In this paper, we propose a novel\nHeterogeneous Graph Structural Attention Neural Network (HetSANN) to directly\nencode structural information of HIN without meta-path and achieve more\ninformative representations. With this method, domain experts will not be\nneeded to design meta-path schemes and the heterogeneous information can be\nprocessed automatically by our proposed model. Specifically, we implicitly\nrepresent heterogeneous information using the following two methods: 1) we\nmodel the transformation between heterogeneous vertices through a projection in\nlow-dimensional entity spaces; 2) afterwards, we apply the graph neural network\nto aggregate multi-relational information of projected neighborhood by means of\nattention mechanism. We also present three extensions of HetSANN, i.e.,\nvoices-sharing product attention for the pairwise relationships in HIN,\ncycle-consistency loss to retain the transformation between heterogeneous\nentity spaces, and multi-task learning with full use of information. The\nexperiments conducted on three public datasets demonstrate that our proposed\nmodels achieve significant and consistent improvements compared to\nstate-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 06:20:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hong", "Huiting", ""], ["Guo", "Hantao", ""], ["Lin", "Yucheng", ""], ["Yang", "Xiaoqing", ""], ["Li", "Zang", ""], ["Ye", "Jieping", ""]]}, {"id": "1912.10834", "submitter": "Stefano Teso", "authors": "Stefano Teso", "title": "Does Symbolic Knowledge Prevent Adversarial Fooling?", "comments": "Short position paper. Accepted at the Ninth International Workshop on\n  Statistical Relational AI (StarIA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguments in favor of injecting symbolic knowledge into neural architectures\nabound. When done right, constraining a sub-symbolic model can substantially\nimprove its performance and sample complexity and prevent it from predicting\ninvalid configurations. Focusing on deep probabilistic (logical) graphical\nmodels -- i.e., constrained joint distributions whose parameters are determined\n(in part) by neural nets based on low-level inputs -- we draw attention to an\nelementary but unintended consequence of symbolic knowledge: that the resulting\nconstraints can propagate the negative effects of adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:50:33 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Teso", "Stefano", ""]]}, {"id": "1912.10836", "submitter": "Aykut \\c{C}ay{\\i}r", "authors": "Aykut \\c{C}ay{\\i}r, U\\u{g}ur \\\"Unal and Hasan Da\\u{g}", "title": "Random CapsNet Forest Model for Imbalanced Malware Type Classification\n  Task", "comments": "30 pages, 10 figures, typos are corrected, references are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Behavior of a malware varies with respect to malware types. Therefore,knowing\ntype of a malware affects strategies of system protection softwares. Many\nmalware type classification models empowered by machine and deep learning\nachieve superior accuracies to predict malware types.Machine learning based\nmodels need to do heavy feature engineering and feature engineering is\ndominantly effecting performance of models.On the other hand, deep learning\nbased models require less feature engineering than machine learning based\nmodels. However, traditional deep learning architectures and components cause\nvery complex and data sensitive models. Capsule network architecture minimizes\nthis complexity and data sensitivity unlike classical convolutional neural\nnetwork architectures. This paper proposes an ensemble capsule network model\nbased on bootstrap aggregating technique. The proposed method are tested on two\nmalware datasets, whose the-state-of-the-art results are well-known.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 06:40:40 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:51:31 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 19:56:53 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 20:21:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["\u00c7ay\u0131r", "Aykut", ""], ["\u00dcnal", "U\u011fur", ""], ["Da\u011f", "Hasan", ""]]}, {"id": "1912.10858", "submitter": "Xuan-Hong Dang", "authors": "Xuan-Hong Dang, Syed Yousaf Shah, Petros Zerfos", "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities\n  for Automated Financial Information Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal analysis that uses numerical time series and textual corpora as\ninput data sources is becoming a promising approach, especially in the\nfinancial industry. However, the main focus of such analysis has been on\nachieving high prediction accuracy while little effort has been spent on the\nimportant task of understanding the association between the two data\nmodalities. Performance on the time series hence receives little explanation\nthough human-understandable textual information is available. In this work, we\naddress the problem of given a numerical time series, and a general corpus of\ntextual stories collected in the same period of the time series, the task is to\ntimely discover a succinct set of textual stories associated with that time\nseries. Towards this goal, we propose a novel multi-modal neural model called\nMSIN that jointly learns both numerical time series and categorical text\narticles in order to unearth the association between them. Through multiple\nsteps of data interrelation between the two data modalities, MSIN learns to\nfocus on a small subset of text articles that best align with the performance\nin the time series. This succinct set is timely discovered and presented as\nrecommended documents, acting as automated information filtering, for the given\ntime series. We empirically evaluate the performance of our model on\ndiscovering relevant news articles for two stock time series from Apple and\nGoogle companies, along with the daily news articles collected from the Thomson\nReuters over a period of seven consecutive years. The experimental results\ndemonstrate that MSIN achieves up to 84.9% and 87.2% in recalling the ground\ntruth articles respectively to the two examined time series, far more superior\nto state-of-the-art algorithms that rely on conventional attention mechanism in\ndeep learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:37:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dang", "Xuan-Hong", ""], ["Shah", "Syed Yousaf", ""], ["Zerfos", "Petros", ""]]}, {"id": "1912.10872", "submitter": "Ari Frankel", "authors": "Ari Frankel, Reese Jones, Laura Swiler", "title": "Tensor Basis Gaussian Process Models of Hyperelastic Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop Gaussian process regression (GPR) models of\nhyperelastic material behavior. First, we consider the direct approach of\nmodeling the components of the Cauchy stress tensor as a function of the\ncomponents of the Finger stretch tensor in a Gaussian process. We then consider\nan improvement on this approach that embeds rotational invariance of the\nstress-stretch constitutive relation in the GPR representation. This approach\nrequires fewer training examples and achieves higher accuracy while maintaining\ninvariance to rotations exactly. Finally, we consider an approach that recovers\nthe strain-energy density function and derives the stress tensor from this\npotential. Although the error of this model for predicting the stress tensor is\nhigher, the strain-energy density is recovered with high accuracy from limited\ntraining data. The approaches presented here are examples of physics-informed\nmachine learning. They go beyond purely data-driven approaches by embedding the\nphysical system constraints directly into the Gaussian process representation\nof materials models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:32:24 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Frankel", "Ari", ""], ["Jones", "Reese", ""], ["Swiler", "Laura", ""]]}, {"id": "1912.10900", "submitter": "Lukas Hewing", "authors": "Lukas Hewing and Elena Arcari and Lukas P. Fr\\\"ohlich and Melanie N.\n  Zeilinger", "title": "On Simulation and Trajectory Prediction with Gaussian Process Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Established techniques for simulation and prediction with Gaussian process\n(GP) dynamics often implicitly make use of an independence assumption on\nsuccessive function evaluations of the dynamics model. This can result in\nsignificant error and underestimation of the prediction uncertainty,\npotentially leading to failures in safety-critical applications. This paper\ndiscusses methods that explicitly take the correlation of successive function\nevaluations into account. We first describe two sampling-based techniques; one\napproach provides samples of the true trajectory distribution, suitable for\n`ground truth' simulations, while the other draws function samples from basis\nfunction approximations of the GP. Second, we propose a linearization-based\ntechnique that directly provides approximations of the trajectory distribution,\ntaking correlations explicitly into account. We demonstrate the procedures in\nsimple numerical examples, contrasting the results with established methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:00:48 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 08:50:53 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 07:39:23 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hewing", "Lukas", ""], ["Arcari", "Elena", ""], ["Fr\u00f6hlich", "Lukas P.", ""], ["Zeilinger", "Melanie N.", ""]]}, {"id": "1912.10903", "submitter": "Thomas Bonald", "authors": "Nathan de Lara (IP Paris, DIG, INFRES, LINCS), Thomas Bonald (DIG,\n  INFRES, IP Paris, LINCS)", "title": "Spectral embedding of regularized block models", "comments": null, "journal-ref": "ICLR, 2020, Addis Abeba, Ethiopia", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding is a popular technique for the representation of graph\ndata. Several regularization techniques have been proposed to improve the\nquality of the embedding with respect to downstream tasks like clustering. In\nthis paper, we explain on a simple block model the impact of the complete graph\nregularization, whereby a constant is added to all entries of the adjacency\nmatrix. Specifically, we show that the regularization forces the spectral\nembedding to focus on the largest blocks, making the representation less\nsensitive to noise or outliers. We illustrate these results on both on both\nsynthetic and real data, showing how regularization improves standard\nclustering scores.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:06:54 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["de Lara", "Nathan", "", "IP Paris, DIG, INFRES, LINCS"], ["Bonald", "Thomas", "", "DIG,\n  INFRES, IP Paris, LINCS"]]}, {"id": "1912.10934", "submitter": "Frederik Gossen", "authors": "Frederik Gossen and Bernhard Steffen", "title": "Large Random Forests: Optimisation for Rapid Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests are one of the most popular classifiers in machine learning.\nThe larger they are, the more precise is the outcome of their predictions.\nHowever, this comes at a cost: their running time for classification grows\nlinearly with the number of trees, i.e. the size of the forest. In this paper,\nwe propose a method to aggregate large Random Forests into a single,\nsemantically equivalent decision diagram. Our experiments on various popular\ndatasets show speed-ups of several orders of magnitude, while, at the same\ntime, also significantly reducing the size of the required data structure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:49:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Gossen", "Frederik", ""], ["Steffen", "Bernhard", ""]]}, {"id": "1912.10979", "submitter": "Florian Lemmerich", "authors": "Michael Ellers, Michael Cochez, Tobias Schumacher, Markus Strohmaier,\n  Florian Lemmerich", "title": "Privacy Attacks on Network Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data ownership and data protection are increasingly important topics with\nethical and legal implications, e.g., with the right to erasure established in\nthe European General Data Protection Regulation (GDPR). In this light, we\ninvestigate network embeddings, i.e., the representation of network nodes as\nlow-dimensional vectors. We consider a typical social network scenario with\nnodes representing users and edges relationships between them. We assume that a\nnetwork embedding of the nodes has been trained. After that, a user demands the\nremoval of his data, requiring the full deletion of the corresponding network\ninformation, in particular the corresponding node and incident edges. In that\nsetting, we analyze whether after the removal of the node from the network and\nthe deletion of the vector representation of the respective node in the\nembedding significant information about the link structure of the removed node\nis still encoded in the embedding vectors of the remaining nodes. This would\nrequire a (potentially computationally expensive) retraining of the embedding.\nFor that purpose, we deploy an attack that leverages information from the\nremaining network and embedding to recover information about the neighbors of\nthe removed node. The attack is based on (i) measuring distance changes in\nnetwork embeddings and (ii) a machine learning classifier that is trained on\nnetworks that are constructed by removing additional nodes. Our experiments\ndemonstrate that substantial information about the edges of a removed node/user\ncan be retrieved across many different datasets. This implies that to fully\nprotect the privacy of users, node deletion requires complete retraining - or\nat least a significant modification - of original network embeddings. Our\nresults suggest that deleting the corresponding vector representation from\nnetwork embeddings alone is not sufficient from a privacy perspective.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:10:20 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ellers", "Michael", ""], ["Cochez", "Michael", ""], ["Schumacher", "Tobias", ""], ["Strohmaier", "Markus", ""], ["Lemmerich", "Florian", ""]]}, {"id": "1912.10985", "submitter": "Felix Dangel", "authors": "Felix Dangel, Frederik Kunstner, Philipp Hennig", "title": "BackPACK: Packing more into backprop", "comments": "Main text: 10 pages, 7 figures, 1 table; Supplements: 10 pages, 4\n  figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation frameworks are optimized for exactly one thing:\ncomputing the average mini-batch gradient. Yet, other quantities such as the\nvariance of the mini-batch gradients or many approximations to the Hessian can,\nin theory, be computed efficiently, and at the same time as the gradient. While\nthese quantities are of great interest to researchers and practitioners,\ncurrent deep-learning software does not support their automatic calculation.\nManually implementing them is burdensome, inefficient if done naively, and the\nresulting code is rarely shared. This hampers progress in deep learning, and\nunnecessarily narrows research to focus on gradient descent and its variants;\nit also complicates replication studies and comparisons between newly developed\nmethods that require those quantities, to the point of impossibility. To\naddress this problem, we introduce BackPACK, an efficient framework built on\ntop of PyTorch, that extends the backpropagation algorithm to extract\nadditional information from first- and second-order derivatives. Its\ncapabilities are illustrated by benchmark reports for computing additional\nquantities on deep neural networks, and an example application by testing\nseveral recent curvature approximations for optimization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:22:45 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 15:10:34 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Dangel", "Felix", ""], ["Kunstner", "Frederik", ""], ["Hennig", "Philipp", ""]]}, {"id": "1912.11006", "submitter": "Gongfan Fang", "authors": "Gongfan Fang, Jie Song, Chengchao Shen, Xinchao Wang, Da Chen, Mingli\n  Song", "title": "Data-Free Adversarial Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) has made remarkable progress in the last few\nyears and become a popular paradigm for model compression and knowledge\ntransfer. However, almost all existing KD algorithms are data-driven, i.e.,\nrelying on a large amount of original training data or alternative data, which\nis usually unavailable in real-world scenarios. In this paper, we devote\nourselves to this challenging problem and propose a novel adversarial\ndistillation mechanism to craft a compact student model without any real-world\ndata. We introduce a model discrepancy to quantificationally measure the\ndifference between student and teacher models and construct an optimizable\nupper bound. In our work, the student and the teacher jointly act the role of\nthe discriminator to reduce this discrepancy, when a generator adversarially\nproduces some \"hard samples\" to enlarge it. Extensive experiments demonstrate\nthat the proposed data-free method yields comparable performance to existing\ndata-driven methods. More strikingly, our approach can be directly extended to\nsemantic segmentation, which is more complicated than classification, and our\napproach achieves state-of-the-art results. Code and pretrained models are\navailable at https://github.com/VainF/Data-Free-Adversarial-Distillation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:08:33 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 07:01:32 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 12:12:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fang", "Gongfan", ""], ["Song", "Jie", ""], ["Shen", "Chengchao", ""], ["Wang", "Xinchao", ""], ["Chen", "Da", ""], ["Song", "Mingli", ""]]}, {"id": "1912.11023", "submitter": "James Ault", "authors": "James Ault, Josiah P. Hanna, Guni Sharon", "title": "Learning an Interpretable Traffic Signal Control Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signalized intersections are managed by controllers that assign right of way\n(green, yellow, and red lights) to non-conflicting directions. Optimizing the\nactuation policy of such controllers is expected to alleviate traffic\ncongestion and its adverse impact. Given such a safety-critical domain, the\naffiliated actuation policy is required to be interpretable in a way that can\nbe understood and regulated by a human. This paper presents and analyzes\nseveral on-line optimization techniques for tuning interpretable control\nfunctions. Although these techniques are defined in a general way, this paper\nassumes a specific class of interpretable control functions (polynomial\nfunctions) for analysis purposes. We show that such an interpretable policy\nfunction can be as effective as a deep neural network for approximating an\noptimized signal actuation policy. We present empirical evidence that supports\nthe use of value-based reinforcement learning for on-line training of the\ncontrol function. Specifically, we present and study three variants of the Deep\nQ-learning algorithm that allow the training of an interpretable policy\nfunction. Our Deep Regulatable Hardmax Q-learning variant is shown to be\nparticularly effective in optimizing our interpretable actuation policy,\nresulting in up to 19.4% reduced vehicles delay compared to commonly deployed\nactuated signal controllers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:41:59 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 20:59:27 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ault", "James", ""], ["Hanna", "Josiah P.", ""], ["Sharon", "Guni", ""]]}, {"id": "1912.11029", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis and Iason Papaioannou and Daniel Straub and Fabio\n  Nobile", "title": "Sparse Polynomial Chaos expansions using Variational Relevance Vector\n  Machines", "comments": "Submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2020.109498", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges for non-intrusive methods for Polynomial Chaos modeling lie in\nthe computational efficiency and accuracy under a limited number of model\nsimulations. These challenges can be addressed by enforcing sparsity in the\nseries representation through retaining only the most important basis terms. In\nthis work, we present a novel sparse Bayesian learning technique for obtaining\nsparse Polynomial Chaos expansions which is based on a Relevance Vector Machine\nmodel and is trained using Variational Inference. The methodology shows great\npotential in high-dimensional data-driven settings using relatively few data\npoints and achieves user-controlled sparse levels that are comparable to other\nmethods such as compressive sensing. The proposed approach is illustrated on\ntwo numerical examples, a synthetic response function that is explored for\nvalidation purposes and a low-carbon steel plate with random Young's modulus\nand random loading, which is modeled by stochastic finite element with 38 input\nrandom variables.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:49:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Papaioannou", "Iason", ""], ["Straub", "Daniel", ""], ["Nobile", "Fabio", ""]]}, {"id": "1912.11040", "submitter": "Chanwoo Kim", "authors": "Chanwoo Kim, Sungsoo Kim, Kwangyoun Kim, Mehul Kumar, Jiyeon Kim,\n  Kyungmin Lee, Changwoo Han, Abhinav Garg, Eunhyang Kim, Minkyoo Shin,\n  Shatrughan Singh, Larry Heck, Dhananjaya Gowda", "title": "end-to-end training of a large vocabulary end-to-end speech recognition\n  system", "comments": "Accepted and presented at the ASRU 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end training framework for building\nstate-of-the-art end-to-end speech recognition systems. Our training system\nutilizes a cluster of Central Processing Units(CPUs) and Graphics Processing\nUnits (GPUs). The entire data reading, large scale data augmentation, neural\nnetwork parameter updates are all performed \"on-the-fly\". We use vocal tract\nlength perturbation [1] and an acoustic simulator [2] for data augmentation.\nThe processed features and labels are sent to the GPU cluster. The Horovod\nallreduce approach is employed to train neural network parameters. We evaluated\nthe effectiveness of our system on the standard Librispeech corpus [3] and the\n10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition\nsystem built using this training infrastructure showed a 2.44 % WER on\ntest-clean of the LibriSpeech test set after applying shallow fusion with a\nTransformer language model (LM). For the proprietary English Bixby open domain\ntest set, we obtained a WER of 7.92 % using a Bidirectional Full Attention\n(BFA) end-to-end model after applying shallow fusion with an RNN-LM. When the\nmonotonic chunckwise attention (MoCha) based approach is employed for streaming\nspeech recognition, we obtained a WER of 9.95 % on the same Bixby open domain\ntest set.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 02:59:28 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Kim", "Chanwoo", ""], ["Kim", "Sungsoo", ""], ["Kim", "Kwangyoun", ""], ["Kumar", "Mehul", ""], ["Kim", "Jiyeon", ""], ["Lee", "Kyungmin", ""], ["Han", "Changwoo", ""], ["Garg", "Abhinav", ""], ["Kim", "Eunhyang", ""], ["Shin", "Minkyoo", ""], ["Singh", "Shatrughan", ""], ["Heck", "Larry", ""], ["Gowda", "Dhananjaya", ""]]}, {"id": "1912.11041", "submitter": "Chanwoo Kim", "authors": "Chanwoo Kim, Mehul Kumar, Kwangyoun Kim, and Dhananjaya Gowda", "title": "power-law nonlinearity with maximally uniform distribution criterion for\n  improved neural network training in automatic speech recognition", "comments": "Accepted and presented at the ASRU 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the Maximum Uniformity of Distribution (MUD)\nalgorithm with the power-law nonlinearity. In this approach, we hypothesize\nthat neural network training will become more stable if feature distribution is\nnot too much skewed. We propose two different types of MUD approaches: power\nfunction-based MUD and histogram-based MUD. In these approaches, we first\nobtain the mel filterbank coefficients and apply nonlinearity functions for\neach filterbank channel. With the power function-based MUD, we apply a\npower-function based nonlinearity where power function coefficients are chosen\nto maximize the likelihood assuming that nonlinearity outputs follow the\nuniform distribution. With the histogram-based MUD, the empirical Cumulative\nDensity Function (CDF) from the training database is employed to transform the\noriginal distribution into a uniform distribution. In MUD processing, we do not\nuse any prior knowledge (e.g. logarithmic relation) about the energy of the\nincoming signal and the perceived intensity by a human. Experimental results\nusing an end-to-end speech recognition system demonstrate that power-function\nbased MUD shows better result than the conventional Mel Filterbank Cepstral\nCoefficients (MFCCs). On the LibriSpeech database, we could achieve 4.02 % WER\non test-clean and 13.34 % WER on test-other without using any Language Models\n(LMs). The major contribution of this work is that we developed a new algorithm\nfor designing the compressive nonlinearity in a data-driven way, which is much\nmore flexible than the previous approaches and may be extended to other domains\nas well.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 04:40:40 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Kim", "Chanwoo", ""], ["Kumar", "Mehul", ""], ["Kim", "Kwangyoun", ""], ["Gowda", "Dhananjaya", ""]]}, {"id": "1912.11077", "submitter": "Olivier Delalleau", "authors": "Olivier Delalleau, Maxim Peter, Eloi Alonso, Adrien Logut", "title": "Discrete and Continuous Action Representation for Practical RL in Video\n  Games", "comments": "Presented at the AAAI-20 Workshop on Reinforcement Learning in Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most current research in Reinforcement Learning (RL) focuses on\nimproving the performance of the algorithms in controlled environments, the use\nof RL under constraints like those met in the video game industry is rarely\nstudied. Operating under such constraints, we propose Hybrid SAC, an extension\nof the Soft Actor-Critic algorithm able to handle discrete, continuous and\nparameterized actions in a principled way. We show that Hybrid SAC can\nsuccessfully solve a highspeed driving task in one of our games, and is\ncompetitive with the state-of-the-art on parameterized actions benchmark tasks.\nWe also explore the impact of using normalizing flows to enrich the\nexpressiveness of the policy at minimal computational cost, and identify a\npotential undesired effect of SAC when used with normalizing flows, that may be\naddressed by optimizing a different objective.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:37:13 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Delalleau", "Olivier", ""], ["Peter", "Maxim", ""], ["Alonso", "Eloi", ""], ["Logut", "Adrien", ""]]}, {"id": "1912.11082", "submitter": "Xinsheng Xuan", "authors": "Xinsheng Xuan, Bo Peng, Wei Wang and Jing Dong", "title": "Scalable Fine-grained Generated Image Classification Based on Deep\n  Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generated images could reach very high quality, even human eyes\ncould not tell them apart from real images. Although there are already some\nmethods for detecting generated images in current forensic community, most of\nthese methods are used to detect a single type of generated images. The new\ntypes of generated images are emerging one after another, and the existing\ndetection methods cannot cope well. These problems prompted us to propose a\nscalable framework for multi-class classification based on deep metric\nlearning, which aims to classify the generated images finer. In addition, we\nhave increased the scalability of our framework to cope with the constant\nemergence of new types of generated images, and through fine-tuning to make the\nmodel obtain better detection performance on the new type of generated data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:19:37 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Xuan", "Xinsheng", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""]]}, {"id": "1912.11113", "submitter": "Yuxiang Ren", "authors": "Yuxiang Ren and Hao Zhu and Jiawei Zhang and Peng Dai and Liefeng Bo", "title": "EnsemFDet: An Ensemble Approach to Fraud Detection based on Bipartite\n  Graph", "comments": "Accepted by ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud detection is extremely critical for e-commerce business. It is the\nintent of the companies to detect and prevent fraud as early as possible.\nExisting fraud detection methods try to identify unexpected dense subgraphs and\ntreat related nodes as suspicious. Spectral relaxation-based methods solve the\nproblem efficiently but hurt the performance due to the relaxed constraints.\nBesides, many methods cannot be accelerated with parallel computation or\ncontrol the number of returned suspicious nodes because they provide a set of\nsubgraphs with diverse node sizes. These drawbacks affect the real-world\napplications of existing methods. In this paper, we propose an Ensemble-based\nFraud Detection (EnsemFDet) method to scale up fraud detection in bipartite\ngraphs by decomposing the original problem into subproblems on small-sized\nsubgraphs. By oversampling the graph and solving the subproblems, the ensemble\napproach further votes suspicious nodes without sacrificing the prediction\naccuracy. Extensive experiments have been done on real transaction data from\nJD.com, which is one of the world's largest e-commerce platforms. Experimental\nresults demonstrate the effectiveness, practicability, and scalability of\nEnsemFDet. More specifically, EnsemFDet is up to 100x faster than the\nstate-of-the-art methods due to its parallelism with all aspects of data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:19:41 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:44:28 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 06:29:03 GMT"}, {"version": "v4", "created": "Fri, 6 Nov 2020 00:46:22 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Ren", "Yuxiang", ""], ["Zhu", "Hao", ""], ["Zhang", "Jiawei", ""], ["Dai", "Peng", ""], ["Bo", "Liefeng", ""]]}, {"id": "1912.11119", "submitter": "Zhu Wang", "authors": "Zhu Wang", "title": "MM for Penalized Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized estimation can conduct variable selection and parameter estimation\nsimultaneously. The general framework is to minimize a loss function subject to\na penalty designed to generate sparse variable selection. The\nmajorization-minimization (MM) algorithm is a computational scheme for\nstability and simplicity, and the MM algorithm has been widely applied in\npenalized estimation. Much of the previous work have focused on convex loss\nfunctions such as generalized linear models. When data are contaminated with\noutliers, robust loss functions can generate more reliable estimates. Recent\nliterature has witnessed a growing impact of nonconvex loss-based methods,\nwhich can generate robust estimation for data contaminated with outliers. This\narticle investigates MM algorithm for penalized estimation, provide innovative\noptimality conditions and establish convergence theory with both convex and\nnonconvex loss functions. With respect to applications, we focus on several\nnonconvex loss functions, which were formerly studied in machine learning for\nregression and classification problems. Performance of the proposed algorithms\nare evaluated on simulated and real data including healthcare costs and cancer\nclinical status. Efficient implementations of the algorithms are available in\nthe R package mpath in CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:40:08 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 22:57:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Zhu", ""]]}, {"id": "1912.11123", "submitter": "Ming Zhong", "authors": "Mauro Maggioni, Jason Miller, Ming Zhong", "title": "Data-driven Discovery of Emergent Behaviors in Collective Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS nlin.AO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle- and agent-based systems are a ubiquitous modeling tool in many\ndisciplines. We consider the fundamental problem of inferring interaction\nkernels from observations of agent-based dynamical systems given observations\nof trajectories, in particular for collective dynamical systems exhibiting\nemergent behaviors with complicated interaction kernels, in a nonparametric\nfashion, and for kernels which are parametrized by a single unknown parameter.\nWe extend the estimators introduced in \\cite{PNASLU}, which are based on\nsuitably regularized least squares estimators, to these larger classes of\nsystems. We provide extensive numerical evidence that the estimators provide\nfaithful approximations to the interaction kernels, and provide accurate\npredictions for trajectories started at new initial conditions, both throughout\nthe ``training'' time interval in which the observations were made, and often\nmuch beyond. We demonstrate these features on prototypical systems displaying\ncollective behaviors, ranging from opinion dynamics, flocking dynamics,\nself-propelling particle dynamics, synchronized oscillator dynamics, and a\ngravitational system. Our experiments also suggest that our estimated systems\ncan display the same emergent behaviors of the observed systems, that occur at\nlarger timescales than those used in the training data. Finally, in the case of\nfamilies of systems governed by a parameterized family of interaction kernels,\nwe introduce novel estimators that estimate the parameterized family of\nkernels, splitting it into a common interaction kernel and the action of\nparameters. We demonstrate this in the case of gravity, by learning both the\n``common component'' $1/r^2$ and the dependency on mass, without any a priori\nknowledge of either one, from observations of planetary motions in our solar\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:54:48 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:51:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Maggioni", "Mauro", ""], ["Miller", "Jason", ""], ["Zhong", "Ming", ""]]}, {"id": "1912.11165", "submitter": "S. Mohammad Mirbagheri", "authors": "S. Mohammad Mirbagheri, Howard J. Hamilton", "title": "High Utility Interval-Based Sequences", "comments": "To appear in Proceedings of the 22nd International Conference on Big\n  Data Analytics and Knowledge Discovery (DaWaK2020), Bratislava, Slovakia,\n  September 14-17. Springer, 2020", "journal-ref": "22nd International Conference on Big Data Analytics and Knowledge\n  Discovery (DaWaK2020), Bratislava, Slovakia, September 14-17, 2020. Springer,\n  Cham", "doi": "10.1007/978-3-030-59065-9_9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern mining is an interesting research area with broad range of\napplications. Most prior research on sequential pattern mining has considered\npoint-based data where events occur instantaneously. However, in many\napplication domains, events persist over intervals of time of varying lengths.\nFurthermore, traditional frameworks for sequential pattern mining assume all\nevents have the same weight or utility. This simplifying assumption neglects\nthe opportunity to find informative patterns in terms of utilities, such as\ncost. To address these issues, we incorporate the concept of utility into\ninterval-based sequences and define a framework to mine high utility patterns\nin interval-based sequences i.e., patterns whose utility meets or exceeds a\nminimum threshold. In the proposed framework, the utility of events is\nconsidered while assuming multiple events can occur coincidentally and persist\nover varying periods of time. An algorithm named High Utility Interval-based\nPattern Miner (HUIPMiner) is proposed and applied to real datasets. To achieve\nan efficient solution, HUIPMiner is augmented with a pruning strategy.\nExperimental results show that HUIPMiner is an effective solution to the\nproblem of mining high utility interval-based sequences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 01:19:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 16:45:36 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Mirbagheri", "S. Mohammad", ""], ["Hamilton", "Howard J.", ""]]}, {"id": "1912.11176", "submitter": "Jie Chen", "authors": "Tengfei Ma, Jie Chen", "title": "Unsupervised Learning of Graph Hierarchical Abstractions with\n  Differentiable Coarsening and Optimal Transport", "comments": "AAAI 2021. Code is available at\n  https://github.com/matenure/OTCoarsening", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical abstractions are a methodology for solving large-scale graph\nproblems in various disciplines. Coarsening is one such approach: it generates\na pyramid of graphs whereby the one in the next level is a structural summary\nof the prior one. With a long history in scientific computing, many coarsening\nstrategies were developed based on mathematically driven heuristics. Recently,\nresurgent interests exist in deep learning to design hierarchical methods\nlearnable through differentiable parameterization. These approaches are paired\nwith downstream tasks for supervised learning. In practice, however, supervised\nsignals (e.g., labels) are scarce and are often laborious to obtain. In this\nwork, we propose an unsupervised approach, coined OTCoarsening, with the use of\noptimal transport. Both the coarsening matrix and the transport cost matrix are\nparameterized, so that an optimal coarsening strategy can be learned and\ntailored for a given set of graphs. We demonstrate that the proposed approach\nproduces meaningful coarse graphs and yields competitive performance compared\nwith supervised methods for graph classification and regression.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:40:32 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 20:04:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ma", "Tengfei", ""], ["Chen", "Jie", ""]]}, {"id": "1912.11187", "submitter": "Yan Kang", "authors": "Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian\n  Chen, Mingyi Hong, Qiang Yang", "title": "A Communication Efficient Collaborative Learning Framework for\n  Distributed Features", "comments": "This paper is published at the 2nd International Workshop on\n  Federated Learning for Data Privacy and Confidentiality, in Conjunction with\n  NeurIPS 2019 (FL-NeurIPS 19):\n  https://nips.cc/Conferences/2019/ScheduleMultitrack?event=13202", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a collaborative learning framework allowing multiple parties\nhaving different sets of attributes about the same user to jointly build models\nwithout exposing their raw data or model parameters. In particular, we propose\na Federated Stochastic Block Coordinate Descent (FedBCD) algorithm, in which\neach party conducts multiple local updates before each communication to\neffectively reduce the number of communication rounds among parties, a\nprincipal bottleneck for collaborative learning problems. We analyze\ntheoretically the impact of the number of local updates and show that when the\nbatch size, sample size, and the local iterations are selected appropriately,\nwithin $T$ iterations, the algorithm performs $\\mathcal{O}(\\sqrt{T})$\ncommunication rounds and achieves some $\\mathcal{O}(1/\\sqrt{T})$ accuracy\n(measured by the average of the gradient norm squared). The approach is\nsupported by our empirical evaluations on a variety of tasks and datasets,\ndemonstrating advantages over stochastic gradient descent (SGD) approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:08:55 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 05:53:13 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 05:27:34 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 02:37:02 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2020 05:51:41 GMT"}, {"version": "v6", "created": "Fri, 31 Jul 2020 13:28:34 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Liu", "Yang", ""], ["Kang", "Yan", ""], ["Zhang", "Xinwei", ""], ["Li", "Liping", ""], ["Cheng", "Yong", ""], ["Chen", "Tianjian", ""], ["Hong", "Mingyi", ""], ["Yang", "Qiang", ""]]}, {"id": "1912.11188", "submitter": "Zhao Zhong", "authors": "Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong", "title": "Adversarial AutoAugment", "comments": "ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) has been widely utilized to improve generalization in\ntraining deep neural networks. Recently, human-designed data augmentation has\nbeen gradually replaced by automatically learned augmentation policy. Through\nfinding the best policy in well-designed search space of data augmentation,\nAutoAugment can significantly improve validation accuracy on image\nclassification tasks. However, this approach is not computationally practical\nfor large-scale problems. In this paper, we develop an adversarial method to\narrive at a computationally-affordable solution called Adversarial AutoAugment,\nwhich can simultaneously optimize target related object and augmentation policy\nsearch loss. The augmentation policy network attempts to increase the training\nloss of a target network through generating adversarial augmentation policies,\nwhile the target network can learn more robust features from harder examples to\nimprove the generalization. In contrast to prior work, we reuse the computation\nin target network training for policy evaluation, and dispense with the\nretraining of the target network. Compared to AutoAugment, this leads to about\n12x reduction in computing cost and 11x shortening in time overhead on\nImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100,\nImageNet, and demonstrate significant performance improvements over\nstate-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is\nthe currently best performing single model. On ImageNet, we achieve a leading\nperformance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D\nwithout extra data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:17:17 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhang", "Xinyu", ""], ["Wang", "Qiang", ""], ["Zhang", "Jian", ""], ["Zhong", "Zhao", ""]]}, {"id": "1912.11193", "submitter": "Wanli Shi", "authors": "Wanli Shi, Bin Gu, Xinag Li, Heng Huang", "title": "Quadruply Stochastic Gradient Method for Large Scale Nonlinear\n  Semi-Supervised Ordinal Regression AUC Optimization", "comments": "12 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised ordinal regression (S$^2$OR) problems are ubiquitous in\nreal-world applications, where only a few ordered instances are labeled and\nmassive instances remain unlabeled. Recent researches have shown that directly\noptimizing concordance index or AUC can impose a better ranking on the data\nthan optimizing the traditional error rate in ordinal regression (OR) problems.\nIn this paper, we propose an unbiased objective function for S$^2$OR AUC\noptimization based on ordinal binary decomposition approach. Besides, to handle\nthe large-scale kernelized learning problems, we propose a scalable algorithm\ncalled QS$^3$ORAO using the doubly stochastic gradients (DSG) framework for\nfunctional optimization. Theoretically, we prove that our method can converge\nto the optimal solution at the rate of $O(1/t)$, where $t$ is the number of\niterations for stochastic data sampling. Extensive experimental results on\nvarious benchmark and real-world datasets also demonstrate that our method is\nefficient and effective while retaining similar generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:45:24 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Shi", "Wanli", ""], ["Gu", "Bin", ""], ["Li", "Xinag", ""], ["Huang", "Heng", ""]]}, {"id": "1912.11194", "submitter": "Qi Qi", "authors": "Qi Qi, Yan Yan, Xiaoyu Wang, Tianbao Yang", "title": "A Simple and Effective Framework for Pairwise Deep Metric Learning", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning (DML) has received much attention in deep learning due\nto its wide applications in computer vision. Previous studies have focused on\ndesigning complicated losses and hard example mining methods, which are mostly\nheuristic and lack of theoretical understanding. In this paper, we cast DML as\na simple pairwise binary classification problem that classifies a pair of\nexamples as similar or dissimilar. It identifies the most critical issue in\nthis problem--imbalanced data pairs. To tackle this issue, we propose a simple\nand effective framework to sample pairs in a batch of data for updating the\nmodel. The key to this framework is to define a robust loss for all pairs over\na mini-batch of data, which is formulated by distributionally robust\noptimization. The flexibility in constructing the uncertainty decision set of\nthe dual variable allows us to recover state-of-the-art complicated losses and\nalso to induce novel variants. Empirical studies on several benchmark data sets\ndemonstrate that our simple and effective method outperforms the\nstate-of-the-art results. Codes are available at:\nhttps://github.com/qiqi-helloworld/A-Simple-and-Effective-Framework-for-Pairewise-Distance-Metric-Learning\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:47:25 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 15:58:11 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 15:44:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Qi", "Qi", ""], ["Yan", "Yan", ""], ["Wang", "Xiaoyu", ""], ["Yang", "Tianbao", ""]]}, {"id": "1912.11206", "submitter": "Chenjun Xiao", "authors": "Chenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans and Martin M\\\"uller", "title": "Learning to Combat Compounding-Error in Model-Based Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its potential to improve sample complexity versus model-free\napproaches, model-based reinforcement learning can fail catastrophically if the\nmodel is inaccurate. An algorithm should ideally be able to trust an imperfect\nmodel over a reasonably long planning horizon, and only rely on model-free\nupdates when the model errors get infeasibly large. In this paper, we\ninvestigate techniques for choosing the planning horizon on a state-dependent\nbasis, where a state's planning horizon is determined by the maximum cumulative\nmodel error around that state. We demonstrate that these state-dependent model\nerrors can be learned with Temporal Difference methods, based on a novel\napproach of temporally decomposing the cumulative model errors. Experimental\nresults show that the proposed method can successfully adapt the planning\nhorizon to account for state-dependent model accuracy, significantly improving\nthe efficiency of policy learning compared to model-based and model-free\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 04:51:47 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Xiao", "Chenjun", ""], ["Wu", "Yifan", ""], ["Ma", "Chen", ""], ["Schuurmans", "Dale", ""], ["M\u00fcller", "Martin", ""]]}, {"id": "1912.11209", "submitter": "Vikas Singh", "authors": "Vikas Singh and Nishchal K. Verma", "title": "An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for\n  High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new fuzzy k-means algorithm for the clustering of high\ndimensional data in various subspaces. Since, In the case of high dimensional\ndata, some features might be irrelevant and relevant but may have different\nsignificance in the clustering. For a better clustering, it is crucial to\nincorporate the contribution of these features in the clustering process. To\ncombine these features, in this paper, we have proposed a new fuzzy k-means\nclustering algorithm in which the objective function of the fuzzy k-means is\nmodified using two different entropy term. The first entropy term helps to\nminimize the within-cluster dispersion and maximize the negative entropy to\ndetermine clusters to contribute to the association of data points. The second\nentropy term helps to control the weight of the features because different\nfeatures have different contributing weights in the clustering process for\nobtaining the better partition of the data. The efficacy of the proposed method\nis presented in terms of various clustering measures on multiple datasets and\ncompared with various state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 04:58:47 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Singh", "Vikas", ""], ["Verma", "Nishchal K.", ""]]}, {"id": "1912.11210", "submitter": "Mohamed Baza", "authors": "Mohamed Baza, Andrew Salazar, Mohamed Mahmoud, Mohamed Abdallah, Kemal\n  Akkaya", "title": "On Sharing Models Instead of Data using Mimic learning for Smart Health\n  Applications", "comments": "This paper is accepted in IEEE International Conference on\n  Informatics, IoT, and Enabling Technologies (ICIoT'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) systems contain vast amounts of medical\ninformation about patients. These data can be used to train machine learning\nmodels that can predict health status, as well as to help prevent future\ndiseases or disabilities. However, getting patients' medical data to obtain\nwell-trained machine learning models is a challenging task. This is because\nsharing the patients' medical records is prohibited by law in most countries\ndue to patients privacy concerns. In this paper, we tackle this problem by\nsharing the models instead of the original sensitive data by using the mimic\nlearning approach. The idea is first to train a model on the original sensitive\ndata, called the teacher model. Then, using this model, we can transfer its\nknowledge to another model, called the student model, without the need to learn\nthe original data used in training the teacher model. The student model is then\nshared to the public and can be used to make accurate predictions. To assess\nthe mimic learning approach, we have evaluated our scheme using different\nmedical datasets. The results indicate that the student model mimics the\nteacher model performance in terms of prediction accuracy without the need to\naccess to the patients' original data records.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 05:02:24 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Baza", "Mohamed", ""], ["Salazar", "Andrew", ""], ["Mahmoud", "Mohamed", ""], ["Abdallah", "Mohamed", ""], ["Akkaya", "Kemal", ""]]}, {"id": "1912.11217", "submitter": "Zhou Zhou", "authors": "Zhou Zhai, Bin Gu, Xiang Li, Heng Huang", "title": "Safe Sample Screening for Robust Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust support vector machine (RSVM) has been shown to perform remarkably\nwell to improve the generalization performance of support vector machine under\nthe noisy environment. Unfortunately, in order to handle the non-convexity\ninduced by ramp loss in RSVM, existing RSVM solvers often adopt the DC\nprogramming framework which is computationally inefficient for running multiple\nouter loops. This hinders the application of RSVM to large-scale problems. Safe\nsample screening that allows for the exclusion of training samples prior to or\nearly in the training process is an effective method to greatly reduce\ncomputational time. However, existing safe sample screening algorithms are\nlimited to convex optimization problems while RSVM is a non-convex problem. To\naddress this challenge, in this paper, we propose two safe sample screening\nrules for RSVM based on the framework of concave-convex procedure (CCCP).\nSpecifically, we provide screening rule for the inner solver of CCCP and\nanother rule for propagating screened samples between two successive solvers of\nCCCP. To the best of our knowledge, this is the first work of safe sample\nscreening to a non-convex optimization problem. More importantly, we provide\nthe security guarantee to our sample screening rules to RSVM. Experimental\nresults on a variety of benchmark datasets verify that our safe sample\nscreening rules can significantly reduce the computational time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 05:52:29 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhai", "Zhou", ""], ["Gu", "Bin", ""], ["Li", "Xiang", ""], ["Huang", "Heng", ""]]}, {"id": "1912.11235", "submitter": "Vikas Singh", "authors": "Vikas Singh and Nishchal K. Verma", "title": "Intelligent Condition Based Monitoring Techniques for Bearing Fault\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, intelligent condition-based monitor-ing of rotary machinery\nsystems has become a major researchfocus of machine fault diagnosis. In\ncondition-based monitoring,it is challenging to form a large-scale\nwell-annotated datasetdue to the expense of data acquisition and costly\nannotation.The generated data have a large number of redundant featureswhich\ndegraded the performance of the machine learning models.To overcome this, we\nhave utilized the advantages of minimumredundancy maximum relevance (mRMR) and\ntransfer learningwith a deep learning model. In this work,mRMRis combinedwith\ndeep learning and deep transfer learning framework toimprove the fault\ndiagnostics performance in terms of accuracyand computational complexity.\nThemRMRreduces the redundantinformation from data and increases the deep\nlearning perfor-mance, whereas transfer learning, reduces a large amount of\ndatadependency for training the model. In the proposed work, twoframeworks,\ni.e.,mRMRwith deep learning andmRMRwith deeptransfer learning, have explored\nand validated on CWRU andIMS rolling element bearings datasets. The analysis\nshows thatthe proposed frameworks can obtain better diagnostic accuracycompared\nto existing methods and can handle the data with alarge number of features more\nquickly.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 07:19:06 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 06:17:03 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 18:34:33 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Singh", "Vikas", ""], ["Verma", "Nishchal K.", ""]]}, {"id": "1912.11238", "submitter": "Guoxian Yu", "authors": "Jingzheng Tu and Guoxian Yu and Jun Wang and Carlotta Domeniconi and\n  Xiangliang Zhang", "title": "Attention-Aware Answers of the Crowd", "comments": "This paper was accepted by SDM'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a relatively economic and efficient solution to collect\nannotations from the crowd through online platforms. Answers collected from\nworkers with different expertise may be noisy and unreliable, and the quality\nof annotated data needs to be further maintained. Various solutions have been\nattempted to obtain high-quality annotations. However, they all assume that\nworkers' label quality is stable over time (always at the same level whenever\nthey conduct the tasks). In practice, workers' attention level changes over\ntime, and the ignorance of which can affect the reliability of the annotations.\nIn this paper, we focus on a novel and realistic crowdsourcing scenario\ninvolving attention-aware annotations. We propose a new probabilistic model\nthat takes into account workers' attention to estimate the label quality.\nExpectation propagation is adopted for efficient Bayesian inference of our\nmodel, and a generalized Expectation Maximization algorithm is derived to\nestimate both the ground truth of all tasks and the label-quality of each\nindividual crowd worker with attention. In addition, the number of tasks best\nsuited for a worker is estimated according to changes in attention. Experiments\nagainst related methods on three real-world and one semi-simulated datasets\ndemonstrate that our method quantifies the relationship between workers'\nattention and label-quality on the given tasks, and improves the aggregated\nlabels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 07:34:10 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 08:22:07 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Tu", "Jingzheng", ""], ["Yu", "Guoxian", ""], ["Wang", "Jun", ""], ["Domeniconi", "Carlotta", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1912.11252", "submitter": "Yimin Huang", "authors": "Yimin Huang, Weiran Huang, Liang Li, Zhenguo Li", "title": "Meta-Learning PAC-Bayes Priors in Model Averaging", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays model uncertainty has become one of the most important problems in\nboth academia and industry. In this paper, we mainly consider the scenario in\nwhich we have a common model set used for model averaging instead of selecting\na single final model via a model selection procedure to account for this\nmodel's uncertainty to improve reliability and accuracy of inferences. Here one\nmain challenge is to learn the prior over the model set. To tackle this\nproblem, we propose two data-based algorithms to get proper priors for model\naveraging. One is for meta-learner, the analysts should use historical similar\ntasks to extract the information about the prior. The other one is for\nbase-learner, a subsampling method is used to deal with the data step by step.\nTheoretically, an upper bound of risk for our algorithm is presented to\nguarantee the performance of the worst situation. In practice, both methods\nperform well in simulations and real data studies, especially with poor quality\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 08:55:16 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 03:11:11 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Huang", "Yimin", ""], ["Huang", "Weiran", ""], ["Li", "Liang", ""], ["Li", "Zhenguo", ""]]}, {"id": "1912.11259", "submitter": "Valentino Servizi", "authors": "Valentino Servizi, Francisco C. Pereira, Marie K. Anderson, and Otto\n  A. Nielsen", "title": "Mining User Behaviour from Smartphone data: a literature review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study users' travel behaviour and travel time between origin and\ndestination, researchers employ travel surveys. Although there is consensus in\nthe field about the potential, after over ten years of research and field\nexperimentation, Smartphone-based travel surveys still did not take off to a\nlarge scale. Here, computer intelligence algorithms take the role that\noperators have in Traditional Travel Surveys; since we train each algorithm on\ndata, performances rest on the data quality, thus on the ground truth.\nInaccurate validations affect negatively: labels, algorithms' training, travel\ndiaries precision, and therefore data validation, within a very critical loop.\nInterestingly, boundaries are proven burdensome to push even for Machine\nLearning methods. To support optimal investment decisions for practitioners, we\nexpose the drivers they should consider when assessing what they need against\nwhat they get. This paper highlights and examines the critical aspects of the\nunderlying research and provides some recommendations: (i) from the device\nperspective, on the main physical limitations; (ii) from the application\nperspective, the methodological framework deployed for the automatic generation\nof travel diaries; (iii)from the ground truth perspective, the relationship\nbetween user interaction, methods, and data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 09:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 16:37:19 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Servizi", "Valentino", ""], ["Pereira", "Francisco C.", ""], ["Anderson", "Marie K.", ""], ["Nielsen", "Otto A.", ""]]}, {"id": "1912.11279", "submitter": "Reza Shokri", "authors": "Hongyan Chang, Virat Shejwalkar, Reza Shokri, Amir Houmansadr", "title": "Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box\n  Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative (federated) learning enables multiple parties to train a model\nwithout sharing their private data, but through repeated sharing of the\nparameters of their local models. Despite its advantages, this approach has\nmany known privacy and security weaknesses and performance overhead, in\naddition to being limited only to models with homogeneous architectures. Shared\nparameters leak a significant amount of information about the local (and\nsupposedly private) datasets. Besides, federated learning is severely\nvulnerable to poisoning attacks, where some participants can adversarially\ninfluence the aggregate parameters. Large models, with high dimensional\nparameter vectors, are in particular highly susceptible to privacy and security\nattacks: curse of dimensionality in federated learning. We argue that sharing\nparameters is the most naive way of information exchange in collaborative\nlearning, as they open all the internal state of the model to inference\nattacks, and maximize the model's malleability by stealthy poisoning attacks.\nWe propose Cronus, a robust collaborative machine learning framework. The\nsimple yet effective idea behind designing Cronus is to control, unify, and\nsignificantly reduce the dimensions of the exchanged information between\nparties, through robust knowledge transfer between their black-box local\nmodels. We evaluate all existing federated learning algorithms against\npoisoning attacks, and we show that Cronus is the only secure method, due to\nits tight robustness guarantee. Treating local models as black-box, reduces the\ninformation leakage through models, and enables us using existing\nprivacy-preserving algorithms that mitigate the risk of information leakage\nthrough the model's output (predictions). Cronus also has a significantly lower\nsample complexity, compared to federated learning, which does not bind its\nsecurity to the number of participants.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 10:20:38 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Chang", "Hongyan", ""], ["Shejwalkar", "Virat", ""], ["Shokri", "Reza", ""], ["Houmansadr", "Amir", ""]]}, {"id": "1912.11316", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson,\n  Isabelle Bloch", "title": "TRADI: Tracking deep neural network weight distributions for uncertainty\n  estimation", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During training, the weights of a Deep Neural Network (DNN) are optimized\nfrom a random initialization towards a nearly optimum value minimizing a loss\nfunction. Only this final state of the weights is typically kept for testing,\nwhile the wealth of information on the geometry of the weight space,\naccumulated over the descent towards the minimum is discarded. In this work we\npropose to make use of this knowledge and leverage it for computing the\ndistributions of the weights of the DNN. This can be further used for\nestimating the epistemic uncertainty of the DNN by sampling an ensemble of\nnetworks from these distributions. To this end we introduce a method for\ntracking the trajectory of the weights during optimization, that does not\nrequire any changes in the architecture nor on the training procedure. We\nevaluate our method on standard classification and regression benchmarks, and\non out-of-distribution detection for classification and semantic segmentation.\nWe achieve competitive results, while preserving computational efficiency in\ncomparison to other popular approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:22:45 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 20:21:09 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 13:05:19 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 19:21:28 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 12:27:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Franchi", "Gianni", ""], ["Bursuc", "Andrei", ""], ["Aldea", "Emanuel", ""], ["Dubuisson", "Severine", ""], ["Bloch", "Isabelle", ""]]}, {"id": "1912.11346", "submitter": "Kamorudeen Amuda", "authors": "Kamorudeen A. Amuda, Adesesan B. Adeyemo", "title": "Customers Churn Prediction in Financial Institution Using Artificial\n  Neural Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, a predictive model using Multi-layer Perceptron of Artificial\nNeural Network architecture was developed to predict customer churn in a\nfinancial institution. Previous researches have used supervised machine\nlearning classifiers such as Logistic Regression, Decision Tree, Support Vector\nMachine, K-Nearest Neighbors, and Random Forest. These classifiers require\nhuman effort to perform feature engineering which leads to over-specified and\nincomplete feature selection. Therefore, this research developed a model to\neliminate manual feature engineering in data preprocessing stage. Fifty\nthousand customers? data were extracted from the database of one of the leading\nfinancial institution in Nigeria for the study. The multi-layer perceptron\nmodel was built with python programming language and used two overfitting\ntechniques (Dropout and L2 regularization). The implementation done in python\nwas compared with another model in Neuro solution infinity software. The\nresults showed that the Artificial Neural Network software development (Python)\nhad comparable performance with that obtained from the Neuro Solution Infinity\nsoftware. The accuracy rates are 97.53% and 97.4% while ROC (Receiver Operating\nCharacteristic) curve graphs are 0.89 and 0.85 respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 08:24:29 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Amuda", "Kamorudeen A.", ""], ["Adeyemo", "Adesesan B.", ""]]}, {"id": "1912.11347", "submitter": "Yunpeng Shi", "authors": "Gilad Lerman and Yunpeng Shi", "title": "Robust Group Synchronization via Cycle-Edge Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for solving the group synchronization problem,\nwhere we focus on the setting of adversarial or uniform corruption and\nsufficiently small noise. Specifically, we apply a novel message passing\nprocedure that uses cycle consistency information in order to estimate the\ncorruption levels of group ratios and consequently solve the synchronization\nproblem in our setting. We first explain why the group cycle consistency\ninformation is essential for effectively solving group synchronization\nproblems. We then establish exact recovery and linear convergence guarantees\nfor the proposed message passing procedure under a deterministic setting with\nadversarial corruption. These guarantees hold as long as the ratio of corrupted\ncycles per edge is bounded by a reasonable constant. We also establish the\nstability of the proposed procedure to sub-Gaussian noise. We further establish\nexact recovery with high probability under a common uniform corruption model.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 13:41:00 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 01:38:27 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 20:54:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lerman", "Gilad", ""], ["Shi", "Yunpeng", ""]]}, {"id": "1912.11350", "submitter": "Jing Gao", "authors": "Jing Gao, N. Anantrasirichai and David Bull", "title": "Atmospheric turbulence removal using convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel deep learning-based method for mitigating the\neffects of atmospheric distortion. We have built an end-to-end supervised\nconvolutional neural network (CNN) to reconstruct turbulence-corrupted video\nsequence. Our framework has been developed on the residual learning concept,\nwhere the spatio-temporal distortions are learnt and predicted. Our experiments\ndemonstrate that the proposed method can deblur, remove ripple effect and\nenhance contrast of the video sequences simultaneously. Our model was trained\nand tested with both simulated and real distortions. Experimental results of\nthe real distortions show that our method outperforms the existing ones by up\nto 3.8% in term of the quality of restored images, and it achieves faster speed\nthan the state-of-the-art methods by up to 23 times with GPU implementation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:22:55 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Gao", "Jing", ""], ["Anantrasirichai", "N.", ""], ["Bull", "David", ""]]}, {"id": "1912.11367", "submitter": "Naresh Manwani", "authors": "Rajarshi Bhattacharjee and Naresh Manwani", "title": "Online Algorithms for Multiclass Classification using Partial Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose online algorithms for multiclass classification\nusing partial labels. We propose two variants of Perceptron called Avg\nPerceptron and Max Perceptron to deal with the partial labeled data. We also\npropose Avg Pegasos and Max Pegasos, which are extensions of Pegasos algorithm.\nWe also provide mistake bounds for Avg Perceptron and regret bound for Avg\nPegasos. We show the effectiveness of the proposed approaches by experimenting\non various datasets and comparing them with the standard Perceptron and\nPegasos.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 13:54:38 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Bhattacharjee", "Rajarshi", ""], ["Manwani", "Naresh", ""]]}, {"id": "1912.11368", "submitter": "Badong Chen", "authors": "Yunfei Zheng, Badong Chen, Senior Member, IEEE, Shiyuan Wang, Senior\n  Member, IEEE, and Weiqun Wang, Member, IEEE", "title": "Broad Learning System Based on Maximum Correntropy Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an effective and efficient discriminative learning method, Broad Learning\nSystem (BLS) has received increasing attention due to its outstanding\nperformance in various regression and classification problems. However, the\nstandard BLS is derived under the minimum mean square error (MMSE) criterion,\nwhich is, of course, not always a good choice due to its sensitivity to\noutliers. To enhance the robustness of BLS, we propose in this work to adopt\nthe maximum correntropy criterion (MCC) to train the output weights, obtaining\na correntropy based broad learning system (C-BLS). Thanks to the inherent\nsuperiorities of MCC, the proposed C-BLS is expected to achieve excellent\nrobustness to outliers while maintaining the original performance of the\nstandard BLS in Gaussian or noise-free environment. In addition, three\nalternative incremental learning algorithms, derived from a weighted\nregularized least-squares solution rather than pseudoinverse formula, for C-BLS\nare developed.With the incremental learning algorithms, the system can be\nupdated quickly without the entire retraining process from the beginning, when\nsome new samples arrive or the network deems to be expanded. Experiments on\nvarious regression and classification datasets are reported to demonstrate the\ndesirable performance of the new methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 13:56:55 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zheng", "Yunfei", ""], ["Chen", "Badong", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Wang", "Shiyuan", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Wang", "Weiqun", ""], ["Member", "", ""], ["IEEE", "", ""]]}, {"id": "1912.11398", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu", "title": "An error bound for Lasso and Group Lasso in high dimensions", "comments": "arXiv admin note: text overlap with arXiv:1910.08880", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage recent advances in high-dimensional statistics to derive new L2\nestimation upper bounds for Lasso and Group Lasso in high-dimensions. For\nLasso, our bounds scale as $(k^*/n) \\log(p/k^*)$---$n\\times p$ is the size of\nthe design matrix and $k^*$ the dimension of the ground truth\n$\\boldsymbol{\\beta}^*$---and match the optimal minimax rate. For Group Lasso,\nour bounds scale as $(s^*/n) \\log\\left( G / s^* \\right) + m^* / n$---$G$ is the\ntotal number of groups and $m^*$ the number of coefficients in the $s^*$ groups\nwhich contain $\\boldsymbol{\\beta}^*$---and improve over existing results. We\nadditionally show that when the signal is strongly group-sparse, Group Lasso is\nsuperior to Lasso.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 14:08:26 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:05:03 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Dedieu", "Antoine", ""]]}, {"id": "1912.11420", "submitter": "Hadi Zare", "authors": "Hadi Zare, Mahdi Hajiabadi, Mahdi Jalili", "title": "Detection of Community Structures in Networks with Nodal Features based\n  on Generative Probabilistic Approach", "comments": "12 pages, 13 figures, 6 tables", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2019", "doi": "10.1109/TKDE.2019.2960222", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is considered as a fundamental task in analyzing social\nnetworks. Even though many techniques have been proposed for community\ndetection, most of them are based exclusively on the connectivity structures.\nHowever, there are node features in real networks, such as gender types in\nsocial networks, feeding behavior in ecological networks, and location on\ne-trading networks, that can be further leveraged with the network structure to\nattain more accurate community detection methods. We propose a novel\nprobabilistic graphical model to detect communities by taking into account both\nnetwork structure and nodes' features. The proposed approach learns the\nrelevant features of communities through a generative probabilistic model\nwithout any prior assumption on the communities. Furthermore, the model is\ncapable of determining the strength of node features and structural elements of\nthe networks on shaping the communities. The effectiveness of the proposed\napproach over the state-of-the-art algorithms is revealed on synthetic and\nbenchmark networks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:48:55 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zare", "Hadi", ""], ["Hajiabadi", "Mahdi", ""], ["Jalili", "Mahdi", ""]]}, {"id": "1912.11436", "submitter": "Sivaraman Balakrishnan", "authors": "Larry Wasserman and Aaditya Ramdas and Sivaraman Balakrishnan", "title": "Universal Inference", "comments": "To appear in the Proceedings of the National Academy of Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method for constructing hypothesis tests and confidence\nsets that have finite sample guarantees without regularity conditions. We refer\nto such procedures as \"universal.\" The method is very simple and is based on a\nmodified version of the usual likelihood ratio statistic, that we call \"the\nsplit likelihood ratio test\" (split LRT). The method is especially appealing\nfor irregular statistical models. Canonical examples include mixture models and\nmodels that arise in shape-constrained inference. %mixture models and\nshape-constrained models are just two examples. Constructing tests and\nconfidence sets for such models is notoriously difficult. Typical inference\nmethods, like the likelihood ratio test, are not useful in these cases because\nthey have intractable limiting distributions. In contrast, the method we\nsuggest works for any parametric model and also for some nonparametric models.\nThe split LRT can also be used with profile likelihoods to deal with nuisance\nparameters, and it can also be run sequentially to yield anytime-valid\n$p$-values and confidence sequences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 16:52:08 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 18:48:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 22:38:36 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""], ["Balakrishnan", "Sivaraman", ""]]}, {"id": "1912.11443", "submitter": "Julian G\\\"oltz", "authors": "Julian G\\\"oltz, Laura Kriener, Andreas Baumbach, Sebastian\n  Billaudelle, Oliver Breitwieser, Benjamin Cramer, Dominik Dold, Akos Ferenc\n  Kungl, Walter Senn, Johannes Schemmel, Karlheinz Meier, Mihai Alexandru\n  Petrovici", "title": "Fast and energy-efficient neuromorphic deep learning with first-spike\n  times", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a biological agent operating under environmental pressure, energy\nconsumption and reaction times are of critical importance. Similarly,\nengineered systems are optimized for short time-to-solution and low\nenergy-to-solution characteristics. At the level of neuronal implementation,\nthis implies achieving the desired results with as few and as early spikes as\npossible. With time-to-first-spike coding both of these goals are inherently\nemerging features of learning. Here, we describe a rigorous derivation of a\nlearning rule for such first-spike times in networks of leaky\nintegrate-and-fire neurons, relying solely on input and output spike times, and\nshow how this mechanism can implement error backpropagation in hierarchical\nspiking networks. Furthermore, we emulate our framework on the BrainScaleS-2\nneuromorphic system and demonstrate its capability of harnessing the system's\nspeed and energy characteristics. Finally, we examine how our approach\ngeneralizes to other neuromorphic platforms by studying how its performance is\naffected by typical distortive effects induced by neuromorphic substrates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 17:18:07 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:27:45 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 18:43:48 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 15:35:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["G\u00f6ltz", "Julian", ""], ["Kriener", "Laura", ""], ["Baumbach", "Andreas", ""], ["Billaudelle", "Sebastian", ""], ["Breitwieser", "Oliver", ""], ["Cramer", "Benjamin", ""], ["Dold", "Dominik", ""], ["Kungl", "Akos Ferenc", ""], ["Senn", "Walter", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Petrovici", "Mihai Alexandru", ""]]}, {"id": "1912.11460", "submitter": "Hamid Karimi", "authors": "Hamid Karimi, Tyler Derr, Jiliang Tang", "title": "Characterizing the Decision Boundary of Deep Neural Networks", "comments": "Please contact the first author for any issue or the question\n  regarding this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and in particular, deep neural classifiers have become\nan integral part of many modern applications. Despite their practical success,\nwe still have limited knowledge of how they work and the demand for such an\nunderstanding is evergrowing. In this regard, one crucial aspect of deep neural\nnetwork classifiers that can help us deepen our knowledge about their\ndecision-making behavior is to investigate their decision boundaries.\nNevertheless, this is contingent upon having access to samples populating the\nareas near the decision boundary. To achieve this, we propose a novel approach\nwe call Deep Decision boundary Instance Generation (DeepDIG). DeepDIG utilizes\na method based on adversarial example generation as an effective way of\ngenerating samples near the decision boundary of any deep neural network model.\nThen, we introduce a set of important principled characteristics that take\nadvantage of the generated instances near the decision boundary to provide\nmultifaceted understandings of deep neural networks. We have performed\nextensive experiments on multiple representative datasets across various deep\nneural network models and characterized their decision boundaries. The code is\npublicly available at https://github.com/hamidkarimi/DeepDIG/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:30:11 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 20:55:18 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 16:18:25 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Karimi", "Hamid", ""], ["Derr", "Tyler", ""], ["Tang", "Jiliang", ""]]}, {"id": "1912.11464", "submitter": "Shuhao Fu", "authors": "Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen", "title": "Attack-Resistant Federated Learning with Residual-based Reweighting", "comments": "8 pages, 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has a variety of applications in multiple domains by\nutilizing private training data stored on different devices. However, the\naggregation process in federated learning is highly vulnerable to adversarial\nattacks so that the global model may behave abnormally under attacks. To tackle\nthis challenge, we present a novel aggregation algorithm with residual-based\nreweighting to defend federated learning. Our aggregation algorithm combines\nrepeated median regression with the reweighting scheme in iteratively\nreweighted least squares. Our experiments show that our aggregation algorithm\noutperforms other alternative algorithms in the presence of label-flipping and\nbackdoor attacks. We also provide theoretical analysis for our aggregation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:42:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:51:45 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 19:39:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Fu", "Shuhao", ""], ["Xie", "Chulin", ""], ["Li", "Bo", ""], ["Chen", "Qifeng", ""]]}, {"id": "1912.11475", "submitter": "Amir Ahmad", "authors": "Amir Ahmad and Srikanth Bezawada", "title": "One-Class Classification by Ensembles of Regression models -- a detailed\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classification (OCC) deals with the classification problem in which\nthe training data has data points belonging only to target class. In this\npaper, we study a one-class classification algorithm, One-Class Classification\nby Ensembles of Regression models (OCCER), that uses regression methods to\naddress OCC problems. The OCCER coverts an OCC problem into many regression\nproblems in the original feature space so that each feature of the original\nfeature space is used as the target variable in one of the regression problems.\nOther features are used as the variables on which the dependent variable\ndepends. The errors of regression of a data point by all the regression models\nare used to compute the outlier score of the data point. An extensive\ncomparison of the OCCER algorithm with state-of-the-art OCC algorithms on\nseveral datasets was conducted to show the effectiveness of the this approach.\nWe also demonstrate that the OCCER algorithm can work well with the latent\nfeature space created by autoencoders for image datasets. The implementation of\nOCCER is available at https://github.com/srikanthBezawada/OCCER.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:47:38 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 09:10:53 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 07:25:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ahmad", "Amir", ""], ["Bezawada", "Srikanth", ""]]}, {"id": "1912.11477", "submitter": "Shizhan Lu", "authors": "Shizhan Lu", "title": "Self-adaption grey DBSCAN clustering", "comments": "8 pages, 4 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1906.11416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis, a classical issue in data mining, is widely used in\nvarious research areas. This article aims at proposing a self-adaption grey\nDBSCAN clustering (SAG-DBSCAN) algorithm. First, the grey relational matrix is\nused to obtain the grey local density indicator, and then this indicator is\napplied to make self-adapting noise identification for obtaining a dense subset\nof clustering dataset, finally, the DBSCAN which automatically selects\nparameters is utilized to cluster the dense subset. Several frequently-used\ndatasets were used to demonstrate the performance and effectiveness of the\nproposed clustering algorithm and to compare the results with those of other\nstate-of-the-art algorithms. The comprehensive comparisons indicate that our\nmethod has advantages over other compared methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:46:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lu", "Shizhan", ""]]}, {"id": "1912.11493", "submitter": "Konpat Preechakul", "authors": "Konpat Preechakul, Boonserm Kijsirikul", "title": "CProp: Adaptive Learning Rate Scaling from Past Gradient Conformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most optimizers including stochastic gradient descent (SGD) and its adaptive\ngradient derivatives face the same problem where an effective learning rate\nduring the training is vastly different. A learning rate scheduling, mostly\ntuned by hand, is usually employed in practice. In this paper, we propose\nCProp, a gradient scaling method, which acts as a second-level learning rate\nadapting throughout the training process based on cues from past gradient\nconformity. When the past gradients agree on direction, CProp keeps the\noriginal learning rate. On the contrary, if the gradients do not agree on\ndirection, CProp scales down the gradient proportionally to its uncertainty.\nSince it works by scaling, it could apply to any existing optimizer extending\nits learning rate scheduling capability. We put CProp to a series of tests\nshowing significant gain in training speed on both SGD and adaptive gradient\nmethod like Adam. Codes are available at https://github.com/phizaz/cprop .\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 19:06:53 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Preechakul", "Konpat", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1912.11511", "submitter": "SiQi Zhou", "authors": "SiQi Zhou and Angela P. Schoellig", "title": "An Analysis of the Expressiveness of Deep Neural Network Architectures\n  Based on Their Lipschitz Constants", "comments": "L4DC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have emerged as a popular mathematical tool for\nfunction approximation due to their capability of modelling highly nonlinear\nfunctions. Their applications range from image classification and natural\nlanguage processing to learning-based control. Despite their empirical\nsuccesses, there is still a lack of theoretical understanding of the\nrepresentative power of such deep architectures. In this work, we provide a\ntheoretical analysis of the expressiveness of fully-connected, feedforward DNNs\nwith 1-Lipschitz activation functions. In particular, we characterize the\nexpressiveness of a DNN by its Lipchitz constant. By leveraging random matrix\ntheory, we show that, given sufficiently large and randomly distributed\nweights, the expected upper and lower bounds of the Lipschitz constant of a DNN\nand hence their expressiveness increase exponentially with depth and\npolynomially with width, which gives rise to the benefit of the depth of DNN\narchitectures for efficient function approximation. This observation is\nconsistent with established results based on alternative expressiveness\nmeasures of DNNs. In contrast to most of the existing work, our analysis based\non the Lipschitz properties of DNNs is applicable to a wider range of\nactivation nonlinearities and potentially allows us to make sensible\ncomparisons between the complexity of a DNN and the function to be approximated\nby the DNN. We consider this work to be a step towards understanding the\nexpressive power of DNNs and towards designing appropriate deep architectures\nfor practical applications such as system control.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:00:26 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhou", "SiQi", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1912.11547", "submitter": "Alison Marczewski", "authors": "Alison Marczewski, Adriano Veloso, N\\'ivio Ziviani", "title": "Learning Transferable Features for Speech Emotion Recognition", "comments": "ACM-MM'17, October 23-27, 2017", "journal-ref": "Proceedings of the on Thematic Workshops of ACM Multimedia 2017.\n  ACM, 2017. Pages 529-536", "doi": "10.1145/3126686.3126735", "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition from speech is one of the key steps towards emotional\nintelligence in advanced human-machine interaction. Identifying emotions in\nhuman speech requires learning features that are robust and discriminative\nacross diverse domains that differ in terms of language, spontaneity of speech,\nrecording conditions, and types of emotions. This corresponds to a learning\nscenario in which the joint distributions of features and labels may change\nsubstantially across domains. In this paper, we propose a deep architecture\nthat jointly exploits a convolutional network for extracting domain-shared\nfeatures and a long short-term memory network for classifying emotions using\ndomain-specific features. We use transferable features to enable model\nadaptation from multiple source domains, given the sparseness of speech emotion\ndata and the fact that target domains are short of labeled data. A\ncomprehensive cross-corpora experiment with diverse speech emotion domains\nreveals that transferable features provide gains ranging from 4.3% to 18.4% in\nspeech emotion recognition. We evaluate several domain adaptation approaches,\nand we perform an ablation study to understand which source domains add the\nmost to the overall recognition effectiveness for a given target domain.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:06:08 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Marczewski", "Alison", ""], ["Veloso", "Adriano", ""], ["Ziviani", "N\u00edvio", ""]]}, {"id": "1912.11548", "submitter": "David Craft", "authors": "Marleen Balvert, Georgios Patoulidis, Andrew Patti, Timo M. Deist,\n  Christine Eyler, Bas E. Dutilh, Alexander Sch\\\"onhuth, David Craft", "title": "A Drug Recommendation System (Dr.S) for cancer cell lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalizing drug prescriptions in cancer care based on genomic information\nrequires associating genomic markers with treatment effects. This is an\nunsolved challenge requiring genomic patient data in yet unavailable volumes as\nwell as appropriate quantitative methods. We attempt to solve this challenge\nfor an experimental proxy for which sufficient data is available: 42 drugs\ntested on 1018 cancer cell lines. Our goal is to develop a method to identify\nthe drug that is most promising based on a cell line's genomic information. For\nthis, we need to identify for each drug the machine learning method, choice of\nhyperparameters and genomic features for optimal predictive performance. We\nextensively compare combinations of gene sets (both curated and random),\ngenetic features, and machine learning algorithms for all 42 drugs. For each\ndrug, the best performing combination (considering only the curated gene sets)\nis selected. We use these top model parameters for each drug to build and\ndemonstrate a Drug Recommendation System (Dr.S). Insights resulting from this\nanalysis are formulated as best practices for developing drug recommendation\nsystems. The complete software system, called the Cell Line Analyzer, is\nwritten in Python and available on github.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 21:49:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Balvert", "Marleen", ""], ["Patoulidis", "Georgios", ""], ["Patti", "Andrew", ""], ["Deist", "Timo M.", ""], ["Eyler", "Christine", ""], ["Dutilh", "Bas E.", ""], ["Sch\u00f6nhuth", "Alexander", ""], ["Craft", "David", ""]]}, {"id": "1912.11554", "submitter": "Neeraj Pradhan", "authors": "Du Phan, Neeraj Pradhan, Martin Jankowiak", "title": "Composable Effects for Flexible and Accelerated Probabilistic\n  Programming in NumPyro", "comments": "10 pages, 2 figures; NeurIPS 2019 Program Transformations for Machine\n  Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NumPyro is a lightweight library that provides an alternate NumPy backend to\nthe Pyro probabilistic programming language with the same modeling interface,\nlanguage primitives and effect handling abstractions. Effect handlers allow\nPyro's modeling API to be extended to NumPyro despite its being built atop a\nfundamentally different JAX-based functional backend. In this work, we\ndemonstrate the power of composing Pyro's effect handlers with the program\ntransformations that enable hardware acceleration, automatic differentiation,\nand vectorization in JAX. In particular, NumPyro provides an iterative\nformulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT\ncompiled, yielding an implementation that is much faster than existing\nalternatives in both the small and large dataset regimes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 22:09:36 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Phan", "Du", ""], ["Pradhan", "Neeraj", ""], ["Jankowiak", "Martin", ""]]}, {"id": "1912.11570", "submitter": "Alex Lamb", "authors": "Alex Lamb, Sherjil Ozair, Vikas Verma, David Ha", "title": "SketchTransfer: A Challenging New Task for Exploring Detail-Invariance\n  and the Abstractions Learned by Deep Networks", "comments": "Accepted WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have achieved excellent results in perceptual tasks, yet their\nability to generalize to variations not seen during training has come under\nincreasing scrutiny. In this work we focus on their ability to have invariance\ntowards the presence or absence of details. For example, humans are able to\nwatch cartoons, which are missing many visual details, without being explicitly\ntrained to do so. As another example, 3D rendering software is a relatively\nrecent development, yet people are able to understand such rendered scenes even\nthough they are missing details (consider a film like Toy Story). The failure\nof machine learning algorithms to do this indicates a significant gap in\ngeneralization between human abilities and the abilities of deep networks. We\npropose a dataset that will make it easier to study the detail-invariance\nproblem concretely. We produce a concrete task for this: SketchTransfer, and we\nshow that state-of-the-art domain transfer algorithms still struggle with this\ntask. The state-of-the-art technique which achieves over 95\\% on MNIST\n$\\xrightarrow{}$ SVHN transfer only achieves 59\\% accuracy on the\nSketchTransfer task, which is much better than random (11\\% accuracy) but falls\nshort of the 87\\% accuracy of a classifier trained directly on labeled\nsketches. This indicates that this task is approachable with today's best\nmethods but has substantial room for improvement.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 00:38:47 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lamb", "Alex", ""], ["Ozair", "Sherjil", ""], ["Verma", "Vikas", ""], ["Ha", "David", ""]]}, {"id": "1912.11580", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Wenxi Wang, Kaiyuan Wang, Marko Vasic, Haris Vikalo,\n  Sarfraz Khurshid", "title": "A Study of the Learnability of Relational Properties: Model Counting\n  Meets Machine Learning (MCML)", "comments": null, "journal-ref": null, "doi": "10.1145/3385412.3386015", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the MCML approach for empirically studying the\nlearnability of relational properties that can be expressed in the well-known\nsoftware design language Alloy. A key novelty of MCML is quantification of the\nperformance of and semantic differences among trained machine learning (ML)\nmodels, specifically decision trees, with respect to entire (bounded) input\nspaces, and not just for given training and test datasets (as is the common\npractice). MCML reduces the quantification problems to the classic complexity\ntheory problem of model counting, and employs state-of-the-art model counters.\nThe results show that relatively simple ML models can achieve surprisingly high\nperformance (accuracy and F1-score) when evaluated in the common setting of\nusing training and test datasets - even when the training dataset is much\nsmaller than the test dataset - indicating the seeming simplicity of learning\nrelational properties. However, MCML metrics based on model counting show that\nthe performance can degrade substantially when tested against the entire\n(bounded) input space, indicating the high complexity of precisely learning\nthese properties, and the usefulness of model counting in quantifying the true\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 02:44:13 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 02:14:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Usman", "Muhammad", ""], ["Wang", "Wenxi", ""], ["Wang", "Kaiyuan", ""], ["Vasic", "Marko", ""], ["Vikalo", "Haris", ""], ["Khurshid", "Sarfraz", ""]]}, {"id": "1912.11589", "submitter": "Xin Liu", "authors": "Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, Lifeng Shang", "title": "Neural Subgraph Isomorphism Counting", "comments": "Accepted by KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new graph learning problem: learning to count\nsubgraph isomorphisms. Different from other traditional graph learning problems\nsuch as node classification and link prediction, subgraph isomorphism counting\nis NP-complete and requires more global inference to oversee the whole graph.\nTo make it scalable for large-scale graphs and patterns, we propose a learning\nframework which augments different representation learning architectures and\niteratively attends pattern and target data graphs to memorize subgraph\nisomorphisms for the global counting. We develop both small graphs (<= 1,024\nsubgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms\nin each) sets to evaluate different models. A mutagenic compound dataset,\nMUTAG, is also used to evaluate neural models and demonstrate the success of\ntransfer learning. While the learning based approach is inexact, we are able to\ngeneralize to count large patterns and data graphs in linear time compared to\nthe exponential time of the original NP-complete problem. Experimental results\nshow that learning based subgraph isomorphism counting can speed up the\ntraditional algorithm, VF2, 10-1,000 times with acceptable errors. Domain\nadaptation based on fine-tuning also shows the usefulness of our approach in\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 04:19:40 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:31:33 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 09:23:15 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Liu", "Xin", ""], ["Pan", "Haojie", ""], ["He", "Mutian", ""], ["Song", "Yangqiu", ""], ["Jiang", "Xin", ""], ["Shang", "Lifeng", ""]]}, {"id": "1912.11591", "submitter": "Tomoyuki Obuchi", "authors": "Alia Abbara, Yoshiyuki Kabashima, Tomoyuki Obuchi, Yingying Xu", "title": "Learning performance in inverse Ising problems with sparse teacher\n  couplings", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1742-5468/ab8c3a", "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the learning performance of the pseudolikelihood maximization\nmethod for inverse Ising problems. In the teacher-student scenario under the\nassumption that the teacher's couplings are sparse and the student does not\nknow the graphical structure, the learning curve and order parameters are\nassessed in the typical case using the replica and cavity methods from\nstatistical mechanics. Our formulation is also applicable to a certain class of\ncost functions having locality; the standard likelihood does not belong to that\nclass. The derived analytical formulas indicate that the perfect inference of\nthe presence/absence of the teacher's couplings is possible in the\nthermodynamic limit taking the number of spins $N$ as infinity while keeping\nthe dataset size $M$ proportional to $N$, as long as $\\alpha=M/N > 2$.\nMeanwhile, the formulas also show that the estimated coupling values\ncorresponding to the truly existing ones in the teacher tend to be\noverestimated in the absolute value, manifesting the presence of estimation\nbias. These results are considered to be exact in the thermodynamic limit on\nlocally tree-like networks, such as the regular random or Erd\\H{o}s--R\\'enyi\ngraphs. Numerical simulation results fully support the theoretical predictions.\nAdditional biases in the estimators on loopy graphs are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 04:31:29 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 01:38:14 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Abbara", "Alia", ""], ["Kabashima", "Yoshiyuki", ""], ["Obuchi", "Tomoyuki", ""], ["Xu", "Yingying", ""]]}, {"id": "1912.11597", "submitter": "Shin'ya Yamaguchi", "authors": "Shin'ya Yamaguchi, Sekitoshi Kanai, Takeharu Eda", "title": "Effective Data Augmentation with Multi-Domain Learning GANs", "comments": "AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deep learning applications, the massive data development (e.g.,\ncollecting, labeling), which is an essential process in building practical\napplications, still incurs seriously high costs. In this work, we propose an\neffective data augmentation method based on generative adversarial networks\n(GANs), called Domain Fusion. Our key idea is to import the knowledge contained\nin an outer dataset to a target model by using a multi-domain learning GAN. The\nmulti-domain learning GAN simultaneously learns the outer and target dataset\nand generates new samples for the target tasks. The simultaneous learning\nprocess makes GANs generate the target samples with high fidelity and variety.\nAs a result, we can obtain accurate models for the target tasks by using these\ngenerated samples even if we only have an extremely low volume target dataset.\nWe experimentally evaluate the advantages of Domain Fusion in image\nclassification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor\nScene Recognition. When trained on each target dataset reduced the samples to\n5,000 images, Domain Fusion achieves better classification accuracy than the\ndata augmentation using fine-tuned GANs. Furthermore, we show that Domain\nFusion improves the quality of generated samples, and the improvements can\ncontribute to higher accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 05:39:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yamaguchi", "Shin'ya", ""], ["Kanai", "Sekitoshi", ""], ["Eda", "Takeharu", ""]]}, {"id": "1912.11603", "submitter": "Shin'ya Yamaguchi", "authors": "Shin'ya Yamaguchi, Sekitoshi Kanai, Tetsuya Shioda, Shoichiro Takeda", "title": "Image Enhanced Rotation Prediction for Self-Supervised Learning", "comments": "Accepted to IEEE ICIP 2021. The title has been changed from \"Multiple\n  Pretext-Task for Self-Supervised Learning via Mixing Multiple Image\n  Transformations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rotation prediction (Rotation) is a simple pretext-task for\nself-supervised learning (SSL), where models learn useful representations for\ntarget vision tasks by solving pretext-tasks. Although Rotation captures\ninformation of object shapes, it hardly captures information of textures. To\ntackle this problem, we introduce a novel pretext-task called image enhanced\nrotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and\nanother pretext-task based on image enhancement (e.g., sharpening and\nsolarizing) while maintaining simplicity. Through the simultaneous prediction\nof rotation and image enhancement, models learn representations to capture the\ninformation of not only object shapes but also textures. Our experimental\nresults show that IE-Rot models outperform Rotation on various standard\nbenchmarks including ImageNet classification, PASCAL-VOC detection, and COCO\ndetection/segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:11:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 08:12:54 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yamaguchi", "Shin'ya", ""], ["Kanai", "Sekitoshi", ""], ["Shioda", "Tetsuya", ""], ["Takeda", "Shoichiro", ""]]}, {"id": "1912.11606", "submitter": "Shen Cai", "authors": "Hui Cao, Haikuan Du, Siyu Zhang, and Shen Cai", "title": "InSphereNet: a Concise Representation and Classification Method for 3D\n  Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an InSphereNet method for the problem of 3D object\nclassification. Unlike previous methods that use points, voxels, or multi-view\nimages as inputs of deep neural network (DNN), the proposed method constructs a\nclass of more representative features named infilling spheres from signed\ndistance field (SDF). Because of the admirable spatial representation of\ninfilling spheres, we can not only utilize very fewer number of spheres to\naccomplish classification task, but also design a lightweight InSphereNet with\nless layers and parameters than previous methods. Experiments on ModelNet40\nshow that the proposed method leads to superior performance than PointNet and\nPointNet++ in accuracy. In particular, if there are only a few dozen sphere\ninputs or about 100000 DNN parameters, the accuracy of our method remains at a\nvery high level (over 88%). This further validates the conciseness and\neffectiveness of the proposed InSphere 3D representation. Keywords: 3D object\nclassification , signed distance field , deep learning , infilling sphere\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:26:20 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 01:48:16 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Cao", "Hui", ""], ["Du", "Haikuan", ""], ["Zhang", "Siyu", ""], ["Cai", "Shen", ""]]}, {"id": "1912.11615", "submitter": "Guixiang Ma", "authors": "Guixiang Ma, Nesreen K. Ahmed, Theodore L. Willke, Philip S. Yu", "title": "Deep Graph Similarity Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains where data are represented as graphs, learning a similarity\nmetric among graphs is considered a key problem, which can further facilitate\nvarious learning tasks, such as classification, clustering, and similarity\nsearch. Recently, there has been an increasing interest in deep graph\nsimilarity learning, where the key idea is to learn a deep learning model that\nmaps input graphs to a target space such that the distance in the target space\napproximates the structural distance in the input space. Here, we provide a\ncomprehensive review of the existing literature of deep graph similarity\nlearning. We propose a systematic taxonomy for the methods and applications.\nFinally, we discuss the challenges and future directions for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 08:04:52 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 05:41:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ma", "Guixiang", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Theodore L.", ""], ["Yu", "Philip S.", ""]]}, {"id": "1912.11675", "submitter": "Zengjie Song", "authors": "Zengjie Song, Oluwasanmi Koyejo, Jiangshe Zhang", "title": "Learning Controllable Disentangled Representations with Decorrelation\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial problem in learning disentangled image representations is\ncontrolling the degree of disentanglement during image editing, while\npreserving the identity of objects. In this work, we propose a simple yet\neffective model with the encoder-decoder architecture to address this\nchallenge. To encourage disentanglement, we devise a distance covariance based\ndecorrelation regularization. Further, for the reconstruction step, our model\nleverages a soft target representation combined with the latent image code. By\nexploiting the real-valued space of the soft target representations, we are\nable to synthesize novel images with the designated properties. We also design\na classification based protocol to quantitatively evaluate the disentanglement\nstrength of our model. Experimental results show that the proposed model\ncompetently disentangles factors of variation, and is able to manipulate face\nimages to synthesize the desired attributes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 14:18:08 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Song", "Zengjie", ""], ["Koyejo", "Oluwasanmi", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "1912.11676", "submitter": "Sajjad Mozaffari", "authors": "Sajjad Mozaffari, Omar Y. Al-Jarrah, Mehrdad Dianati, Paul Jennings,\n  and Alexandros Mouzakitis", "title": "Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving\n  Applications: A Review", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2020.3012034", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviour prediction function of an autonomous vehicle predicts the future\nstates of the nearby vehicles based on the current and past observations of the\nsurrounding environment. This helps enhance their awareness of the imminent\nhazards. However, conventional behaviour prediction solutions are applicable in\nsimple driving scenarios that require short prediction horizons. Most recently,\ndeep learning-based approaches have become popular due to their superior\nperformance in more complex environments compared to the conventional\napproaches. Motivated by this increased popularity, we provide a comprehensive\nreview of the state-of-the-art of deep learning-based approaches for vehicle\nbehaviour prediction in this paper. We firstly give an overview of the generic\nproblem of vehicle behaviour prediction and discuss its challenges, followed by\nclassification and review of the most recent deep learning-based solutions\nbased on three criteria: input representation, output type, and prediction\nmethod. The paper also discusses the performance of several well-known\nsolutions, identifies the research gaps in the literature and outlines\npotential new research directions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 14:22:41 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:52:45 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Mozaffari", "Sajjad", ""], ["Al-Jarrah", "Omar Y.", ""], ["Dianati", "Mehrdad", ""], ["Jennings", "Paul", ""], ["Mouzakitis", "Alexandros", ""]]}, {"id": "1912.11713", "submitter": "Jan Gra{\\ss}hoff", "authors": "Jan Gra{\\ss}hoff, Alexandra Jankowski and Philipp Rostalski", "title": "Scalable Gaussian Process Regression for Kernels with a Non-Stationary\n  Phase", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Gaussian processes (GPs) to large data sets is limited due\nto heavy memory and computational requirements. A variety of methods has been\nproposed to enable scalability, one of which is to exploit structure in the\nkernel matrix. Previous methods, however, cannot easily deal with\nnon-stationary processes. This paper presents an efficient GP framework, that\nextends structured kernel interpolation methods to GPs with a non-stationary\nphase. We particularly treat mixtures of non-stationary processes, which are\ncommonly used in the context of separation problems e.g. in biomedical signal\nprocessing. Our approach employs multiple sets of non-equidistant inducing\npoints to account for the non-stationarity and retrieve Toeplitz and Kronecker\nstructure in the kernel matrix allowing for efficient inference. Kernel\nlearning is done by optimizing the marginal likelihood, which can be\napproximated efficiently using stochastic trace estimation methods. Our\napproach is demonstrated on numerical examples and large biomedical datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 20:15:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gra\u00dfhoff", "Jan", ""], ["Jankowski", "Alexandra", ""], ["Rostalski", "Philipp", ""]]}, {"id": "1912.11757", "submitter": "Min Shi Mr.", "authors": "Min Shi, Yufei Tang, Xingquan Zhu and Jianxun Liu", "title": "Multi-Label Graph Convolutional Network Representation Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation of graph-based systems is fundamental across many\ndisciplines. To date, most existing methods for representation learning\nprimarily focus on networks with simplex labels, yet real-world objects (nodes)\nare inherently complex in nature and often contain rich semantics or labels,\ne.g., a user may belong to diverse interest groups of a social network,\nresulting in multi-label networks for many applications. The multi-label\nnetwork nodes not only have multiple labels for each node, such labels are\noften highly correlated making existing methods ineffective or fail to handle\nsuch correlation for node representation learning. In this paper, we propose a\nnovel multi-label graph convolutional network (ML-GCN) for learning node\nrepresentation for multi-label networks. To fully explore label-label\ncorrelation and network topology structures, we propose to model a multi-label\nnetwork as two Siamese GCNs: a node-node-label graph and a label-label-node\ngraph. The two GCNs each handle one aspect of representation learning for nodes\nand labels, respectively, and they are seamlessly integrated under one\nobjective function. The learned label representations can effectively preserve\nthe inner-label interaction and node label properties, and are then aggregated\nto enhance the node representation learning under a unified training framework.\nExperiments and comparisons on multi-label node classification validate the\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 02:52:47 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Shi", "Min", ""], ["Tang", "Yufei", ""], ["Zhu", "Xingquan", ""], ["Liu", "Jianxun", ""]]}, {"id": "1912.11760", "submitter": "Longfei Li", "authors": "Longfei Li, Ziqi Liu, Chaochao Chen, Ya-Lin Zhang, Jun Zhou, Xiaolong\n  Li", "title": "A Time Attention based Fraud Transaction Detection Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With online payment platforms being ubiquitous and important, fraud\ntransaction detection has become the key for such platforms, to ensure user\naccount safety and platform security. In this work, we present a novel method\nfor detecting fraud transactions by leveraging patterns from both users' static\nprofiles and users' dynamic behaviors in a unified framework. To address and\nexplore the information of users' behaviors in continuous time spaces, we\npropose to use \\emph{time attention based recurrent layers} to embed the\ndetailed information of the time interval, such as the durations of specific\nactions, time differences between different actions and sequential behavior\npatterns,etc., in the same latent space. We further combine the learned\nembeddings and users' static profiles altogether in a unified framework.\nExtensive experiments validate the effectiveness of our proposed methods over\nstate-of-the-art methods on various evaluation metrics, especially on\n\\emph{recall at top percent} which is an important metric for measuring the\nbalance between service experiences and risk of potential losses.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 03:09:43 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 11:13:52 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Li", "Longfei", ""], ["Liu", "Ziqi", ""], ["Chen", "Chaochao", ""], ["Zhang", "Ya-Lin", ""], ["Zhou", "Jun", ""], ["Li", "Xiaolong", ""]]}, {"id": "1912.11762", "submitter": "Rory Bunker", "authors": "Rory Bunker (1), Teo Susnjak (2) ((1) Nagoya Institute of Technology,\n  Japan, (2) Massey University, Auckland, New Zealand)", "title": "The Application of Machine Learning Techniques for Predicting Results in\n  Team Sport: A Review", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, Machine Learning (ML) techniques have been\nincreasingly utilized for the purpose of predicting outcomes in sport. In this\npaper, we provide a review of studies that have used ML for predicting results\nin team sport, covering studies from 1996 to 2019. We sought to answer five key\nresearch questions while extensively surveying papers in this field. This paper\noffers insights into which ML algorithms have tended to be used in this field,\nas well as those that are beginning to emerge with successful outcomes. Our\nresearch highlights defining characteristics of successful studies and\nidentifies robust strategies for evaluating accuracy results in this\napplication domain. Our study considers accuracies that have been achieved\nacross different sports and explores the notion that outcomes of some team\nsports could be inherently more difficult to predict than others. Finally, our\nstudy uncovers common themes of future research directions across all surveyed\npapers, looking for gaps and opportunities, while proposing recommendations for\nfuture researchers in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 03:12:21 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Bunker", "Rory", ""], ["Susnjak", "Teo", ""]]}, {"id": "1912.11785", "submitter": "Zhao Zhang", "authors": "Jiahuan Ren, Zhao Zhang, Sheng Li, Yang Wang, Guangcan Liu, Shuicheng\n  Yan, Meng Wang", "title": "Learning Hybrid Representation by Robust Dictionary Learning in\n  Factorized Compressed Space", "comments": "Accepted by IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the robust dictionary learning (DL) to discover\nthe hybrid salient low-rank and sparse representation in a factorized\ncompressed space. A Joint Robust Factorization and Projective Dictionary\nLearning (J-RFDL) model is presented. The setting of J-RFDL aims at improving\nthe data representations by enhancing the robustness to outliers and noise in\ndata, encoding the reconstruction error more accurately and obtaining hybrid\nsalient coefficients with accurate reconstruction ability. Specifically, J-RFDL\nperforms the robust representation by DL in a factorized compressed space to\neliminate the negative effects of noise and outliers on the results, which can\nalso make the DL process efficient. To make the encoding process robust to\nnoise in data, J-RFDL clearly uses sparse L2, 1-norm that can potentially\nminimize the factorization and reconstruction errors jointly by forcing rows of\nthe reconstruction errors to be zeros. To deliver salient coefficients with\ngood structures to reconstruct given data well, J-RFDL imposes the joint\nlow-rank and sparse constraints on the embedded coefficients with a synthesis\ndictionary. Based on the hybrid salient coefficients, we also extend J-RFDL for\nthe joint classification and propose a discriminative J-RFDL model, which can\nimprove the discriminating abilities of learnt coeffi-cients by minimizing the\nclassification error jointly. Extensive experiments on public datasets\ndemonstrate that our formulations can deliver superior performance over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 06:52:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ren", "Jiahuan", ""], ["Zhang", "Zhao", ""], ["Li", "Sheng", ""], ["Wang", "Yang", ""], ["Liu", "Guangcan", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.11801", "submitter": "Georgios Papayiannis", "authors": "G. Domazakis, D. Drivaliaris, S. Koukoulas, G. Papayiannis, A.\n  Tsekrekos, A. Yannacopoulos", "title": "Clustering measure-valued data with Wasserstein barycenters", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, learning schemes for measure-valued data are proposed, i.e.\ndata that their structure can be more efficiently represented as probability\nmeasures instead of points on $\\R^d$, employing the concept of probability\nbarycenters as defined with respect to the Wasserstein metric. Such type of\nlearning approaches are highly appreciated in many fields where the\nobservational/experimental error is significant (e.g. astronomy, biology,\nremote sensing, etc.) or the data nature is more complex and the traditional\nlearning algorithms are not applicable or effective to treat them (e.g. network\ndata, interval data, high frequency records, matrix data, etc.). Under this\nperspective, each observation is identified by an appropriate probability\nmeasure and the proposed statistical learning schemes rely on discrimination\ncriteria that utilize the geometric structure of the space of probability\nmeasures through core techniques from the optimal transport theory. The\ndiscussed approaches are implemented in two real world applications: (a)\nclustering eurozone countries according to their observed government bond yield\ncurves and (b) classifying the areas of a satellite image to certain land uses\ncategories which is a standard task in remote sensing. In both case studies the\nresults are particularly interesting and meaningful while the accuracy obtained\nis high.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:46:00 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 10:04:04 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Domazakis", "G.", ""], ["Drivaliaris", "D.", ""], ["Koukoulas", "S.", ""], ["Papayiannis", "G.", ""], ["Tsekrekos", "A.", ""], ["Yannacopoulos", "A.", ""]]}, {"id": "1912.11809", "submitter": "Jiaxin Chen", "authors": "Jiaxin Chen, Li-Ming Zhan, Xiao-Ming Wu, Fu-lai Chung", "title": "Variational Metric Scaling for Metric-Based Meta-Learning", "comments": "AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric-based meta-learning has attracted a lot of attention due to its\neffectiveness and efficiency in few-shot learning. Recent studies show that\nmetric scaling plays a crucial role in the performance of metric-based\nmeta-learning algorithms. However, there still lacks a principled method for\nlearning the metric scaling parameter automatically. In this paper, we recast\nmetric-based meta-learning from a Bayesian perspective and develop a\nvariational metric scaling framework for learning a proper metric scaling\nparameter. Firstly, we propose a stochastic variational method to learn a\nsingle global scaling parameter. To better fit the embedding space to a given\ndata distribution, we extend our method to learn a dimensional scaling vector\nto transform the embedding space. Furthermore, to learn task-specific\nembeddings, we generate task-dependent dimensional scaling vectors with\namortized variational inference. Our method is end-to-end without any\npre-training and can be used as a simple plug-and-play module for existing\nmetric-based meta-algorithms. Experiments on mini-ImageNet show that our\nmethods can be used to consistently improve the performance of existing\nmetric-based meta-algorithms including prototypical networks and TADAM. The\nsource code can be downloaded from\nhttps://github.com/jiaxinchen666/variational-scaling.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 09:00:36 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 10:07:54 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Chen", "Jiaxin", ""], ["Zhan", "Li-Ming", ""], ["Wu", "Xiao-Ming", ""], ["Chung", "Fu-lai", ""]]}, {"id": "1912.11852", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao,\n  Jun Zhu", "title": "Benchmarking Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which becomes\none of the most important research problems in the development of deep\nlearning. While a lot of efforts have been made in recent years, it is of great\nsignificance to perform correct and complete evaluations of the adversarial\nattack and defense algorithms. In this paper, we establish a comprehensive,\nrigorous, and coherent benchmark to evaluate adversarial robustness on image\nclassification tasks. After briefly reviewing plenty of representative attack\nand defense methods, we perform large-scale experiments with two robustness\ncurves as the fair-minded evaluation criteria to fully understand the\nperformance of these methods. Based on the evaluation results, we draw several\nimportant findings and provide insights for future research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:37:01 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Dong", "Yinpeng", ""], ["Fu", "Qi-An", ""], ["Yang", "Xiao", ""], ["Pang", "Tianyu", ""], ["Su", "Hang", ""], ["Xiao", "Zihao", ""], ["Zhu", "Jun", ""]]}, {"id": "1912.11853", "submitter": "Yosuke Shinya", "authors": "Laurent Dillard, Yosuke Shinya, Taiji Suzuki", "title": "Domain Adaptation Regularization for Spectral Pruning", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of computer vision related tasks. However, their\ncomputational cost limits their ability to be implemented in embedded systems\nwith restricted resources or strict latency constraints. Model compression has\ntherefore been an active field of research to overcome this issue.\nAdditionally, DNNs typically require massive amounts of labeled data to be\ntrained. This represents a second limitation to their deployment. Domain\nAdaptation (DA) addresses this issue by allowing knowledge learned on one\nlabeled source distribution to be transferred to a target distribution,\npossibly unlabeled. In this paper, we investigate on possible improvements of\ncompression methods in DA setting. We focus on a compression method that was\npreviously developed in the context of a single data distribution and show\nthat, with a careful choice of data to use during compression and additional\nregularization terms directly related to DA objectives, it is possible to\nimprove compression results. We also show that our method outperforms an\nexisting compression method studied in the DA setting by a large margin for\nhigh compression rates. Although our work is based on one specific compression\nmethod, we also outline some general guidelines for improving compression in DA\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:38:13 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 12:27:50 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 09:08:08 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dillard", "Laurent", ""], ["Shinya", "Yosuke", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1912.11856", "submitter": "Issam Hammad", "authors": "Issam Hammad, Kamal El-Sankary, and Jason Gu", "title": "A Comparative Study on Machine Learning Algorithms for the Control of a\n  Wall Following Robot", "comments": "Accepted and presented at IEEE International Conference on Robotics\n  and Biomimetics (ROBIO) -2019", "journal-ref": "IEEE International Conference on Robotics and Biomimetics (ROBIO)\n  2019", "doi": "10.1109/ROBIO49542.2019.8961836", "report-no": null, "categories": "cs.LG cs.CV cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comparison of the performance of various machine learning models to predict\nthe direction of a wall following robot is presented in this paper. The models\nwere trained using an open-source dataset that contains 24 ultrasound sensors\nreadings and the corresponding direction for each sample. This dataset was\ncaptured using SCITOS G5 mobile robot by placing the sensors on the robot\nwaist. In addition to the full format with 24 sensors per record, the dataset\nhas two simplified formats with 4 and 2 input sensor readings per record.\nSeveral control models were proposed previously for this dataset using all\nthree dataset formats. In this paper, two primary research contributions are\npresented. First, presenting machine learning models with accuracies higher\nthan all previously proposed models for this dataset using all three formats. A\nperfect solution for the 4 and 2 inputs sensors formats is presented using\nDecision Tree Classifier by achieving a mean accuracy of 100%. On the other\nhand, a mean accuracy of 99.82% was achieves using the 24 sensor inputs by\nemploying the Gradient Boost Classifier. Second, presenting a comparative study\non the performance of different machine learning and deep learning algorithms\non this dataset. Therefore, providing an overall insight on the performance of\nthese algorithms for similar sensor fusion problems. All the models in this\npaper were evaluated using Monte-Carlo cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:05:05 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 13:27:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hammad", "Issam", ""], ["El-Sankary", "Kamal", ""], ["Gu", "Jason", ""]]}, {"id": "1912.11896", "submitter": "Aly El Gamal", "authors": "Xingchen Wang, Shengtai Ju, Xiwen Zhang, Sharan Ramjee, Aly El Gamal", "title": "Efficient Training of Deep Classifiers for Wireless Source\n  Identification using Test SNR Estimates", "comments": "5 pages, 10 figures, 4 tables, accepted at IEEE Wireless\n  Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient deep learning training algorithms that process received\nwireless signals, if a test Signal to Noise Ratio (SNR) estimate is available.\nWe focus on two tasks that facilitate source identification: 1- Identifying the\nmodulation type, 2- Identifying the wireless technology and channel in the 2.4\nGHz ISM band. For benchmarking, we rely on recent literature on testing deep\nlearning algorithms against two well-known datasets. We first demonstrate that\nusing training data corresponding only to the test SNR value leads to dramatic\nreductions in training time while incurring a small loss in average test\naccuracy, as it improves the accuracy for low SNR values. Further, we show that\nan erroneous test SNR estimate with a small positive offset is better for\ntraining than another having the same error magnitude with a negative offset.\nSecondly, we introduce a greedy training SNR Boosting algorithm that leads to\nuniform improvement in accuracy across all tested SNR values, while using a\nsmall subset of training SNR values at each test SNR. Finally, we demonstrate\nthe potential of bootstrap aggregating (Bagging) based on training SNR values\nto improve generalization at low test SNR values with scarcity of training\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 16:49:56 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 01:56:08 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wang", "Xingchen", ""], ["Ju", "Shengtai", ""], ["Zhang", "Xiwen", ""], ["Ramjee", "Sharan", ""], ["Gamal", "Aly El", ""]]}, {"id": "1912.11912", "submitter": "Devesh Jha", "authors": "Devesh Jha, Arvind Raghunathan, Diego Romeres", "title": "Quasi-Newton Trust Region Policy Optimization", "comments": "3rd Conference on Robot Learning (CoRL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a trust region method for policy optimization that employs\nQuasi-Newton approximation for the Hessian, called Quasi-Newton Trust Region\nPolicy Optimization QNTRPO. Gradient descent is the de facto algorithm for\nreinforcement learning tasks with continuous controls. The algorithm has\nachieved state-of-the-art performance when used in reinforcement learning\nacross a wide range of tasks. However, the algorithm suffers from a number of\ndrawbacks including: lack of stepsize selection criterion, and slow\nconvergence. We investigate the use of a trust region method using dogleg step\nand a Quasi-Newton approximation for the Hessian for policy optimization. We\ndemonstrate through numerical experiments over a wide range of challenging\ncontinuous control tasks that our particular choice is efficient in terms of\nnumber of samples and improves performance\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 18:29:38 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Jha", "Devesh", ""], ["Raghunathan", "Arvind", ""], ["Romeres", "Diego", ""]]}, {"id": "1912.11914", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Inverses of Matern Covariances on Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a study of the aliased spectral densities of Mat\\'ern covariance\nfunctions on a regular grid of points, providing clarity on the properties of a\npopular approximation based on stochastic partial differential equations; while\nothers have shown that it can approximate the covariance function well, we find\nthat it assigns too much power at high frequencies and does not provide\nincreasingly accurate approximations to the inverse as the grid spacing goes to\nzero, except in the one-dimensional exponential covariance case. We provide\nnumerical results to support our theory, and in a simulation study, we\ninvestigate the implications for parameter estimation, finding that the SPDE\napproximation tends to overestimate spatial range parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 18:36:06 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 14:44:05 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 19:22:37 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1912.11926", "submitter": "Art\\\"ur Manukyan", "authors": "Art\\\"ur Manukyan and Elvan Ceyhan", "title": "Parameter Free Clustering with Cluster Catch Digraphs (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose clustering algorithms based on a recently developed geometric\ndigraph family called cluster catch digraphs (CCDs). These digraphs are used to\ndevise clustering methods that are hybrids of density-based and graph-based\nclustering methods. CCDs are appealing digraphs for clustering, since they\nestimate the number of clusters; however, CCDs (and density-based methods in\ngeneral) require some information on a parameter representing the\n\\emph{intensity} of assumed clusters in the data set. We propose algorithms\nthat are parameter free versions of the CCD algorithm and does not require a\nspecification of the intensity parameter whose choice is often critical in\nfinding an optimal partitioning of the data set. We estimate the number of\nconvex clusters by borrowing a tool from spatial data analysis, namely Ripley's\n$K$ function. We call our new digraphs utilizing the $K$ function as RK-CCDs.\nWe show that the minimum dominating sets of RK-CCDs estimate and distinguish\nthe clusters from noise clusters in a data set, and hence allow the estimation\nof the correct number of clusters. Our robust clustering algorithms are\ncomprised of methods that estimate both the number of clusters and the\nintensity parameter, making them completely parameter free. We conduct Monte\nCarlo simulations and use real life data sets to compare RK-CCDs with some\ncommonly used density-based and prototype-based clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:30:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Manukyan", "Art\u00fcr", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1912.11928", "submitter": "Subha Maity", "authors": "Subha Maity, Yuekai Sun, and Moulinath Banerjee", "title": "Communication-Efficient Integrative Regression in High-Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of meta-analysis in high-dimensional settings in which\nthe data sources we wish to integrate are similar but non-identical. To borrow\nstrength across such heterogeneous data sources, we introduce a global\nparameter that addresses several identification issues. We also propose a\none-shot estimator of the global parameter that preserves the anonymity of the\ndata sources and converges at a rate that depends on the size of the combined\ndataset. Finally, we demonstrate the benefits of our approach on a large-scale\ndrug treatment dataset involving several different cancer cell lines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:30:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Maity", "Subha", ""], ["Sun", "Yuekai", ""], ["Banerjee", "Moulinath", ""]]}, {"id": "1912.11939", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani, Michael Field", "title": "On the Principle of Least Symmetry Breaking in Shallow ReLU Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization problem associated with fitting two-layer ReLU\nnetworks with respect to the squared loss, where labels are assumed to be\ngenerated by a target network. Focusing first on standard Gaussian inputs, we\nshow that the structure of spurious local minima detected by stochastic\ngradient descent (SGD) is, in a well-defined sense, the \\emph{least loss of\nsymmetry} with respect to the target weights. A closer look at the analysis\nindicates that this principle of least symmetry breaking may apply to a broader\nrange of settings. Motivated by this, we conduct a series of experiments which\ncorroborate this hypothesis for different classes of non-isotropic non-product\ndistributions, smooth activation functions and networks with a few layers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 22:04:41 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 16:53:26 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Arjevani", "Yossi", ""], ["Field", "Michael", ""]]}, {"id": "1912.11959", "submitter": "Thomas Dowdell BCom(Hons)", "authors": "Thomas Dowdell and Hongyu Zhang", "title": "Is Attention All What You Need? -- An Empirical Investigation on\n  Convolution-Based Active Memory and Self-Attention", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The key to a Transformer model is the self-attention mechanism, which allows\nthe model to analyze an entire sequence in a computationally efficient manner.\nRecent work has suggested the possibility that general attention mechanisms\nused by RNNs could be replaced by active-memory mechanisms. In this work, we\nevaluate whether various active-memory mechanisms could replace self-attention\nin a Transformer. Our experiments suggest that active-memory alone achieves\ncomparable results to the self-attention mechanism for language modelling, but\noptimal results are mostly achieved by using both active-memory and\nself-attention mechanisms together. We also note that, for some specific\nalgorithmic tasks, active-memory mechanisms alone outperform both\nself-attention and a combination of the two.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:01:13 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 09:01:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dowdell", "Thomas", ""], ["Zhang", "Hongyu", ""]]}, {"id": "1912.11960", "submitter": "Sravanti Addepalli", "authors": "Sravanti Addepalli, Gaurav Kumar Nayak, Anirban Chakraborty, R.\n  Venkatesh Babu", "title": "DeGAN : Data-Enriching GAN for Retrieving Representative Samples from a\n  Trained Classifier", "comments": "Accepted at AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of digital information explosion, an abundance of data from\nnumerous modalities is being generated as well as archived everyday. However,\nmost problems associated with training Deep Neural Networks still revolve\naround lack of data that is rich enough for a given task. Data is required not\nonly for training an initial model, but also for future learning tasks such as\nModel Compression and Incremental Learning. A diverse dataset may be used for\ntraining an initial model, but it may not be feasible to store it throughout\nthe product life cycle due to data privacy issues or memory constraints. We\npropose to bridge the gap between the abundance of available data and lack of\nrelevant data, for the future learning tasks of a given trained network. We use\nthe available data, that may be an imbalanced subset of the original training\ndataset, or a related domain dataset, to retrieve representative samples from a\ntrained classifier, using a novel Data-enriching GAN (DeGAN) framework. We\ndemonstrate that data from a related domain can be leveraged to achieve\nstate-of-the-art performance for the tasks of Data-free Knowledge Distillation\nand Incremental Learning on benchmark datasets. We further demonstrate that our\nproposed framework can enrich any data, even from unrelated domains, to make it\nmore useful for the future learning tasks of a given network.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:05:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Addepalli", "Sravanti", ""], ["Nayak", "Gaurav Kumar", ""], ["Chakraborty", "Anirban", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1912.11970", "submitter": "Natalia Arzeno", "authors": "Natalia M. Arzeno, Haris Vikalo", "title": "Evolutionary Clustering via Message Passing", "comments": "To be published in IEEE Transactions on Knowledge and Data\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are often interested in clustering objects that evolve over time and\nidentifying solutions to the clustering problem for every time step.\nEvolutionary clustering provides insight into cluster evolution and temporal\nchanges in cluster memberships while enabling performance superior to that\nachieved by independently clustering data collected at different time points.\nIn this paper we introduce evolutionary affinity propagation (EAP), an\nevolutionary clustering algorithm that groups data points by exchanging\nmessages on a factor graph. EAP promotes temporal smoothness of the solution to\nclustering time-evolving data by linking the nodes of the factor graph that are\nassociated with adjacent data snapshots, and introduces consensus nodes to\nenable cluster tracking and identification of cluster births and deaths. Unlike\nexisting evolutionary clustering methods that require additional processing to\napproximate the number of clusters or match them across time, EAP determines\nthe number of clusters and tracks them automatically. A comparison with\nexisting methods on simulated and experimental data demonstrates effectiveness\nof the proposed EAP algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:09:16 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Arzeno", "Natalia M.", ""], ["Vikalo", "Haris", ""]]}, {"id": "1912.11982", "submitter": "Weibo Shu", "authors": "Weibo Shu, Yaqiang Yao, Shengfei Lyu, Jinlong Li, and Huanhuan Chen", "title": "Use Short Isometric Shapelets to Accelerate Binary Time Series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the research area of time series classification, the ensemble shapelet\ntransform algorithm is one of state-of-the-art algorithms for classification.\nHowever, its high time complexity is an issue to hinder its application since\nits base classifier shapelet transform includes a high time complexity of a\ndistance calculation and shapelet selection. Therefore, in this paper we\nintroduce a novel algorithm, i.e. short isometric shapelet transform, which\ncontains two strategies to reduce the time complexity. The first strategy of\nSIST fixes the length of shapelet based on a simplified distance calculation,\nwhich largely reduces the number of shapelet candidates as well as speeds up\nthe distance calculation in the ensemble shapelet transform algorithm. The\nsecond strategy is to train a single linear classifier in the feature space\ninstead of an ensemble classifier. The theoretical evidences of these two\nstrategies are presented to guarantee a near-lossless accuracy under some\npreconditions while reducing the time complexity. Furthermore, empirical\nexperiments demonstrate the superior performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 04:33:11 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 09:44:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shu", "Weibo", ""], ["Yao", "Yaqiang", ""], ["Lyu", "Shengfei", ""], ["Li", "Jinlong", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1912.12008", "submitter": "Haishan Ye", "authors": "Haishan Ye, Shusen Wang, Zhihua Zhang, Tong Zhang", "title": "Fast Generalized Matrix Regression with Applications in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fast matrix algorithms have become the fundamental tools of machine learning\nin big data era.\n  The generalized matrix regression problem is widely used in the matrix\napproximation such as CUR decomposition, kernel matrix approximation, and\nstream singular value decomposition (SVD), etc.\n  In this paper, we propose a fast generalized matrix regression algorithm\n(Fast GMR) which utilizes sketching technique to solve the GMR problem\nefficiently.\n  Given error parameter $0<\\epsilon<1$, the Fast GMR algorithm can achieve a\n$(1+\\epsilon)$ relative error with the sketching sizes being of order\n$\\cO(\\epsilon^{-1/2})$ for a large group of GMR problems.\n  We apply the Fast GMR algorithm to the symmetric positive definite matrix\napproximation and single pass singular value decomposition and they achieve a\nbetter performance than conventional algorithms.\n  Our empirical study also validates the effectiveness and efficiency of our\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 07:03:37 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ye", "Haishan", ""], ["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""], ["Zhang", "Tong", ""]]}, {"id": "1912.12012", "submitter": "Teng Guo", "authors": "Teng Guo, Feng Xia, Shihao Zhen, Xiaomei Bai, Dongyu Zhang, Zitao Liu,\n  Jiliang Tang", "title": "Graduate Employment Prediction with Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The failure of landing a job for college students could cause serious social\nconsequences such as drunkenness and suicide. In addition to academic\nperformance, unconscious biases can become one key obstacle for hunting jobs\nfor graduating students. Thus, it is necessary to understand these unconscious\nbiases so that we can help these students at an early stage with more\npersonalized intervention. In this paper, we develop a framework, i.e., MAYA\n(Multi-mAjor emploYment stAtus) to predict students' employment status while\nconsidering biases. The framework consists of four major components. Firstly,\nwe solve the heterogeneity of student courses by embedding academic performance\ninto a unified space. Then, we apply a generative adversarial network (GAN) to\novercome the class imbalance problem. Thirdly, we adopt Long Short-Term Memory\n(LSTM) with a novel dropout mechanism to comprehensively capture sequential\ninformation among semesters. Finally, we design a bias-based regularization to\ncapture the job market biases. We conduct extensive experiments on a\nlarge-scale educational dataset and the results demonstrate the effectiveness\nof our prediction framework.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 07:30:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Guo", "Teng", ""], ["Xia", "Feng", ""], ["Zhen", "Shihao", ""], ["Bai", "Xiaomei", ""], ["Zhang", "Dongyu", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "1912.12016", "submitter": "Jun Wang", "authors": "Jun Wang, Hefu Zhang, Qi Liu, Zhen Pan, Hanqing Tao", "title": "Crowdfunding Dynamics Tracking: A Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the increasing interests in research of\ncrowdfunding mechanism. In this area, dynamics tracking is a significant issue\nbut is still under exploration. Existing studies either fit the fluctuations of\ntime-series or employ regularization terms to constrain learned tendencies.\nHowever, few of them take into account the inherent decision-making process\nbetween investors and crowdfunding dynamics. To address the problem, in this\npaper, we propose a Trajectory-based Continuous Control for Crowdfunding (TC3)\nalgorithm to predict the funding progress in crowdfunding. Specifically,\nactor-critic frameworks are employed to model the relationship between\ninvestors and campaigns, where all of the investors are viewed as an agent that\ncould interact with the environment derived from the real dynamics of\ncampaigns. Then, to further explore the in-depth implications of patterns\n(i.e., typical characters) in funding series, we propose to subdivide them into\n$\\textit{fast-growing}$ and $\\textit{slow-growing}$ ones. Moreover, for the\npurpose of switching from different kinds of patterns, the actor component of\nTC3 is extended with a structure of options, which comes to the TC3-Options.\nFinally, extensive experiments on the Indiegogo dataset not only demonstrate\nthe effectiveness of our methods, but also validate our assumption that the\nentire pattern learned by TC3-Options is indeed the U-shaped one.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 08:00:40 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Jun", ""], ["Zhang", "Hefu", ""], ["Liu", "Qi", ""], ["Pan", "Zhen", ""], ["Tao", "Hanqing", ""]]}, {"id": "1912.12049", "submitter": "Luca Scrucca", "authors": "Luca Scrucca and Alessio Serafini", "title": "Projection pursuit based on Gaussian mixtures and evolutionary\n  algorithms", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 2019, 28:4,\n  847-860", "doi": "10.1080/10618600.2019.1598871", "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a projection pursuit (PP) algorithm based on Gaussian mixture\nmodels (GMMs). The negentropy obtained from a multivariate density estimated by\nGMMs is adopted as the PP index to be maximised. For a fixed dimension of the\nprojection subspace, the GMM-based density estimation is projected onto that\nsubspace, where an approximation of the negentropy for Gaussian mixtures is\ncomputed. Then, Genetic Algorithms (GAs) are used to find the optimal,\northogonal projection basis by maximising the former approximation. We show\nthat this semi-parametric approach to PP is flexible and allows highly\ninformative structures to be detected, by projecting multivariate datasets onto\na subspace, where the data can be feasibly visualised. The performance of the\nproposed approach is shown on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:25:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Scrucca", "Luca", ""], ["Serafini", "Alessio", ""]]}, {"id": "1912.12064", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad, Muhammad Haroon Shakeel, Sarwan Ali, Imdadullah Khan,\n  Arif Zaman, Asim Karim", "title": "Efficient Data Analytics on Augmented Similarity Triplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning methods (classification, clustering, etc.) start with a\nknown kernel that provides similarity or distance measure between two objects.\nRecent work has extended this to situations where the information about objects\nis limited to comparisons of distances between three objects (triplets). Humans\nfind the comparison task much easier than the estimation of absolute\nsimilarities, so this kind of data can be easily obtained using crowd-sourcing.\nIn this work, we give an efficient method of augmenting the triplets data, by\nutilizing additional implicit information inferred from the existing data.\nTriplets augmentation improves the quality of kernel-based and kernel-free data\nanalytics tasks. Secondly, we also propose a novel set of algorithms for common\nsupervised and unsupervised machine learning tasks based on triplets. These\nmethods work directly with triplets, avoiding kernel evaluations. Experimental\nevaluation on real and synthetic datasets shows that our methods are more\naccurate than the current best-known techniques.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 11:50:43 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ahmad", "Muhammad", ""], ["Shakeel", "Muhammad Haroon", ""], ["Ali", "Sarwan", ""], ["Khan", "Imdadullah", ""], ["Zaman", "Arif", ""], ["Karim", "Asim", ""]]}, {"id": "1912.12098", "submitter": "Tolga Birdal", "authors": "Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti,\n  Leonidas Guibas, Federico Tombari", "title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds", "comments": "Oral Presentation at ECCV 2020. Find our video under:\n  https://youtu.be/LHh56snwhTA. We release our sources at:\n  http://tolgabirdal.github.io/qecnetworks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D capsule module for processing point clouds that is\nequivariant to 3D rotations and translations, as well as invariant to\npermutations of the input points. The operator receives a sparse set of local\nreference frames, computed from an input point cloud and establishes end-to-end\ntransformation equivariance through a novel dynamic routing procedure on\nquaternions. Further, we theoretically connect dynamic routing between capsules\nto the well-known Weiszfeld algorithm, a scheme for solving \\emph{iterative\nre-weighted least squares} (IRLS) problems with provable convergence\nproperties. It is shown that such group dynamic routing can be interpreted as\nrobust IRLS rotation averaging on capsule votes, where information is routed\nbased on the final inlier scores. Based on our operator, we build a capsule\nnetwork that disentangles geometry from pose, paving the way for more\ninformative descriptors and a structured latent space. Our architecture allows\njoint object classification and orientation estimation without explicit\nsupervision of rotations. We validate our algorithm empirically on common\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:51:17 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 19:09:44 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 13:12:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhao", "Yongheng", ""], ["Birdal", "Tolga", ""], ["Lenssen", "Jan Eric", ""], ["Menegatti", "Emanuele", ""], ["Guibas", "Leonidas", ""], ["Tombari", "Federico", ""]]}, {"id": "1912.12115", "submitter": "Praneeth Vepakomma", "authors": "Maarten G.Poirot, Praneeth Vepakomma, Ken Chang, Jayashree\n  Kalpathy-Cramer, Rajiv Gupta, Ramesh Raskar", "title": "Split Learning for collaborative deep learning in healthcare", "comments": "Workshop paper: 8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of labeled data has been holding the surge of deep learning in\nhealthcare back, as sample sizes are often small, patient information cannot be\nshared openly, and multi-center collaborative studies are a burden to set up.\nDistributed machine learning methods promise to mitigate these problems. We\nargue for a split learning based approach and apply this distributed learning\nmethod for the first time in the medical field to compare performance against\n(1) centrally hosted and (2) non collaborative configurations for a range of\nparticipants. Two medical deep learning tasks are used to compare split\nlearning to conventional single and multi center approaches: a binary\nclassification problem of a data set of 9000 fundus photos, and multi-label\nclassification problem of a data set of 156,535 chest X-rays. The several\ndistributed learning setups are compared for a range of 1-50 distributed\nparticipants. Performance of the split learning configuration remained constant\nfor any number of clients compared to a single center study, showing a marked\ndifference compared to the non collaborative configuration after 2 clients (p <\n0.001) for both sets. Our results affirm the benefits of collaborative training\nof deep neural networks in health care. Our work proves the significant benefit\nof distributed learning in healthcare, and paves the way for future real-world\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 14:39:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Poirot", "Maarten G.", ""], ["Vepakomma", "Praneeth", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Gupta", "Rajiv", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1912.12116", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou, Cecilia Turino, Alexander Steblin, Manuel\n  S\\'anchez-de-la-Torre, Ferran Barb\\'e, Eloisa Vargiu", "title": "Comparative Analysis of Predictive Methods for Early Assessment of\n  Compliance with Continuous Positive Airway Pressure Therapy", "comments": "22 pages, 11 figures", "journal-ref": "BMC Medical Informatics and Decision Making. 81 (2018)", "doi": "10.1186/s12911-018-0657-z", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients suffering from obstructive sleep apnea are mainly treated with\ncontinuous positive airway pressure (CPAP). Good compliance with this therapy\nis broadly accepted as more than 4h of CPAP average use nightly. Although it is\na highly effective treatment, compliance with this therapy is problematic to\nachieve with serious consequences for the patients' health. Previous works\nalready reported factors significantly related to compliance with the therapy.\nHowever, further research is still required to support clinicians to early\nanticipate patients' therapy compliance. This work intends to take a further\nstep in this direction by building compliance classifiers with CPAP therapy at\nthree different moments of the patient follow-up (i.e. before the therapy\nstarts and at months 1 and 3 after the baseline). Results of the clinical trial\nconfirmed that month 3 was the time-point with the most accurate classifier\nreaching an f1-score of 87% and 84% in cross-validation and test. At month 1,\nperformances were almost as high as in month 3 with 82% and 84% of f1-score. At\nbaseline, where no information about patients' CPAP use was given yet, the best\nclassifier achieved 73% and 76% of f1-score in cross-validation and test set\nrespectively. Subsequent analyses carried out with the best classifiers of each\ntime point revealed that certain baseline factors (i.e. headaches,\npsychological symptoms, arterial hypertension and EuroQol visual analogue\nscale) were closely related to the prediction of compliance independently of\nthe time-point. In addition, among the variables taken only during the\nfollow-up of the patients, Epworth and the average nighttime hours were the\nmost important to predict compliance with CPAP.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 14:44:21 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Rafael-Palou", "Xavier", ""], ["Turino", "Cecilia", ""], ["Steblin", "Alexander", ""], ["S\u00e1nchez-de-la-Torre", "Manuel", ""], ["Barb\u00e9", "Ferran", ""], ["Vargiu", "Eloisa", ""]]}, {"id": "1912.12120", "submitter": "Daniel San Martin", "authors": "Daniel San Martin, Daniel Manzano", "title": "A Deep Learning Model for Chilean Bills Classification", "comments": "3 pages, 3 figures, Posters Content EVIC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic bill classification is an attractive task with many potential\napplications such as automated detection and counting in images or videos. To\naddress this purpose we present a Deep Learning Model to classify Chilean\nBanknotes, because of its successful results in image processing applications.\nFor optimal performance of the proposed model, data augmentation techniques are\nintroduced due to the limited number of image samples. Positive results were\nachieved in this work, verifying that it could be a stating point to be\nextended to more complex applications.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 20:29:20 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Martin", "Daniel San", ""], ["Manzano", "Daniel", ""]]}, {"id": "1912.12121", "submitter": "Y. Alex Kolchinski", "authors": "Y. Alex Kolchinski, Sharon Zhou, Shengjia Zhao, Mitchell Gordon,\n  Stefano Ermon", "title": "Approximating Human Judgment of Generated Image Quality", "comments": "To appear in the Shared Visual Representations in Human and Machine\n  Intelligence workshop at NeurIPS 2019. The first two authors contributed\n  equally to the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have made immense progress in recent years, particularly in\ntheir ability to generate high quality images. However, that quality has been\ndifficult to evaluate rigorously, with evaluation dominated by heuristic\napproaches that do not correlate well with human judgment, such as the\nInception Score and Fr\\'echet Inception Distance. Real human labels have also\nbeen used in evaluation, but are inefficient and expensive to collect for each\nimage. Here, we present a novel method to automatically evaluate images based\non their quality as perceived by humans. By not only generating image\nembeddings from Inception network activations and comparing them to the\nactivations for real images, of which other methods perform a variant, but also\nregressing the activation statistics to match gold standard human labels, we\ndemonstrate 66% accuracy in predicting human scores of image realism, matching\nthe human inter-rater agreement rate. Our approach also generalizes across\ngenerative models, suggesting the potential for capturing a model-agnostic\nmeasure of image quality. We open source our dataset of human labels for the\nadvancement of research and techniques in this area.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 19:51:02 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kolchinski", "Y. Alex", ""], ["Zhou", "Sharon", ""], ["Zhao", "Shengjia", ""], ["Gordon", "Mitchell", ""], ["Ermon", "Stefano", ""]]}, {"id": "1912.12132", "submitter": "Cenk Gazen", "authors": "Shreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk\n  Gazen, Jason Hickey", "title": "Machine Learning for Precipitation Nowcasting from Radar Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution nowcasting is an essential tool needed for effective\nadaptation to climate change, particularly for extreme weather. As Deep\nLearning (DL) techniques have shown dramatic promise in many domains, including\nthe geosciences, we present an application of DL to the problem of\nprecipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1\nhour) predictions of precipitation. We treat forecasting as an image-to-image\ntranslation problem and leverage the power of the ubiquitous UNET convolutional\nneural network. We find this performs favorably when compared to three commonly\nused models: optical flow, persistence and NOAA's numerical one-hour HRRR\nnowcasting prediction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:46:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Agrawal", "Shreya", ""], ["Barrington", "Luke", ""], ["Bromberg", "Carla", ""], ["Burge", "John", ""], ["Gazen", "Cenk", ""], ["Hickey", "Jason", ""]]}, {"id": "1912.12138", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yulin Sun, Yang Wang, Zhengjun Zha, Shuicheng Yan, Meng\n  Wang", "title": "Convolutional Dictionary Pair Learning Network for Image Representation\n  Learning", "comments": "Accepted by the 24th European Conference on Artificial Intelligence\n  (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are\npowerful image representation learning systems based on different mechanisms\nand principles, however whether we can seamlessly integrate them to improve the\nper-formance is noteworthy exploring. To address this issue, we propose a novel\ngeneralized end-to-end representation learning architecture, dubbed\nConvolutional Dictionary Pair Learning Network (CDPL-Net) in this paper, which\nintegrates the learning schemes of the CNN and dictionary pair learning into a\nunified framework. Generally, the architecture of CDPL-Net includes two\nconvolutional/pooling layers and two dictionary pair learn-ing (DPL) layers in\nthe representation learning module. Besides, it uses two fully-connected layers\nas the multi-layer perception layer in the nonlinear classification module. In\nparticular, the DPL layer can jointly formulate the discriminative synthesis\nand analysis representations driven by minimizing the batch based\nreconstruction error over the flatted feature maps from the convolution/pooling\nlayer. Moreover, DPL layer uses l1-norm on the analysis dictionary so that\nsparse representation can be delivered, and the embedding process will also be\nrobust to noise. To speed up the training process of DPL layer, the efficient\nstochastic gradient descent is used. Extensive simulations on real databases\nshow that our CDPL-Net can deliver enhanced performance over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:34:28 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 09:49:42 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 12:12:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zhang", "Zhao", ""], ["Sun", "Yulin", ""], ["Wang", "Yang", ""], ["Zha", "Zhengjun", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1912.12144", "submitter": "Joy Bose", "authors": "Kushal Singla, Niloy Mukherjee, Hari Manassery Koduvely, Joy Bose", "title": "Evaluating Usage of Images for App Classification", "comments": "5 pages, 3 figures, 3 tables, INDICON conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App classification is useful in a number of applications such as adding apps\nto an app store or building a user model based on the installed apps. Presently\nthere are a number of existing methods to classify apps based on a given\ntaxonomy on the basis of their text metadata. However, text based methods for\napp classification may not work in all cases, such as when the text\ndescriptions are in a different language, or missing, or inadequate to classify\nthe app. One solution in such cases is to utilize the app images to supplement\nthe text description. In this paper, we evaluate a number of approaches in\nwhich app images can be used to classify the apps. In one approach, we use\nOptical character recognition (OCR) to extract text from images, which is then\nused to supplement the text description of the app. In another, we use pic2vec\nto convert the app images into vectors, then train an SVM to classify the\nvectors to the correct app label. In another, we use the captionbot.ai tool to\ngenerate natural language descriptions from the app images. Finally, we use a\nmethod to detect and label objects in the app images and use a voting technique\nto determine the category of the app based on all the images. We compare the\nperformance of our image-based techniques to classify a number of apps in our\ndataset. We use a text based SVM app classifier as our base and obtained an\nimproved classification accuracy of 96% for some classes when app images are\nadded.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:27:02 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singla", "Kushal", ""], ["Mukherjee", "Niloy", ""], ["Koduvely", "Hari Manassery", ""], ["Bose", "Joy", ""]]}, {"id": "1912.12147", "submitter": "Eduardo Arnold", "authors": "Eduardo Arnold, Mehrdad Dianati, Robert de Temple, Saber Fallah", "title": "Cooperative Perception for 3D Object Detection in Driving Scenarios\n  using Infrastructure Sensors", "comments": "13 pages, 4 tables, 7 figures. Published in IEEE Transactions on\n  Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3028424", "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is a common function within the perception system of an\nautonomous vehicle and outputs a list of 3D bounding boxes around objects of\ninterest. Various 3D object detection methods have relied on fusion of\ndifferent sensor modalities to overcome limitations of individual sensors.\nHowever, occlusion, limited field-of-view and low-point density of the sensor\ndata cannot be reliably and cost-effectively addressed by multi-modal sensing\nfrom a single point of view. Alternatively, cooperative perception incorporates\ninformation from spatially diverse sensors distributed around the environment\nas a way to mitigate these limitations. This article proposes two schemes for\ncooperative 3D object detection using single modality sensors. The early fusion\nscheme combines point clouds from multiple spatially diverse sensing points of\nview before detection. In contrast, the late fusion scheme fuses the\nindependently detected bounding boxes from multiple spatially diverse sensors.\nWe evaluate the performance of both schemes, and their hybrid combination,\nusing a synthetic cooperative dataset created in two complex driving scenarios,\na T-junction and a roundabout. The evaluation shows that the early fusion\napproach outperforms late fusion by a significant margin at the cost of higher\ncommunication bandwidth. The results demonstrate that cooperative perception\ncan recall more than 95% of the objects as opposed to 30% for single-point\nsensing in the most challenging scenario. To provide practical insights into\nthe deployment of such system, we report how the number of sensors and their\nconfiguration impact the detection performance of the system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:19:27 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:59:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Arnold", "Eduardo", ""], ["Dianati", "Mehrdad", ""], ["de Temple", "Robert", ""], ["Fallah", "Saber", ""]]}, {"id": "1912.12150", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Sambit Panda, Joshua T. Vogelstein", "title": "The Chi-Square Test of Distance Correlation", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:16:40 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 15:08:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 21:53:47 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 21:35:39 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 18:09:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Cencheng", ""], ["Panda", "Sambit", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1912.12164", "submitter": "Arthur Pajot", "authors": "Arthur Pajot, Emmanuel de Bezenac, Patrick Gallinari", "title": "Unsupervised Adversarial Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inpainting in an unsupervised setting where there is neither\naccess to paired nor unpaired training data. The only available information is\nprovided by the uncomplete observations and the inpainting process statistics.\nIn this context, an observation should give rise to several plausible\nreconstructions which amounts at learning a distribution over the space of\nreconstructed images. We model the reconstruction process by using a\nconditional GAN with constraints on the stochastic component that introduce an\nexplicit dependency between this component and the generated output. This\nallows us sampling from the latent component in order to generate a\ndistribution of images associated to an observation. We demonstrate the\ncapacity of our model on several image datasets: faces (CelebA), food images\n(Recipe-1M) and bedrooms (LSUN Bedrooms) with different types of imputation\nmasks. The approach yields comparable performance to model variants trained\nwith additional supervision.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:06:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Pajot", "Arthur", ""], ["de Bezenac", "Emmanuel", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1912.12175", "submitter": "Yasemin Bozkurt Varolgunes", "authors": "Yasemin Bozkurt Varolgunes, Tristan Bereau, and Joseph F. Rudzinski", "title": "Interpretable Embeddings From Molecular Simulations Using Gaussian\n  Mixture Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting insight from the enormous quantity of data generated from\nmolecular simulations requires the identification of a small number of\ncollective variables whose corresponding low-dimensional free-energy landscape\nretains the essential features of the underlying system. Data-driven techniques\nprovide a systematic route to constructing this landscape, without the need for\nextensive a priori intuition into the relevant driving forces. In particular,\nautoencoders are powerful tools for dimensionality reduction, as they naturally\nforce an information bottleneck and, thereby, a low-dimensional embedding of\nthe essential features. While variational autoencoders ensure continuity of the\nembedding by assuming a unimodal Gaussian prior, this is at odds with the\nmulti-basin free-energy landscapes that typically arise from the identification\nof meaningful collective variables. In this work, we incorporate this physical\nintuition into the prior by employing a Gaussian mixture variational\nautoencoder (GMVAE), which encourages the separation of metastable states\nwithin the embedding. The GMVAE performs dimensionality reduction and\nclustering within a single unified framework, and is capable of identifying the\ninherent dimensionality of the input data, in terms of the number of Gaussians\nrequired to categorize the data. We illustrate our approach on two toy models,\nalanine dipeptide, and a challenging disordered peptide ensemble, demonstrating\nthe enhanced clustering effect of the GMVAE prior compared to standard VAEs.\nThe resulting embeddings appear to be promising representations for\nconstructing Markov state models, highlighting the transferability of the\ndimensionality reduction from static equilibrium properties to dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 15:30:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Varolgunes", "Yasemin Bozkurt", ""], ["Bereau", "Tristan", ""], ["Rudzinski", "Joseph F.", ""]]}, {"id": "1912.12186", "submitter": "Hu Wang", "authors": "Hu Wang, Guansong Pang, Chunhua Shen, Congbo Ma", "title": "Unsupervised Representation Learning by Predicting Random Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have gained tremendous success in a broad range of\nmachine learning tasks due to its remarkable capability to learn semantic-rich\nfeatures from high-dimensional data. However, they often require large-scale\nlabelled data to successfully learn such features, which significantly hinders\ntheir adaption into unsupervised learning tasks, such as anomaly detection and\nclustering, and limits their applications into critical domains where obtaining\nmassive labelled data is prohibitively expensive. To enable unsupervised\nlearning on those domains, in this work we propose to learn features without\nusing any labelled data by training neural networks to predict data distances\nin a randomly projected space. Random mapping is a theoretically proven\napproach to obtain approximately preserved distances. To well predict these\nrandom distances, the representation learner is optimised to learn genuine\nclass structures that are implicitly embedded in the randomly projected space.\nEmpirical results on 19 real-world datasets show that our learned\nrepresentations substantially outperform a few state-of-the-art competing\nmethods in both anomaly detection and clustering tasks. Code is available at\nhttps://git.io/RDP\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 05:09:11 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 11:57:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Hu", ""], ["Pang", "Guansong", ""], ["Shen", "Chunhua", ""], ["Ma", "Congbo", ""]]}, {"id": "1912.12187", "submitter": "Amina Asif", "authors": "Fayyaz ul Amir Afsar Minhas and Amina Asif", "title": "Learning Neural Activations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial neuron is modelled as a weighted summation followed by an\nactivation function which determines its output. A wide variety of activation\nfunctions such as rectified linear units (ReLU), leaky-ReLU, Swish, MISH, etc.\nhave been explored in the literature. In this short paper, we explore what\nhappens when the activation function of each neuron in an artificial neural\nnetwork is learned natively from data alone. This is achieved by modelling the\nactivation function of each neuron as a small neural network whose weights are\nshared by all neurons in the original network. We list our primary findings in\nthe conclusions section. The code for our analysis is available at:\nhttps://github.com/amina01/Learning-Neural-Activations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:52:07 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Minhas", "Fayyaz ul Amir Afsar", ""], ["Asif", "Amina", ""]]}, {"id": "1912.12209", "submitter": "Wei Wang", "authors": "Wei Wang, Haojie Li, Zhihui Wang, Jing Sun, Zhengming Ding, and Fuming\n  Sun", "title": "Importance Filtered Cross-Domain Adaptation", "comments": "15 pages, 7 figures, IEEE Tansactions on Image Processing. arXiv\n  admin note: text overlap with arXiv:1906.07441 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Domain Adaptation (DA), the category-relevant losses usually occupy a\ndominant position, while they are usually built with hard or soft labels in\nexisting models. We observed that hard labels are overconfident due to hard\nsamples existed, and soft labels are ambiguous as too many small noisy\nprobabilities involved, and both of them are easily to cause negative transfer.\nBesides, the category-irrelevant losses in Closed-Set DA (CSDA) paradigm fail\nto work in Open-Set DA (OSDA), and they also have to be in a category-relevant\nform, since target data samples are split into shared and private classes. To\nthis end, we propose a newly-unified DA framework (i.e., Importance Filtered\nCross-Domain Adaptation, IFCDA). Firstly, an importance filtered mechanism is\ndevised to generate filtered soft labels to mitigate negative transfer\ndesirably. Specifically, the soft labels are divided into confident and\nambiguous ones. Then, only the maximum probability in each confident label is\nretained, and a threshold value is set to truncate each ambiguous label so that\nonly prominent probabilities are reserved. Moreover, a general graph-based\nlabel propagation is contrived to attain soft labels in both CSDA and OSDA,\nwhere an extra component is embedded into label vector, so that it could detect\ntarget novel classes. Finally, the category-relevant losses in both scenarios\nare reformulated using filtered soft labels, while the category-irrelevant MMD\nloss in CSDA is reformulated as a form like class-wise MMD using newly-designed\nimportance filtered soft labels. Notably, CSDA paradigm is a special case when\nall extra components are set to 0, thus the proposed approach is geared to both\nCSDA and OSDA. Comprehensive experiments on benchmark cross-domain object\nrecognition datasets verify that the proposed approach outperforms several\nstate-of-the-art methods in both scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:10:58 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 15:41:04 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 15:51:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Haojie", ""], ["Wang", "Zhihui", ""], ["Sun", "Jing", ""], ["Ding", "Zhengming", ""], ["Sun", "Fuming", ""]]}, {"id": "1912.12211", "submitter": "Carlo Vittorio Cannistraci", "authors": "C. Duran, A. Acevedo, S. Ciucci, A. Muscoloni, and CV. Cannistraci", "title": "Nonlinear Markov Clustering by Minimum Curvilinear Sparse Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of algorithms for unsupervised pattern recognition by\nnonlinear clustering is a notable problem in data science. Markov clustering\n(MCL) is a renowned algorithm that simulates stochastic flows on a network of\nsample similarities to detect the structural organization of clusters in the\ndata, but it has never been generalized to deal with data nonlinearity. Minimum\nCurvilinearity (MC) is a principle that approximates nonlinear sample distances\nin the high-dimensional feature space by curvilinear distances, which are\ncomputed as transversal paths over their minimum spanning tree, and then stored\nin a kernel. Here we propose MC-MCL, which is the first nonlinear kernel\nextension of MCL and exploits Minimum Curvilinearity to enhance the performance\nof MCL in real and synthetic data with underlying nonlinear patterns. MC-MCL is\ncompared with baseline clustering methods, including DBSCAN, K-means and\naffinity propagation. We find that Minimum Curvilinearity provides a valuable\nframework to estimate nonlinear distances also when its kernel is applied in\ncombination with MCL. Indeed, MC-MCL overcomes classical MCL and even baseline\nclustering algorithms in different nonlinear datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:07:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Duran", "C.", ""], ["Acevedo", "A.", ""], ["Ciucci", "S.", ""], ["Muscoloni", "A.", ""], ["Cannistraci", "CV.", ""]]}, {"id": "1912.12213", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Victor Chernozhukov, Whitney K. Newey, Yinchu Zhu", "title": "Minimax Semiparametric Learning With Approximate Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the ability and means to root-n consistently and\nefficiently estimate linear, mean square continuous functionals of a high\ndimensional, approximately sparse regression. Such objects include a wide\nvariety of interesting parameters such as the covariance between two regression\nresiduals, a coefficient of a partially linear model, an average derivative,\nand the average treatment effect. We give lower bounds on the convergence rate\nof estimators of such objects and find that these bounds are substantially\nlarger than in a low dimensional, semiparametric setting. We also give\nautomatic debiased machine learners that are $1/\\sqrt{n}$ consistent and\nasymptotically efficient under minimal conditions. These estimators use no\ncross-fitting or a special kind of cross-fitting to attain efficiency with\nfaster than $n^{-1/4}$ convergence of the regression. This rate condition is\nsubstantially weaker than the product of convergence rates of two functions\nbeing faster than $1/\\sqrt{n},$ as required for many other debiased machine\nlearners.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:13:21 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 15:16:10 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 11:53:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bradic", "Jelena", ""], ["Chernozhukov", "Victor", ""], ["Newey", "Whitney K.", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1912.12264", "submitter": "Sarwan Ali", "authors": "Sarwan Ali, Muhammad Haroon Shakeel, Imdadullah Khan, Safiullah\n  Faizullah, Muhammad Asad Khan", "title": "Predicting Attributes of Nodes Using Network Structure", "comments": "This paper is Published at ACM Transactions on Intelligent Systems\n  and Technology (ACM TIST)", "journal-ref": "ACM Transactions on Intelligent Systems and Technology, 2021", "doi": "10.1145/3442390", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many graphs such as social networks, nodes have associated attributes\nrepresenting their behavior. Predicting node attributes in such graphs is an\nimportant problem with applications in many domains like recommendation\nsystems, privacy preservation, and targeted advertisement. Attributes values\ncan be predicted by analyzing patterns and correlations among attributes and\nemploying classification/regression algorithms. However, these approaches do\nnot utilize readily available network topology information. In this regard,\ninterconnections between different attributes of nodes can be exploited to\nimprove the prediction accuracy. In this paper, we propose an approach to\nrepresent a node by a feature map with respect to an attribute $a_i$ (which is\nused as input for machine learning algorithms) using all attributes of\nneighbors to predict attributes values for $a_i$. We perform extensive\nexperimentation on ten real-world datasets and show that the proposed feature\nmap significantly improves the prediction accuracy as compared to baseline\napproaches on these datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 17:59:33 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 19:59:59 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 12:11:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ali", "Sarwan", ""], ["Shakeel", "Muhammad Haroon", ""], ["Khan", "Imdadullah", ""], ["Faizullah", "Safiullah", ""], ["Khan", "Muhammad Asad", ""]]}, {"id": "1912.12274", "submitter": "Juan Manuel Gorriz Saez Juan M", "authors": "J M Gorriz, SiPBA Group, and CAM neuroscience", "title": "Statistical Agnostic Mapping: a Framework in Neuroimaging based on\n  Concentration Inequalities", "comments": "18 pages, 10 figures, prepared to be submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 70s a novel branch of statistics emerged focusing its effort in\nselecting a function in the pattern recognition problem, which fulfils a\ndefinite relationship between the quality of the approximation and its\ncomplexity. These data-driven approaches are mainly devoted to problems of\nestimating dependencies with limited sample sizes and comprise all the\nempirical out-of sample generalization approaches, e.g. cross validation (CV)\napproaches. Although the latter are \\emph{not designed for testing competing\nhypothesis or comparing different models} in neuroimaging, there are a number\nof theoretical developments within this theory which could be employed to\nderive a Statistical Agnostic (non-parametric) Mapping (SAM) at voxel or\nmulti-voxel level. Moreover, SAMs could relieve i) the problem of instability\nin limited sample sizes when estimating the actual risk via the CV approaches,\ne.g. large error bars, and provide ii) an alternative way of Family-wise-error\n(FWE) corrected p-value maps in inferential statistics for hypothesis testing.\nIn this sense, we propose a novel framework in neuroimaging based on\nconcentration inequalities, which results in (i) a rigorous development for\nmodel validation with a small sample/dimension ratio, and (ii) a\nless-conservative procedure than FWE p-value correction, to determine the brain\nsignificance maps from the inferences made using small upper bounds of the\nactual risk.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:27:50 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gorriz", "J M", ""], ["Group", "SiPBA", ""], ["neuroscience", "CAM", ""]]}, {"id": "1912.12309", "submitter": "Anastasios Tsiamis", "authors": "Anastasios Tsiamis, Nikolai Matni, George J. Pappas", "title": "Sample Complexity of Kalman Filtering for Unknown Systems", "comments": "To appear in L4DC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the task of designing a Kalman Filter (KF) for an\nunknown and partially observed autonomous linear time invariant system driven\nby process and sensor noise. To do so, we propose studying the following two\nstep process: first, using system identification tools rooted in subspace\nmethods, we obtain coarse finite-data estimates of the state-space parameters\nand Kalman gain describing the autonomous system; and second, we use these\napproximate parameters to design a filter which produces estimates of the\nsystem state. We show that when the system identification step produces\nsufficiently accurate estimates, or when the underlying true KF is sufficiently\nrobust, that a Certainty Equivalent (CE) KF, i.e., one designed using the\nestimated parameters directly, enjoys provable sub-optimality guarantees. We\nfurther show that when these conditions fail, and in particular, when the CE KF\nis marginally stable (i.e., has eigenvalues very close to the unit circle),\nthat imposing additional robustness constraints on the filter leads to similar\nsub-optimality guarantees. We further show that with high probability, both the\nCE and robust filters have mean prediction error bounded by $\\tilde\nO(1/\\sqrt{N})$, where $N$ is the number of data points collected in the system\nidentification step. To the best of our knowledge, these are the first\nend-to-end sample complexity bounds for the Kalman Filtering of an unknown\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 19:00:42 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 22:05:03 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 06:23:16 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tsiamis", "Anastasios", ""], ["Matni", "Nikolai", ""], ["Pappas", "George J.", ""]]}, {"id": "1912.12318", "submitter": "Xiaofeng Yang", "authors": "Yabo Fu, Yang Lei, Tonghe Wang, Walter J. Curran, Tian Liu, Xiaofeng\n  Yang", "title": "Deep Learning in Medical Image Registration: A Review", "comments": "32 pages, 4 figures, 9 tables", "journal-ref": null, "doi": "10.1088/1361-6560/ab843e", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a review of deep learning (DL) based medical image\nregistration methods. We summarized the latest developments and applications of\nDL-based registration methods in the medical field. These methods were\nclassified into seven categories according to their methods, functions and\npopularity. A detailed review of each category was presented, highlighting\nimportant contributions and identifying specific challenges. A short assessment\nwas presented following the detailed review of each category to summarize its\nachievements and future potentials. We provided a comprehensive comparison\namong DL-based methods for lung and brain deformable registration using\nbenchmark datasets. Lastly, we analyzed the statistics of all the cited works\nfrom various aspects, revealing the popularity and future trend of development\nin medical image registration using deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 19:32:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Fu", "Yabo", ""], ["Lei", "Yang", ""], ["Wang", "Tonghe", ""], ["Curran", "Walter J.", ""], ["Liu", "Tian", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "1912.12345", "submitter": "Richard Shin", "authors": "Richard Shin, Neel Kant, Kavi Gupta, Christopher Bender, Brandon\n  Trabucco, Rishabh Singh, Dawn Song", "title": "Synthetic Datasets for Neural Program Synthesis", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of program synthesis is to automatically generate programs in a\nparticular language from corresponding specifications, e.g. input-output\nbehavior. Many current approaches achieve impressive results after training on\nrandomly generated I/O examples in limited domain-specific languages (DSLs), as\nwith string transformations in RobustFill. However, we empirically discover\nthat applying test input generation techniques for languages with control flow\nand rich input space causes deep networks to generalize poorly to certain data\ndistributions; to correct this, we propose a new methodology for controlling\nand evaluating the bias of synthetic data distributions over both programs and\nspecifications. We demonstrate, using the Karel DSL and a small Calculator DSL,\nthat training deep networks on these distributions leads to improved\ncross-distribution generalization performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 21:28:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Shin", "Richard", ""], ["Kant", "Neel", ""], ["Gupta", "Kavi", ""], ["Bender", "Christopher", ""], ["Trabucco", "Brandon", ""], ["Singh", "Rishabh", ""], ["Song", "Dawn", ""]]}, {"id": "1912.12355", "submitter": "A. Ali Heydari", "authors": "A. Ali Heydari, Craig A. Thompson and Asif Mehmood", "title": "SoftAdapt: Techniques for Adaptive Loss Weighting of Neural Networks\n  with Multi-Part Loss Functions", "comments": "8 pages with 2 pages of references. 6 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive loss function formulation is an active area of research and has\ngained a great deal of popularity in recent years, following the success of\ndeep learning. However, existing frameworks of adaptive loss functions often\nsuffer from slow convergence and poor choice of weights for the loss\ncomponents. Traditionally, the elements of a multi-part loss function are\nweighted equally or their weights are determined through heuristic approaches\nthat yield near-optimal (or sub-optimal) results. To address this problem, we\npropose a family of methods, called SoftAdapt, that dynamically change function\nweights for multi-part loss functions based on live performance statistics of\nthe component losses. SoftAdapt is mathematically intuitive, computationally\nefficient and straightforward to implement. In this paper, we present the\nmathematical formulation and pseudocode for SoftAdapt, along with results from\napplying our methods to image reconstruction (Sparse Autoencoders) and\nsynthetic data generation (Introspective Variational Autoencoders).\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:23:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Heydari", "A. Ali", ""], ["Thompson", "Craig A.", ""], ["Mehmood", "Asif", ""]]}, {"id": "1912.12360", "submitter": "Hu Sun", "authors": "Hu Sun (1), Ward Manchester (2), Zhenbang Jiao (1), Xiantong Wang (2),\n  Yang Chen (1 and 3) ((1) Department of Statistics, University of Michigan,\n  Ann Arbor, (2) Climate and Space Sciences and Engineering, University of\n  Michigan, Ann Arbor, (3) Michigan Institute for Data Science, University of\n  Michigan, Ann Arbor)", "title": "Interpreting LSTM Prediction on Solar Flare Eruption with Time-series\n  Clustering", "comments": "26 pages, 18 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a post hoc analysis of solar flare predictions made by a Long\nShort Term Memory (LSTM) model employing data in the form of Space-weather HMI\nActive Region Patches (SHARP) parameters calculated from data in proximity to\nthe magnetic polarity inversion line where the flares originate. We train the\nthe LSTM model for binary classification to provide a prediction score for the\nprobability of M/X class flares to occur in next hour. We then develop a\ndimension-reduction technique to reduce the dimensions of SHARP parameter (LSTM\ninputs) and demonstrate the different patterns of SHARP parameters\ncorresponding to the transition from low to high prediction score. Our work\nshows that a subset of SHARP parameters contain the key signals that strong\nsolar flare eruptions are imminent. The dynamics of these parameters have a\nhighly uniform trajectory for many events whose LSTM prediction scores for M/X\nclass flares transition from very low to very high. The results demonstrate the\nexistence of a few threshold values of SHARP parameters that when surpassed\nindicate a high probability of the eruption of a strong flare. Our method has\ndistilled the knowledge of solar flare eruption learnt by deep learning model\nand provides a more interpretable approximation, which provides physical\ninsight to processes driving solar flares.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:56:01 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 21:36:09 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sun", "Hu", "", "1 and 3"], ["Manchester", "Ward", "", "1 and 3"], ["Jiao", "Zhenbang", "", "1 and 3"], ["Wang", "Xiantong", "", "1 and 3"], ["Chen", "Yang", "", "1 and 3"]]}, {"id": "1912.12370", "submitter": "Josh Payne", "authors": "Josh Payne and Ashish Kundu", "title": "Towards Deep Federated Defenses Against Malware in Cloud Ecosystems", "comments": "IEEE International Conference on Trust, Privacy and Security in\n  Intelligent Systems, and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing environments with many virtual machines, containers, and\nother systems, an epidemic of malware can be highly threatening to business\nprocesses. In this vision paper, we introduce a hierarchical approach to\nperforming malware detection and analysis using several recent advances in\nmachine learning on graphs, hypergraphs, and natural language. We analyze\nindividual systems and their logs, inspecting and understanding their behavior\nwith attentional sequence models. Given a feature representation of each\nsystem's logs using this procedure, we construct an attributed network of the\ncloud with systems and other components as vertices and propose an analysis of\nmalware with inductive graph and hypergraph learning models. With this\nfoundation, we consider the multicloud case, in which multiple clouds with\ndiffering privacy requirements cooperate against the spread of malware,\nproposing the use of federated learning to perform inference and training while\npreserving privacy. Finally, we discuss several open problems that remain in\ndefending cloud computing environments against malware related to designing\nrobust ecosystems, identifying cloud-specific optimization problems for\nresponse strategy, action spaces for malware containment and eradication, and\ndeveloping priors and transfer learning tasks for machine learning models in\nthis area.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 23:46:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Payne", "Josh", ""], ["Kundu", "Ashish", ""]]}, {"id": "1912.12384", "submitter": "Abhinav Garg", "authors": "Abhinav Garg, Dhananjaya Gowda, Ankur Kumar, Kwangyoun Kim, Mehul\n  Kumar and Chanwoo Kim", "title": "Improved Multi-Stage Training of Online Attention-based Encoder-Decoder\n  Models", "comments": "Accepted and presented at the ASRU 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a refined multi-stage multi-task training strategy\nto improve the performance of online attention-based encoder-decoder (AED)\nmodels. A three-stage training based on three levels of architectural\ngranularity namely, character encoder, byte pair encoding (BPE) based encoder,\nand attention decoder, is proposed. Also, multi-task learning based on\ntwo-levels of linguistic granularity namely, character and BPE, is used. We\nexplore different pre-training strategies for the encoders including transfer\nlearning from a bidirectional encoder. Our encoder-decoder models with online\nattention show 35% and 10% relative improvement over their baselines for\nsmaller and bigger models, respectively. Our models achieve a word error rate\n(WER) of 5.04% and 4.48% on the Librispeech test-clean data for the smaller and\nbigger models respectively after fusion with long short-term memory (LSTM)\nbased external language model (LM).\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 02:29:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Garg", "Abhinav", ""], ["Gowda", "Dhananjaya", ""], ["Kumar", "Ankur", ""], ["Kim", "Kwangyoun", ""], ["Kumar", "Mehul", ""], ["Kim", "Chanwoo", ""]]}, {"id": "1912.12413", "submitter": "Jean Feng", "authors": "Jean Feng, Scott Emerson, Noah Simon", "title": "Approval policies for modifications to Machine Learning-Based Software\n  as a Medical Device: A study of bio-creep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful deployment of machine learning algorithms in healthcare requires\ncareful assessments of their performance and safety. To date, the FDA approves\nlocked algorithms prior to marketing and requires future updates to undergo\nseparate premarket reviews. However, this negates a key feature of machine\nlearning--the ability to learn from a growing dataset and improve over time.\nThis paper frames the design of an approval policy, which we refer to as an\nautomatic algorithmic change protocol (aACP), as an online hypothesis testing\nproblem. As this process has obvious analogy with noninferiority testing of new\ndrugs, we investigate how repeated testing and adoption of modifications might\nlead to gradual deterioration in prediction accuracy, also known as\n``biocreep'' in the drug development literature. We consider simple policies\nthat one might consider but do not necessarily offer any error-rate guarantees,\nas well as policies that do provide error-rate control. For the latter, we\ndefine two online error-rates appropriate for this context: Bad Approval Count\n(BAC) and Bad Approval and Benchmark Ratios (BABR). We control these rates in\nthe simple setting of a constant population and data source using policies\naACP-BAC and aACP-BABR, which combine alpha-investing, group-sequential, and\ngate-keeping methods. In simulation studies, bio-creep regularly occurred when\nusing policies with no error-rate guarantees, whereas aACP-BAC and -BABR\ncontrolled the rate of bio-creep without substantially impacting our ability to\napprove beneficial modifications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 06:34:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Feng", "Jean", ""], ["Emerson", "Scott", ""], ["Simon", "Noah", ""]]}, {"id": "1912.12418", "submitter": "Carlo Vittorio Cannistraci", "authors": "A. Acevedo, S. Ciucci, MJ. Kuo, C. Duran and CV. Cannistraci", "title": "Measuring group-separability in geometrical space for evaluation of\n  pattern recognition and embedding algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating data separation in a geometrical space is fundamental for pattern\nrecognition. A plethora of dimensionality reduction (DR) algorithms have been\ndeveloped in order to reveal the emergence of geometrical patterns in a low\ndimensional visible representation space, in which high-dimensional samples\nsimilarities are approximated by geometrical distances. However, statistical\nmeasures to evaluate directly in the low dimensional geometrical space the\nsample group separability attaiend by these DR algorithms are missing.\nCertainly, these separability measures could be used both to compare algorithms\nperformance and to tune algorithms parameters. Here, we propose three\nstatistical measures (named as PSI-ROC, PSI-PR, and PSI-P) that have origin\nfrom the Projection Separability (PS) rationale introduced in this study, which\nis expressly designed to assess group separability of data samples in a\ngeometrical space. Traditional cluster validity indices (CVIs) might be applied\nin this context but they show limitations because they are not specifically\ntailored for DR. Our PS measures are compared to six baseline cluster validity\nindices, using five non-linear datasets and six different DR algorithms. The\nresults provide clear evidence that statistical-based measures based on PS\nrationale are more accurate than CVIs and can be adopted to control the tuning\nof parameter-dependent DR algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 07:34:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Acevedo", "A.", ""], ["Ciucci", "S.", ""], ["Kuo", "MJ.", ""], ["Duran", "C.", ""], ["Cannistraci", "CV.", ""]]}, {"id": "1912.12419", "submitter": "Yukuan Yang", "authors": "Yukuan Yang, Lei Deng, Peng Jiao, Yansong Chua, Jing Pei, Cheng Ma,\n  Guoqi Li", "title": "Transfer Learning in General Lensless Imaging through Scattering Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks (DNNs) have been successfully introduced to the\nfield of lensless imaging through scattering media. By solving an inverse\nproblem in computational imaging, DNNs can overcome several shortcomings in the\nconventional lensless imaging through scattering media methods, namely, high\ncost, poor quality, complex control, and poor anti-interference. However, for\ntraining, a large number of training samples on various datasets have to be\ncollected, with a DNN trained on one dataset generally performing poorly for\nrecovering images from another dataset. The underlying reason is that lensless\nimaging through scattering media is a high dimensional regression problem and\nit is difficult to obtain an analytical solution. In this work, transfer\nlearning is proposed to address this issue. Our main idea is to train a DNN on\na relatively complex dataset using a large number of training samples and\nfine-tune the last few layers using very few samples from other datasets.\nInstead of the thousands of samples required to train from scratch, transfer\nlearning alleviates the problem of costly data acquisition. Specifically,\nconsidering the difference in sample sizes and similarity among datasets, we\npropose two DNN architectures, namely LISMU-FCN and LISMU-OCN, and a balance\nloss function designed for balancing smoothness and sharpness. LISMU-FCN, with\nmuch fewer parameters, can achieve imaging across similar datasets while\nLISMU-OCN can achieve imaging across significantly different datasets. What's\nmore, we establish a set of simulation algorithms which are close to the real\nexperiment, and it is of great significance and practical value in the research\non lensless scattering imaging. In summary, this work provides a new solution\nfor lensless imaging through scattering media using transfer learning in DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 07:37:25 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Yang", "Yukuan", ""], ["Deng", "Lei", ""], ["Jiao", "Peng", ""], ["Chua", "Yansong", ""], ["Pei", "Jing", ""], ["Ma", "Cheng", ""], ["Li", "Guoqi", ""]]}, {"id": "1912.12479", "submitter": "Sarwan Ali", "authors": "Sarwan Ali, Haris Mansoor, Imdadullah Khan, Naveed Arshad, Muhammad\n  Asad Khan, Safiullah Faizullah", "title": "Short-Term Load Forecasting Using AMI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate short-term load forecasting is essential for efficient operation of\nthe power sector. Predicting load at a fine granularity such as individual\nhouseholds or buildings is challenging due to higher volatility and uncertainty\nin the load. In aggregate loads such as at grids level, the inherent\nstochasticity and fluctuations are averaged-out, the problem becomes\nsubstantially easier. We propose an approach for short-term load forecasting at\nindividual consumers (households) level, called Forecasting using Matrix\nFactorization (FMF). FMF does not use any consumers' demographic or activity\npatterns information. Therefore, it can be applied to any locality with the\nreadily available smart meters and weather data. We perform extensive\nexperiments on three benchmark datasets and demonstrate that FMF significantly\noutperforms the computationally expensive state-of-the-art methods for this\nproblem. We achieve up to 26.5% and 24.4 % improvement in RMSE over Regression\nTree and Support Vector Machine, respectively and up to 36% and 73.2%\nimprovement in MAPE over Random Forest and Long Short-Term Memory neural\nnetwork, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 16:07:52 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 20:50:28 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 04:35:34 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 20:40:02 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Ali", "Sarwan", ""], ["Mansoor", "Haris", ""], ["Khan", "Imdadullah", ""], ["Arshad", "Naveed", ""], ["Khan", "Muhammad Asad", ""], ["Faizullah", "Safiullah", ""]]}, {"id": "1912.12482", "submitter": "Milan Cvitkovic", "authors": "Keng Wah Loon, Laura Graesser, Milan Cvitkovic", "title": "SLM Lab: A Comprehensive Benchmark and Modular Software Framework for\n  Reproducible Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SLM Lab, a software framework for reproducible reinforcement\nlearning (RL) research. SLM Lab implements a number of popular RL algorithms,\nprovides synchronous and asynchronous parallel experiment execution,\nhyperparameter search, and result analysis. RL algorithms in SLM Lab are\nimplemented in a modular way such that differences in algorithm performance can\nbe confidently ascribed to differences between algorithms, not between\nimplementations. In this work we present the design choices behind SLM Lab and\nuse it to produce a comprehensive single-codebase RL algorithm benchmark. In\naddition, as a consequence of SLM Lab's modular design, we introduce and\nevaluate a discrete-action variant of the Soft Actor-Critic algorithm (Haarnoja\net al., 2018) and a hybrid synchronous/asynchronous training method for RL\nagents.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 16:29:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Loon", "Keng Wah", ""], ["Graesser", "Laura", ""], ["Cvitkovic", "Milan", ""]]}, {"id": "1912.12502", "submitter": "Manuel Arias Chao", "authors": "Manuel Arias Chao, Bryan T. Adey, Olga Fink", "title": "Implicit supervision for fault detection and segmentation of emerging\n  fault types with Deep Variational Autoencoders", "comments": "22 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data-driven fault diagnostics of safety-critical systems often faces the\nchallenge of a complete lack of labeled data associated with faulty system\nconditions (i.e., fault types) at training time. Since an unknown number and\nnature of fault types can arise during deployment, data-driven fault\ndiagnostics in this scenario is an open-set learning problem. Most of the\nalgorithms for open-set diagnostics are one-class classification and\nunsupervised algorithms that do not leverage all the available labeled and\nunlabeled data in the learning algorithm. As a result, their fault detection\nand segmentation performance (i.e., identifying and separating faults of\ndifferent types) are sub-optimal. With this work, we propose training a\nvariational autoencoder (VAE) with labeled and unlabeled samples while inducing\nimplicit supervision on the latent representation of the healthy conditions.\nThis, together with a modified sampling process of VAE, creates a compact and\ninformative latent representation that allows good detection and segmentation\nof unseen fault types using existing one-class and clustering algorithms. We\nrefer to the proposed methodology as \"knowledge induced variational autoencoder\nwith adaptive sampling\" (KIL-AdaVAE). The fault detection and segmentation\ncapabilities of the proposed methodology are demonstrated in a new simulated\ncase study using the Advanced Geared Turbofan 30000 (AGTF30) dynamical model\nunder real flight conditions. In an extensive comparison, we demonstrate that\nthe proposed method outperforms other learning strategies (supervised learning,\nsupervised learning with embedding and semi-supervised learning) and deep\nlearning algorithms, yielding significant performance improvements on fault\ndetection and fault segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 18:40:33 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 19:23:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Chao", "Manuel Arias", ""], ["Adey", "Bryan T.", ""], ["Fink", "Olga", ""]]}, {"id": "1912.12510", "submitter": "Chandramouli Shama Sastry", "authors": "Chandramouli Shama Sastry, Sageev Oore", "title": "Detecting Out-of-Distribution Examples with In-distribution Examples and\n  Gram Matrices", "comments": "NeurIPS 2019 Workshop on Safety and Robustness in Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When presented with Out-of-Distribution (OOD) examples, deep neural networks\nyield confident, incorrect predictions. Detecting OOD examples is challenging,\nand the potential risks are high. In this paper, we propose to detect OOD\nexamples by identifying inconsistencies between activity patterns and class\npredicted. We find that characterizing activity patterns by Gram matrices and\nidentifying anomalies in gram matrix values can yield high OOD detection rates.\nWe identify anomalies in the gram matrices by simply comparing each value with\nits respective range observed over the training data. Unlike many approaches,\nthis can be used with any pre-trained softmax classifier and does not require\naccess to OOD data for fine-tuning hyperparameters, nor does it require OOD\naccess for inferring parameters. The method is applicable across a variety of\narchitectures and vision datasets and, for the important and surprisingly hard\ntask of detecting far-from-distribution out-of-distribution examples, it\ngenerally performs better than or equal to state-of-the-art OOD detection\nmethods (including those that do assume access to OOD examples).\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 19:44:03 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:17:55 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Sastry", "Chandramouli Shama", ""], ["Oore", "Sageev", ""]]}, {"id": "1912.12522", "submitter": "Antoine Yang", "authors": "Antoine Yang, Pedro M. Esperan\\c{c}a, Fabio M. Carlucci", "title": "NAS evaluation is frustratingly hard", "comments": "Published as a conference paper at ICLR2020; 13 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an exciting new field which promises to\nbe as much as a game-changer as Convolutional Neural Networks were in 2012.\nDespite many great works leading to substantial improvements on a variety of\ntasks, comparison between different methods is still very much an open issue.\nWhile most algorithms are tested on the same datasets, there is no shared\nexperimental protocol followed by all. As such, and due to the under-use of\nablation studies, there is a lack of clarity regarding why certain methods are\nmore effective than others. Our first contribution is a benchmark of $8$ NAS\nmethods on $5$ datasets. To overcome the hurdle of comparing methods with\ndifferent search spaces, we propose using a method's relative improvement over\nthe randomly sampled average architecture, which effectively removes advantages\narising from expertly engineered search spaces or training protocols.\nSurprisingly, we find that many NAS techniques struggle to significantly beat\nthe average architecture baseline. We perform further experiments with the\ncommonly used DARTS search space in order to understand the contribution of\neach component in the NAS pipeline. These experiments highlight that: (i) the\nuse of tricks in the evaluation protocol has a predominant impact on the\nreported performance of architectures; (ii) the cell-based search space has a\nvery narrow accuracy range, such that the seed has a considerable impact on\narchitecture rankings; (iii) the hand-designed macro-structure (cells) is more\nimportant than the searched micro-structure (operations); and (iv) the\ndepth-gap is a real phenomenon, evidenced by the change in rankings between $8$\nand $20$ cell architectures. To conclude, we suggest best practices, that we\nhope will prove useful for the community and help mitigate current NAS\npitfalls. The code used is available at\nhttps://github.com/antoyang/NAS-Benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:24:12 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 11:42:17 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 22:10:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yang", "Antoine", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Carlucci", "Fabio M.", ""]]}, {"id": "1912.12534", "submitter": "Charalampos Andriotis", "authors": "C.P. Andriotis, K.G. Papakonstantinou, E.N. Chatzi", "title": "Value of structural health information in partially observable\n  stochastic environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient integration of uncertain observations with decision-making\noptimization is key for prescribing informed intervention actions, able to\npreserve structural safety of deteriorating engineering systems. To this end,\nit is necessary that scheduling of inspection and monitoring strategies be\nobjectively performed on the basis of their expected value-based gains that,\namong others, reflect quantitative metrics such as the Value of Information\n(VoI) and the Value of Structural Health Monitoring (VoSHM). In this work, we\nintroduce and study the theoretical and computational foundations of the above\nmetrics within the context of Partially Observable Markov Decision Processes\n(POMDPs), thus alluding to a broad class of decision-making problems of\npartially observable stochastic deteriorating environments that can be modeled\nas POMDPs. Step-wise and life-cycle VoI and VoSHM definitions are devised and\ntheir bounds are analyzed as per the properties stemming from the Bellman\nequation and the resulting optimal value function. It is shown that a POMDP\npolicy inherently leverages the notion of VoI to guide observational actions in\nan optimal way at every decision step, and that the permanent or intermittent\ninformation provided by SHM or inspection visits, respectively, can only\nimprove the cost of this policy in the long-term, something that is not\nnecessarily true under locally optimal policies, typically adopted in\ndecision-making of structures and infrastructure. POMDP solutions are derived\nbased on point-based value iteration methods, and the various definitions are\nquantified in stationary and non-stationary deteriorating environments, with\nboth infinite and finite planning horizons, featuring single- or\nmulti-component engineering systems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 22:18:48 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:49:06 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Andriotis", "C. P.", ""], ["Papakonstantinou", "K. G.", ""], ["Chatzi", "E. N.", ""]]}, {"id": "1912.12557", "submitter": "Sima Behpour", "authors": "Sima Behpour", "title": "Active Learning in Video Tracking", "comments": null, "journal-ref": "In International Conference on Machine Learning, pp. 563-572. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning methods, like uncertainty sampling, combined with\nprobabilistic prediction techniques have achieved success in various problems\nlike image classification and text classification. For more complex\nmultivariate prediction tasks, the relationships between labels play an\nimportant role in designing structured classifiers with better performance.\nHowever, computational time complexity limits prevalent probabilistic methods\nfrom effectively supporting active learning. Specifically, while\nnon-probabilistic methods based on structured support vector machines can be\ntractably applied to predicting bipartite matchings, conditional random fields\nare intractable for these structures. We propose an adversarial approach for\nactive learning with structured prediction domains that is tractable for\nmatching. We evaluate this approach algorithmically in an important structured\nprediction problems: object tracking in videos. We demonstrate better accuracy\nand computational efficiency for our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:42:06 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 05:53:18 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 00:15:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Behpour", "Sima", ""]]}, {"id": "1912.12576", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Privacy-Preserving Public Release of Datasets for Support Vector Machine\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Big Data, 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of publicly releasing a dataset for support vector\nmachine classification while not infringing on the privacy of data subjects\n(i.e., individuals whose private information is stored in the dataset). The\ndataset is systematically obfuscated using an additive noise for privacy\nprotection. Motivated by the Cramer-Rao bound, inverse of the trace of the\nFisher information matrix is used as a measure of the privacy. Conditions are\nestablished for ensuring that the classifier extracted from the original\ndataset and the obfuscated one are close to each other (capturing the utility).\nThe optimal noise distribution is determined by maximizing a weighted sum of\nthe measures of privacy and utility. The optimal privacy-preserving noise is\nproved to achieve local differential privacy. The results are generalized to a\nbroader class of optimization-based supervised machine learning algorithms.\nApplicability of the methodology is demonstrated on multiple datasets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 03:32:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "1912.12612", "submitter": "Roy Fox", "authors": "Roy Fox, Richard Shin, William Paul, Yitian Zou, Dawn Song, Ken\n  Goldberg, Pieter Abbeel, Ion Stoica", "title": "Hierarchical Variational Imitation Learning of Control Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents can learn by imitating teacher demonstrations of the\nintended behavior. Hierarchical control policies are ubiquitously useful for\nsuch learning, having the potential to break down structured tasks into simpler\nsub-tasks, thereby improving data efficiency and generalization. In this paper,\nwe propose a variational inference method for imitation learning of a control\npolicy represented by parametrized hierarchical procedures (PHP), a\nprogram-like structure in which procedures can invoke sub-procedures to perform\nsub-tasks. Our method discovers the hierarchical structure in a dataset of\nobservation-action traces of teacher demonstrations, by learning an approximate\nposterior distribution over the latent sequence of procedure calls and\nterminations. Samples from this learned distribution then guide the training of\nthe hierarchical control policy. We identify and demonstrate a novel benefit of\nvariational inference in the context of hierarchical imitation learning: in\ndecomposing the policy into simpler procedures, inference can leverage acausal\ninformation that is unused by other methods. Training PHP with variational\ninference outperforms LSTM baselines in terms of data efficiency and\ngeneralization, requiring less than half as much data to achieve a 24% error\nrate in executing the bubble sort algorithm, and to achieve no error in\nexecuting Karel programs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 08:57:02 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Fox", "Roy", ""], ["Shin", "Richard", ""], ["Paul", "William", ""], ["Zou", "Yitian", ""], ["Song", "Dawn", ""], ["Goldberg", "Ken", ""], ["Abbeel", "Pieter", ""], ["Stoica", "Ion", ""]]}, {"id": "1912.12615", "submitter": "Anna Knezevic", "authors": "Anna Knezevic, Nikolai Dokuchaev", "title": "Approximating intractable short ratemodel distribution with neural\n  network", "comments": "Working on adding back the citations + figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm which predicts each subsequent time step relative to\nthe previous timestep of intractable short rate model (when adjusted for drift\nand overall distribution of previous percentile result) and show that the\nmethod achieves superior outcomes to the unbiased estimate both on the trained\ndataset and different validation data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 09:08:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 05:48:04 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 04:00:33 GMT"}, {"version": "v4", "created": "Sat, 18 Jan 2020 06:52:32 GMT"}, {"version": "v5", "created": "Wed, 5 Feb 2020 01:17:21 GMT"}, {"version": "v6", "created": "Tue, 11 Feb 2020 06:42:10 GMT"}, {"version": "v7", "created": "Sun, 23 Feb 2020 00:56:00 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Knezevic", "Anna", ""], ["Dokuchaev", "Nikolai", ""]]}, {"id": "1912.12616", "submitter": "Sherif Tarabishy", "authors": "Sherif Tarabishy, Stamatios Psarras, Marcin Kosicki, Martha Tsigkari", "title": "Deep learning surrogate models for spatial and visual connectivity", "comments": "Accepted manuscript in the International Journal of Architectural\n  Computing (2019)", "journal-ref": null, "doi": "10.1177/1478077119894483", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and visual connectivity are important metrics when developing\nworkplace layouts. Calculating those metrics in real-time can be difficult,\ndepending on the size of the floor plan being analysed and the resolution of\nthe analyses. This paper investigates the possibility of considerably speeding\nup the outcomes of such computationally intensive simulations by using machine\nlearning to create models capable of identifying the spatial and visual\nconnectivity potential of a space. To that end we present the entire process of\ninvestigating different machine learning models and a pipeline for training\nthem on such task, from the incorporation of a bespoke spatial and visual\nconnectivity analysis engine through a distributed computation pipeline, to the\nprocess of synthesizing training data and evaluating the performance of\ndifferent neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 09:17:19 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tarabishy", "Sherif", ""], ["Psarras", "Stamatios", ""], ["Kosicki", "Marcin", ""], ["Tsigkari", "Martha", ""]]}, {"id": "1912.12628", "submitter": "Jose Mena Rold\\'an", "authors": "Jos\\'e Mena, Oriol Pujol, Jordi Vitri\\`a", "title": "Dirichlet uncertainty wrappers for actionable algorithm accuracy\n  accountability and auditability", "comments": "13 pages, 5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the use of machine learning models is becoming a utility in many\napplications. Companies deliver pre-trained models encapsulated as application\nprogramming interfaces (APIs) that developers combine with third party\ncomponents and their own models and data to create complex data products to\nsolve specific problems. The complexity of such products and the lack of\ncontrol and knowledge of the internals of each component used cause unavoidable\neffects, such as lack of transparency, difficulty in auditability, and\nemergence of potential uncontrolled risks. They are effectively black-boxes.\nAccountability of such solutions is a challenge for the auditors and the\nmachine learning community. In this work, we propose a wrapper that given a\nblack-box model enriches its output prediction with a measure of uncertainty.\nBy using this wrapper, we make the black-box auditable for the accuracy risk\n(risk derived from low quality or uncertain decisions) and at the same time we\nprovide an actionable mechanism to mitigate that risk in the form of decision\nrejection; we can choose not to issue a prediction when the risk or uncertainty\nin that decision is significant. Based on the resulting uncertainty measure, we\nadvocate for a rejection system that selects the more confident predictions,\ndiscarding those more uncertain, leading to an improvement in the trustability\nof the resulting system. We showcase the proposed technique and methodology in\na practical scenario where a simulated sentiment analysis API based on natural\nlanguage processing is applied to different domains. Results demonstrate the\neffectiveness of the uncertainty computed by the wrapper and its high\ncorrelation to bad quality predictions and misclassifications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:05:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mena", "Jos\u00e9", ""], ["Pujol", "Oriol", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1912.12630", "submitter": "Pooyan Fazli", "authors": "Yuxiang Sun and Pooyan Fazli", "title": "Real-time Policy Distillation in Deep Reinforcement Learning", "comments": "In Proceedings of the Workshop on ML for Systems, Thirty-third\n  Conference on Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy distillation in deep reinforcement learning provides an effective way\nto transfer control policies from a larger network to a smaller untrained\nnetwork without a significant degradation in performance. However, policy\ndistillation is underexplored in deep reinforcement learning, and existing\napproaches are computationally inefficient, resulting in a long distillation\ntime. In addition, the effectiveness of the distillation process is still\nlimited to the model capacity. We propose a new distillation mechanism, called\nreal-time policy distillation, in which training the teacher model and\ndistilling the policy to the student model occur simultaneously. Accordingly,\nthe teacher's latest policy is transferred to the student model in real time.\nThis reduces the distillation time to half the original time or even less and\nalso makes it possible for extremely small student models to learn skills at\nthe expert level. We evaluated the proposed algorithm in the Atari 2600 domain.\nThe results show that our approach can achieve full distillation in most games,\neven with compression ratios up to 1.7%.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:10:37 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sun", "Yuxiang", ""], ["Fazli", "Pooyan", ""]]}, {"id": "1912.12693", "submitter": "Federico Errica", "authors": "Davide Bacciu, Federico Errica, Alessio Micheli, Marco Podda", "title": "A Gentle Introduction to Deep Learning for Graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2020.06.006", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive processing of graph data is a long-standing research topic which\nhas been lately consolidated as a theme of major interest in the deep learning\ncommunity. The snap increase in the amount and breadth of related research has\ncome at the price of little systematization of knowledge and attention to\nearlier literature. This work is designed as a tutorial introduction to the\nfield of deep learning for graphs. It favours a consistent and progressive\nintroduction of the main concepts and architectural aspects over an exposition\nof the most recent literature, for which the reader is referred to available\nsurveys. The paper takes a top-down view to the problem, introducing a\ngeneralized formulation of graph representation learning based on a local and\niterative approach to structured information processing. It introduces the\nbasic building blocks that can be combined to design novel and effective neural\nmodels for graphs. The methodological exposition is complemented by a\ndiscussion of interesting research challenges and applications in the field.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 16:43:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 07:29:38 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Bacciu", "Davide", ""], ["Errica", "Federico", ""], ["Micheli", "Alessio", ""], ["Podda", "Marco", ""]]}, {"id": "1912.12716", "submitter": "Zhaoxian Wu", "authors": "Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B. Giannakis", "title": "Federated Variance-Reduced Stochastic Gradient Descent with Robustness\n  to Byzantine Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with distributed finite-sum optimization for learning over\nnetworks in the presence of malicious Byzantine attacks. To cope with such\nattacks, most resilient approaches so far combine stochastic gradient descent\n(SGD) with different robust aggregation rules. However, the sizeable\nSGD-induced stochastic gradient noise makes it challenging to distinguish\nmalicious messages sent by the Byzantine attackers from noisy stochastic\ngradients sent by the 'honest' workers. This motivates us to reduce the\nvariance of stochastic gradients as a means of robustifying SGD in the presence\nof Byzantine attacks. To this end, the present work puts forth a Byzantine\nattack resilient distributed (Byrd-) SAGA approach for learning tasks involving\nfinite-sum optimization over networks. Rather than the mean employed by\ndistributed SAGA, the novel Byrd- SAGA relies on the geometric median to\naggregate the corrected stochastic gradients sent by the workers. When less\nthan half of the workers are Byzantine attackers, the robustness of geometric\nmedian to outliers enables Byrd-SAGA to attain provably linear convergence to a\nneighborhood of the optimal solution, with the asymptotic learning error\ndetermined by the number of Byzantine workers. Numerical tests corroborate the\nrobustness to various Byzantine attacks, as well as the merits of Byrd- SAGA\nover Byzantine attack resilient distributed SGD.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 19:46:03 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 07:34:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wu", "Zhaoxian", ""], ["Ling", "Qing", ""], ["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1912.12719", "submitter": "Mirza Rami\\v{c}i\\'c", "authors": "Mirza Ramicic, Andrea Bonarini", "title": "Augmented Replay Memory in Reinforcement Learning With Continuous\n  Control", "comments": null, "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems (2021)\n  1-12", "doi": "10.1109/TCDS.2021.3050723", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reinforcement learning agents are currently able to process an\nincreasing amount of data by converting it into a higher order value functions.\nThis expansion of the information collected from the environment increases the\nagent's state space enabling it to scale up to a more complex problems but also\nincreases the risk of forgetting by learning on redundant or conflicting data.\nTo improve the approximation of a large amount of data, a random mini-batch of\nthe past experiences that are stored in the replay memory buffer is often\nreplayed at each learning step. The proposed work takes inspiration from a\nbiological mechanism which act as a protective layer of human brain higher\ncognitive functions: active memory consolidation mitigates the effect of\nforgetting of previous memories by dynamically processing the new ones. The\nsimilar dynamics are implemented by a proposed augmented memory replay AMR\ncapable of optimizing the replay of the experiences from the agent's memory\nstructure by altering or augmenting their relevance. Experimental results show\nthat an evolved AMR augmentation function capable of increasing the\nsignificance of the specific memories is able to further increase the stability\nand convergence speed of the learning algorithms dealing with the complexity of\ncontinuous action domains.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 20:07:18 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ramicic", "Mirza", ""], ["Bonarini", "Andrea", ""]]}, {"id": "1912.12749", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov, David Saad", "title": "Scalable Influence Estimation Without Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.dis-nn math.PR physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a diffusion process on a network, how many nodes are expected to be\ninfluenced by a set of initial spreaders? This natural problem, often referred\nto as influence estimation, boils down to computing the marginal probability\nthat a given node is active at a given time when the process starts from\nspecified initial condition. Among many other applications, this task is\ncrucial for a well-studied problem of influence maximization: finding optimal\nspreaders in a social network that maximize the influence spread by a certain\ntime horizon. Indeed, influence estimation needs to be called multiple times\nfor comparing candidate seed sets. Unfortunately, in many models of interest an\nexact computation of marginals is #P-hard. In practice, influence is often\nestimated using Monte-Carlo sampling methods that require a large number of\nruns for obtaining a high-fidelity prediction, especially at large times. It is\nthus desirable to develop analytic techniques as an alternative to sampling\nmethods. Here, we suggest an algorithm for estimating the influence function in\npopular independent cascade model based on a scalable dynamic message-passing\napproach. This method has a computational complexity of a single Monte-Carlo\nsimulation and provides an upper bound on the expected spread on a general\ngraph, yielding exact answer for treelike networks. We also provide dynamic\nmessage-passing equations for a stochastic version of the linear threshold\nmodel. The resulting saving of a potentially large sampling factor in the\nrunning time compared to simulation-based techniques hence makes it possible to\naddress large-scale problem instances.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 22:15:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lokhov", "Andrey Y.", ""], ["Saad", "David", ""]]}, {"id": "1912.12761", "submitter": "Hussain Mohammed Kabir Mr", "authors": "H M Dipu Kabir, Abbas Khosravi, Abdollah Kavousi-Fard, Saeid\n  Nahavandi, Dipti Srinivasan", "title": "Optimal Uncertainty-guided Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The neural network (NN)-based direct uncertainty quantification (UQ) methods\nhave achieved the state of the art performance since the first inauguration,\nknown as the lower-upper-bound estimation (LUBE) method. However,\ncurrently-available cost functions for uncertainty guided NN training are not\nalways converging and all converged NNs are not generating optimized prediction\nintervals (PIs). Moreover, several groups have proposed different quality\ncriteria for PIs. These raise a question about their relative effectiveness.\nMost of the existing cost functions of uncertainty guided NN training are not\ncustomizable and the convergence of training is uncertain. Therefore, in this\npaper, we propose a highly customizable smooth cost function for developing NNs\nto construct optimal PIs. The optimized average width of PIs, PI-failure\ndistances and the PI coverage probability (PICP) are computed for the test\ndataset. The performance of the proposed method is examined for the wind power\ngeneration and the electricity demand data. Results show that the proposed\nmethod reduces variation in the quality of PIs, accelerates the training, and\nimproves convergence probability from 99.2% to 99.8%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 00:03:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kabir", "H M Dipu", ""], ["Khosravi", "Abbas", ""], ["Kavousi-Fard", "Abdollah", ""], ["Nahavandi", "Saeid", ""], ["Srinivasan", "Dipti", ""]]}, {"id": "1912.12766", "submitter": "Raman Arora", "authors": "Nils Holzenberger and Raman Arora", "title": "Multiview Representation Learning for a Union of Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a popular technique for learning\nrepresentations that are maximally correlated across multiple views in data. In\nthis paper, we extend the CCA based framework for learning a multiview mixture\nmodel. We show that the proposed model and a set of simple heuristics yield\nimprovements over standard CCA, as measured in terms of performance on\ndownstream tasks. Our experimental results show that our correlation-based\nobjective meaningfully generalizes the CCA objective to a mixture of CCA\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 00:44:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Holzenberger", "Nils", ""], ["Arora", "Raman", ""]]}, {"id": "1912.12773", "submitter": "Karl Schmeckpeper", "authors": "Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen Tian, Kostas\n  Daniilidis, Sergey Levine, Chelsea Finn", "title": "Learning Predictive Models From Observation and Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning predictive models from interaction with the world allows an agent,\nsuch as a robot, to learn about how the world works, and then use this learned\nmodel to plan coordinated sequences of actions to bring about desired outcomes.\nHowever, learning a model that captures the dynamics of complex skills\nrepresents a major challenge: if the agent needs a good model to perform these\nskills, it might never be able to collect the experience on its own that is\nrequired to learn these delicate and complex behaviors. Instead, we can imagine\naugmenting the training set with observational data of other agents, such as\nhumans. Such data is likely more plentiful, but represents a different\nembodiment. For example, videos of humans might show a robot how to use a tool,\nbut (i) are not annotated with suitable robot actions, and (ii) contain a\nsystematic distributional shift due to the embodiment differences between\nhumans and robots. We address the first challenge by formulating the\ncorresponding graphical model and treating the action as an observed variable\nfor the interaction data and an unobserved variable for the observation data,\nand the second challenge by using a domain-dependent prior. In addition to\ninteraction data, our method is able to leverage videos of passive observations\nin a driving dataset and a dataset of robotic manipulation videos. A robotic\nplanning agent equipped with our method can learn to use tools in a tabletop\nrobotic manipulation setting by observing humans without ever seeing a robotic\nvideo of tool use.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 01:10:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Schmeckpeper", "Karl", ""], ["Xie", "Annie", ""], ["Rybkin", "Oleh", ""], ["Tian", "Stephen", ""], ["Daniilidis", "Kostas", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1912.12777", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Lei Wu", "title": "Machine Learning from a Continuous Viewpoint", "comments": "published version", "journal-ref": "Science China Mathematics (2020)", "doi": "10.1007/s11425-020-1773-8", "report-no": null, "categories": "math.NA cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a continuous formulation of machine learning, as a problem in the\ncalculus of variations and differential-integral equations, in the spirit of\nclassical numerical analysis. We demonstrate that conventional machine learning\nmodels and algorithms, such as the random feature model, the two-layer neural\nnetwork model and the residual neural network model, can all be recovered (in a\nscaled form) as particular discretizations of different continuous\nformulations. We also present examples of new models, such as the flow-based\nrandom feature model, and new algorithms, such as the smoothed particle method\nand spectral method, that arise naturally from this continuous formulation. We\ndiscuss how the issues of generalization error and implicit regularization can\nbe studied under this framework.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 01:38:18 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 04:15:09 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wu", "Lei", ""]]}, {"id": "1912.12795", "submitter": "Qinghe Jing", "authors": "Qinghe Jing, Weiyan Wang, Junxue Zhang, Han Tian, Kai Chen", "title": "Quantifying the Performance of Federated Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of data and isolated data islands encourage different\norganizations to share data with each other to train machine learning models.\nHowever, there are increasing concerns on the problems of data privacy and\nsecurity, which urges people to seek a solution like Federated Transfer\nLearning (FTL) to share training data without violating data privacy. FTL\nleverages transfer learning techniques to utilize data from different sources\nfor training, while achieving data privacy protection without significant\naccuracy loss. However, the benefits come with a cost of extra computation and\ncommunication consumption, resulting in efficiency problems. In order to\nefficiently deploy and scale up FTL solutions in practice, we need a deep\nunderstanding on how the infrastructure affects the efficiency of FTL. Our\npaper tries to answer this question by quantitatively measuring a real-world\nFTL implementation FATE on Google Cloud. According to the results of carefully\ndesigned experiments, we verified that the following bottlenecks can be further\noptimized: 1) Inter-process communication is the major bottleneck; 2) Data\nencryption adds considerable computation overhead; 3) The Internet networking\ncondition affects the performance a lot when the model is large.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:10:00 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jing", "Qinghe", ""], ["Wang", "Weiyan", ""], ["Zhang", "Junxue", ""], ["Tian", "Han", ""], ["Chen", "Kai", ""]]}, {"id": "1912.12818", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, William Yang Wang", "title": "Disentangled Representation Learning with Wasserstein Total Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of disentangled representations involves uncovering of\ndifferent factors of variations that contribute to the data generation process.\nTotal correlation penalization has been a key component in recent methods\ntowards disentanglement. However, Kullback-Leibler (KL) divergence-based total\ncorrelation is metric-agnostic and sensitive to data samples. In this paper, we\nintroduce Wasserstein total correlation in both variational autoencoder and\nWasserstein autoencoder settings to learn disentangled latent representations.\nA critic is adversarially trained along with the main objective to estimate the\nWasserstein total correlation term. We discuss the benefits of using\nWasserstein distance over KL divergence to measure independence and conduct\nquantitative and qualitative experiments on several data sets. Moreover, we\nintroduce a new metric to measure disentanglement. We show that the proposed\napproach has comparable performances on disentanglement with smaller sacrifices\nin reconstruction abilities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 05:31:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "1912.12825", "submitter": "Bo Zhang", "authors": "Jixiang Li, Chuming Liang, Bo Zhang, Zhao Wang, Fei Xiang, Xiangxiang\n  Chu", "title": "Neural Architecture Search on Acoustic Scene Classification", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are widely adopted in Acoustic Scene\nClassification (ASC) tasks, but they generally carry a heavy computational\nburden. In this work, we propose a lightweight yet high-performing baseline\nnetwork inspired by MobileNetV2, which replaces square convolutional kernels\nwith unidirectional ones to extract features alternately in temporal and\nfrequency dimensions. Furthermore, we explore a dynamic architecture space\nbuilt on the basis of the proposed baseline with the recent Neural Architecture\nSearch (NAS) paradigm, which first trains a supernet that incorporates all\ncandidate networks and then applies a well-known evolutionary algorithm NSGA-II\nto discover more efficient networks with higher accuracy and lower\ncomputational cost. Experimental results demonstrate that our searched network\nis competent in ASC tasks, which achieves 90.3% F1-score on the DCASE2018 task\n5 evaluation set, marking a new state-of-the-art performance while saving 25%\nof FLOPs compared to our baseline network.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 06:35:12 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 04:58:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Li", "Jixiang", ""], ["Liang", "Chuming", ""], ["Zhang", "Bo", ""], ["Wang", "Zhao", ""], ["Xiang", "Fei", ""], ["Chu", "Xiangxiang", ""]]}, {"id": "1912.12834", "submitter": "Andrew Wilson", "authors": "Ian A. Delbridge, David S. Bindel, Andrew Gordon Wilson", "title": "Randomly Projected Additive Gaussian Processes for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide flexible distributions over functions, with\ninductive biases controlled by a kernel. However, in many applications Gaussian\nprocesses can struggle with even moderate input dimensionality. Learning a low\ndimensional projection can help alleviate this curse of dimensionality, but\nintroduces many trainable hyperparameters, which can be cumbersome, especially\nin the small data regime. We use additive sums of kernels for GP regression,\nwhere each kernel operates on a different random projection of its inputs.\nSurprisingly, we find that as the number of random projections increases, the\npredictive performance of this approach quickly converges to the performance of\na kernel operating on the original full dimensional inputs, over a wide range\nof data sets, even if we are projecting into a single dimension. As a\nconsequence, many problems can remarkably be reduced to one dimensional input\nspaces, without learning a transformation. We prove this convergence and its\nrate, and additionally propose a deterministic approach that converges more\nquickly than purely random projections. Moreover, we demonstrate our approach\ncan achieve faster inference and improved predictive accuracy for\nhigh-dimensional inputs compared to kernels in the original input space.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 07:26:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Delbridge", "Ian A.", ""], ["Bindel", "David S.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1912.12844", "submitter": "Xianfeng Liang", "authors": "Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen,\n  Yifei Cheng", "title": "Variance Reduced Local SGD with Lower Communication Complexity", "comments": "25 pages, 6 figures. The paper presents a novel variance reduction\n  algorithm for Local SGD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate the training of machine learning models, distributed stochastic\ngradient descent (SGD) and its variants have been widely adopted, which apply\nmultiple workers in parallel to speed up training. Among them, Local SGD has\ngained much attention due to its lower communication cost. Nevertheless, when\nthe data distribution on workers is non-identical, Local SGD requires\n$O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its\n\\emph{linear iteration speedup} property, where $T$ is the total number of\niterations and $N$ is the number of workers. In this paper, we propose Variance\nReduced Local SGD (VRL-SGD) to further reduce the communication complexity.\nBenefiting from eliminating the dependency on the gradient variance among\nworkers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration\nspeedup} with a lower communication complexity $O(T^{\\frac{1}{2}}\nN^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct\nexperiments on three machine learning tasks, and the experimental results\ndemonstrate that VRL-SGD performs impressively better than Local SGD when the\ndata among workers are quite diverse.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 08:15:21 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Liang", "Xianfeng", ""], ["Shen", "Shuheng", ""], ["Liu", "Jingchang", ""], ["Pan", "Zhen", ""], ["Chen", "Enhong", ""], ["Cheng", "Yifei", ""]]}, {"id": "1912.12854", "submitter": "Xi Lin", "authors": "Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, Sam Kwong", "title": "Pareto Multi-Task Learning", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a powerful method for solving multiple correlated\ntasks simultaneously. However, it is often impossible to find one single\nsolution to optimize all the tasks, since different tasks might conflict with\neach other. Recently, a novel method is proposed to find one single Pareto\noptimal solution with good trade-off among different tasks by casting\nmulti-task learning as multiobjective optimization. In this paper, we\ngeneralize this idea and propose a novel Pareto multi-task learning algorithm\n(Pareto MTL) to find a set of well-distributed Pareto solutions which can\nrepresent different trade-offs among different tasks. The proposed algorithm\nfirst formulates a multi-task learning problem as a multiobjective optimization\nproblem, and then decomposes the multiobjective optimization problem into a set\nof constrained subproblems with different trade-off preferences. By solving\nthese subproblems in parallel, Pareto MTL can find a set of well-representative\nPareto optimal solutions with different trade-off among all tasks.\nPractitioners can easily select their preferred solution from these Pareto\nsolutions, or use different trade-off solutions for different situations.\nExperimental results confirm that the proposed algorithm can generate\nwell-representative solutions and outperform some state-of-the-art algorithms\non many multi-task learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 08:58:40 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lin", "Xi", ""], ["Zhen", "Hui-Ling", ""], ["Li", "Zhenhua", ""], ["Zhang", "Qingfu", ""], ["Kwong", "Sam", ""]]}, {"id": "1912.12864", "submitter": "Michael Lechner", "authors": "Bart Cockx, Michael Lechner, Joost Bollens", "title": "Priority to unemployed immigrants? A causal machine learning evaluation\n  of training in Belgium", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on administrative data of unemployed in Belgium, we estimate the labour\nmarket effects of three training programmes at various aggregation levels using\nModified Causal Forests, a causal machine learning estimator. While all\nprogrammes have positive effects after the lock-in period, we find substantial\nheterogeneity across programmes and unemployed. Simulations show that\n'black-box' rules that reassign unemployed to programmes that maximise\nestimated individual gains can considerably improve effectiveness: up to 20\npercent more (less) time spent in (un)employment within a 30 months window. A\nshallow policy tree delivers a simple rule that realizes about 70 percent of\nthis gain.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 09:44:34 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 05:56:00 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Cockx", "Bart", ""], ["Lechner", "Michael", ""], ["Bollens", "Joost", ""]]}, {"id": "1912.12867", "submitter": "Martin Spindler", "authors": "Xi Chen, Ye Luo, Martin Spindler", "title": "Adaptive Discrete Smoothing for High-Dimensional and Nonlinear Panel\n  Data", "comments": "18 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a data-driven smoothing technique for\nhigh-dimensional and non-linear panel data models. We allow for individual\nspecific (non-linear) functions and estimation with econometric or machine\nlearning methods by using weighted observations from other individuals. The\nweights are determined by a data-driven way and depend on the similarity\nbetween the corresponding functions and are measured based on initial\nestimates. The key feature of such a procedure is that it clusters individuals\nbased on the distance / similarity between them, estimated in a first stage.\nOur estimation method can be combined with various statistical estimation\nprocedures, in particular modern machine learning methods which are in\nparticular fruitful in the high-dimensional case and with complex,\nheterogeneous data. The approach can be interpreted as a \\textquotedblleft\nsoft-clustering\\textquotedblright\\ in comparison to\ntraditional\\textquotedblleft\\ hard clustering\\textquotedblright that assigns\neach individual to exactly one group. We conduct a simulation study which shows\nthat the prediction can be greatly improved by using our estimator. Finally, we\nanalyze a big data set from didichuxing.com, a leading company in\ntransportation industry, to analyze and predict the gap between supply and\ndemand based on a large set of covariates. Our estimator clearly performs much\nbetter in out-of-sample prediction compared to existing linear panel data\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 09:50:58 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 16:39:10 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Chen", "Xi", ""], ["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1912.12879", "submitter": "Alice Lucas", "authors": "Alice Lucas, Santiago Lopez-Tapia, Rafael Molina and Aggelos K.\n  Katsaggelos", "title": "Self-supervised Fine-tuning for Correcting Super-Resolution\n  Convolutional Neural Networks", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Convolutional Neural Networks (CNNs) trained for image and video\nsuper-resolution (SR) regularly achieve new state-of-the-art performance, they\nalso suffer from significant drawbacks. One of their limitations is their lack\nof robustness to unseen image formation models during training. Other\nlimitations include the generation of artifacts and hallucinated content when\ntraining Generative Adversarial Networks (GANs) for SR. While the Deep Learning\nliterature focuses on presenting new training schemes and settings to resolve\nthese various issues, we show that one can avoid training and correct for SR\nresults with a fully self-supervised fine-tuning approach. More specifically,\nat test time, given an image and its known image formation model, we fine-tune\nthe parameters of the trained network and iteratively update them using a data\nfidelity loss. We apply our fine-tuning algorithm on multiple image and video\nSR CNNs and show that it can successfully correct for a sub-optimal SR solution\nby entirely relying on internal learning at test time. We apply our method on\nthe problem of fine-tuning for unseen image formation models and on removal of\nartifacts introduced by GANs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:02:58 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 13:37:29 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 12:11:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lucas", "Alice", ""], ["Lopez-Tapia", "Santiago", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1912.12912", "submitter": "Julia Moosbauer", "authors": "Martin Binder, Julia Moosbauer, Janek Thomas, Bernd Bischl", "title": "Multi-Objective Hyperparameter Tuning and Feature Selection using Filter\n  Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Both feature selection and hyperparameter tuning are key tasks in machine\nlearning. Hyperparameter tuning is often useful to increase model performance,\nwhile feature selection is undertaken to attain sparse models. Sparsity may\nyield better model interpretability and lower cost of data acquisition, data\nhandling and model inference. While sparsity may have a beneficial or\ndetrimental effect on predictive performance, a small drop in performance may\nbe acceptable in return for a substantial gain in sparseness. We therefore\ntreat feature selection as a multi-objective optimization task. We perform\nhyperparameter tuning and feature selection simultaneously because the choice\nof features of a model may influence what hyperparameters perform well.\n  We present, benchmark, and compare two different approaches for\nmulti-objective joint hyperparameter optimization and feature selection: The\nfirst uses multi-objective model-based optimization. The second is an\nevolutionary NSGA-II-based wrapper approach to feature selection which\nincorporates specialized sampling, mutation and recombination operators. Both\nmethods make use of parameterized filter ensembles.\n  While model-based optimization needs fewer objective evaluations to achieve\ngood performance, it incurs computational overhead compared to the NSGA-II, so\nthe preferred choice depends on the cost of evaluating a model on given data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:04:06 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 10:41:13 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Binder", "Martin", ""], ["Moosbauer", "Julia", ""], ["Thomas", "Janek", ""], ["Bischl", "Bernd", ""]]}, {"id": "1912.12923", "submitter": "Shi-Ju Ran", "authors": "Shi-Ju Ran", "title": "Bayesian Tensor Network with Polynomial Complexity for Probabilistic\n  Machine Learning", "comments": "7 pages, 5 figures; in the second version, results of the BTN with a\n  new structure were added; other modifications including the formulation of\n  Bayes' equation in tensor forms were made", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.str-el cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that describing or calculating the conditional probabilities of\nmultiple events is exponentially expensive. In this work, Bayesian tensor\nnetwork (BTN) is proposed to efficiently capture the conditional probabilities\nof multiple sets of events with polynomial complexity. BTN is a directed\nacyclic graphical model that forms a subset of TN. To testify its validity for\nexponentially many events, BTN is implemented to the image recognition, where\nthe classification is mapped to capturing the conditional probabilities in an\nexponentially large sample space. Competitive performance is achieved by the\nBTN with simple tree network structures. Analogous to the tensor network\nsimulations of quantum systems, the validity of the simple-tree BTN implies an\n``area law'' of fluctuations in image recognition problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:37:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:36:54 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ran", "Shi-Ju", ""]]}, {"id": "1912.12927", "submitter": "Lei Feng", "authors": "Lei Feng, Takuo Kaneko, Bo Han, Gang Niu, Bo An, Masashi Sugiyama", "title": "Learning with Multiple Complementary Labels", "comments": "ICML 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complementary label (CL) simply indicates an incorrect class of an example,\nbut learning with CLs results in multi-class classifiers that can predict the\ncorrect class. Unfortunately, the problem setting only allows a single CL for\neach example, which notably limits its potential since our labelers may easily\nidentify multiple CLs (MCLs) to one example. In this paper, we propose a novel\nproblem setting to allow MCLs for each example and two ways for learning with\nMCLs. In the first way, we design two wrappers that decompose MCLs into many\nsingle CLs, so that we could use any method for learning with CLs. However, the\nsupervision information that MCLs hold is conceptually diluted after\ndecomposition. Thus, in the second way, we derive an unbiased risk estimator;\nminimizing it processes each set of MCLs as a whole and possesses an estimation\nerror bound. We further improve the second way into minimizing properly chosen\nupper bounds. Experiments show that the former way works well for learning with\nMCLs but the latter is even better.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:50:51 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 04:45:52 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 08:50:50 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Feng", "Lei", ""], ["Kaneko", "Takuo", ""], ["Han", "Bo", ""], ["Niu", "Gang", ""], ["An", "Bo", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1912.12941", "submitter": "Kilian Hendrickx", "authors": "Kilian Hendrickx, Wannes Meert, Yves Mollet, Johan Gyselinck, Bram\n  Cornelis, Konstantinos Gryllias, Jesse Davis", "title": "A general anomaly detection framework for fleet-based condition\n  monitoring of machines", "comments": "Accepted in Mechanical Systems and Signal Processing, SI: Machine\n  Diagnostics by AI", "journal-ref": null, "doi": "10.1016/j.ymssp.2019.106585", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine failures decrease up-time and can lead to extra repair costs or even\nto human casualties and environmental pollution. Recent condition monitoring\ntechniques use artificial intelligence in an effort to avoid time-consuming\nmanual analysis and handcrafted feature extraction. Many of these only analyze\na single machine and require a large historical data set. In practice, this can\nbe difficult and expensive to collect. However, some industrial condition\nmonitoring applications involve a fleet of similar operating machines. In most\nof these applications, it is safe to assume healthy conditions for the majority\nof machines. Deviating machine behavior is then an indicator for a machine\nfault. This work proposes an unsupervised, generic, anomaly detection framework\nfor fleet-based condition monitoring. It uses generic building blocks and\noffers three key advantages. First, a historical data set is not required due\nto online fleet-based comparisons. Second, it allows incorporating domain\nexpertise by user-defined comparison measures. Finally, contrary to most\nblack-box artificial intelligence techniques, easy interpretability allows a\ndomain expert to validate the predictions made by the framework. Two use-cases\non an electrical machine fleet demonstrate the applicability of the framework\nto detect a voltage unbalance by means of electrical and vibration signatures.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:35:45 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 11:10:06 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 11:06:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hendrickx", "Kilian", ""], ["Meert", "Wannes", ""], ["Mollet", "Yves", ""], ["Gyselinck", "Johan", ""], ["Cornelis", "Bram", ""], ["Gryllias", "Konstantinos", ""], ["Davis", "Jesse", ""]]}, {"id": "1912.12945", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Xiaojie Mao, Masatoshi Uehara", "title": "Localized Debiased Machine Learning: Efficient Inference on Quantile\n  Treatment Effects and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the efficient estimation of a low-dimensional parameter in an\nestimating equation involving high-dimensional nuisances that depend on the\nparameter of interest. An important example is the (local) quantile treatment\neffect ((L)QTE) in causal inference, for which the efficient estimating\nequation involves as a nuisance the covariate-conditional cumulative\ndistribution function evaluated at the quantile to be estimated. Debiased\nmachine learning (DML) is a data-splitting approach to address the need to\nestimate nuisances using flexible machine learning methods that may not satisfy\nstrong metric entropy conditions, but applying it to problems with\nparameter-dependent nuisances is impractical. For (L)QTE estimation, DML\nrequires we learn the whole conditional cumulative distribution function,\nconditioned on potentially high-dimensional covariates, which is far more\nchallenging than the standard supervised regression task in machine learning.\nWe instead propose localized debiased machine learning (LDML), a new\ndata-splitting approach that avoids this burdensome step and needs only\nestimate the nuisances at a single initial rough guess for the parameter. For\n(L)QTE estimation, this involves just learning two binary regression (i.e.,\nclassification) models, for which many standard, time-tested machine learning\nmethods exist, and the initial rough guess may be given by inverse propensity\nweighting. We prove that under lax rate conditions on nuisances, our estimator\nhas the same favorable asymptotic behavior as the infeasible oracle estimator\nthat solves the estimating equation with the unknown true nuisance functions.\nThus, our proposed approach uniquely enables practically-feasible and\ntheoretically-grounded efficient estimation of important quantities in causal\ninference such as (L)QTEs and in other coarsened data settings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:42:52 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 17:02:07 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 16:50:56 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 20:23:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Uehara", "Masatoshi", ""]]}, {"id": "1912.12969", "submitter": "Luca Guastoni", "authors": "L. Guastoni, M. P. Encinar, P. Schlatter, H. Azizpour, R. Vinuesa", "title": "Prediction of wall-bounded turbulence from wall quantities using\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1522/1/012022", "report-no": null, "categories": "physics.flu-dyn physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully-convolutional neural-network model is used to predict the streamwise\nvelocity fields at several wall-normal locations by taking as input the\nstreamwise and spanwise wall-shear-stress planes in a turbulent open channel\nflow. The training data are generated by performing a direct numerical\nsimulation (DNS) at a friction Reynolds number of $Re_{\\tau}=180$. Various\nnetworks are trained for predictions at three inner-scaled locations ($y^+ =\n15,~30,~50$) and for different time steps between input samples $\\Delta\nt^{+}_{s}$. The inherent non-linearity of the neural-network model enables a\nbetter prediction capability than linear methods, with a lower error in both\nthe instantaneous flow fields and turbulent statistics. Using a dataset with\nhigher $\\Delta t^+_{s}$ improves the generalization at all the considered\nwall-normal locations, as long as the network capacity is sufficient to\ngeneralize over the dataset. The use of a multiple-output network, with\nparallel dedicated branches for two wall-normal locations, does not provide any\nimprovement over two separated single-output networks, other than a moderate\nsaving in training time. Training time can be effectively reduced, by a factor\nof 4, via a transfer learning method that initializes the network parameters\nusing the optimized parameters of a previously-trained network.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 15:34:41 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Guastoni", "L.", ""], ["Encinar", "M. P.", ""], ["Schlatter", "P.", ""], ["Azizpour", "H.", ""], ["Vinuesa", "R.", ""]]}, {"id": "1912.12976", "submitter": "Sebastian Kaltenbach", "authors": "Sebastian Kaltenbach, Phaedon-Stelios Koutsourelakis", "title": "Incorporating physical constraints in a deep probabilistic machine\n  learning framework for coarse-graining dynamical systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109673", "report-no": null, "categories": "physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-based discovery of effective, coarse-grained (CG) models of\nhigh-dimensional dynamical systems presents a unique challenge in computational\nphysics and particularly in the context of multiscale problems. The present\npaper offers a data-based, probablistic perspective that enables the\nquantification of predictive uncertainties. One of the outstanding problems has\nbeen the introduction of physical constraints in the probabilistic machine\nlearning objectives. The primary utility of such constraints stems from the\nundisputed physical laws such as conservation of mass, energy etc. that they\nrepresent. Furthermore and apart from leading to physically realistic\npredictions, they can significantly reduce the requisite amount of training\ndata which for high-dimensional, multiscale systems are expensive to obtain\n(Small Data regime). We formulate the coarse-graining process by employing a\nprobabilistic state-space model and account for the aforementioned equality\nconstraints as virtual observables in the associated densities. We demonstrate\nhow probabilistic inference tools can be employed to identify the\ncoarse-grained variables in combination with deep neural nets and their\nevolution model without ever needing to define a fine-to-coarse (restriction)\nprojection and without needing time-derivatives of state variables.\nFurthermore, it is capable of reconstructing the evolution of the full,\nfine-scale system and therefore the observables of interest need not be\nselected a priori. We demonstrate the efficacy of the proposed framework by\napplying it to systems of interacting particles and an image-series of a\nnonlinear pendulum.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:07:46 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 10:31:19 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 20:12:22 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 13:17:29 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kaltenbach", "Sebastian", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1912.12979", "submitter": "Corinne Jones", "authors": "Corinne Jones, Vincent Roulet, Zaid Harchaoui", "title": "End-to-end Learning, with or without Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for end-to-end learning that allows one to jointly\nlearn a feature representation from unlabeled data (with or without labeled\ndata) and predict labels for unlabeled data. The feature representation is\nassumed to be specified in a differentiable programming framework, that is, as\na parameterized mapping amenable to automatic differentiation. The proposed\napproach can be used with any amount of labeled and unlabeled data, gracefully\nadjusting to the amount of supervision. We provide experimental results\nillustrating the effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:11:40 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jones", "Corinne", ""], ["Roulet", "Vincent", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1912.12988", "submitter": "Ping Li", "authors": "Mostafa Rahmani and Ping Li", "title": "Outlier Detection and Data Clustering via Innovation Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of Innovation Search was proposed as a data clustering method in\nwhich the directions of innovation were utilized to compute the adjacency\nmatrix and it was shown that Innovation Pursuit can notably outperform the self\nrepresentation based subspace clustering methods. In this paper, we present a\nnew discovery that the directions of innovation can be used to design a\nprovable and strong robust (to outlier) PCA method. The proposed approach,\ndubbed iSearch, uses the direction search optimization problem to compute an\noptimal direction corresponding to each data point. iSearch utilizes the\ndirections of innovation to measure the innovation of the data points and it\nidentifies the outliers as the most innovative data points. Analytical\nperformance guarantees are derived for the proposed robust PCA method under\ndifferent models for the distribution of the outliers including randomly\ndistributed outliers, clustered outliers, and linearly dependent outliers. In\naddition, we study the problem of outlier detection in a union of subspaces and\nit is shown that iSearch provably recovers the span of the inliers when the\ninliers lie in a union of subspaces. Moreover, we present theoretical studies\nwhich show that the proposed measure of innovation remains stable in the\npresence of noise and the performance of iSearch is robust to noisy data. In\nthe challenging scenarios in which the outliers are close to each other or they\nare close to the span of the inliers, iSearch is shown to remarkably outperform\nmost of the existing methods. The presented method shows that the directions of\ninnovation are useful representation of the data which can be used to perform\nboth data clustering and outlier detection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:29:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Li", "Ping", ""]]}, {"id": "1912.12999", "submitter": "Laura Kinkead", "authors": "Laura Kinkead, Ahmed Allam, Michael Krauthammer", "title": "AutoDiscern: Rating the Quality of Online Health Information with\n  Hierarchical Encoder Attention-based Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients increasingly turn to search engines and online content before, or in\nplace of, talking with a health professional. Low quality health information,\nwhich is common on the internet, presents risks to the patient in the form of\nmisinformation and a possibly poorer relationship with their physician. To\naddress this, the DISCERN criteria (developed at University of Oxford) are used\nto evaluate the quality of online health information. However, patients are\nunlikely to take the time to apply these criteria to the health websites they\nvisit. We built an automated implementation of the DISCERN instrument (Brief\nversion) using machine learning models. We compared the performance of a\ntraditional model (Random Forest) with that of a hierarchical encoder\nattention-based neural network (HEA) model using two language embeddings, BERT\nand BioBERT. The HEA BERT and BioBERT models achieved average F1-macro scores\nacross all criteria of 0.75 and 0.74, respectively, outperforming the Random\nForest model (average F1-macro = 0.69). Overall, the neural network based\nmodels achieved 81% and 86% average accuracy at 100% and 80% coverage,\nrespectively, compared to 94% manual rating accuracy. The attention mechanism\nimplemented in the HEA architectures not only provided 'model explainability'\nby identifying reasonable supporting sentences for the documents fulfilling the\nBrief DISCERN criteria, but also boosted F1 performance by 0.05 compared to the\nsame architecture without an attention mechanism. Our research suggests that it\nis feasible to automate online health information quality assessment, which is\nan important step towards empowering patients to become informed partners in\nthe healthcare process.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:44:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 13:52:19 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 16:01:39 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kinkead", "Laura", ""], ["Allam", "Ahmed", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1912.13007", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler", "title": "World Programs for Model-Based Learning and Planning in Compositional\n  State and Action Spaces", "comments": "Accepted at the Generative Modeling and Model-Based Reasoning for\n  Robotics and AI workshop at ICML 2019. Presented on June 14th 2019. See\n  https://sites.google.com/view/mbrl-icml2019", "journal-ref": "https://sites.google.com/view/mbrl-icml2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most important tasks take place in environments which lack cheap\nand perfect simulators, thus hampering the application of model-free\nreinforcement learning (RL). While model-based RL aims to learn a dynamics\nmodel, in a more general case the learner does not know a priori what the\naction space is. Here we propose a formalism where the learner induces a world\nprogram by learning a dynamics model and the actions in graph-based\ncompositional environments by observing state-state transition examples. Then,\nthe learner can perform RL with the world program as the simulator for complex\nplanning tasks. We highlight a recent application, and propose a challenge for\nthe community to assess world program-based planning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 17:03:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Segler", "Marwin H. S.", ""]]}, {"id": "1912.13025", "submitter": "Andrew Wilson", "authors": "Pavel Izmailov, Polina Kirichenko, Marc Finzi, Andrew Gordon Wilson", "title": "Semi-Supervised Learning with Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows transform a latent distribution through an invertible\nneural network for a flexible and pleasingly simple approach to generative\nmodelling, while preserving an exact likelihood. We propose FlowGMM, an\nend-to-end approach to generative semi supervised learning with normalizing\nflows, using a latent Gaussian mixture model. FlowGMM is distinct in its\nsimplicity, unified treatment of labelled and unlabelled data with an exact\nlikelihood, interpretability, and broad applicability beyond image data. We\nshow promising results on a wide range of applications, including AG-News and\nYahoo Answers text data, tabular data, and semi-supervised image\nclassification. We also show that FlowGMM can discover interpretable structure,\nprovide real-time optimization-free feature visualizations, and specify well\ncalibrated predictive distributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 17:36:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Izmailov", "Pavel", ""], ["Kirichenko", "Polina", ""], ["Finzi", "Marc", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1912.13032", "submitter": "Alexander Gutfraind", "authors": "Jos\\'e M. Maisog and Wenhong Li and Yanchun Xu and Brian Hurley and\n  Hetal Shah and Ryan Lemberg and Tina Borden and Stephen Bandeian and Melissa\n  Schline and Roxanna Cross and Alan Spiro and Russ Michael and Alexander\n  Gutfraind", "title": "Using massive health insurance claims data to predict very high-cost\n  claimants: a machine learning approach", "comments": "34 pages, 2 figures, In review in PLoS ONE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Due to escalating healthcare costs, accurately predicting which patients will\nincur high costs is an important task for payers and providers of healthcare.\nHigh-cost claimants (HiCCs) are patients who have annual costs above\n$\\$250,000$ and who represent just 0.16% of the insured population but\ncurrently account for 9% of all healthcare costs. In this study, we aimed to\ndevelop a high-performance algorithm to predict HiCCs to inform a novel care\nmanagement system. Using health insurance claims from 48 million people and\naugmented with census data, we applied machine learning to train binary\nclassification models to calculate the personal risk of HiCC. To train the\nmodels, we developed a platform starting with 6,006 variables across all\nclinical and demographic dimensions and constructed over one hundred candidate\nmodels. The best model achieved an area under the receiver operating\ncharacteristic curve of 91.2%. The model exceeds the highest published\nperformance (84%) and remains high for patients with no prior history of\nhigh-cost status (89%), who have less than a full year of enrollment (87%), or\nlack pharmacy claims data (88%). It attains an area under the precision-recall\ncurve of 23.1%, and precision of 74% at a threshold of 0.99. A care management\nprogram enrolling 500 people with the highest HiCC risk is expected to treat\n199 true HiCCs and generate a net savings of $\\$7.3$ million per year. Our\nresults demonstrate that high-performing predictive models can be constructed\nusing claims data and publicly available data alone, even for rare high-cost\nclaimants exceeding $\\$250,000$. Our model demonstrates the transformational\npower of machine learning and artificial intelligence in care management, which\nwould allow healthcare payers and providers to introduce the next generation of\ncare management programs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:01:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Maisog", "Jos\u00e9 M.", ""], ["Li", "Wenhong", ""], ["Xu", "Yanchun", ""], ["Hurley", "Brian", ""], ["Shah", "Hetal", ""], ["Lemberg", "Ryan", ""], ["Borden", "Tina", ""], ["Bandeian", "Stephen", ""], ["Schline", "Melissa", ""], ["Cross", "Roxanna", ""], ["Spiro", "Alan", ""], ["Michael", "Russ", ""], ["Gutfraind", "Alexander", ""]]}, {"id": "1912.13037", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "A New Framework for Query Efficient Active Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to align agent policy with human expert behavior in a reinforcement\nlearning (RL) setting, without any prior knowledge about dynamics, reward\nfunction, and unsafe states. There is a human expert knowing the rewards and\nunsafe states based on his preference and objective, but querying that human\nexpert is expensive. To address this challenge, we propose a new framework for\nimitation learning (IL) algorithm that actively and interactively learns a\nmodel of the user's reward function with efficient queries. We build an\nadversarial generative model of states and a successor feature (SR) model\ntrained over transition experience collected by learning policy. Our method\nuses these models to select state-action pairs, asking the user to comment on\nthe optimality or safety, and trains a adversarial neural network to predict\nthe rewards. Different from previous papers, which are almost all based on\nuncertainty sampling, the key idea is to actively and efficiently select\nstate-action pairs from both on-policy and off-policy experience, by\ndiscriminating the queried (expert) and unqueried (generated) data and\nmaximizing the efficiency of value function learning. We call this method\nadversarial reward query with successor representation. We evaluate the\nproposed method with simulated human on a state-based 2D navigation task,\nrobotic control tasks and the image-based video games, which have\nhigh-dimensional observation and complex state dynamics. The results show that\nthe proposed method significantly outperforms uncertainty-based methods on\nlearning reward models, achieving better query efficiency, where the\nadversarial discriminator can make the agent learn human behavior more\nefficiently and the SR can select states which have stronger impact on value\nfunction. Moreover, the proposed method can also learn to avoid unsafe states\nwhen training the reward model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:12:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1912.13046", "submitter": "Edward Raff", "authors": "Edward Raff, Charles Nicholas, Mark McLean", "title": "A New Burrows Wheeler Transform Markov Distance", "comments": "To appear in: The Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-20), AICS-2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work inspired by compression algorithms has described how the Burrows\nWheeler Transform can be used to create a distance measure for bioinformatics\nproblems. We describe issues with this approach that were not widely known, and\nintroduce our new Burrows Wheeler Markov Distance (BWMD) as an alternative. The\nBWMD avoids the shortcomings of earlier efforts, and allows us to tackle\nproblems in variable length DNA sequence clustering. BWMD is also more\nadaptable to other domains, which we demonstrate on malware classification\ntasks. Unlike other compression-based distance metrics known to us, BWMD works\nby embedding sequences into a fixed-length feature vector. This allows us to\nprovide significantly improved clustering performance on larger malware\ncorpora, a weakness of prior methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:33:32 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""], ["McLean", "Mark", ""]]}, {"id": "1912.13053", "submitter": "Samuel Schoenholz", "authors": "Lechao Xiao, Jeffrey Pennington, Samuel S. Schoenholz", "title": "Disentangling Trainability and Generalization in Deep Neural Networks", "comments": "22 pages, 3 figures, ICML 2020. Associated Colab notebook at\n  https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/Disentangling_Trainability_and_Generalization.ipynb", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding goal in the theory of deep learning is to characterize the\nconditions under which a given neural network architecture will be trainable,\nand if so, how well it might generalize to unseen data. In this work, we\nprovide such a characterization in the limit of very wide and very deep\nnetworks, for which the analysis simplifies considerably. For wide networks,\nthe trajectory under gradient descent is governed by the Neural Tangent Kernel\n(NTK), and for deep networks the NTK itself maintains only weak data\ndependence. By analyzing the spectrum of the NTK, we formulate necessary\nconditions for trainability and generalization across a range of architectures,\nincluding Fully Connected Networks (FCNs) and Convolutional Neural Networks\n(CNNs). We identify large regions of hyperparameter space for which networks\ncan memorize the training set but completely fail to generalize. We find that\nCNNs without global average pooling behave almost identically to FCNs, but that\nCNNs with pooling have markedly different and often better generalization\nperformance. These theoretical results are corroborated experimentally on\nCIFAR10 for a variety of network architectures and we include a colab notebook\nthat reproduces the essential results of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:53:24 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 04:55:53 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xiao", "Lechao", ""], ["Pennington", "Jeffrey", ""], ["Schoenholz", "Samuel S.", ""]]}, {"id": "1912.13075", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa", "title": "Robust Federated Learning Through Representation Matching and Adaptive\n  Hyper-parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed, privacy-aware learning scenario which\ntrains a single model on data belonging to several clients. Each client trains\na local model on its data and the local models are then aggregated by a central\nparty. Current federated learning methods struggle in cases with heterogeneous\nclient-side data distributions which can quickly lead to divergent local models\nand a collapse in performance. Careful hyper-parameter tuning is particularly\nimportant in these cases but traditional automated hyper-parameter tuning\nmethods would require several training trials which is often impractical in a\nfederated learning setting. We describe a two-pronged solution to the issues of\nrobustness and hyper-parameter tuning in federated learning settings. We\npropose a novel representation matching scheme that reduces the divergence of\nlocal models by ensuring the feature representations in the global (aggregate)\nmodel can be derived from the locally learned representations. We also propose\nan online hyper-parameter tuning scheme which uses an online version of the\nREINFORCE algorithm to find a hyper-parameter distribution that maximizes the\nexpected improvements in training loss. We show on several benchmarks that our\ntwo-part scheme of local representation matching and global adaptive\nhyper-parameters significantly improves performance and training robustness.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:19:20 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mostafa", "Hesham", ""]]}, {"id": "1912.13088", "submitter": "Peng Liao", "authors": "Peng Liao, Predrag Klasnja, Susan Murphy", "title": "Off-Policy Estimation of Long-Term Average Outcomes with Applications to\n  Mobile Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advancements in wearables and sensing technology, health\nscientists are increasingly developing mobile health (mHealth) interventions.\nIn mHealth interventions, mobile devices are used to deliver treatment to\nindividuals as they go about their daily lives. These treatments are generally\ndesigned to impact a near time, proximal outcome such as stress or physical\nactivity. The mHealth intervention policies, often called just-in-time adaptive\ninterventions, are decision rules that map an individual's current state (e.g.,\nindividual's past behaviors as well as current observations of time, location,\nsocial activity, stress and urges to smoke) to a particular treatment at each\nof many time points. The vast majority of current mHealth interventions deploy\nexpert-derived policies. In this paper, we provide an approach for conducting\ninference about the performance of one or more such policies using historical\ndata collected under a possibly different policy. Our measure of performance is\nthe average of proximal outcomes over a long time period should the particular\nmHealth policy be followed. We provide an estimator as well as confidence\nintervals. This work is motivated by HeartSteps, an mHealth physical activity\nintervention.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:22:21 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 19:41:48 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 18:00:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Liao", "Peng", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan", ""]]}, {"id": "1912.13091", "submitter": "Chong You", "authors": "Daniel P. Robinson and Rene Vidal and Chong You", "title": "Basis Pursuit and Orthogonal Matching Pursuit for Subspace-preserving\n  Recovery: Theoretical Analysis", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an overcomplete dictionary $A$ and a signal $b = Ac^*$ for some sparse\nvector $c^*$ whose nonzero entries correspond to linearly independent columns\nof $A$, classical sparse signal recovery theory considers the problem of\nwhether $c^*$ can be recovered as the unique sparsest solution to $b = A c$. It\nis now well-understood that such recovery is possible by practical algorithms\nwhen the dictionary $A$ is incoherent or restricted isometric. In this paper,\nwe consider the more general case where $b$ lies in a subspace $\\mathcal{S}_0$\nspanned by a subset of linearly dependent columns of $A$, and the remaining\ncolumns are outside of the subspace. In this case, the sparsest representation\nmay not be unique, and the dictionary may not be incoherent or restricted\nisometric. The goal is to have the representation $c$ correctly identify the\nsubspace, i.e. the nonzero entries of $c$ should correspond to columns of $A$\nthat are in the subspace $\\mathcal{S}_0$. Such a representation $c$ is called\nsubspace-preserving, a key concept that has found important applications for\nlearning low-dimensional structures in high-dimensional data. We present\nvarious geometric conditions that guarantee subspace-preserving recovery. Among\nthem, the major results are characterized by the covering radius and the\nangular distance, which capture the distribution of points in the subspace and\nthe similarity between points in the subspace and points outside the subspace,\nrespectively. Importantly, these conditions do not require the dictionary to be\nincoherent or restricted isometric. By establishing that the\nsubspace-preserving recovery problem and the classical sparse signal recovery\nproblem are equivalent under common assumptions on the latter, we show that\nseveral of our proposed conditions are generalizations of some well-known\nconditions in the sparse signal recovery literature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:31:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""], ["You", "Chong", ""]]}, {"id": "1912.13107", "submitter": "Jennifer Hobbs", "authors": "Jennifer Hobbs, Matthew Holbrook, Nathan Frank, Long Sha, Patrick\n  Lucey", "title": "Improved Structural Discovery and Representation Learning of Multi-Agent\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to all machine learning algorithms is data representation. For\nmulti-agent systems, selecting a representation which adequately captures the\ninteractions among agents is challenging due to the latent group structure\nwhich tends to vary depending on context. However, in multi-agent systems with\nstrong group structure, we can simultaneously learn this structure and map a\nset of agents to a consistently ordered representation for further learning. In\nthis paper, we present a dynamic alignment method which provides a robust\nordering of structured multi-agent data enabling representation learning to\noccur in a fraction of the time of previous methods. We demonstrate the value\nof this approach using a large amount of soccer tracking data from a\nprofessional league.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 22:49:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hobbs", "Jennifer", ""], ["Holbrook", "Matthew", ""], ["Frank", "Nathan", ""], ["Sha", "Long", ""], ["Lucey", "Patrick", ""]]}, {"id": "1912.13109", "submitter": "Vivek Gupta", "authors": "Vivek Kumar Gupta", "title": "\"Hinglish\" Language -- Modeling a Messy Code-Mixed Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a sharp rise in fluency and users of \"Hinglish\" in linguistically\ndiverse country, India, it has increasingly become important to analyze social\ncontent written in this language in platforms such as Twitter, Reddit,\nFacebook. This project focuses on using deep learning techniques to tackle a\nclassification problem in categorizing social content written in Hindi-English\ninto Abusive, Hate-Inducing and Not offensive categories. We utilize\nbi-directional sequence models with easy text augmentation techniques such as\nsynonym replacement, random insertion, random swap, and random deletion to\nproduce a state of the art classifier that outperforms the previous work done\non analyzing this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 23:01:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gupta", "Vivek Kumar", ""]]}, {"id": "1912.13151", "submitter": "Mingyuan Zhou", "authors": "Xinjie Fan, Yizhe Zhang, Zhendong Wang, Mingyuan Zhou", "title": "Adaptive Correlated Monte Carlo for Contextual Categorical Sequence\n  Generation", "comments": "ICLR 2020 (updated to fix a typo in Algorithm 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence generation models are commonly refined with reinforcement learning\nover user-defined metrics. However, high gradient variance hinders the\npractical use of this method. To stabilize this method, we adapt to contextual\ngeneration of categorical sequences a policy gradient estimator, which\nevaluates a set of correlated Monte Carlo (MC) rollouts for variance control.\nDue to the correlation, the number of unique rollouts is random and adaptive to\nmodel uncertainty; those rollouts naturally become baselines for each other,\nand hence are combined to effectively reduce gradient variance. We also\ndemonstrate the use of correlated MC rollouts for binary-tree softmax models,\nwhich reduce the high generation cost in large vocabulary scenarios by\ndecomposing each categorical action into a sequence of binary actions. We\nevaluate our methods on both neural program synthesis and image captioning. The\nproposed methods yield lower gradient variance and consistent improvement over\nrelated baselines.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 03:01:55 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:32:42 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Fan", "Xinjie", ""], ["Zhang", "Yizhe", ""], ["Wang", "Zhendong", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1912.13169", "submitter": "Hufei Zhu", "authors": "Hufei Zhu", "title": "Efficient Decremental Learning Algorithms for Broad Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decremented learning algorithms are required in machine learning, to\nprune redundant nodes and remove obsolete inline training samples. In this\npaper, an efficient decremented learning algorithm to prune redundant nodes is\ndeduced from the incremental learning algorithm 1 proposed in [9] for added\nnodes, and two decremented learning algorithms to remove training samples are\ndeduced from the two incremental learning algorithms proposed in [10] for added\ninputs. The proposed decremented learning algorithm for reduced nodes utilizes\nthe inverse Cholesterol factor of the Herminia matrix in the ridge inverse, to\nupdate the output weights recursively, as the incremental learning algorithm 1\nfor added nodes in [9], while that inverse Cholesterol factor is updated with\nan unitary transformation. The proposed decremented learning algorithm 1 for\nreduced inputs updates the output weights recursively with the inverse of the\nHerminia matrix in the ridge inverse, and updates that inverse recursively, as\nthe incremental learning algorithm 1 for added inputs in [10].\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 04:46:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhu", "Hufei", ""]]}, {"id": "1912.13170", "submitter": "Jeremy Heng", "authors": "Espen Bernton, Jeremy Heng, Arnaud Doucet, Pierre E. Jacob", "title": "Schr\\\"odinger Bridge Samplers", "comments": "53 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a reference Markov process with initial distribution $\\pi_{0}$ and\ntransition kernels $\\{M_{t}\\}_{t\\in[1:T]}$, for some $T\\in\\mathbb{N}$. Assume\nthat you are given distribution $\\pi_{T}$, which is not equal to the marginal\ndistribution of the reference process at time $T$. In this scenario,\nSchr\\\"odinger addressed the problem of identifying the Markov process with\ninitial distribution $\\pi_{0}$ and terminal distribution equal to $\\pi_{T}$\nwhich is the closest to the reference process in terms of Kullback--Leibler\ndivergence. This special case of the so-called Schr\\\"odinger bridge problem can\nbe solved using iterative proportional fitting, also known as the Sinkhorn\nalgorithm. We leverage these ideas to develop novel Monte Carlo schemes, termed\nSchr\\\"odinger bridge samplers, to approximate a target distribution $\\pi$ on\n$\\mathbb{R}^{d}$ and to estimate its normalizing constant. This is achieved by\niteratively modifying the transition kernels of the reference Markov chain to\nobtain a process whose marginal distribution at time $T$ becomes closer to\n$\\pi_T = \\pi$, via regression-based approximations of the corresponding\niterative proportional fitting recursion. We report preliminary experiments and\nmake connections with other problems arising in the optimal transport, optimal\ncontrol and physics literatures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 04:49:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bernton", "Espen", ""], ["Heng", "Jeremy", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1912.13204", "submitter": "Suchet Sapre", "authors": "Suchet Sapre, Pouyan Ahmadi and Khondkar Islam", "title": "A Robust Comparison of the KDDCup99 and NSL-KDD IoT Network Intrusion\n  Detection Datasets Through Various Machine Learning Algorithms", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, as intrusion attacks on IoT networks have grown\nexponentially, there is an immediate need for sophisticated intrusion detection\nsystems (IDSs). A vast majority of current IDSs are data-driven, which means\nthat one of the most important aspects of this area of research is the quality\nof the data acquired from IoT network traffic. Two of the most cited intrusion\ndetection datasets are the KDDCup99 and the NSL-KDD. The main goal of our\nproject was to conduct a robust comparison of both datasets by evaluating the\nperformance of various Machine Learning (ML) classifiers trained on them with a\nlarger set of classification metrics than previous researchers. From our\nresearch, we were able to conclude that the NSL-KDD dataset is of a higher\nquality than the KDDCup99 dataset as the classifiers trained on it were on\naverage 20.18% less accurate. This is because the classifiers trained on the\nKDDCup99 dataset exhibited a bias towards the redundancies within it, allowing\nthem to achieve higher accuracies.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 07:36:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sapre", "Suchet", ""], ["Ahmadi", "Pouyan", ""], ["Islam", "Khondkar", ""]]}, {"id": "1912.13213", "submitter": "Francesco Orabona", "authors": "Francesco Orabona", "title": "A Modern Introduction to Online Learning", "comments": "Fixed more typos, added more history bits, added local norms bounds\n  for OMD and FTRL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this monograph, I introduce the basic concepts of Online Learning through\na modern view of Online Convex Optimization. Here, online learning refers to\nthe framework of regret minimization under worst-case assumptions. I present\nfirst-order and second-order algorithms for online learning with convex losses,\nin Euclidean and non-Euclidean settings. All the algorithms are clearly\npresented as instantiation of Online Mirror Descent or\nFollow-The-Regularized-Leader and their variants. Particular attention is given\nto the issue of tuning the parameters of the algorithms and learning in\nunbounded domains, through adaptive and parameter-free online learning\nalgorithms. Non-convex losses are dealt through convex surrogate losses and\nthrough randomization. The bandit setting is also briefly discussed, touching\non the problem of adversarial and stochastic multi-armed bandits. These notes\ndo not require prior knowledge of convex analysis and all the required\nmathematical tools are rigorously explained. Moreover, all the proofs have been\ncarefully chosen to be as simple and as short as possible.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 08:16:31 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 19:03:07 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 22:14:47 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Orabona", "Francesco", ""]]}, {"id": "1912.13226", "submitter": "Yuntao Du", "authors": "Yuntao Du, Zhiwen Tan, Qian Chen, Yi Zhang, Chongjun Wang", "title": "Homogeneous Online Transfer Learning with Online Distribution\n  Discrepancy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "ECAI 2020", "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Transfer learning has been demonstrated to be successful and essential in\ndiverse applications, which transfers knowledge from related but different\nsource domains to the target domain. Online transfer learning(OTL) is a more\nchallenging problem where the target data arrive in an online manner. Most OTL\nmethods combine source classifier and target classifier directly by assigning a\nweight to each classifier, and adjust the weights constantly. However, these\nmethods pay little attention to reducing the distribution discrepancy between\ndomains. In this paper, we propose a novel online transfer learning method\nwhich seeks to find a new feature representation, so that the marginal\ndistribution and conditional distribution discrepancy can be online reduced\nsimultaneously. We focus on online transfer learning with multiple source\ndomains and use the Hedge strategy to leverage knowledge from source domains.\nWe analyze the theoretical properties of the proposed algorithm and provide an\nupper mistake bound. Comprehensive experiments on two real-world datasets show\nthat our method outperforms state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 08:58:25 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Du", "Yuntao", ""], ["Tan", "Zhiwen", ""], ["Chen", "Qian", ""], ["Zhang", "Yi", ""], ["Wang", "Chongjun", ""]]}, {"id": "1912.13230", "submitter": "Vahid Noroozi", "authors": "Vahid Noroozi, Sara Bahaadini, Samira Sheikhi, Nooshin Mojab, Philip\n  S. Yu", "title": "Leveraging Semi-Supervised Learning for Fairness using Neural Networks", "comments": "6 pages, 5 figures, accepted to ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing concern about the fairness of decision-making\nsystems based on machine learning. The shortage of labeled data has been always\na challenging problem facing machine learning based systems. In such scenarios,\nsemi-supervised learning has shown to be an effective way of exploiting\nunlabeled data to improve upon the performance of model. Notably, unlabeled\ndata do not contain label information which itself can be a significant source\nof bias in training machine learning systems. This inspired us to tackle the\nchallenge of fairness by formulating the problem in a semi-supervised\nframework. In this paper, we propose a semi-supervised algorithm using neural\nnetworks benefiting from unlabeled data to not just improve the performance but\nalso improve the fairness of the decision-making process. The proposed model,\ncalled SSFair, exploits the information in the unlabeled data to mitigate the\nbias in the training data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 09:11:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Noroozi", "Vahid", ""], ["Bahaadini", "Sara", ""], ["Sheikhi", "Samira", ""], ["Mojab", "Nooshin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1912.13243", "submitter": "Pavol Bielik", "authors": "Philippe Schlattner, Pavol Bielik, Martin Vechev", "title": "Learning to Infer User Interface Attributes from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new domain of learning to infer user interface attributes that\nhelps developers automate the process of user interface implementation.\nConcretely, given an input image created by a designer, we learn to infer its\nimplementation which when rendered, looks visually the same as the input image.\nTo achieve this, we take a black box rendering engine and a set of attributes\nit supports (e.g., colors, border radius, shadow or text properties), use it to\ngenerate a suitable synthetic training dataset, and then train specialized\nneural models to predict each of the attribute values. To improve pixel-level\naccuracy, we additionally use imitation learning to train a neural policy that\nrefines the predicted attribute values by learning to compute the similarity of\nthe original and rendered images in their attribute space, rather than based on\nthe difference of pixel values. We instantiate our approach to the task of\ninferring Android Button attribute values and achieve 92.5% accuracy on a\ndataset consisting of real-world Google Play Store applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 09:45:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Schlattner", "Philippe", ""], ["Bielik", "Pavol", ""], ["Vechev", "Martin", ""]]}, {"id": "1912.13256", "submitter": "Lanfei Wang", "authors": "Lanfei Wang and Lingxi Xie and Tianyi Zhang and Jun Guo and Qi Tian", "title": "Scalable NAS with Factorizable Architectural Parameters", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an emerging topic in machine learning and\ncomputer vision. The fundamental ideology of NAS is using an automatic\nmechanism to replace manual designs for exploring powerful network\narchitectures. One of the key factors of NAS is to scale-up the search space,\ne.g., increasing the number of operators, so that more possibilities are\ncovered, but existing search algorithms often get lost in a large number of\noperators. For avoiding huge computing and competition among similar operators\nin the same pool, this paper presents a scalable algorithm by factorizing a\nlarge set of candidate operators into smaller subspaces. As a practical\nexample, this allows us to search for effective activation functions along with\nthe regular operators including convolution, pooling, skip-connect, etc. With a\nsmall increase in search costs and no extra costs in re-training, we find\ninteresting architectures that were not explored before, and achieve\nstate-of-the-art performance on CIFAR10 and ImageNet, two standard image\nclassification benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 10:26:56 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 18:47:42 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wang", "Lanfei", ""], ["Xie", "Lingxi", ""], ["Zhang", "Tianyi", ""], ["Guo", "Jun", ""], ["Tian", "Qi", ""]]}, {"id": "1912.13258", "submitter": "Yiqiang Han", "authors": "Yuan Gao, Yiqiang Han", "title": "Automated Testing for Deep Learning Systems with Differential Behavior\n  Criteria", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we conducted a study on building an automated testing system\nfor deep learning systems based on differential behavior criteria. The\nautomated testing goals were achieved by jointly optimizing two objective\nfunctions: maximizing differential behaviors from models under testing and\nmaximizing neuron coverage. By observing differential behaviors from three\npre-trained models during each testing iteration, the input image that\ntriggered erroneous feedback was registered as a corner-case. The generated\ncorner-cases can be used to examine the robustness of DNNs and consequently\nimprove model accuracy. A project called DeepXplore was also used as a baseline\nmodel. After we fully implemented and optimized the baseline system, we\nexplored its application as an augmenting training dataset with newly generated\ncorner cases. With the GTRSB dataset, by retraining the model based on\nautomated generated corner cases, the accuracy of three generic models\nincreased by 259.2%, 53.6%, and 58.3%, respectively. Further, to extend the\ncapability of automated testing, we explored other approaches based on\ndifferential behavior criteria to generate photo-realistic images for deep\nlearning systems. One approach was to apply various transformations to the seed\nimages for the deep learning framework. The other approach was to utilize the\nGenerative Adversarial Networks (GAN) technique, which was implemented on MNIST\nand Driving datasets. The style transferring capability has been observed very\neffective in adding additional visual effects, replacing image elements, and\nstyle-shifting (virtual image to real images). The GAN-based testing sample\ngeneration system was shown to be the next frontier for automated testing for\ndeep learning systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 10:31:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gao", "Yuan", ""], ["Han", "Yiqiang", ""]]}, {"id": "1912.13263", "submitter": "Gianluca Baldassarre PhD", "authors": "Gianluca Baldassarre", "title": "Intrinsic motivations and open-ended learning", "comments": "24 pages, 2 figures, 2 tables, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest and literature on intrinsic motivations and\nopen-ended learning in both cognitive robotics and machine learning on one\nside, and in psychology and neuroscience on the other. This paper aims to\nreview some relevant contributions from the two literature threads and to draw\nlinks between them. To this purpose, the paper starts by defining intrinsic\nmotivations and by presenting a computationally-driven theoretical taxonomy of\ntheir different types. Then it presents relevant contributions from the\npsychological and neuroscientific literature related to intrinsic motivations,\ninterpreting them based on the grid, and elucidates the mechanisms and\nfunctions they play in animals and humans. Endowed with such concepts and their\nbiological underpinnings, the paper next presents a selection of models from\ncognitive robotics and machine learning that computationally operationalise the\nconcepts of intrinsic motivations and links them to biology concepts. The\ncontribution finally presents some of the open challenges of the field from\nboth the psychological/neuroscientific and computational perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 10:56:02 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Baldassarre", "Gianluca", ""]]}, {"id": "1912.13332", "submitter": "Chidubem Arachie", "authors": "Chidubem Arachie, Manas Gaur, Sam Anzaroot, William Groves, Ke Zhang,\n  Alejandro Jaimes", "title": "Unsupervised Detection of Sub-events in Large Scale Disasters", "comments": "AAAI-20 Social Impact Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media plays a major role during and after major natural disasters\n(e.g., hurricanes, large-scale fires, etc.), as people ``on the ground'' post\nuseful information on what is actually happening. Given the large amounts of\nposts, a major challenge is identifying the information that is useful and\nactionable. Emergency responders are largely interested in finding out what\nevents are taking place so they can properly plan and deploy resources. In this\npaper we address the problem of automatically identifying important sub-events\n(within a large-scale emergency ``event'', such as a hurricane). In particular,\nwe present a novel, unsupervised learning framework to detect sub-events in\nTweets for retrospective crisis analysis. We first extract noun-verb pairs and\nphrases from raw tweets as sub-event candidates. Then, we learn a semantic\nembedding of extracted noun-verb pairs and phrases, and rank them against a\ncrisis-specific ontology. We filter out noisy and irrelevant information then\ncluster the noun-verb pairs and phrases so that the top-ranked ones describe\nthe most important sub-events. Through quantitative experiments on two large\ncrisis data sets (Hurricane Harvey and the 2015 Nepal Earthquake), we\ndemonstrate the effectiveness of our approach over the state-of-the-art. Our\nqualitative evaluation shows better performance compared to our baseline.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:10:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Arachie", "Chidubem", ""], ["Gaur", "Manas", ""], ["Anzaroot", "Sam", ""], ["Groves", "William", ""], ["Zhang", "Ke", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "1912.13335", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Byoung-Dai Lee, Shi Sub Byon, Sung Hyun Kim, and\n  Byung-ilLee", "title": "Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View\n  Residual Learning", "comments": "The manuscript is currently under review and copyright shall be\n  transferred to the publisher upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate quantification of pulmonary nodules can greatly assist the early\ndiagnosis of lung cancer, which can enhance patient survival possibilities. A\nnumber of nodule segmentation techniques have been proposed, however, all of\nthe existing techniques rely on radiologist 3-D volume of interest (VOI) input\nor use the constant region of interest (ROI) and only investigate the presence\nof nodule voxels within the given VOI. Such approaches restrain the solutions\nto investigate the nodule presence outside the given VOI and also include the\nredundant structures into VOI, which may lead to inaccurate nodule\nsegmentation. In this work, a novel semi-automated approach for 3-D\nsegmentation of nodule in volumetric computerized tomography (CT) lung scans\nhas been proposed. The proposed technique can be segregated into two stages, at\nthe first stage, it takes a 2-D ROI containing the nodule as input and it\nperforms patch-wise investigation along the axial axis with a novel adaptive\nROI strategy. The adaptive ROI algorithm enables the solution to dynamically\nselect the ROI for the surrounding slices to investigate the presence of nodule\nusing deep residual U-Net architecture. The first stage provides the initial\nestimation of nodule which is further utilized to extract the VOI. At the\nsecond stage, the extracted VOI is further investigated along the coronal and\nsagittal axis with two different networks and finally, all the estimated masks\nare fed into the consensus module to produce the final volumetric segmentation\nof nodule. The proposed approach has been rigorously evaluated on the LIDC\ndataset, which is the largest publicly available dataset. The result suggests\nthat the approach is significantly robust and accurate as compared to the\nprevious state of the art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:03:18 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 10:57:24 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Usman", "Muhammad", ""], ["Lee", "Byoung-Dai", ""], ["Byon", "Shi Sub", ""], ["Kim", "Sung Hyun", ""], ["Byung-ilLee", "", ""]]}, {"id": "1912.13357", "submitter": "Achraf Bahamou", "authors": "Achraf Bahamou, Donald Goldfarb", "title": "A Dynamic Sampling Adaptive-SGD Method for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a stochastic optimization method for minimizing loss functions,\nexpressed as an expected value, that adaptively controls the batch size used in\nthe computation of gradient approximations and the step size used to move along\nsuch directions, eliminating the need for the user to tune the learning rate.\nThe proposed method exploits local curvature information and ensures that\nsearch directions are descent directions with high probability using an\nacute-angle test and can be used as a method that has a global linear rate of\nconvergence on self-concordant functions with high probability. Numerical\nexperiments show that this method is able to choose the best learning rates and\ncompares favorably to fine-tuned SGD for training logistic regression and DNNs.\nWe also propose an adaptive version of ADAM that eliminates the need to tune\nthe base learning rate and compares favorably to fine-tuned ADAM on training\nDNNs. In our DNN experiments, we rarely encountered negative curvature at the\ncurrent point along the step direction in DNNs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:36:44 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 00:39:06 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Bahamou", "Achraf", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1912.13361", "submitter": "Ali Lotfi Rezaabad", "authors": "Ali Lotfi Rezaabad and Sriram Vishwanath", "title": "Learning Representations by Maximizing Mutual Information in Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) have ushered in a new era of unsupervised\nlearning methods for complex distributions. Although these techniques are\nelegant in their approach, they are typically not useful for representation\nlearning. In this work, we propose a simple yet powerful class of VAEs that\nsimultaneously result in meaningful learned representations. Our solution is to\ncombine traditional VAEs with mutual information maximization, with the goal to\nenhance amortized inference in VAEs using Information Theoretic techniques. We\ncall this approach InfoMax-VAE, and such an approach can significantly boost\nthe quality of learned high-level representations. We realize this through the\nexplicit maximization of information measures associated with the\nrepresentation. Using extensive experiments on varied datasets and setups, we\nshow that InfoMax-VAE outperforms contemporary popular approaches, including\nInfo-VAE and $\\beta$-VAE.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:44:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 05:42:39 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Rezaabad", "Ali Lotfi", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1912.13362", "submitter": "Behnam Kiani Kalejahi", "authors": "Umid Suleymanov, Behnam Kiani Kalejahi, Elkhan Amrahov, Rashid\n  Badirkhanli", "title": "Text Classification for Azerbaijani Language Using Machine Learning and\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classification systems will help to solve the text clustering problem in\nthe Azerbaijani language. There are some text-classification applications for\nforeign languages, but we tried to build a newly developed system to solve this\nproblem for the Azerbaijani language. Firstly, we tried to find out potential\npractice areas. The system will be useful in a lot of areas. It will be mostly\nused in news feed categorization. News websites can automatically categorize\nnews into classes such as sports, business, education, science, etc. The system\nis also used in sentiment analysis for product reviews. For example, the\ncompany shares a photo of a new product on Facebook and the company receives a\nthousand comments for new products. The systems classify the comments into\ncategories like positive or negative. The system can also be applied in\nrecommended systems, spam filtering, etc. Various machine learning techniques\nsuch as Naive Bayes, SVM, Decision Trees have been devised to solve the text\nclassification problem in Azerbaijani language.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:38:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Suleymanov", "Umid", ""], ["Kalejahi", "Behnam Kiani", ""], ["Amrahov", "Elkhan", ""], ["Badirkhanli", "Rashid", ""]]}, {"id": "1912.13366", "submitter": "Seungcheol Park", "authors": "Seungcheol Park, Huiwen Xu, Taehun Kim, Inhwan Hwang, Kyung-Jun Kim\n  and U Kang", "title": "Fast and Accurate Transferability Measurement for Heterogeneous\n  Multivariate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of heterogeneous source datasets with their classifiers, how can\nwe quickly find the most useful source dataset for a specific target task? We\naddress the problem of measuring transferability between source and target\ndatasets, where the source and the target have different feature spaces and\ndistributions. We propose Transmeter, a fast and accurate method to estimate\nthe transferability of two heterogeneous multivariate datasets. We address\nthree challenges in measuring transferability between two heterogeneous\nmultivariate datasets: reducing time, minimizing domain gap, and extracting\nmeaningful homogeneous representations. To overcome the above issues, we\nutilize a pre-trained source model, an adversarial network, and an\nencoder-decoder architecture. Extensive experiments on heterogeneous\nmultivariate datasets show that Transmeter gives the most accurate\ntransferability measurement with up to 10.3 times faster performance than its\ncompetitor. We also show that selecting the best source data with Transmeter\nfollowed by a full transfer leads to the best transfer accuracy and the fastest\nrunning time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:42:17 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 09:25:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Park", "Seungcheol", ""], ["Xu", "Huiwen", ""], ["Kim", "Taehun", ""], ["Hwang", "Inhwan", ""], ["Kim", "Kyung-Jun", ""], ["Kang", "U", ""]]}, {"id": "1912.13377", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa", "title": "Finite-State Extreme Effect Variable", "comments": "15 pages; 2 figures; 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize to the finite-state case the notion of the extreme effect\nvariable $Y$ that accumulates all the effect of a variant variable $V$ observed\nin changes of another variable $X$. We conduct theoretical analysis and turn\nthe problem of finding of an effect variable into a problem of a simultaneous\ndecomposition of a set of distributions. The states of the extreme effect\nvariable, on the one hand, are minimally affected by the variant variable $V$\nand, on the other hand, are extremely different with respect to the observable\nvariable $X$. We apply our technique to online evaluation of a web search\nengine through A/B testing and show its utility.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 21:40:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drutsa", "Alexey", ""]]}, {"id": "1912.13384", "submitter": "Kasra Babaei", "authors": "Kasra Babaei, ZhiYuan Chen, Tomas Maul", "title": "Data Augmentation by AutoEncoders for Unsupervised Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an autoencoder (AE) that is used for improving the\nperformance of once-class classifiers for the purpose of detecting anomalies.\nTraditional one-class classifiers (OCCs) perform poorly under certain\nconditions such as high-dimensionality and sparsity. Also, the size of the\ntraining set plays an important role on the performance of one-class\nclassifiers. Autoencoders have been widely used for obtaining useful latent\nvariables from high-dimensional datasets. In the proposed approach, the AE is\ncapable of deriving meaningful features from high-dimensional datasets while\ndoing data augmentation at the same time. The augmented data is used for\ntraining the OCC algorithms. The experimental results show that the proposed\napproach enhance the performance of OCC algorithms and also outperforms other\nwell-known approaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:30:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Babaei", "Kasra", ""], ["Chen", "ZhiYuan", ""], ["Maul", "Tomas", ""]]}, {"id": "1912.13387", "submitter": "Kasra Babaei", "authors": "Kasra Babaei, Zhi Yuan Chen, Tomas Maul", "title": "AEGR: A simple approach to gradient reversal in autoencoders for network\n  anomaly detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is referred to as a process in which the aim is to detect\ndata points that follow a different pattern from the majority of data points.\nAnomaly detection methods suffer from several well-known challenges that hinder\ntheir performance such as high dimensionality. Autoencoders are unsupervised\nneural networks that have been used for the purpose of reducing dimensionality\nand also detecting network anomalies in large datasets. The performance of\nautoencoders debilitates when the training set contains noise and anomalies. In\nthis paper, a new gradient-reversal method is proposed to overcome the\ninfluence of anomalies on the training phase for the purpose of detecting\nnetwork anomalies. The method is different from other approaches as it does not\nrequire an anomaly-free training set and is based on reconstruction error. Once\nlatent variables are extracted from the network, Local Outlier Factor is used\nto separate normal data points from anomalies. A simple pruning approach and\ndata augmentation is also added to further improve performance. The\nexperimental results show that the proposed model can outperform other\nwell-know approaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:24:02 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 22:28:56 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Babaei", "Kasra", ""], ["Chen", "Zhi Yuan", ""], ["Maul", "Tomas", ""]]}, {"id": "1912.13405", "submitter": "Jesse Read", "authors": "Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank", "title": "Classifier Chains: A Review and Perspectives", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 70 (2021) 683-718", "doi": "10.1613/jair.1.12376", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of methods collectively known as classifier chains has become a\npopular approach to multi-label learning problems. This approach involves\nlinking together off-the-shelf binary classifiers in a chain structure, such\nthat class label predictions become features for other classifiers. Such\nmethods have proved flexible and effective and have obtained state-of-the-art\nempirical performance across many datasets and multi-label evaluation metrics.\nThis performance led to further studies of how exactly it works, and how it\ncould be improved, and in the recent decade numerous studies have explored\nclassifier chains mechanisms on a theoretical level, and many improvements have\nbeen made to the training and inference procedures, such that this method\nremains among the state-of-the-art options for multi-label learning. Given this\npast and ongoing interest, which covers a broad range of applications and\nresearch themes, the goal of this work is to provide a review of classifier\nchains, a survey of the techniques and extensions provided in the literature,\nas well as perspectives for this approach in the domain of multi-label\nclassification in the future. We conclude positively, with a number of\nrecommendations for researchers and practitioners, as well as outlining a\nnumber of areas for future research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 11:44:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 11:36:27 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Read", "Jesse", ""], ["Pfahringer", "Bernhard", ""], ["Holmes", "Geoff", ""], ["Frank", "Eibe", ""]]}, {"id": "1912.13408", "submitter": "Matthew Riemer", "authors": "Matthew Riemer, Ignacio Cases, Clemens Rosenbaum, Miao Liu, Gerald\n  Tesauro", "title": "On the Role of Weight Sharing During Deep Option Learning", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The options framework is a popular approach for building temporally extended\nactions in reinforcement learning. In particular, the option-critic\narchitecture provides general purpose policy gradient theorems for learning\nactions from scratch that are extended in time. However, past work makes the\nkey assumption that each of the components of option-critic has independent\nparameters. In this work we note that while this key assumption of the policy\ngradient theorems of option-critic holds in the tabular case, it is always\nviolated in practice for the deep function approximation setting. We thus\nreconsider this assumption and consider more general extensions of\noption-critic and hierarchical option-critic training that optimize for the\nfull architecture with each update. It turns out that not assuming parameter\nindependence challenges a belief in prior work that training the policy over\noptions can be disentangled from the dynamics of the underlying options. In\nfact, learning can be sped up by focusing the policy over options on states\nwhere options are actually likely to terminate. We put our new algorithms to\nthe test in application to sample efficient learning of Atari games, and\ndemonstrate significantly improved stability and faster convergence when\nlearning long options.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 16:49:13 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 06:19:04 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Riemer", "Matthew", ""], ["Cases", "Ignacio", ""], ["Rosenbaum", "Clemens", ""], ["Liu", "Miao", ""], ["Tesauro", "Gerald", ""]]}, {"id": "1912.13414", "submitter": "Xingyu Lu", "authors": "Xingyu Lu, Stas Tiomkin, Pieter Abbeel", "title": "Predictive Coding for Boosting Deep Reinforcement Learning with Sparse\n  Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent progress in deep reinforcement learning has enabled robots to\nlearn complex behaviors, tasks with long horizons and sparse rewards remain an\nongoing challenge. In this work, we propose an effective reward shaping method\nthrough predictive coding to tackle sparse reward problems. By learning\npredictive representations offline and using these representations for reward\nshaping, we gain access to reward signals that understand the structure and\ndynamics of the environment. In particular, our method achieves better learning\nby providing reward signals that 1) understand environment dynamics 2)\nemphasize on features most useful for learning 3) resist noise in learned\nrepresentations through reward accumulation. We demonstrate the usefulness of\nthis approach in different domains ranging from robotic manipulation to\nnavigation, and we show that reward signals produced through predictive coding\nare as effective for learning as hand-crafted rewards.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 03:32:00 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 01:28:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lu", "Xingyu", ""], ["Tiomkin", "Stas", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1912.13421", "submitter": "Zacharie Naulet", "authors": "Yasaman Mahdaviyeh, Zacharie Naulet", "title": "Risk of the Least Squares Minimum Norm Estimator under the Spike\n  Covariance Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study risk of the minimum norm linear least squares estimator in when the\nnumber of parameters $d$ depends on $n$, and $\\frac{d}{n} \\rightarrow \\infty$.\nWe assume that data has an underlying low rank structure by restricting\nourselves to spike covariance matrices, where a fixed finite number of\neigenvalues grow with $n$ and are much larger than the rest of the eigenvalues,\nwhich are (asymptotically) in the same order. We show that in this setting risk\nof minimum norm least squares estimator vanishes in compare to risk of the null\nestimator. We give asymptotic and non asymptotic upper bounds for this risk,\nand also leverage the assumption of spike model to give an analysis of the bias\nthat leads to tighter bounds in compare to previous works.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 16:58:42 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 08:26:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Mahdaviyeh", "Yasaman", ""], ["Naulet", "Zacharie", ""]]}, {"id": "1912.13440", "submitter": "Vidhi Lalchand Miss", "authors": "Vidhi Lalchand and Carl Edward Rasmussen", "title": "Approximate Inference for Fully Bayesian Gaussian Process Regression", "comments": "Presented at 2nd Symposium on Advances in Approximate Bayesian\n  Inference 2019", "journal-ref": "Proceedings of Machine Learning Research, Volume 118 (2019) 1-12", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in Gaussian Process models occurs through the adaptation of\nhyperparameters of the mean and the covariance function. The classical approach\nentails maximizing the marginal likelihood yielding fixed point estimates (an\napproach called \\textit{Type II maximum likelihood} or ML-II). An alternative\nlearning procedure is to infer the posterior over hyperparameters in a\nhierarchical specification of GPs we call \\textit{Fully Bayesian Gaussian\nProcess Regression} (GPR). This work considers two approximation schemes for\nthe intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC)\nyielding a sampling-based approximation and 2) Variational Inference (VI) where\nthe posterior over hyperparameters is approximated by a factorized Gaussian\n(mean-field) or a full-rank Gaussian accounting for correlations between\nhyperparameters. We analyze the predictive performance for fully Bayesian GPR\non a range of benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:18:48 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 14:22:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lalchand", "Vidhi", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1912.13445", "submitter": "Krishna Pillutla", "authors": "Krishna Pillutla, Sham M. Kakade, Zaid Harchaoui", "title": "Robust Aggregation for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust aggregation approach to make federated learning robust to\nsettings when a fraction of the devices may be sending corrupted updates to the\nserver. The proposed approach relies on a robust secure aggregation oracle\nbased on the geometric median, which returns a robust aggregate using a\nconstant number of calls to a regular non-robust secure average oracle. The\nrobust aggregation oracle is privacy-preserving, similar to the secure average\noracle it builds upon. We provide experimental results of the proposed approach\nwith linear models and deep networks for two tasks in computer vision and\nnatural language processing. The robust aggregation approach is agnostic to the\nlevel of corruption; it outperforms the classical aggregation approach in terms\nof robustness when the level of corruption is high, while being competitive in\nthe regime of low corruption.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:24:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Pillutla", "Krishna", ""], ["Kakade", "Sham M.", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1912.13463", "submitter": "Kaizheng Wang", "authors": "Kaizheng Wang", "title": "Some compact notations for concentration inequalities and user-friendly\n  results", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents compact notations for concentration inequalities and\nconvenient results to streamline probabilistic analysis. The new expressions\ndescribe the typical sizes and tails of random variables, allowing for simple\noperations without heavy use of inessential constants. They bridge classical\nasymptotic notations and modern non-asymptotic tail bounds together. Examples\nof different kinds demonstrate their efficacy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:03:19 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:57:14 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kaizheng", ""]]}, {"id": "1912.13464", "submitter": "Aviral Kumar", "authors": "Aviral Kumar, Sergey Levine", "title": "Model Inversion Networks for Model-Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to solve data-driven optimization problems, where the\ngoal is to find an input that maximizes an unknown score function given access\nto a dataset of inputs with corresponding scores. When the inputs are\nhigh-dimensional and valid inputs constitute a small subset of this space\n(e.g., valid protein sequences or valid natural images), such model-based\noptimization problems become exceptionally difficult, since the optimizer must\navoid out-of-distribution and invalid inputs. We propose to address such\nproblem with model inversion networks (MINs), which learn an inverse mapping\nfrom scores to inputs. MINs can scale to high-dimensional input spaces and\nleverage offline logged data for both contextual and non-contextual\noptimization problems. MINs can also handle both purely offline data sources\nand active data collection. We evaluate MINs on tasks from the Bayesian\noptimization literature, high-dimensional model-based optimization problems\nover images and protein designs, and contextual bandit optimization from logged\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:06:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kumar", "Aviral", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.13465", "submitter": "Aviral Kumar", "authors": "Aviral Kumar, Xue Bin Peng, Sergey Levine", "title": "Reward-Conditioned Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning offers the promise of automating the acquisition of\ncomplex behavioral skills. However, compared to commonly used and\nwell-understood supervised learning methods, reinforcement learning algorithms\ncan be brittle, difficult to use and tune, and sensitive to seemingly innocuous\nimplementation decisions. In contrast, imitation learning utilizes standard and\nwell-understood supervised learning methods, but requires near-optimal expert\ndata. Can we learn effective policies via supervised learning without\ndemonstrations? The main idea that we explore in this work is that non-expert\ntrajectories collected from sub-optimal policies can be viewed as optimal\nsupervision, not for maximizing the reward, but for matching the reward of the\ngiven trajectory. By then conditioning the policy on the numerical value of the\nreward, we can obtain a policy that generalizes to larger returns. We show how\nsuch an approach can be derived as a principled method for policy search,\ndiscuss several variants, and compare the method experimentally to a variety of\ncurrent reinforcement learning methods on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:07:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kumar", "Aviral", ""], ["Peng", "Xue Bin", ""], ["Levine", "Sergey", ""]]}, {"id": "1912.13472", "submitter": "Shiyu Liang", "authors": "Shiyu Liang, Ruoyu Sun and R. Srikant", "title": "Revisiting Landscape Analysis in Deep Neural Networks: Eliminating\n  Decreasing Paths to Infinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional landscape analysis of deep neural networks aims to show that no\nsub-optimal local minima exist in some appropriate sense. From this, one may be\ntempted to conclude that descent algorithms which escape saddle points will\nreach a good local minimum. However, basic optimization theory tell us that it\nis also possible for a descent algorithm to diverge to infinity if there are\npaths leading to infinity, along which the loss function decreases. It is not\nclear whether for non-linear neural networks there exists one setting that no\nbad local-min and no decreasing paths to infinity can be simultaneously\nachieved. In this paper, we give the first positive answer to this question.\nMore specifically, for a large class of over-parameterized deep neural networks\nwith appropriate regularizers, the loss function has no bad local minima and no\ndecreasing paths to infinity. The key mathematical trick is to show that the\nset of regularizers which may be undesirable can be viewed as the image of a\nLipschitz continuous mapping from a lower-dimensional Euclidean space to a\nhigher-dimensional Euclidean space, and thus has zero measure.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:17:34 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Liang", "Shiyu", ""], ["Sun", "Ruoyu", ""], ["Srikant", "R.", ""]]}, {"id": "1912.13480", "submitter": "Aleksander Wieczorek", "authors": "Aleksander Wieczorek and Volker Roth", "title": "On the Difference Between the Information Bottleneck and the Deep\n  Information Bottleneck", "comments": null, "journal-ref": null, "doi": "10.3390/e22020131", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining the Information Bottleneck model with deep learning by replacing\nmutual information terms with deep neural nets has proved successful in areas\nranging from generative modelling to interpreting deep neural networks. In this\npaper, we revisit the Deep Variational Information Bottleneck and the\nassumptions needed for its derivation. The two assumed properties of the data\n$X$, $Y$ and their latent representation $T$ take the form of two Markov chains\n$T-X-Y$ and $X-T-Y$. Requiring both to hold during the optimisation process can\nbe limiting for the set of potential joint distributions $P(X,Y,T)$. We\ntherefore show how to circumvent this limitation by optimising a lower bound\nfor $I(T;Y)$ for which only the latter Markov chain has to be satisfied. The\nactual mutual information consists of the lower bound which is optimised in\nDVIB and cognate models in practice and of two terms measuring how much the\nformer requirement $T-X-Y$ is violated. Finally, we propose to interpret the\nfamily of information bottleneck models as directed graphical models and show\nthat in this framework the original and deep information bottlenecks are\nspecial cases of a fundamental IB model.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:31:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Wieczorek", "Aleksander", ""], ["Roth", "Volker", ""]]}, {"id": "1912.13490", "submitter": "Gianluca Baldassarre PhD", "authors": "Gianluca Baldassarre and Giovanni Granato", "title": "Representation Internal-Manipulation (RIM): A Neuro-Inspired\n  Computational Theory of Consciousness", "comments": "16 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theories, based on neuroscientific and psychological empirical evidence\nand on computational concepts, have been elaborated to explain the emergence of\nconsciousness in the central nervous system. These theories propose key\nfundamental mechanisms to explain consciousness, but they only partially\nconnect such mechanisms to the possible functional and adaptive role of\nconsciousness. Recently, some cognitive and neuroscientific models try to solve\nthis gap by linking consciousness to various aspects of goal-directed\nbehaviour, the pivotal cognitive process that allows mammals to flexibly act in\nchallenging environments. Here we propose the Representation\nInternal-Manipulation (RIM) theory of consciousness, a theory that links the\nmain elements of consciousness theories to components and functions of\ngoal-directed behaviour, ascribing a central role for consciousness to the\ngoal-directed manipulation of internal representations. This manipulation\nrelies on four specific computational operations to perform the flexible\ninternal adaptation of all key elements of goal-directed computation, from the\nrepresentations of objects to those of goals, actions, and plans. Finally, we\npropose the concept of `manipulation agency' relating the sense of agency to\nthe internal manipulation of representations. This allows us to propose that\nthe subjective experience of consciousness is associated to the human capacity\nto generate and control a simulated internal reality that is vividly perceived\nand felt through the same perceptual and emotional mechanisms used to tackle\nthe external world.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:45:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Baldassarre", "Gianluca", ""], ["Granato", "Giovanni", ""]]}, {"id": "1912.13515", "submitter": "Huizhuo Yuan", "authors": "Huizhuo Yuan, Xiangru Lian, Ji Liu", "title": "Stochastic Recursive Variance Reduction for Efficient Smooth Non-Convex\n  Compositional Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic compositional optimization arises in many important machine\nlearning tasks such as value function evaluation in reinforcement learning and\nportfolio management. The objective function is the composition of two\nexpectations of stochastic functions, and is more challenging to optimize than\nvanilla stochastic optimization problems. In this paper, we investigate the\nstochastic compositional optimization in the general smooth non-convex setting.\nWe employ a recently developed idea of \\textit{Stochastic Recursive Gradient\nDescent} to design a novel algorithm named SARAH-Compositional, and prove a\nsharp Incremental First-order Oracle (IFO) complexity upper bound for\nstochastic compositional optimization: $\\mathcal{O}((n+m)^{1/2}\n\\varepsilon^{-2})$ in the finite-sum case and $\\mathcal{O}(\\varepsilon^{-3})$\nin the online case. Such a complexity is known to be the best one among IFO\ncomplexity results for non-convex stochastic compositional optimization, and is\nbelieved to be optimal. Our experiments validate the theoretical performance of\nour algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:59:13 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 10:52:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yuan", "Huizhuo", ""], ["Lian", "Xiangru", ""], ["Liu", "Ji", ""]]}]