[{"id": "1401.0062", "submitter": "Creighton Heaukulani", "authors": "Creighton Heaukulani, Daniel M. Roy", "title": "The combinatorial structure of beta negative binomial processes", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ729 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 4, 2301-2324", "doi": "10.3150/15-BEJ729", "report-no": "IMS-BEJ-BEJ729", "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the combinatorial structure of conditionally-i.i.d. sequences\nof negative binomial processes with a common beta process base measure. In\nBayesian nonparametric applications, such processes have served as models for\nlatent multisets of features underlying data. Analogously, random subsets arise\nfrom conditionally-i.i.d. sequences of Bernoulli processes with a common beta\nprocess base measure, in which case the combinatorial structure is described by\nthe Indian buffet process. Our results give a count analogue of the Indian\nbuffet process, which we call a negative binomial Indian buffet process. As an\nintermediate step toward this goal, we provide a construction for the beta\nnegative binomial process that avoids a representation of the underlying beta\nprocess base measure. We describe the key Markov kernels needed to use a NB-IBP\nrepresentation in a Markov Chain Monte Carlo algorithm targeting a posterior\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 00:48:01 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 15:09:53 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2015 23:33:40 GMT"}, {"version": "v4", "created": "Thu, 23 Jun 2016 08:21:20 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Heaukulani", "Creighton", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1401.0086", "submitter": "Ji Liu", "authors": "Ji Liu and Ryohei Fujimaki and Jieping Ye", "title": "Forward-Backward Greedy Algorithms for General Convex Smooth Functions\n  over A Cardinality Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forward-backward greedy algorithms for solving sparse feature\nselection problems with general convex smooth functions. A state-of-the-art\ngreedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to\nsolve a large number of optimization problems, thus it is not scalable for\nlarge-size problems. The FoBa-gdt algorithm, which uses the gradient\ninformation for feature selection at each forward iteration, significantly\nimproves the efficiency of FoBa-obj. In this paper, we systematically analyze\nthe theoretical properties of both forward-backward greedy algorithms. Our main\ncontributions are: 1) We derive better theoretical bounds than existing\nanalyses regarding FoBa-obj for general smooth convex functions; 2) We show\nthat FoBa-gdt achieves the same theoretical performance as FoBa-obj under the\nsame condition: restricted strong convexity condition. Our new bounds are\nconsistent with the bounds of a special case (least squares) and fills a\npreviously existing theoretical gap for general convex smooth functions; 3) We\nshow that the restricted strong convexity condition is satisfied if the number\nof independent samples is more than $\\bar{k}\\log d$ where $\\bar{k}$ is the\nsparsity number and $d$ is the dimension of the variable; 4) We apply FoBa-gdt\n(with the conditional random field objective) to the sensor selection problem\nfor human indoor activity recognition and our results show that FoBa-gdt\noutperforms other methods (including the ones based on forward greedy selection\nand L1-regularization).\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 04:00:31 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 18:20:18 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Liu", "Ji", ""], ["Fujimaki", "Ryohei", ""], ["Ye", "Jieping", ""]]}, {"id": "1401.0104", "submitter": "Tao Xiong", "authors": "Yukun Bao, Tao Xiong, Zhongyi Hu", "title": "PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction", "comments": "14 pages. IEEE Transactions on Cybernetics. 2013", "journal-ref": null, "doi": "10.1109/TCYB.2013.2265084", "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step-ahead time series prediction is one of the most challenging\nresearch topics in the field of time series modeling and prediction, and is\ncontinually under research. Recently, the multiple-input several\nmultiple-outputs (MISMO) modeling strategy has been proposed as a promising\nalternative for multi-step-ahead time series prediction, exhibiting advantages\ncompared with the two currently dominating strategies, the iterated and the\ndirect strategies. Built on the established MISMO strategy, this study proposes\na particle swarm optimization (PSO)-based MISMO modeling strategy, which is\ncapable of determining the number of sub-models in a self-adaptive mode, with\nvarying prediction horizons. Rather than deriving crisp divides with equal-size\ns prediction horizons from the established MISMO, the proposed PSO-MISMO\nstrategy, implemented with neural networks, employs a heuristic to create\nflexible divides with varying sizes of prediction horizons and to generate\ncorresponding sub-models, providing considerable flexibility in model\nconstruction, which has been validated with simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 07:09:02 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bao", "Yukun", ""], ["Xiong", "Tao", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.0118", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath and Sean Gerrish and David M. Blei", "title": "Black Box Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has become a widely used method to approximate\nposteriors in complex latent variables models. However, deriving a variational\ninference algorithm generally requires significant model-specific analysis, and\nthese efforts can hinder and deter us from quickly developing and exploring a\nvariety of models for a problem at hand. In this paper, we present a \"black\nbox\" variational inference algorithm, one that can be quickly applied to many\nmodels with little additional derivation. Our method is based on a stochastic\noptimization of the variational objective where the noisy gradient is computed\nfrom Monte Carlo samples from the variational distribution. We develop a number\nof methods to reduce the variance of the gradient, always maintaining the\ncriterion that we want to avoid difficult model-based derivations. We evaluate\nour method against the corresponding black box sampling based methods. We find\nthat our method reaches better predictive likelihoods much faster than sampling\nmethods. Finally, we demonstrate that Black Box Variational Inference lets us\neasily explore a wide space of models by quickly constructing and evaluating\nseveral models of longitudinal healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 09:32:43 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Gerrish", "Sean", ""], ["Blei", "David M.", ""]]}, {"id": "1401.0211", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng, Jiancheng Jiang and Xin Tong", "title": "Feature Augmentation via Nonparametrics and Selection (FANS) in High\n  Dimensional Classification", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high dimensional classification method that involves\nnonparametric feature augmentation. Knowing that marginal density ratios are\nthe most powerful univariate classifiers, we use the ratio estimates to\ntransform the original feature measurements. Subsequently, penalized logistic\nregression is invoked, taking as input the newly transformed or augmented\nfeatures. This procedure trains models equipped with local complexity and\nglobal simplicity, thereby avoiding the curse of dimensionality while creating\na flexible nonlinear decision boundary. The resulting method is called Feature\nAugmentation via Nonparametrics and Selection (FANS). We motivate FANS by\ngeneralizing the Naive Bayes model, writing the log ratio of joint densities as\na linear combination of those of marginal densities. It is related to\ngeneralized additive models, but has better interpretability and computability.\nRisk bounds are developed for FANS. In numerical analysis, FANS is compared\nwith competing methods, so as to provide a guideline on its best application\ndomain. Real data analysis demonstrates that FANS performs very competitively\non benchmark email spam and gene expression data sets. Moreover, FANS is\nimplemented by an extremely fast algorithm through parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 19:53:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 17:27:38 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Jiang", "Jiancheng", ""], ["Tong", "Xin", ""]]}, {"id": "1401.0304", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Learning without Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain sharp bounds on the performance of Empirical Risk Minimization\nperformed in a convex class and with respect to the squared loss, without\nassuming that class members and the target are bounded functions or have\nrapidly decaying tails.\n  Rather than resorting to a concentration-based argument, the method used here\nrelies on a `small-ball' assumption and thus holds for classes consisting of\nheavy-tailed functions and for heavy-tailed targets.\n  The resulting estimates scale correctly with the `noise level' of the\nproblem, and when applied to the classical, bounded scenario, always improve\nthe known bounds.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 16:28:19 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 17:59:50 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1401.0334", "submitter": "Vladimir Temlyakov", "authors": "R.A. DeVore and V.N. Temlyakov", "title": "Convex optimization on Banach Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy algorithms which use only function evaluations are applied to convex\noptimization in a general Banach space $X$. Along with algorithms that use\nexact evaluations, algorithms with approximate evaluations are treated. A\npriori upper bounds for the convergence rate of the proposed algorithms are\ngiven. These bounds depend on the smoothness of the objective function and the\nsparsity or compressibility (with respect to a given dictionary) of a point in\n$X$ where the minimum is attained.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 21:14:10 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["DeVore", "R. A.", ""], ["Temlyakov", "V. N.", ""]]}, {"id": "1401.0376", "submitter": "Chao Zhang", "authors": "Chao Zhang, Lei Zhang, Wei Fan, Jieping Ye", "title": "Generalization Bounds for Representative Domain Adaptation", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.1574", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework to analyze the theoretical\nproperties of the learning process for a representative type of domain\nadaptation, which combines data from multiple sources and one target (or\nbriefly called representative domain adaptation). In particular, we use the\nintegral probability metric to measure the difference between the distributions\nof two domains and meanwhile compare it with the H-divergence and the\ndiscrepancy distance. We develop the Hoeffding-type, the Bennett-type and the\nMcDiarmid-type deviation inequalities for multiple domains respectively, and\nthen present the symmetrization inequality for representative domain\nadaptation. Next, we use the derived inequalities to obtain the Hoeffding-type\nand the Bennett-type generalization bounds respectively, both of which are\nbased on the uniform entropy number. Moreover, we present the generalization\nbounds based on the Rademacher complexity. Finally, we analyze the asymptotic\nconvergence and the rate of convergence of the learning process for\nrepresentative domain adaptation. We discuss the factors that affect the\nasymptotic behavior of the learning process and the numerical experiments\nsupport our theoretical findings as well. Meanwhile, we give a comparison with\nthe existing results of domain adaptation and the classical results under the\nsame-distribution assumption.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 07:32:01 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Zhang", "Chao", ""], ["Zhang", "Lei", ""], ["Fan", "Wei", ""], ["Ye", "Jieping", ""]]}, {"id": "1401.0514", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison and Daniel Tarlow", "title": "Structured Generative Models of Natural Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of building generative models of natural source code\n(NSC); that is, source code written and understood by humans. Our primary\ncontribution is to describe a family of generative models for NSC that have\nthree key properties: First, they incorporate both sequential and hierarchical\nstructure. Second, we learn a distributed representation of source code\nelements. Finally, they integrate closely with a compiler, which allows\nleveraging compiler logic and abstractions when building structure into the\nmodel. We also develop an extension that includes more complex structure,\nrefining how the model generates identifier tokens based on what variables are\ncurrently in scope. Our models can be learned efficiently, and we show\nempirically that including appropriate structure greatly improves the models,\nmeasured by the probability of generating test programs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 19:35:31 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 08:12:20 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1401.0579", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma", "title": "More Algorithms for Provable Dictionary Learning", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 02:52:17 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Arora", "Sanjeev", ""], ["Bhaskara", "Aditya", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1401.0604", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten and Michael I. Jordan and Thomas B. Sch\\\"on", "title": "Particle Gibbs with Ancestor Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, 15 (2014) 2145-2184", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining\nthe two main tools used for Monte Carlo statistical inference: sequential Monte\nCarlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC\nalgorithm that we refer to as particle Gibbs with ancestor sampling (PGAS).\nPGAS provides the data analyst with an off-the-shelf class of Markov kernels\nthat can be used to simulate the typically high-dimensional and highly\nautocorrelated state trajectory in a state-space model. The ancestor sampling\nprocedure enables fast mixing of the PGAS kernel even when using seemingly few\nparticles in the underlying SMC sampler. This is important as it can\nsignificantly reduce the computational burden that is typically associated with\nusing SMC. PGAS is conceptually similar to the existing PG with backward\nsimulation (PGBS) procedure. Instead of using separate forward and backward\nsweeps as in PGBS, however, we achieve the same effect in a single forward\nsweep. This makes PGAS well suited for addressing inference problems not only\nin state-space models, but also in models with more complex dependencies, such\nas non-Markovian, Bayesian nonparametric, and general probabilistic graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 08:17:51 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Jordan", "Michael I.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1401.0711", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit\n  Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy rate of sequential data-streams naturally quantifies the complexity\nof the generative process. Thus entropy rate fluctuations could be used as a\ntool to recognize dynamical perturbations in signal sources, and could\npotentially be carried out without explicit background noise characterization.\nHowever, state of the art algorithms to estimate the entropy rate have markedly\nslow convergence; making such entropic approaches non-viable in practice. We\npresent here a fundamentally new approach to estimate entropy rates, which is\ndemonstrated to converge significantly faster in terms of input data lengths,\nand is shown to be effective in diverse applications ranging from the\nestimation of the entropy rate of English texts to the estimation of complexity\nof chaotic dynamical systems. Additionally, the convergence rate of entropy\nestimates do not follow from any standard limit theorem, and reported\nalgorithms fail to provide any confidence bounds on the computed values.\nExploiting a connection to the theory of probabilistic automata, we establish a\nconvergence rate of $O(\\log \\vert s \\vert/\\sqrt[3]{\\vert s \\vert})$ as a\nfunction of the input length $\\vert s \\vert$, which then yields explicit\nuncertainty estimates, as well as required data lengths to satisfy\npre-specified confidence bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 20:30:01 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 07:29:34 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0742", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Data Smashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the underlying physics or biology from empirical data\nrequires a quantifiable notion of similarity - when do two observed data sets\nindicate nearly identical generating processes, and when they do not. The\ndiscriminating characteristics to look for in data is often determined by\nheuristics designed by experts, $e.g.$, distinct shapes of \"folded\" lightcurves\nmay be used as \"features\" to classify variable stars, while determination of\npathological brain states might require a Fourier analysis of brainwave\nactivity. Finding good features is non-trivial. Here, we propose a universal\nsolution to this problem: we delineate a principle for quantifying similarity\nbetween sources of arbitrary data streams, without a priori knowledge, features\nor training. We uncover an algebraic structure on a space of symbolic models\nfor quantized data, and show that such stochastic generators may be added and\nuniquely inverted; and that a model and its inverse always sum to the generator\nof flat white noise. Therefore, every data stream has an anti-stream: data\ngenerated by the inverse model. Similarity between two streams, then, is the\ndegree to which one, when summed to the other's anti-stream, mutually\nannihilates all statistical structure to noise. We call this data smashing. We\npresent diverse applications, including disambiguation of brainwaves pertaining\nto epileptic seizures, detection of anomalous cardiac rhythms, and\nclassification of astronomical objects from raw photometry. In our examples,\nthe data smashing principle, without access to any domain knowledge, meets or\nexceeds the performance of specialized algorithms tuned by domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 22:15:17 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0852", "submitter": "Qing Zhou", "authors": "Bryon Aragam and Qing Zhou", "title": "Concave Penalized Estimation of Sparse Gaussian Bayesian Networks", "comments": "57 pages", "journal-ref": "Journal of Machine Learning Research 16(Nov):2273-2328, 2015", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a penalized likelihood estimation framework to estimate the\nstructure of Gaussian Bayesian networks from observational data. In contrast to\nrecent methods which accelerate the learning problem by restricting the search\nspace, our main contribution is a fast algorithm for score-based structure\nlearning which does not restrict the search space in any way and works on\nhigh-dimensional datasets with thousands of variables. Our use of concave\nregularization, as opposed to the more popular $\\ell_0$ (e.g. BIC) penalty, is\nnew. Moreover, we provide theoretical guarantees which generalize existing\nasymptotic results when the underlying distribution is Gaussian. Most notably,\nour framework does not require the existence of a so-called faithful DAG\nrepresentation, and as a result the theory must handle the inherent\nnonidentifiability of the estimation problem in a novel way. Finally, as a\nmatter of independent interest, we provide a comprehensive comparison of our\napproach to several standard structure learning methods using open-source\npackages developed for the R language. Based on these experiments, we show that\nour algorithm is significantly faster than other competing methods while\nobtaining higher sensitivity with comparable false discovery rates for\nhigh-dimensional data. In particular, the total runtime for our method to\ngenerate a solution path of 20 estimates for DAGs with 8000 nodes is around one\nhour.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 23:27:48 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 23:34:01 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1401.0869", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Yong Zhang", "title": "Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative\n  Reweighted Singular Value Minimization", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularized\nmatrix minimization problems. In particular, we first introduce a class of\nfirst-order stationary points for them, and show that the first-order\nstationary points introduced in [11] for an SPQN regularized $vector$\nminimization problem are equivalent to those of an SPQN regularized $matrix$\nminimization reformulation. We also show that any local minimizer of the SPQN\nregularized matrix minimization problems must be a first-order stationary\npoint. Moreover, we derive lower bounds for nonzero singular values of the\nfirst-order stationary points and hence also of the local minimizers of the\nSPQN regularized matrix minimization problems. The iterative reweighted\nsingular value minimization (IRSVM) methods are then proposed to solve these\nproblems, whose subproblems are shown to have a closed-form solution. In\ncontrast to the analogous methods for the SPQN regularized $vector$\nminimization problems, the convergence analysis of these methods is\nsignificantly more challenging. We develop a novel approach to establishing the\nconvergence of these methods, which makes use of the expression of a specific\nsolution of their subproblems and avoids the intricate issue of finding the\nexplicit expression for the Clarke subdifferential of the objective of their\nsubproblems. In particular, we show that any accumulation point of the sequence\ngenerated by the IRSVM methods is a first-order stationary point of the\nproblems. Our computational results demonstrate that the IRSVM methods\ngenerally outperform some recently developed state-of-the-art methods in terms\nof solution quality and/or speed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 06:37:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 18:41:44 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 17:30:58 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1401.0871", "submitter": "Sakellarios Zairis", "authors": "Amy Rebecca Gansell, Jan-Willem van de Meent, Sakellarios Zairis,\n  Chris H. Wiggins", "title": "Stylistic Clusters and the Syrian/South Syrian Tradition of\n  First-Millennium BCE Levantine Ivory Carving: A Machine Learning Approach", "comments": "28 pages, 16 figures, accepted for publication in the Journal of\n  Archaeological Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thousands of first-millennium BCE ivory carvings have been excavated from\nNeo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and Arslan\nTash) hundreds of miles from their Levantine production contexts. At present,\ntheir specific manufacture dates and workshop localities are unknown. Relying\non subjective, visual methods, scholars have grappled with their classification\nand regional attribution for over a century. This study combines visual\napproaches with machine-learning techniques to offer data-driven perspectives\non the classification and attribution of this early Iron Age corpus. The study\nsample consisted of 162 sculptures of female figures. We have developed an\nalgorithm that clusters the ivories based on a combination of descriptive and\nanthropometric data. The resulting categories, which are based on purely\nstatistical criteria, show good agreement with conventional art historical\nclassifications, while revealing new perspectives, especially with regard to\nthe contested Syrian/South Syrian/Intermediate tradition. Specifically, we have\nidentified that objects of the Syrian/South Syrian/Intermediate tradition may\nbe more closely related to Phoenician objects than to North Syrian objects; we\noffer a reconsideration of a subset of Phoenician objects, and we confirm\nSyrian/South Syrian/Intermediate stylistic subgroups that might distinguish\nnetworks of acquisition among the sites of Nimrud, Khorsabad, Arslan Tash and\nthe Levant. We have also identified which features are most significant in our\ncluster assignments and might thereby be most diagnostic of regional carving\ntraditions. In short, our study both corroborates traditional visual\nclassification methods and demonstrates how machine-learning techniques may be\nemployed to reveal complementary information not accessible through the\nexclusively visual analysis of an archaeological corpus.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 08:15:54 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Gansell", "Amy Rebecca", ""], ["van de Meent", "Jan-Willem", ""], ["Zairis", "Sakellarios", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1401.0872", "submitter": "Philip Schniter", "authors": "Justin Ziniel, Philip Schniter, and Per Sederberg", "title": "Binary Linear Classification and Feature Selection via Generalized\n  Approximate Message Passing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2407311", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of binary linear classification and feature selection, we\npropose algorithmic approaches to classifier design based on the generalized\napproximate message passing (GAMP) algorithm, recently proposed in the context\nof compressive sensing. We are particularly motivated by problems where the\nnumber of features greatly exceeds the number of training examples, but where\nonly a few features suffice for accurate classification. We show that\nsum-product GAMP can be used to (approximately) minimize the classification\nerror rate and max-sum GAMP can be used to minimize a wide variety of\nregularized loss functions. Furthermore, we describe an\nexpectation-maximization (EM)-based scheme to learn the associated model\nparameters online, as an alternative to cross-validation, and we show that\nGAMP's state-evolution framework can be used to accurately predict the\nmisclassification rate. Finally, we present a detailed numerical study to\nconfirm the accuracy, speed, and flexibility afforded by our GAMP-based\napproaches to binary linear classification and feature selection.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 08:23:06 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 10:11:31 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 14:24:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Ziniel", "Justin", ""], ["Schniter", "Philip", ""], ["Sederberg", "Per", ""]]}, {"id": "1401.0887", "submitter": "Dorina Thanou", "authors": "Dorina Thanou, David I Shuman, Pascal Frossard", "title": "Learning parametric dictionaries for graph signals", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2332441", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse signal representation, the choice of a dictionary often involves a\ntradeoff between two desirable properties -- the ability to adapt to specific\nsignal data and a fast implementation of the dictionary. To sparsely represent\nsignals residing on weighted graphs, an additional design challenge is to\nincorporate the intrinsic geometric structure of the irregular data domain into\nthe atoms of the dictionary. In this work, we propose a parametric dictionary\nlearning algorithm to design data-adapted, structured dictionaries that\nsparsely represent graph signals. In particular, we model graph signals as\ncombinations of overlapping local patterns. We impose the constraint that each\ndictionary is a concatenation of subdictionaries, with each subdictionary being\na polynomial of the graph Laplacian matrix, representing a single pattern\ntranslated to different areas of the graph. The learning algorithm adapts the\npatterns to a training set of graph signals. Experimental results on both\nsynthetic and real datasets demonstrate that the dictionaries learned by the\nproposed algorithm are competitive with and often better than unstructured\ndictionaries learned by state-of-the-art numerical learning algorithms in terms\nof sparse approximation of graph signals. In contrast to the unstructured\ndictionaries, however, the dictionaries learned by the proposed algorithm\nfeature localized atoms and can be implemented in a computationally efficient\nmanner in signal processing tasks such as compression, denoising, and\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 12:17:51 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Thanou", "Dorina", ""], ["Shuman", "David I", ""], ["Frossard", "Pascal", ""]]}, {"id": "1401.0898", "submitter": "Vijendra Singh", "authors": "Vijendra Singh and Shivani Pathak", "title": "Feature Selection Using Classifier in High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is frequently used as a pre-processing step to machine\nlearning. It is a process of choosing a subset of original features so that the\nfeature space is optimally reduced according to a certain evaluation criterion.\nThe central objective of this paper is to reduce the dimension of the data by\nfinding a small set of important features which can give good classification\nperformance. We have applied filter and wrapper approach with different\nclassifiers QDA and LDA respectively. A widely-used filter method is used for\nbioinformatics data i.e. a univariate criterion separately on each feature,\nassuming that there is no interaction between features and then applied\nSequential Feature Selection method. Experimental results show that filter\napproach gives better performance in respect of Misclassification Error Rate.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 14:52:27 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Singh", "Vijendra", ""], ["Pathak", "Shivani", ""]]}, {"id": "1401.0942", "submitter": "Andrew Miller", "authors": "Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry", "title": "Factorized Point Process Intensities: A Spatial Analysis of Professional\n  Basketball", "comments": "13 pages, 6 figures, fixed formatting issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a machine learning approach to represent and analyze the\nunderlying spatial structure that governs shot selection among professional\nbasketball players in the NBA. Typically, NBA players are discussed and\ncompared in an heuristic, imprecise manner that relies on unmeasured intuitions\nabout player behavior. This makes it difficult to draw comparisons between\nplayers and make accurate player specific predictions. Modeling shot attempt\ndata as a point process, we create a low dimensional representation of\noffensive player types in the NBA. Using non-negative matrix factorization\n(NMF), an unsupervised dimensionality reduction technique, we show that a\nlow-rank spatial decomposition summarizes the shooting habits of NBA players.\nThe spatial representations discovered by the algorithm correspond to intuitive\ndescriptions of NBA player types, and can be used to model other spatial\neffects, such as shooting accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 21:31:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 01:41:03 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Miller", "Andrew", ""], ["Bornn", "Luke", ""], ["Adams", "Ryan", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1401.0987", "submitter": "Chi Jin", "authors": "Chi Jin, Ziteng Wang, Junliang Huang, Yiqiao Zhong, Liwei Wang", "title": "Differentially Private Data Releasing for Smooth Queries with Synthetic\n  Database Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider accurately answering smooth queries while preserving differential\nprivacy. A query is said to be $K$-smooth if it is specified by a function\ndefined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all\nbounded. We develop an $\\epsilon$-differentially private mechanism for the\nclass of $K$-smooth queries. The major advantage of the algorithm is that it\noutputs a synthetic database. In real applications, a synthetic database output\nis appealing. Our mechanism achieves an accuracy of $O\n(n^{-\\frac{K}{2d+K}}/\\epsilon )$, and runs in polynomial time. We also\ngeneralize the mechanism to preserve $(\\epsilon, \\delta)$-differential privacy\nwith slightly improved accuracy. Extensive experiments on benchmark datasets\ndemonstrate that the mechanisms have good accuracy and are efficient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 05:12:01 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Jin", "Chi", ""], ["Wang", "Ziteng", ""], ["Huang", "Junliang", ""], ["Zhong", "Yiqiao", ""], ["Wang", "Liwei", ""]]}, {"id": "1401.1137", "submitter": "Fran\\c{c}ois Caron", "authors": "Fran\\c{c}ois Caron and Emily B. Fox", "title": "Sparse graphs using exchangeable random measures", "comments": "New title. Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical network modeling has focused on representing the graph as a\ndiscrete structure, namely the adjacency matrix, and considering the\nexchangeability of this array. In such cases, the Aldous-Hoover representation\ntheorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is\nnecessarily either dense or empty. In this paper, we instead consider\nrepresenting the graph as a measure on $\\mathbb{R}_+^2$. For the associated\ndefinition of exchangeability in this continuous space, we rely on the\nKallenberg representation theorem (Kallenberg, 2005). We show that for certain\nchoices of such exchangeable random measures underlying our graph construction,\nour network process is sparse with power-law degree distribution. In\nparticular, we build on the framework of completely random measures (CRMs) and\nuse the theory associated with such processes to derive important network\nproperties, such as an urn representation for our analysis and network\nsimulation. Our theoretical results are explored empirically and compared to\ncommon network models. We then present a Hamiltonian Monte Carlo algorithm for\nefficient exploration of the posterior distribution and demonstrate that we are\nable to recover graphs ranging from dense to sparse--and perform associated\ntests--based on our flexible CRM-based formulation. We explore network\nproperties in a range of real datasets, including Facebook social circles, a\npolitical blogosphere, protein networks, citation networks, and world wide web\nnetworks, including networks with hundreds of thousands of nodes and millions\nof edges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 16:57:16 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 15:13:26 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 12:40:04 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Fox", "Emily B.", ""]]}, {"id": "1401.1436", "submitter": "Richard Wilkinson", "authors": "Richard D Wilkinson", "title": "Accelerating ABC methods using Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods are used to approximate\nposterior distributions using simulation rather than likelihood calculations.\nWe introduce Gaussian process (GP) accelerated ABC, which we show can\nsignificantly reduce the number of simulations required. As computational\nresource is usually the main determinant of accuracy in ABC, GP-accelerated\nmethods can thus enable more accurate inference in some models. GP models of\nthe unknown log-likelihood function are used to exploit continuity and\nsmoothness, reducing the required computation. We use a sequence of models that\nincrease in accuracy, using intermediate models to rule out regions of the\nparameter space as implausible. The methods will not be suitable for all\nproblems, but when they can be used, can result in significant computational\nsavings. For the Ricker model, we are able to achieve accurate approximations\nto the posterior distribution using a factor of 100 fewer simulator evaluations\nthan comparable Monte Carlo approaches, and for a population genetics model we\nare able to approximate the exact posterior for the first time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 16:38:14 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2014 20:29:36 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Wilkinson", "Richard D", ""]]}, {"id": "1401.1489", "submitter": "Romain H\\'erault", "authors": "John Komar and Romain H\\'erault and Ludovic Seifert", "title": "Key point selection and clustering of swimmer coordination through\n  Sparse Fisher-EM", "comments": "Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\n  Mining for Sports Analytics (MLSA2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To answer the existence of optimal swimmer learning/teaching strategies, this\nwork introduces a two-level clustering in order to analyze temporal dynamics of\nmotor learning in breaststroke swimming. Each level have been performed through\nSparse Fisher-EM, a unsupervised framework which can be applied efficiently on\nlarge and correlated datasets. The induced sparsity selects key points of the\ncoordination phase without any prior knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 20:16:05 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Komar", "John", ""], ["H\u00e9rault", "Romain", ""], ["Seifert", "Ludovic", ""]]}, {"id": "1401.1605", "submitter": "James Hensman", "authors": "James Hensman and Magnus Rattray and Neil D. Lawrence", "title": "Fast nonparametric clustering of structured time-series", "comments": "Accepted for publication in special edition of TPAMI on Bayesian\n  Nonparametrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this publication, we combine two Bayesian non-parametric models: the\nGaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP\nmodel is to introduce a variation on the GP prior which enables us to model\nstructured time-series data, i.e. data containing groups where we wish to model\ninter- and intra-group variability. Our innovation in the DP model is an\nimplementation of a new fast collapsed variational inference procedure which\nenables us to optimize our variationala pproximation significantly faster than\nstandard VB approaches. In a biological time series application we show how our\nmodel better captures salient features of the data, leading to better\nconsistency with existing biological classifications, while the associated\ninference algorithm provides a twofold speed-up over EM-based variational\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 08:47:44 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 08:04:46 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Hensman", "James", ""], ["Rattray", "Magnus", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1401.1803", "submitter": "Stanislas Lauly", "authors": "Stanislas Lauly, Alex Boulanger, Hugo Larochelle", "title": "Learning Multilingual Word Representations using a Bag-of-Words\n  Autoencoder", "comments": "This workshop paper was accepted on Octoble 30 2013 at the NIPS 2013\n  workshop on deep learning\n  (https://sites.google.com/site/deeplearningworkshopnips2013/accepted-papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on learning multilingual word representations usually relies on\nthe use of word-level alignements (e.g. infered with the help of GIZA++)\nbetween translated sentences, in order to align the word embeddings in\ndifferent languages. In this workshop paper, we investigate an autoencoder\nmodel for learning multilingual word representations that does without such\nword-level alignements. The autoencoder is trained to reconstruct the\nbag-of-word representation of given sentence from an encoded representation\nextracted from its translation. We evaluate our approach on a multilingual\ndocument classification task, where labeled data is available only for one\nlanguage (e.g. English) while classification must be performed in a different\nlanguage (e.g. French). In our experiments, we observe that our method compares\nfavorably with a previously proposed method that exploits word-level alignments\nto learn word representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 20:36:57 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Lauly", "Stanislas", ""], ["Boulanger", "Alex", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1401.1842", "submitter": "Shuchin Aeron", "authors": "Jason Gejie Liu and Shuchin Aeron", "title": "Robust Large Scale Non-negative Matrix Factorization using Proximal\n  Point Algorithm", "comments": "Appeared in IEEE GlobalSIP, 2013, TX, Austin", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust algorithm for non-negative matrix factorization (NMF) is presented\nin this paper with the purpose of dealing with large-scale data, where the\nseparability assumption is satisfied. In particular, we modify the Linear\nProgramming (LP) algorithm of [9] by introducing a reduced set of constraints\nfor exact NMF. In contrast to the previous approaches, the proposed algorithm\ndoes not require the knowledge of factorization rank (extreme rays [3] or\ntopics [7]). Furthermore, motivated by a similar problem arising in the context\nof metabolic network analysis [13], we consider an entirely different regime\nwhere the number of extreme rays or topics can be much larger than the\ndimension of the data vectors. The performance of the algorithm for different\nsynthetic data sets are provided.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 21:39:03 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Liu", "Jason Gejie", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1401.1895", "submitter": "Mahdi Shahbaba PhD", "authors": "Mahdi Shahbaba and Soosan Beheshti", "title": "Efficient unimodality test in clustering by signature testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a new unimodality test with application in hierarchical\nclustering methods. The proposed method denoted by signature test (Sigtest),\ntransforms the data based on its statistics. The transformed data has much\nsmaller variation compared to the original data and can be evaluated in a\nsimple proposed unimodality test. Compared with the existing unimodality tests,\nSigtest is more accurate in detecting the overlapped clusters and has a much\nless computational complexity. Simulation results demonstrate the efficiency of\nthis statistic test for both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 05:16:35 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Shahbaba", "Mahdi", ""], ["Beheshti", "Soosan", ""]]}, {"id": "1401.1926", "submitter": "Zhongyi Hu", "authors": "Yukun Bao, Zhongyi Hu, Tao Xiong", "title": "A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters\n  Optimization", "comments": "27 pages. Neurocomputing, 2013", "journal-ref": null, "doi": "10.1016/j.neucom.2013.01.027", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing the issue of SVMs parameters optimization, this study proposes an\nefficient memetic algorithm based on Particle Swarm Optimization algorithm\n(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is\nresponsible for exploration of the search space and the detection of the\npotential regions with optimum solutions, while pattern search (PS) is used to\nproduce an effective exploitation on the potential regions obtained by PSO.\nMoreover, a novel probabilistic selection strategy is proposed to select the\nappropriate individuals among the current population to undergo local\nrefinement, keeping a well balance between exploration and exploitation.\nExperimental results confirm that the local refinement with PS and our proposed\nselection strategy are effective, and finally demonstrate effectiveness and\nrobustness of the proposed PSO-PS based MA for SVMs parameters optimization.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 08:41:55 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""], ["Xiong", "Tao", ""]]}, {"id": "1401.1974", "submitter": "Vu Nguyen", "authors": "Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui", "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts", "comments": "Full version of ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian nonparametric framework for multilevel clustering which\nutilizes group-level context information to simultaneously discover\nlow-dimensional structures of the group contents and partitions groups into\nclusters. Using the Dirichlet process as the building block, our model\nconstructs a product base-measure with a nested structure to accommodate\ncontent and context observations at multiple levels. The proposed model\npossesses properties that link the nested Dirichlet processes (nDP) and the\nDirichlet process mixture models (DPM) in an interesting way: integrating out\nall contents results in the DPM over contexts, whereas integrating out\ngroup-specific contexts results in the nDP mixture over content variables. We\nprovide a Polya-urn view of the model and an efficient collapsed Gibbs\ninference procedure. Extensive experiments on real-world datasets demonstrate\nthe advantage of utilizing context information via our model in both text and\nimage domains.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 12:08:07 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 06:28:03 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 08:13:58 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 01:54:57 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Nguyen", "Vu", ""], ["Phung", "Dinh", ""], ["Nguyen", "XuanLong", ""], ["Venkatesh", "Svetha", ""], ["Bui", "Hung Hai", ""]]}, {"id": "1401.2086", "submitter": "L.A. Prashanth", "authors": "H.L Prasad, L.A.Prashanth and Shalabh Bhatnagar", "title": "Actor-Critic Algorithms for Learning Nash Equilibria in N-player\n  General-Sum Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding stationary Nash equilibria (NE) in a\nfinite discounted general-sum stochastic game. We first generalize a non-linear\noptimization problem from Filar and Vrieze [2004] to a $N$-player setting and\nbreak down this problem into simpler sub-problems that ensure there is no\nBellman error for a given state and an agent. We then provide a\ncharacterization of solution points of these sub-problems that correspond to\nNash equilibria of the underlying game and for this purpose, we derive a set of\nnecessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.\nUsing these conditions, we develop two actor-critic algorithms: OFF-SGSP\n(model-based) and ON-SGSP (model-free). Both algorithms use a critic that\nestimates the value function for a fixed policy and an actor that performs\ndescent in the policy space using a descent direction that avoids local minima.\nWe establish that both algorithms converge, in self-play, to the equilibria of\na certain ordinary differential equation (ODE), whose stable limit points\ncoincide with stationary NE of the underlying general-sum stochastic game. On a\nsingle state non-generic game (see Hart and Mas-Colell [2005]) as well as on a\nsynthetic two-player game setup with $810,000$ states, we establish that\nON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ\n[Littman, 2001] algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 12:47:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 20:09:17 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Prasad", "H. L", ""], ["Prashanth", "L. A.", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1401.2139", "submitter": "Alejandro Frery", "authors": "Mart\\'in G\\'omez Ravetti and Laura C. Carpi and Bruna Amin\n  Gon\\c{c}alves and Alejandro C. Frery and Osvaldo A. Rosso", "title": "Distinguishing noise from chaos: objective versus subjective criteria\n  using Horizontal Visibility Graph", "comments": "Submitted to PLOS One", "journal-ref": null, "doi": "10.1371/journal.pone.0108004", "report-no": null, "categories": "stat.ML cs.IT math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently proposed methodology called the Horizontal Visibility Graph (HVG)\n[Luque {\\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes a\ngeometrical simplification of the well known Visibility Graph algorithm [Lacasa\n{\\it et al.\\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used to\nstudy the distinction between deterministic and stochastic components in time\nseries [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].\nSpecifically, the authors propose that the node degree distribution of these\nprocesses follows an exponential functional of the form $P(\\kappa)\\sim\n\\exp(-\\lambda~\\kappa)$, in which $\\kappa$ is the node degree and $\\lambda$ is a\npositive parameter able to distinguish between deterministic (chaotic) and\nstochastic (uncorrelated and correlated) dynamics. In this work, we investigate\nthe characteristics of the node degree distributions constructed by using HVG,\nfor time series corresponding to $28$ chaotic maps and $3$ different stochastic\nprocesses. We thoroughly study the methodology proposed by Lacasa and Toral\nfinding several cases for which their hypothesis is not valid. We propose a\nmethodology that uses the HVG together with Information Theory quantifiers. An\nextensive and careful analysis of the node degree distributions obtained by\napplying HVG allow us to conclude that the Fisher-Shannon information plane is\na remarkable tool able to graphically represent the different nature,\ndeterministic or stochastic, of the systems under study.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 20:08:02 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ravetti", "Mart\u00edn G\u00f3mez", ""], ["Carpi", "Laura C.", ""], ["Gon\u00e7alves", "Bruna Amin", ""], ["Frery", "Alejandro C.", ""], ["Rosso", "Osvaldo A.", ""]]}, {"id": "1401.2288", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal and Angshul Majumdar", "title": "Extension of Sparse Randomized Kaczmarz Algorithm for Multiple\n  Measurement Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kaczmarz algorithm is popular for iteratively solving an overdetermined\nsystem of linear equations. The traditional Kaczmarz algorithm can approximate\nthe solution in few sweeps through the equations but a randomized version of\nthe Kaczmarz algorithm was shown to converge exponentially and independent of\nnumber of equations. Recently an algorithm for finding sparse solution to a\nlinear system of equations has been proposed based on weighted randomized\nKaczmarz algorithm. These algorithms solves single measurement vector problem;\nhowever there are applications were multiple-measurements are available. In\nthis work, the objective is to solve a multiple measurement vector problem with\ncommon sparse support by modifying the randomized Kaczmarz algorithm. We have\nalso modeled the problem of face recognition from video as the multiple\nmeasurement vector problem and solved using our proposed technique. We have\ncompared the proposed algorithm with state-of-art spectral projected gradient\nalgorithm for multiple measurement vectors on both real and synthetic datasets.\nThe Monte Carlo simulations confirms that our proposed algorithm have better\nrecovery and convergence rate than the MMV version of spectral projected\ngradient algorithm under fairness constraints.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 11:24:35 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2014 10:05:15 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2014 08:13:58 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1401.2304", "submitter": "Stefan Hummelsheim", "authors": "Stefan Hummelsheim", "title": "Lasso and equivalent quadratic penalized models", "comments": "7 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least absolute shrinkage and selection operator (lasso) and ridge\nregression produce usually different estimates although input, loss function\nand parameterization of the penalty are identical. In this paper we look for\nridge and lasso models with identical solution set.\n  It turns out, that the lasso model with shrink vector $\\lambda$ and a\nquadratic penalized model with shrink matrix as outer product of $\\lambda$ with\nitself are equivalent, in the sense that they have equal solutions. To achieve\nthis, we have to restrict the estimates to be positive. This doesn't limit the\narea of application since we can easily decompose every estimate in a positive\nand negative part. The resulting problem can be solved with a non negative\nleast square algorithm.\n  Beside this quadratic penalized model, an augmented regression model with\npositive bounded estimates is developed which is also equivalent to the lasso\nmodel, but is probably faster to solve.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:23:47 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Hummelsheim", "Stefan", ""]]}, {"id": "1401.2451", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), Romaric Gaudel (LIFL), St\\'ephan\n  Cl\\'emen\\c{c}on (LTCI)", "title": "Online Matrix Completion Through Nuclear Norm Regularisation", "comments": "Corrected a typo in the affiliation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is the main goal of this paper to propose a novel method to perform matrix\ncompletion on-line. Motivated by a wide variety of applications, ranging from\nthe design of recommender systems to sensor network localization through\nseismic data reconstruction, we consider the matrix completion problem when\nentries of the matrix of interest are observed gradually. Precisely, we place\nourselves in the situation where the predictive rule should be refined\nincrementally, rather than recomputed from scratch each time the sample of\nobserved entries increases. The extension of existing matrix completion methods\nto the sequential prediction context is indeed a major issue in the Big Data\nera, and yet little addressed in the literature. The algorithm promoted in this\narticle builds upon the Soft Impute approach introduced in Mazumder et al.\n(2010). The major novelty essentially arises from the use of a randomised\ntechnique for both computing and updating the Singular Value Decomposition\n(SVD) involved in the algorithm. Though of disarming simplicity, the method\nproposed turns out to be very efficient, while requiring reduced computations.\nSeveral numerical experiments based on real datasets illustrating its\nperformance are displayed, together with preliminary results giving it a\ntheoretical basis.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 20:44:50 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Gaudel", "Romaric", "", "LIFL"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1401.2490", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh", "title": "An Online Expectation-Maximisation Algorithm for Nonnegative Matrix\n  Factorisation Models", "comments": "6 pages, 3 figures", "journal-ref": "16th IFAC Symposium on System Identification, 2012, Volume 16,\n  Part 1,", "doi": "10.3182/20120711-3-BE-2027.00312", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate the nonnegative matrix factorisation (NMF) problem\nas a maximum likelihood estimation problem for hidden Markov models and propose\nonline expectation-maximisation (EM) algorithms to estimate the NMF and the\nother unknown static parameters. We also propose a sequential Monte Carlo\napproximation of our online EM algorithm. We show the performance of the\nproposed method with two numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 00:54:27 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Yildirim", "Sinan", ""], ["Cemgil", "A. Taylan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1401.2497", "submitter": "Xin Yuan", "authors": "Xin Yuan, Vinayak Rao, Shaobo Han and Lawrence Carin", "title": "Multiscale Shrinkage and L\\'evy Processes", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new shrinkage-based construction is developed for a compressible vector\n$\\boldsymbol{x}\\in\\mathbb{R}^n$, for cases in which the components of $\\xv$ are\nnaturally associated with a tree structure. Important examples are when $\\xv$\ncorresponds to the coefficients of a wavelet or block-DCT representation of\ndata. The method we consider in detail, and for which numerical results are\npresented, is based on increments of a gamma process. However, we demonstrate\nthat the general framework is appropriate for many other types of shrinkage\npriors, all within the L\\'{e}vy process family, with the gamma process a\nspecial case. Bayesian inference is carried out by approximating the posterior\nwith samples from an MCMC algorithm, as well as by constructing a heuristic\nvariational approximation to the posterior. We also consider\nexpectation-maximization (EM) for a MAP (point) solution. State-of-the-art\nresults are manifested for compressive sensing and denoising applications, the\nlatter with spiky (non-Gaussian) noise.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 03:13:16 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Yuan", "Xin", ""], ["Rao", "Vinayak", ""], ["Han", "Shaobo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1401.2504", "submitter": "Tao Xiong", "authors": "Yukun Bao, Tao Xiong, Zhongyi Hu", "title": "Multi-Step-Ahead Time Series Prediction using Multiple-Output Support\n  Vector Regression", "comments": "26 pages", "journal-ref": null, "doi": "10.1016/j.neucom.2013.09.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate time series prediction over long future horizons is challenging and\nof great interest to both practitioners and academics. As a well-known\nintelligent algorithm, the standard formulation of Support Vector Regression\n(SVR) could be taken for multi-step-ahead time series prediction, only relying\neither on iterated strategy or direct strategy. This study proposes a novel\nmultiple-step-ahead time series prediction approach which employs\nmultiple-output support vector regression (M-SVR) with multiple-input\nmultiple-output (MIMO) prediction strategy. In addition, the rank of three\nleading prediction strategies with SVR is comparatively examined, providing\npractical implications on the selection of the prediction strategy for\nmulti-step-ahead forecasting while taking SVR as modeling technique. The\nproposed approach is validated with the simulated and real datasets. The\nquantitative and comprehensive assessments are performed on the basis of the\nprediction accuracy and computational cost. The results indicate that: 1) the\nM-SVR using MIMO strategy achieves the best accurate forecasts with accredited\ncomputational load, 2) the standard SVR using direct strategy achieves the\nsecond best accurate forecasts, but with the most expensive computational cost,\nand 3) the standard SVR using iterated strategy is the worst in terms of\nprediction accuracy, but with the least computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 06:14:53 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Bao", "Yukun", ""], ["Xiong", "Tao", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.2569", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar", "title": "Multi Terminal Probabilistic Compressed Sensing", "comments": "11 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:1112.0708 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the `Approximate Message Passing' (AMP) algorithm, initially\ndeveloped for compressed sensing of signals under i.i.d. Gaussian measurement\nmatrices, has been extended to a multi-terminal setting (MAMP algorithm). It\nhas been shown that similar to its single terminal counterpart, the behavior of\nMAMP algorithm is fully characterized by a `State Evolution' (SE) equation for\nlarge block-lengths. This equation has been used to obtain the rate-distortion\ncurve of a multi-terminal memoryless source. It is observed that by spatially\ncoupling the measurement matrices, the rate-distortion curve of MAMP algorithm\nundergoes a phase transition, where the measurement rate region corresponding\nto a low distortion (approximately zero distortion) regime is fully\ncharacterized by the joint and conditional Renyi information dimension (RID) of\nthe multi-terminal source. This measurement rate region is very similar to the\nrate region of the Slepian-Wolf distributed source coding problem where the RID\nplays a role similar to the discrete entropy.\n  Simulations have been done to investigate the empirical behavior of MAMP\nalgorithm. It is observed that simulation results match very well with\npredictions of SE equation for reasonably large block-lengths.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 21:20:09 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Haghighatshoar", "Saeid", ""]]}, {"id": "1401.2678", "submitter": "Arend Voorman", "authors": "Arend Voorman, Ali Shojaie, Daniela Witten", "title": "Inference in High Dimensions with the Penalized Score Test", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been considerable theoretical development\nregarding variable selection consistency of penalized regression techniques,\nsuch as the lasso. However, there has been relatively little work on\nquantifying the uncertainty in these selection procedures. In this paper, we\npropose a new method for inference in high dimensions using a score test based\non penalized regression. In this test, we perform penalized regression of an\noutcome on all but a single feature, and test for correlation of the residuals\nwith the held-out feature. This procedure is applied to each feature in turn.\nInterestingly, when an $\\ell_1$ penalty is used, the sparsity pattern of the\nlasso corresponds exactly to a decision based on the proposed test. Further,\nwhen an $\\ell_2$ penalty is used, the test corresponds precisely to a score\ntest in a mixed effects model, in which the effects of all but one feature are\nassumed to be random. We formulate the hypothesis being tested as a compromise\nbetween the null hypotheses tested in simple linear regression on each feature\nand in multiple linear regression on all features, and develop reference\ndistributions for some well-known penalties. We also examine the behavior of\nthe test on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 22:28:04 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 19:24:35 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 21:47:32 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Voorman", "Arend", ""], ["Shojaie", "Ali", ""], ["Witten", "Daniela", ""]]}, {"id": "1401.2753", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Tong Zhang", "title": "Stochastic Optimization with Importance Sampling", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform sampling of training data has been commonly used in traditional\nstochastic optimization algorithms such as Proximal Stochastic Gradient Descent\n(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although\nuniform sampling can guarantee that the sampled stochastic quantity is an\nunbiased estimate of the corresponding true quantity, the resulting estimator\nmay have a rather high variance, which negatively affects the convergence of\nthe underlying optimization procedure. In this paper we study stochastic\noptimization with importance sampling, which improves the convergence rate by\nreducing the stochastic variance. Specifically, we study prox-SGD (actually,\nstochastic mirror descent) with importance sampling and prox-SDCA with\nimportance sampling. For prox-SGD, instead of adopting uniform sampling\nthroughout the training process, the proposed algorithm employs importance\nsampling to minimize the variance of the stochastic gradient. For prox-SDCA,\nthe proposed importance sampling scheme aims to achieve higher expected dual\nvalue at each dual coordinate ascent step. We provide extensive theoretical\nanalysis to show that the convergence rates with the proposed importance\nsampling methods can be significantly improved under suitable conditions both\nfor prox-SGD and for prox-SDCA. Experiments are provided to verify the\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 08:47:44 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 09:17:48 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhao", "Peilin", ""], ["Zhang", "Tong", ""]]}, {"id": "1401.2771", "submitter": "Konstantinos Themelis", "authors": "Konstantinos E. Themelis and Athanasios A. Rontogiannis and\n  Konstantinos D. Koutroumbas", "title": "A variational Bayes framework for sparse adaptive estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2338839", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of mostly $\\ell_1$-norm regularized least squares type\ndeterministic algorithms have been proposed to address the problem of\n\\emph{sparse} adaptive signal estimation and system identification. From a\nBayesian perspective, this task is equivalent to maximum a posteriori\nprobability estimation under a sparsity promoting heavy-tailed prior for the\nparameters of interest. Following a different approach, this paper develops a\nunifying framework of sparse \\emph{variational Bayes} algorithms that employ\nheavy-tailed priors in conjugate hierarchical form to facilitate posterior\ninference. The resulting fully automated variational schemes are first\npresented in a batch iterative form. Then it is shown that by properly\nexploiting the structure of the batch estimation task, new sparse adaptive\nvariational Bayes algorithms can be derived, which have the ability to impose\nand track sparsity during real-time processing in a time-varying environment.\nThe most important feature of the proposed algorithms is that they completely\neliminate the need for computationally costly parameter fine-tuning, a\nnecessary ingredient of sparse adaptive deterministic algorithms. Extensive\nsimulation results are provided to demonstrate the effectiveness of the new\nsparse variational Bayes algorithms against state-of-the-art deterministic\ntechniques for adaptive channel estimation. The results show that the proposed\nalgorithms are numerically robust and exhibit in general superior estimation\nperformance compared to their deterministic counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 10:14:08 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Themelis", "Konstantinos E.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Koutroumbas", "Konstantinos D.", ""]]}, {"id": "1401.2838", "submitter": "Edward Meeds", "authors": "Edward Meeds and Max Welling", "title": "GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists often express their understanding of the world through a\ncomputationally demanding simulation program. Analyzing the posterior\ndistribution of the parameters given observations (the inverse problem) can be\nextremely challenging. The Approximate Bayesian Computation (ABC) framework is\nthe standard statistical tool to handle these likelihood free problems, but\nthey require a very large number of simulations. In this work we develop two\nnew ABC sampling algorithms that significantly reduce the number of simulations\nnecessary for posterior inference. Both algorithms use confidence estimates for\nthe accept probability in the Metropolis Hastings step to adaptively choose the\nnumber of necessary simulations. Our GPS-ABC algorithm stores the information\nobtained from every simulation in a Gaussian process which acts as a surrogate\nfunction for the simulated statistics. Experiments on a challenging realistic\nbiological problem illustrate the potential of these algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 14:02:37 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Meeds", "Edward", ""], ["Welling", "Max", ""]]}, {"id": "1401.2955", "submitter": "Mahdi Pakdaman Naeini", "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht", "title": "Binary Classifier Calibration: Bayesian Non-Parametric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of probabilistic predictions is well calibrated if the events that are\npredicted to occur with probability p do in fact occur about p fraction of the\ntime. Well calibrated predictions are particularly important when machine\nlearning models are used in decision analysis. This paper presents two new\nnon-parametric methods for calibrating outputs of binary classification models:\na method based on the Bayes optimal selection and a method based on the\nBayesian model averaging. The advantage of these methods is that they are\nindependent of the algorithm used to learn a predictive model, and they can be\napplied in a post-processing step, after the model is learned. This makes them\napplicable to a wide variety of machine learning models and methods. These\ncalibration methods, as well as other methods, are tested on a variety of\ndatasets in terms of both discrimination and calibration performance. The\nresults show the methods either outperform or are comparable in performance to\nthe state-of-the-art calibration methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 19:04:13 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1401.3258", "submitter": "Jeremy Kun", "authors": "Rajmonda Caceres, Kevin Carter, Jeremy Kun", "title": "A Boosting Approach to Learning Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the right graph representation from noisy, multisource data has\ngarnered significant interest in recent years. A central tenet of this problem\nis relational learning. Here the objective is to incorporate the partial\ninformation each data source gives us in a way that captures the true\nunderlying relationships. To address this challenge, we present a general,\nboosting-inspired framework for combining weak evidence of entity associations\ninto a robust similarity metric. We explore the extent to which different\nquality measurements yield graph representations that are suitable for\ncommunity detection. We then present empirical results on both synthetic and\nreal datasets demonstrating the utility of this framework. Our framework leads\nto suitable global graph representations from quality measurements local to\neach edge. Finally, we discuss future extensions and theoretical considerations\nof learning useful graph representations from weak feedback in general\napplication settings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 17:07:01 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Caceres", "Rajmonda", ""], ["Carter", "Kevin", ""], ["Kun", "Jeremy", ""]]}, {"id": "1401.3291", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Alfred Hero", "title": "Detection of Anomalous Crowd Behavior Using Spatio-Temporal\n  Multiresolution Model and Kronecker Sum Decompositions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of detecting anomalous spatio-temporal\nbehavior in videos. Our approach is to learn the normative multiframe pixel\njoint distribution and detect deviations from it using a likelihood based\napproach. Due to the extreme lack of available training samples relative to the\ndimension of the distribution, we use a mean and covariance approach and\nconsider methods of learning the spatio-temporal covariance in the low-sample\nregime. Our approach is to estimate the covariance using parameter reduction\nand sparse models. The first method considered is the representation of the\ncovariance as a sum of Kronecker products as in (Greenewald et al 2013), which\nis found to be an accurate approximation in this setting. We propose learning\nalgorithms relevant to our problem. We then consider the sparse multiresolution\nmodel of (Choi et al 2010) and apply the Kronecker product methods to it for\nfurther parameter reduction, as well as introducing modifications for enhanced\nefficiency and greater applicability to spatio-temporal covariance matrices. We\napply our methods to the detection of crowd behavior anomalies in the\nUniversity of Minnesota crowd anomaly dataset, and achieve competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 19:15:04 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 19:09:47 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Hero", "Alfred", ""]]}, {"id": "1401.3358", "submitter": "Deniz Gencaga", "authors": "D. Gencaga, N. K. Malakar, D. J. Lary", "title": "Survey On The Estimation Of Mutual Information Methods as a Measure of\n  Dependency Versus Correlation Analysis", "comments": null, "journal-ref": null, "doi": "10.1063/1.4903714", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey, we present and compare different approaches to estimate\nMutual Information (MI) from data to analyse general dependencies between\nvariables of interest in a system. We demonstrate the performance difference of\nMI versus correlation analysis, which is only optimal in case of linear\ndependencies. First, we use a piece-wise constant Bayesian methodology using a\ngeneral Dirichlet prior. In this estimation method, we use a two-stage approach\nwhere we approximate the probability distribution first and then calculate the\nmarginal and joint entropies. Here, we demonstrate the performance of this\nBayesian approach versus the others for computing the dependency between\ndifferent variables. We also compare these with linear correlation analysis.\nFinally, we apply MI and correlation analysis to the identification of the bias\nin the determination of the aerosol optical depth (AOD) by the satellite based\nModerate Resolution Imaging Spectroradiometer (MODIS) and the ground based\nAErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurements\nby these two instruments might be different for the same location. The reason\nof this bias is explored by quantifying the dependencies between the bias and\n15 other variables including cloud cover, surface reflectivity and others.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 21:17:21 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Gencaga", "D.", ""], ["Malakar", "N. K.", ""], ["Lary", "D. J.", ""]]}, {"id": "1401.3390", "submitter": "Mahdi Pakdaman Naeini", "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht", "title": "Binary Classifier Calibration: Non-parametric approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate calibration of probabilistic predictive models learned is critical\nfor many practical prediction and decision-making tasks. There are two main\ncategories of methods for building calibrated classifiers. One approach is to\ndevelop methods for learning probabilistic models that are well-calibrated, ab\ninitio. The other approach is to use some post-processing methods for\ntransforming the output of a classifier to be well calibrated, as for example\nhistogram binning, Platt scaling, and isotonic regression. One advantage of the\npost-processing approach is that it can be applied to any existing\nprobabilistic classification model that was constructed using any\nmachine-learning method.\n  In this paper, we first introduce two measures for evaluating how well a\nclassifier is calibrated. We prove three theorems showing that using a simple\nhistogram binning post-processing method, it is possible to make a classifier\nbe well calibrated while retaining its discrimination capability. Also, by\ncasting the histogram binning method as a density-based non-parametric binary\nclassifier, we can extend it using two simple non-parametric density estimation\nmethods. We demonstrate the performance of the proposed calibration methods on\nsynthetic and real datasets. Experimental results show that the proposed\nmethods either outperform or are comparable to existing calibration methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 23:52:16 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1401.3409", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Can Yang, Hongyu Zhao, Weichuan Yu", "title": "Low-Rank Modeling and Its Applications in Image Analysis", "comments": "To appear in ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:17:33 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:40:29 GMT"}, {"version": "v3", "created": "Thu, 23 Oct 2014 02:05:18 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""], ["Yu", "Weichuan", ""]]}, {"id": "1401.3441", "submitter": "Ran El-Yaniv", "authors": "Ran El-Yaniv, Dmitry Pechyony", "title": "Transductive Rademacher Complexity and its Applications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  193-234, 2009", "doi": "10.1613/jair.2587", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for deriving data-dependent error bounds for\ntransductive learning algorithms based on transductive Rademacher complexity.\nOur technique is based on a novel general error bound for transduction in terms\nof transductive Rademacher complexity, together with a novel bounding technique\nfor Rademacher averages for particular algorithms, in terms of their\n\"unlabeled-labeled\" representation. This technique is relevant to many advanced\ngraph-based transductive algorithms and we demonstrate its effectiveness by\nderiving error bounds to three well known algorithms. Finally, we present a new\nPAC-Bayesian bound for mixtures of transductive algorithms based on our\nRademacher bounds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:54:14 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Pechyony", "Dmitry", ""]]}, {"id": "1401.3478", "submitter": "Facundo Bromberg", "authors": "Facundo Bromberg, Dimitris Margaritis, Vasant Honavar", "title": "Efficient Markov Network Structure Discovery Using Independence Tests", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  449-484, 2009", "doi": "10.1613/jair.2773", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms for learning the structure of a Markov network from\ndata: GSMN* and GSIMN. Both algorithms use statistical independence tests to\ninfer the structure by successively constraining the set of structures\nconsistent with the results of these tests. Until very recently, algorithms for\nstructure learning were based on maximum likelihood estimation, which has been\nproved to be NP-hard for Markov networks due to the difficulty of estimating\nthe parameters of the network, needed for the computation of the data\nlikelihood. The independence-based approach does not require the computation of\nthe likelihood, and thus both GSMN* and GSIMN can compute the structure\nefficiently (as shown in our experiments). GSMN* is an adaptation of the\nGrow-Shrink algorithm of Margaritis and Thrun for learning the structure of\nBayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls\nwell-known properties of the conditional independence relation to infer novel\nindependences from known ones, thus avoiding the performance of statistical\ntests to estimate them. To accomplish this efficiently GSIMN uses the Triangle\ntheorem, also introduced in this work, which is a simplified version of the set\nof Markov axioms. Experimental comparisons on artificial and real-world data\nsets show GSIMN can yield significant savings with respect to GSMN*, while\ngenerating a Markov network with comparable or in some cases improved quality.\nWe also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,\nthat produces all possible conditional independences resulting from repeatedly\napplying Pearls theorems on the known conditional independence tests. The\nresults of this comparison show that GSIMN, by the sole use of the Triangle\ntheorem, is nearly optimal in terms of the set of independences tests that it\ninfers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:29 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bromberg", "Facundo", ""], ["Margaritis", "Dimitris", ""], ["Honavar", "Vasant", ""]]}, {"id": "1401.3632", "submitter": "Shaan Qamar", "authors": "Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson", "title": "Bayesian Conditional Density Filtering", "comments": "41 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Conditional Density Filtering (C-DF) algorithm for efficient\nonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,\nsampling from approximations to conditional posterior distributions obtained by\npropagating surrogate conditional sufficient statistics (a function of data and\nparameter estimates) as new data arrive. These quantities eliminate the need to\nstore or process the entire dataset simultaneously and offer a number of\ndesirable features. Often, these include a reduction in memory requirements and\nruntime and improved mixing, along with state-of-the-art parameter inference\nand prediction. These improvements are demonstrated through several\nillustrative examples including an application to high dimensional compressed\nregression. Finally, we show that C-DF samples converge to the target posterior\ndistribution asymptotically as sampling proceeds and more data arrives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 15:40:40 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 21:47:00 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 07:41:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Qamar", "Shaan", ""], ["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1401.3737", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers and \\\"Ur\\\"un Dogan", "title": "Coordinate Descent with Online Adaptation of Coordinate Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate descent (CD) algorithms have become the method of choice for\nsolving a number of optimization problems in machine learning. They are\nparticularly popular for training linear models, including linear support\nvector machine classification, LASSO regression, and logistic regression.\n  We consider general CD with non-uniform selection of coordinates. Instead of\nfixing selection frequencies beforehand we propose an online adaptation\nmechanism for this important parameter, called the adaptive coordinate\nfrequencies (ACF) method. This mechanism removes the need to estimate optimal\ncoordinate frequencies beforehand, and it automatically reacts to changing\nrequirements during an optimization run.\n  We demonstrate the usefulness of our ACF-CD approach for a variety of\noptimization problems arising in machine learning contexts. Our algorithm\noffers significant speed-ups over state-of-the-art training methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 20:50:00 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Dogan", "\u00dcr\u00fcn", ""]]}, {"id": "1401.3813", "submitter": "Heather Patsolic", "authors": "Heather Patsolic, Sancar Adali, Joshua T. Vogelstein, Youngser Park,\n  Carey E. Friebe, Gongkai Li, Vince Lyzinski", "title": "Seeded Graph Matching Via Joint Optimization of Fidelity and\n  Commensurability", "comments": "26 pages, 7 figures. Updated content and added application of\n  simultaneous matching for several time-steps for zebrafish connectomes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approximate graph matching algorithm that incorporates\nseeded data into the graph matching paradigm. Our Joint Optimization of\nFidelity and Commensurability (JOFC) algorithm embeds two graphs into a common\nEuclidean space where the matching inference task can be performed. Through\nreal and simulated data examples, we demonstrate the versatility of our\nalgorithm in matching graphs with various characteristics--weightedness,\ndirectedness, loopiness, many-to-one and many-to-many matchings, and soft\nseedings.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 02:33:44 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 20:01:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Patsolic", "Heather", ""], ["Adali", "Sancar", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["Friebe", "Carey E.", ""], ["Li", "Gongkai", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1401.3818", "submitter": "Xiaoxia Sun", "authors": "Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran", "title": "Structured Priors for Sparse-Representation-Based Hyperspectral Image\n  Classification", "comments": "IEEE Geoscience and Remote Sensing Letter", "journal-ref": null, "doi": "10.1109/LGRS.2013.2290531", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Pixel-wise classification, where each pixel is assigned to a predefined\nclass, is one of the most important procedures in hyperspectral image (HSI)\nanalysis. By representing a test pixel as a linear combination of a small\nsubset of labeled pixels, a sparse representation classifier (SRC) gives rather\nplausible results compared with that of traditional classifiers such as the\nsupport vector machine (SVM). Recently, by incorporating additional structured\nsparsity priors, the second generation SRCs have appeared in the literature and\nare reported to further improve the performance of HSI. These priors are based\non exploiting the spatial dependencies between the neighboring pixels, the\ninherent structure of the dictionary, or both. In this paper, we review and\ncompare several structured priors for sparse-representation-based HSI\nclassification. We also propose a new structured prior called the low rank\ngroup prior, which can be considered as a modification of the low rank prior.\nFurthermore, we will investigate how different structured priors improve the\nresult for the HSI classification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 03:21:26 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Sun", "Xiaoxia", ""], ["Qu", "Qing", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1401.3870", "submitter": "Erik Talvitie", "authors": "Erik Talvitie, Satinder Singh", "title": "Learning to Make Predictions In Partially Observable Environments\n  Without a Generative Model", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  353-392, 2011", "doi": "10.1613/jair.3396", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with the problem of learning a model of a high-dimensional\nenvironment, a common approach is to limit the model to make only a restricted\nset of predictions, thereby simplifying the learning problem. These partial\nmodels may be directly useful for making decisions or may be combined together\nto form a more complete, structured model. However, in partially observable\n(non-Markov) environments, standard model-learning methods learn generative\nmodels, i.e. models that provide a probability distribution over all possible\nfutures (such as POMDPs). It is not straightforward to restrict such models to\nmake only certain predictions, and doing so does not always simplify the\nlearning problem. In this paper we present prediction profile models:\nnon-generative partial models for partially observable systems that make only a\ngiven set of predictions, and are therefore far simpler than generative models\nin some cases. We formalize the problem of learning a prediction profile model\nas a transformation of the original model-learning problem, and show\nempirically that one can learn prediction profile models that make a small set\nof important predictions even in systems that are too complex for standard\ngenerative models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:08:29 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Talvitie", "Erik", ""], ["Singh", "Satinder", ""]]}, {"id": "1401.3877", "submitter": "Botond Cseke", "authors": "Botond Cseke, Tom Heskes", "title": "Properties of Bethe Free Energies and Message Passing in Gaussian Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  1-24, 2011", "doi": "10.1613/jair.3195", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing approximate marginals in Gaussian\nprobabilistic models by using mean field and fractional Bethe approximations.\nWe define the Gaussian fractional Bethe free energy in terms of the moment\nparameters of the approximate marginals, derive a lower and an upper bound on\nthe fractional Bethe free energy and establish a necessary condition for the\nlower bound to be bounded from below. It turns out that the condition is\nidentical to the pairwise normalizability condition, which is known to be a\nsufficient condition for the convergence of the message passing algorithm. We\nshow that stable fixed points of the Gaussian message passing algorithm are\nlocal minima of the Gaussian Bethe free energy. By a counterexample, we\ndisprove the conjecture stating that the unboundedness of the free energy\nimplies the divergence of the message passing algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:11:12 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cseke", "Botond", ""], ["Heskes", "Tom", ""]]}, {"id": "1401.3894", "submitter": "Andr\\'as Gy\\\"orgy", "authors": "Andr\\'as Gy\\\"orgy, Levente Kocsis", "title": "Efficient Multi-Start Strategies for Local Search Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  407-444, 2011", "doi": "10.1613/jair.3313", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search algorithms applied to optimization problems often suffer from\ngetting trapped in a local optimum. The common solution for this deficiency is\nto restart the algorithm when no progress is observed. Alternatively, one can\nstart multiple instances of a local search algorithm, and allocate\ncomputational resources (in particular, processing time) to the instances\ndepending on their behavior. Hence, a multi-start strategy has to decide\n(dynamically) when to allocate additional resources to a particular instance\nand when to start new instances. In this paper we propose multi-start\nstrategies motivated by works on multi-armed bandit problems and Lipschitz\noptimization with an unknown constant. The strategies continuously estimate the\npotential performance of each algorithm instance by supposing a convergence\nrate of the local search algorithm up to an unknown constant, and in every\nphase allocate resources to those instances that could converge to the optimum\nfor a particular range of the constant. Asymptotic bounds are given on the\nperformance of the strategies. In particular, we prove that at most a quadratic\nincrease in the number of times the target function is evaluated is needed to\nachieve the performance of a local search algorithm started from the attraction\nregion of the optimum. Experiments are provided using SPSA (Simultaneous\nPerturbation Stochastic Approximation) and k-means as local search algorithms,\nand the results indicate that the proposed strategies work well in practice,\nand, in all cases studied, need only logarithmically more evaluations of the\ntarget function as opposed to the theoretically suggested quadratic increase.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:17:32 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Kocsis", "Levente", ""]]}, {"id": "1401.3915", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Peter J. Bickel", "title": "Community Detection in Networks using Graph Distance", "comments": "Presented in Networks with Community Structure Workshop, Eurandom,\n  January, 2014. arXiv admin note: text overlap with arXiv:math/0504589 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of networks has received increased attention recently not only from\nthe social sciences and statistics but also from physicists, computer\nscientists and mathematicians. One of the principal problem in networks is\ncommunity detection. Many algorithms have been proposed for community finding\nbut most of them do not have have theoretical guarantee for sparse networks and\nnetworks close to the phase transition boundary proposed by physicists. There\nare some exceptions but all have some incomplete theoretical basis. Here we\npropose an algorithm based on the graph distance of vertices in the network. We\ngive theoretical guarantees that our method works in identifying communities\nfor block models and can be extended for degree-corrected block models and\nblock models with the number of communities growing with number of vertices.\nDespite favorable simulation results, we are not yet able to conclude that our\nmethod is satisfactory for worst possible case. We illustrate on a network of\npolitical blogs, Facebook networks and some other networks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 06:26:37 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 09:01:50 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1401.3940", "submitter": "Han Liu", "authors": "Le Song, Han Liu, Ankur Parikh, Eric Xing", "title": "Nonparametric Latent Tree Graphical Models: Inference, Estimation, and\n  Structure Learning", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree structured graphical models are powerful at expressing long range or\nhierarchical dependency among many variables, and have been widely applied in\ndifferent areas of computer science and statistics. However, existing methods\nfor parameter estimation, inference, and structure learning mainly rely on the\nGaussian or discrete assumptions, which are restrictive under many\napplications. In this paper, we propose new nonparametric methods based on\nreproducing kernel Hilbert space embeddings of distributions that can recover\nthe latent tree structures, estimate the parameters, and perform inference for\nhigh dimensional continuous and non-Gaussian variables. The usefulness of the\nproposed methods are illustrated by thorough numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 09:04:20 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Song", "Le", ""], ["Liu", "Han", ""], ["Parikh", "Ankur", ""], ["Xing", "Eric", ""]]}, {"id": "1401.3973", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Josep Lluis Arcos", "title": "An Empirical Evaluation of Similarity Measures for Time Series\n  Classification", "comments": "28 pages, 5 figures, 3 tables", "journal-ref": "Knowledge-Based Systems 67: 305-314, 2014", "doi": "10.1016/j.knosys.2014.04.035", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are ubiquitous, and a measure to assess their similarity is a\ncore part of many computational systems. In particular, the similarity measure\nis the most essential ingredient of time series clustering and classification\nsystems. Because of this importance, countless approaches to estimate time\nseries similarity have been proposed. However, there is a lack of comparative\nstudies using empirical, rigorous, quantitative, and large-scale assessment\nstrategies. In this article, we provide an extensive evaluation of similarity\nmeasures for time series classification following the aforementioned\nprinciples. We consider 7 different measures coming from alternative measure\n`families', and 45 publicly-available time series data sets coming from a wide\nvariety of scientific domains. We focus on out-of-sample classification\naccuracy, but in-sample accuracies and parameter choices are also discussed.\nOur work is based on rigorous evaluation methodologies and includes the use of\npowerful statistical significance tests to derive meaningful conclusions. The\nobtained results show the equivalence, in terms of accuracy, of a number of\nmeasures, but with one single candidate outperforming the rest. Such findings,\ntogether with the followed methodology, invite researchers on the field to\nadopt a more consistent evaluation criteria and a more informed decision\nregarding the baseline measures to which new developments should be compared.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 10:21:44 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1401.4082", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models", "comments": "Appears In Proceedings of the 31st International Conference on\n  Machine Learning (ICML), JMLR: W\\&CP volume 32, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 16:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 12:53:17 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 10:00:36 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Wierstra", "Daan", ""]]}, {"id": "1401.4408", "submitter": "Brian Baingana Mr", "authors": "Brian Baingana and Georgios B. Giannakis", "title": "Embedding Graphs under Centrality Constraints for Network Visualization", "comments": "Submitted to IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual rendering of graphs is a key task in the mapping of complex network\ndata. Although most graph drawing algorithms emphasize aesthetic appeal,\ncertain applications such as travel-time maps place more importance on\nvisualization of structural network properties. The present paper advocates two\ngraph embedding approaches with centrality considerations to comply with node\nhierarchy. The problem is formulated first as one of constrained\nmulti-dimensional scaling (MDS), and it is solved via block coordinate descent\niterations with successive approximations and guaranteed convergence to a KKT\npoint. In addition, a regularization term enforcing graph smoothness is\nincorporated with the goal of reducing edge crossings. A second approach\nleverages the locally-linear embedding (LLE) algorithm which assumes that the\ngraph encodes data sampled from a low-dimensional manifold. Closed-form\nsolutions to the resulting centrality-constrained optimization problems are\ndetermined yielding meaningful embeddings. Experimental results demonstrate the\nefficacy of both approaches, especially for visualizing large networks on the\norder of thousands of nodes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 17:16:25 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1401.4425", "submitter": "Qing Zhou", "authors": "Qing Zhou", "title": "Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation", "comments": "43 pages, 4 figures", "journal-ref": "Journal of the American Statistical Association, 109: 1495-1516\n  (2014)", "doi": "10.1080/01621459.2014.946035", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized linear regression under the $\\ell_1$ penalty, such as the Lasso,\nhas been shown to be effective in variable selection and sparse modeling. The\nsampling distribution of an $\\ell_1$-penalized estimator $\\hat{\\beta}$ is hard\nto determine as the estimator is defined by an optimization problem that in\ngeneral can only be solved numerically and many of its components may be\nexactly zero. Let $S$ be the subgradient of the $\\ell_1$ norm of the\ncoefficient vector $\\beta$ evaluated at $\\hat{\\beta}$. We find that the joint\nsampling distribution of $\\hat{\\beta}$ and $S$, together called an augmented\nestimator, is much more tractable and has a closed-form density under a normal\nerror distribution in both low-dimensional ($p\\leq n$) and high-dimensional\n($p>n$) settings. Given $\\beta$ and the error variance $\\sigma^2$, one may\nemploy standard Monte Carlo methods, such as Markov chain Monte Carlo and\nimportance sampling, to draw samples from the distribution of the augmented\nestimator and calculate expectations with respect to the sampling distribution\nof $\\hat{\\beta}$. We develop a few concrete Monte Carlo algorithms and\ndemonstrate with numerical examples that our approach may offer huge advantages\nand great flexibility in studying sampling distributions in $\\ell_1$-penalized\nlinear regression. We also establish nonasymptotic bounds on the difference\nbetween the true sampling distribution of $\\hat{\\beta}$ and its estimator\nobtained by plugging in estimated parameters, which justifies the validity of\nMonte Carlo simulation from an estimated sampling distribution even when $p\\gg\nn\\to \\infty$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 18:16:49 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 23:13:50 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Zhou", "Qing", ""]]}, {"id": "1401.4489", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju", "title": "An Analysis of Random Projections in Cancelable Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing concerns about security, the need for highly secure physical\nbiometrics-based authentication systems utilizing \\emph{cancelable biometric}\ntechnologies is on the rise. Because the problem of cancelable template\ngeneration deals with the trade-off between template security and matching\nperformance, many state-of-the-art algorithms successful in generating high\nquality cancelable biometrics all have random projection as one of their early\nprocessing steps. This paper therefore presents a formal analysis of why random\nprojections is an essential step in cancelable biometrics. By formally defining\nthe notion of an \\textit{Independent Subspace Structure} for datasets, it can\nbe shown that random projection preserves the subspace structure of data\nvectors generated from a union of independent linear subspaces. The bound on\nthe minimum number of random vectors required for this to hold is also derived\nand is shown to depend logarithmically on the number of data samples, not only\nin independent subspaces but in disjoint subspace settings as well. The\ntheoretical analysis presented is supported in detail with empirical results on\nreal-world face recognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 23:21:56 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 02:57:25 GMT"}, {"version": "v3", "created": "Fri, 14 Nov 2014 02:38:09 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Srivastava", "Gaurav", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1401.4566", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi and Rong Jin", "title": "Excess Risk Bounds for Exponentially Concave Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overarching goal of this paper is to derive excess risk bounds for\nlearning from exp-concave loss functions in passive and sequential learning\nsettings. Exp-concave loss functions encompass several fundamental problems in\nmachine learning such as squared loss in linear regression, logistic loss in\nclassification, and negative logarithm loss in portfolio management. In batch\nsetting, we obtain sharp bounds on the performance of empirical risk\nminimization performed in a linear hypothesis space and with respect to the\nexp-concave loss functions. We also extend the results to the online setting\nwhere the learner receives the training examples in a sequential manner. We\npropose an online learning algorithm that is a properly modified version of\nonline Newton method to obtain sharp risk bounds. Under an additional mild\nassumption on the loss function, we show that in both settings we are able to\nachieve an excess risk bound of $O(d\\log n/n)$ that holds with a high\nprobability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 17:07:38 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2014 05:02:49 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""]]}, {"id": "1401.4785", "submitter": "Takafumi Ono", "authors": "Satoshi Hara, Takafumi Ono, Ryo Okamoto, Takashi Washio and Shigeki\n  Takeuchi", "title": "Anomaly detection in reconstructed quantum states using a\n  machine-learning technique", "comments": "Accepted for Physical Review A", "journal-ref": null, "doi": "10.1103/PhysRevA.89.022104", "report-no": null, "categories": "quant-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate detection of small deviations in given density matrices is\nimportant for quantum information processing. Here we propose a new method\nbased on the concept of data mining. We demonstrate that the proposed method\ncan more accurately detect small erroneous deviations in reconstructed density\nmatrices, which contain intrinsic fluctuations due to the limited number of\nsamples, than a naive method of checking the trace distance from the average of\nthe given density matrices. This method has the potential to be a key tool in\nbroad areas of physics where the detection of small deviations of quantum\nstates reconstructed using a limited number of samples are essential.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 03:44:08 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hara", "Satoshi", ""], ["Ono", "Takafumi", ""], ["Okamoto", "Ryo", ""], ["Washio", "Takashi", ""], ["Takeuchi", "Shigeki", ""]]}, {"id": "1401.4988", "submitter": "Johan Pensar", "authors": "Johan Pensar, Henrik Nyman, Juha Niiranen, Jukka Corander", "title": "Marginal Pseudo-Likelihood Learning of Markov Network structures", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models known as Markov networks are popular for a wide\nvariety of applications ranging from statistical physics to computational\nbiology. Traditionally, learning of the network structure has been done under\nthe assumption of chordality which ensures that efficient scoring methods can\nbe used. In general, non-chordal graphs have intractable normalizing constants\nwhich renders the calculation of Bayesian and other scores difficult beyond\nvery small-scale systems. Recently, there has been a surge of interest towards\nthe use of regularized pseudo-likelihood methods for structural learning of\nlarge-scale Markov network models, as such an approach avoids the assumption of\nchordality. The currently available methods typically necessitate the use of a\ntuning parameter to adapt the level of regularization for a particular dataset,\nwhich can be optimized for example by cross-validation. Here we introduce a\nBayesian version of pseudo-likelihood scoring of Markov networks, which enables\nan automatic regularization through marginalization over the nuisance\nparameters in the model. We prove consistency of the resulting MPL estimator\nfor the network structure via comparison with the pseudo information criterion.\nIdentification of the MPL-optimal network on a prescanned graph space is\nconsidered with both greedy hill climbing and exact pseudo-Boolean optimization\nalgorithms. We find that for reasonable sample sizes the hill climbing approach\nmost often identifies networks that are at a negligible distance from the\nrestricted global optimum. Using synthetic and existing benchmark networks, the\nmarginal pseudo-likelihood method is shown to generally perform favorably\nagainst recent popular inference methods for Markov networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 17:14:58 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 18:42:05 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Niiranen", "Juha", ""], ["Corander", "Jukka", ""]]}, {"id": "1401.5226", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "The Why and How of Nonnegative Matrix Factorization", "comments": "25 pages, 5 figures. Some typos and errors corrected, Section 3.2\n  reorganized", "journal-ref": null, "doi": null, "report-no": "In: \"Regularization, Optimization, Kernels, and Support Vector\n  Machines\", J.A.K. Suykens, M. Signoretto and A. Argyriou (eds), Chapman &\n  Hall/CRC, Machine Learning and Pattern Recognition Series, pp. 257-291, 2014", "categories": "stat.ML cs.IR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has become a widely used tool for the\nanalysis of high-dimensional data as it automatically extracts sparse and\nmeaningful features from a set of nonnegative data vectors. We first illustrate\nthis property of NMF on three applications, in image processing, text mining\nand hyperspectral imaging --this is the why. Then we address the problem of\nsolving NMF, which is NP-hard in general. We review some standard NMF\nalgorithms, and also present a recent subclass of NMF problems, referred to as\nnear-separable NMF, that can be solved efficiently (that is, in polynomial\ntime), even in the presence of noise --this is the how. Finally, we briefly\ndescribe some problems in mathematics and computer science closely related to\nNMF via the nonnegative rank.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 09:03:12 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 10:32:43 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1401.5408", "submitter": "Cristian Rojas", "authors": "Cristian R. Rojas and Bo Wahlberg", "title": "On change point detection using the fused lasso method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the asymptotic properties of l1 penalized maximum\nlikelihood estimation of signals with piece-wise constant mean values and/or\nvariances. The focus is on segmentation of a non-stationary time series with\nrespect to changes in these model parameters. This change point detection and\nestimation problem is also referred to as total variation denoising or l1 -mean\nfiltering and has many important applications in most fields of science and\nengineering. We establish the (approximate) sparse consistency properties,\nincluding rate of convergence, of the so-called fused lasso signal approximator\n(FLSA). We show that this only holds if the sign of the corresponding\nconsecutive changes are all different, and that this estimator is otherwise\nincapable of correctly detecting the underlying sparsity pattern. The key idea\nis to notice that the optimality conditions for this problem can be analyzed\nusing techniques related to brownian bridge theory.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 18:16:40 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1401.5492", "submitter": "Brendan Ames", "authors": "Brendan P.W. Ames and Mingyi Hong", "title": "Alternating direction method of multipliers for penalized zero-variance\n  discriminant analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of classification in the high dimensional setting where\nthe number of features of the given data is significantly greater than the\nnumber of observations. To accomplish this task, we propose a heuristic, called\nsparse zero-variance discriminant analysis (SZVD), for simultaneously\nperforming linear discriminant analysis and feature selection on high\ndimensional data. This method combines classical zero-variance discriminant\nanalysis, where discriminant vectors are identified in the null space of the\nsample within-class covariance matrix, with penalization applied to induce\nsparse structures in the resulting vectors. To approximately solve the\nresulting nonconvex problem, we develop a simple algorithm based on the\nalternating direction method of multipliers. Further, we show that this\nalgorithm is applicable to a larger class of penalized generalized eigenvalue\nproblems, including a particular relaxation of the sparse principal component\nanalysis problem. Finally, we establish theoretical guarantees for convergence\nof our algorithm to stationary points of the original nonconvex problem, and\nempirically demonstrate the effectiveness of our heuristic for classifying\nsimulated data and data drawn from applications in time-series classification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 21:29:37 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 22:44:29 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 17:45:27 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2015 14:57:07 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Ames", "Brendan P. W.", ""], ["Hong", "Mingyi", ""]]}, {"id": "1401.5508", "submitter": "Arno Solin", "authors": "Arno Solin and Simo S\\\"arkk\\\"a", "title": "Hilbert Space Methods for Reduced-Rank Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-019-09886-w", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel scheme for reduced-rank Gaussian process\nregression. The method is based on an approximate series expansion of the\ncovariance function in terms of an eigenfunction expansion of the Laplace\noperator in a compact subset of $\\mathbb{R}^d$. On this approximate eigenbasis\nthe eigenvalues of the covariance function can be expressed as simple functions\nof the spectral density of the Gaussian process, which allows the GP inference\nto be solved under a computational cost scaling as $\\mathcal{O}(nm^2)$\n(initial) and $\\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis\nfunctions and $n$ data points. Furthermore, the basis functions are independent\nof the parameters of the covariance function, which allows for very fast\nhyperparameter learning. The approach also allows for rigorous error analysis\nwith Hilbert space theory, and we show that the approximation becomes exact\nwhen the size of the compact subset and the number of eigenfunctions go to\ninfinity. We also show that the convergence rate of the truncation error is\nindependent of the input dimensionality provided that the differentiability\norder of the covariance function is increases appropriately, and for the\nsquared exponential covariance function it is always bounded by ${\\sim}1/m$\nregardless of the input dimensionality. The expansion generalizes to Hilbert\nspaces with an inner product which is defined as an integral over a specified\ninput density. The method is compared to previously proposed methods\ntheoretically and through empirical tests with simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 22:24:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 20:16:07 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 20:00:09 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1401.5625", "submitter": "Shohei Shimizu", "authors": "Joe Suzuki, Takanori Inazumi, Takashi Washio, Shohei Shimizu", "title": "Identifiability of an Integer Modular Acyclic Additive Noise Model and\n  its Causal Structure Discovery", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of causality is used in many situations dealing with uncertainty.\nWe consider the problem whether causality can be identified given data set\ngenerated by discrete random variables rather than continuous ones. In\nparticular, for non-binary data, thus far it was only known that causality can\nbe identified except rare cases. In this paper, we present necessary and\nsufficient condition for an integer modular acyclic additive noise (IMAN) of\ntwo variables. In addition, we relate bivariate and multivariate causal\nidentifiability in a more explicit manner, and develop a practical algorithm to\nfind the order of variables and their parent sets. We demonstrate its\nperformance in applications to artificial data and real world body motion data\nwith comparisons to conventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 11:23:50 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Suzuki", "Joe", ""], ["Inazumi", "Takanori", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1401.5636", "submitter": "Shohei Shimizu", "authors": "Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro\n  Yamamoto, Yoshinobu Kawahara", "title": "Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM", "comments": "10 pages. A longer version of our UAI2011 paper (Inazumi et al.,\n  2011). arXiv admin note: text overlap with arXiv:1202.3736", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relations among observed variables in a given data set is\na major objective in studies of statistics and artificial intelligence.\nRecently, some techniques to discover a unique causal model have been explored\nbased on non-Gaussianity of the observed data distribution. However, most of\nthese are limited to continuous data. In this paper, we present a novel causal\nmodel for binary data and propose an efficient new approach to deriving the\nunique causal model governing a given binary data set under skew distributions\nof external binary noises. Experimental evaluation shows excellent performance\nfor both artificial and real world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 11:58:27 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Inazumi", "Takanori", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""], ["Suzuki", "Joe", ""], ["Yamamoto", "Akihiro", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1401.5649", "submitter": "Cedric Fevotte", "authors": "C\\'edric F\\'evotte and Nicolas Dobigeon", "title": "Nonlinear hyperspectral unmixing with robust nonnegative matrix\n  factorization", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2468177", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a robust mixing model to describe hyperspectral data\nresulting from the mixture of several pure spectral signatures. This new model\nnot only generalizes the commonly used linear mixing model, but also allows for\npossible nonlinear effects to be easily handled, relying on mild assumptions\nregarding these nonlinearities. The standard nonnegativity and sum-to-one\nconstraints inherent to spectral unmixing are coupled with a group-sparse\nconstraint imposed on the nonlinearity component. This results in a new form of\nrobust nonnegative matrix factorization. The data fidelity term is expressed as\na beta-divergence, a continuous family of dissimilarity measures that takes the\nsquared Euclidean distance and the generalized Kullback-Leibler divergence as\nspecial cases. The penalized objective is minimized with a block-coordinate\ndescent that involves majorization-minimization updates. Simulation results\nobtained on synthetic and real data show that the proposed strategy competes\nwith state-of-the-art linear and nonlinear unmixing methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 13:04:16 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 16:48:45 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["F\u00e9votte", "C\u00e9dric", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1401.5823", "submitter": "Samuel Gross", "authors": "Samuel M. Gross, Robert Tibshirani", "title": "Collaborative Regression", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the scenario where one observes an outcome variable and sets of\nfeatures from multiple assays, all measured on the same set of samples. One\napproach that has been proposed for dealing with this type of data is ``sparse\nmultiple canonical correlation analysis'' (sparse mCCA). All of the current\nsparse mCCA techniques are biconvex and thus have no guarantees about reaching\na global optimum. We propose a method for performing sparse supervised\ncanonical correlation analysis (sparse sCCA), a specific case of sparse mCCA\nwhen one of the datasets is a vector. Our proposal for sparse sCCA is convex\nand thus does not face the same difficulties as the other methods. We derive\nefficient algorithms for this problem, and illustrate their use on simulated\nand real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 23:00:11 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Gross", "Samuel M.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1401.5899", "submitter": "Badong Chen", "authors": "Badong Chen, Junli Liang, Nanning Zheng, Jose C. Principe", "title": "Kernel Least Mean Square with Adaptive Kernel Size", "comments": "25 pages, 9 figures, and 4 tables", "journal-ref": "Neurocomputing,2016", "doi": "10.1016/j.neucom.2016.01.004", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel adaptive filters (KAF) are a class of powerful nonlinear filters\ndeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is\nusually the default kernel in KAF algorithms, but selecting the proper kernel\nsize (bandwidth) is still an open important issue especially for learning with\nsmall sample sizes. In previous research, the kernel size was set manually or\nestimated in advance by Silvermans rule based on the sample distribution. This\nstudy aims to develop an online technique for optimizing the kernel size of the\nkernel least mean square (KLMS) algorithm. A sequential optimization strategy\nis proposed, and a new algorithm is developed, in which the filter weights and\nthe kernel size are both sequentially updated by stochastic gradient algorithms\nthat minimize the mean square error (MSE). Theoretical results on convergence\nare also presented. The excellent performance of the new algorithm is confirmed\nby simulations on static function estimation and short term chaotic time series\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 09:19:27 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 08:52:30 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 08:48:35 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Chen", "Badong", ""], ["Liang", "Junli", ""], ["Zheng", "Nanning", ""], ["Principe", "Jose C.", ""]]}, {"id": "1401.5900", "submitter": "Nan Wang", "authors": "Nan Wang and Jan Melchior and Laurenz Wiskott", "title": "Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image\n  Statistics", "comments": "Current version is only an early manuscript and is subject to further\n  change", "journal-ref": "PLoS ONE 12(2): e0171015 (2017)", "doi": "10.1371/journal.pone.0171015", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis of Gaussian-binary restricted Boltzmann\nmachines (GRBMs) from the perspective of density models. The key aspect of this\nanalysis is to show that GRBMs can be formulated as a constrained mixture of\nGaussians, which gives a much better insight into the model's capabilities and\nlimitations. We show that GRBMs are capable of learning meaningful features\nboth in a two-dimensional blind source separation task and in modeling natural\nimages. Further, we show that reported difficulties in training GRBMs are due\nto the failure of the training algorithm rather than the model itself. Based on\nour analysis we are able to propose several training recipes, which allowed\nsuccessful and fast training in our experiments. Finally, we discuss the\nrelationship of GRBMs to several modifications that have been proposed to\nimprove the model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 09:21:15 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Wang", "Nan", ""], ["Melchior", "Jan", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1401.6024", "submitter": "Martin Slawski", "authors": "Martin Slawski, Matthias Hein, Pavlo Lutsik", "title": "Matrix factorization with Binary Components", "comments": "appeared in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application in computational biology, we consider low-rank\nmatrix factorization with $\\{0,1\\}$-constraints on one of the factors and\noptionally convex constraints on the second one. In addition to the\nnon-convexity shared with other matrix factorization schemes, our problem is\nfurther complicated by a combinatorial constraint set of size $2^{m \\cdot r}$,\nwhere $m$ is the dimension of the data points and $r$ the rank of the\nfactorization. Despite apparent intractability, we provide - in the line of\nrecent work on non-negative matrix factorization by Arora et al. (2012) - an\nalgorithm that provably recovers the underlying factorization in the exact case\nwith $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain this\nresult, we use theory around the Littlewood-Offord lemma from combinatorics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 16:02:19 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Slawski", "Martin", ""], ["Hein", "Matthias", ""], ["Lutsik", "Pavlo", ""]]}, {"id": "1401.6169", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "Parsimonious Topic Models with Salient Word Discovery", "comments": null, "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 27 (2015)\n  824-837", "doi": "10.1109/TKDE.2014.2345378", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious topic model for text corpora. In related models\nsuch as Latent Dirichlet Allocation (LDA), all words are modeled\ntopic-specifically, even though many words occur with similar frequencies\nacross different topics. Our modeling determines salient words for each topic,\nwhich have topic-specific probabilities, with the rest explained by a universal\nshared model. Further, in LDA all topics are in principle present in every\ndocument. By contrast our model gives sparse topic representation, determining\nthe (small) subset of relevant topics for each document. We derive a Bayesian\nInformation Criterion (BIC), balancing model complexity and goodness of fit.\nHere, interestingly, we identify an effective sample size and corresponding\npenalty specific to each parameter type in our model. We minimize BIC to\njointly determine our entire model -- the topic-specific words,\ndocument-specific topics, all model parameter values, {\\it and} the total\nnumber of topics -- in a wholly unsupervised fashion. Results on three text\ncorpora and an image dataset show that our model achieves higher test set\nlikelihood and better agreement with ground-truth class labels, compared to LDA\nand to a model designed to incorporate sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 21:47:48 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 20:24:41 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1401.6276", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer", "title": "The EM algorithm and the Laplace Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Laplace approximation calls for the computation of second derivatives at\nthe likelihood maximum. When the maximum is found by the EM-algorithm, there is\na convenient way to compute these derivatives. The likelihood gradient can be\nobtained from the EM-auxiliary, while the Hessian can be obtained from this\ngradient with the Pearlmutter trick.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 07:50:28 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Br\u00fcmmer", "Niko", ""]]}, {"id": "1401.6354", "submitter": "Karin Schnass", "authors": "Karin Schnass", "title": "Local Identification of Overcomplete Dictionaries", "comments": "32 pages, 2 figures, final version accepted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first theoretical results showing that stable\nidentification of overcomplete $\\mu$-coherent dictionaries $\\Phi \\in\n\\mathbb{R}^{d\\times K}$ is locally possible from training signals with sparsity\nlevels $S$ up to the order $O(\\mu^{-2})$ and signal to noise ratios up to\n$O(\\sqrt{d})$. In particular the dictionary is recoverable as the local maximum\nof a new maximisation criterion that generalises the K-means criterion. For\nthis maximisation criterion results for asymptotic exact recovery for sparsity\nlevels up to $O(\\mu^{-1})$ and stable recovery for sparsity levels up to\n$O(\\mu^{-2})$ as well as signal to noise ratios up to $O(\\sqrt{d})$ are\nprovided. These asymptotic results translate to finite sample size recovery\nresults with high probability as long as the sample size $N$ scales as $O(K^3dS\n\\tilde \\varepsilon^{-2})$, where the recovery precision $\\tilde \\varepsilon$\ncan go down to the asymptotically achievable precision. Further, to actually\nfind the local maxima of the new criterion, a very simple Iterative\nThresholding and K (signed) Means algorithm (ITKM), which has complexity\n$O(dKN)$ in each iteration, is presented and its local efficiency is\ndemonstrated in several experiments.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 14:41:31 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 13:37:23 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Schnass", "Karin", ""]]}, {"id": "1401.6413", "submitter": "Nuri Denizcan Vanli", "authors": "N. Denizcan Vanli, Muhammed O. Sayin, Suleyman S. Kozat", "title": "Predicting Nearly As Well As the Optimal Twice Differentiable Regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonlinear regression of real valued data in an individual sequence\nmanner, where we provide results that are guaranteed to hold without any\nstatistical assumptions. We address the convergence and undertraining issues of\nconventional nonlinear regression methods and introduce an algorithm that\nelegantly mitigates these issues via an incremental hierarchical structure,\n(i.e., via an incremental decision tree). Particularly, we present a piecewise\nlinear (or nonlinear) regression algorithm that partitions the regressor space\nin a data driven manner and learns a linear model at each region. Unlike the\nconventional approaches, our algorithm gradually increases the number of\ndisjoint partitions on the regressor space in a sequential manner according to\nthe observed data. Through this data driven approach, our algorithm\nsequentially and asymptotically achieves the performance of the optimal twice\ndifferentiable regression function for any data sequence with an unknown and\narbitrary length. The computational complexity of the introduced algorithm is\nonly logarithmic in the data length under certain regularity conditions. We\nprovide the explicit description of the algorithm and demonstrate the\nsignificant gains for the well-known benchmark real data sets and chaotic\nsignals.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 15:51:36 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 23:32:10 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Vanli", "N. Denizcan", ""], ["Sayin", "Muhammed O.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1401.6497", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Liqing Zhang, and Andrzej Cichocki", "title": "Bayesian CP Factorization of Incomplete Tensors with Automatic Rank\n  Determination", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2015.2392756", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful\ntechnique for tensor completion through explicitly capturing the multilinear\nlatent factors. The existing CP algorithms require the tensor rank to be\nmanually specified, however, the determination of tensor rank remains a\nchallenging problem especially for CP rank. In addition, existing approaches do\nnot take into account uncertainty information of latent factors, as well as\nmissing entries. To address these issues, we formulate CP factorization using a\nhierarchical probabilistic model and employ a fully Bayesian treatment by\nincorporating a sparsity-inducing prior over multiple latent factors and the\nappropriate hyperpriors over all hyperparameters, resulting in automatic rank\ndetermination. To learn the model, we develop an efficient deterministic\nBayesian inference algorithm, which scales linearly with data size. Our method\nis characterized as a tuning parameter-free approach, which can effectively\ninfer underlying multilinear factors with a low-rank constraint, while also\nproviding predictive distributions over missing entries. Extensive simulations\non synthetic data illustrate the intrinsic capability of our method to recover\nthe ground-truth of CP rank and prevent the overfitting problem, even when a\nlarge amount of entries are missing. Moreover, the results from real-world\napplications, including image inpainting and facial image synthesis,\ndemonstrate that our method outperforms state-of-the-art approaches for both\ntensor factorization and tensor completion in terms of predictive performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 05:08:33 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 09:48:37 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1401.6597", "submitter": "Sadi Seker E", "authors": "Sadi Evren Seker, Y. Unal, Z. Erdem, and H. Erdinc Kocer", "title": "Ensembled Correlation Between Liver Analysis Outputs", "comments": null, "journal-ref": "International Journal of Biology and Biomedical Engineering, ISSN:\n  1998-4510, Volume 8, pp. 1-5, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining techniques on the biological analysis are spreading for most of\nthe areas including the health care and medical information. We have applied\nthe data mining techniques, such as KNN, SVM, MLP or decision trees over a\nunique dataset, which is collected from 16,380 analysis results for a year.\nFurthermore we have also used meta-classifiers to question the increased\ncorrelation rate between the liver disorder and the liver analysis outputs. The\nresults show that there is a correlation among ALT, AST, Billirubin Direct and\nBillirubin Total down to 15% of error rate. Also the correlation coefficient is\nup to 94%. This makes possible to predict the analysis results from each other\nor disease patterns can be applied over the linear correlation of the\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 23:52:37 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Seker", "Sadi Evren", ""], ["Unal", "Y.", ""], ["Erdem", "Z.", ""], ["Kocer", "H. Erdinc", ""]]}, {"id": "1401.6623", "submitter": "Mathukumalli Vidyasagar", "authors": "Mehmet Eren Ahsen and Mathukumalli Vidyasagar", "title": "Near-Ideal Behavior of Compressed Sensing Algorithms", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, it is shown that the LASSO algorithm exhibits \"near-ideal\nbehavior,\" in the following sense: Suppose $y = Az + \\eta$ where $A$ satisfies\nthe restricted isometry property (RIP) with a sufficiently small constant, and\n$\\Vert \\eta \\Vert_2 \\leq \\epsilon$. Then minimizing $\\Vert z \\Vert_1$ subject\nto $\\Vert y - Az \\Vert_2 \\leq \\epsilon$ leads to an estimate $\\hat{x}$ whose\nerror $\\Vert \\hat{x} - x \\Vert_2$ is bounded by a universal constant times the\nerror achieved by an \"oracle\" that knows the location of the nonzero components\nof $x$. In the world of optimization, the LASSO algorithm has been generalized\nin several directions such as the group LASSO, the sparse group LASSO, either\nwithout or with tree-structured overlapping groups, and most recently, the\nsorted LASSO. In this paper, it is shown that {\\it any algorithm\\/} exhibits\nnear-ideal behavior in the above sense, provided only that (i) the norm used to\ndefine the sparsity index is \"decomposable,\" (ii) the penalty norm that is\nminimized in an effort to enforce sparsity is \"$\\gamma$-decomposable,\" and\n(iii) a \"compressibility condition\" in terms of a group restricted isometry\nproperty is satisfied. Specifically, the group LASSO, and the sparse group\nLASSO (with some permissible overlap in the groups), as well as the sorted\n$\\ell_1$-norm minimization all exhibit near-ideal behavior. Explicit bounds on\nthe residual error are derived that contain previously known results as special\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 07:38:23 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 04:48:12 GMT"}, {"version": "v3", "created": "Mon, 21 Apr 2014 00:17:21 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Ahsen", "Mehmet Eren", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1401.6638", "submitter": "Tong Wu", "authors": "Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid\n  Daubechies and Robert Calderbank", "title": "Painting Analysis Using Wavelets and Probabilistic Topic Models", "comments": "5 pages, 4 figures, ICIP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, computer-based techniques for stylistic analysis of paintings\nare applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto\ndi Bondone. Features are extracted by combining a dual-tree complex wavelet\ntransform with a hidden Markov tree (HMT) model. Hierarchical clustering is\nused to identify stylistic keywords in image patches, and keyword frequencies\nare calculated for sub-images that each contains many patches. A generative\nhierarchical Bayesian model learns stylistic patterns of keywords; these\npatterns are then used to characterize the styles of the sub-images; this in\nturn, permits to discriminate between paintings. Results suggest that such\nunsupervised probabilistic topic models can be useful to distill characteristic\nelements of style.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 11:00:46 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Wu", "Tong", ""], ["Polatkan", "Gungor", ""], ["Steel", "David", ""], ["Brown", "William", ""], ["Daubechies", "Ingrid", ""], ["Calderbank", "Robert", ""]]}, {"id": "1401.6686", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Russell Greiner", "title": "Perturbed Message Passing for Constraint Satisfaction Problems", "comments": null, "journal-ref": "JMLR 16(Jul):1249-1274, 2015", "doi": null, "report-no": null, "categories": "cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient message passing scheme for solving Constraint\nSatisfaction Problems (CSPs), which uses stochastic perturbation of Belief\nPropagation (BP) and Survey Propagation (SP) messages to bypass decimation and\ndirectly produce a single satisfying assignment. Our first CSP solver, called\nPerturbed Blief Propagation, smoothly interpolates two well-known inference\nprocedures; it starts as BP and ends as a Gibbs sampler, which produces a\nsingle sample from the set of solutions. Moreover we apply a similar\nperturbation scheme to SP to produce another CSP solver, Perturbed Survey\nPropagation. Experimental results on random and real-world CSPs show that\nPerturbed BP is often more successful and at the same time tens to hundreds of\ntimes more efficient than standard BP guided decimation. Perturbed BP also\ncompares favorably with state-of-the-art SP-guided decimation, which has a\ncomputational complexity that generally scales exponentially worse than our\nmethod (wrt the cardinality of variable domains and constraints). Furthermore,\nour experiments with random satisfiability and coloring problems demonstrate\nthat Perturbed SP can outperform SP-guided decimation, making it the best\nincomplete random CSP-solver in difficult regimes.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 20:32:35 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 00:05:44 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 00:13:28 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Greiner", "Russell", ""]]}, {"id": "1401.6740", "submitter": "Ichiro Takeuchi Prof.", "authors": "Kohei Ogawa, Yoshiki Suzuki, Shinya Suzumura and Ichiro Takeuchi", "title": "Safe Sample Screening for Support Vector Machines", "comments": "A preliminary version was presented at ICML2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse classifiers such as the support vector machines (SVM) are efficient in\ntest-phases because the classifier is characterized only by a subset of the\nsamples called support vectors (SVs), and the rest of the samples (non SVs)\nhave no influence on the classification result. However, the advantage of the\nsparsity has not been fully exploited in training phases because it is\ngenerally difficult to know which sample turns out to be SV beforehand. In this\npaper, we introduce a new approach called safe sample screening that enables us\nto identify a subset of the non-SVs and screen them out prior to the training\nphase. Our approach is different from existing heuristic approaches in the\nsense that the screened samples are guaranteed to be non-SVs at the optimal\nsolution. We investigate the advantage of the safe sample screening approach\nthrough intensive numerical experiments, and demonstrate that it can\nsubstantially decrease the computational cost of the state-of-the-art SVM\nsolvers such as LIBSVM. In the current big data era, we believe that safe\nsample screening would be of great practical importance since the data size can\nbe reduced without sacrificing the optimality of the final solution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 04:41:37 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Ogawa", "Kohei", ""], ["Suzuki", "Yoshiki", ""], ["Suzumura", "Shinya", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1401.6956", "submitter": "Joon Kwon", "authors": "Joon Kwon and Panayotis Mertikopoulos", "title": "A continuous-time approach to online optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a family of learning strategies for online optimization problems\nthat evolve in continuous time and we show that they lead to no regret. From a\nmore traditional, discrete-time viewpoint, this continuous-time approach allows\nus to derive the no-regret properties of a large class of discrete-time\nalgorithms including as special cases the exponential weight algorithm, online\nmirror descent, smooth fictitious play and vanishingly smooth fictitious play.\nIn so doing, we obtain a unified view of many classical regret bounds, and we\nshow that they can be decomposed into a term stemming from continuous-time\nconsiderations and a term which measures the disparity between discrete and\ncontinuous time. As a result, we obtain a general class of infinite horizon\nlearning strategies that guarantee an $\\mathcal{O}(n^{-1/2})$ regret bound\nwithout having to resort to a doubling trick.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 18:45:54 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 14:36:01 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Kwon", "Joon", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "1401.6978", "submitter": "Jing Lei", "authors": "Jing Lei, Vincent Q. Vu", "title": "Sparsistency and agnostic inference in sparse PCA", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1273 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 299-322", "doi": "10.1214/14-AOS1273", "report-no": "IMS-AOS-AOS1273", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of a sparse \"truth\" has been a constant assumption in the\ntheoretical analysis of sparse PCA and is often implicit in its methodological\ndevelopment. This naturally raises questions about the properties of sparse PCA\nmethods and how they depend on the assumption of sparsity. Under what\nconditions can the relevant variables be selected consistently if the truth is\nassumed to be sparse? What can be said about the results of sparse PCA without\nassuming a sparse and unique truth? We answer these questions by investigating\nthe properties of the recently proposed Fantope projection and selection (FPS)\nmethod in the high-dimensional setting. Our results provide general sufficient\nconditions for sparsistency of the FPS estimator. These conditions are weak and\ncan hold in situations where other estimators are known to fail. On the other\nhand, without assuming sparsity or identifiability, we show that FPS provides a\nsparse, linear dimension-reducing transformation that is close to the best\npossible in terms of maximizing the predictive covariance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 19:38:27 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 12:05:03 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 07:56:59 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Lei", "Jing", ""], ["Vu", "Vincent Q.", ""]]}, {"id": "1401.7020", "submitter": "Samantha Hansen", "authors": "R.H. Byrd, S.L. Hansen, J. Nocedal, Y.Singer", "title": "A Stochastic Quasi-Newton Method for Large-Scale Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to incorporate curvature information in stochastic\napproximation methods is challenging. The direct application of classical\nquasi- Newton updating techniques for deterministic optimization leads to noisy\ncurvature estimates that have harmful effects on the robustness of the\niteration. In this paper, we propose a stochastic quasi-Newton method that is\nefficient, robust and scalable. It employs the classical BFGS update formula in\nits limited memory form, and is based on the observation that it is beneficial\nto collect curvature information pointwise, and at regular intervals, through\n(sub-sampled) Hessian-vector products. This technique differs from the\nclassical approach that would compute differences of gradients, and where\ncontrolling the quality of the curvature estimates can be difficult. We present\nnumerical results on problems arising in machine learning that suggest that the\nproposed method shows much promise.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 21:01:33 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 12:01:35 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Byrd", "R. H.", ""], ["Hansen", "S. L.", ""], ["Nocedal", "J.", ""], ["Singer", "Y.", ""]]}, {"id": "1401.7116", "submitter": "Teemu Roos", "authors": "Andrew Barron, Teemu Roos and Kazuho Watanabe", "title": "Bayesian Properties of Normalized Maximum Likelihood and its Fast\n  Computation", "comments": "Submitted to ISIT-2004 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized maximized likelihood (NML) provides the minimax regret\nsolution in universal data compression, gambling, and prediction, and it plays\nan essential role in the minimum description length (MDL) method of statistical\nmodeling and estimation. Here we show that the normalized maximum likelihood\nhas a Bayes-like representation as a mixture of the component models, even in\nfinite samples, though the weights of linear combination may be both positive\nand negative. This representation addresses in part the relationship between\nMDL and Bayes modeling. This representation has the advantage of speeding the\ncalculation of marginals and conditionals required for coding and prediction\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 09:06:30 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Barron", "Andrew", ""], ["Roos", "Teemu", ""], ["Watanabe", "Kazuho", ""]]}, {"id": "1401.7145", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent and Brooks Paige and Frank Wood", "title": "Tempering by Subsampling", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate that tempering Markov chain Monte Carlo samplers\nfor Bayesian models by recursively subsampling observations without replacement\ncan improve the performance of baseline samplers in terms of effective sample\nsize per computation. We present two tempering by subsampling algorithms,\nsubsampled parallel tempering and subsampled tempered transitions. We provide\nan asymptotic analysis of the computational cost of tempering by subsampling,\nverify that tempering by subsampling costs less than traditional tempering, and\ndemonstrate both algorithms on Bayesian approaches to learning the mean of a\nhigh dimensional multivariate Normal and estimating Gaussian process\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 11:44:29 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Paige", "Brooks", ""], ["Wood", "Frank", ""]]}, {"id": "1401.7388", "submitter": "Benjamin Rubinstein", "authors": "J. Hyam Rubinstein and Benjamin I. P. Rubinstein and Peter L. Bartlett", "title": "Bounding Embeddings of VC Classes into Maximum Classes", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the earliest conjectures in computational learning theory-the Sample\nCompression conjecture-asserts that concept classes (equivalently set systems)\nadmit compression schemes of size linear in their VC dimension. To-date this\nstatement is known to be true for maximum classes---those that possess maximum\ncardinality for their VC dimension. The most promising approach to positively\nresolving the conjecture is by embedding general VC classes into maximum\nclasses without super-linear increase to their VC dimensions, as such\nembeddings would extend the known compression schemes to all VC classes. We\nshow that maximum classes can be characterised by a local-connectivity property\nof the graph obtained by viewing the class as a cubical complex. This geometric\ncharacterisation of maximum VC classes is applied to prove a negative embedding\nresult which demonstrates VC-d classes that cannot be embedded in any maximum\nclass of VC dimension lower than 2d. On the other hand, we show that every VC-d\nclass C embeds in a VC-(d+D) maximum class where D is the deficiency of C,\ni.e., the difference between the cardinalities of a maximum VC-d class and of\nC. For VC-2 classes in binary n-cubes for 4 <= n <= 6, we give best possible\nresults on embedding into maximum classes. For some special classes of Boolean\nfunctions, relationships with maximum classes are investigated. Finally we give\na general recursive procedure for embedding VC-d classes into VC-(d+k) maximum\nclasses for smallest k.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 02:09:10 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Rubinstein", "J. Hyam", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1401.7413", "submitter": "Canyi Lu", "authors": "Canyi Lu, Zhouchen Lin, Shuicheng Yan", "title": "Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted\n  Least Squares Minimization", "comments": "IEEE Transactions on Image Processing 2015", "journal-ref": null, "doi": "10.1109/TIP.2014.2380155", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a general framework for solving the low rank and/or sparse\nmatrix minimization problems, which may involve multiple non-smooth terms. The\nIteratively Reweighted Least Squares (IRLS) method is a fast solver, which\nsmooths the objective function and minimizes it by alternately updating the\nvariables and their weights. However, the traditional IRLS can only solve a\nsparse only or low rank only minimization problem with squared loss or an\naffine constraint. This work generalizes IRLS to solve joint/mixed low rank and\nsparse minimization problems, which are essential formulations for many tasks.\nAs a concrete example, we solve the Schatten-$p$ norm and $\\ell_{2,q}$-norm\nregularized Low-Rank Representation (LRR) problem by IRLS, and theoretically\nprove that the derived solution is a stationary point (globally optimal if\n$p,q\\geq1$). Our convergence proof of IRLS is more general than previous one\nwhich depends on the special properties of the Schatten-$p$ norm and\n$\\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets\ndemonstrate that our IRLS is much more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 05:16:52 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 14:20:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Lu", "Canyi", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1401.7620", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando\n  Perez-Cruz", "title": "Bayesian nonparametric comorbidity analysis of psychiatric disorders", "comments": "Submitted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of comorbidity is an open and complex research field in the\nbranch of psychiatry, where clinical experience and several studies suggest\nthat the relation among the psychiatric disorders may have etiological and\ntreatment implications. In this paper, we are interested in applying latent\nfeature modeling to find the latent structure behind the psychiatric disorders\nthat can help to examine and explain the relationships among them. To this end,\nwe use the large amount of information collected in the National Epidemiologic\nSurvey on Alcohol and Related Conditions (NESARC) database and propose to model\nthese data using a nonparametric latent model based on the Indian Buffet\nProcess (IBP). Due to the discrete nature of the data, we first need to adapt\nthe observation model for discrete random variables. We propose a generative\nmodel in which the observations are drawn from a multinomial-logit distribution\ngiven the IBP matrix. The implementation of an efficient Gibbs sampler is\naccomplished using the Laplace approximation, which allows integrating out the\nweighting factors of the multinomial-logit likelihood model. We also provide a\nvariational inference algorithm for this model, which provides a complementary\n(and less expensive in terms of computational complexity) alternative to the\nGibbs sampler allowing us to deal with a larger number of data. Finally, we use\nthe model to analyze comorbidity among the psychiatric disorders diagnosed by\nexperts from the NESARC database.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 18:46:38 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Valera", "Isabel", ""], ["Blanco", "Carlos", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1401.7625", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alejandro Ribeiro", "title": "RES: Regularized Stochastic BFGS Algorithm", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2014.2357775", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno\n(BFGS) quasi-Newton method is proposed to solve convex optimization problems\nwith stochastic objectives. The use of stochastic gradient descent algorithms\nis widespread, but the number of iterations required to approximate optimal\narguments can be prohibitive in high dimensional problems. Application of\nsecond order methods, on the other hand, is impracticable because computation\nof objective function Hessian inverses incurs excessive computational cost.\nBFGS modifies gradient descent by introducing a Hessian approximation matrix\ncomputed from finite gradient differences. RES utilizes stochastic gradients in\nlieu of deterministic gradients for both, the determination of descent\ndirections and the approximation of the objective function's curvature. Since\nstochastic gradients can be computed at manageable computational cost RES is\nrealizable and retains the convergence rate advantages of its deterministic\ncounterparts. Convergence results show that lower and upper bounds on the\nHessian egeinvalues of the sample functions are sufficient to guarantee\nconvergence to optimal arguments. Numerical experiments showcase reductions in\nconvergence time relative to stochastic gradient descent algorithms and\nnon-regularized stochastic versions of BFGS. An application of RES to the\nimplementation of support vector machines is developed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 19:10:23 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1401.7702", "submitter": "Benjamin Miller", "authors": "Benjamin A. Miller, Michelle S. Beard, Patrick J. Wolfe, and Nadya T.\n  Bliss", "title": "A Spectral Framework for Anomalous Subgraph Detection", "comments": "In submission to the IEEE, 16 pages, 8 figures", "journal-ref": "IEEE Trans. Signal Process. 63 (2015) 4191-4206", "doi": "10.1109/TSP.2015.2437841", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of application domains are concerned with data consisting of\nentities and their relationships or connections, formally represented as\ngraphs. Within these diverse application areas, a common problem of interest is\nthe detection of a subset of entities whose connectivity is anomalous with\nrespect to the rest of the data. While the detection of such anomalous\nsubgraphs has received a substantial amount of attention, no\napplication-agnostic framework exists for analysis of signal detectability in\ngraph-based data. In this paper, we describe a framework that enables such\nanalysis using the principal eigenspace of a graph's residuals matrix, commonly\ncalled the modularity matrix in community detection. Leveraging this analytical\ntool, we show that the framework has a natural power metric in the spectral\nnorm of the anomalous subgraph's adjacency matrix (signal power) and of the\nbackground graph's residuals matrix (noise power). We propose several\nalgorithms based on spectral properties of the residuals matrix, with more\ncomputationally expensive techniques providing greater detection power.\nDetection and identification performance are presented for a number of signal\nand noise models, including clusters and bipartite foregrounds embedded into\nsimple random backgrounds as well as graphs with community structure and\nrealistic degree distributions. The trends observed verify intuition gleaned\nfrom other signal processing areas, such as greater detection power when the\nsignal is embedded within a less active portion of the background. We\ndemonstrate the utility of the proposed techniques in detecting small, highly\nanomalous subgraphs in real graphs derived from Internet traffic and product\nco-purchases.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 23:39:39 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 04:01:00 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Miller", "Benjamin A.", ""], ["Beard", "Michelle S.", ""], ["Wolfe", "Patrick J.", ""], ["Bliss", "Nadya T.", ""]]}, {"id": "1401.7709", "submitter": "Deepayan Chakrabarti", "authors": "Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus A.\n  Macskassy", "title": "Joint Inference of Multiple Label Types in Large Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of inferring node labels in a partially labeled graph\nwhere each node in the graph has multiple label types and each label type has a\nlarge number of possible labels. Our primary example, and the focus of this\npaper, is the joint inference of label types such as hometown, current city,\nand employers, for users connected by a social network. Standard label\npropagation fails to consider the properties of the label types and the\ninteractions between them. Our proposed method, called EdgeExplain, explicitly\nmodels these, while still enabling scalable inference under a distributed\nmessage-passing architecture. On a billion-node subset of the Facebook social\nnetwork, EdgeExplain significantly outperforms label propagation for several\nlabel types, with lifts of up to 120% for recall@1 and 60% for recall@3.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 00:38:53 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Chakrabarti", "Deepayan", ""], ["Funiak", "Stanislav", ""], ["Chang", "Jonathan", ""], ["Macskassy", "Sofus A.", ""]]}, {"id": "1401.8008", "submitter": "David Venuto", "authors": "David Venuto, Toby Dylan Hocking, Lakjaree Sphanurattana, Masashi\n  Sugiyama", "title": "Support vector comparison machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In ranking problems, the goal is to learn a ranking function from labeled\npairs of input points. In this paper, we consider the related comparison\nproblem, where the label indicates which element of the pair is better, or if\nthere is no significant difference. We cast the learning problem as a margin\nmaximization, and show that it can be solved by converting it to a standard\nSVM. We use simulated nonlinear patterns, a real learning to rank sushi data\nset, and a chess data set to show that our proposed SVMcompare algorithm\noutperforms SVMrank when there are equality pairs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 21:49:16 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 21:44:21 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 23:55:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Venuto", "David", ""], ["Hocking", "Toby Dylan", ""], ["Sphanurattana", "Lakjaree", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1401.8017", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Stephane Gaiffas, Bertrand Michel", "title": "Sparse Bayesian Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about variable selection, clustering and estimation in an\nunsupervised high-dimensional setting. Our approach is based on fitting\nconstrained Gaussian mixture models, where we learn the number of clusters $K$\nand the set of relevant variables $S$ using a generalized Bayesian posterior\nwith a sparsity inducing prior. We prove a sparsity oracle inequality which\nshows that this procedure selects the optimal parameters $K$ and $S$. This\nprocedure is implemented using a Metropolis-Hastings algorithm, based on a\nclustering-oriented greedy proposal, which makes the convergence to the\nposterior very fast.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 22:40:35 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Gaiffas", "Stephane", ""], ["Michel", "Bertrand", ""]]}, {"id": "1401.8066", "submitter": "Minh Ha Quang", "authors": "Ha Quang Minh and Loris Bazzani and Vittorio Murino", "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces\n  for Manifold Regularization and Co-Regularized Multi-view Learning", "comments": "72 pages", "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-72", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general vector-valued reproducing kernel Hilbert spaces\n(RKHS) framework for the problem of learning an unknown functional dependency\nbetween a structured input space and a structured output space. Our formulation\nencompasses both Vector-valued Manifold Regularization and Co-regularized\nMulti-view Learning, providing in particular a unifying framework linking these\ntwo important learning approaches. In the case of the least square loss\nfunction, we provide a closed form solution, which is obtained by solving a\nsystem of linear equations. In the case of Support Vector Machine (SVM)\nclassification, our formulation generalizes in particular both the binary\nLaplacian SVM to the multi-class, multi-view settings and the multi-class\nSimplex Cone SVM to the semi-supervised, multi-view settings. The solution is\nobtained by solving a single quadratic optimization problem, as in standard\nSVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results\nobtained on the task of object recognition, using several challenging datasets,\ndemonstrate the competitiveness of our algorithms compared with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 05:29:45 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 16:51:22 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Minh", "Ha Quang", ""], ["Bazzani", "Loris", ""], ["Murino", "Vittorio", ""]]}, {"id": "1401.8078", "submitter": "Henrik Nyman", "authors": "Henrik Nyman, Jie Xiong, Johan Pensar and Jukka Corander", "title": "Marginal and simultaneous predictive classification using stratified\n  graphical models", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s11634-015-0199-5", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inductive probabilistic classification rule must generally obey the\nprinciples of Bayesian predictive inference, such that all observed and\nunobserved stochastic quantities are jointly modeled and the parameter\nuncertainty is fully acknowledged through the posterior predictive\ndistribution. Several such rules have been recently considered and their\nasymptotic behavior has been characterized under the assumption that the\nobserved features or variables used for building a classifier are conditionally\nindependent given a simultaneous labeling of both the training samples and\nthose from an unknown origin. Here we extend the theoretical results to\npredictive classifiers acknowledging feature dependencies either through\ngraphical models or sparser alternatives defined as stratified graphical\nmodels. We also show through experimentation with both synthetic and real data\nthat the predictive classifiers based on stratified graphical models have\nconsistently best accuracy compared with the predictive classifiers based on\neither conditionally independent features or on ordinary graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 07:54:14 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Nyman", "Henrik", ""], ["Xiong", "Jie", ""], ["Pensar", "Johan", ""], ["Corander", "Jukka", ""]]}, {"id": "1401.8126", "submitter": "Chunhua Shen", "authors": "Mehrtash Harandi, Richard Hartley, Chunhua Shen, Brian Lovell, Conrad\n  Sanderson", "title": "Extrinsic Methods for Coding and Dictionary Learning on Grassmann\n  Manifolds", "comments": "Appearing in International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based representations have recently led to notable results in\nvarious visual recognition tasks. In a separate line of research, Riemannian\nmanifolds have been shown useful for dealing with features and models that do\nnot lie in Euclidean spaces. With the aim of building a bridge between the two\nrealms, we address the problem of sparse coding and dictionary learning over\nthe space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping. This in turn enables\nus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we\npropose closed-form solutions for learning a Grassmann dictionary, atom by\natom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann\nsparse coding and dictionary learning algorithms through embedding into Hilbert\nspaces.\n  Experiments on several classification tasks (gender recognition, gesture\nclassification, scene analysis, face recognition, action recognition and\ndynamic texture classification) show that the proposed approaches achieve\nconsiderable improvements in discrimination accuracy, in comparison to\nstate-of-the-art methods such as kernelized Affine Hull Method and\ngraph-embedding Grassmann discriminant analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 10:59:38 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 00:12:44 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Hartley", "Richard", ""], ["Shen", "Chunhua", ""], ["Lovell", "Brian", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1401.8257", "submitter": "Shuai Li", "authors": "Claudio Gentile, Shuai Li, Giovanni Zappella", "title": "Online Clustering of Bandits", "comments": "In E. Xing and T. Jebara (Eds.), Proceedings of 31st International\n  Conference on Machine Learning, Journal of Machine Learning Research Workshop\n  and Conference Proceedings, Vol.32 (JMLR W&CP-32), Beijing, China, Jun.\n  21-26, 2014 (ICML 2014), Submitted by Shuai Li\n  (https://sites.google.com/site/shuailidotsli)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithmic approach to content recommendation based on\nadaptive clustering of exploration-exploitation (\"bandit\") strategies. We\nprovide a sharp regret analysis of this algorithm in a standard stochastic\nnoise setting, demonstrate its scalability properties, and prove its\neffectiveness on a number of artificial and real-world datasets. Our\nexperiments show a significant increase in prediction performance over\nstate-of-the-art methods for bandit problems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 18:49:42 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 07:13:06 GMT"}, {"version": "v3", "created": "Fri, 6 Jun 2014 13:59:04 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Zappella", "Giovanni", ""]]}]