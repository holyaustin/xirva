[{"id": "1709.00025", "submitter": "Nasser Mohammadiha", "authors": "Nasser Mohammadiha, Paris Smaragdis, Ghazaleh Panahandeh, Simon Doclo", "title": "A State-Space Approach to Dynamic Nonnegative Matrix Factorization", "comments": null, "journal-ref": "IEEE Trans. Signal Process., vol. 63, no. 4, pp. 949--959, Dec.\n  2014", "doi": "10.1109/TSP.2014.2385655", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has been actively investigated and\nused in a wide range of problems in the past decade. A significant amount of\nattention has been given to develop NMF algorithms that are suitable to model\ntime series with strong temporal dependencies. In this paper, we propose a\nnovel state-space approach to perform dynamic NMF (D-NMF). In the proposed\nprobabilistic framework, the NMF coefficients act as the state variables and\ntheir dynamics are modeled using a multi-lag nonnegative vector autoregressive\n(N-VAR) model within the process equation. We use expectation maximization and\npropose a maximum-likelihood estimation framework to estimate the basis matrix\nand the N-VAR model parameters. Interestingly, the N-VAR model parameters are\nobtained by simply applying NMF. Moreover, we derive a maximum a posteriori\nestimate of the state variables (i.e., the NMF coefficients) that is based on a\nprediction step and an update step, similarly to the Kalman filter. We\nillustrate the benefits of the proposed approach using different numerical\nsimulations where D-NMF significantly outperforms its static counterpart.\nExperimental results for three different applications show that the proposed\napproach outperforms two state-of-the-art NMF approaches that exploit temporal\ndependencies, namely a nonnegative hidden Markov model and a frame stacking\napproach, while it requires less memory and computational power.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:12:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Mohammadiha", "Nasser", ""], ["Smaragdis", "Paris", ""], ["Panahandeh", "Ghazaleh", ""], ["Doclo", "Simon", ""]]}, {"id": "1709.00037", "submitter": "Shiwei Lan", "authors": "Tapio Schneider, Shiwei Lan, Andrew Stuart and Jo\\~ao Teixeira", "title": "Earth System Modeling 2.0: A Blueprint for Models That Learn From\n  Observations and Targeted High-Resolution Simulations", "comments": "32 pages, 3 figures", "journal-ref": "Geophysical Research Letters 2017", "doi": "10.1002/2017GL076101", "report-no": null, "categories": "stat.ML physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate projections continue to be marred by large uncertainties, which\noriginate in processes that need to be parameterized, such as clouds,\nconvection, and ecosystems. But rapid progress is now within reach. New\ncomputational tools and methods from data assimilation and machine learning\nmake it possible to integrate global observations and local high-resolution\nsimulations in an Earth system model (ESM) that systematically learns from\nboth. Here we propose a blueprint for such an ESM. We outline how\nparameterization schemes can learn from global observations and targeted\nhigh-resolution simulations, for example, of clouds and convection, through\nmatching low-order statistics between ESMs, observations, and high-resolution\nsimulations. We illustrate learning algorithms for ESMs with a simple dynamical\nsystem that shares characteristics of the climate system; and we discuss the\nopportunities the proposed framework presents and the challenges that remain to\nrealize it.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:45:40 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:37:32 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 23:00:55 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Schneider", "Tapio", ""], ["Lan", "Shiwei", ""], ["Stuart", "Andrew", ""], ["Teixeira", "Jo\u00e3o", ""]]}, {"id": "1709.00092", "submitter": "Emre Demirkaya", "authors": "Yingying Fan, Emre Demirkaya, Gaorong Li and Jinchi Lv", "title": "RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs", "comments": "37 pages, 6 tables, 9 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power and reproducibility are key to enabling refined scientific discoveries\nin contemporary big data applications with general high-dimensional nonlinear\nmodels. In this paper, we provide theoretical foundations on the power and\nrobustness for the model-free knockoffs procedure introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2016) in high-dimensional setting when the\ncovariate distribution is characterized by Gaussian graphical model. We\nestablish that under mild regularity conditions, the power of the oracle\nknockoffs procedure with known covariate distribution in high-dimensional\nlinear models is asymptotically one as sample size goes to infinity. When\nmoving away from the ideal case, we suggest the modified model-free knockoffs\nmethod called graphical nonlinear knockoffs (RANK) to accommodate the unknown\ncovariate distribution. We provide theoretical justifications on the robustness\nof our modified procedure by showing that the false discovery rate (FDR) is\nasymptotically controlled at the target level and the power is asymptotically\none with the estimated covariate distribution. To the best of our knowledge,\nthis is the first formal theoretical result on the power for the knockoffs\nprocedure. Simulation results demonstrate that compared to existing approaches,\nour method performs competitively in both FDR control and power. A real data\nset is analyzed to further assess the performance of the suggested knockoffs\nprocedure.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 21:50:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Fan", "Yingying", ""], ["Demirkaya", "Emre", ""], ["Li", "Gaorong", ""], ["Lv", "Jinchi", ""]]}, {"id": "1709.00106", "submitter": "Brendt Wohlberg", "authors": "Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, Wotao Yin", "title": "First and Second Order Methods for Online Convolutional Dictionary\n  Learning", "comments": null, "journal-ref": "SIAM J. Imaging Sci., 11(2), 1589-1628, 2018", "doi": "10.1137/17M1145689", "report-no": null, "categories": "cs.LG cs.CV eess.IV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse representations are a form of sparse representation with\na structured, translation invariant dictionary. Most convolutional dictionary\nlearning algorithms to date operate in batch mode, requiring simultaneous\naccess to all training images during the learning process, which results in\nvery high memory usage and severely limits the training data that can be used.\nVery recently, however, a number of authors have considered the design of\nonline convolutional dictionary learning algorithms that offer far better\nscaling of memory and computational cost with training set size than batch\nmethods. This paper extends our prior work, improving a number of aspects of\nour previous algorithm; proposing an entirely new one, with better performance,\nand that supports the inclusion of a spatial mask for learning from incomplete\ndata; and providing a rigorous theoretical analysis of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 23:19:02 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 17:18:20 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 19:21:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Liu", "Jialin", ""], ["Garcia-Cardona", "Cristina", ""], ["Wohlberg", "Brendt", ""], ["Yin", "Wotao", ""]]}, {"id": "1709.00127", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "Low Permutation-rank Matrices: Structural Properties and Noisy\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of noisy matrix completion, in which the goal is to\nreconstruct a structured matrix whose entries are partially observed in noise.\nStandard approaches to this underdetermined inverse problem are based on\nassuming that the underlying matrix has low rank, or is well-approximated by a\nlow rank matrix. In this paper, we propose a richer model based on what we term\nthe \"permutation-rank\" of a matrix. We first describe how the classical\nnon-negative rank model enforces restrictions that may be undesirable in\npractice, and how and these restrictions can be avoided by using the richer\npermutation-rank model. Second, we establish the minimax rates of estimation\nunder the new permutation-based model, and prove that surprisingly, the minimax\nrates are equivalent up to logarithmic factors to those for estimation under\nthe typical low rank model. Third, we analyze a computationally efficient\nsingular-value-thresholding algorithm, known to be optimal for the low-rank\nsetting, and show that it also simultaneously yields a consistent estimator for\nthe low-permutation rank setting. Finally, we present various structural\nresults characterizing the uniqueness of the permutation-rank decomposition,\nand characterizing convex approximations of the permutation-rank polytope.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 01:25:45 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1709.00139", "submitter": "Hansi Jiang", "authors": "Hansi Jiang, Haoyu Wang, Wenhao Hu, Deovrat Kakde and Arin Chaudhuri", "title": "Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel", "comments": "18 pages, 1 table, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector data description (SVDD) is a machine learning technique that\nis used for single-class classification and outlier detection. The idea of SVDD\nis to find a set of support vectors that defines a boundary around data. When\ndealing with online or large data, existing batch SVDD methods have to be rerun\nin each iteration. We propose an incremental learning algorithm for SVDD that\nuses the Gaussian kernel. This algorithm builds on the observation that all\nsupport vectors on the boundary have the same distance to the center of sphere\nin a higher-dimensional feature space as mapped by the Gaussian kernel\nfunction. Each iteration involves only the existing support vectors and the new\ndata point. Moreover, the algorithm is based solely on matrix manipulations;\nthe support vectors and their corresponding Lagrange multiplier $\\alpha_i$'s\nare automatically selected and determined in each iteration. It can be seen\nthat the complexity of our algorithm in each iteration is only $O(k^2)$, where\n$k$ is the number of support vectors. Experimental results on some real data\nsets indicate that FISVDD demonstrates significant gains in efficiency with\nalmost no loss in either outlier detection accuracy or objective function\nvalue.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 02:47:05 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 18:20:36 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 23:37:31 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 22:13:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Jiang", "Hansi", ""], ["Wang", "Haoyu", ""], ["Hu", "Wenhao", ""], ["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1709.00147", "submitter": "Bharath Sriperumbudur", "authors": "Motonobu Kanagawa, Bharath K. Sriperumbudur and Kenji Fukumizu", "title": "Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in\n  Misspecified Settings", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convergence analysis of kernel-based quadrature rules\nin misspecified settings, focusing on deterministic quadrature in Sobolev\nspaces. In particular, we deal with misspecified settings where a test\nintegrand is less smooth than a Sobolev RKHS based on which a quadrature rule\nis constructed. We provide convergence guarantees based on two different\nassumptions on a quadrature rule: one on quadrature weights, and the other on\ndesign points. More precisely, we show that convergence rates can be derived\n(i) if the sum of absolute weights remains constant (or does not increase\nquickly), or (ii) if the minimum distance between design points does not\ndecrease very quickly. As a consequence of the latter result, we derive a rate\nof convergence for Bayesian quadrature in misspecified settings. We reveal a\ncondition on design points to make Bayesian quadrature robust to\nmisspecification, and show that, under this condition, it may adaptively\nachieve the optimal rate of convergence in the Sobolev space of a lesser order\n(i.e., of the unknown smoothness of a test integrand), under a slightly\nstronger regularity condition on the integrand.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:08:50 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 16:12:25 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Kanagawa", "Motonobu", ""], ["Sriperumbudur", "Bharath K.", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1709.00199", "submitter": "Lior Wolf", "authors": "Naama Hadad, Lior Wolf, Moni Shahar", "title": "A Two-Step Disentanglement Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of disentanglement of factors that generate a given\ndata into those that are correlated with the labeling and those that are not.\nOur solution is simpler than previous solutions and employs adversarial\ntraining. First, the part of the data that is correlated with the labels is\nextracted by training a classifier. Then, the other part is extracted such that\nit enables the reconstruction of the original data but does not contain label\ninformation. The utility of the new method is demonstrated on visual datasets\nas well as on financial data. Our code is available at\nhttps://github.com/naamahadad/A-Two-Step-Disentanglement-Method\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 08:45:43 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 16:19:44 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Hadad", "Naama", ""], ["Wolf", "Lior", ""], ["Shahar", "Moni", ""]]}, {"id": "1709.00291", "submitter": "Vladislav Tadi\\'c B", "authors": "Vladislav B. Tadic, Arnaud Doucet", "title": "Asymptotic Bias of Stochastic Gradient Search", "comments": "arXiv admin note: text overlap with arXiv:0907.1020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behavior of the stochastic gradient algorithm with a biased\ngradient estimator is analyzed. Relying on arguments based on the dynamic\nsystem theory (chain-recurrence) and the differential geometry (Yomdin theorem\nand Lojasiewicz inequality), tight bounds on the asymptotic bias of the\niterates generated by such an algorithm are derived. The obtained results hold\nunder mild conditions and cover a broad class of high-dimensional nonlinear\nalgorithms. Using these results, the asymptotic properties of the\npolicy-gradient (reinforcement) learning and adaptive population Monte Carlo\nsampling are studied. Relying on the same results, the asymptotic behavior of\nthe recursive maximum split-likelihood estimation in hidden Markov models is\nanalyzed, too.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 20:07:51 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tadic", "Vladislav B.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1709.00379", "submitter": "Guanhao Feng", "authors": "Guanhao Feng, Nicholas Polson, Yuexi Wang and Jianeng Xu", "title": "Sparse Regularization in Marketing and Economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse alpha-norm regularization has many data-rich applications in Marketing\nand Economics. Alpha-norm, in contrast to lasso and ridge regularization, jumps\nto a sparse solution. This feature is attractive for ultra high-dimensional\nproblems that occur in demand estimation and forecasting. The alpha-norm\nobjective is nonconvex and requires coordinate descent and proximal operators\nto find the sparse solution. We study a typical marketing demand forecasting\nproblem, grocery store sales for salty snacks, that has many dummy variables as\ncontrols. The key predictors of demand include price, equivalized volume,\npromotion, flavor, scent, and brand effects. By comparing with many commonly\nused machine learning methods, alpha-norm regularization achieves its goal of\nproviding accurate out-of-sample estimates for the promotion lift effects.\nFinally, we conclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 16:01:25 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 16:33:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Feng", "Guanhao", ""], ["Polson", "Nicholas", ""], ["Wang", "Yuexi", ""], ["Xu", "Jianeng", ""]]}, {"id": "1709.00401", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz", "title": "Statistical Inference for Data-adaptive Doubly Robust Estimators with\n  Survival Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency of doubly robust estimators relies on consistent estimation\nof at least one of two nuisance regression parameters. In moderate to large\ndimensions, the use of flexible data-adaptive regression estimators may aid in\nachieving this consistency. However, $n^{1/2}$-consistency of doubly robust\nestimators is not guaranteed if one of the nuisance estimators is inconsistent.\nIn this paper we present a doubly robust estimator for survival analysis with\nthe novel property that it converges to a Gaussian variable at $n^{1/2}$-rate\nfor a large class of data-adaptive estimators of the nuisance parameters, under\nthe only assumption that at least one of them is consistently estimated at a\n$n^{1/4}$-rate. This result is achieved through adaptation of recent ideas in\nsemiparametric inference, which amount to: (i) Gaussianizing (i.e., making\nasymptotically linear) a drift term that arises in the asymptotic analysis of\nthe doubly robust estimator, and (ii) using cross-fitting to avoid entropy\nconditions on the nuisance estimators. We present the formula of the asymptotic\nvariance of the estimator, which allows computation of doubly robust confidence\nintervals and p-values. We illustrate the finite-sample properties of the\nestimator in simulation studies, and demonstrate its use in a phase III\nclinical trial for estimating the effect of a novel therapy for the treatment\nof HER2 positive breast cancer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 17:46:07 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:42:56 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 16:14:18 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""]]}, {"id": "1709.00407", "submitter": "Xueyu Mao", "authors": "Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti", "title": "Estimating Mixed Memberships with Sharp Eigenvector Deviations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating community memberships of nodes in a\nnetwork, where every node is associated with a vector determining its degree of\nmembership in each community. Existing provably consistent algorithms often\nrequire strong assumptions about the population, are computationally expensive,\nand only provide an overall error bound for the whole community membership\nmatrix. This paper provides uniform rates of convergence for the inferred\ncommunity membership vector of each node in a network generated from the Mixed\nMembership Stochastic Blockmodel (MMSB); to our knowledge, this is the first\nwork to establish per-node rates for overlapping community detection in\nnetworks. We achieve this by establishing sharp row-wise eigenvector deviation\nbounds for MMSB. Based on the simplex structure inherent in the\neigen-decomposition of the population matrix, we build on established\ncorner-finding algorithms from the optimization community to infer the\ncommunity membership vectors. Our results hold over a broad parameter regime\nwhere the average degree only grows poly-logarithmically with the number of\nnodes. Using experiments with simulated and real datasets, we show that our\nmethod achieves better error with lower variability over competing methods, and\nprocesses real world networks of up to 100,000 nodes within tens of seconds.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 18:25:02 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 10:05:34 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 00:30:00 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Mao", "Xueyu", ""], ["Sarkar", "Purnamrita", ""], ["Chakrabarti", "Deepayan", ""]]}, {"id": "1709.00440", "submitter": "Briland Hitaj", "authors": "Briland Hitaj, Paolo Gasti, Giuseppe Ateniese, Fernando Perez-Cruz", "title": "PassGAN: A Deep Learning Approach for Password Guessing", "comments": "This is an extended version of the paper which appeared in NeurIPS\n  2018 Workshop on Security in Machine Learning (SecML'18), see\n  https://github.com/secml2018/secml2018.github.io/raw/master/PASSGAN_SECML2018.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art password guessing tools, such as HashCat and John the\nRipper, enable users to check billions of passwords per second against password\nhashes. In addition to performing straightforward dictionary attacks, these\ntools can expand password dictionaries using password generation rules, such as\nconcatenation of words (e.g., \"password123456\") and leet speak (e.g.,\n\"password\" becomes \"p4s5w0rd\"). Although these rules work well in practice,\nexpanding them to model further passwords is a laborious task that requires\nspecialized expertise. To address this issue, in this paper we introduce\nPassGAN, a novel approach that replaces human-generated password rules with\ntheory-grounded machine learning algorithms. Instead of relying on manual\npassword analysis, PassGAN uses a Generative Adversarial Network (GAN) to\nautonomously learn the distribution of real passwords from actual password\nleaks, and to generate high-quality password guesses. Our experiments show that\nthis approach is very promising. When we evaluated PassGAN on two large\npassword datasets, we were able to surpass rule-based and state-of-the-art\nmachine learning password guessing tools. However, in contrast with the other\ntools, PassGAN achieved this result without any a-priori knowledge on passwords\nor common password structures. Additionally, when we combined the output of\nPassGAN with the output of HashCat, we were able to match 51%-73% more\npasswords than with HashCat alone. This is remarkable, because it shows that\nPassGAN can autonomously extract a considerable number of password properties\nthat current state-of-the art rules do not encode.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 18:42:00 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 21:03:46 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 19:51:21 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Hitaj", "Briland", ""], ["Gasti", "Paolo", ""], ["Ateniese", "Giuseppe", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1709.00483", "submitter": "Tao Sun", "authors": "Tao Sun, Hao Jiang, Lizhi Cheng, Wei Zhu", "title": "Iteratively Linearized Reweighted Alternating Direction Method of\n  Multipliers for a Class of Nonconvex Problems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2868269", "report-no": null, "categories": "cs.NA cs.CV math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider solving a class of nonconvex and nonsmooth\nproblems frequently appearing in signal processing and machine learning\nresearch. The traditional alternating direction method of multipliers\nencounters troubles in both mathematics and computations in solving the\nnonconvex and nonsmooth subproblem. In view of this, we propose a reweighted\nalternating direction method of multipliers. In this algorithm, all subproblems\nare convex and easy to solve. We also provide several guarantees for the\nconvergence and prove that the algorithm globally converges to a critical point\nof an auxiliary function with the help of the Kurdyka-{\\L}ojasiewicz property.\nSeveral numerical results are presented to demonstrate the efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 21:20:30 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 04:03:33 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 04:15:28 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 00:52:41 GMT"}, {"version": "v5", "created": "Thu, 22 Mar 2018 23:26:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sun", "Tao", ""], ["Jiang", "Hao", ""], ["Cheng", "Lizhi", ""], ["Zhu", "Wei", ""]]}, {"id": "1709.00503", "submitter": "Cameron Allen", "authors": "Cameron Allen, Kavosh Asadi, Melrose Roderick, Abdel-rahman Mohamed,\n  George Konidaris, Michael Littman", "title": "Mean Actor Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action\ncontinuous-state reinforcement learning. MAC is a policy gradient algorithm\nthat uses the agent's explicit representation of all action values to estimate\nthe gradient of the policy, rather than using only the actions that were\nactually executed. We prove that this approach reduces variance in the policy\ngradient estimate relative to traditional actor-critic methods. We show\nempirical results on two control domains and on six Atari games, where MAC is\ncompetitive with state-of-the-art policy search algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 22:53:03 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 20:20:59 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Allen", "Cameron", ""], ["Asadi", "Kavosh", ""], ["Roderick", "Melrose", ""], ["Mohamed", "Abdel-rahman", ""], ["Konidaris", "George", ""], ["Littman", "Michael", ""]]}, {"id": "1709.00515", "submitter": "Wenqing Hu", "authors": "Wenqing Hu, Chris Junchi Li", "title": "A convergence analysis of the perturbed compositional gradient flow:\n  averaging principle and normal deviations", "comments": "Final version to appear at DCDS-A", "journal-ref": "Discrete and Continuous Dynamical Systems, Series A, Vol 38, Issue\n  10, October 2018", "doi": "10.3934/dcds.2018216", "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this work a system of two stochastic differential equations\nnamed the perturbed compositional gradient flow. By introducing a separation of\nfast and slow scales of the two equations, we show that the limit of the slow\nmotion is given by an averaged ordinary differential equation. We then\ndemonstrate that the deviation of the slow motion from the averaged equation,\nafter proper rescaling, converges to a stochastic process with Gaussian inputs.\nThis indicates that the slow motion can be approximated in the weak sense by a\nstandard perturbed gradient flow or the continuous-time stochastic gradient\ndescent algorithm that solves the optimization problem for a composition of two\nfunctions. As an application, the perturbed compositional gradient flow\ncorresponds to the diffusion limit of the Stochastic Composite Gradient Descent\n(SCGD) algorithm for minimizing a composition of two expected-value functions\nin the optimization literatures. For the strongly convex case, such an analysis\nimplies that the SCGD algorithm has the same convergence time asymptotic as the\nclassical stochastic gradient descent algorithm. Thus it validates, at the\nlevel of continuous approximation, the effectiveness of using the SCGD\nalgorithm in the strongly convex case.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 01:26:13 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 21:58:38 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 03:05:15 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Hu", "Wenqing", ""], ["Li", "Chris Junchi", ""]]}, {"id": "1709.00537", "submitter": "Jineng Ren", "authors": "Jineng Ren and Jarvis Haupt", "title": "Communication-efficient Algorithm for Distributed Sparse Learning via\n  Two-way Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a communicationally and computationally efficient algorithm for\nhigh-dimensional distributed sparse learning. At each iteration, local machines\ncompute the gradient on local data and the master machine solves one shifted\n$l_1$ regularized minimization problem. The communication cost is reduced from\nconstant times of the dimension number for the state-of-the-art algorithm to\nconstant times of the sparsity number via Two-way Truncation procedure.\nTheoretically, we prove that the estimation error of the proposed algorithm\ndecreases exponentially and matches that of the centralized method under mild\nassumptions. Extensive experiments on both simulated data and real data verify\nthat the proposed algorithm is efficient and has performance comparable with\nthe centralized method on solving high-dimensional sparse learning problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 06:02:38 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 05:15:59 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ren", "Jineng", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1709.00566", "submitter": "Ting Li", "authors": "Ting Li, Bingyi Jing, Ningchen Ying, Xianshi Yu", "title": "Adaptive Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preprocessing data is an important step before any data analysis. In this\npaper, we focus on one particular aspect, namely scaling or normalization. We\nanalyze various scaling methods in common use and study their effects on\ndifferent statistical learning models. We will propose a new two-stage scaling\nmethod. First, we use some training data to fit linear regression model and\nthen scale the whole data based on the coefficients of regression. Simulations\nare conducted to illustrate the advantages of our new scaling method. Some real\ndata analysis will also be given.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 11:47:01 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Ting", ""], ["Jing", "Bingyi", ""], ["Ying", "Ningchen", ""], ["Yu", "Xianshi", ""]]}, {"id": "1709.00572", "submitter": "C\\u{a}t\\u{a}lina Cangea", "authors": "C\\u{a}t\\u{a}lina Cangea, Petar Veli\\v{c}kovi\\'c, Pietro Li\\`o", "title": "XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification", "comments": "Accepted at the IEEE ICDL-EPIROB 2017 Workshop on Computational\n  Models for Crossmodal Learning (CMCML), 4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been numerous developments towards solving\nmultimodal tasks, aiming to learn a stronger representation than through a\nsingle modality. Certain aspects of the data can be particularly useful in this\ncase - for example, correlations in the space or time domain across modalities\n- but should be wisely exploited in order to benefit from their full predictive\npotential. We propose two deep learning architectures with multimodal\ncross-connections that allow for dataflow between several feature extractors\n(XFlow). Our models derive more interpretable features and achieve better\nperformances than models which do not exchange representations, usefully\nexploiting correlations between audio and visual data, which have a different\ndimensionality and are nontrivially exchangeable. Our work improves on existing\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\nnovel method for performing cross-modality (before features are learned from\nindividual modalities) and (2) extends the previously proposed\ncross-connections which only transfer information between streams that process\ncompatible data. Illustrating some of the representations learned by the\nconnections, we analyse their contribution to the increase in discrimination\nability and reveal their compatibility with a lip-reading network intermediate\nrepresentation. We provide the research community with Digits, a new dataset\nconsisting of three data types extracted from videos of people saying the\ndigits 0-9. Results show that both cross-modal architectures outperform their\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\ndatasets, achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 12:43:59 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 21:43:42 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Cangea", "C\u0103t\u0103lina", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1709.00583", "submitter": "Chaofei Hong", "authors": "Chaofei Hong", "title": "Training Spiking Neural Networks for Cognitive Tasks: A Versatile\n  Framework Compatible to Various Temporal Codes", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version will be\n  superseded", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional modeling approaches have found limitations in matching the\nincreasingly detailed neural network structures and dynamics recorded in\nexperiments to the diverse brain functionalities. On another approach, studies\nhave demonstrated to train spiking neural networks for simple functions using\nsupervised learning. Here, we introduce a modified SpikeProp learning\nalgorithm, which achieved better learning stability in different activity\nstates. In addition, we show biological realistic features such as lateral\nconnections and sparse activities can be included in the network. We\ndemonstrate the versatility of this framework by implementing three well-known\ntemporal codes for different types of cognitive tasks, which are MNIST digits\nrecognition, spatial coordinate transformation, and motor sequence generation.\nMoreover, we find several characteristic features have evolved alongside the\ntask training, such as selective activity, excitatory-inhibitory balance, and\nweak pair-wise correlation. The coincidence between the self-evolved and\nexperimentally observed features indicates their importance on the brain\nfunctionality. Our results suggest a unified setting in which diverse cognitive\ncomputations and mechanisms can be studied.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 13:59:39 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hong", "Chaofei", ""]]}, {"id": "1709.00614", "submitter": "Xiao Fu", "authors": "Xiao Fu and Kejun Huang and Nicholas D. Sidiropoulos", "title": "On Identifiability of Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2789405", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose a new identification criterion that guarantees the\nrecovery of the low-rank latent factors in the nonnegative matrix factorization\n(NMF) model, under mild conditions. Specifically, using the proposed criterion,\nit suffices to identify the latent factors if the rows of one factor are\n\\emph{sufficiently scattered} over the nonnegative orthant, while no structural\nassumption is imposed on the other factor except being full-rank. This is by\nfar the mildest condition under which the latent factors are provably\nidentifiable from the NMF model.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 18:40:37 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1709.00640", "submitter": "Hao Zhou", "authors": "Hao Henry Zhou, Yilin Zhang, Vamsi K. Ithapu, Sterling C. Johnson,\n  Grace Wahba, Vikas Singh", "title": "When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests,\n  $\\ell_2$-consistency and Neuroscience Applications", "comments": "34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in biomedical and health sciences involve small sample sizes due\nto logistic or financial constraints. Often, identifying weak (but\nscientifically interesting) associations between a set of predictors and a\nresponse necessitates pooling datasets from multiple diverse labs or groups.\nWhile there is a rich literature in statistical machine learning to address\ndistributional shifts and inference in multi-site datasets, it is less clear\n${\\it when}$ such pooling is guaranteed to help (and when it does not) --\nindependent of the inference algorithms we use. In this paper, we present a\nhypothesis test to answer this question, both for classical and high\ndimensional linear regression. We precisely identify regimes where pooling\ndatasets across multiple sites is sensible, and how such policy decisions can\nbe made via simple checks executable on each site before any data transfer ever\nhappens. With a focus on Alzheimer's disease studies, we present empirical\nresults showing that in regimes suggested by our analysis, pooling a local\ndataset with data from an international study improves power.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 22:17:31 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhou", "Hao Henry", ""], ["Zhang", "Yilin", ""], ["Ithapu", "Vamsi K.", ""], ["Johnson", "Sterling C.", ""], ["Wahba", "Grace", ""], ["Singh", "Vikas", ""]]}, {"id": "1709.00668", "submitter": "Evangelos Papalexakis", "authors": "Ekta Gujral, Ravdeep Pasricha, Evangelos E. Papalexakis", "title": "SamBaTen: Sampling-based Batch Incremental Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decompositions are invaluable tools in analyzing multimodal datasets.\nIn many real-world scenarios, such datasets are far from being static, to the\ncontrary they tend to grow over time. For instance, in an online social network\nsetting, as we observe new interactions over time, our dataset gets updated in\nits \"time\" mode. How can we maintain a valid and accurate tensor decomposition\nof such a dynamically evolving multimodal dataset, without having to re-compute\nthe entire decomposition after every single update? In this paper we introduce\nSaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,\nwhich incrementally maintains the decomposition given new updates to the tensor\ndataset. SaMbaTen is able to scale to datasets that the state-of-the-art in\nincremental tensor decomposition is unable to operate on, due to its ability to\neffectively summarize the existing tensor and the incoming updates, and perform\nall computations in the reduced summary space. We extensively evaluate SaMbaTen\nusing synthetic and real datasets. Indicatively, SaMbaTen achieves comparable\naccuracy to state-of-the-art incremental and non-incremental techniques, while\nbeing 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and\ndense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where\nstate-of-the-art incremental approaches were not able to operate.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 06:05:07 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 20:42:33 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Gujral", "Ekta", ""], ["Pasricha", "Ravdeep", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1709.00776", "submitter": "Shohei Shimizu", "authors": "Patrick Bl\\\"obaum and Shohei Shimizu", "title": "Estimation of interventional effects of features on prediction", "comments": "To appear in Proc. IEEE International Workshop on Machine Learning\n  for Signal Processing (MLSP2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretability of prediction mechanisms with respect to the underlying\nprediction problem is often unclear. While several studies have focused on\ndeveloping prediction models with meaningful parameters, the causal\nrelationships between the predictors and the actual prediction have not been\nconsidered. Here, we connect the underlying causal structure of a data\ngeneration process and the causal structure of a prediction mechanism. To\nachieve this, we propose a framework that identifies the feature with the\ngreatest causal influence on the prediction and estimates the necessary causal\nintervention of a feature such that a desired prediction is obtained. The\ngeneral concept of the framework has no restrictions regarding data linearity;\nhowever, we focus on an implementation for linear data here. The framework\napplicability is evaluated using artificial data and demonstrated using\nreal-world data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 23:23:40 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Bl\u00f6baum", "Patrick", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1709.00843", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Extending the scope of the small-ball method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The small-ball method was introduced as a way of obtaining a high\nprobability, isomorphic lower bound on the quadratic empirical process, under\nweak assumptions on the indexing class. The key assumption was that class\nmembers satisfy a uniform small-ball estimate: that $Pr(|f| \\geq\n\\kappa\\|f\\|_{L_2}) \\geq \\delta$ for given constants $\\kappa$ and $\\delta$.\n  Here we extend the small-ball method and obtain a high probability,\nalmost-isometric (rather than isomorphic) lower bound on the quadratic\nempirical process. The scope of the result is considerably wider than the\nsmall-ball method: there is no need for class members to satisfy a uniform\nsmall-ball condition, and moreover, motivated by the notion of tournament\nlearning procedures, the result is stable under a `majority vote'.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 07:38:44 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 06:52:18 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1709.00944", "submitter": "Jen-Cheng Hou", "authors": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang,\n  Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional\n  Neural Network", "comments": "This paper is the same as arXiv:1703.10893v2. Apologies for the\n  inconvenience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus on addressing audio information only. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. In the proposed AVDCNN SE model, audio\nand visual data are first processed using individual CNNs, and then, fused into\na joint network to generate enhanced speech at the output layer. The AVDCNN\nmodel is trained in an end-to-end manner, and parameters are jointly learned\nthrough back-propagation. We evaluate enhanced speech using five objective\ncriteria. Results show that the AVDCNN yields notably better performance,\ncompared with an audio-only CNN-based SE model and two conventional SE\napproaches, confirming the effectiveness of integrating visual information into\nthe SE process.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:17:53 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 16:06:28 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Hou", "Jen-Cheng", ""], ["Wang", "Syu-Siang", ""], ["Lai", "Ying-Hui", ""], ["Tsao", "Yu", ""], ["Chang", "Hsiu-Wen", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1709.01006", "submitter": "Josip Djolonga", "authors": "Josip Djolonga, Andreas Krause", "title": "Learning Implicit Generative Models Using Differentiable Graph Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in the problem of learning rich\nimplicit models - those from which we can sample, but can not evaluate their\ndensity. These models apply some parametric function, such as a deep network,\nto a base measure, and are learned end-to-end using stochastic optimization.\nOne strategy of devising a loss function is through the statistics of two\nsample tests - if we can fool a statistical test, the learned distribution\nshould be a good model of the true data. However, not all tests can easily fit\ninto this framework, as they might not be differentiable with respect to the\ndata points, and hence with respect to the parameters of the implicit model.\nMotivated by this problem, in this paper we show how two such classical tests,\nthe Friedman-Rafsky and k-nearest neighbour tests, can be effectively smoothed\nusing ideas from undirected graphical models - the matrix tree theorem and\ncardinality potentials. Moreover, as we show experimentally, smoothing can\nsignificantly increase the power of the test, which might of of independent\ninterest. Finally, we apply our method to learn implicit models.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 15:34:59 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Djolonga", "Josip", ""], ["Krause", "Andreas", ""]]}, {"id": "1709.01062", "submitter": "Mark Tygert", "authors": "Cinna Wu, Mark Tygert, and Yann LeCun", "title": "A hierarchical loss and its problems when classifying non-hierarchically", "comments": "19 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failing to distinguish between a sheepdog and a skyscraper should be worse\nand penalized more than failing to distinguish between a sheepdog and a poodle;\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\nclassification/recognition via neural networks seldom leverage a-priori\ninformation, such as a sheepdog being more similar to a poodle than to a\nskyscraper. We define a metric that, inter alia, can penalize failure to\ndistinguish between a sheepdog and a skyscraper more than failure to\ndistinguish between a sheepdog and a poodle. Unlike previously employed\npossibilities, this metric is based on an ultrametric tree associated with any\ngiven tree organization into a semantically meaningful hierarchy of a\nclassifier's classes. An ultrametric tree is a tree with a so-called\nultrametric distance metric such that all leaves are at the same distance from\nthe root. Unfortunately, extensive numerical experiments indicate that the\nstandard practice of training neural networks via stochastic gradient descent\nwith random starting points often drives down the hierarchical loss nearly as\nmuch when minimizing the standard cross-entropy loss as when trying to minimize\nthe hierarchical loss directly. Thus, this hierarchical loss is unreliable as\nan objective for plain, randomly started stochastic gradient descent to\nminimize; the main value of the hierarchical loss may be merely as a meaningful\nmetric of success of a classifier.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 23:46:59 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:38:32 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wu", "Cinna", ""], ["Tygert", "Mark", ""], ["LeCun", "Yann", ""]]}, {"id": "1709.01112", "submitter": "Steffen Limmer", "authors": "Steffen Limmer, Slawomir Stanczak", "title": "Optimal deep neural networks for sparse recovery via Laplace techniques", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Laplace techniques for designing a neural network, with\nthe goal of estimating simplex-constraint sparse vectors from compressed\nmeasurements. To this end, we recast the problem of MMSE estimation (w.r.t. a\npre-defined uniform input distribution) as the problem of computing the\ncentroid of some polytope that results from the intersection of the simplex and\nan affine subspace determined by the measurements. Owing to the specific\nstructure, it is shown that the centroid can be computed analytically by\nextending a recent result that facilitates the volume computation of polytopes\nvia Laplace transformations. A main insight of this paper is that the desired\nvolume and centroid computations can be performed by a classical deep neural\nnetwork comprising threshold functions, rectified linear (ReLU) and rectified\npolynomial (ReP) activation functions. The proposed construction of a deep\nneural network for sparse recovery is completely analytic so that\ntime-consuming training procedures are not necessary. Furthermore, we show that\nthe number of layers in our construction is equal to the number of measurements\nwhich might enable novel low-latency sparse recovery algorithms for a larger\nclass of signals than that assumed in this paper. To assess the applicability\nof the proposed uniform input distribution, we showcase the recovery\nperformance on samples that are soft-classification vectors generated by two\nstandard datasets. As both volume and centroid computation are known to be\ncomputationally hard, the network width grows exponentially in the worst-case.\nIt can be, however, decreased by inducing sparse connectivity in the neural\nnetwork via a well-suited basis of the affine subspace. Finally, the presented\nanalytical construction may serve as a viable initialization to be further\noptimized and trained using particular input datasets at hand.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:46:19 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 16:46:26 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Limmer", "Steffen", ""], ["Stanczak", "Slawomir", ""]]}, {"id": "1709.01147", "submitter": "Evangelos Papalexakis", "authors": "Ishmam Zabir, Evangelos E. Papalexakis", "title": "Balancing Interpretability and Predictive Accuracy for Unsupervised\n  Tensor Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PARAFAC tensor decomposition has enjoyed an increasing success in\nexploratory multi-aspect data mining scenarios. A major challenge remains the\nestimation of the number of latent factors (i.e., the rank) of the\ndecomposition, which yields high-quality, interpretable results. Previously, we\nhave proposed an automated tensor mining method which leverages a well-known\nquality heuristic from the field of Chemometrics, the Core Consistency\nDiagnostic (CORCONDIA), in order to automatically determine the rank for the\nPARAFAC decomposition. In this work we set out to explore the trade-off between\n1) the interpretability/quality of the results (as expressed by CORCONDIA), and\n2) the predictive accuracy of the results, in order to further improve the rank\nestimation quality. Our preliminary results indicate that striking a good\nbalance in that trade-off benefits rank estimation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 20:34:43 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Zabir", "Ishmam", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1709.01177", "submitter": "Antonio Sutera", "authors": "Antonio Sutera, C\\'elia Ch\\^atel, Gilles Louppe, Louis Wehenkel,\n  Pierre Geurts", "title": "Random Subspace with Trees for Feature Selection Under Memory\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with datasets of very high dimension is a major challenge in machine\nlearning. In this paper, we consider the problem of feature selection in\napplications where the memory is not large enough to contain all features. In\nthis setting, we propose a novel tree-based feature selection approach that\nbuilds a sequence of randomized trees on small subsamples of variables mixing\nboth variables already identified as relevant by previous models and variables\nrandomly selected among the other variables. As our main contribution, we\nprovide an in-depth theoretical analysis of this method in infinite sample\nsetting. In particular, we study its soundness with respect to common\ndefinitions of feature relevance and its convergence speed under various\nvariable dependance scenarios. We also provide some preliminary empirical\nresults highlighting the potential of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 21:56:25 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 08:50:48 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Sutera", "Antonio", ""], ["Ch\u00e2tel", "C\u00e9lia", ""], ["Louppe", "Gilles", ""], ["Wehenkel", "Louis", ""], ["Geurts", "Pierre", ""]]}, {"id": "1709.01179", "submitter": "Changyou Chen", "authors": "Changyou Chen and Chunyuan Li and Liqun Chen and Wenlin Wang and\n  Yunchen Pu and Lawrence Carin", "title": "Continuous-Time Flows for Efficient Inference and Density Estimation", "comments": "ICML 2018 (fixed a reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two fundamental problems in unsupervised learning are efficient inference for\nlatent-variable models and robust density estimation based on large amounts of\nunlabeled data. Algorithms for the two tasks, such as normalizing flows and\ngenerative adversarial networks (GANs), are often developed independently. In\nthis paper, we propose the concept of {\\em continuous-time flows} (CTFs), a\nfamily of diffusion-based methods that are able to asymptotically approach a\ntarget distribution. Distinct from normalizing flows and GANs, CTFs can be\nadopted to achieve the above two goals in one framework, with theoretical\nguarantees. Our framework includes distilling knowledge from a CTF for\nefficient inference, and learning an explicit energy-based distribution with\nCTFs for density estimation. Both tasks rely on a new technique for\ndistribution matching within amortized learning. Experiments on various tasks\ndemonstrate promising performance of the proposed CTF framework, compared to\nrelated techniques.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 22:00:16 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 16:48:04 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 15:25:49 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 15:11:22 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Chen", "Changyou", ""], ["Li", "Chunyuan", ""], ["Chen", "Liqun", ""], ["Wang", "Wenlin", ""], ["Pu", "Yunchen", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.01180", "submitter": "Changyou Chen", "authors": "Changyou Chen and Wenlin Wang and Yizhe Zhang and Qinliang Su and\n  Lawrence Carin", "title": "A Convergence Analysis for A Class of Practical Variance-Reduction\n  Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov Chain Monte Carlo (SG-MCMC) has been developed as\na flexible family of scalable Bayesian sampling algorithms. However, there has\nbeen little theoretical analysis of the impact of minibatch size to the\nalgorithm's convergence rate. In this paper, we prove that under a limited\ncomputational budget/time, a larger minibatch size leads to a faster decrease\nof the mean squared error bound (thus the fastest one corresponds to using full\ngradients), which motivates the necessity of variance reduction in SG-MCMC.\nConsequently, by borrowing ideas from stochastic optimization, we propose a\npractical variance-reduction technique for SG-MCMC, that is efficient in both\ncomputation and storage. We develop theory to prove that our algorithm induces\na faster convergence rate than standard SG-MCMC. A number of large-scale\nexperiments, ranging from Bayesian learning of logistic regression to deep\nneural networks, validate the theory and demonstrate the superiority of the\nproposed variance-reduction SG-MCMC framework.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 22:11:24 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Chen", "Changyou", ""], ["Wang", "Wenlin", ""], ["Zhang", "Yizhe", ""], ["Su", "Qinliang", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.01215", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo\n  Henao, Lawrence Carin", "title": "ALICE: Towards Understanding Adversarial Learning for Joint Distribution\n  Matching", "comments": "NIPS 2017 (22 pages); short version (9 pages):\n  http://people.duke.edu/~cl319/doc/papers/nips_2017_alice.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the non-identifiability issues associated with bidirectional\nadversarial training for joint distribution matching. Within a framework of\nconditional entropy, we propose both adversarial and non-adversarial approaches\nto learn desirable matched joint distributions for unsupervised and supervised\ntasks. We unify a broad family of adversarial models as joint distribution\nmatching problems. Our approach stabilizes learning of unsupervised\nbidirectional adversarial learning methods. Further, we introduce an extension\nfor semi-supervised learning tasks. Theoretical results are validated in\nsynthetic data and real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 02:18:06 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 03:58:52 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Li", "Chunyuan", ""], ["Liu", "Hao", ""], ["Chen", "Changyou", ""], ["Pu", "Yunchen", ""], ["Chen", "Liqun", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.01231", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Feng Liang, Nebojsa Jojic, Shuicheng Yan, Jiashi Feng,\n  Thomas S. Huang", "title": "Discriminative Similarity for Clustering and Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-based clustering and semi-supervised learning methods separate the\ndata into clusters or classes according to the pairwise similarity between the\ndata, and the pairwise similarity is crucial for their performance. In this\npaper, we propose a novel discriminative similarity learning framework which\nlearns discriminative similarity for either data clustering or semi-supervised\nlearning. The proposed framework learns classifier from each hypothetical\nlabeling, and searches for the optimal labeling by minimizing the\ngeneralization error of the learned classifiers associated with the\nhypothetical labeling. Kernel classifier is employed in our framework. By\ngeneralization analysis via Rademacher complexity, the generalization error\nbound for the kernel classifier learned from hypothetical labeling is expressed\nas the sum of pairwise similarity between the data from different classes,\nparameterized by the weights of the kernel classifier. Such pairwise similarity\nserves as the discriminative similarity for the purpose of clustering and\nsemi-supervised learning, and discriminative similarity with similar form can\nalso be induced by the integrated squared error bound for kernel density\nclassification. Based on the discriminative similarity induced by the kernel\nclassifier, we propose new clustering and semi-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 04:10:44 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Yang", "Yingzhen", ""], ["Liang", "Feng", ""], ["Jojic", "Nebojsa", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1709.01233", "submitter": "Eric Bridgeford", "authors": "Joshua T. Vogelstein, Eric Bridgeford, Minh Tang, Da Zheng,\n  Christopher Douville, Randal Burns, Mauro Maggioni", "title": "Supervised Dimensionality Reduction for Big Data", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve key biomedical problems, experimentalists now routinely measure\nmillions or billions of features (dimensions) per sample, with the hope that\ndata science techniques will be able to build accurate data-driven inferences.\nBecause sample sizes are typically orders of magnitude smaller than the\ndimensionality of these data, valid inferences require finding a\nlow-dimensional representation that preserves the discriminating information\n(e.g., whether the individual suffers from a particular disease). There is a\nlack of interpretable supervised dimensionality reduction methods that scale to\nmillions of dimensions with strong statistical theoretical guarantees.We\nintroduce an approach, XOX, to extending principal components analysis by\nincorporating class-conditional moment estimates into the low-dimensional\nprojection. The simplest ver-sion, \"Linear Optimal Low-rank\" projection (LOL),\nincorporates the class-conditional means. We prove, and substantiate with both\nsynthetic and real data benchmarks, that LOL and its generalizations in the XOX\nframework lead to improved data representations for subsequent classification,\nwhile maintaining computational efficiency and scalability. Using multiple\nbrain imaging datasets consisting of >150 million features, and several\ngenomics datasets with>500,000 features, LOL outperforms other scalable linear\ndimensionality reduction techniques in terms of accuracy, while only requiring\na few minutes on a standard desktop computer.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 04:19:38 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 05:32:45 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 02:32:39 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 06:07:46 GMT"}, {"version": "v5", "created": "Tue, 27 Feb 2018 05:28:42 GMT"}, {"version": "v6", "created": "Wed, 21 Nov 2018 10:17:19 GMT"}, {"version": "v7", "created": "Tue, 7 Jul 2020 09:44:03 GMT"}, {"version": "v8", "created": "Mon, 13 Jul 2020 16:01:37 GMT"}, {"version": "v9", "created": "Sat, 23 Jan 2021 07:37:55 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Vogelstein", "Joshua T.", ""], ["Bridgeford", "Eric", ""], ["Tang", "Minh", ""], ["Zheng", "Da", ""], ["Douville", "Christopher", ""], ["Burns", "Randal", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1709.01249", "submitter": "Pan Li", "authors": "Pan Li, Olgica Milenkovic", "title": "Inhomogeneous Hypergraph Clustering with Applications", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph partitioning is an important problem in machine learning, computer\nvision and network analytics. A widely used method for hypergraph partitioning\nrelies on minimizing a normalized sum of the costs of partitioning hyperedges\nacross clusters. Algorithmic solutions based on this approach assume that\ndifferent partitions of a hyperedge incur the same cost. However, this\nassumption fails to leverage the fact that different subsets of vertices within\nthe same hyperedge may have different structural importance. We hence propose a\nnew hypergraph clustering technique, termed inhomogeneous hypergraph\npartitioning, which assigns different costs to different hyperedge cuts. We\nprove that inhomogeneous partitioning produces a quadratic approximation to the\noptimal solution if the inhomogeneous costs satisfy submodularity constraints.\nMoreover, we demonstrate that inhomogenous partitioning offers significant\nperformance improvements in applications such as structure learning of\nrankings, subspace segmentation and motif clustering.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 06:11:26 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 02:49:07 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 19:39:25 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 18:48:40 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1709.01298", "submitter": "Felipe Tobar", "authors": "Gabriel Parra and Felipe Tobar", "title": "Spectral Mixture Kernels for Multi-Output Gaussian Processes", "comments": "To appear in Advances in Neural Information Processing Systems 31\n  (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early approaches to multiple-output Gaussian processes (MOGPs) relied on\nlinear combinations of independent, latent, single-output Gaussian processes\n(GPs). This resulted in cross-covariance functions with limited parametric\ninterpretation, thus conflicting with the ability of single-output GPs to\nunderstand lengthscales, frequencies and magnitudes to name a few. On the\ncontrary, current approaches to MOGP are able to better interpret the\nrelationship between different channels by directly modelling the\ncross-covariances as a spectral mixture kernel with a phase shift. We extend\nthis rationale and propose a parametric family of complex-valued cross-spectral\ndensities and then build on Cram\\'er's Theorem (the multivariate version of\nBochner's Theorem) to provide a principled approach to design multivariate\ncovariance functions. The so-constructed kernels are able to model delays among\nchannels in addition to phase differences and are thus more expressive than\nprevious methods, while also providing full parametric interpretation of the\nrelationship across channels. The proposed method is first validated on\nsynthetic data and then compared to existing MOGP methods on two real-world\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:20:19 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 22:44:09 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Parra", "Gabriel", ""], ["Tobar", "Felipe", ""]]}, {"id": "1709.01402", "submitter": "Alexander Jung", "authors": "Alexandru Mara and Alexander Jung", "title": "Recovery Conditions and Sampling Strategies for Network Lasso", "comments": "nominated as student paper award finalist at Asilomar 2017. arXiv\n  admin note: substantial text overlap with arXiv:1704.02107", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network Lasso is a recently proposed convex optimization method for\nmachine learning from massive network structured datasets, i.e., big data over\nnetworks. It is a variant of the well-known least absolute shrinkage and\nselection operator (Lasso), which is underlying many methods in learning and\nsignal processing involving sparse models. Highly scalable implementations of\nthe network Lasso can be obtained by state-of-the art proximal methods, e.g.,\nthe alternating direction method of multipliers (ADMM). By generalizing the\nconcept of the compatibility condition put forward by van de Geer and Buehlmann\nas a powerful tool for the analysis of plain Lasso, we derive a sufficient\ncondition, i.e., the network compatibility condition, on the underlying network\ntopology such that network Lasso accurately learns a clustered underlying graph\nsignal. This network compatibility condition relates the location of the\nsampled nodes with the clustering structure of the network. In particular, the\nNCC informs the choice of which nodes to sample, or in machine learning terms,\nwhich data points provide most information if labeled.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 13:05:30 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Mara", "Alexandru", ""], ["Jung", "Alexander", ""]]}, {"id": "1709.01412", "submitter": "Thomas Epelbaum", "authors": "Thomas Epelbaum", "title": "Deep learning: Technical introduction", "comments": "The content of this note can be found on GitHub:\n  https://github.com/tomepel/Technical_Book_DL . If you detect any typo, error,\n  or feel that I forgot to cite an important source, don't hesitate to email:\n  thomas.epelbaum@mediamobile.com. v2: minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents in a technical though hopefully pedagogical way the three\nmost common forms of neural network architectures: Feedforward, Convolutional\nand Recurrent. For each network, their fundamental building blocks are\ndetailed. The forward pass and the update rules for the backpropagation\nalgorithm are then derived in full.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 14:27:08 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 11:39:06 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Epelbaum", "Thomas", ""]]}, {"id": "1709.01427", "submitter": "Alice Schoenauer Sebag", "authors": "Alice Schoenauer-Sebag, Marc Schoenauer and Mich\\`ele Sebag", "title": "Stochastic Gradient Descent: Going As Fast As Possible But Not Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applied to training deep neural networks, stochastic gradient descent\n(SGD) often incurs steady progression phases, interrupted by catastrophic\nepisodes in which loss and gradient norm explode. A possible mitigation of such\nevents is to slow down the learning process. This paper presents a novel\napproach to control the SGD learning rate, that uses two statistical tests. The\nfirst one, aimed at fast learning, compares the momentum of the normalized\ngradient vectors to that of random unit vectors and accordingly gracefully\nincreases or decreases the learning rate. The second one is a change point\ndetection test, aimed at the detection of catastrophic learning episodes; upon\nits triggering the learning rate is instantly halved. Both abilities of\nspeeding up and slowing down the learning rate allows the proposed approach,\ncalled SALeRA, to learn as fast as possible but not faster. Experiments on\nstandard benchmarks show that SALeRA performs well in practice, and compares\nfavorably to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 14:50:55 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Schoenauer-Sebag", "Alice", ""], ["Schoenauer", "Marc", ""], ["Sebag", "Mich\u00e8le", ""]]}, {"id": "1709.01439", "submitter": "Gustavo A Valencia-Zapata", "authors": "Gustavo A Valencia-Zapata, Daniel Mejia, Gerhard Klimeck, Michael\n  Zentner, and Okan Ersoy", "title": "A Statistical Approach to Increase Classification Accuracy in Supervised\n  Learning Algorithms", "comments": "7 pages, 9 figures, IPSI BgD Transactions", "journal-ref": "PSI BGD TRANSACTIONS ON INTERNET RESEARCH 13.2 (2017)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic mixture models have been widely used for different machine\nlearning and pattern recognition tasks such as clustering, dimensionality\nreduction, and classification. In this paper, we focus on trying to solve the\nmost common challenges related to supervised learning algorithms by using\nmixture probability distribution functions. With this modeling strategy, we\nidentify sub-labels and generate synthetic data in order to reach better\nclassification accuracy. It means we focus on increasing the training data\nsynthetically to increase the classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:05:16 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Valencia-Zapata", "Gustavo A", ""], ["Mejia", "Daniel", ""], ["Klimeck", "Gerhard", ""], ["Zentner", "Michael", ""], ["Ersoy", "Okan", ""]]}, {"id": "1709.01447", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Conditional independence testing based on a nearest-neighbor estimator\n  of conditional mutual information", "comments": "17 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence testing is a fundamental problem underlying causal\ndiscovery and a particularly challenging task in the presence of nonlinear and\nhigh-dimensional dependencies. Here a fully non-parametric test for continuous\ndata based on conditional mutual information combined with a local permutation\nscheme is presented. Through a nearest neighbor approach, the test efficiently\nadapts also to non-smooth distributions due to strongly nonlinear dependencies.\nNumerical experiments demonstrate that the test reliably simulates the null\ndistribution even for small sample sizes and with high-dimensional conditioning\nsets. The test is better calibrated than kernel-based tests utilizing an\nanalytical approximation of the null distribution, especially for non-smooth\ndensities, and reaches the same or higher power levels. Combining the local\npermutation scheme with the kernel tests leads to better calibration, but\nsuffers in power. For smaller sample sizes and lower dimensions, the test is\nfaster than random fourier feature-based kernel tests if the permutation scheme\nis (embarrassingly) parallelized, but the runtime increases more sharply with\nsample size and dimensionality. Thus, more theoretical research to analytically\napproximate the null distribution and speed up the estimation for larger sample\nsizes is desirable.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:21:25 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "1709.01471", "submitter": "Edward Raff", "authors": "Edward Raff, Jared Sylvester, Charles Nicholas", "title": "Learning the PE Header, Malware Detection with Minimal Domain Knowledge", "comments": null, "journal-ref": "Proceedings of the 10th ACM Workshop on Artificial Intelligence\n  and Security (2017) 121-132", "doi": "10.1145/3128572.3140442", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts have been made to use various forms of domain knowledge in\nmalware detection. Currently there exist two common approaches to malware\ndetection without domain knowledge, namely byte n-grams and strings. In this\nwork we explore the feasibility of applying neural networks to malware\ndetection and feature learning. We do this by restricting ourselves to a\nminimal amount of domain knowledge in order to extract a portion of the\nPortable Executable (PE) header. By doing this we show that neural networks can\nlearn from raw bytes without explicit feature construction, and perform even\nbetter than a domain knowledge approach that parses the PE header into explicit\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 16:08:56 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 05:18:59 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Nicholas", "Charles", ""]]}, {"id": "1709.01509", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Linking Generative Adversarial Learning and Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we point out a basic link between generative adversarial (GA)\ntraining and binary classification -- any powerful discriminator essentially\ncomputes an (f-)divergence between real and generated samples. The result,\nrepeatedly re-derived in decision theory, has implications for GA Networks\n(GANs), providing an alternative perspective on training f-GANs by designing\nthe discriminator loss function.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 17:55:59 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1709.01584", "submitter": "Jill-J\\^enn Vie", "authors": "Jill-J\\^enn Vie, Florian Yger, Ryan Lahfa, Basile Clement, K\\'evin\n  Cocchi, Thomas Chalumeau and Hisashi Kashima", "title": "Using Posters to Recommend Anime and Mangas in a Cold-Start Scenario", "comments": "6 pages, 3 figures, 1 table, accepted at the MANPU 2017 workshop,\n  co-located with ICDAR 2017 in Kyoto on November 10, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item cold-start is a classical issue in recommender systems that affects\nanime and manga recommendations as well. This problem can be framed as follows:\nhow to predict whether a user will like a manga that received few ratings from\nthe community? Content-based techniques can alleviate this issue but require\nextra information, that is usually expensive to gather. In this paper, we use a\ndeep learning technique, Illustration2Vec, to easily extract tag information\nfrom the manga and anime posters (e.g., sword, or ponytail). We propose BALSE\n(Blended Alternate Least Squares with Explanation), a new model for\ncollaborative filtering, that benefits from this extra information to recommend\nmangas. We show, using real data from an online manga recommender system called\nMangaki, that our model improves substantially the quality of recommendations,\nespecially for less-known manga, and is able to provide an interpretation of\nthe taste of the users.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 16:19:36 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 06:48:31 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Vie", "Jill-J\u00eann", ""], ["Yger", "Florian", ""], ["Lahfa", "Ryan", ""], ["Clement", "Basile", ""], ["Cocchi", "K\u00e9vin", ""], ["Chalumeau", "Thomas", ""], ["Kashima", "Hisashi", ""]]}, {"id": "1709.01604", "submitter": "Samuel Yeom", "authors": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha", "title": "Privacy Risk in Machine Learning: Analyzing the Connection to\n  Overfitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms, when applied to sensitive data, pose a distinct\nthreat to privacy. A growing body of prior work demonstrates that models\nproduced by these algorithms may leak specific private information in the\ntraining data to an attacker, either through the models' structure or their\nobservable behavior. However, the underlying cause of this privacy risk is not\nwell understood beyond a handful of anecdotal accounts that suggest overfitting\nand influence might play a role.\n  This paper examines the effect that overfitting and influence have on the\nability of an attacker to learn information about the training data from\nmachine learning models, either through training set membership inference or\nattribute inference attacks. Using both formal and empirical analyses, we\nillustrate a clear relationship between these factors and the privacy risk that\narises in several popular machine learning algorithms. We find that overfitting\nis sufficient to allow an attacker to perform membership inference and, when\nthe target attribute meets certain conditions about its influence, attribute\ninference attacks. Interestingly, our formal analysis also shows that\noverfitting is not necessary for these attacks and begins to shed light on what\nother factors may be in play. Finally, we explore the connection between\nmembership inference and attribute inference, showing that there are deep\nconnections between the two that lead to effective new attacks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:56:24 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 21:39:21 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 03:38:13 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 23:04:31 GMT"}, {"version": "v5", "created": "Fri, 4 May 2018 22:43:43 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Yeom", "Samuel", ""], ["Giacomelli", "Irene", ""], ["Fredrikson", "Matt", ""], ["Jha", "Somesh", ""]]}, {"id": "1709.01643", "submitter": "Alexander Ratner", "authors": "Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared\n  Dunnmon, Christopher R\\'e", "title": "Learning to Compose Domain-Specific Transformations for Data\n  Augmentation", "comments": "To appear at Neural Information Processing Systems (NIPS) 2017", "journal-ref": "Advances in Neural Information Processing Systems 30, 2017,\n  3236--3246", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a ubiquitous technique for increasing the size of\nlabeled training sets by leveraging task-specific data transformations that\npreserve class labels. While it is often easy for domain experts to specify\nindividual transformations, constructing and tuning the more sophisticated\ncompositions typically needed to achieve state-of-the-art results is a\ntime-consuming manual task in practice. We propose a method for automating this\nprocess by learning a generative sequence model over user-specified\ntransformation functions using a generative adversarial approach. Our method\ncan make use of arbitrary, non-deterministic transformation functions, is\nrobust to misspecified user input, and is trained on unlabeled data. The\nlearned transformation model can then be used to perform data augmentation for\nany end discriminative model. In our experiments, we show the efficacy of our\napproach on both image and text datasets, achieving improvements of 4.0\naccuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task,\nand 3.4 accuracy points when using domain-specific transformation operations on\na medical imaging dataset as compared to standard heuristic augmentation\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 01:17:31 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 18:09:15 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 04:27:53 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Ratner", "Alexander J.", ""], ["Ehrenberg", "Henry R.", ""], ["Hussain", "Zeshan", ""], ["Dunnmon", "Jared", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1709.01648", "submitter": "Zhengping Che", "authors": "Zhengping Che, Yu Cheng, Shuangfei Zhai, Zhaonan Sun, Yan Liu", "title": "Boosting Deep Learning Risk Prediction with Generative Adversarial\n  Networks for Electronic Health Records", "comments": "To appear in ICDM 2017. This is the full version of paper with 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of Electronic Health Records (EHRs), as well as the\naccompanied opportunities in Data-Driven Healthcare (DDH), has been attracting\nwidespread interests and attentions. Recent progress in the design and\napplications of deep learning methods has shown promising results and is\nforcing massive changes in healthcare academia and industry, but most of these\nmethods rely on massive labeled data. In this work, we propose a general deep\nlearning framework which is able to boost risk prediction performance with\nlimited EHR data. Our model takes a modified generative adversarial network\nnamely ehrGAN, which can provide plausible labeled EHR data by mimicking real\npatient records, to augment the training dataset in a semi-supervised learning\nmanner. We use this generative model together with a convolutional neural\nnetwork (CNN) based prediction model to improve the onset prediction\nperformance. Experiments on two real healthcare datasets demonstrate that our\nproposed framework produces realistic data samples and achieves significant\nimprovements on classification tasks with the generated data over several\nstat-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 01:36:12 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Che", "Zhengping", ""], ["Cheng", "Yu", ""], ["Zhai", "Shuangfei", ""], ["Sun", "Zhaonan", ""], ["Liu", "Yan", ""]]}, {"id": "1709.01662", "submitter": "Jun Wang", "authors": "Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang and Pan Zhang", "title": "Unsupervised Generative Modeling Using Matrix Product States", "comments": "11 pages, 12 figures (not including the TNs) GitHub Page:\n  https://congzlwag.github.io/UnsupGenModbyMPS/", "journal-ref": "Phys. Rev. X 8, 031012 (2018)", "doi": "10.1103/PhysRevX.8.031012", "report-no": null, "categories": "cond-mat.stat-mech cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling, which learns joint probability distribution from data\nand generates samples according to it, is an important task in machine learning\nand artificial intelligence. Inspired by probabilistic interpretation of\nquantum physics, we propose a generative model using matrix product states,\nwhich is a tensor network originally proposed for describing (particularly\none-dimensional) entangled quantum states. Our model enjoys efficient learning\nanalogous to the density matrix renormalization group method, which allows\ndynamically adjusting dimensions of the tensors and offers an efficient direct\nsampling approach for generative tasks. We apply our method to generative\nmodeling of several standard datasets including the Bars and Stripes, random\nbinary patterns and the MNIST handwritten digits to illustrate the abilities,\nfeatures and drawbacks of our model over popular generative models such as\nHopfield model, Boltzmann machines and generative adversarial networks. Our\nwork sheds light on many interesting directions of future exploration on the\ndevelopment of quantum-inspired algorithms for unsupervised machine learning,\nwhich are promisingly possible to be realized on quantum devices.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 03:18:33 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 08:45:44 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 03:03:42 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Han", "Zhao-Yu", ""], ["Wang", "Jun", ""], ["Fan", "Heng", ""], ["Wang", "Lei", ""], ["Zhang", "Pan", ""]]}, {"id": "1709.01674", "submitter": "Haizi Yu", "authors": "Haizi Yu, Tianxi Li, Lav R. Varshney", "title": "Probabilistic Rule Realization and Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction and realization are bilateral processes that are key in deriving\nintelligence and creativity. In many domains, the two processes are approached\nthrough rules: high-level principles that reveal invariances within similar yet\ndiverse examples. Under a probabilistic setting for discrete input spaces, we\nfocus on the rule realization problem which generates input sample\ndistributions that follow the given rules. More ambitiously, we go beyond a\nmechanical realization that takes whatever is given, but instead ask for\nproactively selecting reasonable rules to realize. This goal is demanding in\npractice, since the initial rule set may not always be consistent and thus\nintelligent compromises are needed. We formulate both rule realization and\nselection as two strongly connected components within a single and symmetric\nbi-convex problem, and derive an efficient algorithm that works at large scale.\nTaking music compositional rules as the main example throughout the paper, we\ndemonstrate our model's efficiency in not only music realization (composition)\nbut also music interpretation and understanding (analysis).\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 05:08:00 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 05:24:04 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 21:44:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Yu", "Haizi", ""], ["Li", "Tianxi", ""], ["Varshney", "Lav R.", ""]]}, {"id": "1709.01703", "submitter": "Daniel Michelsanti", "authors": "Daniel Michelsanti and Zheng-Hua Tan", "title": "Conditional Generative Adversarial Networks for Speech Enhancement and\n  Noise-Robust Speaker Verification", "comments": "INTERSPEECH 2017 August 20-24, 2017, Stockholm, Sweden", "journal-ref": null, "doi": "10.21437/Interspeech.2017-1620", "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving speech system performance in noisy environments remains a\nchallenging task, and speech enhancement (SE) is one of the effective\ntechniques to solve the problem. Motivated by the promising results of\ngenerative adversarial networks (GANs) in a variety of image processing tasks,\nwe explore the potential of conditional GANs (cGANs) for SE, and in particular,\nwe make use of the image processing framework proposed by Isola et al. [1] to\nlearn a mapping from the spectrogram of noisy speech to an enhanced\ncounterpart. The SE cGAN consists of two networks, trained in an adversarial\nmanner: a generator that tries to enhance the input noisy spectrogram, and a\ndiscriminator that tries to distinguish between enhanced spectrograms provided\nby the generator and clean ones from the database using the noisy spectrogram\nas a condition. We evaluate the performance of the cGAN method in terms of\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and equal error rate (EER) of speaker verification (an\nexample application). Experimental results show that the cGAN method overall\noutperforms the classical short-time spectral amplitude minimum mean square\nerror (STSA-MMSE) SE algorithm, and is comparable to a deep neural\nnetwork-based SE approach (DNN-SE).\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 07:51:18 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 06:07:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Michelsanti", "Daniel", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1709.01713", "submitter": "James Salsman", "authors": "Yuan Gao, Brij Mohan Lal Srivastava, and James Salsman", "title": "Spoken English Intelligibility Remediation with PocketSphinx Alignment\n  and Feature Extraction Improves Substantially over the State of the Art", "comments": "8 pages, 3 figures", "journal-ref": "IEEE Advanced Information Management, Communications, Electronic\n  and Automation Control Conference 2 (2018)", "doi": "10.1109/IMCEC.2018.8469649", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We use automatic speech recognition to assess spoken English learner\npronunciation based on the authentic intelligibility of the learners' spoken\nresponses determined from support vector machine (SVM) classifier or deep\nlearning neural network model predictions of transcription correctness. Using\nnumeric features produced by PocketSphinx alignment mode and many recognition\npasses searching for the substitution and deletion of each expected phoneme and\ninsertion of unexpected phonemes in sequence, the SVM models achieve 82 percent\nagreement with the accuracy of Amazon Mechanical Turk crowdworker\ntranscriptions, up from 75 percent reported by multiple independent\nresearchers. Using such features with SVM classifier probability prediction\nmodels can help computer-aided pronunciation teaching (CAPT) systems provide\nintelligibility remediation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:27:59 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 22:59:37 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 09:59:26 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Yuan", ""], ["Srivastava", "Brij Mohan Lal", ""], ["Salsman", "James", ""]]}, {"id": "1709.01716", "submitter": "Daniel Ting", "authors": "Daniel Ting and Eric Brochu", "title": "Optimal Sub-sampling with Influence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sub-sampling is a common and often effective method to deal with the\ncomputational challenges of large datasets. However, for most statistical\nmodels, there is no well-motivated approach for drawing a non-uniform\nsubsample. We show that the concept of an asymptotically linear estimator and\nthe associated influence function leads to optimal sampling procedures for a\nwide class of popular models. Furthermore, for linear regression models which\nhave well-studied procedures for non-uniform sub-sampling, we show our optimal\ninfluence function based method outperforms previous approaches. We empirically\nshow the improved performance of our method on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:32:50 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ting", "Daniel", ""], ["Brochu", "Eric", ""]]}, {"id": "1709.01779", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Francisco Pereira", "title": "Deep learning from crowds", "comments": "10 pages, The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:41:19 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 12:30:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Pereira", "Francisco", ""]]}, {"id": "1709.01846", "submitter": "Yunchen Pu", "authors": "Liqun Chen, Shuyang Dai, Yunchen Pu, Chunyuan Li, Qinliang Su,\n  Lawrence Carin", "title": "Symmetric Variational Autoencoder and Connections to Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new form of the variational autoencoder (VAE) is proposed, based on the\nsymmetric Kullback-Leibler divergence. It is demonstrated that learning of the\nresulting symmetric VAE (sVAE) has close connections to previously developed\nadversarial-learning methods. This relationship helps unify the previously\ndistinct techniques of VAE and adversarially learning, and provides insights\nthat allow us to ameliorate shortcomings with some previously developed\nadversarial methods. In addition to an analysis that motivates and explains the\nsVAE, an extensive set of experiments validate the utility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:48:19 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 23:43:49 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Chen", "Liqun", ""], ["Dai", "Shuyang", ""], ["Pu", "Yunchen", ""], ["Li", "Chunyuan", ""], ["Su", "Qinliang", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.01860", "submitter": "Christopher Dienes", "authors": "Christopher Dienes", "title": "The low-rank hurdle model", "comments": "14 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A composite loss framework is proposed for low-rank modeling of data\nconsisting of interesting and common values, such as excess zeros or missing\nvalues. The methodology is motivated by the generalized low-rank framework and\nthe hurdle method which is commonly used to analyze zero-inflated counts. The\nmodel is demonstrated on a manufacturing data set and applied to the problem of\nmissing value imputation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 15:41:11 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dienes", "Christopher", ""]]}, {"id": "1709.01867", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi, Cl\\'ement Chatelain, Romain H\\'erault, S\\'ebastien\n  Adam", "title": "Neural Networks Regularization Through Class-wise Invariant\n  Representation Learning", "comments": "Submitted to ELSEVIER, 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks is known to require a large number of training\nsamples. However, in many applications only few training samples are available.\nIn this work, we tackle the issue of training neural networks for\nclassification task when few training samples are available. We attempt to\nsolve this issue by proposing a new regularization term that constrains the\nhidden layers of a network to learn class-wise invariant representations. In\nour regularization framework, learning invariant representations is generalized\nto the class membership where samples with the same class should have the same\nrepresentation. Numerical experiments over MNIST and its variants showed that\nour proposal helps improving the generalization of neural network particularly\nwhen trained with few samples. We provide the source code of our framework\nhttps://github.com/sbelharbi/learning-class-invariant-features .\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 15:53:16 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 12:45:37 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 18:17:15 GMT"}, {"version": "v4", "created": "Fri, 22 Dec 2017 14:11:44 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Belharbi", "Soufiane", ""], ["Chatelain", "Cl\u00e9ment", ""], ["H\u00e9rault", "Romain", ""], ["Adam", "S\u00e9bastien", ""]]}, {"id": "1709.01870", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Mathews Jacob", "title": "Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 15:59:57 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1709.01894", "submitter": "Mark van der Wilk", "authors": "Mark van der Wilk, Carl Edward Rasmussen, James Hensman", "title": "Convolutional Gaussian Processes", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical way of introducing convolutional structure into\nGaussian processes, making them more suited to high-dimensional inputs like\nimages. The main contribution of our work is the construction of an\ninter-domain inducing point approximation that is well-tailored to the\nconvolutional kernel. This allows us to gain the generalisation benefit of a\nconvolutional kernel, together with fast but accurate posterior inference. We\ninvestigate several variations of the convolutional kernel, and apply it to\nMNIST and CIFAR-10, which have both been known to be challenging for Gaussian\nprocesses. We also show how the marginal likelihood can be used to find an\noptimal weighting between convolutional and RBF kernels to further improve\nperformance. We hope that this illustration of the usefulness of a marginal\nlikelihood will help automate discovering architectures in larger models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 16:59:02 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["van der Wilk", "Mark", ""], ["Rasmussen", "Carl Edward", ""], ["Hensman", "James", ""]]}, {"id": "1709.01907", "submitter": "Lingxue Zhu", "authors": "Lingxue Zhu, Nikolay Laptev", "title": "Deep and Confident Prediction for Time Series at Uber", "comments": "To appear in DSBDA-2017 @ ICDM'17", "journal-ref": "2017 IEEE International Conference on Data Mining Workshops\n  (ICDMW)", "doi": "10.1109/ICDMW.2017.19", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable uncertainty estimation for time series prediction is critical in\nmany fields, including physics, biology, and manufacturing. At Uber,\nprobabilistic time series forecasting is used for robust prediction of number\nof trips during special events, driver incentive allocation, as well as\nreal-time anomaly detection across millions of metrics. Classical time series\nmodels are often used in conjunction with a probabilistic formulation for\nuncertainty estimation. However, such models are hard to tune, scale, and add\nexogenous variables to. Motivated by the recent resurgence of Long Short Term\nMemory networks, we propose a novel end-to-end Bayesian deep model that\nprovides time series prediction along with uncertainty estimation. We provide\ndetailed experiments of the proposed solution on completed trips data, and\nsuccessfully apply it to large-scale time series anomaly detection at Uber.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 17:29:50 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhu", "Lingxue", ""], ["Laptev", "Nikolay", ""]]}, {"id": "1709.01919", "submitter": "Ming Yu", "authors": "Ming Yu, Varun Gupta, Mladen Kolar", "title": "Estimation of a Low-rank Topic-Based Model for Information Cascades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the latent structure of a social\nnetwork based on the observed information diffusion events, or cascades, where\nthe observations for a given cascade consist of only the timestamps of\ninfection for infected nodes but not the source of the infection. Most of the\nexisting work on this problem has focused on estimating a diffusion matrix\nwithout any structural assumptions on it. In this paper, we propose a novel\nmodel based on the intuition that an information is more likely to propagate\namong two nodes if they are interested in similar topics which are also\nprominent in the information content. In particular, our model endows each node\nwith an influence vector (which measures how authoritative the node is on each\ntopic) and a receptivity vector (which measures how susceptible the node is for\neach topic). We show how this node-topic structure can be estimated from the\nobserved cascades, and prove the consistency of the estimator. Experiments on\nsynthetic and real data demonstrate the improved performance and better\ninterpretability of our model compared to existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 17:56:45 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 03:48:01 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 01:04:36 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Yu", "Ming", ""], ["Gupta", "Varun", ""], ["Kolar", "Mladen", ""]]}, {"id": "1709.01972", "submitter": "David W. Dreisigmeyer", "authors": "David W. Dreisigmeyer", "title": "A Quasi-isometric Embedding Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Whitney embedding theorem gives an upper bound on the smallest embedding\ndimension of a manifold. If a data set lies on a manifold, a random projection\ninto this reduced dimension will retain the manifold structure. Here we present\nan algorithm to find a projection that distorts the data as little as possible.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 19:35:02 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 17:35:59 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 13:40:50 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Dreisigmeyer", "David W.", ""]]}, {"id": "1709.02012", "submitter": "Geoff Pleiss", "authors": "Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q.\n  Weinberger", "title": "On Fairness and Calibration", "comments": "First two authors contributed equally. NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has become increasingly concerned with the\npotential for bias and discrimination in predictive models. This has motivated\na growing line of work on what it means for a classification procedure to be\n\"fair.\" In this paper, we investigate the tension between minimizing error\ndisparity across different population groups while maintaining calibrated\nprobability estimates. We show that calibration is compatible only with a\nsingle error constraint (i.e. equal false-negatives rates across groups), and\nshow that any algorithm that satisfies this relaxation is no better than\nrandomizing a percentage of predictions for an existing classifier. These\nunsettling findings, which extend and generalize existing results, are\nempirically confirmed on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 21:49:38 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 18:18:41 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pleiss", "Geoff", ""], ["Raghavan", "Manish", ""], ["Wu", "Felix", ""], ["Kleinberg", "Jon", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1709.02023", "submitter": "Murat Kocaoglu", "authors": "Murat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, Sriram\n  Vishwanath", "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adversarial training procedure for learning a causal implicit\ngenerative model for a given causal graph. We show that adversarial training\ncan be used to learn a generative model with true observational and\ninterventional distributions if the generator architecture is consistent with\nthe given causal graph. We consider the application of generating faces based\non given binary labels where the dependency structure between the labels is\npreserved with a causal graph. This problem can be seen as learning a causal\nimplicit generative model for the image and labels. We devise a two-stage\nprocedure for this problem. First we train a causal implicit generative model\nover binary labels using a neural network consistent with a causal graph as the\ngenerator. We empirically show that WassersteinGAN can be used to output\ndiscrete labels. Later, we propose two new conditional GAN architectures, which\nwe call CausalGAN and CausalBEGAN. We show that the optimal generator of the\nCausalGAN, given the labels, samples from the image distributions conditioned\non these labels. The conditional GAN combined with a trained causal implicit\ngenerative model for the labels is then a causal implicit generative model over\nthe labels and the generated image. We show that the proposed architectures can\nbe used to sample from observational and interventional image distributions,\neven for interventions which do not naturally occur in the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 22:53:12 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 22:52:47 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Snyder", "Christopher", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1709.02082", "submitter": "Romain Lopez", "authors": "Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan and Nir\n  Yosef", "title": "A deep generative model for gene expression profiles from single-cell\n  RNA sequencing", "comments": "BayLearn2017, NIPS workshop MLCB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model for interpreting gene expression levels that\nare observed through single-cell RNA sequencing. In the model, each cell has a\nlow-dimensional latent representation. Additional latent variables account for\ntechnical effects that may erroneously set some observations of gene expression\nlevels to zero. Conditional distributions are specified by neural networks,\ngiving the proposed model enough flexibility to fit the data well. We use\nvariational inference and stochastic optimization to approximate the posterior\ndistribution. The inference procedure scales to over one million cells, whereas\ncompeting algorithms do not. Even for smaller datasets, for several tasks, the\nproposed procedure outperforms state-of-the-art methods like ZIFA and\nZINB-WaVE. We also extend our framework to account for batch effects and other\nconfounding factors, and propose a Bayesian hypothesis test for differential\nexpression that outperforms DESeq2.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:59:49 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 01:41:27 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 00:37:51 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 22:44:59 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Lopez", "Romain", ""], ["Regier", "Jeffrey", ""], ["Cole", "Michael", ""], ["Jordan", "Michael", ""], ["Yosef", "Nir", ""]]}, {"id": "1709.02194", "submitter": "Biswa Sengupta", "authors": "Alessandro Bay and Biswa Sengupta", "title": "Approximating meta-heuristics with homotopic recurrent neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much combinatorial optimisation problems constitute a non-polynomial (NP)\nhard optimisation problem, i.e., they can not be solved in polynomial time. One\nsuch problem is finding the shortest route between two nodes on a graph.\nMeta-heuristic algorithms such as $A^{*}$ along with mixed-integer programming\n(MIP) methods are often employed for these problems. Our work demonstrates that\nit is possible to approximate solutions generated by a meta-heuristic algorithm\nusing a deep recurrent neural network. We compare different methodologies based\non reinforcement learning (RL) and recurrent neural networks (RNN) to gauge\ntheir respective quality of approximation. We show the viability of recurrent\nneural network solutions on a graph that has over 300 nodes and argue that a\nsequence-to-sequence network rather than other recurrent networks has improved\napproximation quality. Additionally, we argue that homotopy continuation --\nthat increases chances of hitting an extremum -- further improves the estimate\ngenerated by a vanilla RNN.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 11:54:36 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Bay", "Alessandro", ""], ["Sengupta", "Biswa", ""]]}, {"id": "1709.02280", "submitter": "Pooyan Jamshidi", "authors": "Pooyan Jamshidi, Norbert Siegmund, Miguel Velez, Christian K\\\"astner,\n  Akshay Patel, Yuvraj Agarwal", "title": "Transfer Learning for Performance Modeling of Configurable Systems: An\n  Exploratory Analysis", "comments": "To appear in 32nd IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2017), 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software systems provide many configuration options which\nsignificantly influence their non-functional properties. To understand and\npredict the effect of configuration options, several sampling and learning\nstrategies have been proposed, albeit often with significant cost to cover the\nhighly dimensional configuration space. Recently, transfer learning has been\napplied to reduce the effort of constructing performance models by transferring\nknowledge about performance behavior across environments. While this line of\nresearch is promising to learn more accurate models at a lower cost, it is\nunclear why and when transfer learning works for performance modeling. To shed\nlight on when it is beneficial to apply transfer learning, we conducted an\nempirical study on four popular software systems, varying software\nconfigurations and environmental conditions, such as hardware, workload, and\nsoftware versions, to identify the key knowledge pieces that can be exploited\nfor transfer learning. Our results show that in small environmental changes\n(e.g., homogeneous workload change), by applying a linear transformation to the\nperformance model, we can understand the performance behavior of the target\nenvironment, while for severe environmental changes (e.g., drastic workload\nchange) we can transfer only knowledge that makes sampling more efficient,\ne.g., by reducing the dimensionality of the configuration space.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:31:21 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Jamshidi", "Pooyan", ""], ["Siegmund", "Norbert", ""], ["Velez", "Miguel", ""], ["K\u00e4stner", "Christian", ""], ["Patel", "Akshay", ""], ["Agarwal", "Yuvraj", ""]]}, {"id": "1709.02327", "submitter": "Claudio Reggiani", "authors": "Claudio Reggiani, Yann-A\\\"el Le Borgne, Gianluca Bontempi", "title": "Feature selection in high-dimensional dataset using MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a distributed MapReduce implementation of the minimum\nRedundancy Maximum Relevance algorithm, a popular feature selection method in\nbioinformatics and network inference problems. The proposed approach handles\nboth tall/narrow and wide/short datasets. We further provide an open source\nimplementation based on Hadoop/Spark, and illustrate its scalability on\ndatasets involving millions of observations or features.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:05:51 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Reggiani", "Claudio", ""], ["Borgne", "Yann-A\u00ebl Le", ""], ["Bontempi", "Gianluca", ""]]}, {"id": "1709.02349", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng\n  Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath\n  Chandar, Nan Rosemary Ke, Sai Rajeshwar, Alexandre de Brebisson, Jose M. R.\n  Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau,\n  Yoshua Bengio", "title": "A Deep Reinforcement Learning Chatbot", "comments": "40 pages, 9 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MILABOT: a deep reinforcement learning chatbot developed by the\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\ncompetition. MILABOT is capable of conversing with humans on popular small talk\ntopics through both speech and text. The system consists of an ensemble of\nnatural language generation and retrieval models, including template-based\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\nvariable neural network models. By applying reinforcement learning to\ncrowdsourced data and real-world user interactions, the system has been trained\nto select an appropriate response from the models in its ensemble. The system\nhas been evaluated through A/B testing with real-world users, where it\nperformed significantly better than many competing systems. Due to its machine\nlearning architecture, the system is likely to improve with additional data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:51:09 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:02:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sankar", "Chinnadhurai", ""], ["Germain", "Mathieu", ""], ["Zhang", "Saizheng", ""], ["Lin", "Zhouhan", ""], ["Subramanian", "Sandeep", ""], ["Kim", "Taesup", ""], ["Pieper", "Michael", ""], ["Chandar", "Sarath", ""], ["Ke", "Nan Rosemary", ""], ["Rajeshwar", "Sai", ""], ["de Brebisson", "Alexandre", ""], ["Sotelo", "Jose M. R.", ""], ["Suhubdy", "Dendi", ""], ["Michalski", "Vincent", ""], ["Nguyen", "Alexandre", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1709.02357", "submitter": "Adam Derek Cobb", "authors": "Adam D. Cobb, Andrew Markham, Stephen J. Roberts", "title": "Learning from lions: inferring the utility of agents from their\n  trajectories", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a model using Gaussian processes to infer a spatio-temporal vector\nfield from observed agent trajectories. Significant landmarks or influence\npoints in agent surroundings are jointly derived through vector calculus\noperations that indicate presence of sources and sinks. We evaluate these\ninfluence points by using the Kullback-Leibler divergence between the posterior\nand prior Laplacian of the inferred spatio-temporal vector field. Through\nlocating significant features that influence trajectories, our model aims to\ngive greater insight into underlying causal utility functions that determine\nagent decision-making. A key feature of our model is that it infers a joint\nGaussian process over the observed trajectories, the time-varying vector field\nof utility and canonical vector calculus operators. We apply our model to both\nsynthetic data and lion GPS data collected at the Bubye Valley Conservancy in\nsouthern Zimbabwe.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:10:12 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Cobb", "Adam D.", ""], ["Markham", "Andrew", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1709.02373", "submitter": "Salaheddin Alakkari", "authors": "Salaheddin Alakkari and John Dingliana", "title": "Adaptive PCA for Time-Varying Data", "comments": "Typos fixed, uncited references removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an online adaptive PCA algorithm that is able to\ncompute the full dimensional eigenspace per new time-step of sequential data.\nThe algorithm is based on a one-step update rule that considers all second\norder correlations between previous samples and the new time-step. Our\nalgorithm has O(n) complexity per new time-step in its deterministic mode and\nO(1) complexity per new time-step in its stochastic mode. We test our algorithm\non a number of time-varying datasets of different physical phenomena. Explained\nvariance curves indicate that our technique provides an excellent approximation\nto the original eigenspace computed using standard PCA in batch mode. In\naddition, our experiments show that the stochastic mode, despite its much lower\ncomputational complexity, converges to the same eigenspace computed using the\ndeterministic mode.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:49:47 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:55:44 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Alakkari", "Salaheddin", ""], ["Dingliana", "John", ""]]}, {"id": "1709.02457", "submitter": "Ali Pesaranghader", "authors": "Ali Pesaranghader, Herna Viktor and Eric Paquet", "title": "Reservoir of Diverse Adaptive Learners and Stacking Fast Hoeffding Drift\n  Detection Methods for Evolving Data Streams", "comments": "42 pages, and 14 figures", "journal-ref": null, "doi": "10.1007/s10994-018-5719-z", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a surge of interest in adaptive learning algorithms\nfor data stream classification, with applications ranging from predicting ozone\nlevel peaks, learning stock market indicators, to detecting computer security\nviolations. In addition, a number of methods have been developed to detect\nconcept drifts in these streams. Consider a scenario where we have a number of\nclassifiers with diverse learning styles and different drift detectors.\nIntuitively, the current 'best' (classifier, detector) pair is application\ndependent and may change as a result of the stream evolution. Our research\nbuilds on this observation. We introduce the $\\mbox{Tornado}$ framework that\nimplements a reservoir of diverse classifiers, together with a variety of drift\ndetection algorithms. In our framework, all (classifier, detector) pairs\nproceed, in parallel, to construct models against the evolving data streams. At\nany point in time, we select the pair which currently yields the best\nperformance. We further incorporate two novel stacking-based drift detection\nmethods, namely the $\\mbox{FHDDMS}$ and $\\mbox{FHDDMS}_{add}$ approaches. The\nexperimental evaluation confirms that the current 'best' (classifier, detector)\npair is not only heavily dependent on the characteristics of the stream, but\nalso that this selection evolves as the stream flows. Further, our\n$\\mbox{FHDDMS}$ variants detect concept drifts accurately in a timely fashion\nwhile outperforming the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 21:19:24 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Pesaranghader", "Ali", ""], ["Viktor", "Herna", ""], ["Paquet", "Eric", ""]]}, {"id": "1709.02477", "submitter": "Bryan He", "authors": "Paroma Varma, Bryan He, Payal Bajaj, Imon Banerjee, Nishith Khandwala,\n  Daniel L. Rubin, Christopher R\\'e", "title": "Inferring Generative Model Structure with Static Analysis", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining enough labeled data to robustly train complex discriminative models\nis a major bottleneck in the machine learning pipeline. A popular solution is\ncombining multiple sources of weak supervision using generative models. The\nstructure of these models affects training label quality, but is difficult to\nlearn without any ground truth labels. We instead rely on these weak\nsupervision sources having some structure by virtue of being encoded\nprogrammatically. We present Coral, a paradigm that infers generative model\nstructure by statically analyzing the code for these heuristics, thus reducing\nthe data required to learn structure significantly. We prove that Coral's\nsample complexity scales quasilinearly with the number of heuristics and number\nof relations found, improving over the standard sample complexity, which is\nexponential in $n$ for identifying $n^{\\textrm{th}}$ degree relations.\nExperimentally, Coral matches or outperforms traditional structure learning\napproaches by up to 3.81 F1 points. Using Coral to model dependencies instead\nof assuming independence results in better performance than a fully supervised\nmodel by 3.07 accuracy points when heuristics are used to label radiology data\nwithout ground truth labels.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 22:33:37 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Varma", "Paroma", ""], ["He", "Bryan", ""], ["Bajaj", "Payal", ""], ["Banerjee", "Imon", ""], ["Khandwala", "Nishith", ""], ["Rubin", "Daniel L.", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1709.02532", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence", "comments": "34 pages, 10 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:36:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 05:59:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:56:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:21:46 GMT"}, {"version": "v5", "created": "Sun, 25 Feb 2018 22:58:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1709.02538", "submitter": "Bita Darvish Rouhani", "authors": "Bita Darvish Rouhani, Mohammad Samragh, Mojan Javaheripi, Tara Javidi,\n  Farinaz Koushanfar", "title": "DeepFense: Online Accelerated Defense Against Adversarial Deep Learning", "comments": "Adding hardware acceleration for real-time execution of defender\n  modules", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in adversarial Deep Learning (DL) have opened up a largely\nunexplored surface for malicious attacks jeopardizing the integrity of\nautonomous DL systems. With the wide-spread usage of DL in critical and\ntime-sensitive applications, including unmanned vehicles, drones, and video\nsurveillance systems, online detection of malicious inputs is of utmost\nimportance. We propose DeepFense, the first end-to-end automated framework that\nsimultaneously enables efficient and safe execution of DL models. DeepFense\nformalizes the goal of thwarting adversarial attacks as an optimization problem\nthat minimizes the rarely observed regions in the latent feature space spanned\nby a DL network. To solve the aforementioned minimization problem, a set of\ncomplementary but disjoint modular redundancies are trained to validate the\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\nleverages hardware/software/algorithm co-design and customized acceleration to\nachieve just-in-time performance in resource-constrained settings. The proposed\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\nto train modular redundancies. We further provide an accompanying API to reduce\nthe non-recurring engineering cost and ensure automated adaptation to various\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\nof magnitude performance improvement while enabling online adversarial sample\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:53:51 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 01:37:12 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 18:30:00 GMT"}, {"version": "v4", "created": "Tue, 21 Aug 2018 02:54:23 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Rouhani", "Bita Darvish", ""], ["Samragh", "Mohammad", ""], ["Javaheripi", "Mojan", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1709.02576", "submitter": "Chang Min Hyun", "authors": "Chang Min Hyun, Hwa Pyung Kim, Sung Min Lee, Sungchul Lee and Jin Keun\n  Seo", "title": "Deep learning for undersampled MRI reconstruction", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/aac71a", "report-no": null, "categories": "stat.ML cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning method for faster magnetic resonance\nimaging (MRI) by reducing k-space data with sub-Nyquist sampling strategies and\nprovides a rationale for why the proposed approach works well. Uniform\nsubsampling is used in the time-consuming phase-encoding direction to capture\nhigh-resolution image information, while permitting the image-folding problem\ndictated by the Poisson summation formula. To deal with the localization\nuncertainty due to image folding, very few low-frequency k-space data are\nadded. Training the deep learning net involves input and output images that are\npairs of Fourier transforms of the subsampled and fully sampled k-space data.\nNumerous experiments show the remarkable performance of the proposed method;\nonly 29% of k-space data can generate images of high quality as effectively as\nstandard MRI reconstruction with fully sampled data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 07:35:58 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 00:38:47 GMT"}, {"version": "v3", "created": "Sun, 12 May 2019 12:47:06 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hyun", "Chang Min", ""], ["Kim", "Hwa Pyung", ""], ["Lee", "Sung Min", ""], ["Lee", "Sungchul", ""], ["Seo", "Jin Keun", ""]]}, {"id": "1709.02702", "submitter": "Diego Granziol", "authors": "Diego Granziol, Stephen Roberts", "title": "Entropic Determinants", "comments": "9 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of many powerful machine learning algorithms to deal with large\ndata sets without compromise is often hampered by computationally expensive\nlinear algebra tasks, of which calculating the log determinant is a canonical\nexample. In this paper we demonstrate the optimality of Maximum Entropy methods\nin approximating such calculations. We prove the equivalence between mean value\nconstraints and sample expectations in the big data limit, that Covariance\nmatrix eigenvalue distributions can be completely defined by moment information\nand that the reduction of the self entropy of a maximum entropy proposal\ndistribution, achieved by adding more moments reduces the KL divergence between\nthe proposal and true eigenvalue distribution. We empirically verify our\nresults on a variety of SparseSuite matrices and establish best practices.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 13:41:26 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Granziol", "Diego", ""], ["Roberts", "Stephen", ""]]}, {"id": "1709.02726", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism,\n  Composite Objectives, and Variational Bounds", "comments": "Accepted to The 28th International Conference on Algorithmic Learning\n  Theory (ALT 2017). 40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much work has been done on extending the scope of online learning\nand incremental stochastic optimization algorithms. In this paper we contribute\nto this effort in two ways: First, based on a new regret decomposition and a\ngeneralization of Bregman divergences, we provide a self-contained, modular\nanalysis of the two workhorses of online learning: (general) adaptive versions\nof Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms.\nThe analysis is done with extra care so as not to introduce assumptions not\nneeded in the proofs and allows to combine, in a straightforward way, different\nalgorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning\nsettings (e.g., strongly convex or composite objectives). This way we are able\nto reprove, extend and refine a large body of the literature, while keeping the\nproofs concise. The second contribution is a byproduct of this careful\nanalysis: We present algorithms with improved variational bounds for smooth,\ncomposite objectives, including a new family of optimistic MD algorithms with\nonly one projection step per round. Furthermore, we provide a simple extension\nof adaptive regret bounds to practically relevant non-convex problem settings\nwith essentially no extra effort.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 14:54:44 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1709.02727", "submitter": "Valentin Stanev G.", "authors": "Valentin Stanev, Corey Oses, A. Gilad Kusne, Efrain Rodriguez,\n  Johnpierre Paglione, Stefano Curtarolo, and Ichiro Takeuchi", "title": "Machine learning modeling of superconducting critical temperature", "comments": "17 pages, 7 figures", "journal-ref": "npj Computational Materials 4, Article number: 29 (2018)", "doi": "10.1038/s41524-018-0085-8", "report-no": null, "categories": "cond-mat.supr-con cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superconductivity has been the focus of enormous research effort since its\ndiscovery more than a century ago. Yet, some features of this unique phenomenon\nremain poorly understood; prime among these is the connection between\nsuperconductivity and chemical/structural properties of materials. To bridge\nthe gap, several machine learning schemes are developed herein to model the\ncritical temperatures ($T_{\\mathrm{c}}$) of the 12,000+ known superconductors\navailable via the SuperCon database. Materials are first divided into two\nclasses based on their $T_{\\mathrm{c}}$ values, above and below 10 K, and a\nclassification model predicting this label is trained. The model uses\ncoarse-grained features based only on the chemical compositions. It shows\nstrong predictive power, with out-of-sample accuracy of about 92%. Separate\nregression models are developed to predict the values of $T_{\\mathrm{c}}$ for\ncuprate, iron-based, and \"low-$T_{\\mathrm{c}}$\" compounds. These models also\ndemonstrate good performance, with learned predictors offering potential\ninsights into the mechanisms behind superconductivity in different families of\nmaterials. To improve the accuracy and interpretability of these models, new\nfeatures are incorporated using materials data from the AFLOW Online\nRepositories. Finally, the classification and regression models are combined\ninto a single integrated pipeline and employed to search the entire Inorganic\nCrystallographic Structure Database (ICSD) for potential new superconductors.\nWe identify more than 30 non-cuprate and non-iron-based oxides as candidate\nmaterials.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 14:55:02 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 21:00:51 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Stanev", "Valentin", ""], ["Oses", "Corey", ""], ["Kusne", "A. Gilad", ""], ["Rodriguez", "Efrain", ""], ["Paglione", "Johnpierre", ""], ["Curtarolo", "Stefano", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1709.02739", "submitter": "James Bagrow", "authors": "Mark D. Wagy, Josh C. Bongard, James P. Bagrow and Paul D. H. Hines", "title": "Crowdsourcing Predictors of Residential Electric Energy Usage", "comments": "11 pages, 7 figures", "journal-ref": "IEEE Systems Journal, 2018", "doi": "10.1109/JSYST.2017.2778144", "report-no": null, "categories": "cs.HC physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been successfully applied in many domains including\nastronomy, cryptography and biology. In order to test its potential for useful\napplication in a Smart Grid context, this paper investigates the extent to\nwhich a crowd can contribute predictive hypotheses to a model of residential\nelectric energy consumption. In this experiment, the crowd generated hypotheses\nabout factors that make one home different from another in terms of monthly\nenergy usage. To implement this concept, we deployed a web-based system within\nwhich 627 residential electricity customers posed 632 questions that they\nthought predictive of energy usage. While this occurred, the same group\nprovided 110,573 answers to these questions as they accumulated. Thus users\nboth suggested the hypotheses that drive a predictive model and provided the\ndata upon which the model is built. We used the resulting question and answer\ndata to build a predictive model of monthly electric energy consumption, using\nrandom forest regression. Because of the sparse nature of the answer data,\ncareful statistical work was needed to ensure that these models are valid. The\nresults indicate that the crowd can generate useful hypotheses, despite the\nsparse nature of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 15:17:14 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Wagy", "Mark D.", ""], ["Bongard", "Josh C.", ""], ["Bagrow", "James P.", ""], ["Hines", "Paul D. H.", ""]]}, {"id": "1709.02797", "submitter": "Matti Herranen", "authors": "Heikki Arponen, Matti Herranen, Harri Valpola", "title": "On the exact relationship between the denoising function and the data\n  distribution", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an exact relationship between the optimal denoising function and the\ndata distribution in the case of additive Gaussian noise, showing that\ndenoising implicitly models the structure of data allowing it to be exploited\nin the unsupervised learning of representations. This result generalizes a\nknown relationship [2], which is valid only in the limit of small corruption\nnoise.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 07:26:59 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Arponen", "Heikki", ""], ["Herranen", "Matti", ""], ["Valpola", "Harri", ""]]}, {"id": "1709.02802", "submitter": "EPTCS", "authors": "Guy Katz (Stanford University), Clark Barrett (Stanford University),\n  David L. Dill (Stanford University), Kyle Julian (Stanford University), Mykel\n  J. Kochenderfer (Stanford University)", "title": "Towards Proving the Adversarial Robustness of Deep Neural Networks", "comments": "In Proceedings FVAV 2017, arXiv:1709.02126", "journal-ref": "EPTCS 257, 2017, pp. 19-26", "doi": "10.4204/EPTCS.257.3", "report-no": null, "categories": "cs.LG cs.CR cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles are highly complex systems, required to function reliably\nin a wide variety of situations. Manually crafting software controllers for\nthese vehicles is difficult, but there has been some success in using deep\nneural networks generated using machine-learning. However, deep neural networks\nare opaque to human engineers, rendering their correctness very difficult to\nprove manually; and existing automated techniques, which were not designed to\noperate on neural networks, fail to scale to large systems. This paper focuses\non proving the adversarial robustness of deep neural networks, i.e. proving\nthat small perturbations to a correctly-classified input to the network cannot\ncause it to be misclassified. We describe some of our recent and ongoing work\non verifying the adversarial robustness of networks, and discuss some of the\nopen questions we have encountered and how they might be addressed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 06:34:44 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Katz", "Guy", "", "Stanford University"], ["Barrett", "Clark", "", "Stanford University"], ["Dill", "David L.", "", "Stanford University"], ["Julian", "Kyle", "", "Stanford University"], ["Kochenderfer", "Mykel J.", "", "Stanford University"]]}, {"id": "1709.02840", "submitter": "Osvaldo Simeone", "authors": "Osvaldo Simeone", "title": "A Brief Introduction to Machine Learning for Engineers", "comments": "This is an expanded and improved version of the original posting.\n  Feedback is welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph aims at providing an introduction to key concepts, algorithms,\nand theoretical results in machine learning. The treatment concentrates on\nprobabilistic models for supervised and unsupervised learning problems. It\nintroduces fundamental concepts and algorithms by building on first principles,\nwhile also exposing the reader to more advanced topics with extensive pointers\nto the literature, within a unified notation and mathematical framework. The\nmaterial is organized according to clearly defined categories, such as\ndiscriminative and generative models, frequentist and Bayesian approaches,\nexact and approximate inference, as well as directed and undirected models.\nThis monograph is meant as an entry point for researchers with a background in\nprobability and linear algebra.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 19:21:26 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 09:57:15 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 18:28:52 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Simeone", "Osvaldo", ""]]}, {"id": "1709.02855", "submitter": "Kexin Yi", "authors": "Kexin Yi, Finale Doshi-Velez", "title": "Roll-back Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for Hamiltonian Monte Carlo (HMC) on truncated\nprobability distributions with smooth underlying density functions. Traditional\nHMC requires computing the gradient of potential function associated with the\ntarget distribution, and therefore does not perform its full power on truncated\ndistributions due to lack of continuity and differentiability. In our\nframework, we introduce a sharp sigmoid factor in the density function to\napproximate the probability drop at the truncation boundary. The target\npotential function is approximated by a new potential which smoothly extends to\nthe entire sample space. HMC is then performed on the approximate potential.\nWhile our method is easy to implement and applies to a wide range of problems,\nit also achieves comparable computational efficiency on various sampling tasks\ncompared to other baseline methods. RBHMC also gives rise to a new approach for\nBayesian inference on constrained spaces.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 20:47:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Yi", "Kexin", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1709.02893", "submitter": "Brendt Wohlberg", "authors": "Cristina Garcia-Cardona and Brendt Wohlberg", "title": "Convolutional Dictionary Learning: A Comparative Review and New\n  Algorithms", "comments": "Corrected typos in Eq. (18) and (19)", "journal-ref": "IEEE Transactions on Computational Imaging, vol. 4, no. 3, pp.\n  366-381, Sep 2018", "doi": "10.1109/TCI.2018.2840334", "report-no": null, "categories": "cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse representations are a form of sparse representation with\na dictionary that has a structure that is equivalent to convolution with a set\nof linear filters. While effective algorithms have recently been developed for\nthe convolutional sparse coding problem, the corresponding dictionary learning\nproblem is substantially more challenging. Furthermore, although a number of\ndifferent approaches have been proposed, the absence of thorough comparisons\nbetween them makes it difficult to determine which of them represents the\ncurrent state of the art. The present work both addresses this deficiency and\nproposes some new approaches that outperform existing ones in certain contexts.\nA thorough set of performance comparisons indicates a very wide range of\nperformance differences among the existing and proposed methods, and clearly\nidentifies those that are the most effective.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 01:45:43 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 13:52:50 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 06:50:42 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 16:19:21 GMT"}, {"version": "v5", "created": "Wed, 5 Sep 2018 16:55:28 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Garcia-Cardona", "Cristina", ""], ["Wohlberg", "Brendt", ""]]}, {"id": "1709.02896", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Bo Zhou, and Feiping Nie", "title": "Simultaneously Learning Neighborship and Projection Matrix for\n  Supervised Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicitly or implicitly, most of dimensionality reduction methods need to\ndetermine which samples are neighbors and the similarity between the neighbors\nin the original highdimensional space. The projection matrix is then learned on\nthe assumption that the neighborhood information (e.g., the similarity) is\nknown and fixed prior to learning. However, it is difficult to precisely\nmeasure the intrinsic similarity of samples in high-dimensional space because\nof the curse of dimensionality. Consequently, the neighbors selected according\nto such similarity might and the projection matrix obtained according to such\nsimilarity and neighbors are not optimal in the sense of classification and\ngeneralization. To overcome the drawbacks, in this paper we propose to let the\nsimilarity and neighbors be variables and model them in low-dimensional space.\nBoth the optimal similarity and projection matrix are obtained by minimizing a\nunified objective function. Nonnegative and sum-to-one constraints on the\nsimilarity are adopted. Instead of empirically setting the regularization\nparameter, we treat it as a variable to be optimized. It is interesting that\nthe optimal regularization parameter is adaptive to the neighbors in\nlow-dimensional space and has intuitive meaning. Experimental results on the\nYALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 02:44:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Pang", "Yanwei", ""], ["Zhou", "Bo", ""], ["Nie", "Feiping", ""]]}, {"id": "1709.02909", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Zhe Li, Lijun Zhang", "title": "A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary\n  Convex Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple analysis of {\\bf fast rates} with {\\it\nhigh probability} of {\\bf empirical minimization} for {\\it stochastic composite\noptimization} over a finite-dimensional bounded convex set with exponential\nconcave loss functions and an arbitrary convex regularization. To the best of\nour knowledge, this result is the first of its kind. As a byproduct, we can\ndirectly obtain the fast rate with {\\it high probability} for exponential\nconcave empirical risk minimization with and without any convex regularization,\nwhich not only extends existing results of empirical risk minimization but also\nprovides a unified framework for analyzing exponential concave empirical risk\nminimization with and without {\\it any} convex regularization. Our proof is\nvery simple only exploiting the covering number of a finite-dimensional bounded\nset and a concentration inequality of random vectors.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:44:14 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Yang", "Tianbao", ""], ["Li", "Zhe", ""], ["Zhang", "Lijun", ""]]}, {"id": "1709.02925", "submitter": "Hamed R. Bonab", "authors": "Hamed Bonab and Fazli Can", "title": "Less Is More: A Comprehensive Framework for the Number of Components of\n  Ensemble Classifiers", "comments": "This is an extended version of the work presented as a short paper at\n  the Conference on Information and Knowledge Management (CIKM), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of component classifiers chosen for an ensemble greatly impacts\nthe prediction ability. In this paper, we use a geometric framework for a\npriori determining the ensemble size, which is applicable to most of existing\nbatch and online ensemble classifiers. There are only a limited number of\nstudies on the ensemble size examining Majority Voting (MV) and Weighted\nMajority Voting (WMV). Almost all of them are designed for batch-mode, hardly\naddressing online environments. Big data dimensions and resource limitations,\nin terms of time and memory, make determination of ensemble size crucial,\nespecially for online environments. For the MV aggregation rule, our framework\nproves that the more strong components we add to the ensemble, the more\naccurate predictions we can achieve. For the WMV aggregation rule, our\nframework proves the existence of an ideal number of components, which is equal\nto the number of class labels, with the premise that components are completely\nindependent of each other and strong enough. While giving the exact definition\nfor a strong and independent classifier in the context of an ensemble is a\nchallenging task, our proposed geometric framework provides a theoretical\nexplanation of diversity and its impact on the accuracy of predictions. We\nconduct a series of experimental evaluations to show the practical value of our\ntheorems and existing challenges.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 07:52:58 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 23:48:02 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bonab", "Hamed", ""], ["Can", "Fazli", ""]]}, {"id": "1709.02956", "submitter": "Masato Taki", "authors": "Masato Taki", "title": "Deep Residual Networks and Weight Initialization", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-17", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual Network (ResNet) is the state-of-the-art architecture that realizes\nsuccessful training of really deep neural network. It is also known that good\nweight initialization of neural network avoids problem of vanishing/exploding\ngradients. In this paper, simplified models of ResNets are analyzed. We argue\nthat goodness of ResNet is correlated with the fact that ResNets are relatively\ninsensitive to choice of initial weights. We also demonstrate how batch\nnormalization improves backpropagation of deep ResNets without tuning initial\nvalues of weights.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 14:23:50 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Taki", "Masato", ""]]}, {"id": "1709.03019", "submitter": "Andrew Gardner", "authors": "Andrew Gardner and Jinko Kanno and Christian A. Duncan and Rastko R.\n  Selmic", "title": "Classifying Unordered Feature Sets with Convolutional Deep Averaging\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unordered feature sets are a nonstandard data structure that traditional\nneural networks are incapable of addressing in a principled manner. Providing a\nconcatenation of features in an arbitrary order may lead to the learning of\nspurious patterns or biases that do not actually exist. Another complication is\nintroduced if the number of features varies between each set. We propose\nconvolutional deep averaging networks (CDANs) for classifying and learning\nrepresentations of datasets whose instances comprise variable-size, unordered\nfeature sets. CDANs are efficient, permutation-invariant, and capable of\naccepting sets of arbitrary size. We emphasize the importance of nonlinear\nfeature embeddings for obtaining effective CDAN classifiers and illustrate\ntheir advantages in experiments versus linear embeddings and alternative\npermutation-invariant and -equivariant architectures.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 00:03:37 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Gardner", "Andrew", ""], ["Kanno", "Jinko", ""], ["Duncan", "Christian A.", ""], ["Selmic", "Rastko R.", ""]]}, {"id": "1709.03082", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and\n  Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data", "comments": "5 pages, 4 figures, 5 tables, accepted paper at the International\n  Conference on Machine Learning and Computing (ICMLC) 2018", "journal-ref": null, "doi": "10.1145/3195106.3195117", "report-no": null, "categories": "cs.NE cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gated Recurrent Unit (GRU) is a recently-developed variation of the long\nshort-term memory (LSTM) unit, both of which are types of recurrent neural\nnetwork (RNN). Through empirical evidence, both models have been proven to be\neffective in a wide variety of machine learning tasks such as natural language\nprocessing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and\ntext classification (Yang et al., 2016). Conventionally, like most neural\nnetworks, both of the aforementioned RNN variants employ the Softmax function\nas its final output layer for its prediction, and the cross-entropy function\nfor computing its loss. In this paper, we present an amendment to this norm by\nintroducing linear support vector machine (SVM) as the replacement for Softmax\nin the final output layer of a GRU model. Furthermore, the cross-entropy\nfunction shall be replaced with a margin-based function. While there have been\nsimilar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal is\nprimarily intended for binary classification on intrusion detection using the\n2013 network traffic data from the honeypot systems of Kyoto University.\nResults show that the GRU-SVM model performs relatively higher than the\nconventional GRU-Softmax model. The proposed model reached a training accuracy\nof ~81.54% and a testing accuracy of ~84.15%, while the latter was able to\nreach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In\naddition, the juxtaposition of these two final output layers indicate that the\nSVM would outperform Softmax in prediction time - a theoretical implication\nwhich was supported by the actual training and testing time in the study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 10:43:09 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 06:17:37 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 10:40:13 GMT"}, {"version": "v4", "created": "Sat, 7 Oct 2017 07:01:11 GMT"}, {"version": "v5", "created": "Wed, 25 Oct 2017 02:15:07 GMT"}, {"version": "v6", "created": "Thu, 28 Dec 2017 18:55:35 GMT"}, {"version": "v7", "created": "Sat, 10 Mar 2018 05:50:54 GMT"}, {"version": "v8", "created": "Thu, 7 Feb 2019 06:38:08 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1709.03159", "submitter": "Hardik Goel", "authors": "Hardik Goel, Igor Melnyk, Arindam Banerjee", "title": "R2N2: Residual Recurrent Neural Networks for Multivariate Time Series\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time-series modeling and forecasting is an important problem\nwith numerous applications. Traditional approaches such as VAR (vector\nauto-regressive) models and more recent approaches such as RNNs (recurrent\nneural networks) are indispensable tools in modeling time-series data. In many\nmultivariate time series modeling problems, there is usually a significant\nlinear dependency component, for which VARs are suitable, and a nonlinear\ncomponent, for which RNNs are suitable. Modeling such times series with only\nVAR or only RNNs can lead to poor predictive performance or complex models with\nlarge training times. In this work, we propose a hybrid model called R2N2\n(Residual RNN), which first models the time series with a simple linear model\n(like VAR) and then models its residual errors using RNNs. R2N2s can be trained\nusing existing algorithms for VARs and RNNs. Through an extensive empirical\nevaluation on two real world datasets (aviation and climate domains), we show\nthat R2N2 is competitive, usually better than VAR or RNN, used alone. We also\nshow that R2N2 is faster to train as compared to an RNN, while requiring less\nnumber of hidden units.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 19:29:49 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Goel", "Hardik", ""], ["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1709.03162", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Bayesian bandits: balancing the exploration-exploitation tradeoff via\n  double sampling", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning studies how to balance exploration and exploitation in\nreal-world systems, optimizing interactions with the world while simultaneously\nlearning how the world operates. One general class of algorithms for such\nlearning is the multi-armed bandit setting. Randomized probability matching,\nbased upon the Thompson sampling approach introduced in the 1930s, has recently\nbeen shown to perform well and to enjoy provable optimality properties. It\npermits generative, interpretable modeling in a Bayesian setting, where prior\nknowledge is incorporated, and the computed posteriors naturally capture the\nfull state of knowledge. In this work, we harness the information contained in\nthe Bayesian posterior and estimate its sufficient statistics via sampling. In\nseveral application domains, for example in health and medicine, each\ninteraction with the world can be expensive and invasive, whereas drawing\nsamples from the model is relatively inexpensive. Exploiting this viewpoint, we\ndevelop a double sampling technique driven by the uncertainty in the learning\nprocess: it favors exploitation when certain about the properties of each arm,\nexploring otherwise. The proposed algorithm does not make any distributional\nassumption and it is applicable to complex reward distributions, as long as\nBayesian posterior updates are computable. Utilizing the estimated posterior\nsufficient statistics, double sampling autonomously balances the\nexploration-exploitation tradeoff to make better informed decisions. We\nempirically show its reduced cumulative regret when compared to\nstate-of-the-art alternatives in representative bandit settings.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 19:58:34 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 20:20:27 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1709.03163", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Variational inference for the multi-armed contextual bandit", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics, PMLR 84:698-706, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 19:58:44 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 20:16:40 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 20:40:32 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1709.03183", "submitter": "Jiaming Xu", "authors": "Jiaming Xu", "title": "Rates of Convergence of Spectral Methods for Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating the grahpon model - the\nunderlying generating mechanism of a network. Graphon estimation arises in many\napplications such as predicting missing links in networks and learning user\npreferences in recommender systems. The graphon model deals with a random graph\nof $n$ vertices such that each pair of two vertices $i$ and $j$ are connected\nindependently with probability $\\rho \\times f(x_i,x_j)$, where $x_i$ is the\nunknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric\nfunction, and $\\rho$ is a scaling parameter characterizing the graph sparsity.\nRecent studies have identified the minimax error rate of estimating the graphon\nfrom a single realization of the random graph. However, there exists a wide gap\nbetween the known error rates of computationally efficient estimation\nprocedures and the minimax optimal error rate.\n  Here we analyze a spectral method, namely universal singular value\nthresholding (USVT) algorithm, in the relatively sparse regime with the average\nvertex degree $n\\rho=\\Omega(\\log n)$. When $f$ belongs to H\\\"{o}lder or Sobolev\nspace with smoothness index $\\alpha$, we show the error rate of USVT is at most\n$(n\\rho)^{ -2 \\alpha / (2\\alpha+d)}$, approaching the minimax optimal error\nrate $\\log (n\\rho)/(n\\rho)$ for $d=1$ as $\\alpha$ increases. Furthermore, when\n$f$ is analytic, we show the error rate of USVT is at most $\\log^d\n(n\\rho)/(n\\rho)$. In the special case of stochastic block model with $k$\nblocks, the error rate of USVT is at most $k/(n\\rho)$, which is larger than the\nminimax optimal error rate by at most a multiplicative factor $k/\\log k$. This\ncoincides with the computational gap observed for community detection. A key\nstep of our analysis is to derive the eigenvalue decaying rate of the edge\nprobability matrix using piecewise polynomial approximations of the graphon\nfunction $f$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 21:45:48 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Xu", "Jiaming", ""]]}, {"id": "1709.03202", "submitter": "Taewan Kim", "authors": "Taewan Kim, Joydeep Ghosh", "title": "Semi-Supervised Active Clustering with Weak Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised active clustering (SSAC) utilizes the knowledge of a domain\nexpert to cluster data points by interactively making pairwise \"same-cluster\"\nqueries. However, it is impractical to ask human oracles to answer every\npairwise query. In this paper, we study the influence of allowing \"not-sure\"\nanswers from a weak oracle and propose algorithms to efficiently handle\nuncertainties. Different types of model assumptions are analyzed to cover\nrealistic scenarios of oracle abstraction. In the first model, random-weak\noracle, an oracle randomly abstains with a certain probability. We also\nproposed two distance-weak oracle models which simulate the case of getting\nconfused based on the distance between two points in a pairwise query. For each\nweak oracle model, we show that a small query complexity is adequate for the\neffective $k$ means clustering with high probability. Sufficient conditions for\nthe guarantee include a $\\gamma$-margin property of the data, and an existence\nof a point close to each cluster center. Furthermore, we provide a sample\ncomplexity with a reduced effect of the cluster's margin and only a logarithmic\ndependency on the data dimension. Our results allow significantly less number\nof same-cluster queries if the margin of the clusters is tight, i.e. $\\gamma\n\\approx 1$. Experimental results on synthetic data show the effective\nperformance of our approach in overcoming uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 00:44:17 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Kim", "Taewan", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1709.03239", "submitter": "Xuan Peng", "authors": "Xuan Peng, Xunzhang Gao, Xiang Li", "title": "On better training the infinite restricted Boltzmann machines", "comments": "Submitted to Machine Learning", "journal-ref": null, "doi": "10.1007/s10994-018-5696-2", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinite restricted Boltzmann machine (iRBM) is an extension of the\nclassic RBM. It enjoys a good property of automatically deciding the size of\nthe hidden layer according to specific training data. With sufficient training,\nthe iRBM can achieve a competitive performance with that of the classic RBM.\nHowever, the convergence of learning the iRBM is slow, due to the fact that the\niRBM is sensitive to the ordering of its hidden units, the learned filters\nchange slowly from the left-most hidden unit to right. To break this dependency\nbetween neighboring hidden units and speed up the convergence of training, a\nnovel training strategy is proposed. The key idea of the proposed training\nstrategy is randomly regrouping the hidden units before each gradient descent\nstep. Potentially, a mixing of infinite many iRBMs with different permutations\nof the hidden units can be achieved by this learning method, which has a\nsimilar effect of preventing the model from over-fitting as the dropout. The\noriginal iRBM is also modified to be capable of carrying out discriminative\ntraining. To evaluate the impact of our method on convergence speed of learning\nand the model's generalization ability, several experiments have been performed\non the binarized MNIST and CalTech101 Silhouettes datasets. Experimental\nresults indicate that the proposed training strategy can greatly accelerate\nlearning and enhance generalization ability of iRBMs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 04:41:06 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 19:05:55 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Peng", "Xuan", ""], ["Gao", "Xunzhang", ""], ["Li", "Xiang", ""]]}, {"id": "1709.03252", "submitter": "Ehsan Arbabi", "authors": "Ehsan Arbabi and Mohammad Bagher Shamsollahi", "title": "Evaluation of Classical Features and Classifiers in Brain-Computer\n  Interface Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) uses brain signals in order to provide a new\nmethod for communication between human and outside world. Feature extraction,\nselection and classification are among the main matters of concerns in signal\nprocessing stage of BCI. In this article, we present our findings about the\nmost effective features and classifiers in some brain tasks. Six different\ngroups of classical features and twelve classifiers have been examined in nine\ndatasets of brain signal. The results indicate that energy of brain signals in\n{\\alpha} and \\b{eta} frequency bands, together with some statistical parameters\nare more effective, comparing to the other types of extracted features. In\naddition, Bayesian classifier with Gaussian distribution assumption and also\nSupport Vector Machine (SVM) show to classify different BCI datasets more\naccurately than the other classifiers. We believe that the results can give an\ninsight about a strategy for blind classification of brain signals in\nbrain-computer interface.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 05:57:07 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 07:42:32 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Arbabi", "Ehsan", ""], ["Shamsollahi", "Mohammad Bagher", ""]]}, {"id": "1709.03423", "submitter": "Andrej Junginger", "authors": "Thilo Strauss, Markus Hanselmann, Andrej Junginger, Holger Ulmer", "title": "Ensemble Methods as a Defense to Adversarial Perturbations Against Deep\n  Neural Networks", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the state of the art approach in many machine\nlearning problems such as classification. It has recently been shown that deep\nlearning is highly vulnerable to adversarial perturbations. Taking the camera\nsystems of self-driving cars as an example, small adversarial perturbations can\ncause the system to make errors in important tasks, such as classifying traffic\nsigns or detecting pedestrians. Hence, in order to use deep learning without\nsafety concerns a proper defense strategy is required. We propose to use\nensemble methods as a defense strategy against adversarial perturbations. We\nfind that an attack leading one model to misclassify does not imply the same\nfor other networks performing the same task. This makes ensemble methods an\nattractive defense strategy against adversarial attacks. We empirically show\nfor the MNIST and the CIFAR-10 data sets that ensemble methods not only improve\nthe accuracy of neural networks on test data but also increase their robustness\nagainst adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:01:03 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 08:48:03 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Strauss", "Thilo", ""], ["Hanselmann", "Markus", ""], ["Junginger", "Andrej", ""], ["Ulmer", "Holger", ""]]}, {"id": "1709.03473", "submitter": "Andrii Babii", "authors": "Andrii Babii and Jean-Pierre Florens", "title": "Is completeness necessary? Estimation in nonidentified linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper documents the consequences of the identification failures for a\nclass of linear ill-posed inverse models. The Tikhonov-regularized estimator\nconverges to a well-defined limit equal to the best approximation of the\nstructural parameter in the orthogonal complement to the null space of the\noperator. We illustrate that in many cases the best approximation may coincide\nwith the structural parameter or at least may reasonably approximate it. We\ncharacterize the nonasymptotic Hilbert space norm and the uniform norm\nconvergence rates for the best approximation. Nonidentification has important\nimplications for the large sample distribution of the Tikhonov-regularized\nestimator, and we document the transition between the Gaussian and the weighted\nchi-squared limits. The theoretical results are illustrated for the\nnonparametric IV and the functional linear IV regressions and are further\nsupported by the Monte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 16:49:19 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:06:04 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 20:54:50 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Babii", "Andrii", ""], ["Florens", "Jean-Pierre", ""]]}, {"id": "1709.03528", "submitter": "Shusen Wang", "authors": "Shusen Wang, Farbod Roosta-Khorasani, Peng Xu and Michael W. Mahoney", "title": "GIANT: Globally Improved Approximate Newton Method for Distributed\n  Optimization", "comments": "Fixed some typos. Improved writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For distributed computing environment, we consider the empirical risk\nminimization problem and propose a distributed and communication-efficient\nNewton-type optimization method. At every iteration, each worker locally finds\nan Approximate NewTon (ANT) direction, which is sent to the main driver. The\nmain driver, then, averages all the ANT directions received from workers to\nform a {\\it Globally Improved ANT} (GIANT) direction. GIANT is highly\ncommunication efficient and naturally exploits the trade-offs between local\ncomputations and global communications in that more local computations result\nin fewer overall rounds of communications. Theoretically, we show that GIANT\nenjoys an improved convergence rate as compared with first-order methods and\nexisting distributed Newton-type methods. Further, and in sharp contrast with\nmany existing distributed Newton-type methods, as well as popular first-order\nmethods, a highly advantageous practical feature of GIANT is that it only\ninvolves one tuning parameter. We conduct large-scale experiments on a computer\ncluster and, empirically, demonstrate the superior performance of GIANT.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 18:17:18 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 17:47:15 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 21:45:02 GMT"}, {"version": "v4", "created": "Fri, 18 May 2018 21:05:37 GMT"}, {"version": "v5", "created": "Tue, 11 Sep 2018 15:12:01 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wang", "Shusen", ""], ["Roosta-Khorasani", "Farbod", ""], ["Xu", "Peng", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1709.03615", "submitter": "Kitty Mohammed", "authors": "Kitty Mohammed and Hariharan Narayanan", "title": "Manifold Learning Using Kernel Density Estimation and Local Principal\n  Components Analysis", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of recovering a $d-$dimensional manifold $\\mathcal{M}\n\\subset \\mathbb{R}^n$ when provided with noiseless samples from $\\mathcal{M}$.\nThere are many algorithms (e.g., Isomap) that are used in practice to fit\nmanifolds and thus reduce the dimensionality of a given data set. Ideally, the\nestimate $\\mathcal{M}_\\mathrm{put}$ of $\\mathcal{M}$ should be an actual\nmanifold of a certain smoothness; furthermore, $\\mathcal{M}_\\mathrm{put}$\nshould be arbitrarily close to $\\mathcal{M}$ in Hausdorff distance given a\nlarge enough sample. Generally speaking, existing manifold learning algorithms\ndo not meet these criteria. Fefferman, Mitter, and Narayanan (2016) have\ndeveloped an algorithm whose output is provably a manifold. The key idea is to\ndefine an approximate squared-distance function (asdf) to $\\mathcal{M}$. Then,\n$\\mathcal{M}_\\mathrm{put}$ is given by the set of points where the gradient of\nthe asdf is orthogonal to the subspace spanned by the largest $n - d$\neigenvectors of the Hessian of the asdf. As long as the asdf meets certain\nregularity conditions, $\\mathcal{M}_\\mathrm{put}$ is a manifold that is\narbitrarily close in Hausdorff distance to $\\mathcal{M}$. In this paper, we\ndefine two asdfs that can be calculated from the data and show that they meet\nthe required regularity conditions. The first asdf is based on kernel density\nestimation, and the second is based on estimation of tangent spaces using local\nprincipal components analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 22:45:10 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Mohammed", "Kitty", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "1709.03625", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Elias\n  Bareinboim", "title": "Budgeted Experiment Design for Causal Structure Learning", "comments": null, "journal-ref": "35th International Conference on Machine Learning (ICML), PMLR\n  80:1719-1728, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of causal structure learning when the experimenter is\nlimited to perform at most $k$ non-adaptive experiments of size $1$. We\nformulate the problem of finding the best intervention target set as an\noptimization problem, which aims to maximize the average number of edges whose\ndirections are resolved. We prove that the corresponding objective function is\nsubmodular and a greedy algorithm suffices to achieve\n$(1-\\frac{1}{e})$-approximation of the optimal value. We further present an\naccelerated variant of the greedy algorithm, which can lead to orders of\nmagnitude performance speedup. We validate our proposed approach on synthetic\nand real graphs. The results show that compared to the purely observational\nsetting, our algorithm orients the majority of the edges through a considerably\nsmall number of interventions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 23:43:30 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 21:56:06 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1709.03645", "submitter": "Tao Yang", "authors": "Tao Yang, Paul Thompson, Sihai Zhao, Jieping Ye", "title": "Identifying Genetic Risk Factors via Sparse Group Lasso with Group Graph\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWA studies or GWAS) investigate the\nrelationships between genetic variants such as single-nucleotide polymorphisms\n(SNPs) and individual traits. Recently, incorporating biological priors\ntogether with machine learning methods in GWA studies has attracted increasing\nattention. However, in real-world, nucleotide-level bio-priors have not been\nwell-studied to date. Alternatively, studies at gene-level, for example,\nprotein--protein interactions and pathways, are more rigorous and legitimate,\nand it is potentially beneficial to utilize such gene-level priors in GWAS. In\nthis paper, we proposed a novel two-level structured sparse model, called\nSparse Group Lasso with Group-level Graph structure (SGLGG), for GWAS. It can\nbe considered as a sparse group Lasso along with a group-level graph Lasso.\nEssentially, SGLGG penalizes the nucleotide-level sparsity as well as takes\nadvantages of gene-level priors (both gene groups and networks), to identifying\nphenotype-associated risk SNPs. We employ the alternating direction method of\nmultipliers algorithm to optimize the proposed model. Our experiments on the\nAlzheimer's Disease Neuroimaging Initiative whole genome sequence data and\nneuroimage data demonstrate the effectiveness of SGLGG. As a regression model,\nit is competitive to the state-of-the-arts sparse models; as a variable\nselection method, SGLGG is promising for identifying Alzheimer's\ndisease-related risk SNPs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 01:34:50 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Yang", "Tao", ""], ["Thompson", "Paul", ""], ["Zhao", "Sihai", ""], ["Ye", "Jieping", ""]]}, {"id": "1709.03658", "submitter": "Szu-Wei Fu", "authors": "Szu-Wei Fu, Tao-Wei Wang, Yu Tsao, Xugang Lu, and Hisashi Kawai", "title": "End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics\n  Optimization by Fully Convolutional Neural Networks", "comments": "Accepted in IEEE Transactions on Audio, Speech and Language\n  Processing (TASLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech enhancement model is used to map a noisy speech to a clean speech. In\nthe training stage, an objective function is often adopted to optimize the\nmodel parameters. However, in most studies, there is an inconsistency between\nthe model optimization criterion and the evaluation criterion on the enhanced\nspeech. For example, in measuring speech intelligibility, most of the\nevaluation metric is based on a short-time objective intelligibility (STOI)\nmeasure, while the frame based minimum mean square error (MMSE) between\nestimated and clean speech is widely used in optimizing the model. Due to the\ninconsistency, there is no guarantee that the trained model can provide optimal\nperformance in applications. In this study, we propose an end-to-end\nutterance-based speech enhancement framework using fully convolutional neural\nnetworks (FCN) to reduce the gap between the model optimization and evaluation\ncriterion. Because of the utterance-based optimization, temporal correlation\ninformation of long speech segments, or even at the entire utterance level, can\nbe considered when perception-based objective functions are used for the direct\noptimization. As an example, we implement the proposed FCN enhancement\nframework to optimize the STOI measure. Experimental results show that the STOI\nof test speech is better than conventional MMSE-optimized speech due to the\nconsistency between the training and evaluation target. Moreover, by\nintegrating the STOI in model optimization, the intelligibility of human\nsubjects and automatic speech recognition (ASR) system on the enhanced speech\nis also substantially improved compared to those generated by the MMSE\ncriterion.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 02:24:50 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 10:19:56 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Fu", "Szu-Wei", ""], ["Wang", "Tao-Wei", ""], ["Tsao", "Yu", ""], ["Lu", "Xugang", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1709.03670", "submitter": "Kangwook Lee", "authors": "Kwangjun Ahn, Kangwook Lee, Changho Suh", "title": "Community Recovery in Hypergraphs", "comments": "25 pages, 7 figures. Submitted to IEEE Transacations on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community recovery is a central problem that arises in a wide variety of\napplications such as network clustering, motion segmentation, face clustering\nand protein complex detection. The objective of the problem is to cluster data\npoints into distinct communities based on a set of measurements, each of which\nis associated with the values of a certain number of data points. While most of\nthe prior works focus on a setting in which the number of data points involved\nin a measurement is two, this work explores a generalized setting in which the\nnumber can be more than two. Motivated by applications particularly in machine\nlearning and channel coding, we consider two types of measurements: (1)\nhomogeneity measurement which indicates whether or not the associated data\npoints belong to the same community; (2) parity measurement which denotes the\nmodulo-2 sum of the values of the data points. Such measurements are possibly\ncorrupted by Bernoulli noise. We characterize the fundamental limits on the\nnumber of measurements required to reconstruct the communities for the\nconsidered models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 03:08:33 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Ahn", "Kwangjun", ""], ["Lee", "Kangwook", ""], ["Suh", "Changho", ""]]}, {"id": "1709.03683", "submitter": "Yan Zhao", "authors": "Yan Zhao, Xiao Fang, David Simchi-Levi", "title": "A Practically Competitive and Provably Consistent Algorithm for Uplift\n  Modeling", "comments": "Accepted by 2017 IEEE International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments have been critical tools of decision making for\ndecades. However, subjects can show significant heterogeneity in response to\ntreatments in many important applications. Therefore it is not enough to simply\nknow which treatment is optimal for the entire population. What we need is a\nmodel that correctly customize treatment assignment base on subject\ncharacteristics. The problem of constructing such models from randomized\nexperiments data is known as Uplift Modeling in the literature. Many algorithms\nhave been proposed for uplift modeling and some have generated promising\nresults on various data sets. Yet little is known about the theoretical\nproperties of these algorithms. In this paper, we propose a new tree-based\nensemble algorithm for uplift modeling. Experiments show that our algorithm can\nachieve competitive results on both synthetic and industry-provided data. In\naddition, by properly tuning the \"node size\" parameter, our algorithm is proved\nto be consistent under mild regularity conditions. This is the first consistent\nalgorithm for uplift modeling that we are aware of.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 03:49:57 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Zhao", "Yan", ""], ["Fang", "Xiao", ""], ["Simchi-Levi", "David", ""]]}, {"id": "1709.03698", "submitter": "Bo Chang", "authors": "Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert and\n  Elliot Holtham", "title": "Reversible Architectures for Arbitrarily Deep Residual Neural Networks", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep residual networks have been successfully applied in many\ncomputer vision and natural language processing tasks, pushing the\nstate-of-the-art performance with deeper and wider architectures. In this work,\nwe interpret deep residual networks as ordinary differential equations (ODEs),\nwhich have long been studied in mathematics and physics with rich theoretical\nand empirical success. From this interpretation, we develop a theoretical\nframework on stability and reversibility of deep neural networks, and derive\nthree reversible neural network architectures that can go arbitrarily deep in\ntheory. The reversibility property allows a memory-efficient implementation,\nwhich does not need to store the activations for most hidden layers. Together\nwith the stability of our architectures, this enables training deeper networks\nusing only modest computational resources. We provide both theoretical analyses\nand empirical results. Experimental results demonstrate the efficacy of our\narchitectures against several strong baselines on CIFAR-10, CIFAR-100 and\nSTL-10 with superior or on-par state-of-the-art performance. Furthermore, we\nshow our architectures yield superior results when trained using fewer training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 05:41:13 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 22:10:57 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chang", "Bo", ""], ["Meng", "Lili", ""], ["Haber", "Eldad", ""], ["Ruthotto", "Lars", ""], ["Begert", "David", ""], ["Holtham", "Elliot", ""]]}, {"id": "1709.03741", "submitter": "Junying Li", "authors": "Junying Li, Deng Cai, Xiaofei He", "title": "Learning Graph-Level Representation for Drug Discovery", "comments": "arXiv admin note: text overlap with arXiv:1703.00564,\n  arXiv:1611.03199 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicating macroscopic influences of drugs on human body, like efficacy and\ntoxicity, is a central problem of small-molecule based drug discovery.\nMolecules can be represented as an undirected graph, and we can utilize graph\nconvolution networks to predication molecular properties. However, graph\nconvolutional networks and other graph neural networks all focus on learning\nnode-level representation rather than graph-level representation. Previous\nworks simply sum all feature vectors for all nodes in the graph to obtain the\ngraph feature vector for drug predication. In this paper, we introduce a dummy\nsuper node that is connected with all nodes in the graph by a directed edge as\nthe representation of the graph and modify the graph operation to help the\ndummy super node learn graph-level feature. Thus, we can handle graph-level\nclassification and regression in the same way as node-level classification and\nregression. In addition, we apply focal loss to address class imbalance in drug\ndatasets. The experiments on MoleculeNet show that our method can effectively\nimprove the performance of molecular properties predication.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:41:39 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 03:21:59 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Li", "Junying", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1709.03768", "submitter": "Dacheng Tao", "authors": "Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, Dacheng Tao", "title": "Learning with Bounded Instance- and Label-dependent Label Noise", "comments": "Published in the International Conference on Machine Learning (ICML),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance- and Label-dependent label Noise (ILN) widely exists in real-world\ndatasets but has been rarely studied. In this paper, we focus on Bounded\nInstance- and Label-dependent label Noise (BILN), a particular case of ILN\nwhere the label noise rates -- the probabilities that the true labels of\nexamples flip into the corrupted ones -- have upper bound less than $1$.\nSpecifically, we introduce the concept of distilled examples, i.e. examples\nwhose labels are identical with the labels assigned for them by the Bayes\noptimal classifier, and prove that under certain conditions classifiers learnt\non distilled examples will converge to the Bayes optimal classifier. Inspired\nby the idea of learning with distilled examples, we then propose a learning\nalgorithm with theoretical guarantees for its robustness to BILN. At last,\nempirical evaluations on both synthetic and real-world datasets show\neffectiveness of our algorithm in learning with BILN.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:59:59 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 23:18:29 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 03:38:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cheng", "Jiacheng", ""], ["Liu", "Tongliang", ""], ["Ramamohanarao", "Kotagiri", ""], ["Tao", "Dacheng", ""]]}, {"id": "1709.03831", "submitter": "Tu Dinh Nguyen", "authors": "Tu Dinh Nguyen, Trung Le, Hung Vu, Dinh Phung", "title": "Dual Discriminator Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a novel approach to tackle the problem of mode\ncollapse encountered in generative adversarial network (GAN). Our idea is\nintuitive but proven to be very effective, especially in addressing some key\nlimitations of GAN. In essence, it combines the Kullback-Leibler (KL) and\nreverse KL divergences into a unified objective function, thus it exploits the\ncomplementary statistical properties from these divergences to effectively\ndiversify the estimated density in capturing multi-modes. We term our method\ndual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has\ntwo discriminators; and together with a generator, it also has the analogy of a\nminimax game, wherein a discriminator rewards high scores for samples from data\ndistribution whilst another discriminator, conversely, favoring data from the\ngenerator, and the generator produces data to fool both two discriminators. We\ndevelop theoretical analysis to show that, given the maximal discriminators,\noptimizing the generator of D2GAN reduces to minimizing both KL and reverse KL\ndivergences between data distribution and the distribution induced from the\ndata generated by the generator, hence effectively avoiding the mode collapsing\nproblem. We conduct extensive experiments on synthetic and real-world\nlarge-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made\nour best effort to compare our D2GAN with the latest state-of-the-art GAN's\nvariants in comprehensive qualitative and quantitative evaluations. The\nexperimental results demonstrate the competitive and superior performance of\nour approach in generating good quality and diverse samples over baselines, and\nthe capability of our method to scale up to ImageNet database.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 13:28:48 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Nguyen", "Tu Dinh", ""], ["Le", "Trung", ""], ["Vu", "Hung", ""], ["Phung", "Dinh", ""]]}, {"id": "1709.03891", "submitter": "Sijie He", "authors": "Jamal Golmohammadi, Imme Ebert-Uphoff, Sijie He, Yi Deng and Arindam\n  Banerjee", "title": "High-Dimensional Dependency Structure Learning for Physical Processes", "comments": "21 pages, 8 figures, International Conference on Data Mining 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the use of structure learning methods for\nprobabilistic graphical models to identify statistical dependencies in\nhigh-dimensional physical processes. Such processes are often synthetically\ncharacterized using PDEs (partial differential equations) and are observed in a\nvariety of natural phenomena, including geoscience data capturing atmospheric\nand hydrological phenomena. Classical structure learning approaches such as the\nPC algorithm and variants are challenging to apply due to their high\ncomputational and sample requirements. Modern approaches, often based on sparse\nregression and variants, do come with finite sample guarantees, but are usually\nhighly sensitive to the choice of hyper-parameters, e.g., parameter $\\lambda$\nfor sparsity inducing constraint or regularization. In this paper, we present\nACLIME-ADMM, an efficient two-step algorithm for adaptive structure learning,\nwhich estimates an edge specific parameter $\\lambda_{ij}$ in the first step,\nand uses these parameters to learn the structure in the second step. Both steps\nof our algorithm use (inexact) ADMM to solve suitable linear programs, and all\niterations can be done in closed form in an efficient block parallel manner. We\ncompare ACLIME-ADMM with baselines on both synthetic data simulated by partial\ndifferential equations (PDEs) that model advection-diffusion processes, and\nreal data (50 years) of daily global geopotential heights to study information\nflow in the atmosphere. ACLIME-ADMM is shown to be efficient, stable, and\ncompetitive, usually better than the baselines especially on difficult\nproblems. On real data, ACLIME-ADMM recovers the underlying structure of global\natmospheric circulation, including switches in wind directions at the equator\nand tropics entirely from the data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:54:57 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Golmohammadi", "Jamal", ""], ["Ebert-Uphoff", "Imme", ""], ["He", "Sijie", ""], ["Deng", "Yi", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1709.03907", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang, Alexander Rakhlin", "title": "Weighted Message Passing and Minimum Energy Flow for Heterogeneous\n  Stochastic Block Models with Side Information", "comments": "31 pages, 1 figures", "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-34", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the misclassification error for community detection in general\nheterogeneous stochastic block models (SBM) with noisy or partial label\ninformation. We establish a connection between the misclassification rate and\nthe notion of minimum energy on the local neighborhood of the SBM. We develop\nan optimally weighted message passing algorithm to reconstruct labels for SBM\nbased on the minimum energy flow and the eigenvectors of a certain Markov\ntransition matrix. The general SBM considered in this paper allows for\nunequal-size communities, degree heterogeneity, and different connection\nprobabilities among blocks. We focus on how to optimally weigh the message\npassing to improve misclassification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:22:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1709.03943", "submitter": "Erik Bartos", "authors": "Kabin Kanjamapornkul, Richard Pin\\v{c}\\'ak, Sanphet Chunithpaisan,\n  Erik Barto\\v{s}", "title": "Support Spinor Machine", "comments": "18 pages, 12 figures, 6 tables", "journal-ref": "Digital Signal Processing 70 (2017) 59-72", "doi": "10.1016/j.dsp.2017.07.023", "report-no": null, "categories": "cs.LG eess.SP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize a support vector machine to a support spinor machine by using\nthe mathematical structure of wedge product over vector machine in order to\nextend field from vector field to spinor field. The separated hyperplane is\nextended to Kolmogorov space in time series data which allow us to extend a\nstructure of support vector machine to a support tensor machine and a support\ntensor machine moduli space. Our performance test on support spinor machine is\ndone over one class classification of end point in physiology state of time\nseries data after empirical mode analysis and compared with support vector\nmachine test. We implement algorithm of support spinor machine by using\nHolo-Hilbert amplitude modulation for fully nonlinear and nonstationary time\nseries data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 07:23:57 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Kanjamapornkul", "Kabin", ""], ["Pin\u010d\u00e1k", "Richard", ""], ["Chunithpaisan", "Sanphet", ""], ["Barto\u0161", "Erik", ""]]}, {"id": "1709.04004", "submitter": "Xueying Guo", "authors": "Huasen Wu, Xueying Guo, Xin Liu", "title": "Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits", "comments": "In Proceedings of the 35th International Conference on Machine\n  Learning (ICML), 2018, pp. 5306-5314, Stockholmsm\\\"assan, Stockholm Sweden,\n  ICML 2018. (PMLR 80:5306-5314)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study opportunistic bandits - a new variant of\nbandits where the regret of pulling a suboptimal arm varies under different\nenvironmental conditions, such as network load or produce price. When the\nload/price is low, so is the cost/regret of pulling a suboptimal arm (e.g.,\ntrying a suboptimal network configuration). Therefore, intuitively, we could\nexplore more when the load/price is low and exploit more when the load/price is\nhigh. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound\n(AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff\nfor opportunistic bandits. We prove that AdaUCB achieves $O(\\log T)$ regret\nwith a smaller coefficient than the traditional UCB algorithm. Furthermore,\nAdaUCB achieves $O(1)$ regret with respect to $T$ if the exploration cost is\nzero when the load level is below a certain threshold. Last, based on both\nsynthetic data and real-world traces, experimental results show that AdaUCB\nsignificantly outperforms other bandit algorithms, such as UCB and TS (Thompson\nSampling), under large load/price fluctuations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 18:18:33 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 18:38:12 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Wu", "Huasen", ""], ["Guo", "Xueying", ""], ["Liu", "Xin", ""]]}, {"id": "1709.04024", "submitter": "Hyeji Kim", "authors": "Hyeji Kim, Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath", "title": "Discovering Potential Correlations via Hypercontractivity", "comments": "30 pages, 19 figures, accepted for publication in the 31st Conference\n  on Neural Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": "10.3390/e19110586", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering a correlation from one variable to another variable is of\nfundamental scientific and practical interest. While existing correlation\nmeasures are suitable for discovering average correlation, they fail to\ndiscover hidden or potential correlations. To bridge this gap, (i) we postulate\na set of natural axioms that we expect a measure of potential correlation to\nsatisfy; (ii) we show that the rate of information bottleneck, i.e., the\nhypercontractivity coefficient, satisfies all the proposed axioms; (iii) we\nprovide a novel estimator to estimate the hypercontractivity coefficient from\nsamples; and (iv) we provide numerical experiments demonstrating that this\nproposed estimator discovers potential correlations among various indicators of\nWHO datasets, is robust in discovering gene interactions from gene expression\ntime series data, and is statistically more powerful than the estimators for\nother correlation measures in binary hypothesis testing of canonical examples\nof potential correlations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 19:07:32 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 19:27:19 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 21:18:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kim", "Hyeji", ""], ["Gao", "Weihao", ""], ["Kannan", "Sreeram", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1709.04054", "submitter": "Lars Eidnes", "authors": "Lars Eidnes, Arild N{\\o}kland", "title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple extension to the ReLU-family of activation functions that\nallows them to shift the mean activation across a layer towards zero. Combined\nwith proper weight initialization, this alleviates the need for normalization\nlayers. We explore the training of deep vanilla recurrent neural networks\n(RNNs) with up to 144 layers, and show that bipolar activation functions help\nlearning in this setting. On the Penn Treebank and Text8 language modeling\ntasks we obtain competitive results, improving on the best reported results for\nnon-gated networks. In experiments with convolutional neural networks without\nbatch normalization, we find that bipolar activations produce a faster drop in\ntraining error, and results in a lower test error on the CIFAR-10\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 20:44:15 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 20:18:47 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:59:24 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Eidnes", "Lars", ""], ["N\u00f8kland", "Arild", ""]]}, {"id": "1709.04072", "submitter": "Tao Sun", "authors": "Tao Sun, Hao Jiang, Lizhi Cheng, Wei Zhu", "title": "A convergence framework for inexact nonconvex and nonsmooth algorithms\n  and its applications to several iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the convergence of an abstract inexact nonconvex\nand nonsmooth algorithm. We promise a pseudo sufficient descent condition and a\npseudo relative error condition, which are both related to an auxiliary\nsequence, for the algorithm; and a continuity condition is assumed to hold. In\nfact, a lot of classical inexact nonconvex and nonsmooth algorithms allow these\nthree conditions. Under a special kind of summable assumption on the auxiliary\nsequence, we prove the sequence generated by the general algorithm converges to\na critical point of the objective function if being assumed Kurdyka-\nLojasiewicz property. The core of the proofs lies in building a new Lyapunov\nfunction, whose successive difference provides a bound for the successive\ndifference of the points generated by the algorithm. And then, we apply our\nfindings to several classical nonconvex iterative algorithms and derive the\ncorresponding convergence results\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:28:18 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 03:37:57 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 03:19:48 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 19:34:11 GMT"}, {"version": "v5", "created": "Fri, 10 Aug 2018 21:26:28 GMT"}, {"version": "v6", "created": "Wed, 28 Nov 2018 02:40:16 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Sun", "Tao", ""], ["Jiang", "Hao", ""], ["Cheng", "Lizhi", ""], ["Zhu", "Wei", ""]]}, {"id": "1709.04073", "submitter": "Chandrashekar Lakshminarayanan", "authors": "Chandrashekar Lakshminarayanan and Csaba Szepesv\\'ari", "title": "Linear Stochastic Approximation: Constant Step-Size and Iterate\n  Averaging", "comments": "16 pages, 2 figures, was submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)\nwith a constant step-size and the so called Polyak-Ruppert (PR) averaging of\niterates. LSAs are widely applied in machine learning and reinforcement\nlearning (RL), where the aim is to compute an appropriate $\\theta_{*} \\in\n\\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$\nupdates per iteration. In this paper, we are motivated by the problem (in RL)\nof policy evaluation from experience replay using the \\emph{temporal\ndifference} (TD) class of learning algorithms that are also LSAs. For LSAs with\na constant step-size, and PR averaging, we provide bounds for the mean squared\nerror (MSE) after $t$ iterations. We assume that data is \\iid with finite\nvariance (underlying distribution being $P$) and that the expected dynamics is\nHurwitz. For a given LSA with PR averaging, and data distribution $P$\nsatisfying the said assumptions, we show that there exists a range of constant\nstep-sizes such that its MSE decays as $O(\\frac{1}{t})$.\n  We examine the conditions under which a constant step-size can be chosen\nuniformly for a class of data distributions $\\mathcal{P}$, and show that not\nall data distributions `admit' such a uniform constant step-size. We also\nsuggest a heuristic step-size tuning algorithm to choose a constant step-size\nof a given LSA for a given data distribution $P$. We compare our results with\nrelated work and also discuss the implication of our results in the context of\nTD algorithms that are LSAs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:34:09 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Lakshminarayanan", "Chandrashekar", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1709.04077", "submitter": "Antoine Lesage-Landry", "authors": "Antoine Lesage-Landry and Joshua A. Taylor", "title": "Setpoint Tracking with Partially Observed Loads", "comments": null, "journal-ref": "IEEE Transactions on Power Systems, 32 (5): 5615-5627. September\n  2018", "doi": "10.1109/TPWRS.2018.2804353", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use online convex optimization (OCO) for setpoint tracking with uncertain,\nflexible loads. We consider full feedback from the loads, bandit feedback, and\ntwo intermediate types of feedback: partial bandit where a subset of the loads\nare individually observed and the rest are observed in aggregate, and Bernoulli\nfeedback where in each round the aggregator receives either full or bandit\nfeedback according to a known probability. We give sublinear regret bounds in\nall cases. We numerically evaluate our algorithms on examples with\nthermostatically controlled loads and electric vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:48:07 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 15:52:30 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lesage-Landry", "Antoine", ""], ["Taylor", "Joshua A.", ""]]}, {"id": "1709.04108", "submitter": "Ehsan Mohammady Ardehaly", "authors": "Ehsan Mohammady Ardehaly, Aron Culotta", "title": "Co-training for Demographic Classification Using Deep Learning from\n  Label Proportions", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2017.144", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have recently produced state-of-the-art accuracy in\nmany classification tasks, but this success is typically dependent on access to\nmany annotated training examples. For domains without such data, an attractive\nalternative is to train models with light, or distant supervision. In this\npaper, we introduce a deep neural network for the Learning from Label\nProportion (LLP) setting, in which the training data consist of bags of\nunlabeled instances with associated label distributions for each bag. We\nintroduce a new regularization layer, Batch Averager, that can be appended to\nthe last layer of any deep neural network to convert it from supervised\nlearning to LLP. This layer can be implemented readily with existing deep\nlearning packages. To further support domains in which the data consist of two\nconditionally independent feature views (e.g. image and text), we propose a\nco-training algorithm that iteratively generates pseudo bags and refits the\ndeep LLP model to improve classification accuracy. We demonstrate our models on\ndemographic attribute classification (gender and race/ethnicity), which has\nmany applications in social media analysis, public health, and marketing. We\nconduct experiments to predict demographics of Twitter users based on their\ntweets and profile image, without requiring any user-level annotations for\ntraining. We find that the deep LLP approach outperforms baselines for both\ntext and image features separately. Additionally, we find that co-training\nalgorithm improves image and text classification by 4% and 8% absolute F1,\nrespectively. Finally, an ensemble of text and image classifiers further\nimproves the absolute F1 measure by 4% on average.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 02:06:19 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Ardehaly", "Ehsan Mohammady", ""], ["Culotta", "Aron", ""]]}, {"id": "1709.04114", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi and Cho-Jui Hsieh", "title": "EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial\n  Examples", "comments": "To be published at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have highlighted the vulnerability of deep neural networks\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\ncan easily be crafted to cause a well-trained model to misclassify. Existing\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\nfor the total variation and encourages sparsity in the perturbation, little has\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\nformulate the process of attacking DNNs via adversarial examples as an\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\n(EAD) feature $L_1$-oriented adversarial examples and include the\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\nexamples with small $L_1$ distortion and attains similar attack performance to\nthe state-of-the-art methods in different attack scenarios. More importantly,\nEAD leads to improved attack transferability and complements adversarial\ntraining for DNNs, suggesting novel insights on leveraging $L_1$ distortion in\nadversarial machine learning and security implications of DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 02:40:59 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 19:59:47 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 04:49:12 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Sharma", "Yash", ""], ["Zhang", "Huan", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1709.04135", "submitter": "Xiaogang Su", "authors": "Xiaogang Su, Yaa Wonkye, Pei Wang, and Xiangrong Yin", "title": "Weighted Orthogonal Components Regression Analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiple linear regression setting, we propose a general framework,\ntermed weighted orthogonal components regression (WOCR), which encompasses many\nknown methods as special cases, including ridge regression and principal\ncomponents regression. WOCR makes use of the monotonicity inherent in\northogonal components to parameterize the weight function. The formulation\nallows for efficient determination of tuning parameters and hence is\ncomputationally advantageous. Moreover, WOCR offers insights for deriving new\nbetter variants. Specifically, we advocate weighting components based on their\ncorrelations with the response, which leads to enhanced predictive performance.\nBoth simulated studies and real data examples are provided to assess and\nillustrate the advantages of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 05:01:35 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 23:48:32 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Su", "Xiaogang", ""], ["Wonkye", "Yaa", ""], ["Wang", "Pei", ""], ["Yin", "Xiangrong", ""]]}, {"id": "1709.04186", "submitter": "Ignacio Martin", "authors": "Ignacio Mart\\'in, Jos\\'e Alberto Hern\\'andez, Sergio de los Santos", "title": "On labeling Android malware signatures using minhashing and further\n  classification with Structural Equation Models", "comments": "15 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scanner Antivirus systems provide insightful information on the nature\nof a suspect application; however there is often a lack of consensus and\nconsistency between different Anti-Virus engines. In this article, we analyze\nmore than 250 thousand malware signatures generated by 61 different Anti-Virus\nengines after analyzing 82 thousand different Android malware applications. We\nidentify 41 different malware classes grouped into three major categories,\nnamely Adware, Harmful Threats and Unknown or Generic signatures. We further\ninvestigate the relationships between such 41 classes using community detection\nalgorithms from graph theory to identify similarities between them; and we\nfinally propose a Structure Equation Model to identify which Anti-Virus engines\nare more powerful at detecting each macro-category. As an application, we show\nhow such models can help in identifying whether Unknown malware applications\nare more likely to be of Harmful or Adware type.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 08:38:36 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Mart\u00edn", "Ignacio", ""], ["Hern\u00e1ndez", "Jos\u00e9 Alberto", ""], ["Santos", "Sergio de los", ""]]}, {"id": "1709.04212", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi and Sumio Watanabe", "title": "Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation\n  and Stochastic Matrix Factorization", "comments": "Containing 36 pages, 2 figures, and 1 table. To appear in SN Computer\n  Science", "journal-ref": "SN Computer Science volume 1, Article number: 69 (2020)", "doi": "10.1007/s42979-020-0071-3", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is useful in document analysis, image\nprocessing, and many information systems; however, its generalization\nperformance has been left unknown because it is a singular learning machine to\nwhich regular statistical theory can not be applied.\n  Stochastic matrix factorization (SMF) is a restricted matrix factorization in\nwhich matrix factors are stochastic; the column of the matrix is in a simplex.\nSMF is being applied to image recognition and text mining. We can understand\nSMF as a statistical model by which a stochastic matrix of given data is\nrepresented by a product of two stochastic matrices, whose generalization\nperformance has also been left unknown because of non-regularity.\n  In this paper, by using an algebraic and geometric method, we show the\nanalytic equivalence of LDA and SMF, both of which have the same real log\ncanonical threshold (RLCT), resulting in that they asymptotically have the same\nBayesian generalization error and the same log marginal likelihood. Moreover,\nwe derive the upper bound of the RLCT and prove that it is smaller than the\ndimension of the parameter divided by two, hence the Bayesian generalization\nerrors of them are smaller than those of regular statistical models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 09:37:03 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 10:45:36 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 08:22:09 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2018 13:47:32 GMT"}, {"version": "v5", "created": "Sat, 23 Jun 2018 00:26:47 GMT"}, {"version": "v6", "created": "Mon, 14 Jan 2019 16:20:44 GMT"}, {"version": "v7", "created": "Fri, 22 Mar 2019 14:56:22 GMT"}, {"version": "v8", "created": "Thu, 30 Jan 2020 15:39:39 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Hayashi", "Naoki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1709.04384", "submitter": "Yu-Siang Huang", "authors": "Yu-Siang Huang, Szu-Yu Chou, Yi-Hsuan Yang", "title": "Generating Music Medleys via Playing Music Puzzle Games", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating music medleys is about finding an optimal permutation of a given\nset of music clips. Toward this goal, we propose a self-supervised learning\ntask, called the music puzzle game, to train neural network models to learn the\nsequential patterns in music. In essence, such a game requires machines to\ncorrectly sort a few multisecond music fragments. In the training stage, we\nlearn the model by sampling multiple non-overlapping fragment pairs from the\nsame songs and seeking to predict whether a given pair is consecutive and is in\nthe correct chronological order. For testing, we design a number of puzzle\ngames with different difficulty levels, the most difficult one being music\nmedley, which requiring sorting fragments from different songs. On the basis of\nstate-of-the-art Siamese convolutional network, we propose an improved\narchitecture that learns to embed frame-level similarity scores computed from\nthe input fragment pairs to a common space, where fragment pairs in the correct\norder can be more easily identified. Our result shows that the resulting model,\ndubbed as the similarity embedding network (SEN), performs better than\ncompeting models across different games, including music jigsaw puzzle, music\nsequencing, and music medley. Example results can be found at our project\nwebsite, https://remyhuang.github.io/DJnet.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 15:33:07 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 03:52:58 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Huang", "Yu-Siang", ""], ["Chou", "Szu-Yu", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1709.04395", "submitter": "David W. Dreisigmeyer", "authors": "David W Dreisigmeyer", "title": "Tight Semi-Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonnegative matrix factorization is a widely used, flexible matrix\ndecomposition, finding applications in biology, image and signal processing and\ninformation retrieval, among other areas. Here we present a related matrix\nfactorization. A multi-objective optimization problem finds conical\ncombinations of templates that approximate a given data matrix. The templates\nare chosen so that as far as possible only the initial data set can be\nrepresented this way. However, the templates are not required to be nonnegative\nnor convex combinations of the original data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:05:12 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 13:42:18 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 15:33:45 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Dreisigmeyer", "David W", ""]]}, {"id": "1709.04412", "submitter": "Przemyslaw Biecek", "authors": "Agnieszka Sitko, Przemyslaw Biecek", "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based\n  model selection", "comments": "Submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many statistical tests that verify the null hypothesis: the\nvariable of interest has the same distribution among k-groups. But once the\nnull hypothesis is rejected, how to present the structure of dissimilarity\nbetween groups? In this article, we introduce The Merging Path Plot - a\nmethodology, and factorMerger - an R package, for exploration and visualization\nof k-group dissimilarities. Comparison of k-groups is one of the most important\nissues in exploratory analyses and it has zillions of applications. The\nclassical solution is to test a~null hypothesis that observations from all\ngroups come from the same distribution. If the global null hypothesis is\nrejected, a~more detailed analysis of differences among pairs of groups is\nperformed. The traditional approach is to use pairwise post hoc tests in order\nto verify which groups differ significantly. However, this approach fails with\na large number of groups in both interpretation and visualization layer.\nThe~Merging Path Plot methodology solves this problem by using an\neasy-to-understand description of dissimilarity among groups based on\nLikelihood Ratio Test (LRT) statistic.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:46:11 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 09:12:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Sitko", "Agnieszka", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1709.04451", "submitter": "Wooseok Ha", "authors": "Wooseok Ha, Rina Foygel Barber", "title": "Alternating minimization and alternating descent over nonconvex sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of alternating minimization for loss functions\noptimized over two variables, where each variable may be restricted to lie in\nsome potentially nonconvex constraint set. This type of setting arises\nnaturally in high-dimensional statistics and signal processing, where the\nvariables often reflect different structures or components within the signals\nbeing considered. Our analysis relies on the notion of local concavity\ncoefficients, which has been proposed in Barber and Ha to measure and quantify\nthe concavity of a general nonconvex set. Our results further reveal important\ndistinctions between alternating and non-alternating methods. Since computing\nthe alternating minimization steps may not be tractable for some problems, we\nalso consider an inexact version of the algorithm and provide a set of\nsufficient conditions to ensure fast convergence of the inexact algorithms. We\ndemonstrate our framework on several examples, including low rank + sparse\ndecomposition and multitask regression, and provide numerical experiments to\nvalidate our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:53:44 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 21:44:51 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 16:50:13 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ha", "Wooseok", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1709.04481", "submitter": "Ryan Rossi", "authors": "James P. Canning, Emma E. Ingram, Sammantha Nowak-Wolff, Adriana M.\n  Ortiz, Nesreen K. Ahmed, Ryan A. Rossi, Karl R. B. Schmitt, Sucheta\n  Soundarajan", "title": "Network Classification and Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the best of our knowledge, this paper presents the first large-scale study\nthat tests whether network categories (e.g., social networks vs. web graphs)\nare distinguishable from one another (using both categories of real-world\nnetworks and synthetic graphs). A classification accuracy of $94.2\\%$ was\nachieved using a random forest classifier with both real and synthetic\nnetworks. This work makes two important findings. First, real-world networks\nfrom various domains have distinct structural properties that allow us to\npredict with high accuracy the category of an arbitrary network. Second,\nclassifying synthetic networks is trivial as our models can easily distinguish\nbetween synthetic graphs and the real-world networks they are supposed to\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 18:02:09 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Canning", "James P.", ""], ["Ingram", "Emma E.", ""], ["Nowak-Wolff", "Sammantha", ""], ["Ortiz", "Adriana M.", ""], ["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Schmitt", "Karl R. B.", ""], ["Soundarajan", "Sucheta", ""]]}, {"id": "1709.04495", "submitter": "Christian Donner", "authors": "Christian Donner and Manfred Opper", "title": "Inverse Ising problem in continuous time: A latent variable approach", "comments": "10 pages, 4 figures", "journal-ref": "Phys. Rev. E 96, 062104 (2017)", "doi": "10.1103/PhysRevE.96.062104", "report-no": null, "categories": "stat.ML physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse Ising problem, i.e. the inference of network\ncouplings from observed spin trajectories for a model with continuous time\nGlauber dynamics. By introducing two sets of auxiliary latent random variables\nwe render the likelihood into a form, which allows for simple iterative\ninference algorithms with analytical updates. The variables are: (1) Poisson\nvariables to linearise an exponential term which is typical for point process\nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood\nquadratic in the coupling parameters. Using the augmented likelihood, we derive\nan expectation-maximization (EM) algorithm to obtain the maximum likelihood\nestimate of network parameters. Using a third set of latent variables we extend\nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop\nan efficient approximate Bayesian inference algorithm using a variational\napproach. We demonstrate the performance of our algorithms on data simulated\nfrom an Ising model. For data which are simulated from a more biologically\nplausible network with spiking neurons, we show that the Ising model captures\nwell the low order statistics of the data and how the Ising couplings are\nrelated to the underlying synaptic structure of the simulated network.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 13:34:18 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 11:25:26 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 09:02:32 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Donner", "Christian", ""], ["Opper", "Manfred", ""]]}, {"id": "1709.04546", "submitter": "Zijun Zhang", "authors": "Zijun Zhang, Lin Ma, Zongpeng Li, Chuan Wu", "title": "Normalized Direction-preserving Adam", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive optimization algorithms, such as Adam and RMSprop, have shown better\noptimization performance than stochastic gradient descent (SGD) in some\nscenarios. However, recent studies show that they often lead to worse\ngeneralization performance than SGD, especially for training deep neural\nnetworks (DNNs). In this work, we identify the reasons that Adam generalizes\nworse than SGD, and develop a variant of Adam to eliminate the generalization\ngap. The proposed method, normalized direction-preserving Adam (ND-Adam),\nenables more precise control of the direction and step size for updating weight\nvectors, leading to significantly improved generalization performance.\nFollowing a similar rationale, we further improve the generalization\nperformance in classification tasks by regularizing the softmax logits. By\nbridging the gap between SGD and Adam, we also hope to shed light on why\ncertain optimization algorithms generalize better than others.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 21:38:02 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 02:35:39 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Zhang", "Zijun", ""], ["Ma", "Lin", ""], ["Li", "Zongpeng", ""], ["Wu", "Chuan", ""]]}, {"id": "1709.04555", "submitter": "Wengong Jin", "authors": "Wengong Jin, Connor W. Coley, Regina Barzilay, Tommi Jaakkola", "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network", "comments": "accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of organic reaction outcomes is a fundamental problem in\ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully\nexploring the space of possible transformations is intractable. The current\nsolution utilizes reaction templates to limit the space, but it suffers from\ncoverage and efficiency issues. In this paper, we propose a template-free\napproach to efficiently explore the space of product molecules by first\npinpointing the reaction center -- the set of nodes and edges where graph edits\noccur. Since only a small number of atoms contribute to reaction center, we can\ndirectly enumerate candidate products. The generated candidates are scored by a\nWeisfeiler-Lehman Difference Network that models high-order interactions\nbetween changes occurring at nodes across the molecule. Our framework\noutperforms the top-performing template-based approach with a 10\\% margin,\nwhile running orders of magnitude faster. Finally, we demonstrate that the\nmodel accuracy rivals the performance of domain experts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 22:28:46 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 18:53:33 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 16:31:51 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Jin", "Wengong", ""], ["Coley", "Connor W.", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1709.04574", "submitter": "Victor Shih", "authors": "Victor Shih, David C Jangraw, Paul Sajda, Sameer Saproo", "title": "Towards personalized human AI interaction - adapting the behavior of AI\n  agents using neural signatures of subjective interest", "comments": "11 pages, 9 figures, 1 table, Submitted to IEEE Trans. on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning AI commonly uses reward/penalty signals that are\nobjective and explicit in an environment -- e.g. game score, completion time,\netc. -- in order to learn the optimal strategy for task performance. However,\nHuman-AI interaction for such AI agents should include additional reinforcement\nthat is implicit and subjective -- e.g. human preferences for certain AI\nbehavior -- in order to adapt the AI behavior to idiosyncratic human\npreferences. Such adaptations would mirror naturally occurring processes that\nincrease trust and comfort during social interactions. Here, we show how a\nhybrid brain-computer-interface (hBCI), which detects an individual's level of\ninterest in objects/events in a virtual environment, can be used to adapt the\nbehavior of a Deep Reinforcement Learning AI agent that is controlling a\nvirtual autonomous vehicle. Specifically, we show that the AI learns a driving\nstrategy that maintains a safe distance from a lead vehicle, and most novelly,\npreferentially slows the vehicle when the human passengers of the vehicle\nencounter objects of interest. This adaptation affords an additional 20\\%\nviewing time for subjectively interesting objects. This is the first\ndemonstration of how an hBCI can be used to provide implicit reinforcement to\nan AI agent in a way that incorporates user preferences into the control\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:27:44 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Shih", "Victor", ""], ["Jangraw", "David C", ""], ["Sajda", "Paul", ""], ["Saproo", "Sameer", ""]]}, {"id": "1709.04576", "submitter": "Juhwan Noh", "authors": "Juhwan Noh, Jaehoon Kim, Seoin Back, and Yousung Jung", "title": "Catalyst design using actively learned machine with non-ab initio input\n  features towards CO2 reduction reactions", "comments": "Under review and including Electronic Supplementary Information (ESI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional chemisorption model, the d-band center theory (augmented\nsometimes with the upper edge of d-band for imporved accuarcy) plays a central\nrole in predicting adsorption energies and catalytic activity as a function of\nd-band center of the solid surfaces, but it requires density functional\ncalculations that can be quite costly for large scale screening purposes of\nmaterials. In this work, we propose to use the d-band width of the muffin-tin\norbital theory (to account for local coordination environment) plus\nelectronegativity (to account for adsorbate renormalization) as a simple set of\nalternative descriptors for chemisorption, which do not demand the ab initio\ncalculations. This pair of descriptors are then combined with machine learning\nmethods, namely, artificial neural network (ANN) and kernel ridge regression\n(KRR), to allow large scale materials screenings. We show, for a toy set of 263\nalloy systems, that the CO adsorption energy can be predicted with a remarkably\nsmall mean absolute deviation error of 0.05 eV, a significantly improved result\nas compared to 0.13 eV obtained with descriptors including costly d-band center\ncalculations in literature. We achieved this high accuracy by utilizing an\nactive learning algorithm, without which the accuracy was 0.18 eV otherwise. As\na practical application of this machine, we identified Cu3Y@Cu as a highly\nactive and cost-effective electrochemical CO2 reduction catalyst to produce CO\nwith the overpotential 0.37 V lower than Au catalyst.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:35:00 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Noh", "Juhwan", ""], ["Kim", "Jaehoon", ""], ["Back", "Seoin", ""], ["Jung", "Yousung", ""]]}, {"id": "1709.04594", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Lingfei Wu", "title": "Revisiting Spectral Graph Clustering with Generative Community Models", "comments": "Accepted by IEEE International Conference on Data Mining (ICDM) 2017\n  as a regular paper - full paper with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methodology of community detection can be divided into two principles:\nimposing a network model on a given graph, or optimizing a designed objective\nfunction. The former provides guarantees on theoretical detectability but falls\nshort when the graph is inconsistent with the underlying model. The latter is\nmodel-free but fails to provide quality assurance for the detected communities.\nIn this paper, we propose a novel unified framework to combine the advantages\nof these two principles. The presented method, SGC-GEN, not only considers the\ndetection error caused by the corresponding model mismatch to a given graph,\nbut also yields a theoretical guarantee on community detectability by analyzing\nSpectral Graph Clustering (SGC) under GENerative community models (GCMs).\nSGC-GEN incorporates the predictability on correct community detection with a\nmeasure of community fitness to GCMs. It resembles the formulation of\nsupervised learning problems by enabling various community detection loss\nfunctions and model mismatch metrics. We further establish a theoretical\ncondition for correct community detection using the normalized graph Laplacian\nmatrix under a GCM, which provides a novel data-driven loss function for\nSGC-GEN. In addition, we present an effective algorithm to implement SGC-GEN,\nand show that the computational complexity of SGC-GEN is comparable to the\nbaseline methods. Our experiments on 18 real-world datasets demonstrate that\nSGC-GEN possesses superior and robust performance compared to 6 baseline\nmethods under 7 representative clustering metrics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:34:30 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 04:03:14 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Wu", "Lingfei", ""]]}, {"id": "1709.04596", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan\n  Kong, Theodore L. Willke and Hoda Eldardiry", "title": "A Framework for Generalizing Graph-based Representation Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walks are at the heart of many existing deep learning algorithms for\ngraph data. However, such algorithms have many limitations that arise from the\nuse of random walks, e.g., the features resulting from these methods are unable\nto transfer to new nodes and graphs as they are tied to node identity. In this\nwork, we introduce the notion of attributed random walks which serves as a\nbasis for generalizing existing methods such as DeepWalk, node2vec, and many\nothers that leverage random walks. Our proposed framework enables these methods\nto be more widely applicable for both transductive and inductive learning as\nwell as for use on graphs with attributes (if available). This is achieved by\nlearning functions that generalize to new nodes and graphs. We show that our\nproposed framework is effective with an average AUC improvement of 16.1% while\nrequiring on average 853 times less space than existing methods on a variety of\ngraphs from several domains.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:37:52 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Lee", "John Boaz", ""], ["Kong", "Xiangnan", ""], ["Willke", "Theodore L.", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "1709.04673", "submitter": "Arunselvan Ramaswamy Dr.", "authors": "Arunselvan Ramaswamy and Shalabh Bhatnagar", "title": "Analyzing Approximate Value Iteration Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the stochastic iterative counterpart of the value\niteration scheme wherein only noisy and possibly biased approximations of the\nBellman operator are available. We call this counterpart as the approximate\nvalue iteration (AVI) scheme. Neural networks are often used as function\napproximators, in order to counter Bellman's curse of dimensionality. In this\npaper, they are used to approximate the Bellman operator. Since neural networks\nare typically trained using sample data, errors and biases may be introduced.\nThe design of AVI accounts for implementations with biased approximations of\nthe Bellman operator and sampling errors. We present verifiable sufficient\nconditions under which AVI is stable (almost surely bounded) and converges to a\nfixed point of the approximate Bellman operator. To ensure the stability of\nAVI, we present three different yet related sets of sufficient conditions that\nare based on the existence of an appropriate Lyapunov function. These Lyapunov\nfunction based conditions are easily verifiable and new to the literature. The\nverifiability is enhanced by the fact that a recipe for the construction of the\nnecessary Lyapunov function is also provided. We also show that the stability\nanalysis of AVI can be readily extended to the general case of set-valued\nstochastic approximations. Finally, we show that AVI can also be used in more\ngeneral circumstances, i.e., for finding fixed points of contractive set-valued\nmaps.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 09:04:41 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 08:43:40 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 08:33:45 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2019 09:24:15 GMT"}, {"version": "v5", "created": "Sun, 30 May 2021 11:34:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1709.04695", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann", "title": "The Conditional Analogy GAN: Swapping Fashion Articles on People Images", "comments": "To appear at the International Conference on Computer Vision, ICCV\n  2017, Workshop on Computer Vision for Fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to solve image analogy problems : it allows to\nlearn the relation between paired images present in training data, and then\ngeneralize and generate images that correspond to the relation, but were never\nseen in the training set. Therefore, we call the method Conditional Analogy\nGenerative Adversarial Network (CAGAN), as it is based on adversarial training\nand employs deep convolutional neural networks. An especially interesting\napplication of that technique is automatic swapping of clothing on fashion\nmodel photos. Our work has the following contributions. First, the definition\nof the end-to-end trainable CAGAN architecture, which implicitly learns\nsegmentation masks without expensive supervised labeling data. Second,\nexperimental results show plausible segmentation masks and often convincing\nswapped images, given the target article. Finally, we discuss the next steps\nfor that technique: neural network architecture improvements and more advanced\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 10:39:51 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""]]}, {"id": "1709.04718", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "The Impact of Local Geometry and Batch Size on Stochastic Gradient\n  Descent for Nonconvex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several experimental reports on nonconvex optimization problems in machine\nlearning, stochastic gradient descent (SGD) was observed to prefer minimizers\nwith flat basins in comparison to more deterministic methods, yet there is very\nlittle rigorous understanding of this phenomenon. In fact, the lack of such\nwork has led to an unverified, but widely-accepted stochastic mechanism\ndescribing why SGD prefers flatter minimizers to sharper minimizers. However,\nas we demonstrate, the stochastic mechanism fails to explain this phenomenon.\nHere, we propose an alternative deterministic mechanism that can accurately\nexplain why SGD prefers flatter minimizers to sharper minimizers. We derive\nthis mechanism based on a detailed analysis of a generic stochastic quadratic\nproblem, which generalizes known results for classical gradient descent.\nFinally, we verify the predictions of our deterministic mechanism on two\nnonconvex problems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:59:10 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 18:47:53 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1709.04744", "submitter": "John Lipor", "authors": "John Lipor, David Hong, Yan Shuo Tan, and Laura Balzano", "title": "Subspace Clustering using Ensembles of $K$-Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the unsupervised grouping of points lying near a union\nof low-dimensional linear subspaces. Algorithms based directly on geometric\nproperties of such data tend to either provide poor empirical performance, lack\ntheoretical guarantees, or depend heavily on their initialization. We present a\nnovel geometric approach to the subspace clustering problem that leverages\nensembles of the K-subspaces (KSS) algorithm via the evidence accumulation\nclustering framework. Our algorithm, referred to as ensemble K-subspaces\n(EKSS), forms a co-association matrix whose (i,j)th entry is the number of\ntimes points i and j are clustered together by several runs of KSS with random\ninitializations. We prove general recovery guarantees for any algorithm that\nforms an affinity matrix with entries close to a monotonic transformation of\npairwise absolute inner products. We then show that a specific instance of EKSS\nresults in an affinity matrix with entries of this form, and hence our proposed\nalgorithm can provably recover subspaces under similar conditions to\nstate-of-the-art algorithms. The finding is, to the best of our knowledge, the\nfirst recovery guarantee for evidence accumulation clustering and for KSS\nvariants. We show on synthetic data that our method performs well in the\ntraditionally challenging settings of subspaces with large intersection,\nsubspaces with small principal angles, and noisy data. Finally, we evaluate our\nalgorithm on six common benchmark datasets and show that unlike existing\nmethods, EKSS achieves excellent empirical performance when there are both a\nsmall and large number of points per subspace.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:55:56 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 22:18:48 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 23:39:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Lipor", "John", ""], ["Hong", "David", ""], ["Tan", "Yan Shuo", ""], ["Balzano", "Laura", ""]]}, {"id": "1709.04764", "submitter": "Raif Rustamov", "authors": "Raif M. Rustamov and James T. Klosowski", "title": "Interpretable Graph-Based Semi-Supervised Learning via Flows", "comments": null, "journal-ref": "AAAI 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the interpretability of the foundational\nLaplacian-based semi-supervised learning approaches on graphs. We introduce a\nnovel flow-based learning framework that subsumes the foundational approaches\nand additionally provides a detailed, transparent, and easily understood\nexpression of the learning process in terms of graph flows. As a result, one\ncan visualize and interactively explore the precise subgraph along which the\ninformation from labeled nodes flows to an unlabeled node of interest.\nSurprisingly, the proposed framework avoids trading accuracy for\ninterpretability, but in fact leads to improved prediction accuracy, which is\nsupported both by theoretical considerations and empirical results. The\nflow-based framework guarantees the maximum principle by construction and can\nhandle directed graphs in an out-of-the-box manner.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 13:13:52 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Rustamov", "Raif M.", ""], ["Klosowski", "James T.", ""]]}, {"id": "1709.04836", "submitter": "Jiankang Deng", "authors": "Niannan Xue, Jiankang Deng, Yannis Panagakis, Stefanos Zafeiriou", "title": "Informed Non-convex Robust Principal Component Analysis with Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of robust principal component analysis with features\nacting as prior side information. To this aim, a novel, elegant, non-convex\noptimization approach is proposed to decompose a given observation matrix into\na low-rank core and the corresponding sparse residual. Rigorous theoretical\nanalysis of the proposed algorithm results in exact recovery guarantees with\nlow computational complexity. Aptly designed synthetic experiments demonstrate\nthat our method is the first to wholly harness the power of non-convexity over\nconvexity in terms of both recoverability and speed. That is, the proposed\nnon-convex approach is more accurate and faster compared to the best available\nalgorithms for the problem under study. Two real-world applications, namely\nimage classification and face denoising further exemplify the practical\nsuperiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:06:21 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Xue", "Niannan", ""], ["Deng", "Jiankang", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1709.04862", "submitter": "Xiaogang Su", "authors": "Xiaogang Su, Annette T. Pe\\~na, Lei Liu, and Richard A. Levine", "title": "Random Forests of Interaction Trees for Estimating Individualized\n  Treatment Effects in Randomized Trials", "comments": "32 pages, 6 figures", "journal-ref": "Statistics in Medicine, 37(17): 2547-2560, 2018", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing heterogeneous treatment effects has become a growing interest in\nadvancing precision medicine. Individualized treatment effects (ITE) play a\ncritical role in such an endeavor. Concerning experimental data collected from\nrandomized trials, we put forward a method, termed random forests of\ninteraction trees (RFIT), for estimating ITE on the basis of interaction trees\n(Su et al., 2009). To this end, we first propose a smooth sigmoid surrogate\n(SSS) method, as an alternative to greedy search, to speed up tree\nconstruction. RFIT outperforms the traditional `separate regression' approach\nin estimating ITE. Furthermore, standard errors for the estimated ITE via RFIT\ncan be obtained with the infinitesimal jackknife method. We assess and\nillustrate the use of RFIT via both simulation and the analysis of data from an\nacupuncture headache trial.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:34:02 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Su", "Xiaogang", ""], ["Pe\u00f1a", "Annette T.", ""], ["Liu", "Lei", ""], ["Levine", "Richard A.", ""]]}, {"id": "1709.04875", "submitter": "Haoteng Yin", "authors": "Bing Yu, Haoteng Yin, Zhanxing Zhu", "title": "Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework\n  for Traffic Forecasting", "comments": "Proceedings of the 27th International Joint Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": "10.24963/ijcai.2018/505", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely accurate traffic forecast is crucial for urban traffic control and\nguidance. Due to the high nonlinearity and complexity of traffic flow,\ntraditional methods cannot satisfy the requirements of mid-and-long term\nprediction tasks and often neglect spatial and temporal dependencies. In this\npaper, we propose a novel deep learning framework, Spatio-Temporal Graph\nConvolutional Networks (STGCN), to tackle the time series prediction problem in\ntraffic domain. Instead of applying regular convolutional and recurrent units,\nwe formulate the problem on graphs and build the model with complete\nconvolutional structures, which enable much faster training speed with fewer\nparameters. Experiments show that our model STGCN effectively captures\ncomprehensive spatio-temporal correlations through modeling multi-scale traffic\nnetworks and consistently outperforms state-of-the-art baselines on various\nreal-world traffic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:54:41 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 09:17:45 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 13:52:01 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 07:55:09 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Yu", "Bing", ""], ["Yin", "Haoteng", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1709.05006", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Alexander Cloninger and Ronald R. Coifman", "title": "Two-sample Statistics Based on Anisotropic Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)\nstatistic for measuring the distance between two distributions given\nfinitely-many multivariate samples. When the distributions are locally\nlow-dimensional, the proposed test can be made more powerful to distinguish\ncertain alternatives by incorporating local covariance matrices and\nconstructing an anisotropic kernel. The kernel matrix is asymmetric; it\ncomputes the affinity between $n$ data points and a set of $n_R$ reference\npoints, where $n_R$ can be drastically smaller than $n$. While the proposed\nstatistic can be viewed as a special class of Reproducing Kernel Hilbert Space\nMMD, the consistency of the test is proved, under mild assumptions of the\nkernel, as long as $\\|p-q\\| \\sqrt{n} \\to \\infty $, and a finite-sample lower\nbound of the testing power is obtained. Applications to flow cytometry and\ndiffusion MRI datasets are demonstrated, which motivate the proposed approach\nto compare distributions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 23:06:19 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 15:39:36 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 21:56:28 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1709.05119", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller, Claudia Czado", "title": "Dependence Modeling in Ultra High Dimensions with Vine Copulas and the\n  Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model high dimensional data, Gaussian methods are widely used since they\nremain tractable and yield parsimonious models by imposing strong assumptions\non the data. Vine copulas are more flexible by combining arbitrary marginal\ndistributions and (conditional) bivariate copulas. Yet, this adaptability is\naccompanied by sharply increasing computational effort as the dimension\nincreases. The approach proposed in this paper overcomes this burden and makes\nthe first step into ultra high dimensional non-Gaussian dependence modeling by\nusing a divide-and-conquer approach. First, we apply Gaussian methods to split\ndatasets into feasibly small subsets and second, apply parsimonious and\nflexible vine copulas thereon. Finally, we reconcile them into one joint model.\nWe provide numerical results demonstrating the feasibility of our approach in\nmoderate dimensions and showcase its ability to estimate ultra high dimensional\nnon-Gaussian dependence models in thousands of dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 09:13:58 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Czado", "Claudia", ""]]}, {"id": "1709.05206", "submitter": "Fazle Karim", "authors": "Fazle Karim, Somshubra Majumdar, Houshang Darabi and Shun Chen", "title": "LSTM Fully Convolutional Networks for Time Series Classification", "comments": "7 pages, 3 figures and 2 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2017.2779939", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCN) have been shown to achieve\nstate-of-the-art performance on the task of classifying time series sequences.\nWe propose the augmentation of fully convolutional networks with long short\nterm memory recurrent neural network (LSTM RNN) sub-modules for time series\nclassification. Our proposed models significantly enhance the performance of\nfully convolutional networks with a nominal increase in model size and require\nminimal preprocessing of the dataset. The proposed Long Short Term Memory Fully\nConvolutional Network (LSTM-FCN) achieves state-of-the-art performance compared\nto others. We also explore the usage of attention mechanism to improve time\nseries classification with the Attention Long Short Term Memory Fully\nConvolutional Network (ALSTM-FCN). Utilization of the attention mechanism\nallows one to visualize the decision process of the LSTM cell. Furthermore, we\npropose fine-tuning as a method to enhance the performance of trained models.\nAn overall analysis of the performance of our model is provided and compared to\nother techniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 13:35:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Karim", "Fazle", ""], ["Majumdar", "Somshubra", ""], ["Darabi", "Houshang", ""], ["Chen", "Shun", ""]]}, {"id": "1709.05216", "submitter": "Yingfei Wang", "authors": "Yingfei Wang, Chu Wang, Warren Powell", "title": "Optimal Learning for Sequential Decision Making for Expensive Cost\n  Functions with Stochastic Binary Feedbacks", "comments": "arXiv admin note: text overlap with arXiv:1510.02354", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequentially making decisions that are rewarded by\n\"successes\" and \"failures\" which can be predicted through an unknown\nrelationship that depends on a partially controllable vector of attributes for\neach instance. The learner takes an active role in selecting samples from the\ninstance pool. The goal is to maximize the probability of success in either\noffline (training) or online (testing) phases. Our problem is motivated by\nreal-world applications where observations are time-consuming and/or expensive.\nWe develop a knowledge gradient policy using an online Bayesian linear\nclassifier to guide the experiment by maximizing the expected value of\ninformation of labeling each alternative. We provide a finite-time analysis of\nthe estimated error and show that the maximum likelihood estimator based\nproduced by the KG policy is consistent and asymptotically normal. We also show\nthat the knowledge gradient policy is asymptotically optimal in an offline\nsetting. This work further extends the knowledge gradient to the setting of\ncontextual bandits. We report the results of a series of experiments that\ndemonstrate its efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 22:01:20 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Wang", "Yingfei", ""], ["Wang", "Chu", ""], ["Powell", "Warren", ""]]}, {"id": "1709.05231", "submitter": "Argyris Kalogeratos", "authors": "Kevin Scaman, Argyris Kalogeratos, Luca Corinzia, Nicolas Vayatis", "title": "A Spectral Method for Activity Shaping in Continuous-Time Information\n  Cascades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Cascades Model captures dynamical properties of user activity in\na social network. In this work, we develop a novel framework for activity\nshaping under the Continuous-Time Information Cascades Model which allows the\nadministrator for local control actions by allocating targeted resources that\ncan alter the spread of the process. Our framework employs the optimization of\nthe spectral radius of the Hazard matrix, a quantity that has been shown to\ndrive the maximum influence in a network, while enjoying a simple convex\nrelaxation when used to minimize the influence of the cascade. In addition,\nuse-cases such as quarantine and node immunization are discussed to highlight\nthe generality of the proposed activity shaping framework. Finally, we present\nthe NetShape influence minimization method which is compared favorably to\nbaseline and state-of-the-art approaches through simulations on real social\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 14:32:36 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Scaman", "Kevin", ""], ["Kalogeratos", "Argyris", ""], ["Corinzia", "Luca", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1709.05262", "submitter": "Vikas Garg", "authors": "Vikas K. Garg, Adam Kalai", "title": "Supervising Unsupervised Learning", "comments": "11 two column pages. arXiv admin note: substantial text overlap with\n  arXiv:1612.09030", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework to leverage knowledge acquired from a repository of\n(heterogeneous) supervised datasets to new unsupervised datasets. Our\nperspective avoids the subjectivity inherent in unsupervised learning by\nreducing it to supervised learning, and provides a principled way to evaluate\nunsupervised algorithms. We demonstrate the versatility of our framework via\nsimple agnostic bounds on unsupervised problems. In the context of clustering,\nour approach helps choose the number of clusters and the clustering algorithm,\nremove the outliers, and provably circumvent the Kleinberg's impossibility\nresult. Experimental results across hundreds of problems demonstrate improved\nperformance on unsupervised data with simple algorithms, despite the fact that\nour problems come from heterogeneous domains. Additionally, our framework lets\nus leverage deep networks to learn common features from many such small\ndatasets, and perform zero shot learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:42:41 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 14:08:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Garg", "Vikas K.", ""], ["Kalai", "Adam", ""]]}, {"id": "1709.05276", "submitter": "Anna Seigal", "authors": "Anna Seigal and Guido Montufar", "title": "Mixtures and products in two graphical models", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two statistical models of three binary random variables. One is a\nmixture model and the other is a product of mixtures model called a restricted\nBoltzmann machine. Although the two models we study look different from their\nparametrizations, we show that they represent the same set of distributions on\nthe interior of the probability simplex, and are equal up to closure. We give a\nsemi-algebraic description of the model in terms of six binomial inequalities\nand obtain closed form expressions for the maximum likelihood estimates. We\nbriefly discuss extensions to larger models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:42:04 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Seigal", "Anna", ""], ["Montufar", "Guido", ""]]}, {"id": "1709.05289", "submitter": "Philipp Petersen", "authors": "Philipp Petersen, Felix Voigtlaender", "title": "Optimal approximation of piecewise smooth functions using deep ReLU\n  neural networks", "comments": "Generalized some estimates to $L^p$ norms for $0<p<\\infty$", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the necessary and sufficient complexity of ReLU neural networks---in\nterms of depth and number of weights---which is required for approximating\nclassifier functions in $L^2$. As a model class, we consider the set\n$\\mathcal{E}^\\beta (\\mathbb R^d)$ of possibly discontinuous piecewise $C^\\beta$\nfunctions $f : [-1/2, 1/2]^d \\to \\mathbb R$, where the different smooth regions\nof $f$ are separated by $C^\\beta$ hypersurfaces. For dimension $d \\geq 2$,\nregularity $\\beta > 0$, and accuracy $\\varepsilon > 0$, we construct artificial\nneural networks with ReLU activation function that approximate functions from\n$\\mathcal{E}^\\beta(\\mathbb R^d)$ up to $L^2$ error of $\\varepsilon$. The\nconstructed networks have a fixed number of layers, depending only on $d$ and\n$\\beta$, and they have $O(\\varepsilon^{-2(d-1)/\\beta})$ many nonzero weights,\nwhich we prove to be optimal. In addition to the optimality in terms of the\nnumber of weights, we show that in order to achieve the optimal approximation\nrate, one needs ReLU networks of a certain depth. Precisely, for piecewise\n$C^\\beta(\\mathbb R^d)$ functions, this minimal depth is given---up to a\nmultiplicative constant---by $\\beta/d$. Up to a log factor, our constructed\nnetworks match this bound. This partly explains the benefits of depth for ReLU\nnetworks by showing that deep networks are necessary to achieve efficient\napproximation of (piecewise) smooth functions. Finally, we analyze\napproximation in high-dimensional spaces where the function $f$ to be\napproximated can be factorized into a smooth dimension reducing feature map\n$\\tau$ and classifier function $g$---defined on a low-dimensional feature\nspace---as $f = g \\circ \\tau$. We show that in this case the approximation rate\ndepends only on the dimension of the feature space and not the input dimension.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:14:39 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 14:42:27 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 14:35:54 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 11:24:29 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Petersen", "Philipp", ""], ["Voigtlaender", "Felix", ""]]}, {"id": "1709.05321", "submitter": "Olivier Goudet Dr", "authors": "Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon,\n  David Lopez-Paz, Mich\\`ele Sebag", "title": "Learning Functional Causal Models with Generative Neural Networks", "comments": "Explainable and Interpretable Models in Computer Vision and Machine\n  Learning. Springer Series on Challenges in Machine Learning. 2018. Cham:\n  Springer International Publishing", "journal-ref": null, "doi": "10.1007/978-3-319-98131-4", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to functional causal modeling from observational\ndata, called Causal Generative Neural Networks (CGNN). CGNN leverages the power\nof neural networks to learn a generative model of the joint distribution of the\nobserved variables, by minimizing the Maximum Mean Discrepancy between\ngenerated and observed data. An approximate learning criterion is proposed to\nscale the computational cost of the approach to linear complexity in the number\nof observations. The performance of CGNN is studied throughout three\nexperiments. Firstly, CGNN is applied to cause-effect inference, where the task\nis to identify the best causal hypothesis out of $X\\rightarrow Y$ and\n$Y\\rightarrow X$. Secondly, CGNN is applied to the problem of identifying\nv-structures and conditional independences. Thirdly, CGNN is applied to\nmultivariate functional causal modeling: given a skeleton describing the direct\ndependences in a set of random variables $\\textbf{X} = [X_1, \\ldots, X_d]$,\nCGNN orients the edges in the skeleton to uncover the directed acyclic causal\ngraph describing the causal structure of the random variables. On all three\ntasks, CGNN is extensively assessed on both artificial and real-world data,\ncomparing favorably to the state-of-the-art. Finally, CGNN is extended to\nhandle the case of confounders, where latent variables are involved in the\noverall causal model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:16:21 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 15:11:04 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 09:37:17 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Goudet", "Olivier", ""], ["Kalainathan", "Diviyan", ""], ["Caillou", "Philippe", ""], ["Guyon", "Isabelle", ""], ["Lopez-Paz", "David", ""], ["Sebag", "Mich\u00e8le", ""]]}, {"id": "1709.05328", "submitter": "Xi Luo", "authors": "Yi Zhao and Xi Luo", "title": "Granger Mediation Analysis of Multiple Time Series with an Application\n  to fMRI", "comments": "59 pages. Presented at the 2017 ENAR, JSM, and other meetings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It becomes increasingly popular to perform mediation analysis for complex\ndata from sophisticated experimental studies. In this paper, we present Granger\nMediation Analysis (GMA), a new framework for causal mediation analysis of\nmultiple time series. This framework is motivated by a functional magnetic\nresonance imaging (fMRI) experiment where we are interested in estimating the\nmediation effects between a randomized stimulus time series and brain activity\ntime series from two brain regions. The stable unit treatment assumption for\ncausal mediation analysis is thus unrealistic for this type of time series\ndata. To address this challenge, our framework integrates two types of models:\ncausal mediation analysis across the variables and vector autoregressive models\nacross the temporal observations. We further extend this framework to handle\nmultilevel data to address individual variability and correlated errors between\nthe mediator and the outcome variables. These models not only provide valid\ncausal mediation for time series data but also model the causal dynamics across\ntime. We show that the modeling parameters in our models are identifiable, and\nwe develop computationally efficient methods to maximize the likelihood-based\noptimization criteria. Simulation studies show that our method reduces the\nestimation bias and improve statistical power, compared to existing approaches.\nOn a real fMRI data set, our approach not only infers the causal effects of\nbrain pathways but accurately captures the feedback effect of the outcome\nregion on the mediator region.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:47:51 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1709.05379", "submitter": "Nasser Mohammadiha", "authors": "Ghazaleh Panahandeh, Erik Ek, Nasser Mohammadiha", "title": "Road Friction Estimation for Connected Vehicles using Supervised Machine\n  Learning", "comments": "Published at IV 2017", "journal-ref": null, "doi": "10.1109/IVS.2017.7995885", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of road friction prediction from a fleet of\nconnected vehicles is investigated. A framework is proposed to predict the road\nfriction level using both historical friction data from the connected cars and\ndata from weather stations, and comparative results from different methods are\npresented. The problem is formulated as a classification task where the\navailable data is used to train three machine learning models including\nlogistic regression, support vector machine, and neural networks to predict the\nfriction class (slippery or non-slippery) in the future for specific road\nsegments. In addition to the friction values, which are measured by moving\nvehicles, additional parameters such as humidity, temperature, and rainfall are\nused to obtain a set of descriptive feature vectors as input to the\nclassification methods. The proposed prediction models are evaluated for\ndifferent prediction horizons (0 to 120 minutes in the future) where the\nevaluation shows that the neural networks method leads to more stable results\nin different conditions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 19:52:18 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Panahandeh", "Ghazaleh", ""], ["Ek", "Erik", ""], ["Mohammadiha", "Nasser", ""]]}, {"id": "1709.05380", "submitter": "Brendan O'Donoghue", "authors": "Brendan O'Donoghue, Ian Osband, Remi Munos, Volodymyr Mnih", "title": "The Uncertainty Bellman Equation and Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the exploration/exploitation problem in reinforcement learning.\nFor exploitation, it is well known that the Bellman equation connects the value\nat any time-step to the expected value at subsequent time-steps. In this paper\nwe consider a similar \\textit{uncertainty} Bellman equation (UBE), which\nconnects the uncertainty at any time-step to the expected uncertainties at\nsubsequent time-steps, thereby extending the potential exploratory benefit of a\npolicy beyond individual time-steps. We prove that the unique fixed point of\nthe UBE yields an upper bound on the variance of the posterior distribution of\nthe Q-values induced by any policy. This bound can be much tighter than\ntraditional count-based bonuses that compound standard deviation rather than\nvariance. Importantly, and unlike several existing approaches to optimism, this\nmethod scales naturally to large systems with complex generalization.\nSubstituting our UBE-exploration strategy for $\\epsilon$-greedy improves DQN\nperformance on 51 out of 57 games in the Atari suite.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 19:55:58 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 11:47:43 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 15:42:36 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 15:25:04 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["O'Donoghue", "Brendan", ""], ["Osband", "Ian", ""], ["Munos", "Remi", ""], ["Mnih", "Volodymyr", ""]]}, {"id": "1709.05409", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a and Mauricio A. \\'Alvarez and Neil D. Lawrence", "title": "Gaussian Process Latent Force Models for Learning and Stochastic Control\n  of Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with learning and stochastic control in physical\nsystems which contain unknown input signals. These unknown signals are modeled\nas Gaussian processes (GP) with certain parametrized covariance structures. The\nresulting latent force models (LFMs) can be seen as hybrid models that contain\na first-principles physical model part and a non-parametric GP model part. We\nbriefly review the statistical inference and learning methods for this kind of\nmodels, introduce stochastic control methodology for the models, and provide\nnew theoretical observability and controllability results for them.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:07:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 20:02:42 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1709.05418", "submitter": "Brian McWilliams", "authors": "Simon Kallweit and Thomas M\\\"uller and Brian McWilliams and Markus\n  Gross and Jan Nov\\'ak", "title": "Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting\n  Neural Networks", "comments": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2017)", "journal-ref": null, "doi": "10.1145/3130800.3130880", "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for efficiently synthesizing images of atmospheric\nclouds using a combination of Monte Carlo integration and neural networks. The\nintricacies of Lorenz-Mie scattering and the high albedo of cloud-forming\naerosols make rendering of clouds---e.g. the characteristic silverlining and\nthe \"whiteness\" of the inner body---challenging for methods based solely on\nMonte Carlo integration or diffusion theory. We approach the problem\ndifferently. Instead of simulating all light transport during rendering, we\npre-learn the spatial and directional distribution of radiant flux from tens of\ncloud exemplars. To render a new scene, we sample visible points of the cloud\nand, for each, extract a hierarchical 3D descriptor of the cloud geometry with\nrespect to the shading location and the light source. The descriptor is input\nto a deep neural network that predicts the radiance function for each shading\nconfiguration. We make the key observation that progressively feeding the\nhierarchical descriptor into the network enhances the network's ability to\nlearn faster and predict with high accuracy while using few coefficients. We\nalso employ a block design with residual connections to further improve\nperformance. A GPU implementation of our method synthesizes images of clouds\nthat are nearly indistinguishable from the reference solution within seconds\ninteractively. Our method thus represents a viable solution for applications\nsuch as cloud design and, thanks to its temporal stability, also for\nhigh-quality production of animated content.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:40:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kallweit", "Simon", ""], ["M\u00fcller", "Thomas", ""], ["McWilliams", "Brian", ""], ["Gross", "Markus", ""], ["Nov\u00e1k", "Jan", ""]]}, {"id": "1709.05454", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Donniell E. Fishkind, Keith Levin, Vince Lyzinski,\n  Youngser Park, Yichen Qin, Daniel L. Sussman, Minh Tang, Joshua T.\n  Vogelstein, and Carey E. Priebe", "title": "Statistical inference on random dot product graphs: a survey", "comments": "An expository survey paper on a comprehensive paradigm for inference\n  for random dot product graphs, centered on graph adjacency and Laplacian\n  spectral embeddings. Paper outlines requisite background; summarizes theory,\n  methodology, and applications from previous and ongoing work; and closes with\n  a discussion of several open problems", "journal-ref": "Journal of Machine Learning Research, 2018", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random dot product graph (RDPG) is an independent-edge random graph that\nis analytically tractable and, simultaneously, either encompasses or can\nsuccessfully approximate a wide range of random graphs, from relatively simple\nstochastic block models to complex latent position graphs. In this survey\npaper, we describe a comprehensive paradigm for statistical inference on random\ndot product graphs, a paradigm centered on spectral embeddings of adjacency and\nLaplacian matrices. We examine the analogues, in graph inference, of several\ncanonical tenets of classical Euclidean inference: in particular, we summarize\na body of existing results on the consistency and asymptotic normality of the\nadjacency and Laplacian spectral embeddings, and the role these spectral\nembeddings can play in the construction of single- and multi-sample hypothesis\ntests for graph data. We investigate several real-world applications, including\ncommunity detection and classification in large social networks and the\ndetermination of functional and biologically relevant network properties from\nan exploratory data analysis of the Drosophila connectome. We outline requisite\nbackground and current open problems in spectral graph inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 04:22:57 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Athreya", "Avanti", ""], ["Fishkind", "Donniell E.", ""], ["Levin", "Keith", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Qin", "Yichen", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1709.05480", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou and Grigorios Tsoumakas", "title": "Subset Labeled LDA for Large-Scale Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard\nunsupervised Latent Dirichlet Allocation (LDA) algorithm, to address\nmulti-label learning tasks. Previous work has shown it to perform in par with\nother state-of-the-art multi-label methods. Nonetheless, with increasing label\nsets sizes LLDA encounters scalability issues. In this work, we introduce\nSubset LLDA, a simple variant of the standard LLDA algorithm, that not only can\neffectively scale up to problems with hundreds of thousands of labels but also\nimproves over the LLDA state-of-the-art. We conduct extensive experiments on\neight data sets, with label sets sizes ranging from hundreds to hundreds of\nthousands, comparing our proposed algorithm with the previously proposed LLDA\nalgorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme\nmulti-label classification. The results show a steady advantage of our method\nover the other LLDA algorithms and competitive results compared to the extreme\nmulti-label classification algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 08:35:12 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "1709.05501", "submitter": "Ryan-Rhys Griffiths", "authors": "Ryan-Rhys Griffiths, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Constrained Bayesian Optimization for Automatic Chemical Design", "comments": "Previous versions accepted to the NIPS 2017 Workshop on Bayesian\n  Optimization (BayesOpt 2017) and the NIPS 2017 Workshop on Machine Learning\n  for Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Chemical Design is a framework for generating novel molecules with\noptimized properties. The original scheme, featuring Bayesian optimization over\nthe latent space of a variational autoencoder, suffers from the pathology that\nit tends to produce invalid molecular structures. First, we demonstrate\nempirically that this pathology arises when the Bayesian optimization scheme\nqueries latent points far away from the data on which the variational\nautoencoder has been trained. Secondly, by reformulating the search procedure\nas a constrained Bayesian optimization problem, we show that the effects of\nthis pathology can be mitigated, yielding marked improvements in the validity\nof the generated molecules. We posit that constrained Bayesian optimization is\na good approach for solving this class of training set mismatch in many\ngenerative tasks involving Bayesian optimization over the latent space of a\nvariational autoencoder.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 11:38:35 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 12:00:52 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 21:47:19 GMT"}, {"version": "v4", "created": "Wed, 15 Nov 2017 22:39:08 GMT"}, {"version": "v5", "created": "Wed, 27 Jun 2018 23:37:58 GMT"}, {"version": "v6", "created": "Mon, 12 Aug 2019 12:50:34 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Griffiths", "Ryan-Rhys", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1709.05506", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Patrick Rubin-Delanchy, Joshua Cape, Minh Tang and Carey E. Priebe", "title": "A statistical interpretation of spectral embedding: the generalised\n  random dot product graph", "comments": "30 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalisation of a latent position network model known as the random dot\nproduct graph is considered. We show that, whether the normalised Laplacian or\nadjacency matrix is used, the vector representations of nodes obtained by\nspectral embedding, using the largest eigenvalues by magnitude, provide\nstrongly consistent latent position estimates with asymptotically Gaussian\nerror, up to indefinite orthogonal transformation. The mixed membership and\nstandard stochastic block models constitute special cases where the latent\npositions live respectively inside or on the vertices of a simplex, crucially,\nwithout assuming the underlying block connectivity probability matrix is\npositive-definite. Estimation via spectral embedding can therefore be achieved\nby respectively estimating this simplicial support, or fitting a Gaussian\nmixture model. In the latter case, the use of $K$-means (with Euclidean\ndistance), as has been previously recommended, is suboptimal and for\nidentifiability reasons unsound. Indeed, Euclidean distances and angles are not\npreserved under indefinite orthogonal transformation, and we show stochastic\nblock model examples where such quantities vary appreciably. Empirical\nimprovements in link prediction (over the random dot product graph), as well as\nthe potential to uncover richer latent structure (than posited under the mixed\nmembership or standard stochastic block models) are demonstrated in a\ncyber-security example.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 12:30:40 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 12:58:52 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 19:02:30 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 16:27:46 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Rubin-Delanchy", "Patrick", ""], ["Cape", "Joshua", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1709.05510", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, Barna Saha", "title": "The Geometric Block Model", "comments": "A shorter version of this paper has appeared in 32nd AAAI Conference\n  on Artificial Intelligence. The AAAI proceedings version as well as the\n  previous version in arxiv contained some errors that have been corrected in\n  this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the inherent geometric features of many community detection\nproblems, we propose to use a new random graph model of communities that we\ncall a Geometric Block Model. The geometric block model generalizes the random\ngeometric graphs in the same way that the well-studied stochastic block model\ngeneralizes the Erdos-Renyi random graphs. It is also a natural extension of\nrandom community models inspired by the recent theoretical and practical\nadvancement in community detection. While being a topic of fundamental\ntheoretical interest, our main contribution is to show that many practical\ncommunity structures are better explained by the geometric block model. We also\nshow that a simple triangle-counting algorithm to detect communities in the\ngeometric block model is near-optimal. Indeed, even in the regime where the\naverage degree of the graph grows only logarithmically with the number of\nvertices (sparse-graph), we show that this algorithm performs extremely well,\nboth theoretically and practically. In contrast, the triangle-counting\nalgorithm is far from being optimum for the stochastic block model. We simulate\nour results on both real and synthetic datasets to show superior performance of\nboth the new model as well as our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 13:38:03 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 16:26:40 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""], ["Saha", "Barna", ""]]}, {"id": "1709.05515", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Suhas N., Talasila Sai Teja and Anshul Juneja", "title": "Some variations on Ensembled Random Survival Forest with application to\n  Cancer Research", "comments": "16 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel implementation of adaboost for prediction\nof survival function. We take different variations of the algorithm and compare\nthe algorithms based on system run time and root mean square error. Our\nconstruction includes right censoring data and competing risk data too. We take\ndifferent data set to illustrate the performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:12:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 19:24:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["N.", "Suhas", ""], ["Teja", "Talasila Sai", ""], ["Juneja", "Anshul", ""]]}, {"id": "1709.05545", "submitter": "Yangzi Guo", "authors": "Gitesh Dawer, Yangzi Guo, Adrian Barbu", "title": "Generating Compact Tree Ensembles via Annealing", "comments": "Comparison with Random Forest included in the results section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensembles are flexible predictive models that can capture relevant\nvariables and to some extent their interactions in a compact and interpretable\nmanner. Most algorithms for obtaining tree ensembles are based on versions of\nboosting or Random Forest. Previous work showed that boosting algorithms\nexhibit a cyclic behavior of selecting the same tree again and again due to the\nway the loss is optimized. At the same time, Random Forest is not based on loss\noptimization and obtains a more complex and less interpretable model. In this\npaper we present a novel method for obtaining compact tree ensembles by growing\na large pool of trees in parallel with many independent boosting threads and\nthen selecting a small subset and updating their leaf weights by loss\noptimization. We allow for the trees in the initial pool to have different\ndepths which further helps with generalization. Experiments on real datasets\nshow that the obtained model has usually a smaller loss than boosting, which is\nalso reflected in a lower misclassification error on the test set.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:26:18 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 03:03:27 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 18:27:57 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 03:25:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dawer", "Gitesh", ""], ["Guo", "Yangzi", ""], ["Barbu", "Adrian", ""]]}, {"id": "1709.05548", "submitter": "Evgeny Burnaev", "authors": "Rodrigo Rivera and Evgeny Burnaev", "title": "Forecasting of commercial sales with large scale Gaussian Processes", "comments": "1o pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper argues that there has not been enough discussion in the field of\napplications of Gaussian Process for the fast moving consumer goods industry.\nYet, this technique can be important as it e.g., can provide automatic feature\nrelevance determination and the posterior mean can unlock insights on the data.\nSignificant challenges are the large size and high dimensionality of commercial\ndata at a point of sale. The study reviews approaches in the Gaussian Processes\nmodeling for large data sets, evaluates their performance on commercial sales\nand shows value of this type of models as a decision-making tool for\nmanagement.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:51:58 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Rivera", "Rodrigo", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1709.05552", "submitter": "Xingqi Du", "authors": "Xingqi Du, Subhashis Ghosal", "title": "Multivariate Gaussian Network Structure Learning", "comments": "30 pages, 17 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a graphical model where a multivariate normal vector is\nassociated with each node of the underlying graph and estimate the graphical\nstructure. We minimize a loss function obtained by regressing the vector at\neach node on those at the remaining ones under a group penalty. We show that\nthe proposed estimator can be computed by a fast convex optimization algorithm.\nWe show that as the sample size increases, the estimated regression\ncoefficients and the correct graphical structure are correctly estimated with\nprobability tending to one. By extensive simulations, we show the superiority\nof the proposed method over comparable procedures. We apply the technique on\ntwo real datasets. The first one is to identify gene and protein networks\nshowing up in cancer cell lines, and the second one is to reveal the\nconnections among different industries in the US.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:58:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Du", "Xingqi", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1709.05554", "submitter": "Yan Shu", "authors": "Davis Liang, Yan Shu", "title": "Deep Automated Multi-task Learning", "comments": "IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has recently contributed to learning better\nrepresentations in service of various NLP tasks. MTL aims at improving the\nperformance of a primary task, by jointly training on a secondary task. This\npaper introduces automated tasks, which exploit the sequential nature of the\ninput data, as secondary tasks in an MTL model. We explore next word\nprediction, next character prediction, and missing word completion as potential\nautomated tasks. Our results show that training on a primary task in parallel\nwith a secondary automated task improves both the convergence speed and\naccuracy for the primary task. We suggest two methods for augmenting an\nexisting network with automated tasks and establish better performance in topic\nprediction, sentiment analysis, and hashtag recommendation. Finally, we show\nthat the MTL models can perform well on datasets that are small and colloquial\nby nature.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 19:04:54 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 19:05:39 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Liang", "Davis", ""], ["Shu", "Yan", ""]]}, {"id": "1709.05583", "submitter": "Xiaoyu Cao", "authors": "Xiaoyu Cao, Neil Zhenqiang Gong", "title": "Mitigating Evasion Attacks to Deep Neural Networks via Region-based\n  Classification", "comments": "33rd Annual Computer Security Applications Conference (ACSAC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have transformed several artificial intelligence\nresearch areas including computer vision, speech recognition, and natural\nlanguage processing. However, recent studies demonstrated that DNNs are\nvulnerable to adversarial manipulations at testing time. Specifically, suppose\nwe have a testing example, whose label can be correctly predicted by a DNN\nclassifier. An attacker can add a small carefully crafted noise to the testing\nexample such that the DNN classifier predicts an incorrect label, where the\ncrafted testing example is called adversarial example. Such attacks are called\nevasion attacks. Evasion attacks are one of the biggest challenges for\ndeploying DNNs in safety and security critical applications such as\nself-driving cars. In this work, we develop new methods to defend against\nevasion attacks. Our key observation is that adversarial examples are close to\nthe classification boundary. Therefore, we propose region-based classification\nto be robust to adversarial examples. For a benign/adversarial testing example,\nwe ensemble information in a hypercube centered at the example to predict its\nlabel. In contrast, traditional classifiers are point-based classification,\ni.e., given a testing example, the classifier predicts its label based on the\ntesting example alone. Our evaluation results on MNIST and CIFAR-10 datasets\ndemonstrate that our region-based classification can significantly mitigate\nevasion attacks without sacrificing classification accuracy on benign examples.\nSpecifically, our region-based classification achieves the same classification\naccuracy on testing benign examples as point-based classification, but our\nregion-based classification is significantly more robust than point-based\nclassification to various evasion attacks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 00:18:42 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 06:45:33 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 20:58:56 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 14:36:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Cao", "Xiaoyu", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "1709.05602", "submitter": "Eric Lei", "authors": "Eric Lei, Kyle Miller, Michael R. Pinsky, Artur Dubrawski", "title": "Characterization of Hemodynamic Signal by Learning Multi-View\n  Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data are increasingly prevalent in practice. It is often relevant\nto analyze the relationships between pairs of views by multi-view component\nanalysis techniques such as Canonical Correlation Analysis (CCA). However, data\nmay easily exhibit nonlinear relations, which CCA cannot reveal. We aim to\ninvestigate the usefulness of nonlinear multi-view relations to characterize\nmulti-view data in an explainable manner. To address this challenge, we propose\na method to characterize globally nonlinear multi-view relationships as a\nmixture of linear relationships. A clustering method, it identifies partitions\nof observations that exhibit the same relationships and learns those\nrelationships simultaneously. It defines cluster variables by multi-view rather\nthan spatial relationships, unlike almost all other clustering methods.\nFurthermore, we introduce a supervised classification method that builds on our\nclustering method by employing multi-view relationships as discriminative\nfactors. The value of these methods resides in their capability to find useful\nstructure in the data that single-view or current multi-view methods may\nstruggle to find. We demonstrate the potential utility of the proposed approach\nusing an application in clinical informatics to detect and characterize slow\nbleeding in patients whose central venous pressure (CVP) is monitored at the\nbedside. Presently, CVP is considered an insensitive measure of a subject's\nintravascular volume status or its change. However, we reason that features of\nCVP during inspiration and expiration should be informative in early\nidentification of emerging changes of patient status. We empirically show how\nthe proposed method can help discover and analyze multiple-to-multiple\ncorrelations, which could be nonlinear or vary throughout the population, by\nfinding explainable structure of operational interest to practitioners.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 03:12:27 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 22:34:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lei", "Eric", ""], ["Miller", "Kyle", ""], ["Pinsky", "Michael R.", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1709.05612", "submitter": "Luming Tang", "authors": "Luming Tang, Yexiang Xue, Di Chen, Carla P. Gomes", "title": "Multi-Entity Dependence Learning with Rich Context via Conditional\n  Variational Auto-encoder", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Entity Dependence Learning (MEDL) explores conditional correlations\namong multiple entities. The availability of rich contextual information\nrequires a nimble learning scheme that tightly integrates with deep neural\nnetworks and has the ability to capture correlation structures among\nexponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional\nmultivariate distribution as a generating process. As a result, the variational\nlower bound of the joint likelihood can be optimized via a conditional\nvariational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was\nmotivated by two real-world applications in computational sustainability: one\nstudies the spatial correlation among multiple bird species using the eBird\ndata and the other models multi-dimensional landscape composition and human\nfootprint in the Amazon rainforest with satellite images. We show that\nMEDL_CVAE captures rich dependency structures, scales better than previous\nmethods, and further improves on the joint likelihood taking advantage of very\nlarge datasets that are beyond the capacity of previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 06:21:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Tang", "Luming", ""], ["Xue", "Yexiang", ""], ["Chen", "Di", ""], ["Gomes", "Carla P.", ""]]}, {"id": "1709.05666", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon, \\'Eric Gaussier, Christopher R. Dance, Guillaume\n  Bouchard", "title": "On Inductive Abilities of Latent Factor Models for Relational Learning", "comments": "30+3 pages, submitted to the Journal of Artificial Intelligence\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are increasingly popular for modeling multi-relational\nknowledge graphs. By their vectorial nature, it is not only hard to interpret\nwhy this class of models works so well, but also to understand where they fail\nand how they might be improved. We conduct an experimental survey of\nstate-of-the-art models, not towards a purely comparative end, but as a means\nto get insight about their inductive abilities. To assess the strengths and\nweaknesses of each model, we create simple tasks that exhibit first, atomic\nproperties of binary relations, and then, common inter-relational inference\nthrough synthetic genealogies. Based on these experimental results, we propose\nnew research directions to improve on existing models.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:20:05 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Gaussier", "\u00c9ric", ""], ["Dance", "Christopher R.", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1709.05667", "submitter": "Cl\\'ement Elvira", "authors": "Cl\\'ement Elvira and Pierre Chainais and Nicolas Dobigeon", "title": "Bayesian nonparametric Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is very popular to perform dimension\nreduction. The selection of the number of significant components is essential\nbut often based on some practical heuristics depending on the application. Only\nfew works have proposed a probabilistic approach able to infer the number of\nsignificant components. To this purpose, this paper introduces a Bayesian\nnonparametric principal component analysis (BNP-PCA). The proposed model\nprojects observations onto a random orthogonal basis which is assigned a prior\ndistribution defined on the Stiefel manifold. The prior on factor scores\ninvolves an Indian buffet process to model the uncertainty related to the\nnumber of components. The parameters of interest as well as the nuisance\nparameters are finally inferred within a fully Bayesian framework via Monte\nCarlo sampling. A study of the (in-)consistence of the marginal maximum a\nposteriori estimator of the latent dimension is carried out. A new estimator of\nthe subspace dimension is proposed. Moreover, for sake of statistical\nsignificance, a Kolmogorov-Smirnov test based on the posterior distribution of\nthe principal components is used to refine this estimate.\n  The behaviour of the algorithm is first studied on various synthetic\nexamples. Finally, the proposed BNP dimension reduction approach is shown to be\neasily yet efficiently coupled with clustering or latent factor models within a\nunique framework.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:24:05 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Elvira", "Cl\u00e9ment", ""], ["Chainais", "Pierre", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1709.05673", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Mariela Sued", "title": "Semi-supervised learning", "comments": "The paper as it is now, contains some mistakes in the proofs.\n  Hopefully soon I will submit a new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning deals with the problem of how, if possible, to take\nadvantage of a huge amount of not classified data, to perform classification,\nin situations when, typically, the labelled data are few. Even though this is\nnot always possible (it depends on how useful is to know the distribution of\nthe unlabelled data in the inference of the labels), several algorithm have\nbeen proposed recently. A new algorithm is proposed, that under almost\nneccesary conditions, attains asymptotically the performance of the best\ntheoretical rule, when the size of unlabeled data tends to infinity. The set of\nnecessary assumptions, although reasonables, show that semi-parametric\nclassification only works for very well conditioned problems.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:45:42 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 13:02:03 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Sued", "Mariela", ""]]}, {"id": "1709.05684", "submitter": "Ehsan Arbabi", "authors": "Mohsen Sahraei Ardakani and Ehsan Arbabi", "title": "A Categorical Approach for Recognizing Emotional Effects of Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, digital music libraries have been developed and can be plainly\naccessed. Latest research showed that current organization and retrieval of\nmusic tracks based on album information are inefficient. Moreover, they\ndemonstrated that people use emotion tags for music tracks in order to search\nand retrieve them. In this paper, we discuss separability of a set of emotional\nlabels, proposed in the categorical emotion expression, using Fisher's\nseparation theorem. We determine a set of adjectives to tag music parts: happy,\nsad, relaxing, exciting, epic and thriller. Temporal, frequency and energy\nfeatures have been extracted from the music parts. It could be seen that the\nmaximum separability within the extracted features occurs between relaxing and\nepic music parts. Finally, we have trained a classifier using Support Vector\nMachines to automatically recognize and generate emotional labels for a music\npart. Accuracy for recognizing each label has been calculated; where the\nresults show that epic music can be recognized more accurately (77.4%),\ncomparing to the other types of music.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 16:04:41 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Ardakani", "Mohsen Sahraei", ""], ["Arbabi", "Ehsan", ""]]}, {"id": "1709.05707", "submitter": "Bodhisattva Sen", "authors": "Adityanand Guntuboyina and Bodhisattva Sen", "title": "Nonparametric Shape-restricted Regression", "comments": "This is a survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of nonparametric regression under shape constraints.\nThe main examples include isotonic regression (with respect to any partial\norder), unimodal/convex regression, additive shape-restricted regression, and\nconstrained single index model. We review some of the theoretical properties of\nthe least squares estimator (LSE) in these problems, emphasizing on the\nadaptive nature of the LSE. In particular, we study the behavior of the risk of\nthe LSE, and its pointwise limiting distribution theory, with special emphasis\nto isotonic regression. We survey various methods for constructing pointwise\nconfidence intervals around these shape-restricted functions. We also briefly\ndiscuss the computation of the LSE and indicate some open research problems and\nfuture directions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 19:13:59 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 06:44:04 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1709.05750", "submitter": "NhatHai Phan", "authors": "NhatHai Phan, Xintao Wu, Han Hu, Dejing Dou", "title": "Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep\n  Learning", "comments": "IEEE ICDM 2017 - regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on developing a novel mechanism to preserve\ndifferential privacy in deep neural networks, such that: (1) The privacy budget\nconsumption is totally independent of the number of training steps; (2) It has\nthe ability to adaptively inject noise into features based on the contribution\nof each to the output; and (3) It could be applied in a variety of different\ndeep neural networks. To achieve this, we figure out a way to perturb affine\ntransformations of neurons, and loss functions used in deep neural networks. In\naddition, our mechanism intentionally adds \"more noise\" into features which are\n\"less relevant\" to the model output, and vice-versa. Our theoretical analysis\nfurther derives the sensitivities and error bounds of our mechanism. Rigorous\nexperiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is\nhighly effective and outperforms existing solutions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:37:40 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 02:45:14 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Phan", "NhatHai", ""], ["Wu", "Xintao", ""], ["Hu", "Han", ""], ["Dou", "Dejing", ""]]}, {"id": "1709.05790", "submitter": "Nobuyuki Yoshioka", "authors": "Nobuyuki Yoshioka, Yutaka Akagi, and Hosho Katsura", "title": "Learning Disordered Topological Phases by Statistical Recovery of\n  Symmetry", "comments": "9 pages, 7 figures, to appear in PRB", "journal-ref": "Phys. Rev. B 97, 205110 (2018)", "doi": "10.1103/PhysRevB.97.205110", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cond-mat.supr-con stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we apply the artificial neural network in a supervised manner\nto map out the quantum phase diagram of disordered topological superconductor\nin class DIII. Given the disorder that keeps the discrete symmetries of the\nensemble as a whole, translational symmetry which is broken in the\nquasiparticle distribution individually is recovered statistically by taking an\nensemble average. By using this, we classify the phases by the artificial\nneural network that learned the quasiparticle distribution in the clean limit,\nand show that the result is totally consistent with the calculation by the\ntransfer matrix method or noncommutative geometry approach. If all three\nphases, namely the $\\mathbb{Z}_2$, trivial, and the thermal metal phases appear\nin the clean limit, the machine can classify them with high confidence over the\nentire phase diagram. If only the former two phases are present, we find that\nthe machine remains confused in the certain region, leading us to conclude the\ndetection of the unknown phase which is eventually identified as the thermal\nmetal phase. In our method, only the first moment of the quasiparticle\ndistribution is used for input, but application to a wider variety of systems\nis expected by the inclusion of higher moments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 07:11:12 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 15:07:25 GMT"}, {"version": "v3", "created": "Sat, 28 Apr 2018 06:19:16 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yoshioka", "Nobuyuki", ""], ["Akagi", "Yutaka", ""], ["Katsura", "Hosho", ""]]}, {"id": "1709.05804", "submitter": "Bingzhen Wei", "authors": "Bingzhen Wei, Xu Sun, Xuancheng Ren, Jingjing Xu", "title": "Minimal Effort Back Propagation for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As traditional neural network consumes a significant amount of computing\nresources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet\neffective technique to alleviate this problem. In this technique, only a small\nsubset of the full gradients are computed to update the model parameters. In\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\nto reduce calculation in back propagation, and the surprising results verify\nits validity in CNN: only 5\\% of the gradients are passed back but the model\nstill achieves the same effect as the traditional CNN, or even better. We also\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\nback propagation, which may bring significant computational benefits for high\ncomputational complexity of convolution operation in CNN.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 08:07:40 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Wei", "Bingzhen", ""], ["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Xu", "Jingjing", ""]]}, {"id": "1709.05849", "submitter": "Andriy Temko Dr", "authors": "Alison O'Shea, Gordon Lightbody, Geraldine Boylan, Andriy Temko", "title": "Neonatal Seizure Detection using Convolutional Neural Networks", "comments": "IEEE International Workshop on Machine Learning for Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel end-to-end architecture that learns hierarchical\nrepresentations from raw EEG data using fully convolutional deep neural\nnetworks for the task of neonatal seizure detection. The deep neural network\nacts as both feature extractor and classifier, allowing for end-to-end\noptimization of the seizure detector. The designed system is evaluated on a\nlarge dataset of continuous unedited multi-channel neonatal EEG totaling 835\nhours and comprising of 1389 seizures. The proposed deep architecture, with\nsample-level filters, achieves an accuracy that is comparable to the\nstate-of-the-art SVM-based neonatal seizure detector, which operates on a set\nof carefully designed hand-crafted features. The fully convolutional\narchitecture allows for the localization of EEG waveforms and patterns that\nresult in high seizure probabilities for further clinical examination.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 10:30:43 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["O'Shea", "Alison", ""], ["Lightbody", "Gordon", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "1709.05870", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong\n  Gu, Yuhao Zhou", "title": "ZhuSuan: A Library for Bayesian Deep Learning", "comments": "The GitHub page is at https://github.com/thu-ml/zhusuan", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:30:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Shi", "Jiaxin", ""], ["Chen", "Jianfei", ""], ["Zhu", "Jun", ""], ["Sun", "Shengyang", ""], ["Luo", "Yucen", ""], ["Gu", "Yihong", ""], ["Zhou", "Yuhao", ""]]}, {"id": "1709.05885", "submitter": "Bangti Jin", "authors": "Simon Arridge, Kazufumi Ito, Bangti Jin, Chen Zhang", "title": "Variational Gaussian Approximation for Poisson Data", "comments": "26 pages", "journal-ref": null, "doi": "10.1088/1361-6420/aaa0ab", "report-no": null, "categories": "math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson model is frequently employed to describe count data, but in a\nBayesian context it leads to an analytically intractable posterior probability\ndistribution. In this work, we analyze a variational Gaussian approximation to\nthe posterior distribution arising from the Poisson model with a Gaussian\nprior. This is achieved by seeking an optimal Gaussian distribution minimizing\nthe Kullback-Leibler divergence from the posterior distribution to the\napproximation, or equivalently maximizing the lower bound for the model\nevidence. We derive an explicit expression for the lower bound, and show the\nexistence and uniqueness of the optimal Gaussian approximation. The lower bound\nfunctional can be viewed as a variant of classical Tikhonov regularization that\npenalizes also the covariance. Then we develop an efficient alternating\ndirection maximization algorithm for solving the optimization problem, and\nanalyze its convergence. We discuss strategies for reducing the computational\ncomplexity via low rank structure of the forward operator and the sparsity of\nthe covariance. Further, as an application of the lower bound, we discuss\nhierarchical Bayesian modeling for selecting the hyperparameter in the prior\ndistribution, and propose a monotonically convergent algorithm for determining\nthe hyperparameter. We present extensive numerical experiments to illustrate\nthe Gaussian approximation and the algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 12:12:12 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Arridge", "Simon", ""], ["Ito", "Kazufumi", ""], ["Jin", "Bangti", ""], ["Zhang", "Chen", ""]]}, {"id": "1709.05963", "submitter": "Christian Beck", "authors": "Christian Beck, Weinan E, and Arnulf Jentzen", "title": "Machine learning approximation algorithms for high-dimensional fully\n  nonlinear partial differential equations and second-order backward stochastic\n  differential equations", "comments": "56 pages, 12 figures", "journal-ref": "J. Nonlinear Sci. 29, 1563-1619 (2019)", "doi": "10.1007/s00332-018-9525-3", "report-no": null, "categories": "math.NA cs.LG cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional partial differential equations (PDE) appear in a number of\nmodels from the financial industry, such as in derivative pricing models,\ncredit valuation adjustment (CVA) models, or portfolio optimization models. The\nPDEs in such applications are high-dimensional as the dimension corresponds to\nthe number of financial assets in a portfolio. Moreover, such PDEs are often\nfully nonlinear due to the need to incorporate certain nonlinear phenomena in\nthe model such as default risks, transaction costs, volatility uncertainty\n(Knightian uncertainty), or trading constraints in the model. Such\nhigh-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the\ncomputational effort for standard approximation methods grows exponentially\nwith the dimension. In this work we propose a new method for solving\nhigh-dimensional fully nonlinear second-order PDEs. Our method can in\nparticular be used to sample from high-dimensional nonlinear expectations. The\nmethod is based on (i) a connection between fully nonlinear second-order PDEs\nand second-order backward stochastic differential equations (2BSDEs), (ii) a\nmerged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward\ndiscretization of the 2BSDE and a spatial approximation via deep neural nets,\nand (iv) a stochastic gradient descent-type optimization procedure. Numerical\nresults obtained using ${\\rm T{\\small ENSOR}F{\\small LOW}}$ in ${\\rm P{\\small\nYTHON}}$ illustrate the efficiency and the accuracy of the method in the cases\nof a $100$-dimensional Black-Scholes-Barenblatt equation, a $100$-dimensional\nHamilton-Jacobi-Bellman equation, and a nonlinear expectation of a $ 100\n$-dimensional $ G $-Brownian motion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 14:16:06 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Beck", "Christian", ""], ["E", "Weinan", ""], ["Jentzen", "Arnulf", ""]]}, {"id": "1709.05964", "submitter": "Hajin Shim", "authors": "Hajin Shim, Sung Ju Hwang, Eunho Yang", "title": "Why Pay More When You Can Pay Less: A Joint Learning Framework for\n  Active Feature Acquisition and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of active feature acquisition, where we sequentially\nselect the subset of features in order to achieve the maximum prediction\nperformance in the most cost-effective way. In this work, we formulate this\nactive feature acquisition problem as a reinforcement learning problem, and\nprovide a novel framework for jointly learning both the RL agent and the\nclassifier (environment). We also introduce a more systematic way of encoding\nsubsets of features that can properly handle innate challenge with missing\nentries in active feature acquisition problems, that uses the orderless\nLSTM-based set encoding mechanism that readily fits in the joint learning\nframework. We evaluate our model on a carefully designed synthetic dataset for\nthe active feature acquisition as well as several real datasets such as\nelectric health record (EHR) datasets, on which it outperforms all baselines in\nterms of prediction performance as well feature acquisition cost.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 14:17:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Shim", "Hajin", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "1709.06010", "submitter": "Surbhi Goel", "authors": "Surbhi Goel and Adam Klivans", "title": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time", "comments": "Changed title, included new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial-time algorithm for learning neural networks with one\nlayer of sigmoids feeding into any Lipschitz, monotone activation function\n(e.g., sigmoid or ReLU). We make no assumptions on the structure of the\nnetwork, and the algorithm succeeds with respect to {\\em any} distribution on\nthe unit ball in $n$ dimensions (hidden weight vectors also have unit norm).\nThis is the first assumption-free, provably efficient algorithm for learning\nneural networks with two nonlinear layers.\n  Our algorithm-- {\\em Alphatron}-- is a simple, iterative update rule that\ncombines isotonic regression with kernel methods. It outputs a hypothesis that\nyields efficient oracle access to interpretable features. It also suggests a\nnew approach to Boolean learning problems via real-valued conditional-mean\nfunctions, sidestepping traditional hardness results from computational\nlearning theory.\n  Along these lines, we subsume and improve many longstanding results for PAC\nlearning Boolean functions to the more general, real-valued setting of {\\em\nprobabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.\nnoise-tolerance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 15:36:06 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 17:29:51 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 17:14:38 GMT"}, {"version": "v4", "created": "Fri, 20 Apr 2018 20:16:02 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""]]}, {"id": "1709.06011", "submitter": "Maximilian Huettenrauch", "authors": "Maximilian H\\\"uttenrauch and Adrian \\v{S}o\\v{s}i\\'c and Gerhard\n  Neumann", "title": "Guided Deep Reinforcement Learning for Swarm Systems", "comments": "15 pages, 8 figures, accepted at the AAMAS 2017 Autonomous Robots and\n  Multirobot Systems (ARMS) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to learn to control a group of cooperative\nagents with limited sensing capabilities such as robot swarms. The agents have\nonly very basic sensor capabilities, yet in a group they can accomplish\nsophisticated tasks, such as distributed assembly or search and rescue tasks.\nLearning a policy for a group of agents is difficult due to distributed partial\nobservability of the state. Here, we follow a guided approach where a critic\nhas central access to the global state during learning, which simplifies the\npolicy evaluation problem from a reinforcement learning point of view. For\nexample, we can get the positions of all robots of the swarm using a camera\nimage of a scene. This camera image is only available to the critic and not to\nthe control policies of the robots. We follow an actor-critic approach, where\nthe actors base their decisions only on locally sensed information. In\ncontrast, the critic is learned based on the true global state. Our algorithm\nuses deep reinforcement learning to approximate both the Q-function and the\npolicy. The performance of the algorithm is evaluated on two tasks with simple\nsimulated 2D agents: 1) finding and maintaining a certain distance to each\nothers and 2) locating a target.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 15:37:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["H\u00fcttenrauch", "Maximilian", ""], ["\u0160o\u0161i\u0107", "Adrian", ""], ["Neumann", "Gerhard", ""]]}, {"id": "1709.06030", "submitter": "Bhav Ashok", "authors": "Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, Kris M. Kitani", "title": "N2N Learning: Network to Network Compression via Policy Gradient\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While bigger and deeper neural network architectures continue to advance the\nstate-of-the-art for many computer vision tasks, real-world adoption of these\nnetworks is impeded by hardware and speed constraints. Conventional model\ncompression methods attempt to address this problem by modifying the\narchitecture manually or using pre-defined heuristics. Since the space of all\nreduced architectures is very large, modifying the architecture of a deep\nneural network in this way is a difficult task. In this paper, we tackle this\nissue by introducing a principled method for learning reduced network\narchitectures in a data-driven way using reinforcement learning. Our approach\ntakes a larger `teacher' network as input and outputs a compressed `student'\nnetwork derived from the `teacher' network. In the first stage of our method, a\nrecurrent policy network aggressively removes layers from the large `teacher'\nmodel. In the second stage, another recurrent policy network carefully reduces\nthe size of each remaining layer. The resulting network is then evaluated to\nobtain a reward -- a score based on the accuracy and compression of the\nnetwork. Our approach uses this reward signal with policy gradients to train\nthe policies to find a locally optimal student network. Our experiments show\nthat we can achieve compression rates of more than 10x for models such as\nResNet-34 while maintaining similar performance to the input `teacher' network.\nWe also present a valuable transfer learning result which shows that policies\nwhich are pre-trained on smaller `teacher' networks can be used to rapidly\nspeed up training on larger `teacher' networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 16:26:53 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 11:46:06 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Ashok", "Anubhav", ""], ["Rhinehart", "Nicholas", ""], ["Beainy", "Fares", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1709.06053", "submitter": "Georges Qu\\'enot", "authors": "Anuvabh Dutt, Denis Pellerin and Georges Qu\\'enot", "title": "Coupled Ensembles of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate in this paper the architecture of deep convolutional networks.\nBuilding on existing state of the art models, we propose a reconfiguration of\nthe model parameters into several parallel branches at the global network\nlevel, with each branch being a standalone CNN. We show that this arrangement\nis an efficient way to significantly reduce the number of parameters without\nlosing performance or to significantly improve the performance with the same\nlevel of performance. The use of branches brings an additional form of\nregularization. In addition to the split into parallel branches, we propose a\ntighter coupling of these branches by placing the \"fuse (averaging) layer\"\nbefore the Log-Likelihood and SoftMax layers during training. This gives\nanother significant performance improvement, the tighter coupling favouring the\nlearning of better representations, even at the level of the individual\nbranches. We refer to this branched architecture as \"coupled ensembles\". The\napproach is very generic and can be applied with almost any DCNN architecture.\nWith coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain\nerror rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and\nSVHN tasks. For the same budget, DenseNet-BC has error rate of 3.46%, 17.18%,\nand 1.8% respectively. With ensembles of coupled ensembles, of DenseNet-BC\nnetworks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and\n1.42% respectively on these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:16:26 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Dutt", "Anuvabh", ""], ["Pellerin", "Denis", ""], ["Qu\u00e9not", "Georges", ""]]}, {"id": "1709.06109", "submitter": "Yining Wang", "authors": "Xi Chen, Yining Wang", "title": "A Note on a Tight Lower Bound for MNL-Bandit Assortment Selection Models", "comments": "Final version, 4 pages (double column)", "journal-ref": "Operations Research Letters 46(5):534-537, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we consider a dynamic assortment planning problem under\nthe capacitated multinomial logit (MNL) bandit model. We prove a tight lower\nbound on the accumulated regret that matches existing regret upper bounds for\nall parameters (time horizon $T$, number of items $N$ and maximum assortment\ncapacity $K$) up to logarithmic factors. Our results close an $O(\\sqrt{K})$ gap\nbetween upper and lower regret bounds from existing works.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 18:12:10 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 19:30:54 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2018 17:29:56 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Chen", "Xi", ""], ["Wang", "Yining", ""]]}, {"id": "1709.06123", "submitter": "Qinliang Su", "authors": "Qinliang Su, Xuejun Liao, Lawrence Carin", "title": "A Probabilistic Framework for Nonlinearities in Stochastic Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic framework for nonlinearities, based on doubly\ntruncated Gaussian distributions. By setting the truncation points\nappropriately, we are able to generate various types of nonlinearities within a\nunified framework, including sigmoid, tanh and ReLU, the most commonly used\nnonlinearities in neural networks. The framework readily integrates into\nexisting stochastic neural networks (with hidden units characterized as random\nvariables), allowing one for the first time to learn the nonlinearities\nalongside model weights in these networks. Extensive experiments demonstrate\nthe performance improvements brought about by the proposed framework when\nintegrated with the restricted Boltzmann machine (RBM), temporal RBM and the\ntruncated Gaussian graphical model (TGGM).\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:01:53 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Su", "Qinliang", ""], ["Liao", "Xuejun", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.06129", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Yuandong Tian", "title": "When is a Convolutional Filter Easy To Learn?", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence of (stochastic) gradient descent algorithm for\nlearning a convolutional filter with Rectified Linear Unit (ReLU) activation\nfunction. Our analysis does not rely on any specific form of the input\ndistribution and our proofs only use the definition of ReLU, in contrast with\nprevious works that are restricted to standard Gaussian input. We show that\n(stochastic) gradient descent with random initialization can learn the\nconvolutional filter in polynomial time and the convergence rate depends on the\nsmoothness of the input distribution and the closeness of patches. To the best\nof our knowledge, this is the first recovery guarantee of gradient-based\nalgorithms for convolutional filter on non-Gaussian input distributions. Our\ntheory also justifies the two-stage learning rate strategy in deep neural\nnetworks. While our focus is theoretical, we also present experiments that\nillustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:09:24 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 17:08:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Tian", "Yuandong", ""]]}, {"id": "1709.06138", "submitter": "Rajat Sen", "authors": "Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros\n  G. Dimakis and Sanjay Shakkottai", "title": "Model-Powered Conditional Independence Test", "comments": "19 Pages, 2 figures, Accepted for publication in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of non-parametric Conditional Independence testing\n(CI testing) for continuous random variables. Given i.i.d samples from the\njoint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we\ndetermine whether $X \\perp Y | Z$. We approach this by converting the\nconditional independence test into a classification problem. This allows us to\nharness very powerful classifiers like gradient-boosted trees and deep neural\nnetworks. These models can handle complex probability distributions and allow\nus to perform significantly better compared to the prior state of the art, for\nhigh-dimensional CI testing. The main technical challenge in the classification\nproblem is the need for samples from the conditional product distribution\n$f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X\n\\perp Y | Z.$ -- when given access only to i.i.d. samples from the true joint\ndistribution $f(x,y,z)$. To tackle this problem we propose a novel nearest\nneighbor bootstrap procedure and theoretically show that our generated samples\nare indeed close to $f^{CI}$ in terms of total variational distance. We then\ndevelop theoretical results regarding the generalization bounds for\nclassification for our problem, which translate into error bounds for CI\ntesting. We provide a novel analysis of Rademacher type classification bounds\nin the presence of non-i.i.d near-independent samples. We empirically validate\nthe performance of our algorithm on simulated and real datasets and show\nperformance gains over previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:56:07 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Sen", "Rajat", ""], ["Suresh", "Ananda Theertha", ""], ["Shanmugam", "Karthikeyan", ""], ["Dimakis", "Alexandros G.", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1709.06171", "submitter": "Blake Mason", "authors": "Lalit Jain, Blake Mason, and Robert Nowak", "title": "Learning Low-Dimensional Metrics", "comments": "19 pages, 3 figures, Accepted at NIPS 2017 - Edited version to match\n  final submission to NIPS proceedings and correct several spelling errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the theoretical foundations of metric learning,\nfocused on three key questions that are not fully addressed in prior work: 1)\nwe consider learning general low-dimensional (low-rank) metrics as well as\nsparse metrics; 2) we develop upper and lower (minimax)bounds on the\ngeneralization error; 3) we quantify the sample complexity of metric learning\nin terms of the dimension of the feature space and the dimension/rank of the\nunderlying metric;4) we also bound the accuracy of the learned metric relative\nto the underlying true generative metric. All the results involve novel\nmathematical approaches to the metric learning problem, and lso shed new light\non the special case of ordinal embedding (aka non-metric multidimensional\nscaling).\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 21:26:43 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 20:10:22 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Jain", "Lalit", ""], ["Mason", "Blake", ""], ["Nowak", "Robert", ""]]}, {"id": "1709.06181", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank\n  Wood", "title": "On Nesting Monte Carlo Estimators", "comments": "To appear at International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:01:05 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:36:06 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:04:11 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 17:11:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Warrington", "Andrew", ""], ["Wood", "Frank", ""]]}, {"id": "1709.06201", "submitter": "Jaedeok Kim", "authors": "Jaedeok Kim, and Jingoo Seo", "title": "Human Understandable Explanation Extraction for Black-box Classification\n  Models Based on Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a number of artificial intelligent services have been\ndeveloped such as defect detection system or diagnosis system for customer\nservices. Unfortunately, the core in these services is a black-box in which\nhuman cannot understand the underlying decision making logic, even though the\ninspection of the logic is crucial before launching a commercial service. Our\ngoal in this paper is to propose an analytic method of a model explanation that\nis applicable to general classification models. To this end, we introduce the\nconcept of a contribution matrix and an explanation embedding in a constraint\nspace by using a matrix factorization. We extract a rule-like model explanation\nfrom the contribution matrix with the help of the nonnegative matrix\nfactorization. To validate our method, the experiment results provide with open\ndatasets as well as an industry dataset of a LTE network diagnosis and the\nresults show our method extracts reasonable explanations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 23:44:45 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kim", "Jaedeok", ""], ["Seo", "Jingoo", ""]]}, {"id": "1709.06255", "submitter": "Praneeth Narayanamurthy", "authors": "Namrata Vaswani and Praneeth Narayanamurthy", "title": "Finite Sample Guarantees for PCA in Non-Isotropic and Data-Dependent\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work obtains novel finite sample guarantees for Principal Component\nAnalysis (PCA). These hold even when the corrupting noise is non-isotropic, and\na part (or all of it) is data-dependent. Because of the latter, in general, the\nnoise and the true data are correlated. The results in this work are a\nsignificant improvement over those given in our earlier work where this\n\"correlated-PCA\" problem was first studied. In fact, in certain regimes, our\nresults imply that the sample complexity required to achieve subspace recovery\nerror that is a constant fraction of the noise level is near-optimal. Useful\ncorollaries of our result include guarantees for PCA in sparse data-dependent\nnoise and for PCA with missing data. An important application of the former is\nin proving correctness of the subspace update step of a popular online\nalgorithm for dynamic robust PCA.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 04:56:10 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Vaswani", "Namrata", ""], ["Narayanamurthy", "Praneeth", ""]]}, {"id": "1709.06293", "submitter": "Kyungjae Lee", "authors": "Kyungjae Lee, Sungjoon Choi and Songhwai Oh", "title": "Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy\n  Regularization for Reinforcement Learning", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a sparse Markov decision process (MDP) with novel causal\nsparse Tsallis entropy regularization is proposed.The proposed policy\nregularization induces a sparse and multi-modal optimal policy distribution of\na sparse MDP. The full mathematical analysis of the proposed sparse MDP is\nprovided.We first analyze the optimality condition of a sparse MDP. Then, we\npropose a sparse value iteration method which solves a sparse MDP and then\nprove the convergence and optimality of sparse value iteration using the Banach\nfixed point theorem. The proposed sparse MDP is compared to soft MDPs which\nutilize causal entropy regularization. We show that the performance error of a\nsparse MDP has a constant bound, while the error of a soft MDP increases\nlogarithmically with respect to the number of actions, where this performance\nerror is caused by the introduced regularization term. In experiments, we apply\nsparse MDPs to reinforcement learning problems. The proposed method outperforms\nexisting methods in terms of the convergence speed and performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 08:36:21 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 10:57:25 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 06:22:59 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Lee", "Kyungjae", ""], ["Choi", "Sungjoon", ""], ["Oh", "Songhwai", ""]]}, {"id": "1709.06298", "submitter": "Hao-Wen Dong", "authors": "Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan Yang", "title": "MuseGAN: Multi-track Sequential Generative Adversarial Networks for\n  Symbolic Music Generation and Accompaniment", "comments": "to appear at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating music has a few notable differences from generating images and\nvideos. First, music is an art of time, necessitating a temporal model. Second,\nmusic is usually composed of multiple instruments/tracks with their own\ntemporal dynamics, but collectively they unfold over time interdependently.\nLastly, musical notes are often grouped into chords, arpeggios or melodies in\npolyphonic music, and thereby introducing a chronological ordering of notes is\nnot naturally suitable. In this paper, we propose three models for symbolic\nmulti-track music generation under the framework of generative adversarial\nnetworks (GANs). The three models, which differ in the underlying assumptions\nand accordingly the network architectures, are referred to as the jamming\nmodel, the composer model and the hybrid model. We trained the proposed models\non a dataset of over one hundred thousand bars of rock music and applied them\nto generate piano-rolls of five tracks: bass, drums, guitar, piano and strings.\nA few intra-track and inter-track objective metrics are also proposed to\nevaluate the generative results, in addition to a subjective user study. We\nshow that our models can generate coherent music of four bars right from\nscratch (i.e. without human inputs). We also extend our models to human-AI\ncooperative music generation: given a specific track composed by human, we can\ngenerate four additional tracks to accompany it. All code, the dataset and the\nrendered audio samples are available at https://salu133445.github.io/musegan/ .\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 08:49:40 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 05:11:39 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Dong", "Hao-Wen", ""], ["Hsiao", "Wen-Yi", ""], ["Yang", "Li-Chia", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1709.06304", "submitter": "Ruohui Wang", "authors": "Ruohui Wang, Dahua Lin", "title": "Scalable Estimation of Dirichlet Process Mixture Models on Distributed\n  Data", "comments": "This paper is published on IJCAI 2017.\n  https://www.ijcai.org/proceedings/2017/646", "journal-ref": null, "doi": "10.24963/ijcai.2017/646", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of Dirichlet Process Mixture Models (DPMMs) in\ndistributed environments, where data are distributed across multiple computing\nnodes. A key advantage of Bayesian nonparametric models such as DPMMs is that\nthey allow new components to be introduced on the fly as needed. This, however,\nposts an important challenge to distributed estimation -- how to handle new\ncomponents efficiently and consistently. To tackle this problem, we propose a\nnew estimation method, which allows new components to be created locally in\nindividual computing nodes. Components corresponding to the same cluster will\nbe identified and merged via a probabilistic consolidation scheme. In this way,\nwe can maintain the consistency of estimation with very low communication cost.\nExperiments on large real-world data sets show that the proposed method can\nachieve high scalability in distributed and asynchronous environments without\ncompromising the mixing performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:05:14 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Ruohui", ""], ["Lin", "Dahua", ""]]}, {"id": "1709.06320", "submitter": "Jiali Mei", "authors": "Jiali Mei, Yohann De Castro, Yannig Goude, Jean-Marc Aza\\\"is, Georges\n  H\\'ebrail", "title": "Nonnegative matrix factorization with side information for time series\n  recovery and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the reconstruction and the prediction of electricity\nconsumption, we extend Nonnegative Matrix Factorization~(NMF) to take into\naccount side information (column or row features). We consider general linear\nmeasurement settings, and propose a framework which models non-linear\nrelationships between features and the response variables. We extend previous\ntheoretical results to obtain a sufficient condition on the identifiability of\nthe NMF in this setting. Based the classical Hierarchical Alternating Least\nSquares~(HALS) algorithm, we propose a new algorithm (HALSX, or Hierarchical\nAlternating Least Squares with eXogeneous variables) which estimates the\nfactorization model. The algorithm is validated on both simulated and real\nelectricity consumption datasets as well as a recommendation dataset, to show\nits performance in matrix recovery and prediction for new rows and columns.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:55:07 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Mei", "Jiali", ""], ["De Castro", "Yohann", ""], ["Goude", "Yannig", ""], ["Aza\u00efs", "Jean-Marc", ""], ["H\u00e9brail", "Georges", ""]]}, {"id": "1709.06390", "submitter": "Minh Trung Le", "authors": "Trung Le, Khanh Nguyen, Tu Dinh Nguyen, Dinh Phung", "title": "Analogical-based Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some real-world problems revolve to solve the optimization problem\n\\max_{x\\in\\mathcal{X}}f\\left(x\\right) where f\\left(.\\right) is a black-box\nfunction and X might be the set of non-vectorial objects (e.g., distributions)\nwhere we can only define a symmetric and non-negative similarity score on it.\nThis setting requires a novel view for the standard framework of Bayesian\nOptimization that generalizes the core insightful spirit of this framework.\nWith this spirit, in this paper, we propose Analogical-based Bayesian\nOptimization that can maximize black-box function over a domain where only a\nsimilarity score can be defined. Our pathway is as follows: we first base on\nthe geometric view of Gaussian Processes (GP) to define the concept of\ninfluence level that allows us to analytically represent predictive means and\nvariances of GP posteriors and base on that view to enable replacing kernel\nsimilarity by a more genetic similarity score. Furthermore, we also propose two\nstrategies to find a batch of query points that can efficiently handle high\ndimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:06:39 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Khanh", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1709.06404", "submitter": "Ga\\\"etan Hadjeres", "authors": "Ga\\\"etan Hadjeres and Frank Nielsen", "title": "Interactive Music Generation with Positional Constraints using\n  Anticipation-RNNs", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNS) are now widely used on sequence generation\ntasks due to their ability to learn long-range dependencies and to generate\nsequences of arbitrary length. However, their left-to-right generation\nprocedure only allows a limited control from a potential user which makes them\nunsuitable for interactive and creative usages such as interactive music\ngeneration. This paper introduces a novel architecture called Anticipation-RNN\nwhich possesses the assets of the RNN-based generative models while allowing to\nenforce user-defined positional constraints. We demonstrate its efficiency on\nthe task of generating melodies satisfying positional constraints in the style\nof the soprano parts of the J.S. Bach chorale harmonizations. Sampling using\nthe Anticipation-RNN is of the same order of complexity than sampling from the\ntraditional RNN model. This fast and interactive generation of musical\nsequences opens ways to devise real-time systems that could be used for\ncreative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:29:53 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Hadjeres", "Ga\u00ebtan", ""], ["Nielsen", "Frank", ""]]}, {"id": "1709.06489", "submitter": "Louis Lello", "authors": "Louis Lello, Steven G. Avery, Laurent Tellier, Ana Vazquez, Gustavo de\n  los Campos, Stephen D.H. Hsu", "title": "Accurate Genomic Prediction Of Human Height", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We construct genomic predictors for heritable and extremely complex human\nquantitative traits (height, heel bone density, and educational attainment)\nusing modern methods in high dimensional statistics (i.e., machine learning).\nReplication tests show that these predictors capture, respectively, $\\sim$40,\n20, and 9 percent of total variance for the three traits. For example,\npredicted heights correlate $\\sim$0.65 with actual height; actual heights of\nmost individuals in validation samples are within a few cm of the prediction.\nThe variance captured for height is comparable to the estimated SNP\nheritability from GCTA (GREML) analysis, and seems to be close to its\nasymptotic value (i.e., as sample size goes to infinity), suggesting that we\nhave captured most of the heritability for the SNPs used. Thus, our results\nresolve the common SNP portion of the \"missing heritability\" problem -- i.e.,\nthe gap between prediction R-squared and SNP heritability. The $\\sim$20k\nactivated SNPs in our height predictor reveal the genetic architecture of human\nheight, at least for common SNPs. Our primary dataset is the UK Biobank cohort,\ncomprised of almost 500k individual genotypes with multiple phenotypes. We also\nuse other datasets and SNPs found in earlier GWAS for out-of-sample validation\nof our results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 15:32:37 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Lello", "Louis", ""], ["Avery", "Steven G.", ""], ["Tellier", "Laurent", ""], ["Vazquez", "Ana", ""], ["Campos", "Gustavo de los", ""], ["Hsu", "Stephen D. H.", ""]]}, {"id": "1709.06493", "submitter": "Wei Zhang", "authors": "Wei Zhang, Bowen Zhou", "title": "Learning to update Auto-associative Memory in Recurrent Neural Networks\n  for Improving Sequence Memorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to remember long sequences remains a challenging task for recurrent\nneural networks. Register memory and attention mechanisms were both proposed to\nresolve the issue with either high computational cost to retain memory\ndifferentiability, or by discounting the RNN representation learning towards\nencoding shorter local contexts than encouraging long sequence encoding.\nAssociative memory, which studies the compression of multiple patterns in a\nfixed size memory, were rarely considered in recent years. Although some recent\nwork tries to introduce associative memory in RNN and mimic the energy decay\nprocess in Hopfield nets, it inherits the shortcoming of rule-based memory\nupdates, and the memory capacity is limited. This paper proposes a method to\nlearn the memory update rule jointly with task objective to improve memory\ncapacity for remembering long sequences. Also, we propose an architecture that\nuses multiple such associative memory for more complex input encoding. We\nobserved some interesting facts when compared to other RNN architectures on\nsome well-studied sequence learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 15:55:16 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 20:52:46 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 14:31:03 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Zhang", "Wei", ""], ["Zhou", "Bowen", ""]]}, {"id": "1709.06525", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Yash Deshpande, Andrea Montanari", "title": "Inference in Graphical Models via Semidefinite Programming Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum A posteriori Probability (MAP) inference in graphical models amounts\nto solving a graph-structured combinatorial optimization problem. Popular\ninference algorithms such as belief propagation (BP) and generalized belief\npropagation (GBP) are intimately related to linear programming (LP) relaxation\nwithin the Sherali-Adams hierarchy. Despite the popularity of these algorithms,\nit is well understood that the Sum-of-Squares (SOS) hierarchy based on\nsemidefinite programming (SDP) can provide superior guarantees. Unfortunately,\nSOS relaxations for a graph with $n$ vertices require solving an SDP with\n$n^{\\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In\npractice, for $d\\ge 4$, this approach does not scale beyond a few tens of\nvariables. In this paper, we propose binary SDP relaxations for MAP inference\nusing the SOS hierarchy with two innovations focused on computational\nefficiency. Firstly, in analogy to BP and its variants, we only introduce\ndecision variables corresponding to contiguous regions in the graphical model.\nSecondly, we solve the resulting SDP using a non-convex Burer-Monteiro style\nmethod, and develop a sequential rounding procedure. We demonstrate that the\nresulting algorithm can solve problems with tens of thousands of variables\nwithin minutes, and outperforms BP and GBP on practical problems such as image\ndenoising and Ising spin glasses. Finally, for specific graph types, we\nestablish a sufficient condition for the tightness of the proposed partial SOS\nrelaxation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 16:45:30 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""]]}, {"id": "1709.06533", "submitter": "Christopher Grimm", "authors": "Christopher Grimm, Yuhang Song and Michael L. Littman", "title": "Summable Reparameterizations of Wasserstein Critics in the\n  One-Dimensional Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are an exciting alternative to\nalgorithms for solving density estimation problems---using data to assess how\nlikely samples are to be drawn from the same distribution. Instead of\nexplicitly computing these probabilities, GANs learn a generator that can match\nthe given probabilistic source. This paper looks particularly at this matching\ncapability in the context of problems with one-dimensional outputs. We identify\na class of function decompositions with properties that make them well suited\nto the critic role in a leading approach to GANs known as Wasserstein GANs. We\nshow that Taylor and Fourier series decompositions belong to our class, provide\nexamples of these critics outperforming standard GAN approaches, and suggest\nhow they can be scaled to higher dimensional problems in the future.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 17:03:17 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Grimm", "Christopher", ""], ["Song", "Yuhang", ""], ["Littman", "Michael L.", ""]]}, {"id": "1709.06537", "submitter": "You-Luen Lee", "authors": "You-Luen Lee, Da-Cheng Juan, Xuan-An Tseng, Yu-Ting Chen, and\n  Shih-Chieh Chang", "title": "DC-Prophet: Predicting Catastrophic Machine Failures in DataCenters", "comments": "13 pages, 5 figures, accepted by 2017 ECML PKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When will a server fail catastrophically in an industrial datacenter? Is it\npossible to forecast these failures so preventive actions can be taken to\nincrease the reliability of a datacenter? To answer these questions, we have\nstudied what are probably the largest, publicly available datacenter traces,\ncontaining more than 104 million events from 12,500 machines. Among these\nsamples, we observe and categorize three types of machine failures, all of\nwhich are catastrophic and may lead to information loss, or even worse,\nreliability degradation of a datacenter. We further propose a two-stage\nframework-DC-Prophet-based on One-Class Support Vector Machine and Random\nForest. DC-Prophet extracts surprising patterns and accurately predicts the\nnext failure of a machine. Experimental results show that DC-Prophet achieves\nan AUC of 0.93 in predicting the next machine failure, and a F3-score of 0.88\n(out of 1). On average, DC-Prophet outperforms other classical machine learning\nmethods by 39.45% in F3-score.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 07:46:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Lee", "You-Luen", ""], ["Juan", "Da-Cheng", ""], ["Tseng", "Xuan-An", ""], ["Chen", "Yu-Ting", ""], ["Chang", "Shih-Chieh", ""]]}, {"id": "1709.06548", "submitter": "Zhe Gan", "authors": "Zhe Gan, Liqun Chen, Weiyao Wang, Yunchen Pu, Yizhe Zhang, Hao Liu,\n  Chunyuan Li, Lawrence Carin", "title": "Triangle Generative Adversarial Networks", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for\nsemi-supervised cross-domain joint distribution matching, where the training\ndata consists of samples from each domain, and supervision of domain\ncorrespondence is provided by only a few paired samples. $\\Delta$-GAN consists\nof four neural networks, two generators and two discriminators. The generators\nare designed to learn the two-way conditional distributions between the two\ndomains, while the discriminators implicitly define a ternary discriminative\nfunction, which is trained to distinguish real data pairs and two kinds of fake\ndata pairs. The generators and discriminators are trained together using\nadversarial learning. Under mild assumptions, in theory the joint distributions\ncharacterized by the two generators concentrate to the data distribution. In\nexperiments, three different kinds of domain pairs are considered, image-label,\nimage-image and image-attribute pairs. Experiments on semi-supervised image\nclassification, image-to-image translation and attribute-based image generation\ndemonstrate the superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 17:50:40 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 23:41:49 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Gan", "Zhe", ""], ["Chen", "Liqun", ""], ["Wang", "Weiyao", ""], ["Pu", "Yunchen", ""], ["Zhang", "Yizhe", ""], ["Liu", "Hao", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.06557", "submitter": "Amir-Hossein Karimi", "authors": "Amir-Hossein Karimi", "title": "A Summary Of The Kernel Matrix, And How To Learn It Effectively Using\n  Semidefinite Programming", "comments": "Independent study / summary, 20 pages total (14 pages main body, 6\n  pages appendices and proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based learning algorithms are widely used in machine learning for\nproblems that make use of the similarity between object pairs. Such algorithms\nfirst embed all data points into an alternative space, where the inner product\nbetween object pairs specifies their distance in the embedding space. Applying\nkernel methods to partially labeled datasets is a classical challenge in this\nregard, requiring that the distances between unlabeled pairs must somehow be\nlearnt using the labeled data. In this independent study, I will summarize the\nwork of G. Lanckriet et al.'s work on \"Learning the Kernel Matrix with\nSemidefinite Programming\" used in support vector machines (SVM) algorithms for\nthe transduction problem. Throughout the report, I have provide alternative\nexplanations / derivations / analysis related to this work which is designed to\nease the understanding of the original article.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 23:17:58 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Karimi", "Amir-Hossein", ""]]}, {"id": "1709.06560", "submitter": "Peter Henderson", "authors": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina\n  Precup, David Meger", "title": "Deep Reinforcement Learning that Matters", "comments": "Accepted to the Thirthy-Second AAAI Conference On Artificial\n  Intelligence (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, significant progress has been made in solving challenging\nproblems across various domains using deep reinforcement learning (RL).\nReproducing existing work and accurately judging the improvements offered by\nnovel methods is vital to sustaining this progress. Unfortunately, reproducing\nresults for state-of-the-art deep RL methods is seldom straightforward. In\nparticular, non-determinism in standard benchmark environments, combined with\nvariance intrinsic to the methods, can make reported results tough to\ninterpret. Without significance metrics and tighter standardization of\nexperimental reporting, it is difficult to determine whether improvements over\nthe prior state-of-the-art are meaningful. In this paper, we investigate\nchallenges posed by reproducibility, proper experimental techniques, and\nreporting procedures. We illustrate the variability in reported metrics and\nresults when comparing against common baselines and suggest guidelines to make\nfuture results in deep RL more reproducible. We aim to spur discussion about\nhow to ensure continued progress in the field by minimizing wasted effort\nstemming from results that are non-reproducible and easily misinterpreted.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 06:09:47 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 19:51:33 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 04:21:41 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Henderson", "Peter", ""], ["Islam", "Riashat", ""], ["Bachman", "Philip", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""], ["Meger", "David", ""]]}, {"id": "1709.06622", "submitter": "Chun-Nan Chou", "authors": "Shang-Xuan Zou, Chun-Yen Chen, Jui-Lin Wu, Chun-Nan Chou, Chia-Chin\n  Tsao, Kuan-Chieh Tung, Ting-Wei Lin, Cheng-Lung Sung, and Edward Y. Chang", "title": "Distributed Training Large-Scale Deep Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale of data and scale of computation infrastructures together enable the\ncurrent deep learning renaissance. However, training large-scale deep\narchitectures demands both algorithmic improvement and careful system\nconfiguration. In this paper, we focus on employing the system approach to\nspeed up large-scale training. Via lessons learned from our routine\nbenchmarking effort, we first identify bottlenecks and overheads that hinter\ndata parallelism. We then devise guidelines that help practitioners to\nconfigure an effective system and fine-tune parameters to achieve desired\nspeedup. Specifically, we develop a procedure for setting minibatch size and\nchoosing computation algorithms. We also derive lemmas for determining the\nquantity of key components such as the number of GPUs and parameter servers.\nExperiments and examples show that these guidelines help effectively speed up\nlarge-scale deep learning training.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:24:27 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Zou", "Shang-Xuan", ""], ["Chen", "Chun-Yen", ""], ["Wu", "Jui-Lin", ""], ["Chou", "Chun-Nan", ""], ["Tsao", "Chia-Chin", ""], ["Tung", "Kuan-Chieh", ""], ["Lin", "Ting-Wei", ""], ["Sung", "Cheng-Lung", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1709.06636", "submitter": "Meng Qu", "authors": "Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, Jiawei Han", "title": "An Attention-based Collaboration Framework for Multi-View Network\n  Representation Learning", "comments": "CIKM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning distributed node representations in networks has been attracting\nincreasing attention recently due to its effectiveness in a variety of\napplications. Existing approaches usually study networks with a single type of\nproximity between nodes, which defines a single view of a network. However, in\nreality there usually exists multiple types of proximities between nodes,\nyielding networks with multiple views. This paper studies learning node\nrepresentations for networks with multiple views, which aims to infer robust\nnode representations across different views. We propose a multi-view\nrepresentation learning approach, which promotes the collaboration of different\nviews and lets them vote for the robust representations. During the voting\nprocess, an attention mechanism is introduced, which enables each node to focus\non the most informative views. Experimental results on real-world networks show\nthat the proposed approach outperforms existing state-of-the-art approaches for\nnetwork representation learning with a single view and other competitive\napproaches with multiple views.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:30:30 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Qu", "Meng", ""], ["Tang", "Jian", ""], ["Shang", "Jingbo", ""], ["Ren", "Xiang", ""], ["Zhang", "Ming", ""], ["Han", "Jiawei", ""]]}, {"id": "1709.06662", "submitter": "Nina Narodytska", "authors": "Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly\n  Sagiv, Toby Walsh", "title": "Verifying Properties of Binarized Deep Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding properties of deep neural networks is an important challenge in\ndeep learning. In this paper, we take a step in this direction by proposing a\nrigorous way of verifying properties of a popular class of neural networks,\nBinarized Neural Networks, using the well-developed means of Boolean\nsatisfiability. Our main contribution is a construction that creates a\nrepresentation of a binarized neural network as a Boolean formula. Our encoding\nis the first exact Boolean representation of a deep neural network. Using this\nencoding, we leverage the power of modern SAT solvers along with a proposed\ncounterexample-guided search procedure to verify various properties of these\nnetworks. A particular focus will be on the critical property of robustness to\nadversarial perturbations. For this property, our experimental results\ndemonstrate that our approach scales to medium-size deep neural networks used\nin image classification tasks. To the best of our knowledge, this is the first\nwork on verifying properties of deep neural networks using an exact Boolean\nencoding of the network.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 22:21:49 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 18:30:06 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Narodytska", "Nina", ""], ["Kasiviswanathan", "Shiva Prasad", ""], ["Ryzhyk", "Leonid", ""], ["Sagiv", "Mooly", ""], ["Walsh", "Toby", ""]]}, {"id": "1709.06669", "submitter": "Abhay Harpale", "authors": "Abhay Harpale (1), Abhishek Srivastav (1) ((1) GE Global Research)", "title": "A textual transform of multivariate time-series for prognostics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostics or early detection of incipient faults is an important industrial\nchallenge for condition-based and preventive maintenance. Physics-based\napproaches to modeling fault progression are infeasible due to multiple\ninteracting components, uncontrolled environmental factors and observability\nconstraints. Moreover, such approaches to prognostics do not generalize to new\ndomains. Consequently, domain-agnostic data-driven machine learning approaches\nto prognostics are desirable. Damage progression is a path-dependent process\nand explicitly modeling the temporal patterns is critical for accurate\nestimation of both the current damage state and its progression leading to\ntotal failure. In this paper, we present a novel data-driven approach to\nprognostics that employs a novel textual representation of multivariate\ntemporal sensor observations for predicting the future health state of the\nmonitored equipment early in its life. This representation enables us to\nutilize well-understood concepts from text-mining for modeling, prediction and\nunderstanding distress patterns in a domain agnostic way. The approach has been\ndeployed and successfully tested on large scale multivariate time-series data\nfrom commercial aircraft engines. We report experiments on well-known publicly\navailable benchmark datasets and simulation datasets. The proposed approach is\nshown to be superior in terms of prediction accuracy, lead time to prediction\nand interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 22:54:10 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Harpale", "Abhay", "", "GE Global Research"], ["Srivastav", "Abhishek", "", "GE Global Research"]]}, {"id": "1709.06680", "submitter": "Seungil You", "authors": "Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta", "title": "Deep Lattice Networks and Partial Monotonic Functions", "comments": "9 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose learning deep models that are monotonic with respect to a\nuser-specified set of inputs by alternating layers of linear embeddings,\nensembles of lattices, and calibrators (piecewise linear functions), with\nappropriate constraints for monotonicity, and jointly training the resulting\nnetwork. We implement the layers and projections with new computational graph\nnodes in TensorFlow and use the ADAM optimizer and batched stochastic\ngradients. Experiments on benchmark and real-world datasets show that six-layer\nmonotonic deep lattice networks achieve state-of-the art performance for\nclassification and regression with monotonicity guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 23:48:39 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["You", "Seungil", ""], ["Ding", "David", ""], ["Canini", "Kevin", ""], ["Pfeifer", "Jan", ""], ["Gupta", "Maya", ""]]}, {"id": "1709.06688", "submitter": "Matey Neykov", "authors": "Matey Neykov and Han Liu", "title": "Property Testing in High Dimensional Ising models", "comments": "72 pages, 10 figures; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the information-theoretic limitations of graph property\ntesting in zero-field Ising models. Instead of learning the entire graph\nstructure, sometimes testing a basic graph property such as connectivity, cycle\npresence or maximum clique size is a more relevant and attainable objective.\nSince property testing is more fundamental than graph recovery, any necessary\nconditions for property testing imply corresponding conditions for graph\nrecovery, while custom property tests can be statistically and/or\ncomputationally more efficient than graph recovery based algorithms.\nUnderstanding the statistical complexity of property testing requires the\ndistinction of ferromagnetic (i.e., positive interactions only) and general\nIsing models. Using combinatorial constructs such as graph packing and strong\nmonotonicity, we characterize how target properties affect the corresponding\nminimax upper and lower bounds within the realm of ferromagnets. On the other\nhand, by studying the detection of an antiferromagnetic (i.e., negative\ninteractions only) Curie-Weiss model buried in Rademacher noise, we show that\nproperty testing is strictly more challenging over general Ising models. In\nterms of methodological development, we propose two types of correlation based\ntests: computationally efficient screening for ferromagnets, and score type\ntests for general models, including a fast cycle presence test. Our correlation\nscreening tests match the information-theoretic bounds for property testing in\nferromagnets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 00:48:43 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 14:57:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1709.06716", "submitter": "Abubakar Abid", "authors": "Abubakar Abid, Martin J. Zhang, Vivek K. Bagaria, James Zou", "title": "Contrastive Principal Component Analysis", "comments": "main body is 10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique called contrastive principal component analysis\n(cPCA) that is designed to discover low-dimensional structure that is unique to\na dataset, or enriched in one dataset relative to other data. The technique is\na generalization of standard PCA, for the setting where multiple datasets are\navailable -- e.g. a treatment and a control group, or a mixed versus a\nhomogeneous population -- and the goal is to explore patterns that are specific\nto one of the datasets. We conduct a wide variety of experiments in which cPCA\nidentifies important dataset-specific patterns that are missed by PCA,\ndemonstrating that it is useful for many applications: subgroup discovery,\nvisualizing trends, feature selection, denoising, and data-dependent\nstandardization. We provide geometrical interpretations of cPCA and show that\nit satisfies desirable theoretical guarantees. We also extend cPCA to nonlinear\nsettings in the form of kernel cPCA. We have released our code as a python\npackage and documentation is on Github.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 03:53:03 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 00:26:51 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Abid", "Abubakar", ""], ["Zhang", "Martin J.", ""], ["Bagaria", "Vivek K.", ""], ["Zou", "James", ""]]}, {"id": "1709.06853", "submitter": "Ciara Pike-Burke", "authors": "Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, Steffen\n  Grunewalder", "title": "Bandits with Delayed, Aggregated Anonymous Feedback", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the stochastic $K$-armed bandit problem, which we call\n\"bandits with delayed, aggregated anonymous feedback\". In this problem, when\nthe player pulls an arm, a reward is generated, however it is not immediately\nobserved. Instead, at the end of each round the player observes only the sum of\na number of previously generated rewards which happen to arrive in the given\nround. The rewards are stochastically delayed and due to the aggregated nature\nof the observations, the information of which arm led to a particular reward is\nlost. The question is what is the cost of the information loss due to this\ndelayed, aggregated anonymous feedback? Previous works have studied bandits\nwith stochastic, non-anonymous delays and found that the regret increases only\nby an additive factor relating to the expected delay. In this paper, we show\nthat this additive regret increase can be maintained in the harder delayed,\naggregated anonymous feedback setting when the expected delay (or a bound on\nit) is known. We provide an algorithm that matches the worst case regret of the\nnon-anonymous problem exactly when the delays are bounded, and up to\nlogarithmic factors or an additive variance term for unbounded delays.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 13:36:19 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 17:36:26 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 17:29:28 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Pike-Burke", "Ciara", ""], ["Agrawal", "Shipra", ""], ["Szepesvari", "Csaba", ""], ["Grunewalder", "Steffen", ""]]}, {"id": "1709.06917", "submitter": "Konstantinos Chatzilygeroudis", "authors": "Konstantinos Chatzilygeroudis and Jean-Baptiste Mouret", "title": "Using Parameterized Black-Box Priors to Scale Up Model-Based Policy\n  Search for Robotics", "comments": "Accepted at ICRA 2018; 8 pages, 4 figures, 2 algorithms, 1 table;\n  Video at https://youtu.be/HFkZkhGGzTo ; Spotlight ICRA presentation at\n  https://youtu.be/_MZYDhfWeLc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most data-efficient algorithms for reinforcement learning in robotics are\nmodel-based policy search algorithms, which alternate between learning a\ndynamical model of the robot and optimizing a policy to maximize the expected\nreturn given the model and its uncertainties. Among the few proposed\napproaches, the recently introduced Black-DROPS algorithm exploits a black-box\noptimization algorithm to achieve both high data-efficiency and good\ncomputation times when several cores are used; nevertheless, like all\nmodel-based policy search approaches, Black-DROPS does not scale to high\ndimensional state/action spaces. In this paper, we introduce a new model\nlearning procedure in Black-DROPS that leverages parameterized black-box priors\nto (1) scale up to high-dimensional systems, and (2) be robust to large\ninaccuracies of the prior information. We demonstrate the effectiveness of our\napproach with the \"pendubot\" swing-up task in simulation and with a physical\nhexapod robot (48D state space, 18D action space) that has to walk forward as\nfast as possible. The results show that our new algorithm is more\ndata-efficient than previous model-based policy search algorithms (with and\nwithout priors) and that it can allow a physical 6-legged robot to learn new\ngaits in only 16 to 30 seconds of interaction time.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 15:03:47 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:39:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Chatzilygeroudis", "Konstantinos", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1709.06919", "submitter": "Konstantinos Chatzilygeroudis", "authors": "R\\'emi Pautrat, Konstantinos Chatzilygeroudis and Jean-Baptiste Mouret", "title": "Bayesian Optimization with Automatic Prior Selection for Data-Efficient\n  Direct Policy Search", "comments": "Accepted at ICRA 2018; 8 pages, 4 figures, 1 algorithm; Video at\n  https://youtu.be/xo8mUIZTvNE ; Spotlight ICRA presentation\n  https://youtu.be/iiVaV-U6Kqo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most interesting features of Bayesian optimization for direct\npolicy search is that it can leverage priors (e.g., from simulation or from\nprevious tasks) to accelerate learning on a robot. In this paper, we are\ninterested in situations for which several priors exist but we do not know in\nadvance which one fits best the current situation. We tackle this problem by\nintroducing a novel acquisition function, called Most Likely Expected\nImprovement (MLEI), that combines the likelihood of the priors and the expected\nimprovement. We evaluate this new acquisition function on a transfer learning\ntask for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has\nto learn to walk on flat ground and on stairs, with priors corresponding to\ndifferent stairs and different kinds of damages. Our results show that MLEI\neffectively identifies and exploits the priors, even when there is no obvious\nmatch between the current situations and the priors.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 15:04:50 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:45:49 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Pautrat", "R\u00e9mi", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1709.06970", "submitter": "Zehang Li", "authors": "Zehang Richard Li and Tyler H. McCormick", "title": "An Expectation Conditional Maximization approach for Gaussian graphical\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian graphical models are a useful tool for understanding dependence\nrelationships among many variables, particularly in situations with external\nprior information. In high-dimensional settings, the space of possible graphs\nbecomes enormous, rendering even state-of-the-art Bayesian stochastic search\ncomputationally infeasible. We propose a deterministic alternative to estimate\nGaussian and Gaussian copula graphical models using an Expectation Conditional\nMaximization (ECM) algorithm, extending the EM approach from Bayesian variable\nselection to graphical model estimation. We show that the ECM approach enables\nfast posterior exploration under a sequence of mixture priors, and can\nincorporate multiple sources of information.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 17:05:19 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 19:31:50 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 15:14:36 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Li", "Zehang Richard", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1709.06990", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq and Bruce A. Bassett", "title": "Text Compression for Sentiment Analysis via Evolutionary Algorithms", "comments": "8 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 17:57:16 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1709.06994", "submitter": "Huan Wang", "authors": "Huan Wang, Qiming Zhang, Yuehai Wang, Haoji Hu", "title": "Structured Probabilistic Pruning for Convolutional Neural Network\n  Acceleration", "comments": "CNN model acceleration, 13 pages, 6 figures, accepted by Proceedings\n  of the British Machine Vision Conference (BMVC), 2018 oral", "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC), 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel progressive parameter pruning method for\nConvolutional Neural Network acceleration, named Structured Probabilistic\nPruning (SPP), which effectively prunes weights of convolutional layers in a\nprobabilistic manner. Unlike existing deterministic pruning approaches, where\nunimportant weights are permanently eliminated, SPP introduces a pruning\nprobability for each weight, and pruning is guided by sampling from the pruning\nprobabilities. A mechanism is designed to increase and decrease pruning\nprobabilities based on importance criteria in the training process. Experiments\nshow that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of\ntop-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet\nclassification. Moreover, SPP can be directly applied to accelerate\nmulti-branch CNN networks, such as ResNet, without specific adaptations. Our 2x\nspeedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We\nfurther show the effectiveness of SPP on transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 03:13:40 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 16:39:49 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 01:11:50 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Wang", "Huan", ""], ["Zhang", "Qiming", ""], ["Wang", "Yuehai", ""], ["Hu", "Haoji", ""]]}, {"id": "1709.07036", "submitter": "Junwei Lu", "authors": "Cong Ma, Junwei Lu and Han Liu", "title": "Inter-Subject Analysis: Inferring Sparse Interactions with Dense\n  Intra-Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new modeling framework for Inter-Subject Analysis (ISA). The\ngoal of ISA is to explore the dependency structure between different subjects\nwith the intra-subject dependency as nuisance. It has important applications in\nneuroscience to explore the functional connectivity between brain regions under\nnatural stimuli. Our framework is based on the Gaussian graphical models, under\nwhich ISA can be converted to the problem of estimation and inference of the\ninter-subject precision matrix. The main statistical challenge is that we do\nnot impose sparsity constraint on the whole precision matrix and we only assume\nthe inter-subject part is sparse. For estimation, we propose to estimate an\nalternative parameter to get around the non-sparse issue and it can achieve\nasymptotic consistency even if the intra-subject dependency is dense. For\ninference, we propose an \"untangle and chord\" procedure to de-bias our\nestimator. It is valid without the sparsity assumption on the inverse Hessian\nof the log-likelihood function. This inferential method is general and can be\napplied to many other statistical problems, thus it is of independent\ntheoretical interest. Numerical experiments on both simulated and brain imaging\ndata validate our methods and theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:57:19 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ma", "Cong", ""], ["Lu", "Junwei", ""], ["Liu", "Han", ""]]}, {"id": "1709.07089", "submitter": "Alonso Marco", "authors": "Alonso Marco, Philipp Hennig, Stefan Schaal and Sebastian Trimpe", "title": "On the Design of LQR Kernels for Efficient Controller Learning", "comments": "8 pages, 5 figures, to appear in 56th IEEE Conference on Decision and\n  Control (CDC 2017)", "journal-ref": null, "doi": "10.1109/CDC.2017.8264429", "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding optimal feedback controllers for nonlinear dynamic systems from data\nis hard. Recently, Bayesian optimization (BO) has been proposed as a powerful\nframework for direct controller tuning from experimental trials. For selecting\nthe next query point and finding the global optimum, BO relies on a\nprobabilistic description of the latent objective function, typically a\nGaussian process (GP). As is shown herein, GPs with a common kernel choice can,\nhowever, lead to poor learning outcomes on standard quadratic control problems.\nFor a first-order system, we construct two kernels that specifically leverage\nthe structure of the well-known Linear Quadratic Regulator (LQR), yet retain\nthe flexibility of Bayesian nonparametric learning. Simulations of uncertain\nlinear and nonlinear systems demonstrate that the LQR kernels yield superior\nlearning performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 21:36:45 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Marco", "Alonso", ""], ["Hennig", "Philipp", ""], ["Schaal", "Stefan", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "1709.07093", "submitter": "Xingguo Li", "authors": "Jarvis Haupt and Xingguo Li and David P. Woodruff", "title": "Near Optimal Sketching of Low-Rank Tensor Regression", "comments": "36 pages, 2 figures, 2 tables, Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the least squares regression problem \\begin{align*} \\min_{\\Theta \\in\n\\mathcal{S}_{\\odot D,R}} \\|A\\Theta-b\\|_2, \\end{align*} where\n$\\mathcal{S}_{\\odot D,R}$ is the set of $\\Theta$ for which $\\Theta =\n\\sum_{r=1}^{R} \\theta_1^{(r)} \\circ \\cdots \\circ \\theta_D^{(r)}$ for vectors\n$\\theta_d^{(r)} \\in \\mathbb{R}^{p_d}$ for all $r \\in [R]$ and $d \\in [D]$, and\n$\\circ$ denotes the outer product of vectors. That is, $\\Theta$ is a\nlow-dimensional, low-rank tensor. This is motivated by the fact that the number\nof parameters in $\\Theta$ is only $R \\cdot \\sum_{d=1}^D p_d$, which is\nsignificantly smaller than the $\\prod_{d=1}^{D} p_d$ number of parameters in\nordinary least squares regression. We consider the above CP decomposition model\nof tensors $\\Theta$, as well as the Tucker decomposition. For both models we\nshow how to apply data dimensionality reduction techniques based on {\\it\nsparse} random projections $\\Phi \\in \\mathbb{R}^{m \\times n}$, with $m \\ll n$,\nto reduce the problem to a much smaller problem $\\min_{\\Theta} \\|\\Phi A \\Theta\n- \\Phi b\\|_2$, for which if $\\Theta'$ is a near-optimum to the smaller problem,\nthen it is also a near optimum to the original problem. We obtain significantly\nsmaller dimension and sparsity in $\\Phi$ than is possible for ordinary least\nsquares regression, and we also provide a number of numerical simulations\nsupporting our theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 22:05:49 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Haupt", "Jarvis", ""], ["Li", "Xingguo", ""], ["Woodruff", "David P.", ""]]}, {"id": "1709.07097", "submitter": "Tullia Padellini", "authors": "Tullia Padellini and Pierpaolo Brutti", "title": "Persistence Flamelets: multiscale Persistent Homology for kernel density\n  exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been noticeable interest in the study of the \"shape\nof data\". Among the many ways a \"shape\" could be defined, topology is the most\ngeneral one, as it describes an object in terms of its connectivity structure:\nconnected components (topological features of dimension 0), cycles (features of\ndimension 1) and so on. There is a growing number of techniques, generally\ndenoted as Topological Data Analysis, aimed at estimating topological\ninvariants of a fixed object; when we allow this object to change, however,\nlittle has been done to investigate the evolution in its topology. In this work\nwe define the Persistence Flamelets, a multiscale version of one of the most\npopular tool in TDA, the Persistence Landscape. We examine its theoretical\nproperties and we show how it could be used to gain insights on KDEs bandwidth\nparameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 22:45:27 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Padellini", "Tullia", ""], ["Brutti", "Pierpaolo", ""]]}, {"id": "1709.07100", "submitter": "Tullia Padellini", "authors": "Tullia Padellini and Pierpaolo Brutti", "title": "Supervised Learning with Indefinite Topological Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is a recent and growing branch of statistics\ndevoted to the study of the shape of the data. In this work we investigate the\npredictive power of TDA in the context of supervised learning. Since\ntopological summaries, most noticeably the Persistence Diagram, are typically\ndefined in complex spaces, we adopt a kernel approach to translate them into\nmore familiar vector spaces. We define a topological exponential kernel, we\ncharacterize it, and we show that, despite not being positive semi-definite, it\ncan be successfully used in regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 23:05:00 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Padellini", "Tullia", ""], ["Brutti", "Pierpaolo", ""]]}, {"id": "1709.07109", "submitter": "Dinghan Shen", "authors": "Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, Lawrence Carin", "title": "Deconvolutional Latent-Variable Model for Text Sequence Matching", "comments": "Accepted by AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A latent-variable model is introduced for text matching, inferring sentence\nrepresentations by jointly optimizing generative and discriminative objectives.\nTo alleviate typical optimization challenges in latent-variable models for\ntext, we employ deconvolutional networks as the sequence decoder (generator),\nproviding learned latent codes with more semantic information and better\ngeneralization. Our model, trained in an unsupervised manner, yields stronger\nempirical predictive performance than a decoder based on Long Short-Term Memory\n(LSTM), with less parameters and considerably faster training. Further, we\napply it to text sequence-matching problems. The proposed model significantly\noutperforms several strong sentence-encoding baselines, especially in the\nsemi-supervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 00:23:55 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 01:07:34 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 02:18:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shen", "Dinghan", ""], ["Zhang", "Yizhe", ""], ["Henao", "Ricardo", ""], ["Su", "Qinliang", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.07124", "submitter": "Scott Wisdom", "authors": "Scott Wisdom, Thomas Powers, James Pitton, Les Atlas", "title": "Deep Recurrent NMF for Speech Separation by Unfolding Iterative\n  Thresholding", "comments": "To be presented at WASPAA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel recurrent neural network architecture for\nspeech separation. This architecture is constructed by unfolding the iterations\nof a sequential iterative soft-thresholding algorithm (ISTA) that solves the\noptimization problem for sparse nonnegative matrix factorization (NMF) of\nspectrograms. We name this network architecture deep recurrent NMF (DR-NMF).\nThe proposed DR-NMF network has three distinct advantages. First, DR-NMF\nprovides better interpretability than other deep architectures, since the\nweights correspond to NMF model parameters, even after training. This\ninterpretability also provides principled initializations that enable faster\ntraining and convergence to better solutions compared to conventional random\ninitialization. Second, like many deep networks, DR-NMF is an order of\nmagnitude faster at test time than NMF, since computation of the network output\nonly requires evaluating a few layers at each time step. Third, when a limited\namount of training data is available, DR-NMF exhibits stronger generalization\nand separation performance compared to sparse NMF and state-of-the-art\nlong-short term memory (LSTM) networks. When a large amount of training data is\navailable, DR-NMF achieves lower yet competitive separation performance\ncompared to LSTM networks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 01:46:19 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Wisdom", "Scott", ""], ["Powers", "Thomas", ""], ["Pitton", "James", ""], ["Atlas", "Les", ""]]}, {"id": "1709.07149", "submitter": "Vidyadhar Upadhya", "authors": "Vidyadhar Upadhya, P. S. Sastry", "title": "Learning RBM with a DC programming Approach", "comments": "Accepted in ACML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By exploiting the property that the RBM log-likelihood function is the\ndifference of convex functions, we formulate a stochastic variant of the\ndifference of convex functions (DC) programming to minimize the negative\nlog-likelihood. Interestingly, the traditional contrastive divergence algorithm\nis a special case of the above formulation and the hyperparameters of the two\nalgorithms can be chosen such that the amount of computation per mini-batch is\nidentical. We show that for a given computational budget the proposed algorithm\nalmost always reaches a higher log-likelihood more rapidly, compared to the\nstandard contrastive divergence algorithm. Further, we modify this algorithm to\nuse the centered gradients and show that it is more efficient and effective\ncompared to the standard centered gradient algorithm on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 03:51:16 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 10:26:53 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Upadhya", "Vidyadhar", ""], ["Sastry", "P. S.", ""]]}, {"id": "1709.07150", "submitter": "Udayan Khurana", "authors": "Udayan Khurana and Horst Samulowitz and Deepak Turaga", "title": "Feature Engineering for Predictive Modeling using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering is a crucial step in the process of predictive modeling.\nIt involves the transformation of given feature space, typically using\nmathematical functions, with the objective of reducing the modeling error for a\ngiven target. However, there is no well-defined basis for performing effective\nfeature engineering. It involves domain knowledge, intuition, and most of all,\na lengthy process of trial and error. The human attention involved in\noverseeing this process significantly influences the cost of model generation.\nWe present a new framework to automate feature engineering. It is based on\nperformance driven exploration of a transformation graph, which systematically\nand compactly enumerates the space of given options. A highly efficient\nexploration strategy is derived through reinforcement learning on past\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 04:04:43 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Khurana", "Udayan", ""], ["Samulowitz", "Horst", ""], ["Turaga", "Deepak", ""]]}, {"id": "1709.07172", "submitter": "Tong Yu", "authors": "Tong Yu, Branislav Kveton, Zheng Wen, Hung Bui, Ole J. Mengshoel", "title": "SpectralLeader: Online Spectral Learning for Single Topic Models", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a latent variable model from a stream of\ndata. Latent variable models are popular in practice because they can explain\nobserved data in terms of unobserved concepts. These models have been\ntraditionally studied in the offline setting. In the online setting, on the\nother hand, the online EM is arguably the most popular algorithm for learning\nlatent variable models. Although the online EM is computationally efficient, it\ntypically converges to a local optimum. In this work, we develop a new online\nlearning algorithm for latent variable models, which we call SpectralLeader.\nSpectralLeader always converges to the global optimum, and we derive a\nsublinear upper bound on its $n$-step regret in the bag-of-words model. In both\nsynthetic and real-world experiments, we show that SpectralLeader performs\nsimilarly to or better than the online EM with tuned hyper-parameters.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 06:24:51 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 05:51:34 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 20:01:42 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 02:39:04 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Yu", "Tong", ""], ["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Bui", "Hung", ""], ["Mengshoel", "Ole J.", ""]]}, {"id": "1709.07175", "submitter": "Michael Wojnowicz", "authors": "Michael Wojnowicz, Dinh Nguyen, Li Li, and Xuan Zhao", "title": "Lazy stochastic principal component analysis", "comments": "To be published in: 2017 IEEE International Conference on Data Mining\n  Workshops (ICDMW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic principal component analysis (SPCA) has become a popular\ndimensionality reduction strategy for large, high-dimensional datasets. We\nderive a simplified algorithm, called Lazy SPCA, which has reduced\ncomputational complexity and is better suited for large-scale distributed\ncomputation. We prove that SPCA and Lazy SPCA find the same approximations to\nthe principal subspace, and that the pairwise distances between samples in the\nlower-dimensional space is invariant to whether SPCA is executed lazily or not.\nEmpirical studies find downstream predictive performance to be identical for\nboth methods, and superior to random projections, across a range of predictive\nmodels (linear regression, logistic lasso, and random forests). In our largest\nexperiment with 4.6 million samples, Lazy SPCA reduced 43.7 hours of\ncomputation to 9.9 hours. Overall, Lazy SPCA relies exclusively on matrix\nmultiplications, besides an operation on a small square matrix whose size\ndepends only on the target dimensionality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 06:43:49 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Wojnowicz", "Michael", ""], ["Nguyen", "Dinh", ""], ["Li", "Li", ""], ["Zhao", "Xuan", ""]]}, {"id": "1709.07224", "submitter": "Maximilian H\\\"uttenrauch", "authors": "Maximilian H\\\"uttenrauch and Adrian \\v{S}o\\v{s}i\\'c and Gerhard\n  Neumann", "title": "Local Communication Protocols for Learning Complex Swarm Behaviors with\n  Deep Reinforcement Learning", "comments": "13 pages, 4 figures, version 2, accepted at ANTS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm systems constitute a challenging problem for reinforcement learning\n(RL) as the algorithm needs to learn decentralized control policies that can\ncope with limited local sensing and communication abilities of the agents.\nWhile it is often difficult to directly define the behavior of the agents,\nsimple communication protocols can be defined more easily using prior knowledge\nabout the given task. In this paper, we propose a number of simple\ncommunication protocols that can be exploited by deep reinforcement learning to\nfind decentralized control policies in a multi-robot swarm environment. The\nprotocols are based on histograms that encode the local neighborhood relations\nof the agents and can also transmit task-specific information, such as the\nshortest distance and direction to a desired target. In our framework, we use\nan adaptation of Trust Region Policy Optimization to learn complex\ncollaborative tasks, such as formation building and building a communication\nlink. We evaluate our findings in a simulated 2D-physics environment, and\ncompare the implications of different communication protocols.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 09:18:09 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 08:39:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["H\u00fcttenrauch", "Maximilian", ""], ["\u0160o\u0161i\u0107", "Adrian", ""], ["Neumann", "Gerhard", ""]]}, {"id": "1709.07267", "submitter": "Olivier Colliot", "authors": "Jorge Samper-Gonz\\'alez, Ninon Burgos, Sabrina Fontanella, Hugo\n  Bertin, Marie-Odile Habert, Stanley Durrleman, Theodoros Evgeniou, Olivier\n  Colliot", "title": "Yet Another ADNI Machine Learning Paper? Paving The Way Towards\n  Fully-reproducible Research on Classification of Alzheimer's Disease", "comments": null, "journal-ref": "Proc. Machine Learning in Medical Imaging MLMI 2017, MICCAI\n  Worskhop, Lecture Notes in Computer Science, volume 10541, pp 53-60, Springer", "doi": "10.1007/978-3-319-67389-9_7", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the number of papers on Alzheimer's disease classification\nhas increased dramatically, generating interesting methodological ideas on the\nuse machine learning and feature extraction methods. However, practical impact\nis much more limited and, eventually, one could not tell which of these\napproaches are the most efficient. While over 90\\% of these works make use of\nADNI an objective comparison between approaches is impossible due to variations\nin the subjects included, image pre-processing, performance metrics and\ncross-validation procedures. In this paper, we propose a framework for\nreproducible classification experiments using multimodal MRI and PET data from\nADNI. The core components are: 1) code to automatically convert the full ADNI\ndatabase into BIDS format; 2) a modular architecture based on Nipype in order\nto easily plug-in different classification and feature extraction tools; 3)\nfeature extraction pipelines for MRI and PET data; 4) baseline classification\napproaches for unimodal and multimodal features. This provides a flexible\nframework for benchmarking different feature extraction and classification\ntools in a reproducible manner. We demonstrate its use on all (1519) baseline\nT1 MR images and all (1102) baseline FDG PET images from ADNI 1, GO and 2 with\nSPM-based feature extraction pipelines and three different classification\ntechniques (linear SVM, anatomically regularized SVM and multiple kernel\nlearning SVM). The highest accuracies achieved were: 91% for AD vs CN, 83% for\nMCIc vs CN, 75% for MCIc vs MCInc, 94% for AD-A$\\beta$+ vs CN-A$\\beta$- and 72%\nfor MCIc-A$\\beta$+ vs MCInc-A$\\beta$+. The code is publicly available at\nhttps://gitlab.icm-institute.org/aramislab/AD-ML (depends on the Clinica\nsoftware platform, publicly available at http://www.clinica.run).\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 11:37:01 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Samper-Gonz\u00e1lez", "Jorge", ""], ["Burgos", "Ninon", ""], ["Fontanella", "Sabrina", ""], ["Bertin", "Hugo", ""], ["Habert", "Marie-Odile", ""], ["Durrleman", "Stanley", ""], ["Evgeniou", "Theodoros", ""], ["Colliot", "Olivier", ""]]}, {"id": "1709.07337", "submitter": "Julian Yarkony", "authors": "Chong Zhang, Shaofei Wang, Miguel A. Gonzalez-Ballester, Julian\n  Yarkony", "title": "Efficient Column Generation for Cell Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of instance segmentation in biological images with\ncrowded and compact cells. We formulate this task as an integer program where\nvariables correspond to cells and constraints enforce that cells do not\noverlap. To solve this integer program, we propose a column generation\nformulation where the pricing program is solved via exact optimization of very\nsmall scale integer programs. Column generation is tightened using odd set\ninequalities which fit elegantly into pricing problem optimization. Our column\ngeneration approach achieves fast stable anytime inference for our instance\nsegmentation problems. We demonstrate on three distinct light microscopy\ndatasets, with several hundred cells each, that our proposed algorithm rapidly\nachieves or exceeds state of the art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:15:23 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Zhang", "Chong", ""], ["Wang", "Shaofei", ""], ["Gonzalez-Ballester", "Miguel A.", ""], ["Yarkony", "Julian", ""]]}, {"id": "1709.07359", "submitter": "Lucas C. Uzal", "authors": "Guillermo L. Grinblat, Lucas C. Uzal and Pablo M. Granitto", "title": "Class-Splitting Generative Adversarial Networks", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:55:54 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 14:07:35 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Grinblat", "Guillermo L.", ""], ["Uzal", "Lucas C.", ""], ["Granitto", "Pablo M.", ""]]}, {"id": "1709.07417", "submitter": "Irwan Bello", "authors": "Irwan Bello, Barret Zoph, Vijay Vasudevan, Quoc V. Le", "title": "Neural Optimizer Search with Reinforcement Learning", "comments": "ICML 2017 Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to automate the process of discovering optimization\nmethods, with a focus on deep learning architectures. We train a Recurrent\nNeural Network controller to generate a string in a domain specific language\nthat describes a mathematical update equation based on a list of primitive\nfunctions, such as the gradient, running average of the gradient, etc. The\ncontroller is trained with Reinforcement Learning to maximize the performance\nof a model after a few epochs. On CIFAR-10, our method discovers several update\nrules that are better than many commonly used optimizers, such as Adam,\nRMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two\nnew optimizers, named PowerSign and AddSign, which we show transfer well and\nimprove training on a variety of different tasks and architectures, including\nImageNet classification and Google's neural machine translation system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:01:47 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 15:27:27 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Bello", "Irwan", ""], ["Zoph", "Barret", ""], ["Vasudevan", "Vijay", ""], ["Le", "Quoc V.", ""]]}, {"id": "1709.07433", "submitter": "Robert Bamler", "authors": "Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt", "title": "Perturbative Black Box Variational Inference", "comments": "In the proceedings of Advances in Neural Information Processing\n  Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box variational inference (BBVI) with reparameterization gradients\ntriggered the exploration of divergence measures other than the\nKullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we\nview BBVI with generalized divergences as a form of estimating the marginal\nlikelihood via biased importance sampling. The choice of divergence determines\na bias-variance trade-off between the tightness of a bound on the marginal\nlikelihood (low bias) and the variance of its gradient estimators. Drawing on\nvariational perturbation theory of statistical physics, we use these insights\nto construct a family of new variational bounds. Enumerated by an odd integer\norder $K$, this family captures the standard KL bound for $K=1$, and converges\nto the exact marginal likelihood as $K\\to\\infty$. Compared to\nalpha-divergences, our reparameterization gradients have a lower variance. We\nshow in experiments on Gaussian Processes and Variational Autoencoders that the\nnew bounds are more mass covering, and that the resulting posterior covariances\nare closer to the true posterior and lead to higher likelihoods on held-out\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:50:10 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 22:22:31 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Bamler", "Robert", ""], ["Zhang", "Cheng", ""], ["Opper", "Manfred", ""], ["Mandt", "Stephan", ""]]}, {"id": "1709.07534", "submitter": "Arijit Biswas", "authors": "Arijit Biswas, Mukul Bhutani and Subhajit Sanyal", "title": "MRNet-Product2Vec: A Multi-task Recurrent Neural Network for Product\n  Embeddings", "comments": "Published in ECML-PKDD 2017 (Applied Data Science Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell\nbillions of products. Machine learning (ML) algorithms involving products are\noften used to improve the customer experience and increase revenue, e.g.,\nproduct similarity, recommendation, and price estimation. The products are\nrequired to be represented as features before training an ML algorithm. In this\npaper, we propose an approach called MRNet-Product2Vec for creating generic\nembeddings of products within an e-commerce ecosystem. We learn a dense and\nlow-dimensional embedding where a diverse set of signals related to a product\nare explicitly injected into its representation. We train a Discriminative\nMulti-task Bidirectional Recurrent Neural Network (RNN), where the input is a\nproduct title fed through a Bidirectional RNN and at the output, product labels\ncorresponding to fifteen different tasks are predicted. The task set includes\nseveral intrinsic characteristics about a product such as price, weight, size,\ncolor, popularity, and material. We evaluate the proposed embedding\nquantitatively and qualitatively. We demonstrate that they are almost as good\nas sparse and extremely high-dimensional TF-IDF representation in spite of\nhaving less than 3% of the TF-IDF dimension. We also use a multimodal\nautoencoder for comparing products from different language-regions and show\npreliminary yet promising qualitative results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 22:38:51 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Biswas", "Arijit", ""], ["Bhutani", "Mukul", ""], ["Sanyal", "Subhajit", ""]]}, {"id": "1709.07616", "submitter": "Simon Lyddon", "authors": "Simon Lyddon, Chris Holmes, Stephen Walker", "title": "General Bayesian Updating and the Loss-Likelihood Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the weighted likelihood bootstrap, a method that\ngenerates samples from an approximate Bayesian posterior of a parametric model.\nWe show that the same method can be derived, without approximation, under a\nBayesian nonparametric model with the parameter of interest defined as\nminimising an expected negative log-likelihood under an unknown sampling\ndistribution. This interpretation enables us to extend the weighted likelihood\nbootstrap to posterior sampling for parameters minimizing an expected loss. We\ncall this method the loss-likelihood bootstrap. We make a connection between\nthis and general Bayesian updating, which is a way of updating prior belief\ndistributions without needing to construct a global probability model, yet\nrequires the calibration of two forms of loss function. The loss-likelihood\nbootstrap is used to calibrate the general Bayesian posterior by matching\nasymptotic Fisher information. We demonstrate the methodology on a number of\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 07:25:36 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 10:40:14 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Lyddon", "Simon", ""], ["Holmes", "Chris", ""], ["Walker", "Stephen", ""]]}, {"id": "1709.07625", "submitter": "Daohong Xiang", "authors": "Andreas Christmann and Daohong Xiang and Ding-Xuan Zhou", "title": "Total stability of kernel methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized empirical risk minimization using kernels and their corresponding\nreproducing kernel Hilbert spaces (RKHSs) plays an important role in machine\nlearning. However, the actually used kernel often depends on one or on a few\nhyperparameters or the kernel is even data dependent in a much more complicated\nmanner. Examples are Gaussian RBF kernels, kernel learning, and hierarchical\nGaussian kernels which were recently proposed for deep learning. Therefore, the\nactually used kernel is often computed by a grid search or in an iterative\nmanner and can often only be considered as an approximation to the \"ideal\" or\n\"optimal\" kernel. The paper gives conditions under which classical kernel based\nmethods based on a convex Lipschitz loss function and on a bounded and smooth\nkernel are stable, if the probability measure $P$, the regularization parameter\n$\\lambda$, and the kernel $k$ may slightly change in a simultaneous manner.\nSimilar results are also given for pairwise learning. Therefore, the topic of\nthis paper is somewhat more general than in classical robust statistics, where\nusually only the influence of small perturbations of the probability measure\n$P$ on the estimated function is considered.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 07:58:45 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Christmann", "Andreas", ""], ["Xiang", "Daohong", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1709.07638", "submitter": "Syama Sundar Rangapuram", "authors": "Matthias Seeger, Syama Rangapuram, Yuyang Wang, David Salinas, Jan\n  Gasthaus, Tim Januschowski, Valentin Flunkert", "title": "Approximate Bayesian Inference in Linear State Space Models for\n  Intermittent Demand Forecasting at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable and robust Bayesian inference method for linear state\nspace models. The method is applied to demand forecasting in the context of a\nlarge e-commerce platform, paying special attention to intermittent and bursty\ntarget statistics. Inference is approximated by the Newton-Raphson algorithm,\nreduced to linear-time Kalman smoothing, which allows us to operate on several\norders of magnitude larger problems than previous related work. In a study on\nlarge real-world sales datasets, our method outperforms competing approaches on\nfast and medium moving items.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 08:53:54 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Seeger", "Matthias", ""], ["Rangapuram", "Syama", ""], ["Wang", "Yuyang", ""], ["Salinas", "David", ""], ["Gasthaus", "Jan", ""], ["Januschowski", "Tim", ""], ["Flunkert", "Valentin", ""]]}, {"id": "1709.07731", "submitter": "Saikat  Chatterjee", "authors": "Ahmed Zaki, Partha P. Mitra, Lars K. Rasmussen and Saikat Chatterjee", "title": "Estimate Exchange over Network is Good for Distributed Hard Thresholding\n  Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an existing distributed algorithm for learning sparse signals\nor data over networks. The algorithm is iterative and exchanges intermediate\nestimates of a sparse signal over a network. This learning strategy using\nexchange of intermediate estimates over the network requires a limited\ncommunication overhead for information transmission. Our objective in this\narticle is to show that the strategy is good for learning in spite of limited\ncommunication. In pursuit of this objective, we first provide a restricted\nisometry property (RIP)-based theoretical analysis on convergence of the\niterative algorithm. Then, using simulations, we show that the algorithm\nprovides competitive performance in learning sparse signals vis-a-vis an\nexisting alternate distributed algorithm. The alternate distributed algorithm\nexchanges more information including observations and system parameters.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 13:20:29 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Zaki", "Ahmed", ""], ["Mitra", "Partha P.", ""], ["Rasmussen", "Lars K.", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1709.07796", "submitter": "Vincent Francois-Lavet", "authors": "Vincent Francois-Lavet, Guillaume Rabusseau, Joelle Pineau, Damien\n  Ernst, Raphael Fonteneau", "title": "On overfitting and asymptotic bias in batch reinforcement learning with\n  partial observability", "comments": "Accepted at the Journal of Artificial Intelligence Research (JAIR) -\n  31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an analysis of the tradeoff between asymptotic bias\n(suboptimality with unlimited data) and overfitting (additional suboptimality\ndue to limited data) in the context of reinforcement learning with partial\nobservability. Our theoretical analysis formally characterizes that while\npotentially increasing the asymptotic bias, a smaller state representation\ndecreases the risk of overfitting. This analysis relies on expressing the\nquality of a state representation by bounding L1 error terms of the associated\nbelief states. Theoretical results are empirically illustrated when the state\nrepresentation is a truncated history of observations, both on synthetic POMDPs\nand on a large-scale POMDP in the context of smartgrids, with real-world data.\nFinally, similarly to known results in the fully observable setting, we also\nbriefly discuss and empirically illustrate how using function approximators and\nadapting the discount factor may enhance the tradeoff between asymptotic bias\nand overfitting in the partially observable context.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:56:35 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 18:30:04 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Francois-Lavet", "Vincent", ""], ["Rabusseau", "Guillaume", ""], ["Pineau", "Joelle", ""], ["Ernst", "Damien", ""], ["Fonteneau", "Raphael", ""]]}, {"id": "1709.07808", "submitter": "Mikel Sanz", "authors": "M. Sanz, L. Lamata, E. Solano", "title": "Quantum Memristors in Quantum Photonics", "comments": null, "journal-ref": "APL Photonics 3, 080801 (2018)", "doi": "10.1063/1.5036596", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to build quantum memristors in quantum photonic\nplatforms. We firstly design an effective beam splitter, which is tunable in\nreal-time, by means of a Mach-Zehnder-type array with two equal 50:50 beam\nsplitters and a tunable retarder, which allows us to control its reflectivity.\nThen, we show that this tunable beam splitter, when equipped with weak\nmeasurements and classical feedback, behaves as a quantum memristor. Indeed, in\norder to prove its quantumness, we show how to codify quantum information in\nthe coherent beams. Moreover, we estimate the memory capability of the quantum\nmemristor. Finally, we show the feasibility of the proposed setup in integrated\nquantum photonics.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:24:31 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 15:35:33 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Sanz", "M.", ""], ["Lamata", "L.", ""], ["Solano", "E.", ""]]}, {"id": "1709.07842", "submitter": "Lawrence Stewart", "authors": "Lawrence Stewart, Mark Stalzer", "title": "Bayesian Optimization for Parameter Tuning of the XOR Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When applying Machine Learning techniques to problems, one must select model\nparameters to ensure that the system converges but also does not become stuck\nat the objective function's local minimum. Tuning these parameters becomes a\nnon-trivial task for large models and it is not always apparent if the user has\nfound the optimal parameters. We aim to automate the process of tuning a Neural\nNetwork, (where only a limited number of parameter search attempts are\navailable) by implementing Bayesian Optimization. In particular, by assigning\nGaussian Process Priors to the parameter space, we utilize Bayesian\nOptimization to tune an Artificial Neural Network used to learn the XOR\nfunction, with the result of achieving higher prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 16:52:02 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 19:31:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Stewart", "Lawrence", ""], ["Stalzer", "Mark", ""]]}, {"id": "1709.07848", "submitter": "Lucas Lamata", "authors": "F. A. C\\'ardenas-L\\'opez, L. Lamata, J. C. Retamal, E. Solano", "title": "Multiqubit and multilevel quantum reinforcement learning with quantum\n  technologies", "comments": null, "journal-ref": "PLoS ONE 13(7):e0200455 (2018)", "doi": "10.1371/journal.pone.0200455", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a protocol to perform quantum reinforcement learning with quantum\ntechnologies. At variance with recent results on quantum reinforcement learning\nwith superconducting circuits, in our current protocol coherent feedback during\nthe learning process is not required, enabling its implementation in a wide\nvariety of quantum systems. We consider diverse possible scenarios for an\nagent, an environment, and a register that connects them, involving multiqubit\nand multilevel systems, as well as open-system dynamics. We finally propose\npossible implementations of this protocol in trapped ions and superconducting\ncircuits. The field of quantum reinforcement learning with quantum technologies\nwill enable enhanced quantum control, as well as more efficient machine\nlearning calculations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:04:44 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 18:04:34 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["C\u00e1rdenas-L\u00f3pez", "F. A.", ""], ["Lamata", "L.", ""], ["Retamal", "J. C.", ""], ["Solano", "E.", ""]]}, {"id": "1709.07871", "submitter": "Ethan Perez", "authors": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron\n  Courville", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "comments": "AAAI 2018. Code available at http://github.com/ethanjperez/film .\n  Extends arXiv:1707.03017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:54:12 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:25:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["Strub", "Florian", ""], ["de Vries", "Harm", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1709.07902", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, and James Glass", "title": "Unsupervised Learning of Disentangled and Interpretable Representations\n  from Sequential Data", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:36:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1709.07903", "submitter": "Weitong Ruan", "authors": "Weitong Ruan and Eric L. Miller", "title": "Ensemble Multi-task Gaussian Process Regression with Multiple Latent\n  Processes", "comments": "main body: 9 pages, supplemental material: 7 pages. This version\n  corrected a few typos in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task/Multi-output learning seeks to exploit correlation among tasks to\nenhance performance over learning or solving each task independently. In this\npaper, we investigate this problem in the context of Gaussian Processes (GPs)\nand propose a new model which learns a mixture of latent processes by\ndecomposing the covariance matrix into a sum of structured hidden components\neach of which is controlled by a latent GP over input features and a \"weight\"\nover tasks. From this sum structure, we propose a parallelizable parameter\nlearning algorithm with a predetermined initialization for the \"weights\". We\nalso notice that an ensemble parameter learning approach using mini-batches of\ntraining data not only reduces the computation complexity of learning but also\nimproves the regression performance. We evaluate our model on two datasets, the\nsmaller Swiss Jura dataset and another relatively larger ATMS dataset from\nNOAA. Substantial improvements are observed compared with established\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:38:31 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 20:28:38 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 15:09:58 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Ruan", "Weitong", ""], ["Miller", "Eric L.", ""]]}, {"id": "1709.07915", "submitter": "Amir Karami", "authors": "George Shaw Jr., and Amir Karami", "title": "Computational Content Analysis of Negative Tweets for Obesity, Diet,\n  Diabetes, and Exercise", "comments": "The 2017 Annual Meeting of the Association for Information Science\n  and Technology (ASIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media based digital epidemiology has the potential to support faster\nresponse and deeper understanding of public health related threats. This study\nproposes a new framework to analyze unstructured health related textual data\nvia Twitter users' post (tweets) to characterize the negative health sentiments\nand non-health related concerns in relations to the corpus of negative\nsentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the\ncollection of 6 million Tweets for one month, this study identified the\nprominent topics of users as it relates to the negative sentiments. Our\nproposed framework uses two text mining methods, sentiment analysis and topic\nmodeling, to discover negative topics. The negative sentiments of Twitter users\nsupport the literature narratives and the many morbidity issues that are\nassociated with DDEO and the linkage between obesity and diabetes. The\nframework offers a potential method to understand the publics' opinions and\nsentiments regarding DDEO. More importantly, this research provides new\nopportunities for computational social scientists, medical experts, and public\nhealth professionals to collectively address DDEO-related issues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:18:42 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Shaw", "George", "Jr."], ["Karami", "Amir", ""]]}, {"id": "1709.07916", "submitter": "Amir Karami", "authors": "Amir Karami, Alicia A. Dahl, Gabrielle Turner-McGrievy, Hadi Kharrazi,\n  Jr., George Shaw", "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "comments": "International Journal of Information Management (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media provide a platform for users to express their opinions and share\ninformation. Understanding public health opinions on social media, such as\nTwitter, offers a unique approach to characterizing common health issues such\nas diabetes, diet, exercise, and obesity (DDEO), however, collecting and\nanalyzing a large scale conversational public health data set is a challenging\nresearch task. The goal of this research is to analyze the characteristics of\nthe general public's opinions in regard to diabetes, diet, exercise and obesity\n(DDEO) as expressed on Twitter. A multi-component semantic and linguistic\nframework was developed to collect Twitter data, discover topics of interest\nabout DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8%\nof tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity.\nThe strongest correlation among the topics was determined between exercise and\nobesity. Other notable correlations were: diabetes and obesity, and diet and\nobesity DDEO terms were also identified as subtopics of each of the DDEO\ntopics. The frequent subtopics discussed along with Diabetes, excluding the\nDDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer.\nThe non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities,\nweight loss, religious, and mental health, while subtopics for Exercise\nincluded computer games, brain, fitness, and daily plan. Non-DDEO subtopics for\nObesity included Alzheimer, cancer, and children. With 2.67 billion social\nmedia users in 2016, publicly available data such as Twitter posts can be\nutilized to support clinical providers, public health experts, and social\nscientists in better understanding common public opinions in regard to\ndiabetes, diet, exercise, and obesity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:19:49 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Karami", "Amir", ""], ["Dahl", "Alicia A.", ""], ["Turner-McGrievy", "Gabrielle", ""], ["Kharrazi,", "Hadi", "Jr."], ["Shaw", "George", ""]]}, {"id": "1709.07944", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Marco Loog, Lambertus W. Bartels and Adri\\\"enne M.\n  Mendrik", "title": "MR Acquisition-Invariant Representation Learning", "comments": "36 pages, 2 appendices, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxelwise classification approaches are popular and effective methods for\ntissue quantification in brain magnetic resonance imaging (MRI) scans. However,\ngeneralization of these approaches is hampered by large differences between\nsets of MRI scans such as differences in field strength, vendor or acquisition\nprotocols. Due to this acquisition related variation, classifiers trained on\ndata from a specific scanner fail or under-perform when applied to data that\nwas acquired differently. In order to address this lack of generalization, we\npropose a Siamese neural network (MRAI-net) to learn a representation that\nminimizes the between-scanner variation, while maintaining the contrast between\nbrain tissues necessary for brain tissue quantification. The proposed MRAI-net\nwas evaluated on both simulated and real MRI data. After learning the MR\nacquisition invariant representation, any supervised classification model that\nuses feature vectors can be applied. In this paper, we provide a proof of\nprinciple, which shows that a linear classifier applied on the MRAI\nrepresentation is able to outperform supervised convolutional neural network\nclassifiers for tissue classification when little target training data is\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 20:52:14 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 22:31:50 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""], ["Bartels", "Lambertus W.", ""], ["Mendrik", "Adri\u00ebnne M.", ""]]}, {"id": "1709.08015", "submitter": "Samuel Schoenholz", "authors": "Samuel S. Schoenholz", "title": "Combining Machine Learning and Physics to Understand Glassy Systems", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1036/1/012021", "report-no": null, "categories": "stat.ML cond-mat.soft cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding of supercooled liquids and glasses has lagged significantly\nbehind that of simple liquids and crystalline solids. This is in part due to\nthe many possibly relevant degrees of freedom that are present due to the\ndisorder inherent to these systems and in part to non-equilibrium effects which\nare difficult to treat in the standard context of statistical physics. Together\nthese issues have resulted in a field whose theories are under-constrained by\nexperiment and where fundamental questions are still unresolved. Mean field\nresults have been successful in infinite dimensions but it is unclear to what\nextent they apply to realistic systems and assume uniform local structure. At\nodds with this are theories premised on the existence of structural defects.\nHowever, until recently it has been impossible to find structural signatures\nthat are predictive of dynamics. Here we summarize and recast the results from\nseveral recent papers offering a data driven approach to building a\nphenomenological theory of disordered materials by combining machine learning\nwith physical intuition.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 07:44:05 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Schoenholz", "Samuel S.", ""]]}, {"id": "1709.08073", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Laurynas Karazija, Nicholas D. Lane, Sourav\n  Bhattacharya, Edgar Liberis, Pietro Li\\`o, Angela Chieh, Otmane Bellahsen,\n  Matthieu Vegreville", "title": "Cross-modal Recurrent Models for Weight Objective Prediction from\n  Multimodal Time-series Data", "comments": "To appear in NIPS ML4H 2017 and NIPS TSW 2017", "journal-ref": null, "doi": "10.1145/3240925.3240937", "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse multimodal time-series data corresponding to weight, sleep and\nsteps measurements. We focus on predicting whether a user will successfully\nachieve his/her weight objective. For this, we design several deep long\nshort-term memory (LSTM) architectures, including a novel cross-modal LSTM\n(X-LSTM), and demonstrate their superiority over baseline approaches. The\nX-LSTM improves parameter efficiency by processing each modality separately and\nallowing for information flow between them by way of recurrent\ncross-connections. We present a general hyperparameter optimisation technique\nfor X-LSTMs, which allows us to significantly improve on the LSTM and a prior\nstate-of-the-art cross-modal approach, using a comparable number of parameters.\nFinally, we visualise the model's predictions, revealing implications about\nlatent variables in this task.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 16:42:34 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 07:45:28 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Karazija", "Laurynas", ""], ["Lane", "Nicholas D.", ""], ["Bhattacharya", "Sourav", ""], ["Liberis", "Edgar", ""], ["Li\u00f2", "Pietro", ""], ["Chieh", "Angela", ""], ["Bellahsen", "Otmane", ""], ["Vegreville", "Matthieu", ""]]}, {"id": "1709.08101", "submitter": "James P. Crutchfield", "authors": "S. Loomis, J. R. Mahoney, C. Aghamohammadi, and J. P. Crutchfield", "title": "Optimizing Quantum Models of Classical Channels: The reverse Holevo\n  problem", "comments": "13 pages, 6 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/qfact.htm; substantially updated\n  from v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a classical channel---a stochastic map from inputs to outputs---the\ninput can often be transformed to an intermediate variable that is\ninformationally smaller than the input. The new channel accurately simulates\nthe original but at a smaller transmission rate. Here, we examine this\nprocedure when the intermediate variable is a quantum state. We determine when\nand how well quantum simulations of classical channels may improve upon the\nminimal rates of classical simulation. This inverts Holevo's original question\nof quantifying the capacity of quantum channels with classical resources. We\nalso show that this problem is equivalent to another, involving the local\ngeneration of a distribution from common entanglement.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 18:50:47 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:01:34 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Loomis", "S.", ""], ["Mahoney", "J. R.", ""], ["Aghamohammadi", "C.", ""], ["Crutchfield", "J. P.", ""]]}, {"id": "1709.08104", "submitter": "Martin Slawski", "authors": "Martin Slawski", "title": "On Principal Components Regression, Random Projections, and Column\n  Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Components Regression (PCR) is a traditional tool for dimension\nreduction in linear regression that has been both criticized and defended. One\nconcern about PCR is that obtaining the leading principal components tends to\nbe computationally demanding for large data sets. While random projections do\nnot possess the optimality properties of the leading principal subspace, they\nare computationally appealing and hence have become increasingly popular in\nrecent years. In this paper, we present an analysis showing that for random\nprojections satisfying a Johnson-Lindenstrauss embedding property, the\nprediction error in subsequent regression is close to that of PCR, at the\nexpense of requiring a slightly large number of random projections than\nprincipal components. Column sub-sampling constitutes an even cheaper way of\nrandomized dimension reduction outside the class of Johnson-Lindenstrauss\ntransforms. We provide numerical results based on synthetic and real data as\nwell as basic theory revealing differences and commonalities in terms of\nstatistical performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 19:12:41 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 03:04:27 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Slawski", "Martin", ""]]}, {"id": "1709.08114", "submitter": "Yuanxin Li", "authors": "Yuanxin Li, Yuejie Chi, Huishuai Zhang, Yingbin Liang", "title": "Nonconvex Low-Rank Matrix Recovery with Arbitrary Outliers via\n  Median-Truncated Gradient Descent", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated the effectiveness of gradient descent for\ndirectly recovering the factors of low-rank matrices from random linear\nmeasurements in a globally convergent manner when initialized properly.\nHowever, the performance of existing algorithms is highly sensitive in the\npresence of outliers that may take arbitrary values. In this paper, we propose\na truncated gradient descent algorithm to improve the robustness against\noutliers, where the truncation is performed to rule out the contributions of\nsamples that deviate significantly from the {\\em sample median} of measurement\nresiduals adaptively in each iteration. We demonstrate that, when initialized\nin a basin of attraction close to the ground truth, the proposed algorithm\nconverges to the ground truth at a linear rate for the Gaussian measurement\nmodel with a near-optimal number of measurements, even when a constant fraction\nof the measurements are arbitrarily corrupted. In addition, we propose a new\ntruncated spectral method that ensures an initialization in the basin of\nattraction at slightly higher requirements. We finally provide numerical\nexperiments to validate the superior performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 20:35:12 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Yuanxin", ""], ["Chi", "Yuejie", ""], ["Zhang", "Huishuai", ""], ["Liang", "Yingbin", ""]]}, {"id": "1709.08120", "submitter": "Maria Bauza", "authors": "Maria Bauza, Alberto Rodriguez", "title": "GP-SUM. Gaussian Processes Filtering of non-Gaussian Beliefs", "comments": "WAFR 2018, 16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of stochastic dynamic filtering and state\npropagation with complex beliefs. The main contribution is GP-SUM, a filtering\nalgorithm tailored to dynamic systems and observation models expressed as\nGaussian Processes (GP), and to states represented as a weighted sum of\nGaussians. The key attribute of GP-SUM is that it does not rely on\nlinearizations of the dynamic or observation models, or on unimodal Gaussian\napproximations of the belief, hence enables tracking complex state\ndistributions. The algorithm can be seen as a combination of a sampling-based\nfilter with a probabilistic Bayes filter. On the one hand, GP-SUM operates by\nsampling the state distribution and propagating each sample through the dynamic\nsystem and observation models. On the other hand, it achieves effective\nsampling and accurate probabilistic propagation by relying on the GP form of\nthe system, and the sum-of-Gaussian form of the belief. We show that GP-SUM\noutperforms several GP-Bayes and Particle Filters on a standard benchmark. We\nalso demonstrate its use in a pushing task, predicting with experimental\naccuracy the naturally occurring non-Gaussian distributions.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 21:41:38 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 14:52:45 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 23:28:07 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Bauza", "Maria", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1709.08135", "submitter": "Hossein Sangrody", "authors": "Hossein Sangrody, Morteza Sarailoo, Ning Zhou, Nhu Tran, Mahdi\n  Motalleb, Elham Foruzan", "title": "Weather Forecasting Error in Solar Energy Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As renewable distributed energy resources (DERs) penetrate the power grid at\nan accelerating speed, it is essential for operators to have accurate solar\nphotovoltaic (PV) energy forecasting for efficient operations and planning.\nGenerally, observed weather data are applied in the solar PV generation\nforecasting model while in practice the energy forecasting is based on\nforecasted weather data. In this paper, a study on the uncertainty in weather\nforecasting for the most commonly used weather variables is presented. The\nforecasted weather data for six days ahead is compared with the observed data\nand the results of analysis are quantified by statistical metrics. In addition,\nthe most influential weather predictors in energy forecasting model are\nselected. The performance of historical and observed weather data errors is\nassessed using a solar PV generation forecasting model. Finally, a sensitivity\ntest is performed to identify the influential weather variables whose accurate\nvalues can significantly improve the results of energy forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 01:37:31 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Sangrody", "Hossein", ""], ["Sarailoo", "Morteza", ""], ["Zhou", "Ning", ""], ["Tran", "Nhu", ""], ["Motalleb", "Mahdi", ""], ["Foruzan", "Elham", ""]]}, {"id": "1709.08148", "submitter": "Tong Li", "authors": "Krishnakumar Balasubramanian, Tong Li, Ming Yuan", "title": "On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducing kernel Hilbert space (RKHS) embedding of distributions offers\na general and flexible framework for testing problems in arbitrary domains and\nhas attracted considerable amount of attention in recent years. To gain\ninsights into their operating characteristics, we study here the statistical\nperformance of such approaches within a minimax framework. Focusing on the case\nof goodness-of-fit tests, our analyses show that a vanilla version of the\nkernel-embedding based test could be suboptimal, and suggest a simple remedy by\nmoderating the embedding. We prove that the moderated approach provides optimal\ntests for a wide range of deviations from the null and can also be made\nadaptive over a large collection of interpolation spaces. Numerical experiments\nare presented to further demonstrate the merits of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 05:28:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Li", "Tong", ""], ["Yuan", "Ming", ""]]}, {"id": "1709.08201", "submitter": "Siyuan Li", "authors": "Siyuan Li and Chongjie Zhang", "title": "An Optimal Online Method of Selecting Source Policies for Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning significantly accelerates the reinforcement learning\nprocess by exploiting relevant knowledge from previous experiences. The problem\nof optimally selecting source policies during the learning process is of great\nimportance yet challenging. There has been little theoretical analysis of this\nproblem. In this paper, we develop an optimal online method to select source\npolicies for reinforcement learning. This method formulates online source\npolicy selection as a multi-armed bandit problem and augments Q-learning with\npolicy reuse. We provide theoretical guarantees of the optimal selection\nprocess and convergence to the optimal policy. In addition, we conduct\nexperiments on a grid-based robot navigation domain to demonstrate its\nefficiency and robustness by comparing to the state-of-the-art transfer\nlearning method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 14:17:14 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Siyuan", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1709.08294", "submitter": "Dinghan Shen", "authors": "Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin", "title": "Learning Context-Sensitive Convolutional Filters for Text Processing", "comments": "Accepted by EMNLP 2018 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have recently emerged as a popular\nbuilding block for natural language processing (NLP). Despite their success,\nmost existing CNN models employed in NLP share the same learned (and static)\nset of filters for all input sentences. In this paper, we consider an approach\nof using a small meta network to learn context-sensitive convolutional filters\nfor text processing. The role of meta network is to abstract the contextual\ninformation of a sentence or document into a set of input-aware filters. We\nfurther generalize this framework to model sentence pairs, where a\nbidirectional filter generation mechanism is introduced to encapsulate\nco-dependent sentence representations. In our benchmarks on four different\ntasks, including ontology classification, sentiment analysis, answer sentence\nselection, and paraphrase identification, our proposed model, a modified CNN\nwith context-sensitive filters, consistently outperforms the standard CNN and\nattention-based CNN baselines. By visualizing the learned context-sensitive\nfilters, we further validate and rationalize the effectiveness of proposed\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 02:29:26 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 04:15:40 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 16:29:50 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Shen", "Dinghan", ""], ["Min", "Martin Renqiang", ""], ["Li", "Yitong", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.08432", "submitter": "Lai Wei", "authors": "Xiaochen Chen, Lai Wei, Jiaxin Xu", "title": "House Price Prediction Using LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the house price data ranging from January 2004 to\nOctober 2016 to predict the average house price of November and December in\n2016 for each district in Beijing, Shanghai, Guangzhou and Shenzhen. We apply\nAutoregressive Integrated Moving Average model to generate the baseline while\nLSTM networks to build prediction model. These algorithms are compared in terms\nof Mean Squared Error. The result shows that the LSTM model has excellent\nproperties with respect to predict time series. Also, stateful LSTM networks\nand stack LSTM networks are employed to further study the improvement of\naccuracy of the house prediction model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 11:31:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Chen", "Xiaochen", ""], ["Wei", "Lai", ""], ["Xu", "Jiaxin", ""]]}, {"id": "1709.08461", "submitter": "Vincent Branders", "authors": "Vincent Branders, Pierre Schaus and Pierre Dupont", "title": "Mining a Sub-Matrix of Maximal Sum", "comments": "12 pages, 1 figure, Presented at NFMCP 2017, The 6th International\n  Workshop on New Frontiers in Mining Complex Patterns, Skopje, Macedonia, Sep\n  22, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering techniques have been widely used to identify homogeneous\nsubgroups within large data matrices, such as subsets of genes similarly\nexpressed across subsets of patients. Mining a max-sum sub-matrix is a related\nbut distinct problem for which one looks for a (non-necessarily contiguous)\nrectangular sub-matrix with a maximal sum of its entries. Le Van et al. (Ranked\nTiling, 2014) already illustrated its applicability to gene expression analysis\nand addressed it with a constraint programming (CP) approach combined with\nlarge neighborhood search (CP-LNS). In this work, we exhibit some key\nproperties of this NP-hard problem and define a bounding function such that\nlarger problems can be solved in reasonable time. Two different algorithms are\nproposed in order to exploit the highlighted characteristics of the problem: a\nCP approach with a global constraint (CPGC) and mixed integer linear\nprogramming (MILP). Practical experiments conducted both on synthetic and real\ngene expression data exhibit the characteristics of these approaches and their\nrelative benefits over the original CP-LNS method. Overall, the CPGC approach\ntends to be the fastest to produce a good solution. Yet, the MILP formulation\nis arguably the easiest to formulate and can also be competitive.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 12:54:17 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Branders", "Vincent", ""], ["Schaus", "Pierre", ""], ["Dupont", "Pierre", ""]]}, {"id": "1709.08491", "submitter": "Olivier Colliot", "authors": "Igor Koval, Jean-Baptiste Schiratti, Alexandre Routier, Michael Bacci,\n  Olivier Colliot, St\\'ephanie Allassonni\\`ere, Stanley Durrleman", "title": "Statistical learning of spatiotemporal patterns from longitudinal\n  manifold-valued networks", "comments": null, "journal-ref": "Proc. Medical Image Computing and Computer-Assisted Intervention,\n  MICCAI 2017, Lecture Notes in Computer Science, volume 10433, pp 451-459,\n  Springer", "doi": "10.1007/978-3-319-66182-7_52", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a mixed-effects model to learn spatiotempo-ral patterns on a\nnetwork by considering longitudinal measures distributed on a fixed graph. The\ndata come from repeated observations of subjects at different time points which\ntake the form of measurement maps distributed on a graph such as an image or a\nmesh. The model learns a typical group-average trajectory characterizing the\npropagation of measurement changes across the graph nodes. The subject-specific\ntrajectories are defined via spatial and temporal transformations of the\ngroup-average scenario, thus estimating the variability of spatiotemporal\npatterns within the group. To estimate population and individual model\nparameters, we adapted a stochastic version of the Expectation-Maximization\nalgorithm, the MCMC-SAEM. The model is used to describe the propagation of\ncortical atrophy during the course of Alzheimer's Disease. Model parameters\nshow the variability of this average pattern of atrophy in terms of\ntrajectories across brain regions, age at disease onset and pace of\npropagation. We show that the personalization of this model yields accurate\nprediction of maps of cortical thickness in patients.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 13:57:08 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Koval", "Igor", ""], ["Schiratti", "Jean-Baptiste", ""], ["Routier", "Alexandre", ""], ["Bacci", "Michael", ""], ["Colliot", "Olivier", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1709.08519", "submitter": "Mikel Sanz", "authors": "F. A. C\\'ardenas-L\\'opez, M. Sanz, J. C. Retamal, E. Solano", "title": "Enhanced Quantum Synchronization via Quantum Machine Learning", "comments": null, "journal-ref": "Adv. Quantum Technol. 1800076 (2019)", "doi": "10.1002/qute.201800076", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantum synchronization between a pair of two-level systems\ninside two coupled cavities. By using a digital-analog decomposition of the\nmaster equation that rules the system dynamics, we show that this approach\nleads to quantum synchronization between both two-level systems. Moreover, we\ncan identify in this digital-analog block decomposition the fundamental\nelements of a quantum machine learning protocol, in which the agent and the\nenvironment (learning units) interact through a mediating system, namely, the\nregister. If we can additionally equip this algorithm with a classical feedback\nmechanism, which consists of projective measurements in the register,\nreinitialization of the register state and local conditional operations on the\nagent and environment subspace, a powerful and flexible quantum machine\nlearning protocol emerges. Indeed, numerical simulations show that this\nprotocol enhances the synchronization process, even when every subsystem\nexperience different loss/decoherence mechanisms, and give us the flexibility\nto choose the synchronization state. Finally, we propose an implementation\nbased on current technologies in superconducting circuits.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:40:11 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 11:21:39 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["C\u00e1rdenas-L\u00f3pez", "F. A.", ""], ["Sanz", "M.", ""], ["Retamal", "J. C.", ""], ["Solano", "E.", ""]]}, {"id": "1709.08520", "submitter": "Nicholas Rhinehart", "authors": "Arun Venkatraman, Nicholas Rhinehart, Wen Sun, Lerrel Pinto, Martial\n  Hebert, Byron Boots, Kris M. Kitani, J. Andrew Bagnell", "title": "Predictive-State Decoders: Encoding the Future into Recurrent Networks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are a vital modeling technique that rely on\ninternal states learned indirectly by optimization of a supervised,\nunsupervised, or reinforcement training loss. RNNs are used to model dynamic\nprocesses that are characterized by underlying latent states whose form is\noften unknown, precluding its analytic representation inside an RNN. In the\nPredictive-State Representation (PSR) literature, latent state processes are\nmodeled by an internal state representation that directly models the\ndistribution of future observations, and most recent work in this area has\nrelied on explicitly representing and targeting sufficient statistics of this\nprobability distribution. We seek to combine the advantages of RNNs and PSRs by\naugmenting existing state-of-the-art recurrent neural networks with\nPredictive-State Decoders (PSDs), which add supervision to the network's\ninternal state representation to target predicting future observations.\nPredictive-State Decoders are simple to implement and easily incorporated into\nexisting training pipelines via additional loss regularization. We demonstrate\nthe effectiveness of PSDs with experimental results in three different domains:\nprobabilistic filtering, Imitation Learning, and Reinforcement Learning. In\neach, our method improves statistical performance of state-of-the-art recurrent\nbaselines and does so with fewer iterations and less data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:40:18 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Venkatraman", "Arun", ""], ["Rhinehart", "Nicholas", ""], ["Sun", "Wen", ""], ["Pinto", "Lerrel", ""], ["Hebert", "Martial", ""], ["Boots", "Byron", ""], ["Kitani", "Kris M.", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1709.08524", "submitter": "Alexander Shekhovtsov", "authors": "Boris Flach, Alexander Shekhovtsov, Ondrej Fikar", "title": "Generative learning for deep networks", "comments": "submitted to AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning, taking into account full distribution of the data, referred to as\ngenerative, is not feasible with deep neural networks (DNNs) because they model\nonly the conditional distribution of the outputs given the inputs. Current\nsolutions are either based on joint probability models facing difficult\nestimation problems or learn two separate networks, mapping inputs to outputs\n(recognition) and vice-versa (generation). We propose an intermediate approach.\nFirst, we show that forward computation in DNNs with logistic sigmoid\nactivations corresponds to a simplified approximate Bayesian inference in a\ndirected probabilistic multi-layer model. This connection allows to interpret\nDNN as a probabilistic model of the output and all hidden units given the\ninput. Second, we propose that in order for the recognition and generation\nnetworks to be more consistent with the joint model of the data, weights of the\nrecognition and generator network should be related by transposition. We\ndemonstrate in a tentative experiment that such a coupled pair can be learned\ngeneratively, modelling the full distribution of the data, and has enough\ncapacity to perform well in both recognition and generation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:43:53 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Flach", "Boris", ""], ["Shekhovtsov", "Alexander", ""], ["Fikar", "Ondrej", ""]]}, {"id": "1709.08568", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "The Consciousness Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new prior is proposed for learning representations of high-level concepts\nof the kind we manipulate with language. This prior can be combined with other\npriors in order to help disentangling abstract factors from each other. It is\ninspired by cognitive neuroscience theories of consciousness, seen as a\nbottleneck through which just a few elements, after having been selected by\nattention from a broader pool, are then broadcast and condition further\nprocessing, both in perception and decision-making. The set of recently\nselected elements one becomes aware of is seen as forming a low-dimensional\nconscious state. This conscious state is combining the few concepts\nconstituting a conscious thought, i.e., what one is immediately conscious of at\na particular moment. We claim that this architectural and\ninformation-processing constraint corresponds to assumptions about the joint\ndistribution between high-level concepts. To the extent that these assumptions\nare generally true (and the form of natural language seems consistent with\nthem), they can form a useful prior for representation learning. A\nlow-dimensional thought or conscious state is analogous to a sentence: it\ninvolves only a few variables and yet can make a statement with very high\nprobability of being true. This is consistent with a joint distribution (over\nhigh-level concepts) which has the form of a sparse factor graph, i.e., where\nthe dependencies captured by each factor of the factor graph involve only very\nfew variables while creating a strong dip in the overall energy function. The\nconsciousness prior also makes it natural to map conscious states to natural\nlanguage utterances or to express classical AI knowledge in a form similar to\nfacts and rules, albeit capturing uncertainty as well as efficient search\nmechanisms implemented by attention mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:59:11 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 22:53:39 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1709.08571", "submitter": "Tianbao Yang", "authors": "Mingrui Liu, Tianbao Yang", "title": "On Noisy Negative Curvature Descent: Competing with Gradient Descent for\n  Faster Non-convex Optimization", "comments": "added a stochastic algorithm with high probability second-order\n  convergence and corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hessian-vector product has been utilized to find a second-order\nstationary solution with strong complexity guarantee (e.g., almost linear time\ncomplexity in the problem's dimensionality). In this paper, we propose to\nfurther reduce the number of Hessian-vector products for faster non-convex\noptimization. Previous algorithms need to approximate the smallest eigen-value\nwith a sufficient precision (e.g., $\\epsilon_2\\ll 1$) in order to achieve a\nsufficiently accurate second-order stationary solution (i.e.,\n$\\lambda_{\\min}(\\nabla^2 f(\\x))\\geq -\\epsilon_2)$. In contrast, the proposed\nalgorithms only need to compute the smallest eigen-vector approximating the\ncorresponding eigen-value up to a small power of current gradient's norm. As a\nresult, it can dramatically reduce the number of Hessian-vector products during\nthe course of optimization before reaching first-order stationary points (e.g.,\nsaddle points). The key building block of the proposed algorithms is a novel\nupdating step named the NCG step, which lets a noisy negative curvature descent\ncompete with the gradient descent. We show that the worst-case time complexity\nof the proposed algorithms with their favorable prescribed accuracy\nrequirements can match the best in literature for achieving a second-order\nstationary point but with an arguably smaller per-iteration cost. We also show\nthat the proposed algorithms can benefit from inexact Hessian by developing\ntheir variants accepting inexact Hessian under a mild condition for achieving\nthe same goal. Moreover, we develop a stochastic algorithm for a finite or\ninfinite sum non-convex optimization problem. To the best of our knowledge, the\nproposed stochastic algorithm is the first one that converges to a second-order\nstationary point in {\\it high probability} with a time complexity independent\nof the sample size and almost linear in dimensionality.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 16:19:37 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 20:47:53 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Liu", "Mingrui", ""], ["Yang", "Tianbao", ""]]}, {"id": "1709.08669", "submitter": "Konstantina Christakopoulou", "authors": "Konstantina Christakopoulou, Adam Tauman Kalai", "title": "Glass-Box Program Synthesis: A Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed models which learn to write computer programs from data use\neither input/output examples or rich execution traces. Instead, we argue that a\nnovel alternative is to use a glass-box loss function, given as a program\nitself that can be directly inspected. Glass-box optimization covers a wide\nrange of problems, from computing the greatest common divisor of two integers,\nto learning-to-learn problems.\n  In this paper, we present an intelligent search system which learns, given\nthe partial program and the glass-box problem, the probabilities over the space\nof programs. We empirically demonstrate that our informed search procedure\nleads to significant improvements compared to brute-force program search, both\nin terms of accuracy and time. For our experiments we use rich context free\ngrammars inspired by number theory, text processing, and algebra. Our results\nshow that (i) performing 4 rounds of our framework typically solves about 70%\nof the target problems, (ii) our framework can improve itself even in domain\nagnostic scenarios, and (iii) it can solve problems that would be otherwise too\nslow to solve with brute-force search.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 18:43:56 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Christakopoulou", "Konstantina", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1709.08730", "submitter": "Gustavo Daniel Sosa Cabrera", "authors": "Gustavo Sosa-Cabrera, Miguel Garc\\'ia-Torres, Santiago G\\'omez,\n  Christian Schaerer, Federico Divina", "title": "Understanding a Version of Multivariate Symmetric Uncertainty to assist\n  in Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. We discovered a condition that preserves\ngood quality in the MSU under different combinations of these three factors,\nproviding a new useful criterion to help drive the process of dimension\nreduction.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 21:41:20 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Sosa-Cabrera", "Gustavo", ""], ["Garc\u00eda-Torres", "Miguel", ""], ["G\u00f3mez", "Santiago", ""], ["Schaerer", "Christian", ""], ["Divina", "Federico", ""]]}, {"id": "1709.08770", "submitter": "Iku Ohama", "authors": "Iku Ohama, Issei Sato, Takuya Kida, Hiroki Arimura", "title": "On the Model Shrinkage Effect of Gamma Process Edge Partition Models", "comments": "To appear in the 31st Annual Conference on Neural Information\n  Processing Systems (NIPS2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edge partition model (EPM) is a fundamental Bayesian nonparametric model\nfor extracting an overlapping structure from binary matrix. The EPM adopts a\ngamma process ($\\Gamma$P) prior to automatically shrink the number of active\natoms. However, we empirically found that the model shrinkage of the EPM does\nnot typically work appropriately and leads to an overfitted solution. An\nanalysis of the expectation of the EPM's intensity function suggested that the\ngamma priors for the EPM hyperparameters disturb the model shrinkage effect of\nthe internal $\\Gamma$P. In order to ensure that the model shrinkage effect of\nthe EPM works in an appropriate manner, we proposed two novel generative\nconstructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM\nincorporating Dirichlet priors instead of the gamma priors. Furthermore, all\nDEPM's model parameters including the infinite atoms of the $\\Gamma$P prior\ncould be marginalized out, and thus it was possible to derive a truly infinite\nDEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler.\nWe experimentally confirmed that the model shrinkage of the proposed models\nworks well and that the IDEPM indicated state-of-the-art performance in\ngeneralization ability, link prediction accuracy, mixing efficiency, and\nconvergence speed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:00:13 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Ohama", "Iku", ""], ["Sato", "Issei", ""], ["Kida", "Takuya", ""], ["Arimura", "Hiroki", ""]]}, {"id": "1709.08795", "submitter": "Krishnakumar Balasubramanian", "authors": "Zhuoran Yang, Krishnakumar Balasubramanian, Han Liu", "title": "On Stein's Identity and Near-Optimal Estimation in High-dimensional\n  Index Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the parametric components of semi-parametric multiple\nindex models in a high-dimensional and non-Gaussian setting. Such models form a\nrich class of non-linear models with applications to signal processing, machine\nlearning and statistics. Our estimators leverage the score function based first\nand second-order Stein's identities and do not require the covariates to\nsatisfy Gaussian or elliptical symmetry assumptions common in the literature.\nMoreover, to handle score functions and responses that are heavy-tailed, our\nestimators are constructed via carefully thresholding their empirical\ncounterparts. We show that our estimator achieves near-optimal statistical rate\nof convergence in several settings. We supplement our theoretical results via\nsimulation experiments that confirm the theory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 03:04:11 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:48:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Yang", "Zhuoran", ""], ["Balasubramanian", "Krishnakumar", ""], ["Liu", "Han", ""]]}, {"id": "1709.08862", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela", "title": "Bayesian Inference of Spreading Processes on Networks", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2018.0129", "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases are studied to understand their spreading mechanisms, to\nevaluate control strategies and to predict the risk and course of future\noutbreaks. Because people only interact with a small number of individuals, and\nbecause the structure of these interactions matters for spreading processes,\nthe pairwise relationships between individuals in a population can be usefully\nrepresented by a network. Although the underlying processes of transmission are\ndifferent, the network approach can be used to study the spread of pathogens in\na contact network or the spread of rumors in an online social network. We study\nsimulated simple and complex epidemics on synthetic networks and on two\nempirical networks, a social / contact network in an Indian village and an\nonline social network in the U.S. Our goal is to learn simultaneously about the\nspreading process parameters and the source node (first infected node) of the\nepidemic, given a fixed and known network structure, and observations about\nstate of nodes at several points in time. Our inference scheme is based on\napproximate Bayesian computation (ABC), an inference technique for complex\nmodels with likelihood functions that are either expensive to evaluate or\nanalytically intractable. ABC enables us to adopt a Bayesian approach to the\nproblem despite the posterior distribution being very complex. Our method is\nagnostic about the topology of the network and the nature of the spreading\nprocess. It generally performs well and, somewhat counter-intuitively, the\ninference problem appears to be easier on more heterogeneous network\ntopologies, which enhances its future applicability to real-world settings\nwhere few networks have homogeneous topologies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:00:46 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 21:35:41 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 13:55:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1709.08868", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Energy-Based Models as Generative ConvNets via Multi-grid\n  Modeling and Sampling", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-grid method for learning energy-based generative\nConvNet models of images. For each grid, we learn an energy-based probabilistic\nmodel where the energy function is defined by a bottom-up convolutional neural\nnetwork (ConvNet or CNN). Learning such a model requires generating synthesized\nexamples from the model. Within each iteration of our learning algorithm, for\neach observed training image, we generate synthesized images at multiple grids\nby initializing the finite-step MCMC sampling from a minimal 1 x 1 version of\nthe training image. The synthesized image at each subsequent grid is obtained\nby a finite-step MCMC initialized from the synthesized image generated at the\nprevious coarser grid. After obtaining the synthesized examples, the parameters\nof the models at multiple grids are updated separately and simultaneously based\non the differences between synthesized and observed examples. We show that this\nmulti-grid method can learn realistic energy-based generative ConvNet models,\nand it outperforms the original contrastive divergence (CD) and persistent CD.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:48:52 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 08:30:01 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 22:43:45 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gao", "Ruiqi", ""], ["Lu", "Yang", ""], ["Zhou", "Junpei", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1709.08878", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, Percy Liang", "title": "Generating Sentences by Editing Prototypes", "comments": "14 pages, Transactions of the Association for Computational\n  Linguistics (TACL), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative model of sentences that first samples a prototype\nsentence from the training corpus and then edits it into a new sentence.\nCompared to traditional models that generate from scratch either left-to-right\nor by first sampling a latent sentence vector, our prototype-then-edit model\nimproves perplexity on language modeling and generates higher quality outputs\naccording to human evaluation. Furthermore, the model gives rise to a latent\nedit vector that captures interpretable semantics such as sentence similarity\nand sentence-level analogies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:11:33 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 04:57:15 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Guu", "Kelvin", ""], ["Hashimoto", "Tatsunori B.", ""], ["Oren", "Yonatan", ""], ["Liang", "Percy", ""]]}, {"id": "1709.08894", "submitter": "Henning Petzka", "authors": "Henning Petzka, Asja Fischer, Denis Lukovnicov", "title": "On the regularization of Wasserstein GANs", "comments": "Published as a conference paper at ICLR 2018. * Henning Petzka and\n  Asja Fischer contributed equally to this work (11 pages +13 pages appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their invention, generative adversarial networks (GANs) have become a\npopular approach for learning to model a distribution of real (unlabeled) data.\nConvergence problems during training are overcome by Wasserstein GANs which\nminimize the distance between the model and the empirical distribution in terms\nof a different metric, but thereby introduce a Lipschitz constraint into the\noptimization problem. A simple way to enforce the Lipschitz constraint on the\nclass of functions, which can be modeled by the neural network, is weight\nclipping. It was proposed that training can be improved by instead augmenting\nthe loss by a regularization term that penalizes the deviation of the gradient\nof the critic (as a function of the network's input) from one. We present\ntheoretical arguments why using a weaker regularization term enforcing the\nLipschitz constraint is preferable. These arguments are supported by\nexperimental results on toy data sets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:53:41 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 13:55:14 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Petzka", "Henning", ""], ["Fischer", "Asja", ""], ["Lukovnicov", "Denis", ""]]}, {"id": "1709.08915", "submitter": "Alexander Marx", "authors": "Alexander Marx and Jilles Vreeken", "title": "Telling Cause from Effect using MDL-based Local and Global Regression", "comments": "10 pages, To appear in ICDM17", "journal-ref": null, "doi": "10.1109/ICDM.2017.40", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of inferring the causal direction between\ntwo univariate numeric random variables $X$ and $Y$ from observational data.\nThe two-variable case is especially difficult to solve since it is not possible\nto use standard conditional independence tests between the variables.\n  To tackle this problem, we follow an information theoretic approach based on\nKolmogorov complexity and use the Minimum Description Length (MDL) principle to\nprovide a practical solution. In particular, we propose a compression scheme to\nencode local and global functional relations using MDL-based regression. We\ninfer $X$ causes $Y$ in case it is shorter to describe $Y$ as a function of $X$\nthan the inverse direction. In addition, we introduce Slope, an efficient\nlinear-time algorithm that through thorough empirical evaluation on both\nsynthetic and real world data we show outperforms the state of the art by a\nwide margin.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 09:49:18 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Marx", "Alexander", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1709.09018", "submitter": "Zhi-Hua Zhou", "authors": "Ji Feng and Zhi-Hua Zhou", "title": "AutoEncoder by Forest", "comments": null, "journal-ref": "AAAI 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoding is an important task which is typically realized by deep neural\nnetworks (DNNs) such as convolutional neural networks (CNN). In this paper, we\npropose EncoderForest (abbrv. eForest), the first tree ensemble based\nauto-encoder. We present a procedure for enabling forests to do backward\nreconstruction by utilizing the equivalent classes defined by decision paths of\nthe trees, and demonstrate its usage in both supervised and unsupervised\nsetting. Experiments show that, compared with DNN autoencoders, eForest is able\nto obtain lower reconstruction error with fast training speed, while the model\nitself is reusable and damage-tolerable.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 13:54:34 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Feng", "Ji", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1709.09102", "submitter": "Vladimir Spokoiny", "authors": "Kirill Efimov, Larisa Adamyan, Vladimir Spokoiny", "title": "Adaptive Nonparametric Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to non-parametric cluster analysis called\nAdaptive Weights Clustering (AWC). The idea is to identify the clustering\nstructure by checking at different points and for different scales on departure\nfrom local homogeneity. The proposed procedure describes the clustering\nstructure in terms of weights \\( w_{ij} \\) each of them measures the degree of\nlocal inhomogeneity for two neighbor local clusters using statistical tests of\n\"no gap\" between them. % The procedure starts from very local scale, then the\nparameter of locality grows by some factor at each step. The method is fully\nadaptive and does not require to specify the number of clusters or their\nstructure. The clustering results are not sensitive to noise and outliers, the\nprocedure is able to recover different clusters with sharp edges or manifold\nstructure. The method is scalable and computationally feasible. An intensive\nnumerical study shows a state-of-the-art performance of the method in various\nartificial examples and applications to text data. Our theoretical study states\noptimal sensitivity of AWC to local inhomogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:59:59 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Efimov", "Kirill", ""], ["Adamyan", "Larisa", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1709.09130", "submitter": "Souradeep Dutta", "authors": "Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, Ashish Tiwari", "title": "Output Range Analysis for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (NN) are extensively used for machine learning tasks\nsuch as image classification, perception and control of autonomous systems.\nIncreasingly, these deep NNs are also been deployed in high-assurance\napplications. Thus, there is a pressing need for developing techniques to\nverify neural networks to check whether certain user-expected properties are\nsatisfied. In this paper, we study a specific verification problem of computing\na guaranteed range for the output of a deep neural network given a set of\ninputs represented as a convex polyhedron. Range estimation is a key primitive\nfor verifying deep NNs. We present an efficient range estimation algorithm that\nuses a combination of local search and linear programming problems to\nefficiently find the maximum and minimum values taken by the outputs of the NN\nover the given input set. In contrast to recently proposed \"monolithic\"\noptimization approaches, we use local gradient descent to repeatedly find and\neliminate local minima of the function. The final global optimum is certified\nusing a mixed integer programming instance. We implement our approach and\ncompare it with Reluplex, a recently proposed solver for deep neural networks.\nWe demonstrate the effectiveness of the proposed approach for verification of\nNNs used in automated control as well as those used in classification.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:56:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Dutta", "Souradeep", ""], ["Jha", "Susmit", ""], ["Sanakaranarayanan", "Sriram", ""], ["Tiwari", "Ashish", ""]]}, {"id": "1709.09161", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "EDEN: Evolutionary Deep Networks for Efficient Machine Learning", "comments": "7 pages, 3 figures, 3 tables and see video\n  https://vimeo.com/234510097", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks continue to show improved performance with increasing\ndepth, an encouraging trend that implies an explosion in the possible\npermutations of network architectures and hyperparameters for which there is\nlittle intuitive guidance. To address this increasing complexity, we propose\nEvolutionary DEep Networks (EDEN), a computationally efficient\nneuro-evolutionary algorithm which interfaces to any deep neural network\nplatform, such as TensorFlow. We show that EDEN evolves simple yet successful\narchitectures built from embedding, 1D and 2D convolutional, max pooling and\nfully connected layers along with their hyperparameters. Evaluation of EDEN\nacross seven image and sentiment classification datasets shows that it reliably\nfinds good networks -- and in three cases achieves state-of-the-art results --\neven on a single GPU, in just 6-24 hours. Our study provides a first attempt at\napplying neuro-evolution to the creation of 1D convolutional networks for\nsentiment analysis including the optimisation of the embedding layer.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 17:56:31 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1709.09216", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Ryan P. Adams, and Tamara Broderick", "title": "PASS-GLM: polynomial approximate sufficient statistics for scalable\n  Bayesian GLM inference", "comments": "In Proceedings of the 31st Annual Conference on Neural Information\n  Processing Systems (NIPS 2017). v3: corrected typos in Appendix A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) -- such as logistic regression, Poisson\nregression, and robust regression -- provide interpretable models for diverse\ndata types. Probabilistic approaches, particularly Bayesian ones, allow\ncoherent estimates of uncertainty, incorporation of prior information, and\nsharing of power across experiments via hierarchical models. In practice,\nhowever, the approximate Bayesian methods necessary for inference have either\nfailed to scale to large data sets or failed to provide theoretical guarantees\non the quality of inference. We propose a new approach based on constructing\npolynomial approximate sufficient statistics for GLMs (PASS-GLM). We\ndemonstrate that our method admits a simple algorithm as well as trivial\nstreaming and distributed extensions that do not compound error across\ncomputations. We provide theoretical guarantees on the quality of point (MAP)\nestimates, the approximate posterior, and posterior mean and uncertainty\nestimates. We validate our approach empirically in the case of logistic\nregression using a quadratic approximation and show competitive performance\nwith stochastic gradient descent, MCMC, and the Laplace approximation in terms\nof speed and multiple measures of accuracy -- including on an advertising data\nset with 40 million data points and 20,000 covariates.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:48:13 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:11:11 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 21:09:06 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Adams", "Ryan P.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1709.09268", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Nima Bari, Roman Vichr, Farhad A. Goodarzi", "title": "FSL-BM: Fuzzy Supervised Learning with Binary Meta-Feature for\n  Classification", "comments": "FICC2018", "journal-ref": null, "doi": "10.1007/978-3-030-03405-4_46", "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel real-time Fuzzy Supervised Learning with Binary\nMeta-Feature (FSL-BM) for big data classification task. The study of real-time\nalgorithms addresses several major concerns, which are namely: accuracy, memory\nconsumption, and ability to stretch assumptions and time complexity. Attaining\na fast computational model providing fuzzy logic and supervised learning is one\nof the main challenges in the machine learning. In this research paper, we\npresent FSL-BM algorithm as an efficient solution of supervised learning with\nfuzzy logic processing using binary meta-feature representation using Hamming\nDistance and Hash function to relax assumptions. While many studies focused on\nreducing time complexity and increasing accuracy during the last decade, the\nnovel contribution of this proposed solution comes through integration of\nHamming Distance, Hash function, binary meta-features, binary classification to\nprovide real time supervised method. Hash Tables (HT) component gives a fast\naccess to existing indices; and therefore, the generation of new indices in a\nconstant time complexity, which supersedes existing fuzzy supervised algorithms\nwith better or comparable results. To summarize, the main contribution of this\ntechnique for real-time Fuzzy Supervised Learning is to represent hypothesis\nthrough binary input as meta-feature space and creating the Fuzzy Supervised\nHash table to train and validate model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 21:52:41 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 23:34:10 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kowsari", "Kamran", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Goodarzi", "Farhad A.", ""]]}, {"id": "1709.09274", "submitter": "Abhishek Srivastav", "authors": "Devesh K Jha, Nurali Virani, Jan Reimann, Abhishek Srivastav, Asok Ray", "title": "Symbolic Analysis-based Reduced Order Markov Modeling of Time Series\n  Data", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for reduced-order Markov modeling for compact\nrepresentation of time-series data. In this work, symbolic dynamics-based tools\nhave been used to infer an approximate generative Markov model. The time-series\ndata are first symbolized by partitioning the continuous measurement space of\nthe signal and then, the discrete sequential data are modeled using symbolic\ndynamics. In the proposed approach, the size of temporal memory of the symbol\nsequence is estimated from spectral properties of the resulting stochastic\nmatrix corresponding to a first-order Markov model of the symbol sequence.\nThen, hierarchical clustering is used to represent the states of the\ncorresponding full-state Markov model to construct a reduced-order or size\nMarkov model with a non-deterministic algebraic structure. Subsequently, the\nparameters of the reduced-order Markov model are identified from the original\nmodel by making use of a Bayesian inference rule. The final model is selected\nusing information-theoretic criteria. The proposed concept is elucidated and\nvalidated on two different data sets as examples. The first example analyzes a\nset of pressure data from a swirl-stabilized combustor, where controlled\nprotocols are used to induce flame instabilities. Variations in the complexity\nof the derived Markov model represent how the system operating condition\nchanges from a stable to an unstable combustion regime. In the second example,\nthe data set is taken from NASA's data repository for prognostics of bearings\non rotating shafts. We show that, even with a very small state-space, the\nreduced-order models are able to achieve comparable performance and that the\nproposed approach provides flexibility in the selection of a final model for\nrepresentation and learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 22:11:26 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Jha", "Devesh K", ""], ["Virani", "Nurali", ""], ["Reimann", "Jan", ""], ["Srivastav", "Abhishek", ""], ["Ray", "Asok", ""]]}, {"id": "1709.09301", "submitter": "Mikhail Yurochkin", "authors": "Mikhail Yurochkin, XuanLong Nguyen and Nikolaos Vasiloglou", "title": "Multi-way Interacting Regression via Factorization Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian regression method that accounts for multi-way\ninteractions of arbitrary orders among the predictor variables. Our model makes\nuse of a factorization mechanism for representing the regression coefficients\nof interactions among the predictors, while the interaction selection is guided\nby a prior distribution on random hypergraphs, a construction which generalizes\nthe Finite Feature Model. We present a posterior inference algorithm based on\nGibbs sampling, and establish posterior consistency of our regression model.\nOur method is evaluated with extensive experiments on simulated data and\ndemonstrated to be able to identify meaningful interactions in applications in\ngenetics and retail demand forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 01:51:19 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Yurochkin", "Mikhail", ""], ["Nguyen", "XuanLong", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1709.09328", "submitter": "Chen Gao", "authors": "Chen Gao, Brian E. Moore, and Raj Rao Nadakuditi", "title": "Augmented Robust PCA For Foreground-Background Separation on Noisy,\n  Moving Camera Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach for robust PCA with total variation\nregularization for foreground-background separation and denoising on noisy,\nmoving camera video. Our proposed algorithm registers the raw (possibly\ncorrupted) frames of a video and then jointly processes the registered frames\nto produce a decomposition of the scene into a low-rank background component\nthat captures the static components of the scene, a smooth foreground component\nthat captures the dynamic components of the scene, and a sparse component that\ncan isolate corruptions and other non-idealities. Unlike existing methods, our\nproposed algorithm produces a panoramic low-rank component that spans the\nentire field of view, automatically stitching together corrupted data from\npartially overlapping scenes. The low-rank portion of our robust PCA model is\nbased on a recently discovered optimal low-rank matrix estimator (OptShrink)\nthat requires no parameter tuning. We demonstrate the performance of our\nalgorithm on both static and moving camera videos corrupted by noise and\noutliers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 04:17:43 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Gao", "Chen", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1709.09527", "submitter": "Benjamin Donnot", "authors": "Benjamin Donnot (TAU, LRI), Isabelle Guyon (LRI, TAU), Marc Schoenauer\n  (TAU, LRI), Patrick Panciatici, Antoine Marot", "title": "Introducing machine learning for power system operation support", "comments": "IREP Symposium, Aug 2017, Espinho, Portugal. 2017,\n  \\&\\#x3008;http://irep2017.inesctec.pt/\\&\\#x3009", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of assisting human dispatchers in operating power\ngrids in today's changing context using machine learning, with theaim of\nincreasing security and reducing costs. Power networks are highly regulated\nsystems, which at all times must meet varying demands of electricity with a\ncomplex production system, including conventional power plants, less\npredictable renewable energies (such as wind or solar power), and the\npossibility of buying/selling electricity on the international market with more\nand more actors involved at a Europeanscale. This problem is becoming ever more\nchallenging in an aging network infrastructure. One of the primary goals of\ndispatchers is to protect equipment (e.g. avoid that transmission lines\noverheat) with few degrees of freedom: we are considering in this paper solely\nmodifications in network topology, i.e. re-configuring the way in which lines,\ntransformers, productions and loads are connected in sub-stations. Using years\nof historical data collected by the French Transmission Service Operator (TSO)\n\"R\\'eseau de Transport d'Electricit\\'e\" (RTE), we develop novel machine\nlearning techniques (drawing on \"deep learning\") to mimic human decisions to\ndevise \"remedial actions\" to prevent any line to violate power flow limits\n(so-called \"thermal limits\"). The proposed technique is hybrid. It does not\nrely purely on machine learning: every action will be tested with actual\nsimulators before being proposed to the dispatchers or implemented on the grid.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:59:35 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Donnot", "Benjamin", "", "TAU, LRI"], ["Guyon", "Isabelle", "", "LRI, TAU"], ["Schoenauer", "Marc", "", "TAU, LRI"], ["Panciatici", "Patrick", ""], ["Marot", "Antoine", ""]]}, {"id": "1709.09735", "submitter": "Zackory Erickson", "authors": "Zackory Erickson, Henry M. Clever, Greg Turk, C. Karen Liu, and\n  Charles C. Kemp", "title": "Deep Haptic Model Predictive Control for Robot-Assisted Dressing", "comments": "8 pages, 12 figures, 1 table, 2018 IEEE International Conference on\n  Robotics and Automation (ICRA)", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460656", "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted dressing offers an opportunity to benefit the lives of many\npeople with disabilities, such as some older adults. However, robots currently\nlack common sense about the physical implications of their actions on people.\nThe physical implications of dressing are complicated by non-rigid garments,\nwhich can result in a robot indirectly applying high forces to a person's body.\nWe present a deep recurrent model that, when given a proposed action by the\nrobot, predicts the forces a garment will apply to a person's body. We also\nshow that a robot can provide better dressing assistance by using this model\nwith model predictive control. The predictions made by our model only use\nhaptic and kinematic observations from the robot's end effector, which are\nreadily attainable. Collecting training data from real world physical\nhuman-robot interaction can be time consuming, costly, and put people at risk.\nInstead, we train our predictive model using data collected in an entirely\nself-supervised fashion from a physics-based simulation. We evaluated our\napproach with a PR2 robot that attempted to pull a hospital gown onto the arms\nof 10 human participants. With a 0.2s prediction horizon, our controller\nsucceeded at high rates and lowered applied force while navigating the garment\naround a persons fist and elbow without getting caught. Shorter prediction\nhorizons resulted in significantly reduced performance with the sleeve catching\non the participants' fists and elbows, demonstrating the value of our model's\npredictions. These behaviors of mitigating catches emerged from our deep\npredictive model and the controller objective function, which primarily\npenalizes high forces.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 21:10:26 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 14:43:31 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 21:30:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Erickson", "Zackory", ""], ["Clever", "Henry M.", ""], ["Turk", "Greg", ""], ["Liu", "C. Karen", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1709.09761", "submitter": "Pegah Fakhari", "authors": "Pegah Fakhari, Arash Khodadadi, Jerome Busemeyer", "title": "The detour problem in a stochastic environment: Tolman revisited", "comments": "44 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a grid world task to study human planning and re-planning\nbehavior in an unknown stochastic environment. In our grid world, participants\nwere asked to travel from a random starting point to a random goal position\nwhile maximizing their reward. Because they were not familiar with the\nenvironment, they needed to learn its characteristics from experience to plan\noptimally. Later in the task, we randomly blocked the optimal path to\ninvestigate whether and how people adjust their original plans to find a\ndetour. To this end, we developed and compared 12 different models. These\nmodels were different on how they learned and represented the environment and\nhow they planned to catch the goal. The majority of our participants were able\nto plan optimally. We also showed that people were capable of revising their\nplans when an unexpected event occurred. The result from the model comparison\nshowed that the model-based reinforcement learning approach provided the best\naccount for the data and outperformed heuristics in explaining the behavioral\ndata in the re-planning trials.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 23:22:06 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Fakhari", "Pegah", ""], ["Khodadadi", "Arash", ""], ["Busemeyer", "Jerome", ""]]}, {"id": "1709.09844", "submitter": "Amit Mandelbaum", "authors": "Amit Mandelbaum and Daphna Weinshall", "title": "Distance-based Confidence Score for Neural Network Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable measurement of confidence in classifiers' predictions is very\nimportant for many applications and is, therefore, an important part of\nclassifier design. Yet, although deep learning has received tremendous\nattention in recent years, not much progress has been made in quantifying the\nprediction confidence of neural network classifiers. Bayesian models offer a\nmathematically grounded framework to reason about model uncertainty, but\nusually come with prohibitive computational costs. In this paper we propose a\nsimple, scalable method to achieve a reliable confidence score, based on the\ndata embedding derived from the penultimate layer of the network. We\ninvestigate two ways to achieve desirable embeddings, by using either a\ndistance-based loss or Adversarial Training. We then test the benefits of our\nmethod when used for classification error prediction, weighting an ensemble of\nclassifiers, and novelty detection. In all tasks we show significant\nimprovement over traditional, commonly used confidence scores.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 08:09:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mandelbaum", "Amit", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1709.09868", "submitter": "Albert Swart", "authors": "Albert Swart and Niko Brummer", "title": "A Generative Model for Score Normalization in Speaker Recognition", "comments": null, "journal-ref": "InterSpeech 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretical framework for thinking about score normalization,\nwhich confirms that normalization is not needed under (admittedly fragile)\nideal conditions. If, however, these conditions are not met, e.g. under\ndata-set shift between training and runtime, our theory reveals dependencies\nbetween scores that could be exploited by strategies such as score\nnormalization. Indeed, it has been demonstrated over and over experimentally,\nthat various ad-hoc score normalization recipes do work. We present a first\nattempt at using probability theory to design a generative score-space\nnormalization model which gives similar improvements to ZT-norm on the\ntext-dependent RSR 2015 database.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 09:32:10 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Swart", "Albert", ""], ["Brummer", "Niko", ""]]}, {"id": "1709.09929", "submitter": "Milad Zafar Nezhad", "authors": "Milad Zafar Nezhad, Dongxiao Zhu, Najibesadat Sadati, Kai Yang,\n  Phillip Levy", "title": "SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2017.00-68", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional medicine typically applies one-size-fits-all treatment for the\nentire patient population whereas precision medicine develops tailored\ntreatment schemes for different patient subgroups. The fact that some factors\nmay be more significant for a specific patient subgroup motivates clinicians\nand medical researchers to develop new approaches to subgroup detection and\nanalysis, which is an effective strategy to personalize treatment. In this\nstudy, we propose a novel patient subgroup detection method, called Supervised\nBiclustring (SUBIC) using convex optimization and apply our approach to detect\npatient subgroups and prioritize risk factors for hypertension (HTN) in a\nvulnerable demographic subgroup (African-American). Our approach not only finds\npatient subgroups with guidance of a clinically relevant target variable but\nalso identifies and prioritizes risk factors by pursuing sparsity of the input\nvariables and encouraging similarity among the input variables and between the\ninput and target variables\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 22:21:46 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Nezhad", "Milad Zafar", ""], ["Zhu", "Dongxiao", ""], ["Sadati", "Najibesadat", ""], ["Yang", "Kai", ""], ["Levy", "Phillip", ""]]}, {"id": "1709.10029", "submitter": "Bart Paul Gerard Van Parys", "authors": "Dimitris Bertsimas, Bart Van Parys", "title": "Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase\n  Transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel binary convex reformulation of the sparse regression\nproblem that constitutes a new duality perspective. We devise a new cutting\nplane method and provide evidence that it can solve to provable optimality the\nsparse regression problem for sample sizes n and number of regressors p in the\n100,000s, that is two orders of magnitude better than the current state of the\nart, in seconds. The ability to solve the problem for very high dimensions\nallows us to observe new phase transition phenomena. Contrary to traditional\ncomplexity theory which suggests that the difficulty of a problem increases\nwith problem size, the sparse regression problem has the property that as the\nnumber of samples $n$ increases the problem becomes easier in that the solution\nrecovers 100% of the true signal, and our approach solves the problem extremely\nfast (in fact faster than Lasso), while for small number of samples n, our\napproach takes a larger amount of time to solve the problem, but importantly\nthe optimal solution provides a statistically more relevant regressor. We argue\nthat our exact sparse regression approach presents a superior alternative over\nheuristic methods available at present.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:00:49 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Van Parys", "Bart", ""]]}, {"id": "1709.10030", "submitter": "Bart Paul Gerard Van Parys", "authors": "Dimitris Bertsimas, Bart Van Parys", "title": "Sparse Hierarchical Regression with Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for exact hierarchical sparse polynomial\nregression. Our regressor is that degree $r$ polynomial which depends on at\nmost $k$ inputs, counting at most $\\ell$ monomial terms, which minimizes the\nsum of the squares of its prediction errors. The previous hierarchical sparse\nspecification aligns well with modern big data settings where many inputs are\nnot relevant for prediction purposes and the functional complexity of the\nregressor needs to be controlled as to avoid overfitting. We present a two-step\napproach to this hierarchical sparse regression problem. First, we discard\nirrelevant inputs using an extremely fast input ranking heuristic. Secondly, we\ntake advantage of modern cutting plane methods for integer optimization to\nsolve our resulting reduced hierarchical $(k, \\ell)$-sparse problem exactly.\nThe ability of our method to identify all $k$ relevant inputs and all $\\ell$\nmonomial terms is shown empirically to experience a phase transition.\nCrucially, the same transition also presents itself in our ability to reject\nall irrelevant features and monomials as well. In the regime where our method\nis statistically powerful, its computational complexity is interestingly on par\nwith Lasso based heuristics. The presented work fills a void in terms of a lack\nof powerful disciplined nonlinear sparse regression methods in high-dimensional\nsettings. Our method is shown empirically to scale to regression problems with\n$n\\approx 10,000$ observations for input dimension $p\\approx 1,000$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:01:08 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Van Parys", "Bart", ""]]}, {"id": "1709.10041", "submitter": "Ivo Shterev", "authors": "Ivo D. Shterev, David B. Dunson, Cliburn Chan, Gregory D. Sempowski", "title": "Bayesian Multi Plate High Throughput Screening of Compounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput screening of compounds (chemicals) is an essential part of\ndrug discovery [7], involving thousands to millions of compounds, with the\npurpose of identifying candidate hits. Most statistical tools, including the\nindustry standard B-score method, work on individual compound plates and do not\nexploit cross-plate correlation or statistical strength among plates. We\npresent a new statistical framework for high throughput screening of compounds\nbased on Bayesian nonparametric modeling. The proposed approach is able to\nidentify candidate hits from multiple plates simultaneously, sharing\nstatistical strength among plates and providing more robust estimates of\ncompound activity. It can flexibly accommodate arbitrary distributions of\ncompound activities and is applicable to any plate geometry. The algorithm\nprovides a principled statistical approach for hit identification and false\ndiscovery rate control. Experiments demonstrate significant improvements in hit\nidentification sensitivity and specificity over the B-score method, which is\nhighly sensitive to threshold choice. The framework is implemented as an\nefficient R extension package BHTSpack and is suitable for large scale data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:17:25 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Shterev", "Ivo D.", ""], ["Dunson", "David B.", ""], ["Chan", "Cliburn", ""], ["Sempowski", "Gregory D.", ""]]}, {"id": "1709.10056", "submitter": "Peter Xenopoulos", "authors": "Peter Xenopoulos", "title": "Introducing DeepBalance: Random Deep Belief Network Ensembles to Address\n  Class Imbalance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance problems manifest in domains such as financial fraud\ndetection or network intrusion analysis, where the prevalence of one class is\nmuch higher than another. Typically, practitioners are more interested in\npredicting the minority class than the majority class as the minority class may\ncarry a higher misclassification cost. However, classifier performance\ndeteriorates in the face of class imbalance as oftentimes classifiers may\npredict every point as the majority class. Methods for dealing with class\nimbalance include cost-sensitive learning or resampling techniques. In this\npaper, we introduce DeepBalance, an ensemble of deep belief networks trained\nwith balanced bootstraps and random feature selection. We demonstrate that our\nproposed method outperforms baseline resampling methods such as SMOTE and\nunder- and over-sampling in metrics such as AUC and sensitivity when applied to\na highly imbalanced financial transaction data. Additionally, we explore\nperformance and training time implications of various model parameters.\nFurthermore, we show that our model is easily parallelizable, which can reduce\ntraining times. Finally, we present an implementation of DeepBalance in R.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:49:14 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 07:48:10 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Xenopoulos", "Peter", ""]]}, {"id": "1709.10106", "submitter": "Bryan Ostdiek", "authors": "Spencer Chang, Timothy Cohen, and Bryan Ostdiek", "title": "What is the Machine Learning?", "comments": "6 pages, 3 figures. Version published in PRD, discussion added", "journal-ref": "Phys. Rev. D 97, 056009 (2018)", "doi": "10.1103/PhysRevD.97.056009", "report-no": null, "categories": "hep-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of machine learning tools to problems of physical interest are\noften criticized for producing sensitivity at the expense of transparency. To\naddress this concern, we explore a data planing procedure for identifying\ncombinations of variables -- aided by physical intuition -- that can\ndiscriminate signal from background. Weights are introduced to smooth away the\nfeatures in a given variable(s). New networks are then trained on this modified\ndata. Observed decreases in sensitivity diagnose the variable's discriminating\npower. Planing also allows the investigation of the linear versus non-linear\nnature of the boundaries between signal and background. We demonstrate the\nefficacy of this approach using a toy example, followed by an application to an\nidealized heavy resonance scenario at the Large Hadron Collider. By unpacking\nthe information being utilized by these algorithms, this method puts in context\nwhat it means for a machine to learn.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 18:00:00 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 16:38:43 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Chang", "Spencer", ""], ["Cohen", "Timothy", ""], ["Ostdiek", "Bryan", ""]]}, {"id": "1709.10142", "submitter": "Arash Rahnama", "authors": "Arash Rahnama and Panos J. Antsaklis", "title": "Resilient Learning-Based Control for Synchronization of Passive\n  Multi-Agent Systems under Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show synchronization for a group of output passive agents\nthat communicate with each other according to an underlying communication graph\nto achieve a common goal. We propose a distributed event-triggered control\nframework that will guarantee synchronization and considerably decrease the\nrequired communication load on the band-limited network. We define a general\nByzantine attack on the event-triggered multi-agent network system and\ncharacterize its negative effects on synchronization. The Byzantine agents are\ncapable of intelligently falsifying their data and manipulating the underlying\ncommunication graph by altering their respective control feedback weights. We\nintroduce a decentralized detection framework and analyze its steady-state and\ntransient performances. We propose a way of identifying individual Byzantine\nneighbors and a learning-based method of estimating the attack parameters.\nLastly, we propose learning-based control approaches to mitigate the negative\neffects of the adversarial attack.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 19:36:53 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Rahnama", "Arash", ""], ["Antsaklis", "Panos J.", ""]]}, {"id": "1709.10152", "submitter": "Cheolmin Kim", "authors": "Cheolmin Kim, Diego Klabjan", "title": "A Simple and Fast Algorithm for L1-norm Kernel PCA", "comments": "14 pages, 7 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2019)", "doi": "10.1109/TPAMI.2019.2903505", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for L1-norm kernel PCA and provide a convergence\nanalysis for it. While an optimal solution of L2-norm kernel PCA can be\nobtained through matrix decomposition, finding that of L1-norm kernel PCA is\nnot trivial due to its non-convexity and non-smoothness. We provide a novel\nreformulation through which an equivalent, geometrically interpretable problem\nis obtained. Based on the geometric interpretation of the reformulated problem,\nwe present a fixed-point type algorithm that iteratively computes a binary\nweight for each observation. As the algorithm requires only inner products of\ndata vectors, it is computationally efficient and the kernel trick is\napplicable. In the convergence analysis, we show that the algorithm converges\nto a local optimal solution in a finite number of steps. Moreover, we provide a\nrate of convergence analysis, which has been never done for any L1-norm PCA\nalgorithm, proving that the sequence of objective values converges at a linear\nrate. In numerical experiments, we show that the algorithm is robust in the\npresence of entry-wise perturbations and computationally scalable, especially\nin a large-scale setting. Lastly, we introduce an application to outlier\ndetection where the model based on the proposed algorithm outperforms the\nbenchmark algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 20:03:14 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 06:14:27 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kim", "Cheolmin", ""], ["Klabjan", "Diego", ""]]}, {"id": "1709.10222", "submitter": "Miron Ivanov", "authors": "Miron Ivanov", "title": "Comparison of PCA with ICA from data distribution perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We performed an empirical comparison of ICA and PCA algorithms by applying\nthem on two simulated noisy time series with varying distribution parameters\nand level of noise. In general, ICA shows better results than PCA because it\ntakes into account higher moments of data distribution. On the other hand, PCA\nremains quite sensitive to the level of correlations among signals.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 02:51:52 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Ivanov", "Miron", ""]]}, {"id": "1709.10250", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Jianbo Chen, Martin J. Wainwright, Michael I. Jordan", "title": "DAGGER: A sequential algorithm for FDR control on DAGs", "comments": "29 pages, 10 figures, accepted for publication by Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a linear-time, single-pass, top-down algorithm for multiple\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\nedges specify a partial ordering in which hypotheses must be tested. The\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\n(FDR) while satisfying the logical constraint that a rejected node's parents\nmust also be rejected. It is designed for sequential testing settings, when the\nDAG structure is known a priori, but the $p$-values are obtained selectively\n(such as in a sequence of experiments), but the algorithm is also applicable in\nnon-sequential settings when all $p$-values can be calculated in advance (such\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\nEvolving Rejections on DAGs, provably controls the false discovery rate under\nindependence, positive dependence or arbitrary dependence of the $p$-values.\nThe DAGGER procedure specializes to known algorithms in the special cases of\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\nprocedure when the DAG has no edges. We explore the empirical performance of\nDAGGER using simulations, as well as a real dataset corresponding to a gene\nontology, showing favorable performance in terms of time and power.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:38:11 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 01:21:47 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 20:06:02 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Chen", "Jianbo", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1709.10276", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai", "title": "Fast online low-rank tensor subspace tracking by CP decomposition using\n  recursive least squares from incomplete observations", "comments": "Extended version of arXiv:1602.07067 (IEEE International Conference\n  on Acoustics, Speech and Signal Processing (ICASSP 2016))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online subspace tracking of a partially observed\nhigh-dimensional data stream corrupted by noise, where we assume that the data\nlie in a low-dimensional linear subspace. This problem is cast as an online\nlow-rank tensor completion problem. We propose a novel online tensor subspace\ntracking algorithm based on the CANDECOMP/PARAFAC (CP) decomposition, dubbed\nOnLine Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). The\nproposed algorithm especially addresses the case in which the subspace of\ninterest is dynamically time-varying. To this end, we build up our proposed\nalgorithm exploiting the recursive least squares (RLS), which is the\nsecond-order gradient algorithm. Numerical evaluations on synthetic datasets\nand real-world datasets such as communication network traffic, environmental\ndata, and surveillance videos, show that the proposed OLSTEC algorithm\noutperforms state-of-the-art online algorithms in terms of the convergence rate\nper iteration.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:12:39 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Kasai", "Hiroyuki", ""]]}, {"id": "1709.10297", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Slava Voloshynovskiy, Dimche Kostadinov and Olga\n  Taran", "title": "Privacy Preserving Identification Using Sparse Approximation with\n  Ambiguization", "comments": "submitted to WIFS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a privacy preserving encoding framework for\nidentification applications covering biometrics, physical object security and\nthe Internet of Things (IoT). The proposed framework is based on a sparsifying\ntransform, which consists of a trained linear map, an element-wise\nnonlinearity, and privacy amplification. The sparsifying transform and privacy\namplification are not symmetric for the data owner and data user. We\ndemonstrate that the proposed approach is closely related to sparse ternary\ncodes (STC), a recent information-theoretic concept proposed for fast\napproximate nearest neighbor (ANN) search in high dimensional feature spaces\nthat being machine learning in nature also offers significant benefits in\ncomparison to sparse approximation and binary embedding approaches. We\ndemonstrate that the privacy of the database outsourced to a server as well as\nthe privacy of the data user are preserved at a low computational cost, storage\nand communication burdens.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:24:06 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""], ["Taran", "Olga", ""]]}, {"id": "1709.10323", "submitter": "Nino Antulov-Fantulin", "authors": "Dijana Tolic, Nino Antulov-Fantulin, Ivica Kopriva", "title": "A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to\n  Subspace Clustering", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2018.04.029", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent theoretical analysis shows the equivalence between non-negative\nmatrix factorization (NMF) and spectral clustering based approach to subspace\nclustering. As NMF and many of its variants are essentially linear, we\nintroduce a nonlinear NMF with explicit orthogonality and derive general\nkernel-based orthogonal multiplicative update rules to solve the subspace\nclustering problem. In nonlinear orthogonal NMF framework, we propose two\nsubspace clustering algorithms, named kernel-based non-negative subspace\nclustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral\nnormalized cut and ratio cut clustering. We further extend the nonlinear\northogonal NMF framework and introduce a graph regularization to obtain a\nfactorization that respects a local geometric structure of the data after the\nnonlinear mapping. The proposed NMF-based approach to subspace clustering takes\ninto account the nonlinear nature of the manifold, as well as its intrinsic\nlocal geometry, which considerably improves the clustering performance when\ncompared to the several recently proposed state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:02:16 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Tolic", "Dijana", ""], ["Antulov-Fantulin", "Nino", ""], ["Kopriva", "Ivica", ""]]}, {"id": "1709.10367", "submitter": "Maja Rudolph", "authors": "Maja Rudolph, Francisco Ruiz, Susan Athey, David Blei", "title": "Structured Embedding Models for Grouped Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful approach for analyzing language, and\nexponential family embeddings (EFE) extend them to other types of data. Here we\ndevelop structured exponential family embeddings (S-EFE), a method for\ndiscovering embeddings that vary across related groups of data. We study how\nthe word usage of U.S. Congressional speeches varies across states and party\naffiliation, how words are used differently across sections of the ArXiv, and\nhow the co-purchase patterns of groceries can vary across seasons. Key to the\nsuccess of our method is that the groups share statistical information. We\ndevelop two sharing strategies: hierarchical modeling and amortization. We\ndemonstrate the benefits of this approach in empirical studies of speeches,\nabstracts, and shopping baskets. We show how S-EFE enables group-specific\ninterpretation of word usage, and outperforms EFE in predicting held-out data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 14:14:58 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Rudolph", "Maja", ""], ["Ruiz", "Francisco", ""], ["Athey", "Susan", ""], ["Blei", "David", ""]]}, {"id": "1709.10432", "submitter": "Qi Meng", "authors": "Qi Meng, Wei Chen, Yue Wang, Zhi-Ming Ma, Tie-Yan Liu", "title": "Convergence Analysis of Distributed Stochastic Gradient Descent with\n  Shuffling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using stochastic gradient descent to solve large-scale machine learning\nproblems, a common practice of data processing is to shuffle the training data,\npartition the data across multiple machines if needed, and then perform several\nepochs of training on the re-shuffled (either locally or globally) data. The\nabove procedure makes the instances used to compute the gradients no longer\nindependently sampled from the training data set. Then does the distributed SGD\nmethod have desirable convergence properties in this practical situation? In\nthis paper, we give answers to this question. First, we give a mathematical\nformulation for the practical data processing procedure in distributed machine\nlearning, which we call data partition with global/local shuffling. We observe\nthat global shuffling is equivalent to without-replacement sampling if the\nshuffling operations are independent. We prove that SGD with global shuffling\nhas convergence guarantee in both convex and non-convex cases. An interesting\nfinding is that, the non-convex tasks like deep learning are more suitable to\napply shuffling comparing to the convex tasks. Second, we conduct the\nconvergence analysis for SGD with local shuffling. The convergence rate for\nlocal shuffling is slower than that for global shuffling, since it will lose\nsome information if there's no communication between partitioned data. Finally,\nwe consider the situation when the permutation after shuffling is not uniformly\ndistributed (insufficient shuffling), and discuss the condition under which\nthis insufficiency will not influence the convergence rate. Our theoretical\nresults provide important insights to large-scale machine learning, especially\nin the selection of data processing methods in order to achieve faster\nconvergence and good speedup. Our theoretical findings are verified by\nextensive experiments on logistic regression and deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:44:05 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Meng", "Qi", ""], ["Chen", "Wei", ""], ["Wang", "Yue", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1709.10433", "submitter": "Vishnu Naresh Boddeti", "authors": "Sixue Gong, Vishnu Naresh Boddeti, Anil K. Jain", "title": "On the Capacity of Face Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the following question, given a face representation,\nhow many identities can it resolve? In other words, what is the capacity of the\nface representation? A scientific basis for estimating the capacity of a given\nface representation will not only benefit the evaluation and comparison of\ndifferent representation methods, but will also establish an upper bound on the\nscalability of an automatic face recognition system. We cast the face capacity\nproblem in terms of packing bounds on a low-dimensional manifold embedded\nwithin a deep representation space. By explicitly accounting for the manifold\nstructure of the representation as well two different sources of\nrepresentational noise: epistemic (model) uncertainty and aleatoric (data)\nvariability, our approach is able to estimate the capacity of a given face\nrepresentation. To demonstrate the efficacy of our approach, we estimate the\ncapacity of two deep neural network based face representations, namely\n128-dimensional FaceNet and 512-dimensional SphereFace. Numerical experiments\non unconstrained faces (IJB-C) provides a capacity upper bound of\n$2.7\\times10^4$ for FaceNet and $8.4\\times10^4$ for SphereFace representation\nat a false acceptance rate (FAR) of 1%. As expected, capacity reduces\ndrastically at lower FARs. The capacity at FAR of 0.1% and 0.001% is\n$2.2\\times10^3$ and $1.6\\times10^{1}$, respectively for FaceNet and\n$3.6\\times10^3$ and $6.0\\times10^0$, respectively for SphereFace.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:47:13 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 15:14:24 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 19:45:51 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gong", "Sixue", ""], ["Boddeti", "Vishnu Naresh", ""], ["Jain", "Anil K.", ""]]}]