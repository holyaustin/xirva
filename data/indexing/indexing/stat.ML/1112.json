[{"id": "1112.0076", "submitter": "Nicol\\'as Della Penna", "authors": "Nicolas Della Penna, Mark D. Reid", "title": "Bandit Market Makers", "comments": "A previous version of this work appeared in the NIPS 2011 Workshop on\n  Computational Social Science and the Wisdom of the Crowds", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.GT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce a modular framework for market making. It combines cost-function\nbased automated market makers with bandit algorithms. We obtain worst-case\nprofits guarantee's relative to the best in hindsight within a class of natural\n\"overround\" cost functions . This combination allow us to have\ndistribution-free guarantees on the regret of profits while preserving the\nbounded worst-case losses and computational tractability over combinatorial\nspaces of the cost function based approach. We present simulation results to\nbetter understand the practical behaviour of market makers from the framework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 03:48:11 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2011 14:48:12 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2012 19:47:11 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2013 01:39:36 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Della Penna", "Nicolas", ""], ["Reid", "Mark D.", ""]]}, {"id": "1112.0463", "submitter": "Aleksandar Dogandzic", "authors": "Aleksandar Dogandzic, Renliang Gu, Kun Qiu", "title": "Mask Iterative Hard Thresholding Algorithms for Sparse Image\n  Reconstruction of Objects with Known Contour", "comments": "6 pages, 19 figures, 2011 45th Asilomar Conf. Signals, Syst. Comput.,\n  Pacific Grove, CA, Nov. 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop mask iterative hard thresholding algorithms (mask IHT and mask\nDORE) for sparse image reconstruction of objects with known contour. The\nmeasurements follow a noisy underdetermined linear model common in the\ncompressive sampling literature. Assuming that the contour of the object that\nwe wish to reconstruct is known and that the signal outside the contour is\nzero, we formulate a constrained residual squared error minimization problem\nthat incorporates both the geometric information (i.e. the knowledge of the\nobject's contour) and the signal sparsity constraint. We first introduce a mask\nIHT method that aims at solving this minimization problem and guarantees\nmonotonically non-increasing residual squared error for a given signal sparsity\nlevel. We then propose a double overrelaxation scheme for accelerating the\nconvergence of the mask IHT algorithm. We also apply convex mask reconstruction\napproaches that employ a convex relaxation of the signal sparsity constraint.\nIn X-ray computed tomography (CT), we propose an automatic scheme for\nextracting the convex hull of the inspected object from the measured sinograms;\nthe obtained convex hull is used to capture the object contour information. We\ncompare the proposed mask reconstruction schemes with the existing large-scale\nsparse signal reconstruction methods via numerical simulations and demonstrate\nthat, by exploiting both the geometric contour information of the underlying\nimage and sparsity of its wavelet coefficients, we can reconstruct this image\nusing a significantly smaller number of measurements than the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 13:59:36 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["Dogandzic", "Aleksandar", ""], ["Gu", "Renliang", ""], ["Qiu", "Kun", ""]]}, {"id": "1112.0467", "submitter": "Erwin Riegler", "authors": "Erwin Riegler, Gunvor Elisabeth Kirkelund, Carles Navarro Manch\\'on,\n  Mihai-Alin Badiu, Bernard Henry Fleury", "title": "Merging Belief Propagation and the Mean Field Approximation: A Free\n  Energy Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint message passing approach that combines belief propagation\nand the mean field approximation. Our analysis is based on the region-based\nfree energy approximation method proposed by Yedidia et al. We show that the\nmessage passing fixed-point equations obtained with this combination correspond\nto stationary points of a constrained region-based free energy approximation.\nMoreover, we present a convergent implementation of these message passing\nfixedpoint equations provided that the underlying factor graph fulfills certain\ntechnical conditions. In addition, we show how to include hard constraints in\nthe part of the factor graph corresponding to belief propagation. Finally, we\ndemonstrate an application of our method to iterative channel estimation and\ndecoding in an orthogonal frequency division multiplexing (OFDM) system.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 14:21:57 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 10:57:20 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2012 11:50:49 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Riegler", "Erwin", ""], ["Kirkelund", "Gunvor Elisabeth", ""], ["Manch\u00f3n", "Carles Navarro", ""], ["Badiu", "Mihai-Alin", ""], ["Fleury", "Bernard Henry", ""]]}, {"id": "1112.0611", "submitter": "Masashi Sugiyama", "authors": "Masashi Sugiyama, Makoto Yamada, Manabu Kimura, and Hirotaka Hachiya", "title": "Information-Maximization Clustering based on Squared-Loss Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-maximization clustering learns a probabilistic classifier in an\nunsupervised manner so that mutual information between feature vectors and\ncluster assignments is maximized. A notable advantage of this approach is that\nit only involves continuous optimization of model parameters, which is\nsubstantially easier to solve than discrete optimization of cluster\nassignments. However, existing methods still involve non-convex optimization\nproblems, and therefore finding a good local optimal solution is not\nstraightforward in practice. In this paper, we propose an alternative\ninformation-maximization clustering method based on a squared-loss variant of\nmutual information. This novel approach gives a clustering solution\nanalytically in a computationally efficient way via kernel eigenvalue\ndecomposition. Furthermore, we provide a practical model selection procedure\nthat allows us to objectively optimize tuning parameters included in the kernel\nfunction. Through experiments, we demonstrate the usefulness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 00:27:50 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Sugiyama", "Masashi", ""], ["Yamada", "Makoto", ""], ["Kimura", "Manabu", ""], ["Hachiya", "Hirotaka", ""]]}, {"id": "1112.0698", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula and Cynthia Rudin", "title": "Machine Learning with Operational Costs", "comments": "Current version: Final version appearing in JMLR 2013. v2: Many parts\n  have been rewritten including the introduction, Minor correction of Theorem\n  6. 38 pages. Previously: v1: 36 pages, 8 figures. Short version appears in\n  Proceedings of the International Symposium on Artificial Intelligence and\n  Mathematics, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a way to align statistical modeling with decision making.\nWe provide a method that propagates the uncertainty in predictive modeling to\nthe uncertainty in operational cost, where operational cost is the amount spent\nby the practitioner in solving the problem. The method allows us to explore the\nrange of operational costs associated with the set of reasonable statistical\nmodels, so as to provide a useful way for practitioners to understand\nuncertainty. To do this, the operational cost is cast as a regularization term\nin a learning algorithm's objective function, allowing either an optimistic or\npessimistic view of possible costs, depending on the regularization parameter.\nFrom another perspective, if we have prior knowledge about the operational\ncost, for instance that it should be low, this knowledge can help to restrict\nthe hypothesis space, and can help with generalization. We provide a\ntheoretical generalization bound for this scenario. We also show that learning\nwith operational costs is related to robust optimization.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 22:32:04 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 04:17:04 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2012 14:00:02 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2013 03:24:33 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1112.0716", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T. Tokdar", "title": "Dimension adaptability of Gaussian process models with variable\n  selection and projection", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now known that an extended Gaussian process model equipped with\nrescaling can adapt to different smoothness levels of a function valued\nparameter in many nonparametric Bayesian analyses, offering a posterior\nconvergence rate that is optimal (up to logarithmic factors) for the smoothness\nclass the true function belongs to. This optimal rate also depends on the\ndimension of the function's domain and one could potentially obtain a faster\nrate of convergence by casting the analysis in a lower dimensional subspace\nthat does not amount to any loss of information about the true function. In\ngeneral such a subspace is not known a priori but can be explored by equipping\nthe model with variable selection or linear projection. We demonstrate that for\nnonparametric regression, classification, density estimation and density\nregression, a rescaled Gaussian process model equipped with variable selection\nor linear projection offers a posterior convergence rate that is optimal (up to\nlogarithmic factors) for the lowest dimension in which the analysis could be\ncast without any loss of information about the true function. Theoretical\nexploration of such dimension reduction features appears novel for Bayesian\nnonparametric models with or without Gaussian processes.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 04:35:07 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Tokdar", "Surya T.", ""]]}, {"id": "1112.0918", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "On best subset regression", "comments": "This paper has been withdrawn by the author. A related paper entitled\n  \"Better subset regression\" is arXiv:1212.0634", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss the variable selection method from \\ell0-norm\nconstrained regression, which is equivalent to the problem of finding the best\nsubset of a fixed size. Our study focuses on two aspects, consistency and\ncomputation. We prove that the sparse estimator from such a method can retain\nall of the important variables asymptotically for even exponentially growing\ndimensionality under regularity conditions. This indicates that the best subset\nregression method can efficiently shrink the full model down to a submodel of a\nsize less than the sample size, which can be analyzed by well-developed\nregression techniques for such cases in a follow-up study. We provide an\niterative algorithm, called orthogonalizing subset selection (OSS), to address\ncomputational issues in best subset regression. OSS is an EM algorithm, and\nthus possesses the monotonicity property. For any sparse estimator, OSS can\nimprove its fit of the model by putting it as an initial point. After this\nimprovement, the sparsity of the estimator is kept. Another appealing feature\nof OSS is that, similarly to an effective algorithm for a continuous\noptimization problem, OSS can converge to the global solution to the \\ell0-norm\nconstrained regression problem if the initial point lies in a neighborhood of\nthe global solution. An accelerating algorithm of OSS and its combination with\nforward stepwise selection are also investigated. Simulations and a real\nexample are presented to evaluate the performances of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 13:13:25 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 07:29:32 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1112.1120", "submitter": "Joan Bruna", "authors": "Joan Bruna and St\\'ephane Mallat", "title": "Classification with Invariant Scattering Representations", "comments": "6 pages, 2 figures; IVMSP Workshop, 2011 IEEE 10th", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scattering transform defines a signal representation which is invariant to\ntranslations and Lipschitz continuous relatively to deformations. It is\nimplemented with a non-linear convolution network that iterates over wavelet\nand modulus operators. Lipschitz continuity locally linearizes deformations.\nComplex classes of signals and textures can be modeled with low-dimensional\naffine spaces, computed with a PCA in the scattering domain. Classification is\nperformed with a penalized model selection. State of the art results are\nobtained for handwritten digit recognition over small training sets, and for\ntexture classification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 23:25:07 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1112.1217", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Christian J. Schuler", "title": "Entropy Search for Information-Efficient Global Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary global optimization algorithms are based on local measures of\nutility, rather than a probability measure over location and value of the\noptimum. They thus attempt to collect low function values, not to learn about\nthe optimum. The reason for the absence of probabilistic global optimizers is\nthat the corresponding inference problem is intractable in several ways. This\npaper develops desiderata for probabilistic optimization algorithms, then\npresents a concrete algorithm which addresses each of the computational\nintractabilities with a sequence of approximations and explicitly adresses the\ndecision problem of maximizing information gain from each evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 10:17:31 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Hennig", "Philipp", ""], ["Schuler", "Christian J.", ""]]}, {"id": "1112.1450", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky, Jorge Silva, Svetlana Lazebnik and Rebecca Willett", "title": "A recursive procedure for density estimation on the binary hypercube", "comments": "revision submitted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a recursive estimation procedure for multivariate binary\ndensities (probability distributions of vectors of Bernoulli random variables)\nusing orthogonal expansions. For $d$ covariates, there are $2^d$ basis\ncoefficients to estimate, which renders conventional approaches computationally\nprohibitive when $d$ is large. However, for a wide class of densities that\nsatisfy a certain sparsity condition, our estimator runs in probabilistic\npolynomial time and adapts to the unknown sparsity of the underlying density in\ntwo key ways: (1) it attains near-minimax mean-squared error for moderate\nsample sizes, and (2) the computational complexity is lower for sparser\ndensities. Our method also allows for flexible control of the trade-off between\nmean-squared error and computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 00:30:17 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2012 01:35:32 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Raginsky", "Maxim", ""], ["Silva", "Jorge", ""], ["Lazebnik", "Svetlana", ""], ["Willett", "Rebecca", ""]]}, {"id": "1112.1556", "submitter": "Sivan Sabato", "authors": "Alon Gonen, Sivan Sabato and Shai Shalev-Shwartz", "title": "Active Learning of Halfspaces under a Margin Assumption", "comments": "A more detailed exposition; Added a description of a simpler\n  implementation and results of experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and analyze a new, efficient, pool-based active learning algorithm\nfor halfspaces, called ALuMA. Most previous algorithms show exponential\nimprovement in the label complexity assuming that the distribution over the\ninstance space is close to uniform. This assumption rarely holds in practical\napplications. Instead, we study the label complexity under a large-margin\nassumption -- a much more realistic condition, as evident by the success of\nmargin-based algorithms such as SVM. Our algorithm is computationally efficient\nand comes with formal guarantees on its label complexity. It also naturally\nextends to the non-separable case and to non-linear kernels. Experiments\nillustrate the clear advantage of ALuMA over other active learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 13:34:25 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2012 11:48:41 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2012 08:07:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gonen", "Alon", ""], ["Sabato", "Sivan", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1112.2093", "submitter": "Peter K\\\"oves\\'arki", "authors": "Peter Kovesarki, Ian C. Brock and A. Elizabeth Nuncio Quiroz", "title": "Green's function based unparameterised multi-dimensional kernel density\n  and likelihood ratio estimator", "comments": "7 pages, 4 figures. JPCS accepted it as a proceedings to the ACAT\n  workshop", "journal-ref": null, "doi": "10.1088/1742-6596/368/1/012041", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a probability density estimator based on Green's\nfunction identities. A density model is constructed under the sole assumption\nthat the probability density is differentiable. The method is implemented as a\nbinary likelihood estimator for classification purposes, so issues such as\nmis-modeling and overtraining are also discussed. The identity behind the\ndensity estimator can be interpreted as a real-valued, non-scalar kernel method\nwhich is able to reconstruct differentiable density functions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 13:16:05 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 11:11:04 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Kovesarki", "Peter", ""], ["Brock", "Ian C.", ""], ["Quiroz", "A. Elizabeth Nuncio", ""]]}, {"id": "1112.2288", "submitter": "Steven Perkins", "authors": "Steven Perkins and David S. Leslie", "title": "Asynchronous Stochastic Approximation with Differential Inclusions", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic pseudo-trajectory approach to stochastic approximation of\nBenaim, Hofbauer and Sorin is extended for asynchronous stochastic\napproximations with a set-valued mean field. The asynchronicity of the process\nis incorporated into the mean field to produce convergence results which remain\nsimilar to those of an equivalent synchronous process. In addition, this allows\nmany of the restrictive assumptions previously associated with asynchronous\nstochastic approximation to be removed. The framework is extended for a coupled\nasynchronous stochastic approximation process with set-valued mean fields.\nTwo-timescales arguments are used here in a similar manner to the original work\nin this area by Borkar. The applicability of this approach is demonstrated\nthrough learning in a Markov decision process.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 16:49:43 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Perkins", "Steven", ""], ["Leslie", "David S.", ""]]}, {"id": "1112.2289", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato and Daniel Hern\\'andez-Lobato", "title": "Convergent Expectation Propagation in Linear Models with Spike-and-slab\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact inference in the linear regression model with spike and slab priors is\noften intractable. Expectation propagation (EP) can be used for approximate\ninference. However, the regular sequential form of EP (R-EP) may fail to\nconverge in this model when the size of the training set is very small. As an\nalternative, we propose a provably convergent EP algorithm (PC-EP). PC-EP is\nproved to minimize an energy function which, under some constraints, is bounded\nfrom below and whose stationary points coincide with the solution of R-EP.\nExperiments with synthetic data indicate that when R-EP does not converge, the\napproximation generated by PC-EP is often better. By contrast, when R-EP\nconverges, both methods perform similarly.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 17:04:01 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""]]}, {"id": "1112.2315", "submitter": "Michalis Smyrnakis", "authors": "Michalis Smyrnakis and David S. Leslie", "title": "Adaptive Forgetting Factor Fictitious Play", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well known that decentralised optimisation can be formulated as a\npotential game, and game-theoretical learning algorithms can be used to find an\noptimum. One of the most common learning techniques in game theory is\nfictitious play. However fictitious play is founded on an implicit assumption\nthat opponents' strategies are stationary. We present a novel variation of\nfictitious play that allows the use of a more realistic model of opponent\nstrategy. It uses a heuristic approach, from the online streaming data\nliterature, to adaptively update the weights assigned to recently observed\nactions. We compare the results of the proposed algorithm with those of\nstochastic and geometric fictitious play in a simple strategic form game, a\nvehicle target assignment game and a disaster management problem. In all the\ntests the rate of convergence of the proposed algorithm was similar or better\nthan the variations of fictitious play we compared it with. The new algorithm\ntherefore improves the performance of game-theoretical learning in\ndecentralised optimisation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 01:52:50 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Smyrnakis", "Michalis", ""], ["Leslie", "David S.", ""]]}, {"id": "1112.2319", "submitter": "Jing Qian", "authors": "Jing Qian, Venkatesh Saligrama, Manqi Zhao", "title": "Graph Construction for Learning with Unbalanced Data", "comments": "14 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbalanced data arises in many learning tasks such as clustering of\nmulti-class data, hierarchical divisive clustering and semisupervised learning.\nGraph-based approaches are popular tools for these problems. Graph construction\nis an important aspect of graph-based learning. We show that graph-based\nalgorithms can fail for unbalanced data for many popular graphs such as k-NN,\n\\epsilon-neighborhood and full-RBF graphs. We propose a novel graph\nconstruction technique that encodes global statistical information into node\ndegrees through a ranking scheme. The rank of a data sample is an estimate of\nits p-value and is proportional to the total number of data samples with\nsmaller density. This ranking scheme serves as a surrogate for density; can be\nreliably estimated; and indicates whether a data sample is close to\nvalleys/modes. This rank-modulated degree(RMD) scheme is able to significantly\nsparsify the graph near valleys and provides an adaptive way to cope with\nunbalanced data. We then theoretically justify our method through limit cut\nanalysis. Unsupervised and semi-supervised experiments on synthetic and real\ndata sets demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 04:25:29 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""], ["Zhao", "Manqi", ""]]}, {"id": "1112.2679", "submitter": "Tong Zhang", "authors": "Xiao-Tong Yuan and Tong Zhang", "title": "Truncated Power Method for Sparse Eigenvalue Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the sparse eigenvalue problem, which is to extract\ndominant (largest) sparse eigenvectors with at most $k$ non-zero components. We\npropose a simple yet effective solution called truncated power method that can\napproximately solve the underlying nonconvex optimization problem. A strong\nsparse recovery result is proved for the truncated power method, and this\ntheory is our key motivation for developing the new algorithm. The proposed\nmethod is tested on applications such as sparse principal component analysis\nand the densest $k$-subgraph problem. Extensive experiments on several\nsynthetic and real-world large scale datasets demonstrate the competitive\nempirical performance of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:11:41 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Zhang", "Tong", ""]]}, {"id": "1112.2738", "submitter": "Jonas Peters", "authors": "Bernhard Sch\\\"olkopf, Dominik Janzing, Jonas Peters, Kun Zhang", "title": "Robust Learning via Cause-Effect Models", "comments": null, "journal-ref": "A version of this paper has been published as \"On Causal and\n  Anticausal Learning\" in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of function estimation in the case where the data\ndistribution may shift between training and test time, and additional\ninformation about it may be available at test time. This relates to popular\nscenarios such as covariate shift, concept drift, transfer learning and\nsemi-supervised learning. This working paper discusses how these tasks could be\ntackled depending on the kind of changes of the distributions. It argues that\nknowledge of an underlying causal direction can facilitate several of these\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 22:33:55 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""], ["Peters", "Jonas", ""], ["Zhang", "Kun", ""]]}, {"id": "1112.2962", "submitter": "Pablo Huijse", "authors": "Pablo Huijse, Pablo A. Est\\'evez, Pablo Zegers, Jos\\'e Pr\\'incipe,\n  Pavlos Protopapas", "title": "Period Estimation in Astronomical Time Series Using Slotted Correntropy", "comments": null, "journal-ref": "IEEE Signal Processing Letters, vol. 18, no. 6, pp. 371-374, year\n  2011", "doi": "10.1109/LSP.2011.2141987", "report-no": null, "categories": "cs.IT astro-ph.IM math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this letter, we propose a method for period estimation in light curves\nfrom periodic variable stars using correntropy. Light curves are astronomical\ntime series of stellar brightness over time, and are characterized as being\nnoisy and unevenly sampled. We propose to use slotted time lags in order to\nestimate correntropy directly from irregularly sampled time series. A new\ninformation theoretic metric is proposed for discriminating among the peaks of\nthe correntropy spectral density. The slotted correntropy method outperformed\nslotted correlation, string length, VarTools (Lomb-Scargle periodogram and\nAnalysis of Variance), and SigSpec applications on a set of light curves drawn\nfrom the MACHO survey.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 17:09:19 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Huijse", "Pablo", ""], ["Est\u00e9vez", "Pablo A.", ""], ["Zegers", "Pablo", ""], ["Pr\u00edncipe", "Jos\u00e9", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1112.3329", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela, Tommi Vatanen, Eric Malmi, Tapani Raiko, Timo Aaltonen\n  and Yoshikazu Nagai", "title": "Semi-Supervised Anomaly Detection - Towards Model-Independent Searches\n  of New Physics", "comments": "Proceedings of ACAT 2011 conference (Uxbridge, UK), 9 pages, 4\n  figures", "journal-ref": null, "doi": "10.1088/1742-6596/368/1/012032", "report-no": null, "categories": "physics.data-an hep-ex stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification algorithms used in high energy physics fall under the\ncategory of supervised machine learning. Such methods require a training set\ncontaining both signal and background events and are prone to classification\nerrors should this training data be systematically inaccurate for example due\nto the assumed MC model. To complement such model-dependent searches, we\npropose an algorithm based on semi-supervised anomaly detection techniques,\nwhich does not require a MC training sample for the signal data. We first model\nthe background using a multivariate Gaussian mixture model. We then search for\ndeviations from this model by fitting to the observations a mixture of the\nbackground model and a number of additional Gaussians. This allows us to\nperform pattern recognition of any anomalous excess over the background. We\nshow by a comparison to neural network classifiers that such an approach is a\nlot more robust against misspecification of the signal MC than supervised\nclassification. In cases where there is an unexpected signal, a neural network\nmight fail to correctly identify it, while anomaly detection does not suffer\nfrom such a limitation. On the other hand, when there are no systematic errors\nin the training data, both methods perform comparably.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 20:32:34 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 12:43:16 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2012 20:45:26 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Kuusela", "Mikael", ""], ["Vatanen", "Tommi", ""], ["Malmi", "Eric", ""], ["Raiko", "Tapani", ""], ["Aaltonen", "Timo", ""], ["Nagai", "Yoshikazu", ""]]}, {"id": "1112.3605", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lauren Hannah and David Dunson and Lawrence Carin", "title": "Beta-Negative Binomial Process and Poisson Factor Analysis", "comments": "Appearing in AISTATS 2012 (submitted on Oct. 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A beta-negative binomial (BNB) process is proposed, leading to a\nbeta-gamma-Poisson process, which may be viewed as a \"multi-scoop\"\ngeneralization of the beta-Bernoulli process. The BNB process is augmented into\na beta-gamma-gamma-Poisson hierarchical structure, and applied as a\nnonparametric Bayesian prior for an infinite Poisson factor analysis model. A\nfinite approximation for the beta process Levy random measure is constructed\nfor convenient implementation. Efficient MCMC computations are performed with\ndata augmentation and marginalization techniques. Encouraging results are shown\non document count matrix factorization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 18:59:45 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 19:02:44 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2012 03:46:24 GMT"}, {"version": "v4", "created": "Sat, 4 Feb 2012 16:43:41 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Hannah", "Lauren", ""], ["Dunson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1112.3699", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Ensemble Models with Trees and Rules", "comments": "The code that was used in this article is available from the user on\n  request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we have proposed several approaches for post processing a\nlarge ensemble of prediction models or rules. The results from our simulations\nshow that the post processing methods we have considered here are promising. We\nhave used the techniques developed here for estimation of quantitative traits\nfrom markers, on the benchmark \"Bostob Housing\"data set and in some\nsimulations. In most cases, the produced models had better prediction\nperformance than, for example, the ones produced by the random forest or the\nrulefit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 01:14:48 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2012 15:35:33 GMT"}, {"version": "v3", "created": "Sat, 7 Jan 2012 02:44:40 GMT"}, {"version": "v4", "created": "Mon, 16 Jan 2012 17:13:39 GMT"}, {"version": "v5", "created": "Thu, 8 Mar 2012 16:54:49 GMT"}, {"version": "v6", "created": "Fri, 9 Mar 2012 20:40:51 GMT"}, {"version": "v7", "created": "Sun, 20 May 2012 15:38:15 GMT"}, {"version": "v8", "created": "Thu, 23 Aug 2012 17:14:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1112.3827", "submitter": "Antoine Salomon", "authors": "Antoine Salomon, Jean-Yves Audibert (INRIA Paris - Rocquencourt),\n  Issam El Alaoui", "title": "Regret lower bounds and extended Upper Confidence Bounds policies in\n  stochastic multi-armed bandit problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to regret lower bounds in the classical model of\nstochastic multi-armed bandit. A well-known result of Lai and Robbins, which\nhas then been extended by Burnetas and Katehakis, has established the presence\nof a logarithmic bound for all consistent policies. We relax the notion of\nconsistence, and exhibit a generalisation of the logarithmic bound. We also\nshow the non existence of logarithmic bound in the general case of Hannan\nconsistency. To get these results, we study variants of popular Upper\nConfidence Bounds (ucb) policies. As a by-product, we prove that it is\nimpossible to design an adaptive policy that would select the best of two\nalgorithms by taking advantage of the properties of the environment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 14:38:04 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Salomon", "Antoine", "", "INRIA Paris - Rocquencourt"], ["Audibert", "Jean-Yves", "", "INRIA Paris - Rocquencourt"], ["Alaoui", "Issam El", ""]]}, {"id": "1112.4258", "submitter": "Mahdi Soltanolkotabi", "authors": "Mahdi Soltanolkotabi, Emmanuel J. Cand\\'es", "title": "A geometric analysis of subspace clustering with outliers", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1034 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2195-2238", "doi": "10.1214/12-AOS1034", "report-no": "IMS-AOS-AOS1034", "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of clustering a collection of unlabeled data\npoints assumed to lie near a union of lower-dimensional planes. As is common in\ncomputer vision or unsupervised learning applications, we do not know in\nadvance how many subspaces there are nor do we have any information about their\ndimensions. We develop a novel geometric analysis of an algorithm named sparse\nsubspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern\nRecognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantly\nbroadens the range of problems where it is provably effective. For instance, we\nshow that SSC can recover multiple subspaces, each of dimension comparable to\nthe ambient dimension. We also prove that SSC can correctly cluster data points\neven when the subspaces of interest intersect. Further, we develop an extension\nof SSC that succeeds when the data set is corrupted with possibly\noverwhelmingly many outliers. Underlying our analysis are clear geometric\ninsights, which may bear on other sparse recovery problems. A numerical study\ncomplements our theoretical analysis and demonstrates the effectiveness of\nthese methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 07:42:21 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 19:01:38 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2012 20:30:16 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2012 14:12:49 GMT"}, {"version": "v5", "created": "Wed, 30 Jan 2013 14:20:53 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""], ["Cand\u00e9s", "Emmanuel J.", ""]]}, {"id": "1112.4321", "submitter": "Mohit Dayal", "authors": "Mohit Dayal", "title": "A New Algorithm for Exploratory Projection Pursuit", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new algorithm for exploratory projection pursuit.\nThe basis of the algorithm is the insight that previous approaches used fairly\nnarrow definitions of interestingness / non interestingness. We argue that\nallowing these definitions to depend on the problem / data at hand is a more\nnatural approach in an exploratory technique. This also allows our technique\nmuch greater applicability than the approaches extant in the literature.\nComplementing this insight, we propose a class of projection indices based on\nthe spatial distribution function that can make use of such information.\n  Finally, with the help of real datasets, we demonstrate how a range of\nmultivariate exploratory tasks can be addressed with our algorithm. The\nexamples further demonstrate that the proposed indices are quite capable of\nfocussing on the interesting structure in the data, even when this structure is\notherwise hard to detect or arises from very subtle patterns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 12:44:03 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Dayal", "Mohit", ""]]}, {"id": "1112.4394", "submitter": "David Duvenaud", "authors": "David Duvenaud, Hannes Nickisch, Carl Edward Rasmussen", "title": "Additive Gaussian Processes", "comments": "Appearing in Neural Information Processing Systems 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce a Gaussian process model of functions which are additive. An\nadditive function is one which decomposes into a sum of low-dimensional\nfunctions, each depending on only a subset of the input variables. Additive GPs\ngeneralize both Generalized Additive Models, and the standard GP models which\nuse squared-exponential kernels. Hyperparameter learning in this model can be\nseen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive\nbut tractable parameterization of the kernel function, which allows efficient\nevaluation of all input interaction terms, whose number is exponential in the\ninput dimension. The additional structure discoverable by this model results in\nincreased interpretability, as well as state-of-the-art predictive power in\nregression tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 16:22:09 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Duvenaud", "David", ""], ["Nickisch", "Hannes", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1112.4463", "submitter": "Boris Defourny", "authors": "Boris Defourny, Damien Ernst and Louis Wehenkel", "title": "Scenario trees and policy selection for multistage stochastic\n  programming using machine learning", "comments": null, "journal-ref": null, "doi": "10.1287/ijoc.1120.0516", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid algorithmic strategy for complex stochastic optimization\nproblems, which combines the use of scenario trees from multistage stochastic\nprogramming with machine learning techniques for learning a policy in the form\nof a statistical model, in the context of constrained vector-valued decisions.\nSuch a policy allows one to run out-of-sample simulations over a large number\nof independent scenarios, and obtain a signal on the quality of the\napproximation scheme used to solve the multistage stochastic program. We\npropose to apply this fast simulation technique to choose the best tree from a\nset of scenario trees. A solution scheme is introduced, where several scenario\ntrees with random branching structure are solved in parallel, and where the\ntree from which the best policy for the true problem could be learned is\nultimately retained. Numerical tests show that excellent trade-offs can be\nachieved between run times and solution quality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 20:47:25 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Defourny", "Boris", ""], ["Ernst", "Damien", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1112.4607", "submitter": "Arash Afkanpour", "authors": "Arash Afkanpour and Csaba Szepesvari and Michael Bowling", "title": "Alignment Based Kernel Learning with a Continuous Set of Base Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of kernel-based learning methods depend on the choice of kernel.\nRecently, kernel learning methods have been proposed that use data to select\nthe most appropriate kernel, usually by combining a set of base kernels. We\nintroduce a new algorithm for kernel learning that combines a {\\em continuous\nset of base kernels}, without the common step of discretizing the space of base\nkernels. We demonstrate that our new method achieves state-of-the-art\nperformance across a variety of real-world datasets. Furthermore, we explicitly\ndemonstrate the importance of combining the right dictionary of kernels, which\nis problematic for methods based on a finite set of base kernels chosen a\npriori. Our method is not the first approach to work with continuously\nparameterized kernels. However, we show that our method requires substantially\nless computation than previous such approaches, and so is more amenable to\nmultiple dimensional parameterizations of base kernels, which we demonstrate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 08:52:56 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["Afkanpour", "Arash", ""], ["Szepesvari", "Csaba", ""], ["Bowling", "Michael", ""]]}, {"id": "1112.4863", "submitter": "Gilad Lerman Dr", "authors": "Teng Zhang and Gilad Lerman", "title": "A Novel M-Estimator for Robust PCA", "comments": null, "journal-ref": "Journal of Machine Learning Research 15 (2014) 749-808", "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the basic problem of robust subspace recovery. That is, we assume a\ndata set that some of its points are sampled around a fixed subspace and the\nrest of them are spread in the whole ambient space, and we aim to recover the\nfixed underlying subspace. We first estimate \"robust inverse sample covariance\"\nby solving a convex minimization procedure; we then recover the subspace by the\nbottom eigenvectors of this matrix (their number correspond to the number of\neigenvalues close to 0). We guarantee exact subspace recovery under some\nconditions on the underlying data. Furthermore, we propose a fast iterative\nalgorithm, which linearly converges to the matrix minimizing the convex\nproblem. We also quantify the effect of noise and regularization and discuss\nmany other practical and theoretical issues for improving the subspace recovery\nin various settings. When replacing the sum of terms in the convex energy\nfunction (that we minimize) with the sum of squares of terms, we obtain that\nthe new minimizer is a scaled version of the inverse sample covariance (when\nexists). We thus interpret our minimizer and its subspace (spanned by its\nbottom eigenvectors) as robust versions of the empirical inverse covariance and\nthe PCA subspace respectively. We compare our method with many other algorithms\nfor robust PCA on synthetic and real data sets and demonstrate state-of-the-art\nspeed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 21:56:34 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 05:15:52 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2013 20:10:08 GMT"}, {"version": "v4", "created": "Mon, 23 Jun 2014 20:30:20 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Teng", ""], ["Lerman", "Gilad", ""]]}, {"id": "1112.5016", "submitter": "Ariel Kleiner", "authors": "Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan", "title": "A Scalable Bootstrap for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap provides a simple and powerful means of assessing the quality\nof estimators. However, in settings involving large datasets---which are\nincreasingly prevalent---the computation of bootstrap-based quantities can be\nprohibitively demanding computationally. While variants such as subsampling and\nthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost of\nbootstrap computations, we find that these methods are generally not robust to\nspecification of hyperparameters (such as the number of subsampled data\npoints), and they often require use of more prior information (such as rates of\nconvergence of estimators) than the bootstrap. As an alternative, we introduce\nthe Bag of Little Bootstraps (BLB), a new procedure which incorporates features\nof both the bootstrap and subsampling to yield a robust, computationally\nefficient means of assessing the quality of estimators. BLB is well suited to\nmodern parallel and distributed computing architectures and furthermore retains\nthe generic applicability and statistical efficiency of the bootstrap. We\ndemonstrate BLB's favorable statistical performance via a theoretical analysis\nelucidating the procedure's properties, as well as a simulation study comparing\nBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In\naddition, we present results from a large-scale distributed implementation of\nBLB demonstrating its computational superiority on massive data, a method for\nadaptively selecting BLB's hyperparameters, an empirical study applying BLB to\nseveral real datasets, and an extension of BLB to time series data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 13:18:57 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 03:30:16 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Kleiner", "Ariel", ""], ["Talwalkar", "Ameet", ""], ["Sarkar", "Purnamrita", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1112.5215", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou and Dacheng Tao", "title": "Bilateral Random Projections", "comments": "17 pages, 3 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Low-rank structure have been profoundly studied in data mining and machine\nlearning. In this paper, we show a dense matrix $X$'s low-rank approximation\ncan be rapidly built from its left and right random projections $Y_1=XA_1$ and\n$Y_2=X^TA_2$, or bilateral random projection (BRP). We then show power scheme\ncan further improve the precision. The deterministic, average and deviation\nbounds of the proposed method and its power scheme modification are proved\ntheoretically. The effectiveness and the efficiency of BRP based low-rank\napproximation is empirically verified on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 01:16:20 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1112.5404", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Prateek Jain", "title": "Similarity-based Learning via Data Driven Embeddings", "comments": "To appear in the proceedings of NIPS 2011, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification using similarity/distance functions\nover data. Specifically, we propose a framework for defining the goodness of a\n(dis)similarity function with respect to a given learning task and propose\nalgorithms that have guaranteed generalization properties when working with\nsuch good functions. Our framework unifies and generalizes the frameworks\nproposed by [Balcan-Blum ICML 2006] and [Wang et al ICML 2007]. An attractive\nfeature of our framework is its adaptability to data - we do not promote a\nfixed notion of goodness but rather let data dictate it. We show, by giving\ntheoretical guarantees that the goodness criterion best suited to a problem can\nitself be learned which makes our approach applicable to a variety of domains\nand problems. We propose a landmarking-based approach to obtaining a classifier\nfrom such learned goodness criteria. We then provide a novel diversity based\nheuristic to perform task-driven selection of landmark points instead of random\nselection. We demonstrate the effectiveness of our goodness criteria learning\nmethod as well as the landmark selection heuristic on a variety of\nsimilarity-based learning datasets and benchmark UCI datasets on which our\nmethod consistently outperforms existing approaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 18:08:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1112.5441", "submitter": "John Snyder", "authors": "John C. Snyder, Matthias Rupp, Katja Hansen, Klaus-Robert M\\\"uller,\n  and Kieron Burke", "title": "Finding Density Functionals with Machine Learning", "comments": "4 pages, 4 figures, 1 table. The Supplemental Material is included at\n  the end of the manuscript (2 pages, 3 tables)", "journal-ref": null, "doi": "10.1103/PhysRevLett.108.253002", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is used to approximate density functionals. For the model\nproblem of the kinetic energy of non-interacting fermions in 1d, mean absolute\nerrors below 1 kcal/mol on test densities similar to the training set are\nreached with fewer than 100 training densities. A predictor identifies if a\ntest density is within the interpolation region. Via principal component\nanalysis, a projected functional derivative finds highly accurate\nself-consistent densities. Challenges for application of our method to real\nelectronic structure problems are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 20:29:32 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Snyder", "John C.", ""], ["Rupp", "Matthias", ""], ["Hansen", "Katja", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1112.5627", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Alessandro Rinaldo, Don Sheehy, Aarti Singh,\n  Larry Wasserman", "title": "Minimax Rates for Homology Inference", "comments": "16 pages, 4 figures. Artificial Intelligence and Statistics, AISTATS\n  2012, Accepted as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, high dimensional data lie close to a low-dimensional submanifold and\nit is of interest to understand the geometry of these submanifolds. The\nhomology groups of a manifold are important topological invariants that provide\nan algebraic summary of the manifold. These groups contain rich topological\ninformation, for instance, about the connected components, holes, tunnels and\nsometimes the dimension of the manifold. In this paper, we consider the\nstatistical problem of estimating the homology of a manifold from noisy samples\nunder several different noise models. We derive upper and lower bounds on the\nminimax risk for this problem. Our upper bounds are based on estimators which\nare constructed from a union of balls of appropriate radius around carefully\nselected points. In each case we establish complementary lower bounds using Le\nCam's lemma.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 18:12:33 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Sheehy", "Don", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1112.5629", "submitter": "Brian Eriksson", "authors": "Brian Eriksson and Laura Balzano and Robert Nowak", "title": "High-Rank Matrix Completion and Subspace Clustering with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of completing a matrix with many missing\nentries under the assumption that the columns of the matrix belong to a union\nof multiple low-rank subspaces. This generalizes the standard low-rank matrix\ncompletion problem to situations in which the matrix rank can be quite high or\neven full rank. Since the columns belong to a union of subspaces, this problem\nmay also be viewed as a missing-data version of the subspace clustering\nproblem. Let X be an n x N matrix whose (complete) columns lie in a union of at\nmost k subspaces, each of rank <= r < n, and assume N >> kn. The main result of\nthe paper shows that under mild assumptions each column of X can be perfectly\nrecovered with high probability from an incomplete version so long as at least\nCrNlog^2(n) entries of X are observed uniformly at random, with C>1 a constant\ndepending on the usual incoherence conditions, the geometrical arrangement of\nsubspaces, and the distribution of columns over the subspaces. The result is\nillustrated with numerical experiments and an application to Internet distance\nmatrix completion and topology identification.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 18:25:17 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 15:22:13 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Eriksson", "Brian", ""], ["Balzano", "Laura", ""], ["Nowak", "Robert", ""]]}, {"id": "1112.5745", "submitter": "Neil Houlsby", "authors": "Neil Houlsby, Ferenc Husz\\'ar, Zoubin Ghahramani, M\\'at\\'e Lengyel", "title": "Bayesian Active Learning for Classification and Preference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic active learning has been widely studied for\nprobabilistic models. For simple regression an optimal myopic policy is easily\ntractable. However, for other tasks and with more complex models, such as\nclassification with nonparametric models, the optimal solution is harder to\ncompute. Current approaches make approximations to achieve tractability. We\npropose an approach that expresses information gain in terms of predictive\nentropies, and apply this method to the Gaussian Process Classifier (GPC). Our\napproach makes minimal approximations to the full information theoretic\nobjective. Our experimental performance compares favourably to many popular\nactive learning algorithms, and has equal or lower computational complexity. We\ncompare well to decision theoretic approaches also, which are privy to more\ninformation and require much more computational time. Secondly, by developing\nfurther a reformulation of binary preference learning to a classification\nproblem, we extend our algorithm to Gaussian Process preference learning.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2011 17:53:19 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Houlsby", "Neil", ""], ["Husz\u00e1r", "Ferenc", ""], ["Ghahramani", "Zoubin", ""], ["Lengyel", "M\u00e1t\u00e9", ""]]}, {"id": "1112.6363", "submitter": "Cun-Hui Zhang", "authors": "Jian Huang and Cun-Hui Zhang", "title": "Estimation And Selection Via Absolute Penalized Convex Minimization And\n  Its Multistage Adaptive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\ell_1$-penalized method, or the Lasso, has emerged as an important tool\nfor the analysis of large data sets. Many important results have been obtained\nfor the Lasso in linear regression which have led to a deeper understanding of\nhigh-dimensional statistical problems. In this article, we consider a class of\nweighted $\\ell_1$-penalized estimators for convex loss functions of a general\nform, including the generalized linear models. We study the estimation,\nprediction, selection and sparsity properties of the weighted\n$\\ell_1$-penalized estimator in sparse, high-dimensional settings where the\nnumber of predictors $p$ can be much larger than the sample size $n$. Adaptive\nLasso is considered as a special case. A multistage method is developed to\napply an adaptive Lasso recursively. We provide $\\ell_q$ oracle inequalities, a\ngeneral selection consistency theorem, and an upper bound on the dimension of\nthe Lasso estimator. Important models including the linear regression, logistic\nregression and log-linear models are used throughout to illustrate the\napplications of the general results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 18:17:35 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Huang", "Jian", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1112.6411", "submitter": "Ali Jalali", "authors": "Christopher C. Johnson, Ali Jalali and Pradeep Ravikumar", "title": "High-dimensional Sparse Inverse Covariance Estimation using Greedy\n  Methods", "comments": "Accepted to AI STAT 2012 for Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the task of estimating the non-zero pattern of the\nsparse inverse covariance matrix of a zero-mean Gaussian random vector from a\nset of iid samples. Note that this is also equivalent to recovering the\nunderlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We\npresent two novel greedy approaches to solving this problem. The first\nestimates the non-zero covariates of the overall inverse covariance matrix\nusing a series of global forward and backward greedy steps. The second\nestimates the neighborhood of each node in the graph separately, again using\ngreedy forward and backward steps, and combines the intermediate neighborhoods\nto form an overall estimate. The principal contribution of this paper is a\nrigorous analysis of the sparsistency, or consistency in recovering the\nsparsity pattern of the inverse covariance matrix. Surprisingly, we show that\nboth the local and global greedy methods learn the full structure of the model\nwith high probability given just $O(d\\log(p))$ samples, which is a\n\\emph{significant} improvement over state of the art $\\ell_1$-regularized\nGaussian MLE (Graphical Lasso) that requires $O(d^2\\log(p))$ samples. Moreover,\nthe restricted eigenvalue and smoothness conditions imposed by our greedy\nmethods are much weaker than the strong irrepresentable conditions required by\nthe $\\ell_1$-regularization based methods. We corroborate our results with\nextensive simulations and examples, comparing our local and global greedy\nmethods to the $\\ell_1$-regularized Gaussian MLE as well as the Neighborhood\nGreedy method to that of nodewise $\\ell_1$-regularized linear regression\n(Neighborhood Lasso).\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 20:35:40 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Johnson", "Christopher C.", ""], ["Jalali", "Ali", ""], ["Ravikumar", "Pradeep", ""]]}]