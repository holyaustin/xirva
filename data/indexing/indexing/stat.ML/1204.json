[{"id": "1204.0033", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Luke K. McDowell, David W. Aha and Jennifer Neville", "title": "Transforming Graph Representations for Statistical Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data representations have become an increasingly important topic\ndue to the recent proliferation of network datasets (e.g., social, biological,\ninformation networks) and a corresponding increase in the application of\nstatistical relational learning (SRL) algorithms to these domains. In this\narticle, we examine a range of representation issues for graph-based relational\ndata. Since the choice of relational data representation for the nodes, links,\nand features can dramatically affect the capabilities of SRL algorithms, we\nsurvey approaches and opportunities for relational representation\ntransformation designed to improve the performance of these algorithms. This\nleads us to introduce an intuitive taxonomy for data representation\ntransformations in relational domains that incorporates link transformation and\nnode transformation as symmetric representation tasks. In particular, the\ntransformation tasks for both nodes and links include (i) predicting their\nexistence, (ii) predicting their label or type, (iii) estimating their weight\nor importance, and (iv) systematically constructing their relevant features. We\nmotivate our taxonomy through detailed examples and use it to survey and\ncompare competing approaches for each of these tasks. We also discuss general\nconditions for transforming links, nodes, and features. Finally, we highlight\nchallenges that remain to be addressed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 21:38:52 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Rossi", "Ryan A.", ""], ["McDowell", "Luke K.", ""], ["Aha", "David W.", ""], ["Neville", "Jennifer", ""]]}, {"id": "1204.0047", "submitter": "Ali Jalali", "authors": "Ali Jalali, Javad Azimi, Xiaoli Fern and Ruofei Zhang", "title": "A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization", "comments": "ECML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimizing unknown costly-to-evaluate functions has been\nstudied for a long time in the context of Bayesian Optimization. Algorithms in\nthis field aim to find the optimizer of the function by asking only a few\nfunction evaluations at locations carefully selected based on a posterior\nmodel. In this paper, we assume the unknown function is Lipschitz continuous.\nLeveraging the Lipschitz property, we propose an algorithm with a distinct\nexploration phase followed by an exploitation phase. The exploration phase aims\nto select samples that shrink the search space as much as possible. The\nexploitation phase then focuses on the reduced search space and selects samples\nclosest to the optimizer. Considering the Expected Improvement (EI) as a\nbaseline, we empirically show that the proposed algorithm significantly\noutperforms EI.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 23:39:29 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 18:03:20 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Jalali", "Ali", ""], ["Azimi", "Javad", ""], ["Fern", "Xiaoli", ""], ["Zhang", "Ruofei", ""]]}, {"id": "1204.0147", "submitter": "Adityanand Guntuboyina", "authors": "Adityanand Guntuboyina and Bodhisattva Sen", "title": "Covering Numbers for Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the covering numbers of the space of convex and\nuniformly bounded functions in multi-dimension. We find optimal upper and lower\nbounds for the $\\epsilon$-covering number of $\\C([a, b]^d, B)$, in the\n$L_p$-metric, $1 \\le p < \\infty$, in terms of the relevant constants, where $d\n\\geq 1$, $a < b \\in \\mathbb{R}$, $B>0$, and $\\C([a,b]^d, B)$ denotes the set of\nall convex functions on $[a, b]^d$ that are uniformly bounded by $B$. We\nsummarize previously known results on covering numbers for convex functions and\nalso provide alternate proofs of some known results. Our results have direct\nimplications in the study of rates of convergence of empirical minimization\nprocedures as well as optimal convergence rates in the numerous convexity\nconstrained function estimation problems.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 23:25:53 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1204.0563", "submitter": "Boumediene Hamzi", "authors": "Jake Bouvrie and Boumediene Hamzi", "title": "Kernel Methods for the Approximation of Some Key Quantities of Nonlinear\n  Systems", "comments": "An abbreviated version of this report will appear in Proc. American\n  Control Conference (ACC), Montreal, Canada, 2012. The paper has been\n  rewritten to improve readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-based approach to estimating key quantities which arise\nin the study of nonlinear control systems and random nonlinear dynamical\nsystems. Our approach hinges on the observation that much of the existing\nlinear theory may be readily extended to nonlinear systems - with a reasonable\nexpectation of success - once the nonlinear system has been mapped into a high\nor infinite dimensional feature space. In particular, we develop computable,\nnon-parametric estimators approximating controllability and observability\nenergy functions for nonlinear systems, and study the ellipsoids they induce.\nIn all cases the relevant quantities are estimated from simulated or observed\ndata. It is then shown that the controllability energy estimator provides a key\nmeans for approximating the invariant measure of an ergodic, stochastically\nforced nonlinear system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 00:21:14 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 21:00:24 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Bouvrie", "Jake", ""], ["Hamzi", "Boumediene", ""]]}, {"id": "1204.0585", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis and Alfred O. Hero III and Shuheng Zhou", "title": "Convergence Properties of Kronecker Graphical Lasso Algorithms", "comments": "47 pages, accepted to IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 61, Issue 7, pp.\n  1743-1755, April 2013", "doi": "10.1109/TSP.2013.2240157", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies iteration convergence of Kronecker graphical lasso\n(KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian random\nsample under a sparse Kronecker-product covariance model and MSE convergence\nrates. The KGlasso model, originally called the transposable regularized\ncovariance model by Allen [\"Transposable regularized covariance models with an\napplication to missing data imputation,\" Ann. Appl. Statist., vol. 4, no. 2,\npp. 764-790, 2010], implements a pair of $\\ell_1$ penalties on each Kronecker\nfactor to enforce sparsity in the covariance estimator. The KGlasso algorithm\ngeneralizes Glasso, introduced by Yuan and Lin [\"Model selection and estimation\nin the Gaussian graphical model,\" Biometrika, vol. 94, pp. 19-35, 2007] and\nBanerjee [\"Model selection through sparse maximum likelihood estimation for\nmultivariate Gaussian or binary data,\" J. Mach. Learn. Res., vol. 9, pp.\n485-516, Mar. 2008], to estimate covariances having Kronecker product form. It\nalso generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul [\"The\nMLE algorithm for the matrix normal distribution,\" J. Statist. Comput. Simul.,\nvol. 64, pp. 105-123, 1999] and Werner [\"On estimation of covariance matrices\nwith Kronecker product structure,\" IEEE Trans. Signal Process., vol. 56, no. 2,\npp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establish\nthat the KGlasso iterates converge pointwise to a local maximum of the\npenalized likelihood function. We derive high dimensional rates of convergence\nto the true covariance as both the number of samples and the number of\nvariables go to infinity. Our results establish that KGlasso has significantly\nfaster asymptotic convergence than Glasso and FF. Simulations are presented\nthat validate the results of our analysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 03:28:10 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2012 06:37:59 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 22:16:48 GMT"}, {"version": "v4", "created": "Fri, 1 Nov 2013 17:46:31 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O.", "III"], ["Zhou", "Shuheng", ""]]}, {"id": "1204.0656", "submitter": "Niels Lovmand  Pedersen", "authors": "Niels Lovmand Pedersen and Carles Navarro Manch\\'on and Dmitriy Shutin\n  and Bernard Henri Fleury", "title": "Application of Bayesian Hierarchical Prior Modeling to Sparse Channel\n  Estimation", "comments": "accepted for publication in Proc. IEEE Int Communications (ICC) Conf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for sparse channel estimation typically provide an estimate\ncomputed as the solution maximizing an objective function defined as the sum of\nthe log-likelihood function and a penalization term proportional to the l1-norm\nof the parameter of interest. However, other penalization terms have proven to\nhave strong sparsity-inducing properties. In this work, we design\npilot-assisted channel estimators for OFDM wireless receivers within the\nframework of sparse Bayesian learning by defining hierarchical Bayesian prior\nmodels that lead to sparsity-inducing penalization terms. The estimators result\nas an application of the variational message-passing algorithm on the factor\ngraph representing the signal model extended with the hierarchical prior\nmodels. Numerical results demonstrate the superior performance of our channel\nestimators as compared to traditional and state-of-the-art sparse methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 11:12:52 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Pedersen", "Niels Lovmand", ""], ["Manch\u00f3n", "Carles Navarro", ""], ["Shutin", "Dmitriy", ""], ["Fleury", "Bernard Henri", ""]]}, {"id": "1204.0684", "submitter": "Matthias Scholz", "authors": "Matthias Scholz", "title": "Validation of nonlinear PCA", "comments": "12 pages, 5 figures", "journal-ref": "Neural Processing Letters, 2012", "doi": "10.1007/s11063-012-9220-6", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear principal component analysis (PCA) can be extended to a nonlinear PCA\nby using artificial neural networks. But the benefit of curved components\nrequires a careful control of the model complexity. Moreover, standard\ntechniques for model selection, including cross-validation and more generally\nthe use of an independent test set, fail when applied to nonlinear PCA because\nof its inherent unsupervised characteristics. This paper presents a new\napproach for validating the complexity of nonlinear PCA models by using the\nerror in missing data estimation as a criterion for model selection. It is\nmotivated by the idea that only the model of optimal complexity is able to\npredict missing values with the highest accuracy. While standard test set\nvalidation usually favours over-fitted nonlinear PCA models, the proposed model\nvalidation approach correctly selects the optimal model complexity.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 13:22:07 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Scholz", "Matthias", ""]]}, {"id": "1204.0870", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Ohad Shamir, Karthik Sridharan", "title": "Relax and Localize: From Value to Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a principled way of deriving online learning algorithms from a\nminimax analysis. Various upper bounds on the minimax value, previously thought\nto be non-constructive, are shown to yield algorithms. This allows us to\nseamlessly recover known methods and to derive new ones. Our framework also\ncaptures such \"unorthodox\" methods as Follow the Perturbed Leader and the R^2\nforecaster. We emphasize that understanding the inherent complexity of the\nlearning problem leads to the development of algorithms.\n  We define local sequential Rademacher complexities and associated algorithms\nthat allow us to obtain faster rates in online learning, similarly to\nstatistical learning theory. Based on these localized complexities we build a\ngeneral adaptive method that can take advantage of the suboptimality of the\nobserved sequence.\n  We present a number of new algorithms, including a family of randomized\nmethods that use the idea of a \"random playout\". Several new versions of the\nFollow-the-Perturbed-Leader algorithms are presented, as well as methods based\non the Littlestone's dimension, efficient methods for matrix completion with\ntrace norm, and algorithms for the problems of transductive learning and\nprediction with static experts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 05:49:56 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1204.0991", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos and Georgios B. Giannakis", "title": "Distributed Robust Power System State Estimation", "comments": "Revised submission to IEEE Trans. on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deregulation of energy markets, penetration of renewables, advanced metering\ncapabilities, and the urge for situational awareness, all call for system-wide\npower system state estimation (PSSE). Implementing a centralized estimator\nthough is practically infeasible due to the complexity scale of an\ninterconnection, the communication bottleneck in real-time monitoring, regional\ndisclosure policies, and reliability issues. In this context, distributed PSSE\nmethods are treated here under a unified and systematic framework. A novel\nalgorithm is developed based on the alternating direction method of\nmultipliers. It leverages existing PSSE solvers, respects privacy policies,\nexhibits low communication load, and its convergence to the centralized\nestimates is guaranteed even in the absence of local observability. Beyond the\nconventional least-squares based PSSE, the decentralized framework accommodates\na robust state estimator. By exploiting interesting links to the compressive\nsampling advances, the latter jointly estimates the state and identifies\ncorrupted measurements. The novel algorithms are numerically evaluated using\nthe IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate that\nthe attainable accuracy can be reached within a few inter-area exchanges, while\nlargest residual tests are outperformed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 16:39:18 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 19:17:31 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1204.1276", "submitter": "Sivan Sabato", "authors": "Sivan Sabato, Nathan Srebro and Naftali Tishby", "title": "Distribution-Dependent Sample Complexity of Large Margin Learning", "comments": "arXiv admin note: text overlap with arXiv:1011.5053", "journal-ref": "S. Sabato, N. Srebro and N. Tishby, \"Distribution-Dependent Sample\n  Complexity of Large Margin Learning\", Journal of Machine Learning Research,\n  14(Jul):2119-2149, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a tight distribution-specific characterization of the sample\ncomplexity of large-margin classification with L2 regularization: We introduce\nthe margin-adapted dimension, which is a simple function of the second order\nstatistics of the data distribution, and show distribution-specific upper and\nlower bounds on the sample complexity, both governed by the margin-adapted\ndimension of the data distribution. The upper bounds are universal, and the\nlower bounds hold for the rich family of sub-Gaussian distributions with\nindependent features. We conclude that this new quantity tightly characterizes\nthe true sample complexity of large-margin classification. To prove the lower\nbound, we develop several new tools of independent interest. These include new\nconnections between shattering and hardness of learning, new properties of\nshattering with linear classifiers, and a new lower bound on the smallest\neigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our\nresults can be used to quantitatively compare large margin learning to other\nlearning rules, and to improve the effectiveness of methods that use sample\ncomplexity bounds, such as active learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 16:59:29 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2012 14:00:26 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2013 15:58:47 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2013 13:17:05 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Sabato", "Sivan", ""], ["Srebro", "Nathan", ""], ["Tishby", "Naftali", ""]]}, {"id": "1204.1437", "submitter": "Suvrit Sra", "authors": "Suvrit Sra", "title": "Fast projections onto mixed-norm balls with applications", "comments": "Preprint of paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint sparsity offers powerful structural cues for feature selection,\nespecially for variables that are expected to demonstrate a \"grouped\" behavior.\nSuch behavior is commonly modeled via group-lasso, multitask lasso, and related\nmethods where feature selection is effected via mixed-norms. Several mixed-norm\nbased sparse models have received substantial attention, and for some cases\nefficient algorithms are also available. Surprisingly, several constrained\nsparse models seem to be lacking scalable algorithms. We address this\ndeficiency by presenting batch and online (stochastic-gradient) optimization\nmethods, both of which rely on efficient projections onto mixed-norm balls. We\nillustrate our methods by applying them to the multitask lasso. We conclude by\nmentioning some open problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 08:55:38 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Sra", "Suvrit", ""]]}, {"id": "1204.1624", "submitter": "Wassim Jouini", "authors": "Wassim Jouini and Christophe Moy", "title": "UCB Algorithm for Exponential Distributions", "comments": "10 pages. Introduces Multiplicative Upper Confidence Bound (MUCB)\n  algorithms for Multi-Armed Bandit problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)\nproblems. A machine learning paradigm popular within Cognitive Network related\ntopics (e.g., Spectrum Sensing and Allocation). We focus on the case where the\nrewards are exponentially distributed, which is common when dealing with\nRayleigh fading channels. This strategy, named Multiplicative Upper Confidence\nBound (MUCB), associates a utility index to every available arm, and then\nselects the arm with the highest index. For every arm, the associated index is\nequal to the product of a multiplicative factor by the sample mean of the\nrewards collected by this arm. We show that the MUCB policy has a low\ncomplexity and is order optimal.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 12:17:03 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Jouini", "Wassim", ""], ["Moy", "Christophe", ""]]}, {"id": "1204.1664", "submitter": "David Duvenaud", "authors": "Ferenc Husz\\'ar and David Duvenaud", "title": "Optimally-Weighted Herding is Bayesian Quadrature", "comments": "Accepted as an oral presentation at Uncertainty in Artificial\n  Intelligence 2012. Updated to fix several typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herding and kernel herding are deterministic methods of choosing samples\nwhich summarise a probability distribution. A related task is choosing samples\nfor estimating integrals using Bayesian quadrature. We show that the criterion\nminimised when selecting samples in kernel herding is equivalent to the\nposterior variance in Bayesian quadrature. We then show that sequential\nBayesian quadrature can be viewed as a weighted version of kernel herding which\nachieves performance superior to any other weighted herding method. We\ndemonstrate empirically a rate of convergence faster than O(1/N). Our results\nalso imply an upper bound on the empirical error of the Bayesian quadrature\nestimate.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 17:54:07 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 16:13:31 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 18:39:09 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""], ["Duvenaud", "David", ""]]}, {"id": "1204.1681", "submitter": "Mohamed Ali Mahjoub", "authors": "Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub", "title": "The threshold EM algorithm for parameter learning in bayesian network\n  with incomplete data", "comments": "6 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 2, No. 7, pp 86-90, July 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks (BN) are used in a big range of applications but they have\none issue concerning parameter learning. In real application, training data are\nalways incomplete or some nodes are hidden. To deal with this problem many\nlearning parameter algorithms are suggested foreground EM, Gibbs sampling and\nRBE algorithms. In order to limit the search space and escape from local maxima\nproduced by executing EM algorithm, this paper presents a learning parameter\nalgorithm that is a fusion of EM and RBE algorithms. This algorithm\nincorporates the range of a parameter into the EM algorithm. This range is\ncalculated by the first step of RBE algorithm allowing a regularization of each\nparameter in bayesian network after the maximization step of the EM algorithm.\nThe threshold EM algorithm is applied in brain tumor diagnosis and show some\nadvantages and disadvantages over the EM algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 21:09:48 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Lamine", "Fradj Ben", ""], ["Kalti", "Karim", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1204.1685", "submitter": "Martin Azizyan", "authors": "Martin Azizyan, Aarti Singh, Larry Wasserman", "title": "Density-sensitive semisupervised inference", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1092 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 751-771", "doi": "10.1214/13-AOS1092", "report-no": "IMS-AOS-AOS1092", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semisupervised methods are techniques for using labeled data\n$(X_1,Y_1),\\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\\ldots,X_N$\nto make predictions. These methods invoke some assumptions that link the\nmarginal distribution $P_X$ of X to the regression function f(x). For example,\nit is common to assume that f is very smooth over high density regions of\n$P_X$. Many of the methods are ad-hoc and have been shown to work in specific\nexamples but are lacking a theoretical foundation. We provide a minimax\nframework for analyzing semisupervised methods. In particular, we study methods\nbased on metrics that are sensitive to the distribution $P_X$. Our model\nincludes a parameter $\\alpha$ that controls the strength of the semisupervised\nassumption. We then use the data to adapt to $\\alpha$.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 21:49:22 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 13:14:50 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1204.1688", "submitter": "John C. Duchi", "authors": "John C. Duchi, Lester Mackey, Michael I. Jordan", "title": "The asymptotics of ranking algorithms", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1142 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2292-2323", "doi": "10.1214/13-AOS1142", "report-no": "IMS-AOS-AOS1142", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the predictive problem of supervised ranking, where the task is\nto rank sets of candidate items returned in response to queries. Although there\nexist statistical procedures that come with guarantees of consistency in this\nsetting, these procedures require that individuals provide a complete ranking\nof all items, which is rarely feasible in practice. Instead, individuals\nroutinely provide partial preference information, such as pairwise comparisons\nof items, and more practical approaches to ranking have aimed at modeling this\npartial preference data directly. As we show, however, such an approach raises\nserious theoretical challenges. Indeed, we demonstrate that many commonly used\nsurrogate losses for pairwise comparison data do not yield consistency;\nsurprisingly, we show inconsistency even in low-noise settings. With these\nnegative results as motivation, we present a new approach to supervised ranking\nbased on aggregation of partial preferences, and we develop $U$-statistic-based\nempirical risk minimization procedures. We present an asymptotic analysis of\nthese new procedures, showing that they yield consistency results that parallel\nthose available for classification. We complement our theoretical results with\nan experiment studying the new procedures in a large-scale web-ranking task.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 22:33:22 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2013 18:04:41 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 09:25:03 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Duchi", "John C.", ""], ["Mackey", "Lester", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1204.1795", "submitter": "Shohei Shimizu", "authors": "Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio", "title": "Estimation of causal orders in a linear non-Gaussian acyclic model: a\n  method robust against latent confounders", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider to learn a causal ordering of variables in a linear non-Gaussian\nacyclic model called LiNGAM. Several existing methods have been shown to\nconsistently estimate a causal ordering assuming that all the model assumptions\nare correct. But, the estimation results could be distorted if some assumptions\nactually are violated. In this paper, we propose a new algorithm for learning\ncausal orders that is robust against one typical violation of the model\nassumptions: latent confounders. We demonstrate the effectiveness of our method\nusing artificial data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 05:29:07 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Tashiro", "Tatsuya", ""], ["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Washio", "Takashi", ""]]}, {"id": "1204.1800", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar and Ambedkar Dukkipati", "title": "On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and\n  Applications", "comments": "7 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of kernels is central to machine learning. Motivated by the\nimportance of power-law distributions in statistical modeling, in this paper,\nwe propose the notion of power-law kernels to investigate power-laws in\nlearning problem. We propose two power-law kernels by generalizing Gaussian and\nLaplacian kernels. This generalization is based on distributions, arising out\nof maximization of a generalized information measure known as nonextensive\nentropy that is very well studied in statistical mechanics. We prove that the\nproposed kernels are positive definite, and provide some insights regarding the\ncorresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical\nsignificance of both kernels in classification and regression, and present some\nsimulation results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 05:53:27 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2013 07:12:43 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1204.1992", "submitter": "Bin Nan Dr", "authors": "Shengchun Kong and Bin Nan", "title": "Non-asymptotic Oracle Inequalities for the High-Dimensional Cox\n  Regression via Lasso", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the finite sample properties of the regularized high-dimensional\nCox regression via lasso. Existing literature focuses on linear models or\ngeneralized linear models with Lipschitz loss functions, where the empirical\nrisk functions are the summations of independent and identically distributed\n(iid) losses. The summands in the negative log partial likelihood function for\ncensored survival data, however, are neither iid nor Lipschitz. We first\napproximate the negative log partial likelihood function by a sum of iid\nnon-Lipschitz terms, then derive the non-asymptotic oracle inequalities for the\nlasso penalized Cox regression using pointwise arguments to tackle the\ndifficulty caused by the lack of iid and Lipschitz property.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 21:16:17 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Kong", "Shengchun", ""], ["Nan", "Bin", ""]]}, {"id": "1204.2003", "submitter": "Christopher Quinn", "authors": "Christopher J. Quinn, Negar Kiyavash, and Todd P. Coleman", "title": "Directed Information Graphs", "comments": "41 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graphical model for representing networks of stochastic\nprocesses, the minimal generative model graph. It is based on reduced\nfactorizations of the joint distribution over time. We show that under\nappropriate conditions, it is unique and consistent with another type of\ngraphical model, the directed information graph, which is based on a\ngeneralization of Granger causality. We demonstrate how directed information\nquantifies Granger causality in a particular sequential prediction setting. We\nalso develop efficient methods to estimate the topological structure from data\nthat obviate estimating the joint statistics. One algorithm assumes\nupper-bounds on the degrees and uses the minimal dimension statistics\nnecessary. In the event that the upper-bounds are not valid, the resulting\ngraph is nonetheless an optimal approximation. Another algorithm uses\nnear-minimal dimension statistics when no bounds are known but the distribution\nsatisfies a certain criterion. Analogous to how structure learning algorithms\nfor undirected graphical models use mutual information estimates, these\nalgorithms use directed information estimates. We characterize the\nsample-complexity of two plug-in directed information estimators and obtain\nconfidence intervals. For the setting when point estimates are unreliable, we\npropose an algorithm that uses confidence intervals to identify the best\napproximation that is robust to estimation error. Lastly, we demonstrate the\neffectiveness of the proposed algorithms through analysis of both synthetic\ndata and real data from the Twitter network. In the latter case, we identify\nwhich news sources influence users in the network by merely analyzing tweet\ntimes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 22:54:59 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 21:44:23 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Quinn", "Christopher J.", ""], ["Kiyavash", "Negar", ""], ["Coleman", "Todd P.", ""]]}, {"id": "1204.2049", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Guang Dai and Michael I. Jordan", "title": "Coherence Functions with Applications in Large-Margin Classification\n  Methods", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) naturally embody sparseness due to their use\nof hinge loss functions. However, SVMs can not directly estimate conditional\nclass probabilities. In this paper we propose and study a family of coherence\nfunctions, which are convex and differentiable, as surrogates of the hinge\nfunction. The coherence function is derived by using the maximum-entropy\nprinciple and is characterized by a temperature parameter. It bridges the hinge\nfunction and the logit function in logistic regression. The limit of the\ncoherence function at zero temperature corresponds to the hinge function, and\nthe limit of the minimizer of its expected error is the minimizer of the\nexpected error of the hinge loss. We refer to the use of the coherence function\nin large-margin classification as C-learning, and we present efficient\ncoordinate descent algorithms for the training of regularized ${\\cal\nC}$-learning models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 05:38:58 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Zhang", "Zhihua", ""], ["Dai", "Guang", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1204.2069", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Asymptotic Accuracy of Distribution-Based Estimation for Latent\n  Variables", "comments": "25pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical statistical models are widely employed in information science\nand data engineering. The models consist of two types of variables: observable\nvariables that represent the given data and latent variables for the\nunobservable labels. An asymptotic analysis of the models plays an important\nrole in evaluating the learning process; the result of the analysis is applied\nnot only to theoretical but also to practical situations, such as optimal model\nselection and active learning. There are many studies of generalization errors,\nwhich measure the prediction accuracy of the observable variables. However, the\naccuracy of estimating the latent variables has not yet been elucidated. For a\nquantitative evaluation of this, the present paper formulates\ndistribution-based functions for the errors in the estimation of the latent\nvariables. The asymptotic behavior is analyzed for both the maximum likelihood\nand the Bayes methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:50:07 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 02:10:58 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2012 07:27:52 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2014 00:46:44 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1204.2296", "submitter": "Karl Rohe", "authors": "Karl Rohe and Tai Qin and Bin Yu", "title": "Co-clustering for directed graphs: the Stochastic co-Blockmodel and\n  spectral algorithm Di-Sim", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphs have asymmetric connections, yet the current graph clustering\nmethodologies cannot identify the potentially global structure of these\nasymmetries. We give a spectral algorithm called di-sim that builds on a dual\nmeasure of similarity that correspond to how a node (i) sends and (ii) receives\nedges. Using di-sim, we analyze the global asymmetries in the networks of Enron\nemails, political blogs, and the c elegans neural connectome. In each example,\na small subset of nodes have persistent asymmetries; these nodes send edges\nwith one cluster, but receive edges with another cluster. Previous approaches\nwould have assigned these asymmetric nodes to only one cluster, failing to\nidentify their sending/receiving asymmetries. Regularization and \"projection\"\nare two steps of di-sim that are essential for spectral clustering algorithms\nto work in practice. The theoretical results show that these steps make the\nalgorithm weakly consistent under the degree corrected Stochastic\nco-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow for\nboth (i) degree heterogeneity and (ii) the global asymmetries that we intend to\ndetect. The theoretical results make no assumptions on the smallest degree\nnodes. Instead, the theorem requires that the average degree grows sufficiently\nfast and that the weak consistency only applies to the subset of the nodes with\nsufficiently large leverage scores. The results results also apply to bipartite\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 22:30:16 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 20:35:40 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Rohe", "Karl", ""], ["Qin", "Tai", ""], ["Yu", "Bin", ""]]}, {"id": "1204.2311", "submitter": "Bin Shen", "authors": "Bin Shen, Luo Si, Rongrong Ji, Baodi Liu", "title": "Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) is a widely used technique in many\napplications such as face recognition, motion segmentation, etc. It\napproximates the nonnegative data in an original high dimensional space with a\nlinear representation in a low dimensional space by using the product of two\nnonnegative matrices. In many applications data are often partially corrupted\nwith large additive noise. When the positions of noise are known, some existing\nvariants of NMF can be applied by treating these corrupted entries as missing\nvalues. However, the positions are often unknown in many real world\napplications, which prevents the usage of traditional NMF or other existing\nvariants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization\n(RobustNMF) algorithm that explicitly models the partial corruption as large\nadditive noise without requiring the information of positions of noise. In\npractice, large additive noise can be used to model outliers. In particular,\nthe proposed method jointly approximates the clean data matrix with the product\nof two nonnegative matrices and estimates the positions and values of\noutliers/noise. An efficient iterative optimization algorithm with a solid\ntheoretical justification has been proposed to learn the desired matrix\nfactorization. Experimental results demonstrate the advantages of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 01:03:03 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Shen", "Bin", ""], ["Si", "Luo", ""], ["Ji", "Rongrong", ""], ["Liu", "Baodi", ""]]}, {"id": "1204.2353", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard\n  Thresholding", "comments": "variable selection, pseudo-hard thresholding", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in linear models plays a pivotal role in modern\nstatistics. Hard-thresholding methods such as $l_0$ regularization are\ntheoretically ideal but computationally infeasible. In this paper, we propose a\nnew approach, called the LAGS, short for \"least absulute gradient selector\", to\nthis challenging yet interesting problem by mimicking the discrete selection\nprocess of $l_0$ regularization. To estimate $\\beta$ under the influence of\nnoise, we consider, nevertheless, the following convex program [\\hat{\\beta} =\n\\textrm{arg min}\\frac{1}{n}\\|X^{T}(y - X\\beta)\\|_1 + \\lambda_n\\sum_{i =\n1}^pw_i(y;X;n)|\\beta_i|]\n  $\\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and\n$n$ is the weights on different $\\beta_i$; $n$ is the sample size.\nSurprisingly, we shall show in the paper, both geometrically and analytically,\nthat LAGS enjoys two attractive properties: (1) LAGS demonstrates discrete\nselection behavior and hard thresholding property as $l_0$ regularization by\nstrategically chosen $w_i$, we call this property \"pseudo-hard thresholding\";\n(2) Asymptotically, LAGS is consistent and capable of discovering the true\nmodel; nonasymptotically, LAGS is capable of identifying the sparsity in the\nmodel and the prediction error of the coefficients is bounded at the noise\nlevel up to a logarithmic factor---$\\log p$, where $p$ is the number of\npredictors.\n  Computationally, LAGS can be solved efficiently by convex program routines\nfor its convexity or by simplex algorithm after recasting it into a linear\nprogram. The numeric simulation shows that LAGS is superior compared to\nsoft-thresholding methods in terms of mean squared error and parsimony of the\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 06:57:39 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 05:28:28 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2012 23:52:09 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2012 03:56:01 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1204.2436", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Sparse and Unique Nonnegative Matrix Factorization Through Data\n  Preprocessing", "comments": "34 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research 13 (Nov), pp. 3349-3386, 2012", "doi": null, "report-no": null, "categories": "stat.ML math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has become a very popular technique in\nmachine learning because it automatically extracts meaningful features through\na sparse and part-based representation. However, NMF has the drawback of being\nhighly ill-posed, that is, there typically exist many different but equivalent\nfactorizations. In this paper, we introduce a completely new way to obtaining\nmore well-posed NMF problems whose solutions are sparser. Our technique is\nbased on the preprocessing of the nonnegative input data matrix, and relies on\nthe theory of M-matrices and the geometric interpretation of NMF. This approach\nprovably leads to optimal and sparse solutions under the separability\nassumption of Donoho and Stodden (NIPS, 2003), and, for rank-three matrices,\nmakes the number of exact factorizations finite. We illustrate the\neffectiveness of our technique on several image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 13:03:26 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1204.2477", "submitter": "Matthew Johnson", "authors": "Matthew James Johnson", "title": "A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple linear algebraic explanation of the algorithm in \"A Spectral\nAlgorithm for Learning Hidden Markov Models\" (COLT 2009). Most of the content\nis in Figure 2; the text just makes everything precise in four nearly-trivial\nclaims.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 15:35:43 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Johnson", "Matthew James", ""]]}, {"id": "1204.2523", "submitter": "Khalid El-Arini", "authors": "Khalid El-Arini, Emily B. Fox, Carlos Guestrin", "title": "Concept Modeling with Superwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information retrieval, a fundamental goal is to transform a document into\nconcepts that are representative of its content. The term \"representative\" is\nin itself challenging to define, and various tasks require different\ngranularities of concepts. In this paper, we aim to model concepts that are\nsparse over the vocabulary, and that flexibly adapt their content based on\nother relevant semantic information such as textual structure or associated\nimage features. We explore a Bayesian nonparametric model based on nested beta\nprocesses that allows for inferring an unknown number of strictly sparse\nconcepts. The resulting model provides an inherently different representation\nof concepts than a standard LDA (or HDP) based topic model, and allows for\ndirect incorporation of semantic features. We demonstrate the utility of this\nrepresentation on multilingual blog data and the Congressional Record.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 18:53:58 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["El-Arini", "Khalid", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1204.2581", "submitter": "Sheng Gao", "authors": "Sheng Gao and Ludovic Denoyer and Patrick Gallinari", "title": "Modeling Relational Data via Latent Factor Blockmodel", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of modeling relational data, which\nappear in many applications such as social network analysis, recommender\nsystems and bioinformatics. Previous studies either consider latent feature\nbased models but disregarding local structure in the network, or focus\nexclusively on capturing local structure of objects based on latent blockmodels\nwithout coupling with latent characteristics of objects. To combine the\nbenefits of the previous work, we propose a novel model that can simultaneously\nincorporate the effect of latent features and covariates if any, as well as the\neffect of latent structure that may exist in the data. To achieve this, we\nmodel the relation graph as a function of both latent feature factors and\nlatent cluster memberships of objects to collectively discover globally\npredictive intrinsic properties of objects and capture latent block structure\nin the network to improve prediction performance. We also develop an\noptimization transfer algorithm based on the generalized EM-style strategy to\nlearn the latent factors. We prove the efficacy of our proposed model through\nthe link prediction task and cluster analysis task, and extensive experiments\non the synthetic data and several real world datasets suggest that our proposed\nLFBM model outperforms the other state of the art approaches in the evaluated\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 22:14:05 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gao", "Sheng", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1204.2588", "submitter": "Sheng Gao", "authors": "Sheng Gao and Ludovic Denoyer and Patrick Gallinari", "title": "Probabilistic Latent Tensor Factorization Model for Link Pattern\n  Prediction in Multi-relational Networks", "comments": "19pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at the problem of link pattern prediction in collections of\nobjects connected by multiple relation types, where each type may play a\ndistinct role. While common link analysis models are limited to single-type\nlink prediction, we attempt here to capture the correlations among different\nrelation types and reveal the impact of various relation types on performance\nquality. For that, we define the overall relations between object pairs as a\n\\textit{link pattern} which consists in interaction pattern and connection\nstructure in the network, and then use tensor formalization to jointly model\nand predict the link patterns, which we refer to as \\textit{Link Pattern\nPrediction} (LPP) problem. To address the issue, we propose a Probabilistic\nLatent Tensor Factorization (PLTF) model by introducing another latent factor\nfor multiple relation types and furnish the Hierarchical Bayesian treatment of\nthe proposed probabilistic model to avoid overfitting for solving the LPP\nproblem. To learn the proposed model we develop an efficient Markov Chain Monte\nCarlo sampling method. Extensive experiments are conducted on several real\nworld datasets and demonstrate significant improvements over several existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 22:58:46 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gao", "Sheng", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1204.3005", "submitter": "Wassim Jouini", "authors": "Wassim Jouini and Marco Di Felice and Luciano Bononi and Christophe\n  Moy", "title": "Collaboration and Coordination in Secondary Networks for Opportunistic\n  Spectrum Access", "comments": "28 pages. Paper submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we address the general case of a coordinated secondary network\nwilling to exploit communication opportunities left vacant by a licensed\nprimary network. Since secondary users (SU) usually have no prior knowledge on\nthe environment, they need to learn the availability of each channel through\nsensing techniques, which however can be prone to detection errors. We argue\nthat cooperation among secondary users can enable efficient learning and\ncoordination mechanisms in order to maximize the spectrum exploitation by SUs,\nwhile minimizing the impact on the primary network. To this goal, we provide\nthree novel contributions in this paper. First, we formulate the spectrum\nselection in secondary networks as an instance of the Multi-Armed Bandit (MAB)\nproblem, and we extend the analysis to the collaboration learning case, in\nwhich each SU learns the spectrum occupation, and shares this information with\nother SUs. We show that collaboration among SUs can mitigate the impact of\nsensing errors on system performance, and improve the convergence of the\nlearning process to the optimal solution. Second, we integrate the learning\nalgorithms with two collaboration techniques based on modified versions of the\nHungarian algorithm and of the Round Robin algorithm that allows reducing the\ninterference among SUs. Third, we derive fundamental limits to the performance\nof cooperative learning algorithms based on Upper Confidence Bound (UCB)\npolicies in a symmetric scenario where all SU have the same perception of the\nquality of the resources. Extensive simulation results confirm the\neffectiveness of our joint learning-collaboration algorithm in protecting the\noperations of Primary Users (PUs), while maximizing the performance of SUs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 14:15:53 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Jouini", "Wassim", ""], ["Di Felice", "Marco", ""], ["Bononi", "Luciano", ""], ["Moy", "Christophe", ""]]}, {"id": "1204.3523", "submitter": "Avishek Saha", "authors": "Hal Daume III, Jeff M. Phillips, Avishek Saha, Suresh\n  Venkatasubramanian", "title": "Efficient Protocols for Distributed Classification and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed learning, the goal is to perform a learning task over data\ndistributed across multiple nodes with minimal (expensive) communication. Prior\nwork (Daume III et al., 2012) proposes a general model that bounds the\ncommunication required for learning classifiers while allowing for $\\eps$\ntraining error on linearly separable data adversarially distributed across\nnodes.\n  In this work, we develop key improvements and extensions to this basic model.\nOur first result is a two-party multiplicative-weight-update based protocol\nthat uses $O(d^2 \\log{1/\\eps})$ words of communication to classify distributed\ndata in arbitrary dimension $d$, $\\eps$-optimally. This readily extends to\nclassification over $k$ nodes with $O(kd^2 \\log{1/\\eps})$ words of\ncommunication. Our proposed protocol is simple to implement and is considerably\nmore efficient than baselines compared, as demonstrated by our empirical\nresults.\n  In addition, we illustrate general algorithm design paradigms for doing\nefficient learning over distributed data. We show how to solve\nfixed-dimensional and high dimensional linear programming efficiently in a\ndistributed setting where constraints may be distributed across nodes. Since\nmany learning problems can be viewed as convex optimization problems where\nconstraints are generated by individual points, this models many typical\ndistributed learning scenarios. Our techniques make use of a novel connection\nfrom multipass streaming, as well as adapting the multiplicative-weight-update\nframework more generally to a distributed setting. As a consequence, our\nmethods extend to the wide range of problems solvable using these techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 15:25:50 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Daume", "Hal", "III"], ["Phillips", "Jeff M.", ""], ["Saha", "Avishek", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1204.3573", "submitter": "Lorenzo Rosasco", "authors": "Ernesto De Vito, Lorenzo Rosasco, Alessandro Toigo", "title": "Learning Sets with Separating Kernels", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a set from random samples. We show how\nrelevant geometric and topological properties of a set can be studied\nanalytically using concepts from the theory of reproducing kernel Hilbert\nspaces. A new kind of reproducing kernel, that we call separating kernel, plays\na crucial role in our study and is analyzed in detail. We prove a new analytic\ncharacterization of the support of a distribution, that naturally leads to a\nfamily of provably consistent regularized learning algorithms and we discuss\nthe stability of these methods with respect to random sampling. Numerical\nexperiments show that the approach is competitive, and often better, than other\nstate of the art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 17:09:24 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 15:41:47 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["De Vito", "Ernesto", ""], ["Rosasco", "Lorenzo", ""], ["Toigo", "Alessandro", ""]]}, {"id": "1204.3742", "submitter": "Mihai-Alin Badiu", "authors": "Mihai-Alin Badiu, Carles Navarro Manch\\'on, Vasile Bota and Bernard\n  Henri Fleury", "title": "Distributed Iterative Processing for Interference Channels with Receiver\n  Cooperation", "comments": "Submitted to 7th International Symposium on Turbo Codes & Iterative\n  Information Processing (ISTC 2012) for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for the derivation and evaluation of distributed\niterative algorithms for receiver cooperation in interference-limited wireless\nsystems. Our approach views the processing within and collaboration between\nreceivers as the solution to an inference problem in the probabilistic model of\nthe whole system. The probabilistic model is formulated to explicitly\nincorporate the receivers' ability to share information of a predefined type.\nWe employ a recently proposed unified message-passing tool to infer the\nvariables of interest in the factor graph representation of the probabilistic\nmodel. The exchange of information between receivers arises in the form of\npassing messages along some specific edges of the factor graph; the rate of\nupdating and passing these messages determines the communication overhead\nassociated with cooperation. Simulation results illustrate the high performance\nof the proposed algorithm even with a low number of message exchanges between\nreceivers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 09:17:23 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Badiu", "Mihai-Alin", ""], ["Manch\u00f3n", "Carles Navarro", ""], ["Bota", "Vasile", ""], ["Fleury", "Bernard Henri", ""]]}, {"id": "1204.3942", "submitter": "Genevera Allen", "authors": "Genevera I. Allen, Christine Peterson, Marina Vannucci, and Mirjana\n  Maletic-Savatic", "title": "Regularized Partial Least Squares with an Application to NMR\n  Spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data common in genomics, proteomics, and chemometrics often\ncontains complicated correlation structures. Recently, partial least squares\n(PLS) and Sparse PLS methods have gained attention in these areas as dimension\nreduction techniques in the context of supervised data analysis. We introduce a\nframework for Regularized PLS by solving a relaxation of the SIMPLS\noptimization problem with penalties on the PLS loadings vectors. Our approach\nenjoys many advantages including flexibility, general penalties, easy\ninterpretation of results, and fast computation in high-dimensional settings.\nWe also outline extensions of our methods leading to novel methods for\nNon-negative PLS and Generalized PLS, an adaption of PLS for structured data.\nWe demonstrate the utility of our methods through simulations and a case study\non proton Nuclear Magnetic Resonance (NMR) spectroscopy data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 23:38:29 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Allen", "Genevera I.", ""], ["Peterson", "Christine", ""], ["Vannucci", "Marina", ""], ["Maletic-Savatic", "Mirjana", ""]]}, {"id": "1204.3965", "submitter": "Takafumi Kanamori Dr.", "authors": "Masanori Kawakita, Takafumi Kanamori", "title": "Semi-Supervised learning with Density-Ratio Estimation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical properties of semi-supervised learning,\nwhich is considered as an important problem in the community of machine\nlearning. In the standard supervised learning, only the labeled data is\nobserved. The classification and regression problems are formalized as the\nsupervised learning. In semi-supervised learning, unlabeled data is also\nobtained in addition to labeled data. Hence, exploiting unlabeled data is\nimportant to improve the prediction accuracy in semi-supervised learning. This\nproblems is regarded as a semiparametric estimation problem with missing data.\nUnder the the discriminative probabilistic models, it had been considered that\nthe unlabeled data is useless to improve the estimation accuracy. Recently, it\nwas revealed that the weighted estimator using the unlabeled data achieves\nbetter prediction accuracy in comparison to the learning method using only\nlabeled data, especially when the discriminative probabilistic model is\nmisspecified. That is, the improvement under the semiparametric model with\nmissing data is possible, when the semiparametric model is misspecified. In\nthis paper, we apply the density-ratio estimator to obtain the weight function\nin the semi-supervised learning. The benefit of our approach is that the\nproposed estimator does not require well-specified probabilistic models for the\nprobability of the unlabeled data. Based on the statistical asymptotic theory,\nwe prove that the estimation accuracy of our method outperforms the supervised\nlearning using only labeled data. Some numerical experiments present the\nusefulness of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 03:06:40 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Kawakita", "Masanori", ""], ["Kanamori", "Takafumi", ""]]}, {"id": "1204.3972", "submitter": "Bo Dai", "authors": "Yuan Qi and Bo Dai and Yao Zhu", "title": "EigenGP: Sparse Gaussian process models with data-dependent\n  eigenfunctions", "comments": "10 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost and it is\ndifficult to design nonstationary GP priors in practice. In this paper, we\npropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve\n(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data\ndependent eigenfunctions and select these eigenfunctions by evidence\nmaximization. This selection reduces the number of eigenfunctions in our model\nand provides a nonstationary covariance function. To handle nonlinear\nlikelihoods, we develop an efficient expectation propagation (EP) inference\nalgorithm, and couple it with expectation maximization for eigenfunction\nselection. Because the eigenfunctions of a Gaussian kernel are associated with\nclusters of samples - including both the labeled and unlabeled - selecting\nrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.\nOur experimental results demonstrate improved predictive performance of EigenGP\nover alternative state-of-the-art sparse GP and semisupervised learning methods\nfor regression, classification, and semisupervised classification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 04:43:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2012 21:23:54 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 21:55:59 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Qi", "Yuan", ""], ["Dai", "Bo", ""], ["Zhu", "Yao", ""]]}, {"id": "1204.4154", "submitter": "Nathan Lay", "authors": "Nathan Lay (Department of Scientific Computing, FSU) and Adrian Barbu\n  (Department of Statistics, FSU)", "title": "The Artificial Regression Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Prediction Market is a recent machine learning technique for\nmulti-class classification, inspired from the financial markets. It involves a\nnumber of trained market participants that bet on the possible outcomes and are\nrewarded if they predict correctly. This paper generalizes the scope of the\nArtificial Prediction Markets to regression, where there are uncountably many\npossible outcomes and the error is usually the MSE. For that, we introduce the\nreward kernel that rewards each participant based on its prediction error and\nwe derive the price equations. Using two reward kernels we obtain two different\nlearning rules, one of which is approximated using Hermite-Gauss quadrature.\nThe market setting makes it easy to aggregate specialized regressors that only\npredict when an observation falls into their specialization domain. Experiments\nshow that regression markets based on the two learning rules outperform Random\nForest Regression on many UCI datasets and are rarely outperformed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 18:16:59 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Lay", "Nathan", "", "Department of Scientific Computing, FSU"], ["Barbu", "Adrian", "", "Department of Statistics, FSU"]]}, {"id": "1204.4166", "submitter": "Yandong Guo", "authors": "Yuan Qi and Yandong Guo", "title": "Message passing with relaxed moment matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is often hampered by large computational expense. As a\npowerful generalization of popular belief propagation, expectation propagation\n(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP\ncan be sensitive to outliers and suffer from divergence for difficult cases. To\naddress this issue, we propose a new approximate inference approach, relaxed\nexpectation propagation (REP). It relaxes the moment matching requirement of\nexpectation propagation by adding a relaxation factor into the KL minimization.\nWe penalize this relaxation with a $l_1$ penalty. As a result, when two\ndistributions in the relaxed KL divergence are similar, the relaxation factor\nwill be penalized to zero and, therefore, we obtain the original moment\nmatching; In the presence of outliers, these two distributions are\nsignificantly different and the relaxation factor will be used to reduce the\ncontribution of the outlier. Based on this penalized KL minimization, REP is\nrobust to outliers and can greatly improve the posterior approximation quality\nover EP. To examine the effectiveness of REP, we apply it to Gaussian process\nclassification, a task known to be suitable to EP. Our classification results\non synthetic and UCI benchmark datasets demonstrate significant improvement of\nREP over EP and Power EP--in terms of algorithmic stability, estimation\naccuracy and predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 19:21:59 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 16:02:21 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Qi", "Yuan", ""], ["Guo", "Yandong", ""]]}, {"id": "1204.4227", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Estimating Unknown Sparsity in Compressed Sensing", "comments": "This is version 2. Many aspects of the paper have been revised. The\n  restriction to non-negative signals has been removed. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the theory of compressed sensing (CS), the sparsity ||x||_0 of the unknown\nsignal x\\in\\R^p is commonly assumed to be a known parameter. However, it is\ntypically unknown in practice. Due to the fact that many aspects of CS depend\non knowing ||x||_0, it is important to estimate this parameter in a data-driven\nway. A second practical concern is that ||x||_0 is a highly unstable function\nof x. In particular, for real signals with entries not exactly equal to 0, the\nvalue ||x||_0=p is not a useful description of the effective number of\ncoordinates. In this paper, we propose to estimate a stable measure of sparsity\ns(x):=||x||_1^2/||x||_2^2, which is a sharp lower bound on ||x||_0. Our\nestimation procedure uses only a small number of linear measurements, does not\nrely on any sparsity assumptions, and requires very little computation. A\nconfidence interval for s(x) is provided, and its width is shown to have no\ndependence on the signal dimension p. Moreover, this result extends naturally\nto the matrix recovery setting, where a soft version of matrix rank can be\nestimated with analogous guarantees. Finally, we show that the use of\nrandomized measurements is essential to estimating s(x). This is accomplished\nby proving that the minimax risk for estimating s(x) with deterministic\nmeasurements is large when n<<p.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 00:43:05 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 14:51:32 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1204.4243", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Shusen Wang, Dehua Liu and Michael I. Jordan", "title": "EP-GIG Priors and Applications in Bayesian Sparse Learning", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel framework for the construction of\nsparsity-inducing priors. In particular, we define such priors as a mixture of\nexponential power distributions with a generalized inverse Gaussian density\n(EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the\nspecial cases include Gaussian scale mixtures and Laplace scale mixtures.\nFurthermore, Laplace scale mixtures can subserve a Bayesian framework for\nsparse learning with nonconvex penalization. The densities of EP-GIG can be\nexplicitly expressed. Moreover, the corresponding posterior distribution also\nfollows a generalized inverse Gaussian distribution. These properties lead us\nto EM algorithms for Bayesian sparse learning. We show that these algorithms\nbear an interesting resemblance to iteratively re-weighted $\\ell_2$ or $\\ell_1$\nmethods. In addition, we present two extensions for grouped variable selection\nand logistic regression.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 02:59:03 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Zhang", "Zhihua", ""], ["Wang", "Shusen", ""], ["Liu", "Dehua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1204.4521", "submitter": "Ayan Acharya", "authors": "Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh", "title": "A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster\n  Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a privacy-aware Bayesian approach that combines\nensembles of classifiers and clusterers to perform semi-supervised and\ntransductive learning. We consider scenarios where instances and their\nclassification/clustering results are distributed across different data sites\nand have sharing restrictions. As a special case, the privacy aware computation\nof the model when instances of the target data are distributed across different\ndata sites, is also discussed. Experimental results show that the proposed\napproach can provide good classification accuracies while adhering to the\ndata/model sharing constraints.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 03:01:56 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Acharya", "Ayan", ""], ["Hruschka", "Eduardo R.", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1204.4539", "submitter": "Julien Mairal", "authors": "Julien Mairal and Bin Yu", "title": "Supervised Feature Selection in Graphs with Path Coding Penalties and\n  Network Flows", "comments": "37 pages; to appear in the Journal of Machine Learning Research\n  (JMLR)", "journal-ref": "Journal of Machine Learning Research 14(Aug) (2013) 2449-2485", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider supervised learning problems where the features are embedded in a\ngraph, such as gene expressions in a gene network. In this context, it is of\nmuch interest to automatically select a subgraph with few connected components;\nby exploiting prior knowledge, one can indeed improve the prediction\nperformance or obtain results that are easier to interpret. Regularization or\npenalty functions for selecting features in graphs have recently been proposed,\nbut they raise new algorithmic challenges. For example, they typically require\nsolving a combinatorially hard selection problem among all connected subgraphs.\nIn this paper, we propose computationally feasible strategies to select a\nsparse and well-connected subset of features sitting on a directed acyclic\ngraph (DAG). We introduce structured sparsity penalties over paths on a DAG\ncalled \"path coding\" penalties. Unlike existing regularization functions that\nmodel long-range interactions between features in a graph, path coding\npenalties are tractable. The penalties and their proximal operators involve\npath selection problems, which we efficiently solve by leveraging network flow\noptimization. We experimentally show on synthetic, image, and genomic data that\nour approach is scalable and leads to more connected subgraphs than other\nregularization functions for graphs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 06:24:37 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2013 10:16:21 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 13:12:00 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Mairal", "Julien", ""], ["Yu", "Bin", ""]]}, {"id": "1204.4699", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen and Subhadeep Mukhopadhyay (Deep)", "title": "Modeling, dependence, classification, united statistical science, many\n  cultures", "comments": "31 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman (2001) proposed to statisticians awareness of two cultures: 1.\nParametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2.\nAlgorithmic predictive culture, pioneered by machine learning research.\n  Parzen (2001), as a part of discussing Breiman (2001), proposed that\nresearchers be aware of many cultures, including the focus of our research: 3.\nNonparametric, quantile based, information theoretic modeling. We provide a\nunification of many statistical methods for traditional small data sets and\nemerging big data sets in terms of comparison density, copula density, measure\nof dependence, correlation, information, new measures (called LP score\ncomoments) that apply to long tailed distributions with out finite second order\nmoments. A very important goal is to unify methods for discrete and continuous\nrandom variables. Our research extends these methods to modern high dimensional\ndata modeling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 18:50:20 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 01:12:08 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 02:11:01 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Parzen", "Emanuel", "", "Deep"], ["Mukhopadhyay", "Subhadeep", "", "Deep"]]}, {"id": "1204.4708", "submitter": "Ricardo Henao", "authors": "Ricardo Henao and Joseph E. Lucas", "title": "Efficient hierarchical clustering for continuous data", "comments": "17 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an new sequential Monte Carlo sampler for coalescent based\nBayesian hierarchical clustering. Our model is appropriate for modeling\nnon-i.i.d. data and offers a substantial reduction of computational cost when\ncompared to the original sampler without resorting to approximations. We also\npropose a quadratic complexity approximation that in practice shows almost no\nloss in performance compared to its counterpart. We show that as a byproduct of\nour formulation, we obtain a greedy algorithm that exhibits performance\nimprovement over other greedy algorithms, particularly in small data sets. In\norder to exploit the correlation structure of the data, we describe how to\nincorporate Gaussian process priors in the model as a flexible way to model\nnon-i.i.d. data. Results on artificial and real data show significant\nimprovements over closely related approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 19:19:05 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Henao", "Ricardo", ""], ["Lucas", "Joseph E.", ""]]}, {"id": "1204.4710", "submitter": "Sebastien Bubeck", "authors": "Jean-Yves Audibert, S\\'ebastien Bubeck and G\\'abor Lugosi", "title": "Regret in Online Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address online linear optimization problems when the possible actions of\nthe decision maker are represented by binary vectors. The regret of the\ndecision maker is the difference between her realized loss and the best loss\nshe would have achieved by picking, in hindsight, the best possible action. Our\ngoal is to understand the magnitude of the best possible (minimax) regret. We\nstudy the problem under three different assumptions for the feedback the\ndecision maker receives: full information, and the partial information models\nof the so-called \"semi-bandit\" and \"bandit\" problems. Combining the Mirror\nDescent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we\nare able to prove optimal bounds for the semi-bandit case. We also recover the\noptimal bounds for the full information setting. In the bandit case we discuss\nexisting results in light of a new lower bound, and suggest a conjecture on the\noptimal regret in that case. Finally we also prove that the standard\nexponentially weighted average forecaster is provably suboptimal in the setting\nof online combinatorial optimization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 19:26:05 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 22:04:06 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Audibert", "Jean-Yves", ""], ["Bubeck", "S\u00e9bastien", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1204.4927", "submitter": "Casey Bennett", "authors": "Casey Bennett, Tom Doub, Rebecca Selove", "title": "EHRs Connect Research and Practice: Where Predictive Modeling,\n  Artificial Intelligence, and Clinical Decision Support Intersect", "comments": "Keywords: Data Mining; Decision Support Systems, Clinical; Electronic\n  Health Records; Implementation; Evidence-Based Medicine; Data Warehouse;\n  (2012). EHRs Connect Research and Practice: Where Predictive Modeling,\n  Artificial Intelligence, and Clinical Decision Support Intersect. Health\n  Policy and Technology. arXiv admin note: substantial text overlap with\n  arXiv:1112.1668", "journal-ref": "Health Policy and Technology 1(2): 105-114 (2012)", "doi": "10.1016/j.hlpt.2012.03.001", "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Electronic health records (EHRs) are only a first step in\ncapturing and utilizing health-related data - the challenge is turning that\ndata into useful information. Furthermore, EHRs are increasingly likely to\ninclude data relating to patient outcomes, functionality such as clinical\ndecision support, and genetic information as well, and, as such, can be seen as\nrepositories of increasingly valuable information about patients' health\nconditions and responses to treatment over time. Methods: We describe a case\nstudy of 423 patients treated by Centerstone within Tennessee and Indiana in\nwhich we utilized electronic health record data to generate predictive\nalgorithms of individual patient treatment response. Multiple models were\nconstructed using predictor variables derived from clinical, financial and\ngeographic data. Results: For the 423 patients, 101 deteriorated, 223 improved\nand in 99 there was no change in clinical condition. Based on modeling of\nvarious clinical indicators at baseline, the highest accuracy in predicting\nindividual patient response ranged from 70-72% within the models tested. In\nterms of individual predictors, the Centerstone Assessment of Recovery Level -\nAdult (CARLA) baseline score was most significant in predicting outcome over\ntime (odds ratio 4.1 + 2.27). Other variables with consistently significant\nimpact on outcome included payer, diagnostic category, location and provision\nof case management services. Conclusions: This approach represents a promising\navenue toward reducing the current gap between research and practice across\nhealthcare, developing data-driven clinical decision support based on\nreal-world populations, and serving as a component of embedded clinical\nartificial intelligences that \"learn\" over time.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 19:24:40 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Bennett", "Casey", ""], ["Doub", "Tom", ""], ["Selove", "Rebecca", ""]]}, {"id": "1204.5043", "submitter": "Andreas Argyriou", "authors": "Andreas Argyriou and Rina Foygel and Nathan Srebro", "title": "Sparse Prediction with the $k$-Support Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a novel norm that corresponds to the tightest convex relaxation of\nsparsity combined with an $\\ell_2$ penalty. We show that this new {\\em\n$k$-support norm} provides a tighter relaxation than the elastic net and is\nthus a good replacement for the Lasso or the elastic net in sparse prediction\nproblems. Through the study of the $k$-support norm, we also bound the\nlooseness of the elastic net, thus shedding new light on it and providing\njustification for its use.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 12:35:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2012 08:59:52 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Argyriou", "Andreas", ""], ["Foygel", "Rina", ""], ["Srebro", "Nathan", ""]]}, {"id": "1204.5357", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Learning AMP Chain Graphs under Faithfulness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the alternative\nAndersson-Madigan-Perlman (AMP) interpretation. In particular, we present a\nconstraint based algorithm for learning an AMP chain graph a given probability\ndistribution is faithful to. We also show that the extension of Meek's\nconjecture to AMP chain graphs does not hold, which compromises the development\nof efficient and correct score+search learning algorithms under assumptions\nweaker than faithfulness.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 12:49:47 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1204.5540", "submitter": "Rui Wu", "authors": "Rui Wu, R. Srikant and Jian Ni", "title": "Learning Loosely Connected Markov Random Fields", "comments": "45 pages, minor revision", "journal-ref": "Wu, Rui, Srikant, R., Ni, Jian, Learning loosely connected Markov\n  random fields, Stochastic Systems, 3, (2013), 362-404", "doi": "10.1214/12-SSY073", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the structure learning problem for graphical models that we call\nloosely connected Markov random fields, in which the number of short paths\nbetween any pair of nodes is small, and present a new conditional independence\ntest based algorithm for learning the underlying graph structure. The novel\nmaximization step in our algorithm ensures that the true edges are detected\ncorrectly even when there are short cycles in the graph. The number of samples\nrequired by our algorithm is C*log p, where p is the size of the graph and the\nconstant C depends on the parameters of the model. We show that several\npreviously studied models are examples of loosely connected Markov random\nfields, and our algorithm achieves the same or lower computational complexity\nthan the previously designed algorithms for individual cases. We also get new\nresults for more general graphical models, in particular, our algorithm learns\ngeneral Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with\nrunning time O(np^5).\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 02:39:40 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2013 15:13:05 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 20:33:38 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Wu", "Rui", ""], ["Srikant", "R.", ""], ["Ni", "Jian", ""]]}, {"id": "1204.5721", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Nicol\\`o Cesa-Bianchi", "title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit\n  Problems", "comments": "To appear in Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are the most basic examples of sequential\ndecision problems with an exploration-exploitation trade-off. This is the\nbalance between staying with the option that gave highest payoffs in the past\nand exploring new options that might give higher payoffs in the future.\nAlthough the study of bandit problems dates back to the Thirties,\nexploration-exploitation trade-offs arise in several modern applications, such\nas ad placement, website optimization, and packet routing. Mathematically, a\nmulti-armed bandit is defined by the payoff process associated with each\noption. In this survey, we focus on two extreme cases in which the analysis of\nregret is particularly simple and elegant: i.i.d. payoffs and adversarial\npayoffs. Besides the basic setting of finitely many actions, we also analyze\nsome of the most important variants and extensions, such as the contextual\nbandit model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 18:04:32 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 18:50:58 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1204.6160", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on and Tarn Duong", "title": "Data-driven density derivative estimation, with applications to\n  nonparametric clustering and bump hunting", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important information concerning a multivariate data set, such as clusters\nand modal regions, is contained in the derivatives of the probability density\nfunction. Despite this importance, nonparametric estimation of higher order\nderivatives of the density functions have received only relatively scant\nattention. Kernel estimators of density functions are widely used as they\nexhibit excellent theoretical and practical properties, though their\ngeneralization to density derivatives has progressed more slowly due to the\nmathematical intractabilities encountered in the crucial problem of bandwidth\n(or smoothing parameter) selection. This paper presents the first fully\nautomatic, data-based bandwidth selectors for multivariate kernel density\nderivative estimators. This is achieved by synthesizing recent advances in\nmatrix analytic theory which allow mathematically and computationally tractable\nrepresentations of higher order derivatives of multivariate vector valued\nfunctions. The theoretical asymptotic properties as well as the finite sample\nbehaviour of the proposed selectors are studied. {In addition, we explore in\ndetail the applications of the new data-driven methods for two other\nstatistical problems: clustering and bump hunting. The introduced techniques\nare combined with the mean shift algorithm to develop novel automatic,\nnonparametric clustering procedures which are shown to outperform mixture-model\ncluster analysis and other recent nonparametric approaches in practice.\nFurthermore, the advantage of the use of smoothing parameters designed for\ndensity derivative estimation for feature significance analysis for bump\nhunting is illustrated with a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 10:04:13 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 18:22:25 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 19:18:10 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""], ["Duong", "Tarn", ""]]}, {"id": "1204.6452", "submitter": "Qi Zhang", "authors": "Jiashun Jin, Cun-Hui Zhang, Qi Zhang", "title": "Optimality of Graphlet Screening in High Dimensional Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Consider a linear regression model where the design matrix X has n rows and p\ncolumns. We assume (a) p is much large than n, (b) the coefficient vector beta\nis sparse in the sense that only a small fraction of its coordinates is\nnonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each row\nhas relatively few large coordinates (diagonals of G are normalized to 1).\n  The sparsity in G naturally induces the sparsity of the so-called graph of\nstrong dependence (GOSD). We find an interesting interplay between the signal\nsparsity and the graph sparsity, which ensures that in a broad context, the set\nof true signals decompose into many different small-size components of GOSD,\nwhere different components are disconnected.\n  We propose Graphlet Screening (GS) as a new approach to variable selection,\nwhich is a two-stage Screen and Clean method. The key methodological innovation\nof GS is to use GOSD to guide both the screening and cleaning. Compared to\nm-variate brute-forth screening that has a computational cost of p^m, the GS\nonly has a computational cost of p (up to some multi-log(p) factors) in\nscreening.\n  We measure the performance of any variable selection procedure by the minimax\nHamming distance. We show that in a very broad class of situations, GS achieves\nthe optimal rate of convergence in terms of the Hamming distance. Somewhat\nsurprisingly, the well-known procedures subset selection and the lasso are rate\nnon-optimal, even in very simple settings and even when their tuning parameters\nare ideally set.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 03:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 08:23:49 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Jin", "Jiashun", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Qi", ""]]}, {"id": "1204.6509", "submitter": "Fabrice Rossi", "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (SAMM)", "title": "Dissimilarity Clustering by Hierarchical Multi-Level Refinement", "comments": "20-th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new way of optimizing the natural extension of\nthe quantization error using in k-means clustering to dissimilarity data. The\nproposed method is based on hierarchical clustering analysis combined with\nmulti-level heuristic refinement. The method is computationally efficient and\nachieves better quantization errors than the\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 19:31:15 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Conan-Guez", "Brieuc", "", "LITA"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1204.6537", "submitter": "Gonzalo Mateos", "authors": "Morteza Mardani, Gonzalo Mateos, and Georgios B. Giannakis", "title": "Recovery of Low-Rank Plus Compressed Sparse Matrices with Application to\n  Unveiling Traffic Anomalies", "comments": "38 pages, submitted to the IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2257913", "report-no": null, "categories": "cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the superposition of a low-rank matrix plus the product of a known fat\ncompression matrix times a sparse matrix, the goal of this paper is to\nestablish deterministic conditions under which exact recovery of the low-rank\nand sparse components becomes possible. This fundamental identifiability issue\narises with traffic anomaly detection in backbone networks, and subsumes\ncompressed sensing as well as the timely low-rank plus sparse matrix recovery\ntasks encountered in matrix decomposition problems. Leveraging the ability of\n$\\ell_1$- and nuclear norms to recover sparse and low-rank matrices, a convex\nprogram is formulated to estimate the unknowns. Analysis and simulations\nconfirm that the said convex program can recover the unknowns for sufficiently\nlow-rank and sparse enough components, along with a compression matrix\npossessing an isometry property when restricted to operate on sparse vectors.\nWhen the low-rank, sparse, and compression matrices are drawn from certain\nrandom ensembles, it is established that exact recovery is possible with high\nprobability. First-order algorithms are developed to solve the nonsmooth convex\noptimization problem with provable iteration complexity guarantees. Insightful\ntests with synthetic and real network data corroborate the effectiveness of the\nnovel approach in unveiling traffic anomalies across flows and time, and its\nability to outperform existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 02:39:34 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Mardani", "Morteza", ""], ["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1204.6583", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Akiko Takeda, Taiji Suzuki", "title": "A Conjugate Property between Loss Functions and Uncertainty Sets in\n  Classification Problems", "comments": "41 pages, 4 figures. The shorter version is accepted by COLT2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary classification problems, mainly two approaches have been proposed;\none is loss function approach and the other is uncertainty set approach. The\nloss function approach is applied to major learning algorithms such as support\nvector machine (SVM) and boosting methods. The loss function represents the\npenalty of the decision function on the training samples. In the learning\nalgorithm, the empirical mean of the loss function is minimized to obtain the\nclassifier. Against a backdrop of the development of mathematical programming,\nnowadays learning algorithms based on loss functions are widely applied to\nreal-world data analysis. In addition, statistical properties of such learning\nalgorithms are well-understood based on a lots of theoretical works. On the\nother hand, the learning method using the so-called uncertainty set is used in\nhard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. In\nthe learning algorithm, firstly, the uncertainty set is defined for each binary\nlabel based on the training samples. Then, the best separating hyperplane\nbetween the two uncertainty sets is employed as the decision function. This is\nregarded as an extension of the maximum-margin approach. The uncertainty set\napproach has been studied as an application of robust optimization in the field\nof mathematical programming. The statistical properties of learning algorithms\nwith uncertainty sets have not been intensively studied. In this paper, we\nconsider the relation between the above two approaches. We point out that the\nuncertainty set is described by using the level set of the conjugate of the\nloss function. Based on such relation, we study statistical properties of\nlearning algorithms using uncertainty sets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 09:53:08 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Takeda", "Akiko", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1204.6703", "submitter": "Daniel Hsu", "authors": "Animashree Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade,\n  Yi-Kai Liu", "title": "A Spectral Algorithm for Latent Dirichlet Allocation", "comments": "Changed title to match conference version, which appears in Advances\n  in Neural Information Processing Systems 25, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of topic modeling can be seen as a generalization of the\nclustering problem, in that it posits that observations are generated due to\nmultiple latent factors (e.g., the words in each document are generated as a\nmixture of several active topics, as opposed to just one). This increased\nrepresentational power comes at the cost of a more challenging unsupervised\nlearning problem of estimating the topic probability vectors (the distributions\nover words for each topic), when only the words are observed and the\ncorresponding topics are hidden.\n  We provide a simple and efficient learning procedure that is guaranteed to\nrecover the parameters for a wide class of mixture models, including the\npopular latent Dirichlet allocation (LDA) model. For LDA, the procedure\ncorrectly recovers both the topic probability vectors and the prior over the\ntopics, using only trigram statistics (i.e., third order moments, which may be\nestimated with documents containing just three words). The method, termed\nExcess Correlation Analysis (ECA), is based on a spectral decomposition of low\norder moments (third and fourth order) via two singular value decompositions\n(SVDs). Moreover, the algorithm is scalable since the SVD operations are\ncarried out on $k\\times k$ matrices, where $k$ is the number of latent factors\n(e.g. the number of topics), rather than in the $d$-dimensional observed space\n(typically $d \\gg k$).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 17:06:06 GMT"}, {"version": "v2", "created": "Tue, 22 May 2012 02:08:38 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2012 20:01:11 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2013 21:01:29 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Foster", "Dean P.", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Liu", "Yi-Kai", ""]]}]