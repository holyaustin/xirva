[{"id": "1410.0095", "submitter": "Xu Wang", "authors": "Xu Wang, Konstantinos Slavakis, Gilad Lerman", "title": "Riemannian Multi-Manifold Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates a novel framework for segmenting a dataset in a\nRiemannian manifold $M$ into clusters lying around low-dimensional submanifolds\nof $M$. Important examples of $M$, for which the proposed clustering algorithm\nis computationally efficient, are the sphere, the set of positive definite\nmatrices, and the Grassmannian. The clustering problem with these examples of\n$M$ is already useful for numerous application domains such as action\nidentification in video sequences, dynamic texture clustering, brain fiber\nsegmentation in medical imaging, and clustering of deformed images. The\nproposed clustering algorithm constructs a data-affinity matrix by thoroughly\nexploiting the intrinsic geometry and then applies spectral clustering. The\nintrinsic local geometry is encoded by local sparse coding and more importantly\nby directional information of local tangent spaces and geodesics. Theoretical\nguarantees are established for a simplified variant of the algorithm even when\nthe clusters intersect. To avoid complication, these guarantees assume that the\nunderlying submanifolds are geodesic. Extensive validation on synthetic and\nreal data demonstrates the resiliency of the proposed method against deviations\nfrom the theoretical model as well as its superior performance over\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 02:37:12 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Wang", "Xu", ""], ["Slavakis", "Konstantinos", ""], ["Lerman", "Gilad", ""]]}, {"id": "1410.0123", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Heng Luo, Aaron Courville and Yoshua Bengio", "title": "Deep Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are one of the fundamental building\nblocks of deep learning. Approximate maximum likelihood training of RBMs\ntypically necessitates sampling from these models. In many training scenarios,\ncomputationally efficient Gibbs sampling procedures are crippled by poor\nmixing. In this work we propose a novel method of sampling from Boltzmann\nmachines that demonstrates a computationally efficient way to promote mixing.\nOur approach leverages an under-appreciated property of deep generative models\nsuch as the Deep Belief Network (DBN), where Gibbs sampling from deeper levels\nof the latent variable hierarchy results in dramatically increased ergodicity.\nOur approach is thus to train an auxiliary latent hierarchical model, based on\nthe DBN. When used in conjunction with parallel-tempering, the method is\nasymptotically guaranteed to simulate samples from the target RBM. Experimental\nresults confirm the effectiveness of this sampling strategy in the context of\nRBM training.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 06:55:11 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Luo", "Heng", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.0334", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (LHC)", "title": "Domain adaptation of weighted majority votes via perturbed\n  variation-based self-labeling", "comments": null, "journal-ref": "Pattern Recognition Letters (2014) To be published", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the domain adaptation problem arrives when the test\n(target) and the train (source) data are generated from different\ndistributions. A key applied issue is thus the design of algorithms able to\ngeneralize on a new distribution, for which we have no label information. We\nfocus on learning classification models defined as a weighted majority vote\nover a set of real-val ued functions. In this context, Germain et al. (2013)\nhave shown that a measure of disagreement between these functions is crucial to\ncontrol. The core of this measure is a theoretical bound--the C-bound (Lacasse\net al., 2007)--which involves the disagreement and leads to a well performing\nmajority vote learning algorithm in usual non-adaptative supervised setting:\nMinCq. In this work, we propose a framework to extend MinCq to a domain\nadaptation scenario. This procedure takes advantage of the recent perturbed\nvariation divergence between distributions proposed by Harel and Mannor (2012).\nJustified by a theoretical bound on the target risk of the vote, we provide to\nMinCq a target sample labeled thanks to a perturbed variation-based\nself-labeling focused on the regions where the source and target marginals\nappear similar. We also study the influence of our self-labeling, from which we\ndeduce an original process for tuning the hyperparameters. Finally, our\nframework called PV-MinCq shows very promising results on a rotation and\ntranslation synthetic problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 19:09:02 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Morvant", "Emilie", "", "LHC"]]}, {"id": "1410.0342", "submitter": "Madeleine Udell", "authors": "Madeleine Udell, Corinne Horn, Reza Zadeh and Stephen Boyd", "title": "Generalized Low Rank Models", "comments": "84 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is a well-known technique for\napproximating a tabular data set by a low rank matrix. Here, we extend the idea\nof PCA to handle arbitrary data sets consisting of numerical, Boolean,\ncategorical, ordinal, and other data types. This framework encompasses many\nwell known techniques in data analysis, such as nonnegative matrix\nfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,\nand maximum margin matrix factorization. The method handles heterogeneous data\nsets, and leads to coherent schemes for compressing, denoising, and imputing\nmissing entries across all data types simultaneously. It also admits a number\nof interesting interpretations of the low rank factors, which allow clustering\nof examples or of features. We propose several parallel algorithms for fitting\ngeneralized low rank models, and describe implementations and numerical\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 19:31:40 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 01:48:22 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 06:27:48 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 18:53:24 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Udell", "Madeleine", ""], ["Horn", "Corinne", ""], ["Zadeh", "Reza", ""], ["Boyd", "Stephen", ""]]}, {"id": "1410.0389", "submitter": "Novi Quadrianto", "authors": "Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert", "title": "Learning to Transfer Privileged Information", "comments": "20 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning framework called learning using privileged\ninformation (LUPI) to the computer vision field. We focus on the prototypical\ncomputer vision problem of teaching computers to recognize objects in images.\nWe want the computers to be able to learn faster at the expense of providing\nextra information during training time. As additional information about the\nimage data, we look at several scenarios that have been studied in computer\nvision before: attributes, bounding boxes and image tags. The information is\nprivileged as it is available at training time but not at test time. We explore\ntwo maximum-margin techniques that are able to make use of this additional\nsource of information, for binary and multiclass object classification. We\ninterpret these methods as learning easiness and hardness of the objects in the\nprivileged space and then transferring this knowledge to train a better\nclassifier in the original space. We provide a thorough analysis and comparison\nof information transfer from privileged to the original data spaces for both\nLUPI methods. Our experiments show that incorporating privileged information\ncan improve the classification accuracy. Finally, we conduct user studies to\nunderstand which samples are easy and which are hard for human learning, and\nexplore how this information is related to easy and hard samples when learning\na classifier.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 21:29:45 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Sharmanska", "Viktoriia", ""], ["Quadrianto", "Novi", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1410.0440", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus\n  Telgarsky", "title": "Scalable Nonlinear Learning with Adaptive Polynomial Expansions", "comments": "To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we effectively learn a nonlinear representation in time comparable to\nlinear learning? We describe a new algorithm that explicitly and adaptively\nexpands higher-order interaction features over base linear representations. The\nalgorithm is designed for extreme computational efficiency, and an extensive\nexperimental study shows that its computation/prediction tradeoff ability\ncompares very favorably against strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 02:28:04 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Agarwal", "Alekh", ""], ["Beygelzimer", "Alina", ""], ["Hsu", "Daniel", ""], ["Langford", "John", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1410.0555", "submitter": "Jaakko Luttinen", "authors": "Jaakko Luttinen, Tapani Raiko, Alexander Ilin", "title": "Linear State-Space Model with Time-Varying Dynamics", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-662-44851-9_22", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. Lecture\n  Notes in Computer Science Volume 8725, 2014, pp 338-353", "doi": "10.1007/978-3-662-44851-9_22", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a linear state-space model with time-varying dynamics.\nThe time dependency is obtained by forming the state dynamics matrix as a\ntime-varying linear combination of a set of matrices. The time dependency of\nthe weights in the linear combination is modelled by another linear Gaussian\ndynamical model allowing the model to learn how the dynamics of the process\nchanges. Previous approaches have used switching models which have a small set\nof possible state dynamics matrices and the model selects one of those matrices\nat each time, thus jumping between them. Our model forms the dynamics as a\nlinear combination and the changes can be smooth and more continuous. The model\nis motivated by physical processes which are described by linear partial\ndifferential equations whose parameters vary in time. An example of such a\nprocess could be a temperature field whose evolution is driven by a varying\nwind direction. The posterior inference is performed using variational Bayesian\napproximation. The experiments on stochastic advection-diffusion processes and\nreal-world weather processes show that the model with time-varying dynamics can\noutperform previously introduced approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 13:59:15 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 07:25:22 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Luttinen", "Jaakko", ""], ["Raiko", "Tapani", ""], ["Ilin", "Alexander", ""]]}, {"id": "1410.0576", "submitter": "Kewei Tu", "authors": "Maria Pavlovskaia, Kewei Tu and Song-Chun Zhu", "title": "Mapping Energy Landscapes of Non-Convex Learning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical learning problems, the target functions to be optimized\nare highly non-convex in various model spaces and thus are difficult to\nanalyze. In this paper, we compute \\emph{Energy Landscape Maps} (ELMs) which\ncharacterize and visualize an energy function with a tree structure, in which\neach leaf node represents a local minimum and each non-leaf node represents the\nbarrier between adjacent energy basins. The ELM also associates each node with\nthe estimated probability mass and volume for the corresponding energy basin.\nWe construct ELMs by adopting the generalized Wang-Landau algorithm and\nmulti-domain sampler that simulates a Markov chain traversing the model space\nby dynamically reweighting the energy function. We construct ELMs in the model\nspace for two classic statistical learning problems: i) clustering with\nGaussian mixture models or Bernoulli templates; and ii) bi-clustering. We\npropose a way to measure the difficulties (or complexity) of these learning\nproblems and study how various conditions affect the landscape complexity, such\nas separability of the clusters, the number of examples, and the level of\nsupervision; and we also visualize the behaviors of different algorithms, such\nas K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 14:49:59 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Pavlovskaia", "Maria", ""], ["Tu", "Kewei", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1410.0630", "submitter": "Sherjil Ozair", "authors": "Sherjil Ozair and Yoshua Bengio", "title": "Deep Directed Generative Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:09:42 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ozair", "Sherjil", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.0633", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on, Robert D. Nowak, Nigel Boston", "title": "Deterministic Conditions for Subspace Identifiability from Incomplete\n  Sampling", "comments": "To appear in Proc. of IEEE ISIT, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a generic $r$-dimensional subspace of $\\mathbb{R}^d$, $r<d$, and\nsuppose that we are only given projections of this subspace onto small subsets\nof the canonical coordinates. The paper establishes necessary and sufficient\ndeterministic conditions on the subsets for subspace identifiability.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 18:20:04 GMT"}, {"version": "v2", "created": "Sun, 5 Oct 2014 15:43:50 GMT"}, {"version": "v3", "created": "Sun, 24 May 2015 17:31:55 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""], ["Nowak", "Robert D.", ""], ["Boston", "Nigel", ""]]}, {"id": "1410.0723", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Leon Bottou", "title": "A Lower Bound for the Optimization of Finite Sums", "comments": "Added an erratum, we are currently working on extending the result to\n  randomized algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a lower bound for optimizing a finite sum of $n$\nfunctions, where each function is $L$-smooth and the sum is $\\mu$-strongly\nconvex. We show that no algorithm can reach an error $\\epsilon$ in minimizing\nall functions from this class in fewer than $\\Omega(n +\n\\sqrt{n(\\kappa-1)}\\log(1/\\epsilon))$ iterations, where $\\kappa=L/\\mu$ is a\nsurrogate condition number. We then compare this lower bound to upper bounds\nfor recently developed methods specializing to this setting. When the functions\ninvolved in this sum are not arbitrary, but based on i.i.d. random data, then\nwe further contrast these complexity results with those for optimal first-order\nmethods to directly optimize the sum. The conclusion we draw is that a lot of\ncaution is necessary for an accurate comparison, and identify machine learning\nscenarios where the new methods help computationally.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 22:05:13 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 15:04:57 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 13:07:28 GMT"}, {"version": "v4", "created": "Sun, 4 Oct 2015 01:09:33 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bottou", "Leon", ""]]}, {"id": "1410.0736", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis\n  DeCoste, Wei Di, Yizhou Yu", "title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\n  Visual Recognition", "comments": "Add new results on ImageNet using VGG-16-layer building block net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification, visual separability between different object\ncategories is highly uneven, and some categories are more difficult to\ndistinguish than others. Such difficult categories demand more dedicated\nclassifiers. However, existing deep convolutional neural networks (CNN) are\ntrained as flat N-way classifiers, and few efforts have been made to leverage\nthe hierarchical structure of categories. In this paper, we introduce\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\nwhile distinguishing difficult classes using fine category classifiers. During\nHD-CNN training, component-wise pretraining is followed by global finetuning\nwith a multinomial logistic loss regularized by a coarse category consistency\nterm. In addition, conditional executions of fine category classifiers and\nlayer parameter compression make HD-CNNs scalable for large-scale visual\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\nCNNs by 2.65%, 3.1% and 1.1%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 01:17:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:51:51 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 03:11:49 GMT"}, {"version": "v4", "created": "Sat, 16 May 2015 03:36:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Piramuthu", "Robinson", ""], ["Jagadeesh", "Vignesh", ""], ["DeCoste", "Dennis", ""], ["Di", "Wei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1410.0860", "submitter": "Sahand Negahban", "authors": "Yu Lu and Sahand N. Negahban", "title": "Individualized Rank Aggregation using Nuclear Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years rank aggregation has received significant attention from the\nmachine learning community. The goal of such a problem is to combine the\n(partially revealed) preferences over objects of a large population into a\nsingle, relatively consistent ordering of those objects. However, in many\ncases, we might not want a single ranking and instead opt for individual\nrankings. We study a version of the problem known as collaborative ranking. In\nthis problem we assume that individual users provide us with pairwise\npreferences (for example purchasing one item over another). From those\npreferences we wish to obtain rankings on items that the users have not had an\nopportunity to explore. The results here have a very interesting connection to\nthe standard matrix completion problem. We provide a theoretical justification\nfor a nuclear norm regularized optimization procedure, and provide\nhigh-dimensional scaling results that show how the error in estimating user\npreferences behaves as the number of observations increase.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:34:27 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Lu", "Yu", ""], ["Negahban", "Sahand N.", ""]]}, {"id": "1410.0870", "submitter": "Jaakko Luttinen", "authors": "Jaakko Luttinen", "title": "BayesPy: Variational Bayesian Inference in Python", "comments": "Submitted to Journal of Machine Learning Research - Machine Learning\n  Open Source Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BayesPy is an open-source Python software package for performing variational\nBayesian inference. It is based on the variational message passing framework\nand supports conjugate exponential family models. By removing the tedious task\nof implementing the variational Bayesian update equations, the user can\nconstruct models faster and in a less error-prone way. Simple syntax, flexible\nmodel construction and efficient inference make BayesPy suitable for both\naverage and expert Bayesian users. It also supports some advanced methods such\nas stochastic and collapsed variational inference.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:51:09 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 12:07:09 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 14:55:19 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Luttinen", "Jaakko", ""]]}, {"id": "1410.0908", "submitter": "Ernest Fokoue", "authors": "Xingchen Yu and Ernest Fokoue", "title": "Probit Normal Correlated Topic Models", "comments": "11 pages, 2 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logistic normal distribution has recently been adapted via the\ntransformation of multivariate Gaus- sian variables to model the topical\ndistribution of documents in the presence of correlations among topics. In this\npaper, we propose a probit normal alternative approach to modelling correlated\ntopical structures. Our use of the probit model in the context of topic\ndiscovery is novel, as many authors have so far con- centrated solely of the\nlogistic model partly due to the formidable inefficiency of the multinomial\nprobit model even in the case of very small topical spaces. We herein\ncircumvent the inefficiency of multinomial probit estimation by using an\nadaptation of the diagonal orthant multinomial probit in the topic models\ncontext, resulting in the ability of our topic modelling scheme to handle\ncorpuses with a large number of latent topics. An additional and very important\nbenefit of our method lies in the fact that unlike with the logistic normal\nmodel whose non-conjugacy leads to the need for sophisticated sampling schemes,\nour ap- proach exploits the natural conjugacy inherent in the auxiliary\nformulation of the probit model to achieve greater simplicity. The application\nof our proposed scheme to a well known Associated Press corpus not only helps\ndiscover a large number of meaningful topics but also reveals the capturing of\ncompellingly intuitive correlations among certain topics. Besides, our proposed\napproach lends itself to even further scalability thanks to various existing\nhigh performance algorithms and architectures capable of handling millions of\ndocuments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 16:38:53 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Yu", "Xingchen", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1410.0949", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari", "title": "Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits", "comments": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic combinatorial semi-bandit is an online learning problem where at\neach step a learning agent chooses a subset of ground items subject to\nconstraints, and then observes stochastic weights of these items and receives\ntheir sum as a payoff. In this paper, we close the problem of computationally\nand sample efficient learning in stochastic combinatorial semi-bandits. In\nparticular, we analyze a UCB-like algorithm for solving the problem, which is\nknown to be computationally efficient; and prove $O(K L (1 / \\Delta) \\log n)$\nand $O(\\sqrt{K L n \\log n})$ upper bounds on its $n$-step regret, where $L$ is\nthe number of ground items, $K$ is the maximum number of chosen items, and\n$\\Delta$ is the gap between the expected returns of the optimal and best\nsuboptimal solutions. The gap-dependent bound is tight up to a constant factor\nand the gap-free bound is tight up to a polylogarithmic factor.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 19:38:16 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 04:30:17 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 05:15:20 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1410.0996", "submitter": "Steve Hanneke", "authors": "Steve Hanneke and Liu Yang", "title": "Minimax Analysis of Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes distribution-free upper and lower bounds on the minimax\nlabel complexity of active learning with general hypothesis classes, under\nvarious noise models. The results reveal a number of surprising facts. In\nparticular, under the noise model of Tsybakov (2004), the minimax label\ncomplexity of active learning with a VC class is always asymptotically smaller\nthan that of passive learning, and is typically significantly smaller than the\nbest previously-published upper bounds in the active learning literature. In\nhigh-noise regimes, it turns out that all active learning problems of a given\nVC dimension have roughly the same minimax label complexity, which contrasts\nwith well-known results for bounded noise. In low-noise regimes, we find that\nthe label complexity is well-characterized by a simple combinatorial complexity\nmeasure we call the star number. Interestingly, we find that almost all of the\ncomplexity measures previously explored in the active learning literature have\nworst-case values exactly equal to the star number. We also propose new active\nlearning strategies that nearly achieve these minimax label complexities.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 23:30:16 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1410.1068", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury, Brian Kulis", "title": "Gamma Processes, Stick-Breaking, and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most Bayesian nonparametric models in machine learning have focused on\nthe Dirichlet process, the beta process, or their variants, the gamma process\nhas recently emerged as a useful nonparametric prior in its own right. Current\ninference schemes for models involving the gamma process are restricted to\nMCMC-based methods, which limits their scalability. In this paper, we present a\nvariational inference framework for models involving gamma process priors. Our\napproach is based on a novel stick-breaking constructive definition of the\ngamma process. We prove correctness of this stick-breaking process by using the\ncharacterization of the gamma process as a completely random measure (CRM), and\nwe explicitly derive the rate measure of our construction using Poisson process\nmachinery. We also derive error bounds on the truncation of the infinite\nprocess required for variational inference, similar to the truncation analyses\nfor other nonparametric models based on the Dirichlet and beta processes. Our\nrepresentation is then used to derive a variational inference algorithm for a\nparticular Bayesian nonparametric latent structure formulation known as the\ninfinite Gamma-Poisson model, where the latent variables are drawn from a gamma\nprocess prior with Poisson likelihoods. Finally, we present results for our\nalgorithms on nonnegative matrix factorization tasks on document corpora, and\nshow that we compare favorably to both sampling-based techniques and\nvariational approaches based on beta-Bernoulli priors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 17:36:58 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Roychowdhury", "Anirban", ""], ["Kulis", "Brian", ""]]}, {"id": "1410.1141", "submitter": "Roi Livni", "authors": "Roi Livni and Shai Shalev-Shwartz and Ohad Shamir", "title": "On the Computational Efficiency of Training Neural Networks", "comments": "Section 2 is revised due to a mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that neural networks are computationally hard to train. On\nthe other hand, in practice, modern day neural networks are trained efficiently\nusing SGD and a variety of tricks that include different activation functions\n(e.g. ReLU), over-specification (i.e., train networks which are larger than\nneeded), and regularization. In this paper we revisit the computational\ncomplexity of training neural networks from a modern perspective. We provide\nboth positive and negative results, some of them yield new provably efficient\nand practical algorithms for training certain types of neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 10:54:07 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 19:14:37 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Livni", "Roi", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1410.1174", "submitter": "Yiyuan She", "authors": "Yiyuan She, Yuejia He, and Dapeng Wu", "title": "Learning Topology and Dynamics of Large Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2358956", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale recurrent networks have drawn increasing attention recently\nbecause of their capabilities in modeling a large variety of real-world\nphenomena and physical mechanisms. This paper studies how to identify all\nauthentic connections and estimate system parameters of a recurrent network,\ngiven a sequence of node observations. This task becomes extremely challenging\nin modern network applications, because the available observations are usually\nvery noisy and limited, and the associated dynamical system is strongly\nnonlinear. By formulating the problem as multivariate sparse sigmoidal\nregression, we develop simple-to-implement network learning algorithms, with\nrigorous convergence guarantee in theory, for a variety of sparsity-promoting\npenalty forms. A quantile variant of progressive recurrent network screening is\nproposed for efficient computation and allows for direct cardinality control of\nnetwork topology in estimation. Moreover, we investigate recurrent network\nstability conditions in Lyapunov's sense, and integrate such stability\nconstraints into sparse network learning. Experiments show excellent\nperformance of the proposed algorithms in network topology identification and\nforecasting.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 16:46:04 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["She", "Yiyuan", ""], ["He", "Yuejia", ""], ["Wu", "Dapeng", ""]]}, {"id": "1410.1184", "submitter": "Alexander Jung", "authors": "Alexander Jung, Gabor Hannak and Norbert G\\\"ortz", "title": "Graphical LASSO Based Model Selection for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel graphical model selection (GMS) scheme for\nhigh-dimensional stationary time series or discrete time process. The method is\nbased on a natural generalization of the graphical LASSO (gLASSO), introduced\noriginally for GMS based on i.i.d. samples, and estimates the conditional\nindependence graph (CIG) of a time series from a finite length observation. The\ngLASSO for time series is defined as the solution of an l1-regularized maximum\n(approximate) likelihood problem. We solve this optimization problem using the\nalternating direction method of multipliers (ADMM). Our approach is\nnonparametric as we do not assume a finite dimensional (e.g., an\nautoregressive) parametric model for the observed process. Instead, we require\nthe process to be sufficiently smooth in the spectral domain. For Gaussian\nprocesses, we characterize the performance of our method theoretically by\nderiving an upper bound on the probability that our algorithm fails to\ncorrectly identify the CIG. Numerical experiments demonstrate the ability of\nour method to recover the correct CIG from a limited amount of samples.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 17:57:23 GMT"}, {"version": "v2", "created": "Sun, 19 Oct 2014 19:59:48 GMT"}, {"version": "v3", "created": "Tue, 28 Oct 2014 19:52:52 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Jung", "Alexander", ""], ["Hannak", "Gabor", ""], ["G\u00f6rtz", "Norbert", ""]]}, {"id": "1410.1771", "submitter": "Nicolas Chopin", "authors": "James Ridgway and Pierre Alquier and Nicolas Chopin and Feng Liang", "title": "PAC-Bayesian AUC classification and scoring", "comments": "Accepted at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scoring and classification procedure based on the PAC-Bayesian\napproach and the AUC (Area Under Curve) criterion. We focus initially on the\nclass of linear score functions. We derive PAC-Bayesian non-asymptotic bounds\nfor two types of prior for the score parameters: a Gaussian prior, and a\nspike-and-slab prior; the latter makes it possible to perform feature\nselection. One important advantage of our approach is that it is amenable to\npowerful Bayesian computational tools. We derive in particular a Sequential\nMonte Carlo algorithm, as an efficient method which may be used as a gold\nstandard, and an Expectation-Propagation algorithm, as a much faster but\napproximate method. We also extend our method to a class of non-linear score\nfunctions, essentially leading to a nonparametric procedure, by considering a\nGaussian process prior.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 15:27:56 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 09:13:13 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Ridgway", "James", ""], ["Alquier", "Pierre", ""], ["Chopin", "Nicolas", ""], ["Liang", "Feng", ""]]}, {"id": "1410.2046", "submitter": "Lan Jiang", "authors": "Lan Jiang, Sumeetpal S. Singh, Sinan Y{\\i}ld{\\i}r{\\i}m", "title": "Bayesian tracking and parameter learning for non-linear multiple target\n  tracking models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2454474", "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Bayesian tracking and parameter learning algorithm for\nnon-linear non-Gaussian multiple target tracking (MTT) models. We design a\nMarkov chain Monte Carlo (MCMC) algorithm to sample from the posterior\ndistribution of the target states, birth and death times, and association of\nobservations to targets, which constitutes the solution to the tracking\nproblem, as well as the model parameters. In the numerical section, we present\nperformance comparisons with several competing techniques and demonstrate\nsignificant performance improvements in all cases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 10:02:06 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""]]}, {"id": "1410.2455", "submitter": "Stephan Gouws", "authors": "Stephan Gouws, Yoshua Bengio, Greg Corrado", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word\n  Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data. This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 13:41:18 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 20:52:32 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 05:51:59 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Gouws", "Stephan", ""], ["Bengio", "Yoshua", ""], ["Corrado", "Greg", ""]]}, {"id": "1410.2479", "submitter": "Andreas Schwarz", "authors": "Andreas Schwarz, Christian Huemmer, Roland Maas, Walter Kellermann", "title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy\n  and Reverberant Environments", "comments": "accepted for ICASSP2015", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178798", "report-no": null, "categories": "cs.CL cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a spatial diffuseness feature for deep neural network (DNN)-based\nautomatic speech recognition to improve recognition accuracy in reverberant and\nnoisy environments. The feature is computed in real-time from multiple\nmicrophone signals without requiring knowledge or estimation of the direction\nof arrival, and represents the relative amount of diffuse noise in each time\nand frequency bin. It is shown that using the diffuseness feature as an\nadditional input to a DNN-based acoustic model leads to a reduced word error\nrate for the REVERB challenge corpus, both compared to logmelspec features\nextracted from noisy signals, and features enhanced by spectral subtraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:15:42 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 13:54:06 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Schwarz", "Andreas", ""], ["Huemmer", "Christian", ""], ["Maas", "Roland", ""], ["Kellermann", "Walter", ""]]}, {"id": "1410.2500", "submitter": "Eric Bax", "authors": "Eric Bax, Lingjie Weng, Xu Tian", "title": "Speculate-Correct Error Bounds for k-Nearest Neighbor Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the speculate-correct method to derive error bounds for local\nclassifiers. Using it, we show that k nearest neighbor classifiers, in spite of\ntheir famously fractured decision boundaries, have exponential error bounds\nwith O(sqrt((k + ln n) / n)) error bound range for n in-sample examples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 15:08:57 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:52:02 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 05:10:42 GMT"}, {"version": "v4", "created": "Sat, 7 Jan 2017 17:27:11 GMT"}, {"version": "v5", "created": "Mon, 27 Feb 2017 20:20:36 GMT"}, {"version": "v6", "created": "Fri, 15 Sep 2017 23:31:16 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bax", "Eric", ""], ["Weng", "Lingjie", ""], ["Tian", "Xu", ""]]}, {"id": "1410.2596", "submitter": "Trevor Hastie", "authors": "Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh", "title": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix-completion problem has attracted a lot of attention, largely as a\nresult of the celebrated Netflix competition. Two popular approaches for\nsolving the problem are nuclear-norm-regularized matrix approximation (Candes\nand Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-margin\nmatrix factorization (Srebro, Rennie and Jaakkola, 2005). These two procedures\nare in some cases solving equivalent problems, but with quite different\nalgorithms. In this article we bring the two approaches together, leading to an\nefficient algorithm for large matrix factorization and completion that\noutperforms both of these. We develop a software package \"softImpute\" in R for\nimplementing our approaches, and a distributed version for very large matrices\nusing the \"Spark\" cluster programming environment.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 19:48:00 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Hastie", "Trevor", ""], ["Mazumder", "Rahul", ""], ["Lee", "Jason", ""], ["Zadeh", "Reza", ""]]}, {"id": "1410.2653", "submitter": "Qiang Liu", "authors": "Qiang Liu and Alexander Ihler", "title": "Distributed Estimation, Information Loss and Exponential Families", "comments": "To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning of probabilistic models from multiple data repositories\nwith minimum communication is increasingly important. We study a simple\ncommunication-efficient learning framework that first calculates the local\nmaximum likelihood estimates (MLE) based on the data subsets, and then combines\nthe local MLEs to achieve the best possible approximation to the global MLE\ngiven the whole dataset. We study this framework's statistical properties,\nshowing that the efficiency loss compared to the global setting relates to how\nmuch the underlying distribution families deviate from full exponential\nfamilies, drawing connection to the theory of information loss by Fisher, Rao\nand Efron. We show that the \"full-exponential-family-ness\" represents the lower\nbound of the error rate of arbitrary combinations of local MLEs, and is\nachieved by a KL-divergence-based combination method but not by a more common\nlinear combination method. We also study the empirical properties of both\nmethods, showing that the KL method significantly outperforms linear\ncombination in practical settings with issues such as model misspecification,\nnon-convexity, and heterogeneous data partitions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 23:24:07 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1410.2724", "submitter": "Joao Mota", "authors": "Jo\\~ao F. C. Mota, Nikos Deligiannis, and Miguel R. D. Rodrigues", "title": "Compressed Sensing With Side Information: Geometrical Interpretation and\n  Performance Bounds", "comments": "This paper, to be presented at GlobalSIP 2014, is a shorter version\n  of http://arxiv.org/abs/1408.5250", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Compressed Sensing (CS) with side information.\nNamely, when reconstructing a target CS signal, we assume access to a similar\nsignal. This additional knowledge, the side information, is integrated into CS\nvia L1-L1 and L1-L2 minimization. We then provide lower bounds on the number of\nmeasurements that these problems require for successful reconstruction of the\ntarget signal. If the side information has good quality, the number of\nmeasurements is significantly reduced via L1-L1 minimization, but not so much\nvia L1-L2 minimization. We provide geometrical interpretations and experimental\nresults illustrating our findings.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 10:06:03 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Mota", "Jo\u00e3o F. C.", ""], ["Deligiannis", "Nikos", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1410.2954", "submitter": "Biao Luo", "authors": "Biao Luo and Derong Liu and Tingwen Huang", "title": "Q-learning for Optimal Control of Continuous-time Systems", "comments": "Submitted for Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two Q-learning (QL) methods are proposed and their convergence\ntheories are established for addressing the model-free optimal control problem\nof general nonlinear continuous-time systems. By introducing the Q-function for\ncontinuous-time systems, policy iteration based QL (PIQL) and value iteration\nbased QL (VIQL) algorithms are proposed for learning the optimal control policy\nfrom real system data rather than using mathematical system model. It is proved\nthat both PIQL and VIQL methods generate a nonincreasing Q-function sequence,\nwhich converges to the optimal Q-function. For implementation of the QL\nalgorithms, the method of weighted residuals is applied to derived the\nparameters update rule. The developed PIQL and VIQL algorithms are essentially\noff-policy reinforcement learning approachs, where the system data can be\ncollected arbitrary and thus the exploration ability is increased. With the\ndata collected from the real system, the QL methods learn the optimal control\npolicy offline, and then the convergent control policy will be employed to real\nsystem. The effectiveness of the developed QL algorithms are verified through\ncomputer simulation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Oct 2014 06:06:20 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Luo", "Biao", ""], ["Liu", "Derong", ""], ["Huang", "Tingwen", ""]]}, {"id": "1410.3111", "submitter": "Mijung Park", "authors": "Mijung Park and Jakob H. Macke", "title": "Hierarchical models for neural population dynamics in the presence of\n  non-stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural population activity often exhibits rich variability and temporal\nstructure. This variability is thought to arise from single-neuron\nstochasticity, neural dynamics on short time-scales, as well as from\nmodulations of neural firing properties on long time-scales, often referred to\nas \"non-stationarity\". To better understand the nature of co-variability in\nneural circuits and their impact on cortical information processing, we need\nstatistical models that are able to capture multiple sources of variability on\ndifferent time-scales. Here, we introduce a hierarchical statistical model of\nneural population activity which models both neural population dynamics as well\nas inter-trial modulations in firing rates. In addition, we extend the model to\nallow us to capture non-stationarities in the population dynamics itself (i.e.,\ncorrelations across neurons).\n  We develop variational inference methods for learning model parameters, and\ndemonstrate that the method can recover non-stationarities in both average\nfiring rates and correlation structure. Applied to neural population recordings\nfrom anesthetized macaque primary visual cortex, our models provide a better\naccount of the structure of neural firing than stationary dynamics models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 16:07:22 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Park", "Mijung", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1410.3169", "submitter": "Ellen Gasparovic", "authors": "Paul Bendich, Ellen Gasparovic, John Harer, Rauf Izmailov, and Linda\n  Ness", "title": "Multi-Scale Local Shape Analysis and Feature Selection in Machine\n  Learning Applications", "comments": "15 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method called multi-scale local shape analysis, or MLSA, for\nextracting features that describe the local structure of points within a\ndataset. The method uses both geometric and topological features at multiple\nlevels of granularity to capture diverse types of local information for\nsubsequent machine learning algorithms operating on the dataset. Using\nsynthetic and real dataset examples, we demonstrate significant performance\nimprovement of classification algorithms constructed for these datasets with\ncorrespondingly augmented features.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 00:21:59 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Bendich", "Paul", ""], ["Gasparovic", "Ellen", ""], ["Harer", "John", ""], ["Izmailov", "Rauf", ""], ["Ness", "Linda", ""]]}, {"id": "1410.3192", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Learning without Concentration for General Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study prediction and estimation problems using empirical risk\nminimization, relative to a general convex loss function. We obtain sharp error\nrates even when concentration is false or is very restricted, for example, in\nheavy-tailed scenarios. Our results show that the error rate depends on two\nparameters: one captures the intrinsic complexity of the class, and essentially\nleads to the error rate in a noise-free (or realizable) problem; the other\nmeasures interactions between class members the target and the loss, and is\ndominant when the problem is far from realizable. We also explain how one may\ndeal with outliers by choosing the loss in a way that is calibrated to the\nintrinsic complexity of the class and to the noise-level of the problem (the\nlatter is measured by the distance between the target and the class).\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 05:37:29 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1410.3234", "submitter": "Ao Kong", "authors": "Ao Kong and Robert Azencott", "title": "Markov Random Fields and Mass Spectra Discrimination", "comments": "43pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mass spectra acquired from cancer patients by MALDI or SELDI techniques,\nautomated discrimination between cancer types or stages has often been\nimplemented by machine learnings. These techniques typically generate\n\"black-box\" classifiers, which are difficult to interpret biologically. We\ndevelop new and efficient signature discovery algorithms leading to\ninterpretable signatures combining the discriminating power of explicitly\nselected small groups of biomarkers, identified by their m/z ratios. Our\napproach is based on rigorous stochastic modeling of \"homogeneous\" datasets of\nmass spectra by a versatile class of parameterized Markov Random Fields. We\npresent detailed algorithms validated by precise theoretical results. We also\noutline the successful tests of our approach to generate efficient explicit\nsignatures for six benchmark discrimination tasks, based on mass spectra\nacquired from colorectal cancer patients, as well as from ovarian cancer\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 09:31:36 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Kong", "Ao", ""], ["Azencott", "Robert", ""]]}, {"id": "1410.3314", "submitter": "Roman Garnett", "authors": "Marion Neumann and Roman Garnett and Christian Bauckhage and Kristian\n  Kersting", "title": "Propagation Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce propagation kernels, a general graph-kernel framework for\nefficiently measuring the similarity of structured data. Propagation kernels\nare based on monitoring how information spreads through a set of given graphs.\nThey leverage early-stage distributions from propagation schemes such as random\nwalks to capture structural information encoded in node labels, attributes, and\nedge information. This has two benefits. First, off-the-shelf propagation\nschemes can be used to naturally construct kernels for many graph types,\nincluding labeled, partially labeled, unlabeled, directed, and attributed\ngraphs. Second, by leveraging existing efficient and informative propagation\nschemes, propagation kernels can be considerably faster than state-of-the-art\napproaches without sacrificing predictive performance. We will also show that\nif the graphs at hand have a regular structure, for instance when modeling\nimage or video data, one can exploit this regularity to scale the kernel\ncomputation to large databases of graphs with thousands of nodes. We support\nour contributions by exhaustive experiments on a number of real-world graphs\nfrom a variety of application domains.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 14:04:15 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Neumann", "Marion", ""], ["Garnett", "Roman", ""], ["Bauckhage", "Christian", ""], ["Kersting", "Kristian", ""]]}, {"id": "1410.3348", "submitter": "Ilya Safro", "authors": "Talayeh Razzaghi and Ilya Safro", "title": "Fast Multilevel Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving different types of optimization models (including parameters fitting)\nfor support vector machines on large-scale training data is often an expensive\ncomputational task. This paper proposes a multilevel algorithmic framework that\nscales efficiently to very large data sets. Instead of solving the whole\ntraining set in one optimization process, the support vectors are obtained and\ngradually refined at multiple levels of coarseness of the data. The proposed\nframework includes: (a) construction of hierarchy of large-scale data coarse\nrepresentations, and (b) a local processing of updating the hyperplane\nthroughout this hierarchy. Our multilevel framework substantially improves the\ncomputational time without loosing the quality of classifiers. The algorithms\nare demonstrated for both regular and weighted support vector machines.\nExperimental results are presented for balanced and imbalanced classification\nproblems. Quality improvement on several imbalanced data sets has been\nobserved.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:27:45 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Safro", "Ilya", ""]]}, {"id": "1410.3351", "submitter": "Antonio Ache", "authors": "Antonio G. Ache and Micah W. Warren", "title": "Ricci Curvature and the Manifold Learning Problem", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.LG math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a sample of $n$ points taken i.i.d from a submanifold $\\Sigma$ of\nEuclidean space. We show that there is a way to estimate the Ricci curvature of\n$\\Sigma$ with respect to the induced metric from the sample. Our method is\ngrounded in the notions of Carr\\'e du Champ for diffusion semi-groups, the\ntheory of Empirical processes and local Principal Component Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:37:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 14:38:23 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 05:06:39 GMT"}, {"version": "v4", "created": "Fri, 20 Oct 2017 02:11:21 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 20:47:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Ache", "Antonio G.", ""], ["Warren", "Micah W.", ""]]}, {"id": "1410.3517", "submitter": "Asad Haris", "authors": "Asad Haris, Daniela Witten and Noah Simon", "title": "Convex Modeling of Interactions with Strong Heredity", "comments": "Final version accepted for publication in JCGS", "journal-ref": "Journal of Computational and Graphical Statistics 2016, Vol. 25,\n  No. 4, 981-1004", "doi": "10.1080/10618600.2015.1067217", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of fitting a regression model involving interactions\namong a potentially large set of covariates, in which we wish to enforce strong\nheredity. We propose FAMILY, a very general framework for this task. Our\nproposal is a generalization of several existing methods, such as VANISH\n[Radchenko and James, 2010], hierNet [Bien et al., 2013], the all-pairs lasso,\nand the lasso using only main effects. It can be formulated as the solution to\na convex optimization problem, which we solve using an efficient alternating\ndirections method of multipliers (ADMM) algorithm. This algorithm has\nguaranteed convergence to the global optimum, can be easily specialized to any\nconvex penalty function of interest, and allows for a straightforward extension\nto the setting of generalized linear models. We derive an unbiased estimator of\nthe degrees of freedom of FAMILY, and explore its performance in a simulation\nstudy and on an HIV sequence data set.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 21:24:37 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2015 03:16:47 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Haris", "Asad", ""], ["Witten", "Daniela", ""], ["Simon", "Noah", ""]]}, {"id": "1410.3595", "submitter": "Masahiro Yukawa", "authors": "Masa-aki Takizawa, Masahiro Yukawa, and Cedric Richard", "title": "A stochastic behavior analysis of stochastic restricted-gradient descent\n  algorithm in reproducing kernel Hilbert spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a stochastic behavior analysis of a kernel-based\nstochastic restricted-gradient descent method. The restricted gradient gives a\nsteepest ascent direction within the so-called dictionary subspace. The\nanalysis provides the transient and steady state performance in the mean\nsquared error criterion. It also includes stability conditions in the mean and\nmean-square sense. The present study is based on the analysis of the kernel\nnormalized least mean square (KNLMS) algorithm initially proposed by Chen et\nal. Simulation results validate the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 07:29:35 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Takizawa", "Masa-aki", ""], ["Yukawa", "Masahiro", ""], ["Richard", "Cedric", ""]]}, {"id": "1410.3596", "submitter": "Masayuki Ohzeki", "authors": "Shogo Yamanaka, Masayuki Ohzeki, Aurelien Decelle", "title": "Detection of cheating by decimation algorithm", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": "10.7566/JPSJ.84.024801", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand the item response theory to study the case of \"cheating students\"\nfor a set of exams, trying to detect them by applying a greedy algorithm of\ninference. This extended model is closely related to the Boltzmann machine\nlearning. In this paper we aim to infer the correct biases and interactions of\nour model by considering a relatively small number of sets of training data.\nNevertheless, the greedy algorithm that we employed in the present study\nexhibits good performance with a few number of training data. The key point is\nthe sparseness of the interactions in our problem in the context of the\nBoltzmann machine learning: the existence of cheating students is expected to\nbe very rare (possibly even in real world). We compare a standard approach to\ninfer the sparse interactions in the Boltzmann machine learning to our greedy\nalgorithm and we find the latter to be superior in several aspects.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 07:41:34 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 02:24:07 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Yamanaka", "Shogo", ""], ["Ohzeki", "Masayuki", ""], ["Decelle", "Aurelien", ""]]}, {"id": "1410.3748", "submitter": "Chee Seng Chan", "authors": "Wai Lam Hoo and Chee Seng Chan", "title": "Zero-Shot Object Recognition System based on Topic Model", "comments": "To appear in IEEE Transactions on Human-Machine Systems", "journal-ref": null, "doi": "10.1109/THMS.2014.2358649", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition systems usually require fully complete manually labeled\ntraining data to train the classifier. In this paper, we study the problem of\nobject recognition where the training samples are missing during the classifier\nlearning stage, a task also known as zero-shot learning. We propose a novel\nzero-shot learning strategy that utilizes the topic model and hierarchical\nclass concept. Our proposed method advanced where cumbersome human annotation\nstage (i.e. attribute-based classification) is eliminated. We achieve\ncomparable performance with state-of-the-art algorithms in four public\ndatasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), and\nAnimals with Attributes (49.65%) when unseen classes exist in the\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:11:43 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Hoo", "Wai Lam", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3751", "submitter": "Chee Seng Chan", "authors": "Wei Ren Tan, Chee Seng Chan, Pratheepan Yogarajah and Joan Condell", "title": "A Fusion Approach for Efficient Human Skin Detection", "comments": "Accepted in IEEE Transactions on Industrial Informatics, vol. 8(1),\n  pp. 138-147, new skin detection + ground truth (Pratheepan) dataset", "journal-ref": "IEEE Transactions on Industrial Informatics, vol. 8(1), pp.\n  138-147, 2012", "doi": "10.1109/TII.2011.2172451", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable human skin detection method that is adaptable to different human\nskin colours and illu- mination conditions is essential for better human skin\nsegmentation. Even though different human skin colour detection solutions have\nbeen successfully applied, they are prone to false skin detection and are not\nable to cope with the variety of human skin colours across different ethnic.\nMoreover, existing methods require high computational cost. In this paper, we\npropose a novel human skin de- tection approach that combines a smoothed 2D\nhistogram and Gaussian model, for automatic human skin detection in colour\nimage(s). In our approach an eye detector is used to refine the skin model for\na specific person. The proposed approach reduces computational costs as no\ntraining is required; and it improves the accuracy of skin detection despite\nwide variation in ethnicity and illumination. To the best of our knowledge,\nthis is the first method to employ fusion strategy for this purpose.\nQualitative and quantitative results on three standard public datasets and a\ncomparison with state-of-the-art methods have shown the effectiveness and\nrobustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:12:58 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Tan", "Wei Ren", ""], ["Chan", "Chee Seng", ""], ["Yogarajah", "Pratheepan", ""], ["Condell", "Joan", ""]]}, {"id": "1410.3752", "submitter": "Chee Seng Chan", "authors": "Wai Lam Hoo, Tae-Kyun Kim, Yuru Pei and Chee Seng Chan", "title": "Enhanced Random Forest with Image/Patch-Level Learning for Image\n  Understanding", "comments": "Accepted in ICPR 2014 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image understanding is an important research domain in the computer vision\ndue to its wide real-world applications. For an image understanding framework\nthat uses the Bag-of-Words model representation, the visual codebook is an\nessential part. Random forest (RF) as a tree-structure discriminative codebook\nhas been a popular choice. However, the performance of the RF can be degraded\nif the local patch labels are poorly assigned. In this paper, we tackle this\nproblem by a novel way to update the RF codebook learning for a more\ndiscriminative codebook with the introduction of the soft class labels,\nestimated from the pLSA model based on a feedback scheme. The feedback scheme\nis performed on both the image and patch levels respectively, which is in\ncontrast to the state- of-the-art RF codebook learning that focused on either\nimage or patch level only. Experiments on 15-Scene and C-Pascal datasets had\nshown the effectiveness of the proposed method in image understanding task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:13:45 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Hoo", "Wai Lam", ""], ["Kim", "Tae-Kyun", ""], ["Pei", "Yuru", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3756", "submitter": "Chee Seng Chan", "authors": "Mei Kuan Lim, Ven Jyn Kok, Chen Change Loy and Chee Seng Chan", "title": "Crowd Saliency Detection via Global Similarity Structure", "comments": "Accepted in ICPR 2014 (Oral). Mei Kuan Lim and Ven Jyn Kok share\n  equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for CCTV operators to overlook inter- esting events taking place\nwithin the crowd due to large number of people in the crowded scene (i.e.\nmarathon, rally). Thus, there is a dire need to automate the detection of\nsalient crowd regions acquiring immediate attention for a more effective and\nproactive surveillance. This paper proposes a novel framework to identify and\nlocalize salient regions in a crowd scene, by transforming low-level features\nextracted from crowd motion field into a global similarity structure. The\nglobal similarity structure representation allows the discovery of the\nintrinsic manifold of the motion dynamics, which could not be captured by the\nlow-level representation. Ranking is then performed on the global similarity\nstructure to identify a set of extrema. The proposed approach is unsupervised\nso learning stage is eliminated. Experimental results on public datasets\ndemonstrates the effectiveness of exploiting such extrema in identifying\nsalient regions in various crowd scenarios that exhibit crowding, local\nirregular motion, and unique motion areas such as sources and sinks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:24:24 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Lim", "Mei Kuan", ""], ["Kok", "Ven Jyn", ""], ["Loy", "Chen Change", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1410.3831", "submitter": "Pankaj Mehta", "authors": "Pankaj Mehta and David J. Schwab", "title": "An exact mapping between the Variational Renormalization Group and Deep\n  Learning", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 20:00:09 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Mehta", "Pankaj", ""], ["Schwab", "David J.", ""]]}, {"id": "1410.3886", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi", "title": "Tighter Low-rank Approximation via Sampling the Leveraged Element", "comments": "36 pages, 3 figures, Extended abstract to appear in the proceedings\n  of ACM-SIAM Symposium on Discrete Algorithms (SODA15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers).\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 22:41:20 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1410.3916", "submitter": "Jason  Weston", "authors": "Jason Weston, Sumit Chopra, Antoine Bordes", "title": "Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a new class of learning models called memory networks. Memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. The long-term memory can be\nread and written to, with the goal of using it for prediction. We investigate\nthese models in the context of question answering (QA) where the long-term\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\nmore complex, toy task generated from a simulated world. In the latter, we show\nthe reasoning power of such models by chaining multiple supporting sentences to\nanswer questions that require understanding the intension of verbs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 03:13:18 GMT"}, {"version": "v10", "created": "Tue, 19 May 2015 21:48:30 GMT"}, {"version": "v11", "created": "Sun, 29 Nov 2015 07:00:41 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 16:33:51 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 03:53:26 GMT"}, {"version": "v4", "created": "Wed, 24 Dec 2014 21:56:07 GMT"}, {"version": "v5", "created": "Wed, 21 Jan 2015 07:26:45 GMT"}, {"version": "v6", "created": "Sat, 7 Feb 2015 01:25:50 GMT"}, {"version": "v7", "created": "Wed, 25 Feb 2015 04:08:49 GMT"}, {"version": "v8", "created": "Sat, 7 Mar 2015 15:29:16 GMT"}, {"version": "v9", "created": "Thu, 9 Apr 2015 20:45:05 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Weston", "Jason", ""], ["Chopra", "Sumit", ""], ["Bordes", "Antoine", ""]]}, {"id": "1410.4009", "submitter": "Dean Eckles", "authors": "Dean Eckles and Maurits Kaptein", "title": "Thompson sampling with the online bootstrap", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling provides a solution to bandit problems in which new\nobservations are allocated to arms with the posterior probability that an arm\nis optimal. While sometimes easy to implement and asymptotically optimal,\nThompson sampling can be computationally demanding in large scale bandit\nproblems, and its performance is dependent on the model fit to the observed\ndata. We introduce bootstrap Thompson sampling (BTS), a heuristic method for\nsolving bandit problems which modifies Thompson sampling by replacing the\nposterior distribution used in Thompson sampling by a bootstrap distribution.\nWe first explain BTS and show that the performance of BTS is competitive to\nThompson sampling in the well-studied Bernoulli bandit case. Subsequently, we\ndetail why BTS using the online bootstrap is more scalable than regular\nThompson sampling, and we show through simulation that BTS is more robust to a\nmisspecified error distribution. BTS is an appealing modification of Thompson\nsampling, especially when samples from the posterior are otherwise not\navailable or are costly.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 11:01:52 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Eckles", "Dean", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1410.4062", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Johan Suykens", "title": "Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms\n  for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe algorithms for convex minimization have recently gained\nconsiderable attention from the Optimization and Machine Learning communities,\nas their properties make them a suitable choice in a variety of applications.\nHowever, as each iteration requires to optimize a linear model, a clever\nimplementation is crucial to make such algorithms viable on large-scale\ndatasets. For this purpose, approximation strategies based on a random sampling\nhave been proposed by several researchers. In this work, we perform an\nexperimental study on the effectiveness of these techniques, analyze possible\nalternatives and provide some guidelines based on our results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 13:50:34 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Suykens", "Johan", ""]]}, {"id": "1410.4355", "submitter": "Erik Ferragut", "authors": "Robert A. Bridges, John Collins, Erik M. Ferragut, Jason Laska, Blair\n  D. Sullivan", "title": "Multi-Level Anomaly Detection on Time-Varying Graph Data", "comments": "8 pages. Updated paper to address reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel modeling and analysis framework for graph\nsequences which addresses the challenge of detecting and contextualizing\nanomalies in labelled, streaming graph data. We introduce a generalization of\nthe BTER model of Seshadhri et al. by adding flexibility to community\nstructure, and use this model to perform multi-scale graph anomaly detection.\nSpecifically, probability models describing coarse subgraphs are built by\naggregating probabilities at finer levels, and these closely related\nhierarchical models simultaneously detect deviations from expectation. This\ntechnique provides insight into a graph's structure and internal context that\nmay shed light on a detected event. Additionally, this multi-scale analysis\nfacilitates intuitive visualizations by allowing users to narrow focus from an\nanomalous graph to particular subgraphs or nodes causing the anomaly.\n  For evaluation, two hierarchical anomaly detectors are tested against a\nbaseline Gaussian method on a series of sampled graphs. We demonstrate that our\ngraph statistics-based approach outperforms both a distribution-based detector\nand the baseline in a labeled setting with community structure, and it\naccurately detects anomalies in synthetic and real-world datasets at the node,\nsubgraph, and graph levels. To illustrate the accessibility of information made\npossible via this technique, the anomaly detector and an associated interactive\nvisualization tool are tested on NCAA football data, where teams and\nconferences that moved within the league are identified with perfect recall,\nand precision greater than 0.786.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 09:57:20 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 19:08:37 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 16:58:08 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2015 11:55:53 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bridges", "Robert A.", ""], ["Collins", "John", ""], ["Ferragut", "Erik M.", ""], ["Laska", "Jason", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1410.4391", "submitter": "Justin Bedo", "authors": "Justin Bedo and Cheng Soon Ong", "title": "Multivariate Spearman's rho for aggregating ranks using copulas", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(201):1-30, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of rank aggregation: given a set of ranked lists, we\nwant to form a consensus ranking. Furthermore, we consider the case of extreme\nlists: i.e., only the rank of the best or worst elements are known. We impute\nmissing ranks by the average value and generalise Spearman's \\rho to extreme\nranks. Our main contribution is the derivation of a non-parametric estimator\nfor rank aggregation based on multivariate extensions of Spearman's \\rho, which\nmeasures correlation between a set of ranked lists. Multivariate Spearman's\n\\rho is defined using copulas, and we show that the geometric mean of\nnormalised ranks maximises multivariate correlation. Motivated by this, we\npropose a weighted geometric mean approach for learning to rank which has a\nclosed form least squares solution. When only the best or worst elements of a\nranked list are known, we impute the missing ranks by the average value,\nallowing us to apply Spearman's \\rho. Finally, we demonstrate good performance\non the rank aggregation benchmarks MQ2007 and MQ2008.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 12:15:17 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 22:59:24 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 06:44:16 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 00:05:32 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Bedo", "Justin", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1410.4510", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez and Byron Wallace and Ryan Adams", "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originally designed to model text, topic modeling has become a powerful tool\nfor uncovering latent structure in domains including medicine, finance, and\nvision. The goals for the model vary depending on the application: in some\ncases, the discovered topics may be used for prediction or some other\ndownstream task. In other cases, the content of the topic itself may be of\nintrinsic scientific interest.\n  Unfortunately, even using modern sparse techniques, the discovered topics are\noften difficult to interpret due to the high dimensionality of the underlying\nspace. To improve topic interpretability, we introduce Graph-Sparse LDA, a\nhierarchical topic model that leverages knowledge of relationships between\nwords (e.g., as encoded by an ontology). In our model, topics are summarized by\na few latent concept-words from the underlying graph that explain the observed\nwords. Graph-Sparse LDA recovers sparse, interpretable summaries on two\nreal-world biomedical datasets while matching state-of-the-art prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 17:35:31 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 16:38:59 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Wallace", "Byron", ""], ["Adams", "Ryan", ""]]}, {"id": "1410.4599", "submitter": "Erte Pan", "authors": "Erte Pan and Zhu Han", "title": "Non-parametric Bayesian Learning with Deep Learning Structure and Its\n  Applications in Wireless Networks", "comments": "5 pages, 2 figures and 1 algorithm list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an infinite hierarchical non-parametric Bayesian\nmodel to extract the hidden factors over observed data, where the number of\nhidden factors for each layer is unknown and can be potentially infinite.\nMoreover, the number of layers can also be infinite. We construct the model\nstructure that allows continuous values for the hidden factors and weights,\nwhich makes the model suitable for various applications. We use the\nMetropolis-Hastings method to infer the model structure. Then the performance\nof the algorithm is evaluated by the experiments. Simulation results show that\nthe model fits the underlying structure of simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 22:29:12 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 21:55:30 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Pan", "Erte", ""], ["Han", "Zhu", ""]]}, {"id": "1410.4622", "submitter": "Alireza Dirafzoon", "authors": "Alireza Dirafzoon and Edgar Lobaton", "title": "Robust Topological Feature Extraction for Mapping of Environments using\n  Bio-Inspired Sensor Networks", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit minimal sensing information gathered from\nbiologically inspired sensor networks to perform exploration and mapping in an\nunknown environment. A probabilistic motion model of mobile sensing nodes,\ninspired by motion characteristics of cockroaches, is utilized to extract weak\nencounter information in order to build a topological representation of the\nenvironment.\n  Neighbor to neighbor interactions among the nodes are exploited to build\npoint clouds representing spatial features of the manifold characterizing the\nenvironment based on the sampled data.\n  To extract dominant features from sampled data, topological data analysis is\nused to produce persistence intervals for features, to be used for topological\nmapping. In order to improve robustness characteristics of the sampled data\nwith respect to outliers, density based subsampling algorithms are employed.\nMoreover, a robust scale-invariant classification algorithm for persistence\ndiagrams is proposed to provide a quantitative representation of desired\nfeatures in the data. Furthermore, various strategies for defining encounter\nmetrics with different degrees of information regarding agents' motion are\nsuggested to enhance the precision of the estimation and classification\nperformance of the topological method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 03:32:52 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Dirafzoon", "Alireza", ""], ["Lobaton", "Edgar", ""]]}, {"id": "1410.4650", "submitter": "Yilun Wang", "authors": "Yilun Wang and Junjie Zheng and Sheng Zhang and Xujun Duan and Huafu\n  Chen", "title": "Randomized Structural Sparsity via Constrained Block Subsampling for\n  Improved Sensitivity of Discriminative Voxel Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider voxel selection for functional Magnetic Resonance\nImaging (fMRI) brain data with the aim of finding a more complete set of\nprobably correlated discriminative voxels, thus improving interpretation of the\ndiscovered potential biomarkers. The main difficulty in doing this is an\nextremely high dimensional voxel space and few training samples, resulting in\nunreliable feature selection. In order to deal with the difficulty, stability\nselection has received a great deal of attention lately, especially due to its\nfinite sample control of false discoveries and transparent principle for\nchoosing a proper amount of regularization. However, it fails to make explicit\nuse of the correlation property or structural information of these\ndiscriminative features and leads to large false negative rates. In other\nwords, many relevant but probably correlated discriminative voxels are missed.\nThus, we propose a new variant on stability selection \"randomized structural\nsparsity\", which incorporates the idea of structural sparsity. Numerical\nexperiments demonstrate that our method can be superior in controlling for\nfalse negatives while also keeping the control of false positives inherited\nfrom stability selection.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 07:02:47 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 05:47:29 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Yilun", ""], ["Zheng", "Junjie", ""], ["Zhang", "Sheng", ""], ["Duan", "Xujun", ""], ["Chen", "Huafu", ""]]}, {"id": "1410.4744", "submitter": "Martin Takac", "authors": "Jakub Kone\\v{c}n\\'y, Jie Liu, Peter Richt\\'arik, Martin Tak\\'a\\v{c}", "title": "mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal\n  Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mini-batching scheme for improving the theoretical complexity\nand practical performance of semi-stochastic gradient descent applied to the\nproblem of minimizing a strongly convex composite function represented as the\nsum of an average of a large number of smooth convex functions, and simple\nnonsmooth convex function. Our method first performs a deterministic step\n(computation of the gradient of the objective function at the starting point),\nfollowed by a large number of stochastic steps. The process is repeated a few\ntimes with the last iterate becoming the new starting point. The novelty of our\nmethod is in introduction of mini-batching into the computation of stochastic\nsteps. In each step, instead of choosing a single function, we sample $b$\nfunctions, compute their gradients, and compute the direction based on this. We\nanalyze the complexity of the method and show that the method benefits from two\nspeedup effects. First, we prove that as long as $b$ is below a certain\nthreshold, we can reach predefined accuracy with less overall work than without\nmini-batching. Second, our mini-batching scheme admits a simple parallel\nimplementation, and hence is suitable for further acceleration by\nparallelization.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 14:43:43 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Liu", "Jie", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1410.4777", "submitter": "Michael Smith", "authors": "Richard G. Morris and Tony Martinez and Michael R. Smith", "title": "A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output\n  Dependence Learning", "comments": "10 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Output Dependence (MOD) learning is a generalization of standard\nclassification problems that allows for multiple outputs that are dependent on\neach other. A primary issue that arises in the context of MOD learning is that\nfor any given input pattern there can be multiple correct output patterns. This\nchanges the learning task from function approximation to relation\napproximation. Previous algorithms do not consider this problem, and thus\ncannot be readily applied to MOD problems. To perform MOD learning, we\nintroduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) that\nemploys a basic learning model for each output and a modified nearest neighbor\napproach to refine the initial results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 15:55:46 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Morris", "Richard G.", ""], ["Martinez", "Tony", ""], ["Smith", "Michael R.", ""]]}, {"id": "1410.4792", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Rebecca C. Steorts", "title": "Variational Bayes for Merging Noisy Databases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian entity resolution merges together multiple, noisy databases and\nreturns the minimal collection of unique individuals represented, together with\ntheir true, latent record values. Bayesian methods allow flexible generative\nmodels that share power across databases as well as principled quantification\nof uncertainty for queries of the final, resolved database. However, existing\nBayesian methods for entity resolution use Markov monte Carlo method (MCMC)\napproximations and are too slow to run on modern databases containing millions\nor billions of records. Instead, we propose applying variational approximations\nto allow scalable Bayesian inference in these models. We derive a\ncoordinate-ascent approximation for mean-field variational Bayes, qualitatively\ncompare our algorithm to existing methods, note unique challenges for inference\nthat arise from the expected distribution of cluster sizes in entity\nresolution, and discuss directions for future work in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 16:46:45 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Broderick", "Tamara", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1410.4812", "submitter": "Reshad Hosseini", "authors": "Reshad Hosseini, Suvrit Sra, Lucas Theis, Matthias Bethge", "title": "Inference and Mixture Modeling with the Elliptical Gamma Distribution", "comments": "23 pages, 11 figures", "journal-ref": "Computational Statistics & Data Analysis 2016, Vol. 101, 29-43", "doi": "10.1016/j.csda.2016.02.009", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study modeling and inference with the Elliptical Gamma Distribution (EGD).\nWe consider maximum likelihood (ML) estimation for EGD scatter matrices, a task\nfor which we develop new fixed-point algorithms. Our algorithms are efficient\nand converge to global optima despite nonconvexity. Moreover, they turn out to\nbe much faster than both a well-known iterative algorithm of Kent & Tyler\n(1991) and sophisticated manifold optimization algorithms. Subsequently, we\ninvoke our ML algorithms as subroutines for estimating parameters of a mixture\nof EGDs. We illustrate our methods by applying them to model natural image\nstatistics---the proposed EGD mixture model yields the most parsimonious model\namong several competing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 18:19:27 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 07:51:26 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1410.4821", "submitter": "Madeleine Udell", "authors": "Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven\n  Diamond, and Stephen Boyd", "title": "Convex Optimization in Julia", "comments": "To appear in Proceedings of the Workshop on High Performance\n  Technical Computing in Dynamic Languages (HPTCDL) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Convex, a convex optimization modeling framework in\nJulia. Convex translates problems from a user-friendly functional language into\nan abstract syntax tree describing the problem. This concise representation of\nthe global structure of the problem allows Convex to infer whether the problem\ncomplies with the rules of disciplined convex programming (DCP), and to pass\nthe problem to a suitable solver. These operations are carried out in Julia\nusing multiple dispatch, which dramatically reduces the time required to verify\nDCP compliance and to parse a problem into conic form. Convex then\nautomatically chooses an appropriate backend solver to solve the conic form\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 18:53:04 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Udell", "Madeleine", ""], ["Mohan", "Karanveer", ""], ["Zeng", "David", ""], ["Hong", "Jenny", ""], ["Diamond", "Steven", ""], ["Boyd", "Stephen", ""]]}, {"id": "1410.4828", "submitter": "Yao-Liang Yu", "authors": "Yaoliang Yu, Xinhua Zhang, and Dale Schuurmans", "title": "Generalized Conditional Gradient for Sparse Estimation", "comments": "67 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sparsity is an important modeling tool that expands the\napplicability of convex formulations for data analysis, however it also creates\nsignificant challenges for efficient algorithm design. In this paper we\ninvestigate the generalized conditional gradient (GCG) algorithm for solving\nstructured sparse optimization problems---demonstrating that, with some\nenhancements, it can provide a more efficient alternative to current state of\nthe art approaches. After providing a comprehensive overview of the convergence\nproperties of GCG, we develop efficient methods for evaluating polar operators,\na subroutine that is required in each GCG iteration. In particular, we show how\nthe polar operator can be efficiently evaluated in two important scenarios:\ndictionary learning and structured sparse estimation. A further improvement is\nachieved by interleaving GCG with fixed-rank local subspace optimization. A\nseries of experiments on matrix completion, multi-class classification,\nmulti-view dictionary learning and overlapping group lasso shows that the\nproposed method can significantly reduce the training cost of current\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 19:19:29 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Yu", "Yaoliang", ""], ["Zhang", "Xinhua", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1410.4984", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Andreas Damianou, James Hensman, Neil Lawrence", "title": "Gaussian Process Models with Parallelization and GPU acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an extension of Gaussian process (GP) models with\nsophisticated parallelization and GPU acceleration. The parallelization scheme\narises naturally from the modular computational structure w.r.t. datapoints in\nthe sparse Gaussian process formulation. Additionally, the computational\nbottleneck is implemented with GPU acceleration for further speed up. Combining\nboth techniques allows applying Gaussian process models to millions of\ndatapoints. The efficiency of our algorithm is demonstrated with a synthetic\ndataset. Its source code has been integrated into our popular software library\nGPy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 18:12:57 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Dai", "Zhenwen", ""], ["Damianou", "Andreas", ""], ["Hensman", "James", ""], ["Lawrence", "Neil", ""]]}, {"id": "1410.5137", "submitter": "Purushottam Kar", "authors": "Prateek Jain, Ambuj Tewari, Purushottam Kar", "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "comments": "20 pages, 3 figures, To appear in the proceedings of the 28th Annual\n  Conference on Neural Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of M-estimators in generalized linear regression models in high\ndimensional settings requires risk minimization with hard $L_0$ constraints. Of\nthe known methods, the class of projected gradient descent (also known as\niterative hard thresholding (IHT)) methods is known to offer the fastest and\nmost scalable solutions. However, the current state-of-the-art is only able to\nanalyze these methods in extremely restrictive settings which do not hold in\nhigh dimensional statistical models. In this work we bridge this gap by\nproviding the first analysis for IHT-style methods in the high dimensional\nstatistical setting. Our bounds are tight and match known minimax lower bounds.\nOur results rely on a general analysis framework that enables us to analyze\nseveral popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in\nthe high dimensional regression setting. We also extend our analysis to a large\nfamily of \"fully corrective methods\" that includes two-stage and partial\nhard-thresholding algorithms. We show that our results hold for the problem of\nsparse regression, as well as low-rank matrix recovery.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 02:29:27 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 08:45:56 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Jain", "Prateek", ""], ["Tewari", "Ambuj", ""], ["Kar", "Purushottam", ""]]}, {"id": "1410.5362", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik\n  Maharatna", "title": "Prediction of Synchrostate Transitions in EEG Signals Using Markov Chain\n  Models", "comments": "5 pages, 5 figures", "journal-ref": "Signal Processing Letters, IEEE, Volume 22, Issue 2, Pages 149 -\n  152, Feb. 2015", "doi": "10.1109/LSP.2014.2352251", "report-no": null, "categories": "q-bio.NC physics.med-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a stochastic model using the concept of Markov chains for\nthe inter-state transitions of the millisecond order quasi-stable phase\nsynchronized patterns or synchrostates, found in multi-channel\nElectroencephalogram (EEG) signals. First and second order transition\nprobability matrices are estimated for Markov chain modelling from 100 trials\nof 128-channel EEG signals during two different face perception tasks.\nPrediction accuracies with such finite Markov chain models for synchrostate\ntransition are also compared, under a data-partitioning based cross-validation\nscheme.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:28:13 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Oprescu", "Ioana-Anastasia", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1410.5392", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng and Shang-Hua Teng", "title": "Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 18:59:58 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5410", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner\n  Product Search (MIPS)", "comments": "arXiv admin note: text overlap with arXiv:1405.5869", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 19:54:58 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 20:48:36 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1410.5491", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Using Mechanical Turk to Build Machine Translation Evaluation Sets", "comments": "4 pages, 2 tables; appeared in Proceedings of the NAACL HLT 2010\n  Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,\n  June 2010", "journal-ref": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech\n  and Language Data with Amazon's Mechanical Turk, pages 208-211, Los Angeles,\n  California, June 2010. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machine translation (MT) test sets is a relatively expensive task.\nAs MT becomes increasingly desired for more and more language pairs and more\nand more domains, it becomes necessary to build test sets for each case. In\nthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT\ntest sets cheaply. We find that MTurk can be used to make test sets much\ncheaper than professionally-produced test sets. More importantly, in\nexperiments with multiple MT systems, we find that the MTurk-produced test sets\nyield essentially the same conclusions regarding system performance as the\nprofessionally-produced test sets yield.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 22:28:55 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.5518", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Nathan Srebro", "title": "On Symmetric and Asymmetric LSHs for Inner Product Search", "comments": "11 pages, 3 figures, In Proceedings of The 32nd International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 02:00:34 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 21:31:06 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 19:30:35 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1410.5522", "submitter": "Ilias Bilionis", "authors": "Panagiotis Tsilifis, Ilias Bilionis, Ioannis Katsounaros, Nicholas\n  Zabaras", "title": "Variational Reformulation of Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to inverse problems is based on the optimization of a\nmisfit function. Despite its computational appeal, such an approach suffers\nfrom many shortcomings, e.g., non-uniqueness of solutions, modeling prior\nknowledge, etc. The Bayesian formalism to inverse problems avoids most of the\ndifficulties encountered by the optimization approach, albeit at an increased\ncomputational cost. In this work, we use information theoretic arguments to\ncast the Bayesian inference problem in terms of an optimization problem. The\nresulting scheme combines the theoretical soundness of fully Bayesian inference\nwith the computational efficiency of a simple optimization.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 02:34:07 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Bilionis", "Ilias", ""], ["Katsounaros", "Ioannis", ""], ["Zabaras", "Nicholas", ""]]}, {"id": "1410.5684", "submitter": "Saahil Ognawala", "authors": "Saahil Ognawala and Justin Bayer", "title": "Regularizing Recurrent Networks - On Injected Noise and Norm-based\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in parallel processing have lead to a surge in multilayer\nperceptrons' (MLP) applications and deep learning in the past decades.\nRecurrent Neural Networks (RNNs) give additional representational power to\nfeedforward MLPs by providing a way to treat sequential data. However, RNNs are\nhard to train using conventional error backpropagation methods because of the\ndifficulty in relating inputs over many time-steps. Regularization approaches\nfrom MLP sphere, like dropout and noisy weight training, have been\ninsufficiently applied and tested on simple RNNs. Moreover, solutions have been\nproposed to improve convergence in RNNs but not enough to improve the long term\ndependency remembering capabilities thereof.\n  In this study, we aim to empirically evaluate the remembering and\ngeneralization ability of RNNs on polyphonic musical datasets. The models are\ntrained with injected noise, random dropout, norm-based regularizers and their\nrespective performances compared to well-initialized plain RNNs and advanced\nregularization methods like fast-dropout. We conclude with evidence that\ntraining with noise does not improve performance as conjectured by a few works\nin RNN optimization before ours.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 14:36:26 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Ognawala", "Saahil", ""], ["Bayer", "Justin", ""]]}, {"id": "1410.5792", "submitter": "Andrey Bogomolov", "authors": "Andrey Bogomolov, Bruno Lepri, Fabio Pianesi", "title": "Generalized Compression Dictionary Distance as Universal Similarity\n  Measure", "comments": "2014 Conference on Big Data from Space (BiDS 14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new similarity measure based on information theoretic measures\nwhich is superior than Normalized Compression Distance for clustering problems\nand inherits the useful properties of conditional Kolmogorov complexity. We\nshow that Normalized Compression Dictionary Size and Normalized Compression\nDictionary Entropy are computationally more efficient, as the need to perform\nthe compression itself is eliminated. Also they scale linearly with exponential\nvector size growth and are content independent. We show that normalized\ncompression dictionary distance is compressor independent, if limited to\nlossless compressors, which gives space for optimizations and implementation\nspeed improvement for real-time and big data applications. The introduced\nmeasure is applicable for machine learning tasks of parameter-free unsupervised\nclustering, supervised learning such as classification and regression, feature\nselection, and is applicable for big data problems with order of magnitude\nspeed increase.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 19:17:19 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bogomolov", "Andrey", ""], ["Lepri", "Bruno", ""], ["Pianesi", "Fabio", ""]]}, {"id": "1410.5816", "submitter": "Andrey Bogomolov", "authors": "Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex\n  (Sandy) Pentland", "title": "Daily Stress Recognition from Mobile Phone Data, Weather Conditions and\n  Individual Traits", "comments": "ACM Multimedia 2014, November 3-7, 2014, Orlando, Florida, USA", "journal-ref": null, "doi": "10.1145/2647868.2654933", "report-no": null, "categories": "cs.CY cs.LG physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has proven that stress reduces quality of life and causes many\ndiseases. For this reason, several researchers devised stress detection systems\nbased on physiological parameters. However, these systems require that\nobtrusive sensors are continuously carried by the user. In our paper, we\npropose an alternative approach providing evidence that daily stress can be\nreliably recognized based on behavioral metrics, derived from the user's mobile\nphone activity and from additional indicators, such as the weather conditions\n(data pertaining to transitory properties of the environment) and the\npersonality traits (data concerning permanent dispositions of individuals). Our\nmultifactorial statistical model, which is person-independent, obtains the\naccuracy score of 72.28% for a 2-class daily stress recognition problem. The\nmodel is efficient to implement for most of multimedia applications due to\nhighly reduced low-dimensional feature space (32d). Moreover, we identify and\ndiscuss the indicators which have strong predictive power.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 18:54:53 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Bogomolov", "Andrey", "", "Sandy"], ["Lepri", "Bruno", "", "Sandy"], ["Ferron", "Michela", "", "Sandy"], ["Pianesi", "Fabio", "", "Sandy"], ["Alex", "", "", "Sandy"], ["Pentland", "", ""]]}, {"id": "1410.5877", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for\n  Statistical Machine Translation", "comments": "11 pages, 14 figures; appeared in Proceedings of the 48th Annual\n  Meeting of the Association for Computational Linguistics, July 2010", "journal-ref": "In Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics, pages 854-864, Uppsala, Sweden, July 2010.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how to improve machine translation systems by adding more\ntranslation data in situations where we already have substantial resources. The\nmain challenge is how to buck the trend of diminishing returns that is commonly\nencountered. We present an active learning-style data solicitation algorithm to\nmeet this challenge. We test it, gathering annotations via Amazon Mechanical\nTurk, and find that we get an order of magnitude increase in performance rates\nof improvement.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 22:55:48 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.5884", "submitter": "Yujia Li", "authors": "Yujia Li and Richard Zemel", "title": "Mean-Field Networks", "comments": "Published in ICML 2014 workshop on Learning Tractable Probabilistic\n  Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The mean field algorithm is a widely used approximate inference algorithm for\ngraphical models whose exact inference is intractable. In each iteration of\nmean field, the approximate marginals for each variable are updated by getting\ninformation from the neighbors. This process can be equivalently converted into\na feedforward network, with each layer representing one iteration of mean field\nand with tied weights on all layers. This conversion enables a few natural\nextensions, e.g. untying the weights in the network. In this paper, we study\nthese mean field networks (MFNs), and use them as inference tools as well as\ndiscriminative models. Preliminary experiment results show that MFNs can learn\nto do inference very efficiently and perform significantly better than mean\nfield as discriminative models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 23:32:24 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Li", "Yujia", ""], ["Zemel", "Richard", ""]]}, {"id": "1410.5920", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Remi Munos", "title": "Active Regression by Stratification", "comments": null, "journal-ref": "S. Sabato and R. Munos, \"Active Regression by Stratification\",\n  Advances in Neural Information Processing Systems (NIPS) 469-477, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning algorithm for parametric linear regression\nwith random design. We provide finite sample convergence guarantees for general\ndistributions in the misspecified model. This is the first active learner for\nthis setting that provably can improve over passive learning. Unlike other\nlearning settings (such as classification), in regression the passive learning\nrate of $O(1/\\epsilon)$ cannot in general be improved upon. Nonetheless, the\nso-called `constant' in the rate of convergence, which is characterized by a\ndistribution-dependent risk, can be improved in many cases. For a given\ndistribution, achieving the optimal risk requires prior knowledge of the\ndistribution. Following the stratification technique advocated in Monte-Carlo\nfunction integration, our active learner approaches the optimal risk using\npiecewise constant approximations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 06:09:58 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""], ["Munos", "Remi", ""]]}, {"id": "1410.6031", "submitter": "Dmitry Kobak", "authors": "Dmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E.\n  Feierstein, Adam Kepecs, Zachary F. Mainen, Ranulfo Romo, Xue-Lian Qi,\n  Naoshige Uchida, Christian K. Machens", "title": "Demixed principal component analysis of population activity in higher\n  cortical areas reveals independent representation of task parameters", "comments": "23 pages, 6 figures + supplementary information (21 pages, 15\n  figures)", "journal-ref": "Elife 5, 2016", "doi": "10.7554/eLife.10989", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons in higher cortical areas, such as the prefrontal cortex, are known to\nbe tuned to a variety of sensory and motor variables. The resulting diversity\nof neural tuning often obscures the represented information. Here we introduce\na novel dimensionality reduction technique, demixed principal component\nanalysis (dPCA), which automatically discovers and highlights the essential\nfeatures in complex population activities. We reanalyze population data from\nthe prefrontal areas of rats and monkeys performing a variety of working memory\nand decision-making tasks. In each case, dPCA summarizes the relevant features\nof the population response in a single figure. The population activity is\ndecomposed into a few demixed components that capture most of the variance in\nthe data and that highlight dynamic tuning of the population to various task\nparameters, such as stimuli, decisions, rewards, etc. Moreover, dPCA reveals\nstrong, condition-independent components of the population activity that remain\nunnoticed with conventional approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 13:21:04 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Kobak", "Dmitry", ""], ["Brendel", "Wieland", ""], ["Constantinidis", "Christos", ""], ["Feierstein", "Claudia E.", ""], ["Kepecs", "Adam", ""], ["Mainen", "Zachary F.", ""], ["Romo", "Ranulfo", ""], ["Qi", "Xue-Lian", ""], ["Uchida", "Naoshige", ""], ["Machens", "Christian K.", ""]]}, {"id": "1410.6095", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos, Georgios B. Giannakis, and Ross Baldick", "title": "Online Energy Price Matrix Factorization for Power Grid Topology\n  Tracking", "comments": "Submitted to the IEEE Trans. on Smart Grid", "journal-ref": null, "doi": "10.1109/TSG.2015.2469098", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid security and open markets are two major smart grid goals. Transparency\nof market data facilitates a competitive and efficient energy environment, yet\nit may also reveal critical physical system information. Recovering the grid\ntopology based solely on publicly available market data is explored here.\nReal-time energy prices are calculated as the Lagrange multipliers of\nnetwork-constrained economic dispatch; that is, via a linear program (LP)\ntypically solved every 5 minutes. Granted the grid Laplacian is a parameter of\nthis LP, one could infer such a topology-revealing matrix upon observing\nsuccessive LP dual outcomes. The matrix of spatio-temporal prices is first\nshown to factor as the product of the inverse Laplacian times a sparse matrix.\nLeveraging results from sparse matrix decompositions, topology recovery schemes\nwith complementary strengths are subsequently formulated. Solvers scalable to\nhigh-dimensional and streaming market data are devised. Numerical validation\nusing real load data on the IEEE 30-bus grid provide useful input for current\nand future market designs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 16:14:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""], ["Baldick", "Ross", ""]]}, {"id": "1410.6131", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova, James Booth and Martin T. Wells", "title": "Penalized versus constrained generalized eigenvalue problems", "comments": "18 pages, 8 figures", "journal-ref": "Journal of Computational and Graphical Statistics 2017, Vol. 26,\n  No. 2, 379-387", "doi": "10.1080/10618600.2016.1172017", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the difference between using an $\\ell_1$ penalty versus an\n$\\ell_1$ constraint in generalized eigenvalue problems, such as principal\ncomponent analysis and discriminant analysis. Our main finding is that an\n$\\ell_1$ penalty may fail to provide very sparse solutions; a severe\ndisadvantage for variable selection that can be remedied by using an $\\ell_1$\nconstraint. Our claims are supported both by empirical evidence and theoretical\nanalysis. Finally, we illustrate the advantages of an $\\ell_1$ constraint in\nthe context of discriminant analysis and principal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 18:30:38 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 14:24:10 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 15:54:42 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Booth", "James", ""], ["Wells", "Martin T.", ""]]}, {"id": "1410.6264", "submitter": "Alessandro Perina", "authors": "Alessandro Perina and Nebojsa Jojic", "title": "Capturing spatial interdependence in image features: the counting grid,\n  an epitomic representation for bags of features", "comments": "The counting grid code is available at www.alessandroperina.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent scene recognition research images or large image regions are often\nrepresented as disorganized \"bags\" of features which can then be analyzed using\nmodels originally developed to capture co-variation of word counts in text.\nHowever, image feature counts are likely to be constrained in different ways\nthan word counts in text. For example, as a camera pans upwards from a building\nentrance over its first few floors and then further up into the sky Fig. 1,\nsome feature counts in the image drop while others rise -- only to drop again\ngiving way to features found more often at higher elevations. The space of all\npossible feature count combinations is constrained both by the properties of\nthe larger scene and the size and the location of the window into it. To\ncapture such variation, in this paper we propose the use of the counting grid\nmodel. This generative model is based on a grid of feature counts, considerably\nlarger than any of the modeled images, and considerably smaller than the real\nestate needed to tile the images next to each other tightly. Each modeled image\nis assumed to have a representative window in the grid in which the feature\ncounts mimic the feature distribution in the image. We provide a learning\nprocedure that jointly maps all images in the training set to the counting grid\nand estimates the appropriate local counts in it. Experimentally, we\ndemonstrate that the resulting representation captures the space of feature\ncount combinations more accurately than the traditional models, not only when\nthe input images come from a panning camera, but even when modeling images of\ndifferent scenes from the same category.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 07:04:22 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Perina", "Alessandro", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1410.6271", "submitter": "Yilun Wang", "authors": "Yilun Wang and Christine A. Shoemaker", "title": "A General Stochastic Algorithmic Framework for Minimizing Expensive\n  Black Box Objective Functions Based on Surrogate Models and Sensitivity\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are focusing on bound constrained global optimization problems, whose\nobjective functions are computationally expensive black-box functions and have\nmultiple local minima. The recently popular Metric Stochastic Response Surface\n(MSRS) algorithm proposed by \\cite{Regis2007SRBF} based on adaptive or\nsequential learning based on response surfaces is revisited and further\nextended for better performance in case of higher dimensional problems.\nSpecifically, we propose a new way to generate the candidate points which the\nnext function evaluation point is picked from according to the metric criteria,\nbased on a new definition of distance, and prove the global convergence of the\ncorresponding. Correspondingly, a more adaptive implementation of MSRS, named\n\"SO-SA\", is presented. \"SO-SA\" is is more likely to perturb those most\nsensitive coordinates when generating the candidate points, instead of\nperturbing all coordinates simultaneously.\n  Numerical experiments on both synthetic problems and real problems\ndemonstrate the advantages of our new algorithm, compared with many state of\nthe art alternatives.}\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 07:42:18 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Wang", "Yilun", ""], ["Shoemaker", "Christine A.", ""]]}, {"id": "1410.6289", "submitter": "Sebastian  Dorn", "authors": "Sebastian Dorn, Torsten A. En{\\ss}lin, Maksim Greiner, Marco Selig,\n  and Vanessa Boehm", "title": "Signal inference with unknown response: Calibration-uncertainty\n  renormalized estimator", "comments": null, "journal-ref": "PhysRevE 91, 013311 (2015)", "doi": "10.1103/PhysRevE.91.013311", "report-no": null, "categories": "physics.data-an astro-ph.IM cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calibration of a measurement device is crucial for every scientific\nexperiment, where a signal has to be inferred from data. We present CURE, the\ncalibration uncertainty renormalized estimator, to reconstruct a signal and\nsimultaneously the instrument's calibration from the same data without knowing\nthe exact calibration, but its covariance structure. The idea of CURE,\ndeveloped in the framework of information field theory, is starting with an\nassumed calibration to successively include more and more portions of\ncalibration uncertainty into the signal inference equations and to absorb the\nresulting corrections into renormalized signal (and calibration) solutions.\nThereby, the signal inference and calibration problem turns into solving a\nsingle system of ordinary differential equations and can be identified with\ncommon resummation techniques used in field theories. We verify CURE by\napplying it to a simplistic toy example and compare it against existent\nself-calibration schemes, Wiener filter solutions, and Markov Chain Monte Carlo\nsampling. We conclude that the method is able to keep up in accuracy with the\nbest self-calibration methods and serves as a non-iterative alternative to it.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 08:38:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 09:42:30 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dorn", "Sebastian", ""], ["En\u00dflin", "Torsten A.", ""], ["Greiner", "Maksim", ""], ["Selig", "Marco", ""], ["Boehm", "Vanessa", ""]]}, {"id": "1410.6382", "submitter": "Doron Kukliansky", "authors": "Doron Kukliansky, Ohad Shamir", "title": "Attribute Efficient Linear Regression with Data-Dependent Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze a budgeted learning setting, in which the learner\ncan only choose and observe a small subset of the attributes of each training\nexample. We develop efficient algorithms for ridge and lasso linear regression,\nwhich utilize the geometry of the data by a novel data-dependent sampling\nscheme. When the learner has prior knowledge on the second moments of the\nattributes, the optimal sampling probabilities can be calculated precisely, and\nresult in data-dependent improvements factors for the excess risk over the\nstate-of-the-art that may be as large as $O(\\sqrt{d})$, where $d$ is the\nproblem's dimension. Moreover, under reasonable assumptions our algorithms can\nuse less attributes than full-information algorithms, which is the main concern\nin budgeted learning settings. To the best of our knowledge, these are the\nfirst algorithms able to do so in our setting. Where no such prior knowledge is\navailable, we develop a simple estimation technique that given a sufficient\namount of training examples, achieves similar improvements. We complement our\ntheoretical analysis with experiments on several data sets which support our\nclaims.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 14:55:09 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Kukliansky", "Doron", ""], ["Shamir", "Ohad", ""]]}, {"id": "1410.6460", "submitter": "Tim Salimans", "authors": "Tim Salimans, Diederik P. Kingma, Max Welling", "title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in stochastic gradient variational inference have made it\npossible to perform variational Bayesian inference with posterior\napproximations containing auxiliary random variables. This enables us to\nexplore a new synthesis of variational inference and Monte Carlo methods where\nwe incorporate one or more steps of MCMC into our variational approximation. By\ndoing so we obtain a rich class of inference algorithms bridging the gap\nbetween variational methods and MCMC, and offering the best of both worlds:\nfast posterior approximation through the maximization of an explicit objective,\nwith the option of trading off additional computation for additional accuracy.\nWe describe the theoretical foundations that make this possible and show some\npromising first results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:23:53 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 12:21:53 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 18:03:43 GMT"}, {"version": "v4", "created": "Tue, 19 May 2015 13:53:13 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Salimans", "Tim", ""], ["Kingma", "Diederik P.", ""], ["Welling", "Max", ""]]}, {"id": "1410.6466", "submitter": "Dehua Cheng", "authors": "Dehua Cheng, Xinran He, Yan Liu", "title": "Model Selection for Topic Models via Spectral Decomposition", "comments": "accepted in AISTATS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have achieved significant successes in analyzing large-scale\ntext corpus. In practical applications, we are always confronted with the\nchallenge of model selection, i.e., how to appropriately set the number of\ntopics. Following recent advances in topic model inference via tensor\ndecomposition, we make a first attempt to provide theoretical analysis on model\nselection in latent Dirichlet allocation. Under mild conditions, we derive the\nupper bound and lower bound on the number of topics given a text collection of\nfinite size. Experimental results demonstrate that our bounds are accurate and\ntight. Furthermore, using Gaussian mixture model as an example, we show that\nour methodology can be easily generalized to model selection analysis for other\nlatent models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 19:38:44 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 01:39:14 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Cheng", "Dehua", ""], ["He", "Xinran", ""], ["Liu", "Yan", ""]]}, {"id": "1410.6604", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Peichao Peng, David Dunson", "title": "Median Selection Subset Aggregation for Parallel Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data sets, efficient computation commonly relies on distributed\nalgorithms that store and process subsets of the data on different machines,\nminimizing communication costs. Our focus is on regression and classification\nproblems involving many features. A variety of distributed algorithms have been\nproposed in this context, but challenges arise in defining an algorithm with\nlow communication, theoretical guarantees and excellent practical performance\nin general settings. We propose a MEdian Selection Subset AGgregation Estimator\n(message) algorithm, which attempts to solve these problems. The algorithm\napplies feature selection in parallel for each subset using Lasso or another\nmethod, calculates the `median' feature inclusion index, estimates coefficients\nfor the selected features in parallel for each subset, and then averages these\nestimates. The algorithm is simple, involves very minimal communication, scales\nefficiently in both sample and feature size, and has theoretical guarantees. In\nparticular, we show model selection consistency and coefficient estimation\nefficiency. Extensive experiments show excellent performance in variable\nselection, estimation, prediction, and computation time relative to usual\ncompetitors.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 07:52:55 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Peng", "Peichao", ""], ["Dunson", "David", ""]]}, {"id": "1410.6714", "submitter": "Li Chen", "authors": "Li Chen and Matthew Patton", "title": "Stochastic Blockmodeling for Online Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising is an important and huge industry. Having knowledge of the\nwebsite attributes can contribute greatly to business strategies for\nad-targeting, content display, inventory purchase or revenue prediction.\nClassical inferences on users and sites impose challenge, because the data is\nvoluminous, sparse, high-dimensional and noisy. In this paper, we introduce a\nstochastic blockmodeling for the website relations induced by the event of\nonline user visitation. We propose two clustering algorithms to discover the\ninstrinsic structures of websites, and compare the performance with a\ngoodness-of-fit method and a deterministic graph partitioning method. We\ndemonstrate the effectiveness of our algorithms on both simulation and AOL\nwebsite dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 15:50:32 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 16:41:28 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Chen", "Li", ""], ["Patton", "Matthew", ""]]}, {"id": "1410.6776", "submitter": "Purushottam Kar", "authors": "Purushottam Kar, Harikrishna Narasimhan, Prateek Jain", "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss\n  Functions", "comments": "25 pages, 3 figures, To appear in the proceedings of the 28th Annual\n  Conference on Neural Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications in sensitive domains such as biometrics and medicine\nfrequently require the use of non-decomposable loss functions such as\nprecision@k, F-measure etc. Compared to point loss functions such as\nhinge-loss, these offer much more fine grained control over prediction, but at\nthe same time present novel challenges in terms of algorithm design and\nanalysis. In this work we initiate a study of online learning techniques for\nsuch non-decomposable loss functions with an aim to enable incremental learning\nas well as design scalable solvers for batch problems. To this end, we propose\nan online learning framework for such loss functions. Our model enjoys several\nnice properties, chief amongst them being the existence of efficient online\nlearning algorithms with sublinear regret and online to batch conversion\nbounds. Our model is a provable extension of existing online learning models\nfor point loss functions. We instantiate two popular losses, prec@k and pAUC,\nin our model and prove sublinear regret bounds for both of them. Our proofs\nrequire a novel structural lemma over ranked lists which may be of independent\ninterest. We then develop scalable stochastic gradient descent solvers for\nnon-decomposable loss functions. We show that for a large family of loss\nfunctions satisfying a certain uniform convergence property (that includes\nprec@k, pAUC, and F-measure), our methods provably converge to the empirical\nrisk minimizer. Such uniform convergence results were not known for these\nlosses and we establish these using novel proof techniques. We then use\nextensive experimentation on real life and benchmark datasets to establish that\nour method can be orders of magnitude faster than a recently proposed cutting\nplane method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 18:45:23 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Kar", "Purushottam", ""], ["Narasimhan", "Harikrishna", ""], ["Jain", "Prateek", ""]]}, {"id": "1410.6791", "submitter": "Mijung Park", "authors": "Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zoltan Szabo, Lars\n  Buesing, Maneesh Sahani", "title": "Bayesian Manifold Learning: The Locally Linear Latent Variable Model\n  (LL-LVM)", "comments": "accepted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Locally Linear Latent Variable Model (LL-LVM), a\nprobabilistic model for non-linear manifold discovery that describes a joint\ndistribution over observations, their manifold coordinates and locally linear\nmaps conditioned on a set of neighbourhood relationships. The model allows\nstraightforward variational optimisation of the posterior distribution on\ncoordinates and locally linear maps from the latent space to the observation\nspace given the data. Thus, the LL-LVM encapsulates the local-geometry\npreserving intuitions that underlie non-probabilistic methods such as locally\nlinear embedding (LLE). Its probabilistic semantics make it easy to evaluate\nthe quality of hypothesised neighbourhood relationships, select the intrinsic\ndimensionality of the manifold, construct out-of-sample extensions and to\ncombine the manifold model with additional probabilistic models that capture\nthe structure of coordinates within the manifold.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 19:17:17 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 13:24:21 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 14:00:04 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 11:11:51 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Park", "Mijung", ""], ["Jitkrittum", "Wittawat", ""], ["Qamar", "Ahmad", ""], ["Szabo", "Zoltan", ""], ["Buesing", "Lars", ""], ["Sahani", "Maneesh", ""]]}, {"id": "1410.6834", "submitter": "Yves-Laurent Kom Samo", "authors": "Yves-Laurent Kom Samo, Stephen Roberts", "title": "Scalable Nonparametric Bayesian Inference on Point Processes with\n  Gaussian Processes", "comments": "To appear at the International Conference on Machine Learning (ICML),\n  2015", "journal-ref": "JMLR W&CP, vol 37, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first non-parametric Bayesian model using\nGaussian Processes to make inference on Poisson Point Processes without\nresorting to gridding the domain or to introducing latent thinning points.\nUnlike competing models that scale cubically and have a squared memory\nrequirement in the number of data points, our model has a linear complexity and\nmemory requirement. We propose an MCMC sampler and show that our model is\nfaster, more accurate and generates less correlated samples than competing\nmodels on both synthetic and real-life data. Finally, we show that our model\neasily handles data sizes not considered thus far by alternate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 20:38:20 GMT"}, {"version": "v2", "created": "Sat, 9 May 2015 21:40:26 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Samo", "Yves-Laurent Kom", ""], ["Roberts", "Stephen", ""]]}, {"id": "1410.6853", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick", "title": "Covariance Matrices for Mean Field Variational Bayes", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 23:31:44 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:07:47 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""]]}, {"id": "1410.6880", "submitter": "Seunghak Lee", "authors": "Seunghak Lee and Eric P. Xing", "title": "Screening Rules for Overlapping Group Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, to solve large-scale lasso and group lasso problems, screening\nrules have been developed, the goal of which is to reduce the problem size by\nefficiently discarding zero coefficients using simple rules independently of\nthe others. However, screening for overlapping group lasso remains an open\nchallenge because the overlaps between groups make it infeasible to test each\ngroup independently. In this paper, we develop screening rules for overlapping\ngroup lasso. To address the challenge arising from groups with overlaps, we\ntake into account overlapping groups only if they are inclusive of the group\nbeing tested, and then we derive screening rules, adopting the dual polytope\nprojection approach. This strategy allows us to screen each group independently\nof each other. In our experiments, we demonstrate the efficiency of our\nscreening rules on various datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 04:06:49 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1410.6959", "submitter": "Zhe Liu", "authors": "Zhe Liu", "title": "An Aggregation Method for Sparse Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $L_1$ regularized logistic regression has now become a workhorse of data\nmining and bioinformatics: it is widely used for many classification problems,\nparticularly ones with many features. However, $L_1$ regularization typically\nselects too many features and that so-called false positives are unavoidable.\nIn this paper, we demonstrate and analyze an aggregation method for sparse\nlogistic regression in high dimensions. This approach linearly combines the\nestimators from a suitable set of logistic models with different underlying\nsparsity patterns and can balance the predictive ability and model\ninterpretability. Numerical performance of our proposed aggregation method is\nthen investigated using simulation studies. We also analyze a published\ngenome-wide case-control dataset to further evaluate the usefulness of the\naggregation method in multilocus association mapping.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 20:52:51 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 02:19:56 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Liu", "Zhe", ""]]}, {"id": "1410.6984", "submitter": "Momiao Xiong", "authors": "Getie Zewdie and Momiao Xiong", "title": "Fully Automated Myocardial Infarction Classification using Ordinary\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portable, Wearable and Wireless electrocardiogram (ECG) Systems have the\npotential to be used as point-of-care for cardiovascular disease diagnostic\nsystems. Such wearable and wireless ECG systems require automatic detection of\ncardiovascular disease. Even in the primary care, automation of ECG diagnostic\nsystems will improve efficiency of ECG diagnosis and reduce the minimal\ntraining requirement of local healthcare workers. However, few fully automatic\nmyocardial infarction (MI) disease detection algorithms have well been\ndeveloped. This paper presents a novel automatic MI classification algorithm\nusing second order ordinary differential equation (ODE) with time varying\ncoefficients, which simultaneously captures morphological and dynamic feature\nof highly correlated ECG signals. By effectively estimating the unobserved\nstate variables and the parameters of the second order ODE, the accuracy of the\nclassification was significantly improved. The estimated time varying\ncoefficients of the second order ODE were used as an input to the support\nvector machine (SVM) for the MI classification. The proposed method was applied\nto the PTB diagnostic ECG database within Physionet. The overall sensitivity,\nspecificity, and classification accuracy of 12 lead ECGs for MI binary\nclassifications were 98.7%, 96.4% and 98.3%, respectively. We also found that\neven using one lead ECG signals, we can reach accuracy as high as 97%.\nMulticlass MI classification is a challenging task but the developed ODE\napproach for 12 lead ECGs coupled with multiclass SVM reached 96.4% accuracy\nfor classifying 5 subgroups of MI and healthy controls.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 04:24:58 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Zewdie", "Getie", ""], ["Xiong", "Momiao", ""]]}, {"id": "1410.6990", "submitter": "Dacheng Tao", "authors": "Chang Xu, Tongliang Liu, Dacheng Tao, Chao Xu", "title": "Local Rademacher Complexity for Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the local Rademacher complexity of empirical risk minimization\n(ERM)-based multi-label learning algorithms, and in doing so propose a new\nalgorithm for multi-label learning. Rather than using the trace norm to\nregularize the multi-label predictor, we instead minimize the tail sum of the\nsingular values of the predictor in multi-label learning. Benefiting from the\nuse of the local Rademacher complexity, our algorithm, therefore, has a sharper\ngeneralization error bound and a faster convergence rate. Compared to methods\nthat minimize over all singular values, concentrating on the tail singular\nvalues results in better recovery of the low-rank structure of the multi-label\npredictor, which plays an import role in exploiting label correlations. We\npropose a new conditional singular value thresholding algorithm to solve the\nresulting objective function. Empirical studies on real-world datasets validate\nour theoretical results and demonstrate the effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 05:52:33 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Xu", "Chang", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1410.6991", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan", "title": "A provable SVD-based algorithm for learning topics in dominant admixture\n  corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents\nare drawn from admixtures of distributions over words, known as topics. The\ninference problem of recovering topics from admixtures, is NP-hard. Assuming\nseparability, a strong assumption, [4] gave the first provable algorithm for\ninference. For LDA model, [6] gave a provable algorithm using tensor-methods.\nBut [4,6] do not learn topic vectors with bounded $l_1$ error (a natural\nmeasure for probability vectors). Our aim is to develop a model which makes\nintuitive and empirically supported assumptions and to design an algorithm with\nnatural, simple components such as SVD, which provably solves the inference\nproblem for the model with bounded $l_1$ error. A topic in LDA and other models\nis essentially characterized by a group of co-occurring words. Motivated by\nthis, we introduce topic specific Catchwords, group of words which occur with\nstrictly greater frequency in a topic than any other topic individually and are\nrequired to have high frequency together rather than individually. A major\ncontribution of the paper is to show that under this more realistic assumption,\nwhich is empirically verified on real corpora, a singular value decomposition\n(SVD) based algorithm with a crucial pre-processing step of thresholding, can\nprovably recover the topics from a collection of documents drawn from Dominant\nadmixtures. Dominant admixtures are convex combination of distributions in\nwhich one distribution has a significantly higher contribution than others.\nApart from the simplicity of the algorithm, the sample complexity has near\noptimal dependence on $w_0$, the lowest probability that a topic is dominant,\nand is better than [4]. Empirical evidence shows that on several real world\ncorpora, both Catchwords and Dominant admixture assumptions hold and the\nproposed algorithm substantially outperforms the state of the art [5].\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 06:00:36 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 17:27:07 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 05:14:25 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Bansal", "Trapit", ""], ["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravindran", ""]]}, {"id": "1410.7029", "submitter": "Momiao Xiong", "authors": "Lerong Li and Momiao Xiong", "title": "A Novel Statistical Method Based on Dynamic Models for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realizations of stochastic process are often observed temporal data or\nfunctional data. There are growing interests in classification of dynamic or\nfunctional data. The basic feature of functional data is that the functional\ndata have infinite dimensions and are highly correlated. An essential issue for\nclassifying dynamic and functional data is how to effectively reduce their\ndimension and explore dynamic feature. However, few statistical methods for\ndynamic data classification have directly used rich dynamic features of the\ndata. We propose to use second order ordinary differential equation (ODE) to\nmodel dynamic process and principal differential analysis to estimate constant\nor time-varying parameters in the ODE. We examine differential dynamic\nproperties of the dynamic system across different conditions including\nstability and transient-response, which determine how the dynamic systems\nmaintain their functions and performance under a broad range of random internal\nand external perturbations. We use the parameters in the ODE as features for\nclassifiers. As a proof of principle, the proposed methods are applied to\nclassifying normal and abnormal QRS complexes in the electrocardiogram (ECG)\ndata analysis, which is of great clinical values in diagnosis of cardiovascular\ndiseases. We show that the ODE-based classification methods in QRS complex\nclassification outperform the currently widely used neural networks with\nFourier expansion coefficients of the functional data as their features. We\nexpect that the dynamic model-based classification methods may open a new\navenue for functional data classification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 13:47:49 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Li", "Lerong", ""], ["Xiong", "Momiao", ""]]}, {"id": "1410.7057", "submitter": "Bijit Kumar Das", "authors": "Bijit Kumar Das, Mrityunjoy Chakraborty and Jer\\'onimo Arenas-Garc\\'ia", "title": "Sparse Distributed Learning via Heterogeneous Diffusion Adaptive\n  Networks", "comments": "4 pages, 1 figure, conference, submitted to IEEE ISCAS 2015, Lisbon,\n  Portugal", "journal-ref": null, "doi": "10.1109/ISCAS.2015.7168664", "report-no": null, "categories": "cs.LG cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-network distributed estimation of sparse parameter vectors via diffusion\nLMS strategies has been studied and investigated in recent years. In all the\nexisting works, some convex regularization approach has been used at each node\nof the network in order to achieve an overall network performance superior to\nthat of the simple diffusion LMS, albeit at the cost of increased computational\noverhead. In this paper, we provide analytical as well as experimental results\nwhich show that the convex regularization can be selectively applied only to\nsome chosen nodes keeping rest of the nodes sparsity agnostic, while still\nenjoying the same optimum behavior as can be realized by deploying the convex\nregularization at all the nodes. Due to the incorporation of unregularized\nlearning at a subset of nodes, less computational cost is needed in the\nproposed approach. We also provide a guideline for selection of the sparsity\naware nodes and a closed form expression for the optimum regularization\nparameter.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 16:38:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Das", "Bijit Kumar", ""], ["Chakraborty", "Mrityunjoy", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""]]}, {"id": "1410.7098", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh and Andre Wibisono", "title": "Concavity of reweighted Kikuchi approximation", "comments": "To appear at the Neural Information Processing Systems (NIPS)\n  conference, December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a reweighted version of the Kikuchi approximation for estimating\nthe log partition function of a product distribution defined over a region\ngraph. We establish sufficient conditions for the concavity of our reweighted\nobjective function in terms of weight assignments in the Kikuchi expansion, and\nshow that a reweighted version of the sum product algorithm applied to the\nKikuchi region graph will produce global optima of the Kikuchi approximation\nwhenever the algorithm converges. When the region graph has two layers,\ncorresponding to a Bethe approximation, we show that our sufficient conditions\nfor concavity are also necessary. Finally, we provide an explicit\ncharacterization of the polytope of concavity in terms of the cycle structure\nof the region graph. We conclude with simulations that demonstrate the\nadvantages of the reweighted Kikuchi approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 23:49:14 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wibisono", "Andre", ""]]}, {"id": "1410.7100", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Estimating the intrinsic dimension in fMRI space via dataset fractal\n  analysis - Counting the `cpu cores' of the human brain", "comments": "27 pages, 10 figures, 2 tables, 47 references", "journal-ref": null, "doi": null, "report-no": "HG/AI.1014.27v1 (draft/preprint)", "categories": "cs.AI cs.CV q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool\nfor localizing and analyzing brain activity. This study focuses on one very\nimportant aspect of the functional properties of human brain, specifically the\nestimation of the level of parallelism when performing complex cognitive tasks.\nUsing fMRI as the main modality, the human brain activity is investigated\nthrough a purely data-driven signal processing and dimensionality analysis\napproach. Specifically, the fMRI signal is treated as a multi-dimensional data\nspace and its intrinsic `complexity' is studied via dataset fractal analysis\nand blind-source separation (BSS) methods. One simulated and two real fMRI\ndatasets are used in combination with Independent Component Analysis (ICA) and\nfractal analysis for estimating the intrinsic (true) dimensionality, in order\nto provide data-driven experimental evidence on the number of independent brain\nprocesses that run in parallel when visual or visuo-motor tasks are performed.\nAlthough this number is can not be defined as a strict threshold but rather as\na continuous range, when a specific activation level is defined, a\ncorresponding number of parallel processes or the casual equivalent of `cpu\ncores' can be detected in normal human brain activity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 00:25:24 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1410.7172", "submitter": "John-Alexander Assael", "authors": "John-Alexander M. Assael, Ziyu Wang, Bobak Shahriari, Nando de Freitas", "title": "Heteroscedastic Treed Bayesian Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimising black-box functions is important in many disciplines, such as\ntuning machine learning models, robotics, finance and mining exploration.\nBayesian optimisation is a state-of-the-art technique for the global\noptimisation of black-box functions which are expensive to evaluate. At the\ncore of this approach is a Gaussian process prior that captures our belief\nabout the distribution over functions. However, in many cases a single Gaussian\nprocess is not flexible enough to capture non-stationarity in the objective\nfunction. Consequently, heteroscedasticity negatively affects performance of\ntraditional Bayesian methods. In this paper, we propose a novel prior model\nwith hierarchical parameter learning that tackles the problem of\nnon-stationarity in Bayesian optimisation. Our results demonstrate substantial\nimprovements in a wide range of applications, including automatic machine\nlearning and mining exploration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 10:28:36 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 20:03:23 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Assael", "John-Alexander M.", ""], ["Wang", "Ziyu", ""], ["Shahriari", "Bobak", ""], ["de Freitas", "Nando", ""]]}, {"id": "1410.7220", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Abhishek Kumar", "title": "Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization", "comments": "22 pages, 6 figures. New: comparison with k-means initialization,\n  numerical results on real data, ill-posedness of semi-NMF", "journal-ref": "SIAM J. Matrix Anal. & Appl. 36 (4), pp. 1404-1424, 2015", "doi": "10.1137/140993272", "report-no": null, "categories": "math.NA cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $M$ (not necessarily nonnegative) and a factorization rank\n$r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$\nwith $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ is\nthe best possible approximation of $M$ according to some metric. In this paper,\nwe study the properties of semi-NMF from which we develop exact and heuristic\nalgorithms. Our contribution is threefold. First, we prove that the error of a\nsemi-NMF of rank $r$ has to be smaller than the best unconstrained\napproximation of rank $r-1$. This leads us to a new initialization procedure\nbased on the singular value decomposition (SVD) with a guarantee on the quality\nof the approximation. Second, we propose an exact algorithm (that is, an\nalgorithm that finds an optimal solution), also based on the SVD, for a certain\nclass of matrices (including nonnegative irreducible matrices) from which we\nderive an initialization for matrices not belonging to that class. Numerical\nexperiments illustrate that this second approach performs extremely well, and\nallows us to compute optimal semi-NMF decompositions in many situations.\nFinally, we analyze the computational complexity of semi-NMF proving its\nNP-hardness, already in the rank-one case (that is, for $r = 1$), and we show\nthat semi-NMF is sometimes ill-posed (that is, an optimal solution does not\nexist).\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:09:29 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 07:05:37 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 05:55:15 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gillis", "Nicolas", ""], ["Kumar", "Abhishek", ""]]}, {"id": "1410.7241", "submitter": "Fabian Wauthier", "authors": "Fabian L. Wauthier and Peter Donnelly", "title": "A Greedy Homotopy Method for Regression with Nonconvex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained least squares regression is an essential tool for\nhigh-dimensional data analysis. Given a partition $\\mathcal{G}$ of input\nvariables, this paper considers a particular class of nonconvex constraint\nfunctions that encourage the linear model to select a small number of variables\nfrom a small number of groups in $\\mathcal{G}$. Such constraints are relevant\nin many practical applications, such as Genome-Wide Association Studies (GWAS).\nMotivated by the efficiency of the Lasso homotopy method, we present RepLasso,\na greedy homotopy algorithm that tries to solve the induced sequence of\nnonconvex problems by solving a sequence of suitably adapted convex surrogate\nproblems. We prove that in some situations RepLasso recovers the global minima\nof the nonconvex problem. Moreover, even if it does not recover global minima,\nwe prove that in relevant cases it will still do no worse than the Lasso in\nterms of support and signed support recovery, while in practice outperforming\nit. We show empirically that the strategy can also be used to improve over\nother Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitis\nhighlights our method's practical utility.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:54:59 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Wauthier", "Fabian L.", ""], ["Donnelly", "Peter", ""]]}, {"id": "1410.7279", "submitter": "Johannes Lederer", "authors": "Johannes Lederer, Christian M\\\"uller", "title": "Topology Adaptive Graph Estimation in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Graphical TREX (GTREX), a novel method for graph estimation in\nhigh-dimensional Gaussian graphical models. By conducting neighborhood\nselection with TREX, GTREX avoids tuning parameters and is adaptive to the\ngraph topology. We compare GTREX with standard methods on a new simulation\nset-up that is designed to assess accurately the strengths and shortcomings of\ndifferent methods. These simulations show that a neighborhood selection scheme\nbased on Lasso and an optimal (in practice unknown) tuning parameter\noutperforms other standard methods over a large spectrum of scenarios.\nMoreover, we show that GTREX can rival this scheme and, therefore, can provide\ncompetitive graph estimation without the need for tuning parameter calibration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 15:41:23 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1410.7291", "submitter": "Yilun Wang", "authors": "Yilun Wang and Christine A. Shoemaker", "title": "Sensitivity Analysis for Computationally Expensive Models using\n  Optimization and Objective-oriented Surrogate Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on developing efficient sensitivity analysis methods\nfor a computationally expensive objective function $f(x)$ in the case that the\nminimization of it has just been performed. Here \"computationally expensive\"\nmeans that each of its evaluation takes significant amount of time, and\ntherefore our main goal to use a small number of function evaluations of $f(x)$\nto further infer the sensitivity information of these different parameters.\nCorrespondingly, we consider the optimization procedure as an adaptive\nexperimental design and re-use its available function evaluations as the\ninitial design points to establish a surrogate model $s(x)$ (or called response\nsurface). The sensitivity analysis is performed on $s(x)$, which is an lieu of\n$f(x)$. Furthermore, we propose a new local multivariate sensitivity measure,\nfor example, around the optimal solution, for high dimensional problems. Then a\ncorresponding \"objective-oriented experimental design\" is proposed in order to\nmake the generated surrogate $s(x)$ better suitable for the accurate\ncalculation of the proposed specific local sensitivity quantities. In addition,\nwe demonstrate the better performance of the Gaussian radial basis function\ninterpolator over Kriging in our cases, which are of relatively high\ndimensionality and few experimental design points. Numerical experiments\ndemonstrate that the optimization procedure and the \"objective-oriented\nexperimental design\" behavior much better than the classical Latin Hypercube\nDesign. In addition, the performance of Kriging is not as good as Gaussian RBF,\nespecially in the case of high dimensional problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 16:11:09 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 18:49:19 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wang", "Yilun", ""], ["Shoemaker", "Christine A.", ""]]}, {"id": "1410.7365", "submitter": "Jussi Gillberg Mr.", "authors": "Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J. Kangas, Pasi\n  Soininen, Mehreen Ali, Aki S. Havulinna, Marjo-Riitta Marjo-Riitta\n  J\\\"arvelin, Mika Ala-Korpela, Samuel Kaski", "title": "Multiple Output Regression with Latent Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data, structured noise caused by observed and unobserved\nfactors affecting multiple target variables simultaneously, imposes a serious\nchallenge for modeling, by masking the often weak signal. Therefore, (1)\nexplaining away the structured noise in multiple-output regression is of\nparamount importance. Additionally, (2) assumptions about the correlation\nstructure of the regression weights are needed. We note that both can be\nformulated in a natural way in a latent variable model, in which both the\ninteresting signal and the noise are mediated through the same latent factors.\nUnder this assumption, the signal model then borrows strength from the noise\nmodel by encouraging similar effects on correlated targets. We introduce a\nhyperparameter for the \\emph{latent signal-to-noise ratio} which turns out to\nbe important for modelling weak signals, and an ordered infinite-dimensional\nshrinkage prior that resolves the rotational unidentifiability in reduced-rank\nregression models. Simulations and prediction experiments with metabolite, gene\nexpression, FMRI measurement, and macroeconomic time series data show that our\nmodel equals or exceeds the state-of-the-art performance and, in particular,\noutperforms the standard approach of assuming independent noise and signal\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 19:38:27 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 12:25:43 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Gillberg", "Jussi", ""], ["Marttinen", "Pekka", ""], ["Pirinen", "Matti", ""], ["Kangas", "Antti J.", ""], ["Soininen", "Pasi", ""], ["Ali", "Mehreen", ""], ["Havulinna", "Aki S.", ""], ["J\u00e4rvelin", "Marjo-Riitta Marjo-Riitta", ""], ["Ala-Korpela", "Mika", ""], ["Kaski", "Samuel", ""]]}, {"id": "1410.7371", "submitter": "Momiao Xiong", "authors": "L. Ma, N. Lin, C.I. Amos and M.M. Xiong", "title": "A General Statistic Framework for Genome-based Disease Risk Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances of modern sensing and sequencing technologies generate a deluge of\nhigh dimensional space-temporal physiological and next-generation sequencing\n(NGS) data. Physiological traits are observed either as continuous random\nfunctions, or on a dense grid and referred to as function-valued traits. Both\nphysiological and NGS data are highly correlated data with their inherent\norder, spacing, and functional nature which are ignored by traditional\nsummary-based univariate and multivariate regression methods designed for\nquantitative genetic analysis of scalar trait and common variants. To capture\nmorphological and dynamic features of the data and utilize their dependent\nstructure, we propose a functional linear model (FLM) in which a trait curve is\nmodeled as a response function, the genetic variation in a genomic region or\ngene is modeled as a functional predictor, and the genetic effects are modeled\nas a function of both time and genomic position (FLMF) for genetic analysis of\nfunction-valued trait with both GWAS and NGS data. By extensive simulations, we\ndemonstrate that the FLMF has the correct type 1 error rates and much higher\npower to detect association than the existing methods. The FLMF is applied to\nsleep data from Starr County health studies where oxygen saturation were\nmeasured in 22,670 seconds on average for 833 individuals. We found 65 genes\nthat were significantly associated with oxygen saturation functional trait with\nP-values ranging from 2.40E-06 to 2.53E-21. The results clearly demonstrate\nthat the FLMF substantially outperforms the traditional genetic models with\nscalar trait.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 19:45:13 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Ma", "L.", ""], ["Lin", "N.", ""], ["Amos", "C. I.", ""], ["Xiong", "M. M.", ""]]}, {"id": "1410.7383", "submitter": "Guy Baruch", "authors": "Guy Baruch", "title": "A Ternary Non-Commutative Latent Factor Model for Scalable Three-Way\n  Real Tensor Completion", "comments": "12 pages, journal preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by large-scale Collaborative-Filtering applications, we present a\nNon-Commuting Latent Factor (NCLF) tensor-completion approach for modeling\nthree-way arrays, which is diagonal like the standard PARAFAC, but wherein\ndifferent terms distinguish different kinds of three-way relations of\nco-clusters, as determined by permutations of latent factors. The first key\ncomponent of the algebraic representation is the usage of two non-commutative\nreal trilinear operations as the building blocks of the approximation. These\noperations are the standard three dimensional triple-product and a trilinear\nproduct on a two-dimensional real vector space, which is a representation of\nthe real Clifford Algebra Cl(1,1) (a certain Majorana spinor). Both operations\nare purely ternary in that they cannot be decomposed into two group-operations\non the relevant spaces. The second key component of the method is combining\nthese operations using permutation-symmetry preserving linear combinations. We\napply the model to the MovieLens and Fannie Mae datasets, and find that it\noutperforms the PARAFAC model. We propose some future directions, such as\nunsupervised-learning.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 13:08:49 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 20:53:01 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 20:53:48 GMT"}, {"version": "v4", "created": "Wed, 28 Jan 2015 20:47:27 GMT"}, {"version": "v5", "created": "Wed, 16 Sep 2015 13:14:16 GMT"}, {"version": "v6", "created": "Fri, 18 Sep 2015 06:22:41 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Baruch", "Guy", ""]]}, {"id": "1410.7404", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Maximally Informative Hierarchical Representations of High-Dimensional\n  Data", "comments": "13 pages, 8 figures. Appearing in Proceedings of the 18th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set of probabilistic functions of some input variables as a\nrepresentation of the inputs. We present bounds on how informative a\nrepresentation is about input data. We extend these bounds to hierarchical\nrepresentations so that we can quantify the contribution of each layer towards\ncapturing the information in the original data. The special form of these\nbounds leads to a simple, bottom-up optimization procedure to construct\nhierarchical representations that are also maximally informative about the\ndata. This optimization has linear computational complexity and constant sample\ncomplexity in the number of variables. These results establish a new approach\nto unsupervised learning of deep representations that is both principled and\npractical. We demonstrate the usefulness of the approach on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 20:00:40 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 00:38:54 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1410.7414", "submitter": "Junier Oliva", "authors": "Junier Oliva, Willie Neiswanger, Barnabas Poczos, Eric Xing, Jeff\n  Schneider", "title": "Fast Function to Function Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of regression when both input covariates and output\nresponses are functions from a nonparametric function class. Function to\nfunction regression (FFR) covers a large range of interesting applications\nincluding time-series prediction problems, and also more general tasks like\nstudying a mapping between two separate types of distributions. However,\nprevious nonparametric estimators for FFR type problems scale badly\ncomputationally with the number of input/output pairs in a data-set. Given the\ncomplexity of a mapping between general functions it may be necessary to\nconsider large data-sets in order to achieve a low estimation risk. To address\nthis issue, we develop a novel scalable nonparametric estimator, the\nTriple-Basis Estimator (3BE), which is capable of operating over datasets with\nmany instances. To the best of our knowledge, the 3BE is the first\nnonparametric FFR estimator that can scale to massive datasets. We analyze the\n3BE's risk and derive an upperbound rate. Furthermore, we show an improvement\nof several orders of magnitude in terms of prediction speed and a reduction in\nerror over previous estimators in various real-world data-sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 20:15:18 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Oliva", "Junier", ""], ["Neiswanger", "Willie", ""], ["Poczos", "Barnabas", ""], ["Xing", "Eric", ""], ["Schneider", "Jeff", ""]]}, {"id": "1410.7455", "submitter": "Daniel Povey", "authors": "Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur", "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging", "comments": "Accepted as workshop contribution to ICLR 2015. 12 pages plus 16\n  pages of appendices, International Conference on Learning Representations\n  (ICLR): Workshop track, 2015. [2 sets of minor fixes post-publication.]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 22:45:41 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 21:53:05 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2014 03:35:52 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 02:16:31 GMT"}, {"version": "v5", "created": "Fri, 20 Feb 2015 05:12:34 GMT"}, {"version": "v6", "created": "Sun, 29 Mar 2015 18:25:06 GMT"}, {"version": "v7", "created": "Tue, 9 Jun 2015 22:34:22 GMT"}, {"version": "v8", "created": "Mon, 22 Jun 2015 22:07:56 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Povey", "Daniel", ""], ["Zhang", "Xiaohui", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1410.7550", "submitter": "Marc Deisenroth", "authors": "Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc Peter Deisenroth", "title": "Learning deep dynamical models from image pixels", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems is important in many disciplines, e.g., control,\nrobotics, or neurotechnology. Commonly the state of these systems is not\ndirectly observed, but only available through noisy and potentially\nhigh-dimensional observations. In these cases, system identification, i.e.,\nfinding the measurement mapping and the transition mapping (system dynamics) in\nlatent space can be challenging. For linear system dynamics and measurement\nmappings efficient solutions for system identification are available. However,\nin practical applications, the linearity assumptions does not hold, requiring\nnon-linear system identification techniques. If additionally the observations\nare high-dimensional (e.g., images), non-linear system identification is\ninherently hard. To address the problem of non-linear system identification\nfrom high-dimensional observations, we combine recent advances in deep learning\nand system identification. In particular, we jointly learn a low-dimensional\nembedding of the observation by means of deep auto-encoders and a predictive\ntransition model in this low-dimensional space. We demonstrate that our model\nenables learning good predictive models of dynamical systems from pixel\ninformation only.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 08:37:01 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1410.7659", "submitter": "Guy Bresler", "authors": "Guy Bresler and David Gamarnik and Devavrat Shah", "title": "Learning graphical models from the Glauber dynamics", "comments": "9 pages. Appeared in Allerton Conference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of learning undirected graphical models\nfrom data generated according to the Glauber dynamics. The Glauber dynamics is\na Markov chain that sequentially updates individual nodes (variables) in a\ngraphical model and it is frequently used to sample from the stationary\ndistribution (to which it converges given sufficient time). Additionally, the\nGlauber dynamics is a natural dynamical model in a variety of settings. This\nwork deviates from the standard formulation of graphical model learning in the\nliterature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards\nfinding algorithms with low computational cost. As the main result of this\nwork, we establish that the problem of reconstructing binary pairwise graphical\nmodels is computationally tractable when we observe the Glauber dynamics.\nSpecifically, we show that a binary pairwise graphical model on $p$ nodes with\nmaximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function\n$f(d)$, using nearly the information-theoretic minimum number of samples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 15:32:09 GMT"}, {"version": "v2", "created": "Sat, 29 Nov 2014 02:31:20 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1410.7660", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli and U N Niranjan and Sujay Sanghavi and Animashree\n  Anandkumar and Prateek Jain", "title": "Non-convex Robust PCA", "comments": "Extended abstract to appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for robust PCA -- the task of recovering a low-rank\nmatrix from sparse corruptions that are of unknown value and support. Our\nmethod involves alternating between projecting appropriate residuals onto the\nset of low-rank matrices, and the set of sparse matrices; each projection is\n{\\em non-convex} but easy to compute. In spite of this non-convexity, we\nestablish exact recovery of the low-rank matrix, under the same conditions that\nare required by existing methods (which are based on convex optimization). For\nan $m \\times n$ input matrix ($m \\leq n)$, our method has a running time of\n$O(r^2mn)$ per iteration, and needs $O(\\log(1/\\epsilon))$ iterations to reach\nan accuracy of $\\epsilon$. This is close to the running time of simple PCA via\nthe power method, which requires $O(rmn)$ per iteration, and\n$O(\\log(1/\\epsilon))$ iterations. In contrast, existing methods for robust PCA,\nwhich are based on convex optimization, have $O(m^2n)$ complexity per\niteration, and take $O(1/\\epsilon)$ iterations, i.e., exponentially more\niterations for the same accuracy.\n  Experiments on both synthetic and real data establishes the improved speed\nand accuracy of our method over existing convex implementations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 15:33:13 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Niranjan", "U N", ""], ["Sanghavi", "Sujay", ""], ["Anandkumar", "Animashree", ""], ["Jain", "Prateek", ""]]}, {"id": "1410.7690", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani", "title": "Trend Filtering on Graphs", "comments": "A short version appeared in AISTATS'2015", "journal-ref": "Journal of Machine Learning Research Volume (2016) Volume 17\n  Article 15-147", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of adaptive estimators on graphs, based on penalizing\nthe $\\ell_1$ norm of discrete graph differences. This generalizes the idea of\ntrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate\nnonparametric regression, to graphs. Analogous to the univariate case, graph\ntrend filtering exhibits a level of local adaptivity unmatched by the usual\n$\\ell_2$-based graph smoothers. It is also defined by a convex minimization\nproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). We\ndemonstrate the merits of graph trend filtering through examples and theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 16:22:32 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 01:21:54 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 06:16:02 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2015 00:42:15 GMT"}, {"version": "v5", "created": "Sat, 4 Jun 2016 17:03:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Sharpnack", "James", ""], ["Smola", "Alex", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1410.7795", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik\n  Maharatna, Fabio Apicella, Federico Sicca", "title": "Classification of Autism Spectrum Disorder Using Supervised Learning of\n  Brain Connectivity Measures Extracted from Synchrostates", "comments": "27 pages, 17 figures", "journal-ref": "Journal of Neural Engineering, Volume 11, Number 4, pp. 046019,\n  August 2014", "doi": "10.1088/1741-2560/11/4/046019", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. The paper investigates the presence of autism using the functional\nbrain connectivity measures derived from electro-encephalogram (EEG) of\nchildren during face perception tasks. Approach. Phase synchronized patterns\nfrom 128-channel EEG signals are obtained for typical children and children\nwith autism spectrum disorder (ASD). The phase synchronized states or\nsynchrostates temporally switch amongst themselves as an underlying process for\nthe completion of a particular cognitive task. We used 12 subjects in each\ngroup (ASD and typical) for analyzing their EEG while processing fearful, happy\nand neutral faces. The minimal and maximally occurring synchrostates for each\nsubject are chosen for extraction of brain connectivity features, which are\nused for classification between these two groups of subjects. Among different\nsupervised learning techniques, we here explored the discriminant analysis and\nsupport vector machine both with polynomial kernels for the classification\ntask. Main results. The leave one out cross-validation of the classification\nalgorithm gives 94.7% accuracy as the best performance with corresponding\nsensitivity and specificity values as 85.7% and 100% respectively.\nSignificance. The proposed method gives high classification accuracies and\noutperforms other contemporary research results. The effectiveness of the\nproposed method for classification of autistic and typical children suggests\nthe possibility of using it on a larger population to validate it for clinical\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:16:04 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Oprescu", "Ioana-Anastasia", ""], ["Maharatna", "Koushik", ""], ["Apicella", "Fabio", ""], ["Sicca", "Federico", ""]]}, {"id": "1410.7812", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Beta-Negative Binomial Process and Exchangeable Random Partitions for\n  Mixed-Membership Modeling", "comments": "in Neural Information Processing Systems (NIPS) 2014. 9 pages + 3\n  page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta-negative binomial process (BNBP), an integer-valued stochastic\nprocess, is employed to partition a count vector into a latent random count\nmatrix. As the marginal probability distribution of the BNBP that governs the\nexchangeable random partitions of grouped data has not yet been developed,\ncurrent inference for the BNBP has to truncate the number of atoms of the beta\nprocess. This paper introduces an exchangeable partition probability function\nto explicitly describe how the BNBP clusters the data points of each group into\na random number of exchangeable partitions, which are shared across all the\ngroups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a\nnovel nonparametric Bayesian topic model that is distinct from existing ones,\nwith simple implementation, fast convergence, good mixing, and state-of-the-art\npredictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 21:08:39 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 22:52:17 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1410.7827", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, David J. Fleet", "title": "Generalized Product of Experts for Automatic and Principled Fusion of\n  Gaussian Process Predictions", "comments": "Modern Nonparametrics 3: Automating the Learning Pipeline workshop at\n  NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a generalized product of experts (gPoE) framework\nfor combining the predictions of multiple probabilistic models. We identify\nfour desirable properties that are important for scalability, expressiveness\nand robustness, when learning and inferring with a combination of multiple\nmodels. Through analysis and experiments, we show that gPoE of Gaussian\nprocesses (GP) have these qualities, while no other existing combination\nschemes satisfy all of them at the same time. The resulting GP-gPoE is highly\nscalable as individual GP experts can be independently learned in parallel;\nvery expressive as the way experts are combined depends on the input rather\nthan fixed; the combined prediction is still a valid probabilistic model with\nnatural interpretation; and finally robust to unreliable predictions from\nindividual experts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 22:04:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:12:57 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Cao", "Yanshuai", ""], ["Fleet", "David J.", ""]]}, {"id": "1410.7875", "submitter": "John Halloran", "authors": "Shengjie Wang, John T. Halloran, Jeff A. Bilmes, William S. Noble", "title": "Faster graphical model identification of tandem mass spectra using\n  peptide word lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid chromatography coupled with tandem mass spectrometry, also known as\nshotgun proteomics, is a widely-used high-throughput technology for identifying\nproteins in complex biological samples. Analysis of the tens of thousands of\nfragmentation spectra produced by a typical shotgun proteomics experiment\nbegins by assigning to each observed spectrum the peptide hypothesized to be\nresponsible for generating the spectrum, typically done by searching each\nspectrum against a database of peptides. We have recently described a machine\nlearning method---Dynamic Bayesian Network for Rapid Identification of Peptides\n(DRIP)---that not only achieves state-of-the-art spectrum identification\nperformance on a variety of datasets but also provides a trainable model\ncapable of returning valuable auxiliary information regarding specific\npeptide-spectrum matches. In this work, we present two significant improvements\nto DRIP. First, we describe how to use word lattices, which are widely used in\nnatural language processing, to significantly speed up DRIP's computations. To\nour knowledge, all existing shotgun proteomics search engines compute\nindependent scores between a given observed spectrum and each possible\ncandidate peptide from the database. The key idea of the word lattice is to\nrepresent the set of candidate peptides in a single data structure, thereby\nallowing sharing of redundant computations among the different candidates. We\ndemonstrate that using lattices in conjunction with DRIP leads to speedups on\nthe order of tens across yeast and worm data sets. Second, we introduce a\nvariant of DRIP that uses a discriminative training framework, performing\nmaximum mutual entropy estimation rather than maximum likelihood estimation.\nThis modification improves DRIP's statistical power, enabling us to increase\nthe number of identified spectrum at a 1% false discovery rate on yeast and\nworm data sets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:10:03 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Wang", "Shengjie", ""], ["Halloran", "John T.", ""], ["Bilmes", "Jeff A.", ""], ["Noble", "William S.", ""]]}, {"id": "1410.7876", "submitter": "Minh Dao", "authors": "Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, and Trac D. Tran", "title": "Collaborative Multi-sensor Classification via Sparsity-based\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general collaborative sparse representation\nframework for multi-sensor classification, which takes into account the\ncorrelations as well as complementary information between heterogeneous sensors\nsimultaneously while considering joint sparsity within each sensor's\nobservations. We also robustify our models to deal with the presence of sparse\nnoise and low-rank interference signals. Specifically, we demonstrate that\nincorporating the noise or interference signal as a low-rank component in our\nmodels is essential in a multi-sensor classification problem when multiple\nco-located sources/sensors simultaneously record the same physical event. We\nfurther extend our frameworks to kernelized models which rely on sparsely\nrepresenting a test sample in terms of all the training samples in a feature\nspace induced by a kernel function. A fast and efficient algorithm based on\nalternative direction method is proposed where its convergence to an optimal\nsolution is guaranteed. Extensive experiments are conducted on several real\nmulti-sensor data sets and results are compared with the conventional\nclassifiers to verify the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:25:44 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:46:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Dao", "Minh", ""], ["Nguyen", "Nam H.", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1410.8034", "submitter": "Xudong Liu", "authors": "Xudong Liu, Bin Zhang, Ting Zhang and Chang Liu", "title": "Latent Feature Based FM Model For Rating Prediction", "comments": "4 pages, 3 figures, Large Scale Recommender Systems:workshop of\n  Recsys 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating Prediction is a basic problem in Recommender System, and one of the\nmost widely used method is Factorization Machines(FM). However, traditional\nmatrix factorization methods fail to utilize the benefit of implicit feedback,\nwhich has been proved to be important in Rating Prediction problem. In this\nwork, we consider a specific situation, movie rating prediction, where we\nassume that watching history has a big influence on his/her rating behavior on\nan item. We introduce two models, Latent Dirichlet Allocation(LDA) and\nword2vec, both of which perform state-of-the-art results in training latent\nfeatures. Based on that, we propose two feature based models. One is the\nTopic-based FM Model which provides the implicit feedback to the matrix\nfactorization. The other is the Vector-based FM Model which expresses the order\ninfo of watching history. Empirical results on three datasets demonstrate that\nour method performs better than the baseline model and confirm that\nVector-based FM Model usually works better as it contains the order info.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:51:54 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Liu", "Xudong", ""], ["Zhang", "Bin", ""], ["Zhang", "Ting", ""], ["Liu", "Chang", ""]]}, {"id": "1410.8043", "submitter": "Wei Dai", "authors": "Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, Eric\n  P. Xing", "title": "High-Performance Distributed ML at Scale through Parameter Server\n  Consistency Models", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Machine Learning (ML) applications increase in data size and model\ncomplexity, practitioners turn to distributed clusters to satisfy the increased\ncomputational and memory demands. Unfortunately, effective use of clusters for\nML requires considerable expertise in writing distributed code, while\nhighly-abstracted frameworks like Hadoop have not, in practice, approached the\nperformance seen in specialized ML implementations. The recent Parameter Server\n(PS) paradigm is a middle ground between these extremes, allowing easy\nconversion of single-machine parallel ML applications into distributed ones,\nwhile maintaining high throughput through relaxed \"consistency models\" that\nallow inconsistent parameter reads. However, due to insufficient theoretical\nstudy, it is not clear which of these consistency models can really ensure\ncorrect ML algorithm output; at the same time, there remain many\ntheoretically-motivated but undiscovered opportunities to maximize\ncomputational throughput. Motivated by this challenge, we study both the\ntheoretical guarantees and empirical behavior of iterative-convergent ML\nalgorithms in existing PS consistency models. We then use the gleaned insights\nto improve a consistency model using an \"eager\" PS communication mechanism, and\nimplement it as a new PS system that enables ML algorithms to reach their\nsolution more quickly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 16:19:21 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Dai", "Wei", ""], ["Kumar", "Abhimanu", ""], ["Wei", "Jinliang", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth", ""], ["Xing", "Eric P.", ""]]}, {"id": "1410.8229", "submitter": "Mathukumalli Vidyasagar", "authors": "Mehmet Eren Ahsen and Niharika Challapalli and Mathukumalli Vidyasagar", "title": "Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse\n  Recovery and the Grouping Effect", "comments": "22 pages, 3 figures, to appear in the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new optimization formulation for sparse\nregression and compressed sensing, called CLOT (Combined L-One and Two),\nwherein the regularizer is a convex combination of the $\\ell_1$- and\n$\\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation,\nin which the regularizer is a convex combination of the $\\ell_1$- and\n$\\ell_2$-norm squared. It is shown that, in the context of compressed sensing,\nthe EN formulation does not achieve robust recovery of sparse vectors, whereas\nthe new CLOT formulation achieves robust recovery. Also, like EN but unlike\nLASSO, the CLOT formulation achieves the grouping effect, wherein coefficients\nof highly correlated columns of the measurement (or design) matrix are assigned\nroughly comparable values. It is already known LASSO does not have the grouping\neffect. Therefore the CLOT formulation combines the best features of both LASSO\n(robust sparse recovery) and EN (grouping effect).\n  The CLOT formulation is a special case of another one called SGL (Sparse\nGroup LASSO) which was introduced into the literature previously, but without\nany analysis of either the grouping effect or robust sparse recovery. It is\nshown here that SGL achieves robust sparse recovery, and also achieves a\nversion of the grouping effect in that coefficients of highly correlated\ncolumns belonging to the same group of the measurement (or design) matrix are\nassigned roughly comparable values.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 01:47:09 GMT"}, {"version": "v2", "created": "Sat, 12 Mar 2016 09:11:11 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 05:00:44 GMT"}, {"version": "v4", "created": "Tue, 20 Jun 2017 15:21:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Ahsen", "Mehmet Eren", ""], ["Challapalli", "Niharika", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1410.8275", "submitter": "Stefan Wager", "authors": "Julie Josse and Stefan Wager", "title": "Bootstrap-Based Regularization for Low-Rank Matrix Estimation", "comments": "To appear in the Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a flexible framework for low-rank matrix estimation that allows us\nto transform noise models into regularization schemes via a simple bootstrap\nalgorithm. Effectively, our procedure seeks an autoencoding basis for the\nobserved matrix that is stable with respect to the specified noise model; we\ncall the resulting procedure a stable autoencoder. In the simplest case, with\nan isotropic noise model, our method is equivalent to a classical singular\nvalue shrinkage estimator. For non-isotropic noise models, e.g., Poisson noise,\nthe method does not reduce to singular value shrinkage, and instead yields new\nestimators that perform well in experiments. Moreover, by iterating our stable\nautoencoding scheme, we can automatically generate low-rank estimates without\nspecifying the target rank as a tuning parameter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 07:22:33 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 02:11:14 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 05:13:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Josse", "Julie", ""], ["Wager", "Stefan", ""]]}, {"id": "1410.8372", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry\n  Wasserman", "title": "On Estimating $L_2^2$ Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a comprehensive theoretical characterization of a nonparametric\nestimator for the $L_2^2$ divergence between two continuous distributions. We\nfirst bound the rate of convergence of our estimator, showing that it is\n$\\sqrt{n}$-consistent provided the densities are sufficiently smooth. In this\nsmooth regime, we then show that our estimator is asymptotically normal,\nconstruct asymptotic confidence intervals, and establish a Berry-Ess\\'{e}en\nstyle inequality characterizing the rate of convergence to normality. We also\nshow that this estimator is minimax optimal.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 13:37:39 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Kandasamy", "Kirthevasan", ""], ["Poczos", "Barnabas", ""], ["Wasserman", "Larry", ""]]}, {"id": "1410.8546", "submitter": "Florian Bernard", "authors": "Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas\n  Husch, Jorge Goncalves", "title": "A Solution for Multi-Alignment by Transformation Synchronisation", "comments": "Accepted for CVPR 2015 (please cite CVPR version)", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298828", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alignment of a set of objects by means of transformations plays an\nimportant role in computer vision. Whilst the case for only two objects can be\nsolved globally, when multiple objects are considered usually iterative methods\nare used. In practice the iterative methods perform well if the relative\ntransformations between any pair of objects are free of noise. However, if only\nnoisy relative transformations are available (e.g. due to missing data or wrong\ncorrespondences) the iterative methods may fail.\n  Based on the observation that the underlying noise-free transformations can\nbe retrieved from the null space of a matrix that can directly be obtained from\npairwise alignments, this paper presents a novel method for the synchronisation\nof pairwise transformations such that they are transitively consistent.\n  Simulations demonstrate that for noisy transformations, a large proportion of\nmissing data and even for wrong correspondence assignments the method delivers\nencouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:29:08 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 16:19:45 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Bernard", "Florian", ""], ["Thunberg", "Johan", ""], ["Gemmar", "Peter", ""], ["Hertel", "Frank", ""], ["Husch", "Andreas", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1410.8553", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic and David\n  Doermann", "title": "A random forest system combination approach for error detection in\n  digital dictionaries", "comments": "9 pages, 7 figures, 10 tables; appeared in Proceedings of the\n  Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,\n  April 2012", "journal-ref": "In Proceedings of the Workshop on Innovative Hybrid Approaches to\n  the Processing of Textual Data, pages 78-86, Avignon, France, April 2012.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When digitizing a print bilingual dictionary, whether via optical character\nrecognition or manual entry, it is inevitable that errors are introduced into\nthe electronic version that is created. We investigate automating the process\nof detecting errors in an XML representation of a digitized print dictionary\nusing a hybrid approach that combines rule-based, feature-based, and language\nmodel-based methods. We investigate combining methods and show that using\nrandom forests is a promising approach. We find that in isolation, unsupervised\nmethods rival the performance of supervised methods. Random forests typically\nrequire training data so we investigate how we can apply random forests to\ncombine individual base methods that are themselves unsupervised without\nrequiring large amounts of training data. Experiments reveal empirically that a\nrelatively small amount of data is sufficient and can potentially be further\nreduced through specific selection criteria.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:52:48 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Bloodgood", "Michael", ""], ["Ye", "Peng", ""], ["Rodrigues", "Paul", ""], ["Zajic", "David", ""], ["Doermann", "David", ""]]}, {"id": "1410.8576", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu", "title": "An ensemble-based system for automatic screening of diabetic retinopathy", "comments": null, "journal-ref": "Knowledge-Based Systems, Elsevier, Volume 60, April 2014, Pages\n  20-27", "doi": "10.1016/j.knosys.2013.12.023", "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an ensemble-based method for the screening of diabetic\nretinopathy (DR) is proposed. This approach is based on features extracted from\nthe output of several retinal image processing algorithms, such as image-level\n(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,\nexudates) and anatomical (macula, optic disc) components. The actual decision\nabout the presence of the disease is then made by an ensemble of machine\nlearning classifiers. We have tested our approach on the publicly available\nMessidor database, where 90% sensitivity, 91% specificity and 90% accuracy and\n0.989 AUC are achieved in a disease/no-disease setting. These results are\nhighly competitive in this field and suggest that retinal image processing is a\nvalid approach for automatic DR screening.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:14:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.8577", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu", "title": "An Ensemble-based System for Microaneurysm Detection and Diabetic\n  Retinopathy Grading", "comments": null, "journal-ref": "IEEE Transactions on Biomedical Engineering, vol.59, no.6, pp.\n  1720-1726, June 2012", "doi": "10.1109/TBME.2012.2193126", "report-no": null, "categories": "cs.CV cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable microaneurysm detection in digital fundus images is still an open\nissue in medical image processing. We propose an ensemble-based framework to\nimprove microaneurysm detection. Unlike the well-known approach of considering\nthe output of multiple classifiers, we propose a combination of internal\ncomponents of microaneurysm detectors, namely preprocessing methods and\ncandidate extractors. We have evaluated our approach for microaneurysm\ndetection in an online competition, where this algorithm is currently ranked as\nfirst and also on two other databases. Since microaneurysm detection is\ndecisive in diabetic retinopathy grading, we also tested the proposed method\nfor this task on the publicly available Messidor database, where a promising\nAUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification\nbased on the presence or absence of the microaneurysms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:21:02 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""]]}, {"id": "1410.8675", "submitter": "Hidekazu Oiwa", "authors": "Hidekazu Oiwa, Ryohei Fujimaki", "title": "Partition-wise Linear Models", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-specific linear models are widely used in practical applications\nbecause of their non-linear but highly interpretable model representations. One\nof the key challenges in their use is non-convexity in simultaneous\noptimization of regions and region-specific models. This paper proposes novel\nconvex region-specific linear models, which we refer to as partition-wise\nlinear models. Our key ideas are 1) assigning linear models not to regions but\nto partitions (region-specifiers) and representing region-specific linear\nmodels by linear combinations of partition-specific models, and 2) optimizing\nregions via partition selection from a large number of given partition\ncandidates by means of convex structured regularizations. In addition to\nproviding initialization-free globally-optimal solutions, our convex\nformulation makes it possible to derive a generalization bound and to use such\nadvanced optimization techniques as proximal methods and decomposition of the\nproximal maps for sparsity-inducing regularizations. Experimental results\ndemonstrate that our partition-wise linear models perform better than or are at\nleast competitive with state-of-the-art region-specific or locally linear\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 09:01:27 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Oiwa", "Hidekazu", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1410.8864", "submitter": "Dohyung Park", "authors": "Dohyung Park, Constantine Caramanis, Sujay Sanghavi", "title": "Greedy Subspace Clustering", "comments": "To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of subspace clustering: given points that lie on or\nnear the union of many low-dimensional linear subspaces, recover the subspaces.\nTo this end, one first identifies sets of points close to the same subspace and\nuses the sets to estimate the subspaces. As the geometric structure of the\nclusters (linear subspaces) forbids proper performance of general distance\nbased approaches such as K-means, many model-specific methods have been\nproposed. In this paper, we provide new simple and efficient algorithms for\nthis problem. Our statistical analysis shows that the algorithms are guaranteed\nexact (perfect) clustering performance under certain conditions on the number\nof points and the affinity between subspaces. These conditions are weaker than\nthose considered in the standard statistical literature. Experimental results\non synthetic data generated from the standard unions of subspaces model\ndemonstrate our theory. We also show that our algorithm performs competitively\nagainst state-of-the-art algorithms on real-world applications such as motion\nsegmentation and face clustering, with much simpler implementation and lower\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:50:42 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Park", "Dohyung", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}]