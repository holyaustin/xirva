[{"id": "1402.0030", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Karol Gregor", "title": "Neural Variational Inference and Learning in Belief Networks", "comments": null, "journal-ref": "Proceedings of the 31st International Conference on Machine\n  Learning (ICML), JMLR: W&CP volume 32, 2014 pgs 1791-1799", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly expressive directed latent variable models, such as sigmoid belief\nnetworks, are difficult to train on large datasets because exact inference in\nthem is intractable and none of the approximate inference methods that have\nbeen applied to them scale well. We propose a fast non-iterative approximate\ninference method that uses a feedforward network to implement efficient exact\nsampling from the variational posterior. The model and this inference network\nare trained jointly by maximizing a variational lower bound on the\nlog-likelihood. Although the naive estimator of the inference model gradient is\ntoo high-variance to be useful, we make it practical by applying several\nstraightforward model-independent variance reduction techniques. Applying our\napproach to training sigmoid belief networks and deep autoregressive networks,\nwe show that it outperforms the wake-sleep algorithm on MNIST and achieves\nstate-of-the-art results on the Reuters RCV1 document dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 23:33:21 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 17:12:03 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Mnih", "Andriy", ""], ["Gregor", "Karol", ""]]}, {"id": "1402.0099", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly, Martin Kreuzer, and Louis Theran", "title": "Dual-to-kernel learning with ideals", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AC math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a theory which unifies kernel learning and symbolic\nalgebraic methods. We show that both worlds are inherently dual to each other,\nand we use this duality to combine the structure-awareness of algebraic methods\nwith the efficiency and generality of kernels. The main idea lies in relating\npolynomial rings to feature space, and ideals to manifolds, then exploiting\nthis generative-discriminative duality on kernel matrices. We illustrate this\nby proposing two algorithms, IPCA and AVICA, for simultaneous manifold and\nfeature learning, and test their accuracy on synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 16:38:59 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Kreuzer", "Martin", ""], ["Theran", "Louis", ""]]}, {"id": "1402.0108", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Markov Blanket Ranking using Kernel-based Conditional Dependence\n  Measures", "comments": "10 pages, 4 figures, 2 algorithms, NIPS 2013 Workshop on Causality,\n  code: github.com/ericstrobl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing feature selection algorithms that move beyond a pure correlational\nto a more causal analysis of observational data is an important problem in the\nsciences. Several algorithms attempt to do so by discovering the Markov blanket\nof a target, but they all contain a forward selection step which variables must\npass in order to be included in the conditioning set. As a result, these\nalgorithms may not consider all possible conditional multivariate combinations.\nWe improve on this limitation by proposing a backward elimination method that\nuses a kernel-based conditional dependence measure to identify the Markov\nblanket in a fully multivariate fashion. The algorithm is easy to implement and\ncompares favorably to other methods on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 17:51:54 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 22:16:00 GMT"}, {"version": "v3", "created": "Sat, 3 May 2014 01:07:49 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1402.0119", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard\n  Sch\\\"olkopf", "title": "Randomized Nonlinear Component Analysis", "comments": "Appearing in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical methods such as Principal Component Analysis (PCA) and Canonical\nCorrelation Analysis (CCA) are ubiquitous in statistics. However, these\ntechniques are only able to reveal linear relationships in data. Although\nnonlinear variants of PCA and CCA have been proposed, these are computationally\nprohibitive in the large scale.\n  In a separate strand of recent research, randomized methods have been\nproposed to construct features that help reveal nonlinear patterns in data. For\nbasic tasks such as regression or classification, random features exhibit\nlittle or no loss in performance, while achieving drastic savings in\ncomputational requirements.\n  In this paper we leverage randomness to design scalable new variants of\nnonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such\nas spectral clustering or LDA. We demonstrate our algorithms through\nexperiments on real-world data, on which we compare against the\nstate-of-the-art. A simple R implementation of the presented algorithms is\nprovided.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 19:54:06 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 16:41:11 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Lopez-Paz", "David", ""], ["Sra", "Suvrit", ""], ["Smola", "Alex", ""], ["Ghahramani", "Zoubin", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1402.0170", "submitter": "Shu Kong", "authors": "Shu Kong, Zhuolin Jiang, Qiang Yang", "title": "Collaborative Receptive Field Learning", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 10:11:57 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Kong", "Shu", ""], ["Jiang", "Zhuolin", ""], ["Yang", "Qiang", ""]]}, {"id": "1402.0282", "submitter": "Benjamin Rubinstein", "authors": "Duo Zhang and Benjamin I. P. Rubinstein and Jim Gemmell", "title": "Principled Graph Matching Algorithms for Integrating Multiple Data\n  Sources", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores combinatorial optimization for problems of max-weight\ngraph matching on multi-partite graphs, which arise in integrating multiple\ndata sources. Entity resolution-the data integration problem of performing\nnoisy joins on structured data-typically proceeds by first hashing each record\ninto zero or more blocks, scoring pairs of records that are co-blocked for\nsimilarity, and then matching pairs of sufficient similarity. In the most\ncommon case of matching two sources, it is often desirable for the final\nmatching to be one-to-one (a record may be matched with at most one other);\nmembers of the database and statistical record linkage communities accomplish\nsuch matchings in the final stage by weighted bipartite graph matching on\nsimilarity scores. Such matchings are intuitively appealing: they leverage a\nnatural global property of many real-world entity stores-that of being nearly\ndeduped-and are known to provide significant improvements to precision and\nrecall. Unfortunately unlike the bipartite case, exact max-weight matching on\nmulti-partite graphs is known to be NP-hard. Our two-fold algorithmic\ncontributions approximate multi-partite max-weight matching: our first\nalgorithm borrows optimization techniques common to Bayesian probabilistic\ninference; our second is a greedy approximation algorithm. In addition to a\ntheoretical guarantee on the latter, we present comparisons on a real-world ER\nproblem from Bing significantly larger than typically found in the literature,\npublication data, and on a series of synthetic problems. Our results quantify\nsignificant improvements due to exploiting multiple sources, which are made\npossible by global one-to-one constraints linking otherwise independent\nmatching sub-problems. We also discover that our algorithms are complementary:\none being much more robust under noise, and the other being simple to implement\nand very fast to run.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 04:56:58 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Zhang", "Duo", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""]]}, {"id": "1402.0288", "submitter": "Gang Niu", "authors": "Gang Niu, Bo Dai, Marthinus Christoffel du Plessis, and Masashi\n  Sugiyama", "title": "Transductive Learning with Multi-class Volume Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypothesis space, the large volume principle by Vladimir Vapnik\nprioritizes equivalence classes according to their volume in the hypothesis\nspace. The volume approximation has hitherto been successfully applied to\nbinary learning problems. In this paper, we extend it naturally to a more\ngeneral definition which can be applied to several transductive problem\nsettings, such as multi-class, multi-label and serendipitous learning. Even\nthough the resultant learning method involves a non-convex optimization\nproblem, the globally optimal solution is almost surely unique and can be\nobtained in O(n^3) time. We theoretically provide stability and error analyses\nfor the proposed method, and then experimentally show that it is promising.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 06:09:52 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Niu", "Gang", ""], ["Dai", "Bo", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1402.0330", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Sequential Monte Carlo for Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for how to use sequential Monte Carlo (SMC)\nalgorithms for inference in probabilistic graphical models (PGM). Via a\nsequential decomposition of the PGM we find a sequence of auxiliary\ndistributions defined on a monotonically increasing sequence of probability\nspaces. By targeting these auxiliary distributions using SMC we are able to\napproximate the full joint distribution defined by the PGM. One of the key\nmerits of the SMC sampler is that it provides an unbiased estimate of the\npartition function of the model. We also show how it can be used within a\nparticle Markov chain Monte Carlo framework in order to construct\nhigh-dimensional block-sampling algorithms for general PGMs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 10:21:18 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 06:32:56 GMT"}, {"version": "v3", "created": "Fri, 8 Aug 2014 08:06:51 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 05:55:20 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1402.0422", "submitter": "Andrea Lancichinetti", "authors": "Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna,\n  Konrad K\\\"ording, Lu\\'is A. Nunes Amaral", "title": "A high-reproducibility and high-accuracy method for automated topic\n  classification", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge sits in large databases of unstructured text.\nLeveraging this knowledge requires algorithms that extract and record metadata\non unstructured text documents. Assigning topics to documents will enable\nintelligent search, statistical characterization, and meaningful\nclassification. Latent Dirichlet allocation (LDA) is the state-of-the-art in\ntopic classification. Here, we perform a systematic theoretical and numerical\nanalysis that demonstrates that current optimization techniques for LDA often\nyield results which are not accurate in inferring the most suitable model\nparameters. Adapting approaches for community detection in networks, we propose\na new algorithm which displays high-reproducibility and high-accuracy, and also\nhas high computational efficiency. We apply it to a large set of documents in\nthe English Wikipedia and reveal its hierarchical structure. Our algorithm\npromises to make \"big data\" text analysis systems more reliable.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:45:13 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lancichinetti", "Andrea", ""], ["Sirer", "M. Irmak", ""], ["Wang", "Jane X.", ""], ["Acuna", "Daniel", ""], ["K\u00f6rding", "Konrad", ""], ["Amaral", "Lu\u00eds A. Nunes", ""]]}, {"id": "1402.0453", "submitter": "Qi Qian", "authors": "Qi Qian, Rong Jin, Shenghuo Zhu and Yuanqing Lin", "title": "Fine-Grained Visual Categorization via Multi-stage Metric Learning", "comments": "in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization (FGVC) is to categorize objects into\nsubordinate classes instead of basic classes. One major challenge in FGVC is\nthe co-occurrence of two issues: 1) many subordinate classes are highly\ncorrelated and are difficult to distinguish, and 2) there exists the large\nintra-class variation (e.g., due to object pose). This paper proposes to\nexplicitly address the above two issues via distance metric learning (DML). DML\naddresses the first issue by learning an embedding so that data points from the\nsame class will be pulled together while those from different classes should be\npushed apart from each other; and it addresses the second issue by allowing the\nflexibility that only a portion of the neighbors (not all data points) from the\nsame class need to be pulled together. However, feature representation of an\nimage is often high dimensional, and DML is known to have difficulty in dealing\nwith high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$\nfor storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a\nmulti-stage metric learning framework that divides the large-scale high\ndimensional learning problem to a series of simple subproblems, achieving\n$\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC\nbenchmark datasets verifies that our method is both effective and efficient\ncompared to the state-of-the-art FGVC approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:20:53 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 17:28:51 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Qian", "Qi", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1402.0459", "submitter": "Haoyang (Hubert) Duan", "authors": "Hubert Haoyang Duan", "title": "Applying Supervised Learning Algorithms and a New Feature Selection\n  Method to Predict Coronary Artery Disease", "comments": "This is a Master of Science in Mathematics thesis under the\n  supervision of Dr. Vladimir Pestov and Dr. George Wells submitted on January\n  31, 2014 at the University of Ottawa; 102 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a fresh data science perspective, this thesis discusses the prediction\nof coronary artery disease based on genetic variations at the DNA base pair\nlevel, called Single-Nucleotide Polymorphisms (SNPs), collected from the\nOntario Heart Genomics Study (OHGS).\n  First, the thesis explains two commonly used supervised learning algorithms,\nthe k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes a\ncomplete proof that the k-NN classifier is universally consistent in any finite\ndimensional normed vector space. Second, the thesis introduces two\ndimensionality reduction steps, Random Projections, a known feature extraction\ntechnique based on the Johnson-Lindenstrauss lemma, and a new method termed\nMass Transportation Distance (MTD) Feature Selection for discrete domains.\nThen, this thesis compares the performance of Random Projections with the k-NN\nclassifier against MTD Feature Selection and Random Forest, for predicting\nartery disease based on accuracy, the F-Measure, and area under the Receiver\nOperating Characteristic (ROC) curve.\n  The comparative results demonstrate that MTD Feature Selection with Random\nForest is vastly superior to Random Projections and k-NN. The Random Forest\nclassifier is able to obtain an accuracy of 0.6660 and an area under the ROC\ncurve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTD\nFeature Selection for classification. This area is considerably better than the\nprevious high score of 0.608 obtained by Davies et al. in 2010 on the same\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 18:47:41 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Duan", "Hubert Haoyang", ""]]}, {"id": "1402.0480", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Max Welling", "title": "Efficient Gradient-Based Inference through Transformations between Bayes\n  Nets and Neural Nets", "comments": null, "journal-ref": "Proceedings of The 31st International Conference on Machine\n  Learning, pp. 1782-1790, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Bayesian networks and neural networks with stochastic hidden\nunits are commonly perceived as two separate types of models. We show that\neither of these types of models can often be transformed into an instance of\nthe other, by switching between centered and differentiable non-centered\nparameterizations of the latent variables. The choice of parameterization\ngreatly influences the efficiency of gradient-based posterior inference; we\nshow that they are often complementary to eachother, we clarify when each\nparameterization is preferred and show how inference can be made robust. In the\nnon-centered form, a simple Monte Carlo estimator of the marginal likelihood\ncan be used for learning the parameters. Theoretical results are supported by\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 19:39:20 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 13:56:26 GMT"}, {"version": "v3", "created": "Tue, 13 May 2014 11:17:41 GMT"}, {"version": "v4", "created": "Mon, 16 Jun 2014 09:04:26 GMT"}, {"version": "v5", "created": "Thu, 22 Jan 2015 11:05:53 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Welling", "Max", ""]]}, {"id": "1402.0555", "submitter": "Daniel Hsu", "authors": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and\n  Robert E. Schapire", "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the contextual bandit learning problem, where\nthe learner repeatedly takes one of $K$ actions in response to the observed\ncontext, and observes the reward only for that chosen action. Our method\nassumes access to an oracle for solving fully supervised cost-sensitive\nclassification problems and achieves the statistically optimal regret guarantee\nwith only $\\tilde{O}(\\sqrt{KT/\\log N})$ oracle calls across all $T$ rounds,\nwhere $N$ is the number of policies in the policy class we compete against. By\ndoing so, we obtain the most practical contextual bandit learning algorithm\namongst approaches that work for general policy classes. We further conduct a\nproof-of-concept experiment which demonstrates the excellent computational and\nprediction performance of (an online variant of) our algorithm relative to\nseveral baselines.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 00:48:29 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 01:41:47 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Agarwal", "Alekh", ""], ["Hsu", "Daniel", ""], ["Kale", "Satyen", ""], ["Langford", "John", ""], ["Li", "Lihong", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1402.0562", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Alessandro Lazaric and Emma Brunskill", "title": "Online Stochastic Optimization under Correlated Bandit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of online stochastic optimization of a\nlocally smooth function under bandit feedback. We introduce the high-confidence\ntree (HCT) algorithm, a novel any-time $\\mathcal{X}$-armed bandit algorithm,\nand derive regret bounds matching the performance of existing state-of-the-art\nin terms of dependency on number of steps and smoothness factor. The main\nadvantage of HCT is that it handles the challenging case of correlated rewards,\nwhereas existing methods require that the reward-generating process of each arm\nis an identically and independent distributed (iid) random process. HCT also\nimproves on the state-of-the-art in terms of its memory requirement as well as\nrequiring a weaker smoothness assumption on the mean-reward function in compare\nto the previous anytime algorithms. Finally, we discuss how HCT can be applied\nto the problem of policy search in reinforcement learning and we report\npreliminary empirical results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:34:50 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 20:50:52 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 17:30:53 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1402.0635", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy, Zheng Wen", "title": "Generalization and Exploration via Randomized Value Functions", "comments": "arXiv admin note: text overlap with arXiv:1307.4847", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose randomized least-squares value iteration (RLSVI) -- a new\nreinforcement learning algorithm designed to explore and generalize efficiently\nvia linearly parameterized value functions. We explain why versions of\nleast-squares value iteration that use Boltzmann or epsilon-greedy exploration\ncan be highly inefficient, and we present computational results that\ndemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish\nan upper bound on the expected regret of RLSVI that demonstrates\nnear-optimality in a tabula rasa learning context. More broadly, our results\nsuggest that randomized value functions offer a promising approach to tackling\na critical challenge in reinforcement learning: synthesizing efficient\nexploration and effective generalization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 06:41:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 23:11:02 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 10:20:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""], ["Wen", "Zheng", ""]]}, {"id": "1402.0694", "submitter": "Christopher Nemeth", "authors": "Chris Nemeth, Paul Fearnhead", "title": "Particle Metropolis adjusted Langevin algorithms for state space models", "comments": "Replaced with updated article with new title at arXiv:1412.7299", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle MCMC is a class of algorithms that can be used to analyse\nstate-space models. They use MCMC moves to update the parameters of the models,\nand particle filters to propose values for the path of the state-space model.\nCurrently the default is to use random walk Metropolis to update the parameter\nvalues. We show that it is possible to use information from the output of the\nparticle filter to obtain better proposal distributions for the parameters. In\nparticular it is possible to obtain estimates of the gradient of the log\nposterior from each run of the particle filter, and use these estimates within\na Langevin-type proposal. We propose using the recent computationally efficient\napproach of Nemeth et al. (2013) for obtaining such estimates. We show\nempirically that for a variety of state-space models this proposal is more\nefficient than the standard random walk Metropolis proposal in terms of:\nreducing autocorrelation of the posterior samples, reducing the burn-in time of\nthe MCMC sampler and increasing the squared jump distance between posterior\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 11:14:17 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 07:58:27 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Nemeth", "Chris", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1402.0779", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathanael Perraudin, Vassilis Kalofolias, David Shuman, Pierre\n  Vandergheynst", "title": "UNLocBoX: A MATLAB convex optimization toolbox for proximal-splitting\n  methods", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization is an essential tool for machine learning, as many of its\nproblems can be formulated as minimization problems of specific objective\nfunctions. While there is a large variety of algorithms available to solve\nconvex problems, we can argue that it becomes more and more important to focus\non efficient, scalable methods that can deal with big data. When the objective\nfunction can be written as a sum of \"simple\" terms, proximal splitting methods\nare a good choice. UNLocBoX is a MATLAB library that implements many of these\nmethods, designed to solve convex optimization problems of the form $\\min_{x\n\\in \\mathbb{R}^N} \\sum_{n=1}^K f_n(x).$ It contains the most recent solvers\nsuch as FISTA, Douglas-Rachford, SDMM as well a primal dual techniques such as\nChambolle-Pock and forward-backward-forward. It also includes an extensive list\nof common proximal operators that can be combined, allowing for a quick\nimplementation of a large variety of convex problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 15:58:44 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 11:09:57 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 07:38:32 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Kalofolias", "Vassilis", ""], ["Shuman", "David", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1402.0796", "submitter": "Alexandre Lacoste", "authors": "Alexandre Lacoste, Hugo Larochelle, Fran\\c{c}ois Laviolette, Mario\n  Marchand", "title": "Sequential Model-Based Ensemble Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most tedious tasks in the application of machine learning is model\nselection, i.e. hyperparameter selection. Fortunately, recent progress has been\nmade in the automation of this process, through the use of sequential\nmodel-based optimization (SMBO) methods. This can be used to optimize a\ncross-validation performance of a learning algorithm over the value of its\nhyperparameters. However, it is well known that ensembles of learned models\nalmost consistently outperform a single model, even if properly selected. In\nthis paper, we thus propose an extension of SMBO methods that automatically\nconstructs such ensembles. This method builds on a recently proposed ensemble\nconstruction paradigm known as agnostic Bayesian learning. In experiments on 22\nregression and 39 classification data sets, we confirm the success of this\nproposed approach, which is able to outperform model selection with SMBO.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 17:01:16 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Lacoste", "Alexandre", ""], ["Larochelle", "Hugo", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1402.0859", "submitter": "Varun Jampani", "authors": "Varun Jampani and Sebastian Nowozin and Matthew Loper and Peter V.\n  Gehler", "title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in\n  Generative Computer Vision Models", "comments": "Appearing in Computer Vision and Image Understanding Journal (Special\n  Issue on Generative Models in Computer Vision)", "journal-ref": null, "doi": "10.1016/j.cviu.2015.03.002", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is hard because of a large variability in lighting, shape,\nand texture; in addition the image signal is non-additive due to occlusion.\nGenerative models promised to account for this variability by accurately\nmodelling the image formation process as a function of latent variables with\nprior beliefs. Bayesian posterior inference could then, in principle, explain\nthe observation. While intuitively appealing, generative models for computer\nvision have largely failed to deliver on that promise due to the difficulty of\nposterior inference. As a result the community has favoured efficient\ndiscriminative approaches. We still believe in the usefulness of generative\nmodels in computer vision, but argue that we need to leverage existing\ndiscriminative or even heuristic computer vision methods. We implement this\nidea in a principled way with an \"informed sampler\" and in careful experiments\ndemonstrate it on challenging generative models which contain renderer programs\nas their components. We concentrate on the problem of inverting an existing\ngraphics rendering engine, an approach that can be understood as \"Inverse\nGraphics\". The informed sampler, using simple discriminative proposals based on\nexisting computer vision technology, achieves significant improvements of\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 20:52:26 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 11:28:13 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2015 19:50:59 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Jampani", "Varun", ""], ["Nowozin", "Sebastian", ""], ["Loper", "Matthew", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1402.0914", "submitter": "Scott Linderman", "authors": "Scott W. Linderman and Ryan P. Adams", "title": "Discovering Latent Network Structure in Point Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks play a central role in modern data analysis, enabling us to reason\nabout systems by studying the relationships between their parts. Most often in\nnetwork analysis, the edges are given. However, in many systems it is difficult\nor impossible to measure the network directly. Examples of latent networks\ninclude economic interactions linking financial instruments and patterns of\nreciprocity in gang violence. In these cases, we are limited to noisy\nobservations of events associated with each node. To enable analysis of these\nimplicit networks, we develop a probabilistic model that combines\nmutually-exciting point processes with random graph models. We show how the\nPoisson superposition principle enables an elegant auxiliary variable\nformulation and a fully-Bayesian, parallel inference algorithm. We evaluate\nthis new model empirically on several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 23:48:23 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Linderman", "Scott W.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.0915", "submitter": "Oren Rippel", "authors": "Oren Rippel, Michael A. Gelbart, Ryan P. Adams", "title": "Learning Ordered Representations with Nested Dropout", "comments": "11 pages, 5 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study ordered representations of data in which different\ndimensions have different degrees of importance. To learn these representations\nwe introduce nested dropout, a procedure for stochastically removing coherent\nnested sets of hidden units in a neural network. We first present a sequence of\ntheoretical results in the simple case of a semi-linear autoencoder. We\nrigorously show that the application of nested dropout enforces identifiability\nof the units, which leads to an exact equivalence with PCA. We then extend the\nalgorithm to deep models and demonstrate the relevance of ordered\nrepresentations to a number of applications. Specifically, we use the ordered\nproperty of the learned codes to construct hash-based data structures that\npermit very fast retrieval, achieving retrieval in time logarithmic in the\ndatabase size and independent of the dimensionality of the representation. This\nallows codes that are hundreds of times longer than currently feasible for\nretrieval. We therefore avoid the diminished quality associated with short\ncodes, while still performing retrieval that is competitive in speed with\nexisting methods. We also show that ordered representations are a promising way\nto learn adaptive compression for efficient online data reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 00:41:58 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Rippel", "Oren", ""], ["Gelbart", "Michael A.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.0929", "submitter": "Jasper Snoek", "authors": "Jasper Snoek, Kevin Swersky, Richard S. Zemel and Ryan P. Adams", "title": "Input Warping for Bayesian Optimization of Non-stationary Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has proven to be a highly effective methodology for the\nglobal optimization of unknown, expensive and multimodal functions. The ability\nto accurately model distributions over functions is critical to the\neffectiveness of Bayesian optimization. Although Gaussian processes provide a\nflexible prior over functions which can be queried efficiently, there are\nvarious classes of functions that remain difficult to model. One of the most\nfrequently occurring of these is the class of non-stationary functions. The\noptimization of the hyperparameters of machine learning algorithms is a problem\ndomain in which parameters are often manually transformed a priori, for example\nby optimizing in \"log-space,\" to mitigate the effects of spatially-varying\nlength scale. We develop a methodology for automatically learning a wide family\nof bijective transformations or warpings of the input space using the Beta\ncumulative distribution function. We further extend the warping framework to\nmulti-task Bayesian optimization so that multiple tasks can be warped into a\njointly stationary space. On a set of challenging benchmark optimization tasks,\nwe observe that the inclusion of warping greatly improves on the\nstate-of-the-art, producing better results faster and more reliably.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 03:55:39 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 22:00:38 GMT"}, {"version": "v3", "created": "Wed, 11 Jun 2014 20:32:11 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Snoek", "Jasper", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard S.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1402.1128", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Fran\\c{c}oise Beaufays", "title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:01:51 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1402.1267", "submitter": "Jiaming Xu", "authors": "Yudong Chen, Jiaming Xu", "title": "Statistical-Computational Tradeoffs in Planted Problems and Submatrix\n  Localization with a Growing Number of Clusters and Submatrices", "comments": "We updated the statements for Theorems 2.1 and 2.3. Partial results\n  appeared at the International Conference on Machine Learning (ICML) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two closely related problems: planted clustering and submatrix\nlocalization. The planted clustering problem assumes that a random graph is\ngenerated based on some underlying clusters of the nodes; the task is to\nrecover these clusters given the graph. The submatrix localization problem\nconcerns locating hidden submatrices with elevated means inside a large\nreal-valued random matrix. Of particular interest is the setting where the\nnumber of clusters/submatrices is allowed to grow unbounded with the problem\nsize. These formulations cover several classical models such as planted clique,\nplanted densest subgraph, planted partition, planted coloring, and stochastic\nblock model, which are widely used for studying community detection and\nclustering/bi-clustering.\n  For both problems, we show that the space of the model parameters\n(cluster/submatrix size, cluster density, and submatrix mean) can be\npartitioned into four disjoint regions corresponding to decreasing statistical\nand computational complexities: (1) the \\emph{impossible} regime, where all\nalgorithms fail; (2) the \\emph{hard} regime, where the computationally\nexpensive Maximum Likelihood Estimator (MLE) succeeds; (3) the \\emph{easy}\nregime, where the polynomial-time convexified MLE succeeds; (4) the\n\\emph{simple} regime, where a simple counting/thresholding procedure succeeds.\nMoreover, we show that each of these algorithms provably fails in the previous\nharder regimes.\n  Our theorems establish the minimax recovery limit, which are tight up to\nconstants and hold with a growing number of clusters/submatrices, and provide a\nstronger performance guarantee than previously known for polynomial-time\nalgorithms. Our study demonstrates the tradeoffs between statistical and\ncomputational considerations, and suggests that the minimax recovery limit may\nnot be achievable by polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 07:58:38 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 18:17:32 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 19:54:55 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Chen", "Yudong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1402.1298", "submitter": "Lenka Zdeborova", "authors": "Yoshiyuki Kabashima, Florent Krzakala, Marc M\\'ezard, Ayaka Sakata,\n  and Lenka Zdeborov\\'a", "title": "Phase transitions and sample complexity in Bayes-optimal matrix\n  factorization", "comments": "50 pages, 10 figures", "journal-ref": "IEEE Transactions on Information Theory (Volume:62 , Issue: 7,\n  Pages: 4228 - 4265) 2016", "doi": "10.1109/TIT.2016.2556702", "report-no": null, "categories": "cs.NA cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the matrix factorization problem. Given a noisy measurement of a\nproduct of two matrices, the problem is to estimate back the original matrices.\nIt arises in many applications such as dictionary learning, blind matrix\ncalibration, sparse principal component analysis, blind source separation, low\nrank matrix completion, robust principal component analysis or factor analysis.\nIt is also important in machine learning: unsupervised representation learning\ncan often be studied through matrix factorization. We use the tools of\nstatistical mechanics - the cavity and replica methods - to analyze the\nachievability and computational tractability of the inference problems in the\nsetting of Bayes-optimal inference, which amounts to assuming that the two\nmatrices have random independent elements generated from some known\ndistribution, and this information is available to the inference algorithm. In\nthis setting, we compute the minimal mean-squared-error achievable in principle\nin any computational time, and the error that can be achieved by an efficient\napproximate message passing algorithm. The computation is based on the\nasymptotic state-evolution analysis of the algorithm. The performance that our\nanalysis predicts, both in terms of the achieved mean-squared-error, and in\nterms of sample complexity, is extremely promising and motivating for a further\ndevelopment of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 09:56:50 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 20:56:04 GMT"}, {"version": "v3", "created": "Mon, 21 Mar 2016 18:07:08 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kabashima", "Yoshiyuki", ""], ["Krzakala", "Florent", ""], ["M\u00e9zard", "Marc", ""], ["Sakata", "Ayaka", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1402.1349", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, Marco Loog", "title": "Dissimilarity-based Ensembles for Multiple Instance Learning", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems, Special Issue on Learning in Non-(geo)metric Spaces", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, Volume\n  27, Issue 6, 2016, pages 1379 - 1391", "doi": "10.1109/TNNLS.2015.2424254", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple instance learning, objects are sets (bags) of feature vectors\n(instances) rather than individual feature vectors. In this paper we address\nthe problem of how these bags can best be represented. Two standard approaches\nare to use (dis)similarities between bags and prototype bags, or between bags\nand prototype instances. The first approach results in a relatively\nlow-dimensional representation determined by the number of training bags, while\nthe second approach results in a relatively high-dimensional representation,\ndetermined by the total number of instances in the training set. In this paper\na third, intermediate approach is proposed, which links the two approaches and\ncombines their strengths. Our classifier is inspired by a random subspace\nensemble, and considers subspaces of the dissimilarity space, defined by\nsubsets of instances, as prototypes. We provide guidelines for using such an\nensemble, and show state-of-the-art performances on a range of multiple\ninstance learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 13:35:01 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1402.1389", "submitter": "Yarin Gal", "authors": "Yarin Gal, Mark van der Wilk, Carl E. Rasmussen", "title": "Distributed Variational Inference in Sparse Gaussian Process Regression\n  and Latent Variable Models", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are a powerful tool for probabilistic inference over\nfunctions. They have been applied to both regression and non-linear\ndimensionality reduction, and offer desirable properties such as uncertainty\nestimates, robustness to over-fitting, and principled ways for tuning\nhyper-parameters. However the scalability of these models to big datasets\nremains an active topic of research. We introduce a novel re-parametrisation of\nvariational inference for sparse GP regression and latent variable models that\nallows for an efficient distributed algorithm. This is done by exploiting the\ndecoupling of the data given the inducing points to re-formulate the evidence\nlower bound in a Map-Reduce setting. We show that the inference scales well\nwith data and computational resources, while preserving a balanced distribution\nof the load among the nodes. We further demonstrate the utility in scaling\nGaussian processes to big data. We show that GP performance improves with\nincreasing amounts of data in regression (on flight data with 2 million\nrecords) and latent variable modelling (on MNIST). The results show that GPs\nperform better than many common models often used for big data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 16:08:40 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 21:16:47 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Gal", "Yarin", ""], ["van der Wilk", "Mark", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1402.1412", "submitter": "Yarin Gal", "authors": "Yarin Gal, Mark van der Wilk", "title": "Variational Inference in Sparse Gaussian Process Regression and Latent\n  Variable Models - a Gentle Tutorial", "comments": "20 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial we explain the inference procedures developed for the sparse\nGaussian process (GP) regression and Gaussian process latent variable model\n(GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias &\nLawrence (2010) is brief, hence getting a full picture of it requires\ncollecting results from several different sources and a substantial amount of\nalgebra to fill-in the gaps. Our main goal is thus to collect all the results\nand full derivations into one place to help speed up understanding this work.\nIn doing so we present a re-parametrisation of the inference that allows it to\nbe carried out in parallel. A secondary goal for this document is, therefore,\nto accompany our paper and open-source implementation of the parallel inference\nscheme for the models. We hope that this document will bridge the gap between\nthe equations as implemented in code and those published in the original\npapers, in order to make it easier to extend existing work. We assume prior\nknowledge of Gaussian processes and variational inference, but we also include\nreferences for further reading where appropriate.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 17:10:24 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 21:38:31 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Gal", "Yarin", ""], ["van der Wilk", "Mark", ""]]}, {"id": "1402.1454", "submitter": "Sarath Chandar A P", "authors": "Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M.\n  Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-language learning allows us to use training data from one language to\nbuild models for a different language. Many approaches to bilingual learning\nrequire that we have word-level alignment of sentences from parallel corpora.\nIn this work we explore the use of autoencoder-based methods for cross-language\nlearning of vectorial word representations that are aligned between two\nlanguages, while not relying on word-level alignments. We show that by simply\nlearning to reconstruct the bag-of-words representations of aligned sentences,\nwithin and between languages, we can in fact learn high-quality representations\nand do without word alignments. Since training autoencoders on word\nobservations presents certain computational issues, we propose and compare\ndifferent variations adapted to this setting. We also propose an explicit\ncorrelation maximizing regularizer that leads to significant improvement in the\nperformance. We empirically investigate the success of our approach on the\nproblem of cross-language test classification, where a classifier trained on a\ngiven language (e.g., English) must learn to generalize to a different language\n(e.g., German). These experiments demonstrate that our approaches are\ncompetitive with the state-of-the-art, achieving up to 10-14 percentage point\nimprovements over the best reported results on this task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 18:53:30 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["P", "Sarath Chandar A", ""], ["Lauly", "Stanislas", ""], ["Larochelle", "Hugo", ""], ["Khapra", "Mitesh M.", ""], ["Ravindran", "Balaraman", ""], ["Raykar", "Vikas", ""], ["Saha", "Amrita", ""]]}, {"id": "1402.1473", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Leonidas J. Guibas and Qi-Xing Huang", "title": "Near-Optimal Joint Object Matching via Convex Relaxation", "comments": null, "journal-ref": "31st International Conference on Machine Learning, vol. 32, pp.\n  100 - 108, June 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint matching over a collection of objects aims at aggregating information\nfrom a large collection of similar instances (e.g. images, graphs, shapes) to\nimprove maps between pairs of them. Given multiple matches computed between a\nfew object pairs in isolation, the goal is to recover an entire collection of\nmaps that are (1) globally consistent, and (2) close to the provided maps ---\nand under certain conditions provably the ground-truth maps. Despite recent\nadvances on this problem, the best-known recovery guarantees are limited to a\nsmall constant barrier --- none of the existing methods find theoretical\nsupport when more than $50\\%$ of input correspondences are corrupted. Moreover,\nprior approaches focus mostly on fully similar objects, while it is practically\nmore demanding to match instances that are only partially similar to each\nother.\n  In this paper, we develop an algorithm to jointly match multiple objects that\nexhibit only partial similarities, given a few pairwise matches that are\ndensely corrupted. Specifically, we propose to recover the ground-truth maps\nvia a parameter-free convex program called MatchLift, following a spectral\nmethod that pre-estimates the total number of distinct elements to be matched.\nEncouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.\nin the asymptotic regime it is guaranteed to work even when a dominant fraction\n$1-\\Theta\\left(\\frac{\\log^{2}n}{\\sqrt{n}}\\right)$ of the input maps behave like\nrandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,\nnamely, perfect matching can be achieved as soon as the provided maps form a\nconnected map graph. We evaluate the proposed algorithm on various benchmark\ndata sets including synthetic examples and real-world examples, all of which\nconfirm the practical applicability of MatchLift.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:16:35 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Chen", "Yuxin", ""], ["Guibas", "Leonidas J.", ""], ["Huang", "Qi-Xing", ""]]}, {"id": "1402.1700", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan and Mohamed Hebiri, and Johannes Lederer", "title": "On the Prediction Performance of the Lasso", "comments": null, "journal-ref": "Bernoulli 23(1), 2017, 552-581", "doi": "10.3150/15-BEJ756", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Lasso has been extensively studied, the relationship between its\nprediction performance and the correlations of the covariates is not fully\nunderstood. In this paper, we give new insights into this relationship in the\ncontext of multiple linear regression. We show, in particular, that the\nincorporation of a simple correlation measure into the tuning parameter can\nlead to a nearly optimal prediction performance of the Lasso even for highly\ncorrelated covariates. However, we also reveal that for moderately correlated\ncovariates, the prediction performance of the Lasso can be mediocre\nirrespective of the choice of the tuning parameter. We finally show that our\nresults also lead to near-optimal rates for the least-squares estimator with\ntotal variation penalty.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 17:23:32 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 12:47:26 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Hebiri", "Mohamed", ""], ["Lederer", "Johannes", ""]]}, {"id": "1402.1754", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur", "title": "Two-stage Sampled Learning Theory on Distributions", "comments": "v6: accepted at AISTATS-2015 for oral presentation; final version;\n  code: https://bitbucket.org/szzoli/ite/; extension to the misspecified and\n  vector-valued case: http://arxiv.org/abs/1411.2066", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the distribution regression problem: regressing to a real-valued\nresponse from a probability distribution. Although there exist a large number\nof similarity measures between distributions, very little is known about their\ngeneralization performance in specific learning tasks. Learning problems\nformulated on distributions have an inherent two-stage sampled difficulty: in\npractice only samples from sampled distributions are observable, and one has to\nbuild an estimate on similarities computed between sets of points. To the best\nof our knowledge, the only existing method with consistency guarantees for\ndistribution regression requires kernel density estimation as an intermediate\nstep (which suffers from slow convergence issues in high dimensions), and the\ndomain of the distributions to be compact Euclidean. In this paper, we provide\ntheoretical guarantees for a remarkably simple algorithmic alternative to solve\nthe distribution regression problem: embed the distributions to a reproducing\nkernel Hilbert space, and learn a ridge regressor from the embeddings to the\noutputs. Our main contribution is to prove the consistency of this technique in\nthe two-stage sampled setting under mild conditions (on separable, topological\ndomains endowed with kernels). For a given total number of observations, we\nderive convergence rates as an explicit function of the problem difficulty. As\na special case, we answer a 15-year-old open question: we establish the\nconsistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002]\nin regression, and cover more recent kernels on distributions, including those\ndue to [Christmann and Steinwart, 2010].\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 20:37:59 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 11:35:58 GMT"}, {"version": "v3", "created": "Sun, 4 May 2014 19:29:36 GMT"}, {"version": "v4", "created": "Sat, 7 Jun 2014 17:42:06 GMT"}, {"version": "v5", "created": "Sat, 25 Oct 2014 21:03:01 GMT"}, {"version": "v6", "created": "Mon, 26 Jan 2015 22:20:59 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Szabo", "Zoltan", ""], ["Gretton", "Arthur", ""], ["Poczos", "Barnabas", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "1402.1783", "submitter": "Jason J Corso", "authors": "Caiming Xiong, David Johnson, Jason J. Corso", "title": "Active Clustering with Model-Based Uncertainty Reduction", "comments": "14 pages, 8 figures, submitted to TPAMI (second version just fixes a\n  missing reference and format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering seeks to augment traditional clustering methods by\nincorporating side information provided via human expertise in order to\nincrease the semantic meaningfulness of the resulting clusters. However, most\ncurrent methods are \\emph{passive} in the sense that the side information is\nprovided beforehand and selected randomly. This may require a large number of\nconstraints, some of which could be redundant, unnecessary, or even detrimental\nto the clustering results. Thus in order to scale such semi-supervised\nalgorithms to larger problems it is desirable to pursue an \\emph{active}\nclustering method---i.e. an algorithm that maximizes the effectiveness of the\navailable human labor by only requesting human input where it will have the\ngreatest impact. Here, we propose a novel online framework for active\nsemi-supervised spectral clustering that selects pairwise constraints as\nclustering proceeds, based on the principle of uncertainty reduction. Using a\nfirst-order Taylor expansion, we decompose the expected uncertainty reduction\nproblem into a gradient and a step-scale, computed via an application of matrix\nperturbation theory and cluster-assignment entropy, respectively. The resulting\nmodel is used to estimate the uncertainty reduction potential of each sample in\nthe dataset. We then present the human user with pairwise queries with respect\nto only the best candidate sample. We evaluate our method using three different\nimage datasets (faces, leaves and dogs), a set of common UCI machine learning\ndatasets and a gene dataset. The results validate our decomposition formulation\nand show that our method is consistently superior to existing state-of-the-art\ntechniques, as well as being robust to noise and to unknown numbers of\nclusters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 22:13:03 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 02:53:32 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Xiong", "Caiming", ""], ["Johnson", "David", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.1792", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi, Lijun Zhang, and Rong Jin", "title": "Binary Excess Risk for Smooth Convex Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical learning theory, convex surrogates of the 0-1 loss are highly\npreferred because of the computational and theoretical virtues that convexity\nbrings in. This is of more importance if we consider smooth surrogates as\nwitnessed by the fact that the smoothness is further beneficial both\ncomputationally- by attaining an {\\it optimal} convergence rate for\noptimization, and in a statistical sense- by providing an improved {\\it\noptimistic} rate for generalization bound. In this paper we investigate the\nsmoothness property from the viewpoint of statistical consistency and show how\nit affects the binary excess risk. We show that in contrast to optimization and\ngeneralization errors that favor the choice of smooth surrogate loss, the\nsmoothness of loss function may degrade the binary excess risk. Motivated by\nthis negative result, we provide a unified analysis that integrates\noptimization error, generalization bound, and the error in translating convex\nexcess risk into a binary excess risk when examining the impact of smoothness\non the binary excess risk. We show that under favorable conditions appropriate\nchoice of smooth convex loss will result in a binary excess risk that is better\nthan $O(1/\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 23:02:50 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Zhang", "Lijun", ""], ["Jin", "Rong", ""]]}, {"id": "1402.1864", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "An Inequality with Applications to Structured Sparsity and Multitask\n  Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From concentration inequalities for the suprema of Gaussian or Rademacher\nprocesses an inequality is derived. It is applied to sharpen existing and to\nderive novel bounds on the empirical Rademacher complexities of unit balls in\nvarious norms appearing in the context of structured sparsity and multitask\ndictionary learning or matrix factorization. A key role is played by the\nlargest eigenvalue of the data covariance matrix.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 16:39:30 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 07:37:52 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1402.1869", "submitter": "KyungHyun Cho", "authors": "Guido Mont\\'ufar, Razvan Pascanu, Kyunghyun Cho and Yoshua Bengio", "title": "On the Number of Linear Regions of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of functions computable by deep feedforward neural\nnetworks with piecewise linear activations in terms of the symmetries and the\nnumber of linear regions that they have. Deep networks are able to sequentially\nmap portions of each layer's input-space to the same output. In this way, deep\nmodels compute functions that react equally to complicated patterns of\ndifferent inputs. The compositional structure of these functions enables them\nto re-use pieces of computation exponentially often in terms of the network's\ndepth. This paper investigates the complexity of such compositional maps and\ncontributes new theoretical results regarding the advantage of depth for neural\nnetworks with piecewise linear activation functions. In particular, our\nanalysis is not specific to a single family of models, and as an example, we\nemploy it for rectifier and maxout networks. We improve complexity bounds from\npre-existing work and investigate the behavior of units in higher layers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 17:16:27 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 19:56:14 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Mont\u00fafar", "Guido", ""], ["Pascanu", "Razvan", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1402.1892", "submitter": "Zachary Lipton", "authors": "Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy", "title": "Thresholding Classifiers to Maximize F1 Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new insight into maximizing F1 scores in the context of\nbinary classification and also in the context of multilabel classification. The\nharmonic mean of precision and recall, F1 score is widely used to measure the\nsuccess of a binary classifier when one class is rare. Micro average, macro\naverage, and per instance average F1 scores are used in multilabel\nclassification. For any classifier that produces a real-valued output, we\nderive the relationship between the best achievable F1 score and the\ndecision-making threshold that achieves this optimum. As a special case, if the\nclassifier outputs are well-calibrated conditional probabilities, then the\noptimal threshold is half the optimal F1 score. As another special case, if the\nclassifier is completely uninformative, then the optimal behavior is to\nclassify all examples as positive. Since the actual prevalence of positive\nexamples typically is low, this behavior can be considered undesirable. As a\ncase study, we discuss the results, which can be surprising, of applying this\nprocedure when predicting 26,853 labels for Medline documents.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 21:14:29 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 01:29:47 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Lipton", "Zachary Chase", ""], ["Elkan", "Charles", ""], ["Narayanaswamy", "Balakrishnan", ""]]}, {"id": "1402.1958", "submitter": "Arthur Guez", "authors": "Arthur Guez, David Silver, Peter Dayan", "title": "Better Optimism By Bayes: Adaptive Planning with Rich Models", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 15:38:57 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Guez", "Arthur", ""], ["Silver", "David", ""], ["Dayan", "Peter", ""]]}, {"id": "1402.1973", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Mike Davies, Pascal Frossard", "title": "Dictionary learning for fast classification based on soft-thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers based on sparse representations have recently been shown to\nprovide excellent results in many visual recognition and classification tasks.\nHowever, the high cost of computing sparse representations at test time is a\nmajor obstacle that limits the applicability of these methods in large-scale\nproblems, or in scenarios where computational power is restricted. We consider\nin this paper a simple yet efficient alternative to sparse coding for feature\nextraction. We study a classification scheme that applies the soft-thresholding\nnonlinear mapping in a dictionary, followed by a linear classifier. A novel\nsupervised dictionary learning algorithm tailored for this low complexity\nclassification architecture is proposed. The dictionary learning problem, which\njointly learns the dictionary and linear classifier, is cast as a difference of\nconvex (DC) program and solved efficiently with an iterative DC solver. We\nconduct experiments on several datasets, and show that our learning algorithm\nthat leverages the structure of the classification problem outperforms generic\nlearning procedures. Our simple classifier based on soft-thresholding also\ncompetes with the recent sparse coding classifiers, when the dictionary is\nlearned appropriately. The adopted classification scheme further requires less\ncomputational time at the testing stage, compared to other classifiers. The\nproposed scheme shows the potential of the adequately trained soft-thresholding\nmapping for classification and paves the way towards the development of very\nefficient classification methods for vision problems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 18:18:33 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 16:45:19 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Davies", "Mike", ""], ["Frossard", "Pascal", ""]]}, {"id": "1402.2026", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Genomic Prediction of Quantitative Traits using Sparse and Locally\n  Epistatic Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1302.3463", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In plant and animal breeding studies a distinction is made between the\ngenetic value (additive + epistatic genetic effects) and the breeding value\n(additive genetic effects) of an individual since it is expected that some of\nthe epistatic genetic effects will be lost due to recombination. In this paper,\nwe argue that the breeder can take advantage of some of the epistatic marker\neffects in regions of low recombination. The models introduced here aim to\nestimate local epistatic line heritability by using the genetic map information\nand combine the local additive and epistatic effects. To this end, we have used\nsemi-parametric mixed models with multiple local genomic relationship matrices\nwith hierarchical designs and lasso post-processing for sparsity in the final\nmodel. Our models produce good predictive performance along with good\nexplanatory information.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 03:30:17 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1402.2043", "submitter": "Gilles Stoltz", "authors": "Shie Mannor (EE-Technion), Vianney Perchet, Gilles Stoltz (GREGH)", "title": "Approachability in unknown games: Online learning meets multi-objective\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the standard setting of approachability there are two players and a target\nset. The players play repeatedly a known vector-valued game where the first\nplayer wants to have the average vector-valued payoff converge to the target\nset which the other player tries to exclude it from this set. We revisit this\nsetting in the spirit of online learning and do not assume that the first\nplayer knows the game structure: she receives an arbitrary vector-valued reward\nvector at every round. She wishes to approach the smallest (\"best\") possible\nset given the observed average payoffs in hindsight. This extension of the\nstandard setting has implications even when the original target set is not\napproachable and when it is not obvious which expansion of it should be\napproached instead. We show that it is impossible, in general, to approach the\nbest target set in hindsight and propose achievable though ambitious\nalternative goals. We further propose a concrete strategy to approach these\ngoals. Our method does not require projection onto a target set and amounts to\nswitching between scalar regret minimization algorithms that are performed in\nepisodes. Applications to global cost minimization and to approachability under\nsample path constraints are considered.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 05:44:40 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 06:52:49 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mannor", "Shie", "", "EE-Technion"], ["Perchet", "Vianney", "", "GREGH"], ["Stoltz", "Gilles", "", "GREGH"]]}, {"id": "1402.2044", "submitter": "Gilles Stoltz", "authors": "Pierre Gaillard (GREGH), Gilles Stoltz (GREGH), Tim Van Erven (INRIA\n  Saclay - Ile de France)", "title": "A Second-order Bound with Excess Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online aggregation of the predictions of experts, and first show new\nsecond-order regret bounds in the standard setting, which are obtained via a\nversion of the Prod algorithm (and also a version of the polynomially weighted\naverage algorithm) with multiple learning rates. These bounds are in terms of\nexcess losses, the differences between the instantaneous losses suffered by the\nalgorithm and the ones of a given expert. We then demonstrate the interest of\nthese bounds in the context of experts that report their confidences as a\nnumber in the interval [0,1] using a generic reduction to the standard setting.\nWe conclude by two other applications in the standard setting, which improve\nthe known bounds in case of small excess losses and show a bounded regret\nagainst i.i.d. sequences of losses.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 05:45:29 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Gaillard", "Pierre", "", "GREGH"], ["Stoltz", "Gilles", "", "GREGH"], ["Van Erven", "Tim", "", "INRIA\n  Saclay - Ile de France"]]}, {"id": "1402.2058", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig", "title": "Probabilistic Interpretation of Linear Solvers", "comments": "final version, in press at SIAM J Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript proposes a probabilistic framework for algorithms that\niteratively solve unconstrained linear problems $Bx = b$ with positive definite\n$B$ for $x$. The goal is to replace the point estimates returned by existing\nmethods with a Gaussian posterior belief over the elements of the inverse of\n$B$, which can be used to estimate errors. Recent probabilistic interpretations\nof the secant family of quasi-Newton optimization algorithms are extended.\nCombined with properties of the conjugate gradient algorithm, this leads to\nuncertainty-calibrated methods with very limited cost overhead over conjugate\ngradients, a self-contained novel interpretation of the quasi-Newton and\nconjugate gradient algorithms, and a foundation for new nonlinear optimization\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 07:56:13 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 08:23:52 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Hennig", "Philipp", ""]]}, {"id": "1402.2148", "submitter": "Ichiro Takeuchi Prof.", "authors": "Yoshiki Suzuki, Kohei Ogawa, Yuki Shinmura and Ichiro Takeuchi", "title": "An Algorithmic Framework for Computing Validation Performance Bounds by\n  Using Suboptimal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical model building processes are often time-consuming because many\ndifferent models must be trained and validated. In this paper, we introduce a\nnovel algorithm that can be used for computing the lower and the upper bounds\nof model validation errors without actually training the model itself. A key\nidea behind our algorithm is using a side information available from a\nsuboptimal model. If a reasonably good suboptimal model is available, our\nalgorithm can compute lower and upper bounds of many useful quantities for\nmaking inferences on the unknown target model. We demonstrate the advantage of\nour algorithm in the context of model selection for regularized learning\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 13:57:32 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Suzuki", "Yoshiki", ""], ["Ogawa", "Kohei", ""], ["Shinmura", "Yuki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1402.2300", "submitter": "Aaron Karper", "authors": "Aaron Karper", "title": "Feature and Variable Selection in Classification", "comments": "Part of master seminar in document analysis held by Marcus\n  Eichenberger-Liwicki", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The amount of information in the form of features and variables avail- able\nto machine learning algorithms is ever increasing. This can lead to classifiers\nthat are prone to overfitting in high dimensions, high di- mensional models do\nnot lend themselves to interpretable results, and the CPU and memory resources\nnecessary to run on high-dimensional datasets severly limit the applications of\nthe approaches. Variable and feature selection aim to remedy this by finding a\nsubset of features that in some way captures the information provided best. In\nthis paper we present the general methodology and highlight some specific\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 21:05:58 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Karper", "Aaron", ""]]}, {"id": "1402.2324", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Prateek Jain", "title": "Universal Matrix Completion", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low-rank matrix completion has recently generated a lot of\ninterest leading to several results that offer exact solutions to the problem.\nHowever, in order to do so, these methods make assumptions that can be quite\nrestrictive in practice. More specifically, the methods assume that: a) the\nobserved indices are sampled uniformly at random, and b) for every new matrix,\nthe observed indices are sampled afresh. In this work, we address these issues\nby providing a universal recovery guarantee for matrix completion that works\nfor a variety of sampling schemes. In particular, we show that if the set of\nsampled indices come from the edges of a bipartite graph with large spectral\ngap (i.e. gap between the first and the second singular value), then the\nnuclear norm minimization based method exactly recovers all low-rank matrices\nthat satisfy certain incoherence properties. Moreover, we also show that under\ncertain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entries\nare enough to recover any rank-$r$ $n\\times n$ matrix, in contrast to the\n$O(nr\\log n)$ sample complexity required by other matrix completion algorithms\nas well as existing analyses of the nuclear norm method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 22:53:15 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 15:21:56 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Jain", "Prateek", ""]]}, {"id": "1402.2333", "submitter": "Vincent Michalski", "authors": "Vincent Michalski, Roland Memisevic, Kishore Konda", "title": "Modeling sequential data using higher-order relational features and\n  predictive training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-linear feature learning models, like the gated autoencoder, were proposed\nas a way to model relationships between frames in a video. By minimizing\nreconstruction error of one frame, given the previous frame, these models learn\n\"mapping units\" that encode the transformations inherent in a sequence, and\nthereby learn to encode motion. In this work we extend bi-linear models by\nintroducing \"higher-order mapping units\" that allow us to encode\ntransformations between frames and transformations between transformations.\n  We show that this makes it possible to encode temporal structure that is more\ncomplex and longer-range than the structure captured within standard bi-linear\nmodels. We also show that a natural way to train the model is by replacing the\ncommonly used reconstruction objective with a prediction objective which forces\nthe model to correctly predict the evolution of the input multiple steps into\nthe future. Learning can be achieved by back-propagating the multi-step\nprediction through time. We test the model on various temporal prediction\ntasks, and show that higher-order mappings and predictive training both yield a\nsignificant improvement over bi-linear models in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 23:53:29 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Michalski", "Vincent", ""], ["Memisevic", "Roland", ""], ["Konda", "Kishore", ""]]}, {"id": "1402.2447", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer, Albert Swart and David van Leeuwen", "title": "A comparison of linear and non-linear calibrations for speaker\n  recognition", "comments": "accepted for Odyssey 2014: The Speaker and Language Recognition\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work on both generative and discriminative score to\nlog-likelihood-ratio calibration, it was shown that linear transforms give good\naccuracy only for a limited range of operating points. Moreover, these methods\nrequired tailoring of the calibration training objective functions in order to\ntarget the desired region of best accuracy. Here, we generalize the linear\nrecipes to non-linear ones. We experiment with a non-linear, non-parametric,\ndiscriminative PAV solution, as well as parametric, generative,\nmaximum-likelihood solutions that use Gaussian, Student's T and\nnormal-inverse-Gaussian score distributions. Experiments on NIST SRE'12 scores\nsuggest that the non-linear methods provide wider ranges of optimal accuracy\nand can be trained without having to resort to objective function tailoring.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 11:13:51 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 10:49:48 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Swart", "Albert", ""], ["van Leeuwen", "David", ""]]}, {"id": "1402.2499", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Bastian Steudel, Naji Shajarisales, Bernhard\n  Sch\\\"olkopf", "title": "Justifying Information-Geometric Causal Inference", "comments": "3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Geometric Causal Inference (IGCI) is a new approach to\ndistinguish between cause and effect for two variables. It is based on an\nindependence assumption between input distribution and causal mechanism that\ncan be phrased in terms of orthogonality in information space. We describe two\nintuitive reinterpretations of this approach that makes IGCI more accessible to\na broader audience.\n  Moreover, we show that the described independence is related to the\nhypothesis that unsupervised learning and semi-supervised learning only works\nfor predicting the cause from the effect and not vice versa.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:24:54 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Janzing", "Dominik", ""], ["Steudel", "Bastian", ""], ["Shajarisales", "Naji", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1402.2594", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "Online Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish optimal rates for online regression for arbitrary classes of\nregression functions in terms of the sequential entropy introduced in (Rakhlin,\nSridharan, Tewari, 2010). The optimal rates are shown to exhibit a phase\ntransition analogous to the i.i.d./statistical learning case, studied in\n(Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situation\nwhen sequential entropy and i.i.d. empirical entropy match, our results point\nto the interesting phenomenon that the rates for statistical learning with\nsquared loss and online nonparametric regression are the same.\n  In addition to a non-algorithmic study of minimax regret, we exhibit a\ngeneric forecaster that enjoys the established optimal rates. We also provide a\nrecipe for designing online regression algorithms that can be computationally\nefficient. We illustrate the techniques by deriving existing and new\nforecasters for the case of finite experts and for online linear regression.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 18:36:11 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1402.2667", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Hariharan Narayanan and Alexander Rakhlin", "title": "On Zeroth-Order Stochastic Convex Optimization via Random Walks", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for zeroth order stochastic convex optimization that\nattains the suboptimality rate of $\\tilde{\\mathcal{O}}(n^{7}T^{-1/2})$ after\n$T$ queries for a convex bounded function $f:{\\mathbb R}^n\\to{\\mathbb R}$. The\nmethod is based on a random walk (the \\emph{Ball Walk}) on the epigraph of the\nfunction. The randomized approach circumvents the problem of gradient\nestimation, and appears to be less sensitive to noisy function evaluations\ncompared to noiseless zeroth order methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:18:11 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Liang", "Tengyuan", ""], ["Narayanan", "Hariharan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1402.2676", "submitter": "Parameswaran Raman", "authors": "Hyokun Yun, Parameswaran Raman, S.V.N. Vishwanathan", "title": "Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:39:54 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 21:08:34 GMT"}, {"version": "v3", "created": "Fri, 11 Apr 2014 06:19:04 GMT"}, {"version": "v4", "created": "Thu, 21 Aug 2014 06:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Yun", "Hyokun", ""], ["Raman", "Parameswaran", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1402.2679", "submitter": "Wen-Yu Hua", "authors": "Wen-Yu Hua and Debashis Ghosh (for the Alzheimer's Disease\n  Neuroimaging Initiative)", "title": "Equivalence of Kernel Machine Regression and Kernel Distance Covariance\n  for Multidimensional Trait Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating genetic markers with a multidimensional phenotype is an important\nyet challenging problem. In this work, we establish the equivalence between two\npopular methods: kernel-machine regression (KMR), and kernel distance\ncovariance (KDC). KMR is a semiparametric regression frameworks that models the\ncovariate effects parametrically, while the genetic markers are considered\nnon-parametrically. KDC represents a class of methods that includes distance\ncovariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which are\nnonparametric tests of independence. We show the equivalence between the score\ntest of KMR and the KDC statistic under certain conditions. This result leads\nto a novel generalization of the KDC test that incorporates the covariates. Our\ncontributions are three-fold: (1) establishing the equivalence between KMR and\nKDC; (2) showing that the principles of kernel machine regression can be\napplied to the interpretation of KDC; (3) the development of a broader class of\nKDC statistics, that the members are the quantities of different kernels. We\ndemonstrate the proposals using simulation studies. Data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) is used to explore the association\nbetween the genetic variants on gene \\emph{FLJ16124} and phenotypes represented\nin 3D structural brain MR images adjusting for age and gender. The results\nsuggest that SNPs of \\emph{FLJ16124} exhibit strong pairwise interaction\neffects that are correlated to the changes of brain region volumes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:52:30 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 00:13:00 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Hua", "Wen-Yu", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Ghosh", "Debashis", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"]]}, {"id": "1402.2864", "submitter": "Liang Dai", "authors": "Liang Dai, Kristiaan Pelckmans", "title": "Sparse Estimation From Noisy Observations of an Overdetermined Linear\n  System", "comments": "This paper is provisionally accepted by Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note studies a method for the efficient estimation of a finite number of\nunknown parameters from linear equations, which are perturbed by Gaussian\nnoise.\n  In case the unknown parameters have only few nonzero entries, the proposed\nestimator performs more efficiently than a traditional approach.\n  The method consists of three steps:\n  (1) a classical Least Squares Estimate (LSE),\n  (2) the support is recovered through a Linear Programming (LP) optimization\nproblem which can be computed using a soft-thresholding step,\n  (3) a de-biasing step using a LSE on the estimated support set.\n  The main contribution of this note is a formal derivation of an associated\nORACLE property of the final estimate.\n  That is, when the number of samples is large enough, the estimate is shown to\nequal the LSE based on the support of the {\\em true} parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 16:02:43 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 18:44:04 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Dai", "Liang", ""], ["Pelckmans", "Kristiaan", ""]]}, {"id": "1402.2966", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry\n  Wasserman", "title": "Nonparametric Estimation of Renyi Divergence and Friends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric estimation of $L_2$, Renyi-$\\alpha$ and\nTsallis-$\\alpha$ divergences between continuous distributions. Our approach is\nto construct estimators for particular integral functionals of two densities\nand translate them into divergence estimators. For the integral functionals,\nour estimators are based on corrections of a preliminary plug-in estimator. We\nshow that these estimators achieve the parametric convergence rate of\n$n^{-1/2}$ when the densities' smoothness, $s$, are both at least $d/4$ where\n$d$ is the dimension. We also derive minimax lower bounds for this problem\nwhich confirm that $s > d/4$ is necessary to achieve the $n^{-1/2}$ rate of\nconvergence. We validate our theoretical guarantees with a number of\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 20:35:11 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 12:47:52 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Kandasamy", "Kirthevasan", ""], ["Poczos", "Barnabas", ""], ["Wasserman", "Larry", ""]]}, {"id": "1402.3032", "submitter": "Ziming Zhang", "authors": "Ziming Zhang", "title": "Regularization for Multiple Kernel Learning via Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in constructing general graph-based\nregularizers for multiple kernel learning (MKL) given a structure which is used\nto describe the way of combining basis kernels. Such structures are represented\nby sum-product networks (SPNs) in our method. Accordingly we propose a new\nconvex regularization method for MLK based on a path-dependent kernel weighting\nfunction which encodes the entire SPN structure in our method. Under certain\nconditions and from the view of probability, this function can be considered to\nfollow multinomial distributions over the weights associated with product nodes\nin SPNs. We also analyze the convexity of our regularizer and the complexity of\nour induced classifiers, and further propose an efficient wrapper algorithm to\noptimize our formulation. In our experiments, we apply our method to ......\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 05:06:53 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Zhang", "Ziming", ""]]}, {"id": "1402.3070", "submitter": "Parth Gupta", "authors": "Parth Gupta, Rafael E. Banchs and Paolo Rosso", "title": "Squeezing bottlenecks: exploring the limits of autoencoder semantic\n  representation capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study on the use of autoencoders for modelling\ntext data, in which (differently from previous studies) we focus our attention\non the following issues: i) we explore the suitability of two different models\nbDA and rsDA for constructing deep autoencoders for text data at the sentence\nlevel; ii) we propose and evaluate two novel metrics for better assessing the\ntext-reconstruction capabilities of autoencoders; and iii) we propose an\nautomatic method to find the critical bottleneck dimensionality for text\nlanguage representations (below which structural information is lost).\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 09:54:01 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Gupta", "Parth", ""], ["Banchs", "Rafael E.", ""], ["Rosso", "Paolo", ""]]}, {"id": "1402.3085", "submitter": "Yue Wu", "authors": "Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani", "title": "Gaussian Process Volatility Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of time-changing variances is an important task in\nthe modeling of financial data. Standard econometric models are often limited\nas they assume rigid functional relationships for the variances. Moreover,\nfunction parameters are usually learned using maximum likelihood, which can\nlead to overfitting. To address these problems we introduce a novel model for\ntime-changing variances using Gaussian Processes. A Gaussian Process (GP)\ndefines a distribution over functions, which allows us to capture highly\nflexible functional relationships for the variances. In addition, we develop an\nonline algorithm to perform inference. The algorithm has two main advantages.\nFirst, it takes a Bayesian approach, thereby avoiding overfitting. Second, it\nis much quicker than current offline inference procedures. Finally, our new\nmodel was evaluated on financial data and showed significant improvement in\npredictive performance over current standard models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 10:48:46 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Wu", "Yue", ""], ["Lobato", "Jose Miguel Hernandez", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.3144", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor", "title": "A Robust Ensemble Approach to Learn From Positive and Unlabeled Data\n  Using SVM Base Models", "comments": "34 pages, 6 figures, 6 tables. Accepted for publication in\n  Neurocomputing: Special Issue on Advances in Learning with Label Noise", "journal-ref": null, "doi": "10.1016/j.neucom.2014.10.081", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to learn binary classifiers when only positive\nand unlabeled instances are available (PU learning). This problem is routinely\ncast as a supervised task with label noise in the negative set. We use an\nensemble of SVM models trained on bootstrap resamples of the training data for\nincreased robustness against label noise. The approach can be considered in a\nbagging framework which provides an intuitive explanation for its mechanics in\na semi-supervised setting. We compared our method to state-of-the-art\napproaches in simulations using multiple public benchmark data sets. The\nincluded benchmark comprises three settings with increasing label noise: (i)\nfully supervised, (ii) PU learning and (iii) PU learning with false positives.\nOur approach shows a marginal improvement over existing methods in the second\nsetting and a significant improvement in the third.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 14:18:17 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 12:29:58 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan A. K.", ""], ["De Moor", "Bart", ""]]}, {"id": "1402.3337", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic, David Krueger", "title": "Zero-bias autoencoders and the benefits of co-adapting features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized training of an autoencoder typically results in hidden unit\nbiases that take on large negative values. We show that negative biases are a\nnatural result of using a hidden layer whose responsibility is to both\nrepresent the input data and act as a selection mechanism that ensures sparsity\nof the representation. We then show that negative biases impede the learning of\ndata distributions whose intrinsic dimensionality is high. We also propose a\nnew activation function that decouples the two roles of the hidden layer and\nthat allows us to learn representations on data with very high intrinsic\ndimensionality, where standard autoencoders typically fail. Since the decoupled\nactivation function acts like an implicit regularizer, the model can be trained\nby minimizing the reconstruction error of training data, without requiring any\nadditional regularization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 23:37:39 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 21:39:48 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 02:07:47 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 01:15:33 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:51:11 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""], ["Krueger", "David", ""]]}, {"id": "1402.3346", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Nihat Ay, Keyan Ghazi-Zahedi", "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann\n  Machines", "comments": "30 pages, 5 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional restricted Boltzmann machines are undirected stochastic neural\nnetworks with a layer of input and output units connected bipartitely to a\nlayer of hidden units. These networks define models of conditional probability\ndistributions on the states of the output units given the states of the input\nunits, parametrized by interaction weights and biases. We address the\nrepresentational power of these models, proving results their ability to\nrepresent conditional Markov random fields and conditional distributions with\nrestricted supports, the minimal size of universal approximators, the maximal\nmodel approximation errors, and on the dimension of the set of representable\nconditional distributions. We contribute new tools for investigating\nconditional probability models, which allow us to improve the results that can\nbe derived from existing work on restricted Boltzmann machine probability\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 17:00:35 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 15:20:04 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Montufar", "Guido", ""], ["Ay", "Nihat", ""], ["Ghazi-Zahedi", "Keyan", ""]]}, {"id": "1402.3405", "submitter": "Daniele Cerra", "authors": "Daniele Cerra, Mihai Datcu, and Peter Reinartz", "title": "Authorship Analysis based on Data Compression", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2014.01.019", "report-no": null, "categories": "cs.CL cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to perform authorship analysis using the Fast Compression\nDistance (FCD), a similarity measure based on compression with dictionaries\ndirectly extracted from the written texts. The FCD computes a similarity\nbetween two documents through an effective binary search on the intersection\nset between the two related dictionaries. In the reported experiments the\nproposed method is applied to documents which are heterogeneous in style,\nwritten in five different languages and coming from different historical\nperiods. Results are comparable to the state of the art and outperform\ntraditional compression-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 09:25:59 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""], ["Reinartz", "Peter", ""]]}, {"id": "1402.3580", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin,\n  Mick D. Mantle, Lynn F. Gladden, Andrew Blake", "title": "Bayesian Inference for NMR Spectroscopy with Applications to Chemical\n  Quantification", "comments": "26 pages, 13 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic\nproperties of atomic nuclei to discover the structure, reaction state and\nchemical environment of molecules. We propose a probabilistic generative model\nand inference procedures for NMR spectroscopy. Specifically, we use a weighted\nsum of trigonometric functions undergoing exponential decay to model free\ninduction decay (FID) signals. We discuss the challenges in estimating the\ncomponents of this general model -- amplitudes, phase shifts, frequencies,\ndecay rates, and noise variances -- and offer practical solutions. We compare\nwith conventional Fourier transform spectroscopy for estimating the relative\nconcentrations of chemicals in a mixture, using synthetic and experimentally\nacquired FID signals. We find the proposed model is particularly robust to low\nsignal to noise ratios (SNR), and overlapping peaks in the Fourier transform of\nthe FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) which\nare not possible with conventional spectroscopy (5% sensitivity).\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 20:47:58 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 02:24:00 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Wu", "Yuting", ""], ["Holland", "Daniel J.", ""], ["Nowozin", "Sebastian", ""], ["Mantle", "Mick D.", ""], ["Gladden", "Lynn F.", ""], ["Blake", "Andrew", ""]]}, {"id": "1402.3722", "submitter": "Yoav Goldberg", "authors": "Yoav Goldberg and Omer Levy", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling\n  word-embedding method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\n  This note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 21:03:02 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Goldberg", "Yoav", ""], ["Levy", "Omer", ""]]}, {"id": "1402.3811", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Zhi-Hua Zhou", "title": "Dropout Rademacher Complexity of Deep Neural Networks", "comments": "20 pagea", "journal-ref": "Science China Information Sciences, 2016, 59(7):\n  072104:1-072104:12", "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great successes of deep neural networks have been witnessed in various real\napplications. Many algorithmic and implementation techniques have been\ndeveloped, however, theoretical understanding of many aspects of deep neural\nnetworks is far from clear. A particular interesting issue is the usefulness of\ndropout, which was motivated from the intuition of preventing complex\nco-adaptation of feature detectors. In this paper, we study the Rademacher\ncomplexity of different types of dropout, and our theoretical results disclose\nthat for shallow neural networks (with one or none hidden layer) dropout is\nable to reduce the Rademacher complexity in polynomial, whereas for deep neural\nnetworks it can amazingly lead to an exponential reduction of the Rademacher\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 15:54:37 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 14:36:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gao", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1402.3973", "submitter": "Sjoerd Dirksen", "authors": "Sjoerd Dirksen", "title": "Dimensionality reduction with subgaussian matrices: a unified theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theory for Euclidean dimensionality reduction with subgaussian\nmatrices which unifies several restricted isometry property and\nJohnson-Lindenstrauss type results obtained earlier for specific data sets. In\nparticular, we recover and, in several cases, improve results for sets of\nsparse and structured sparse vectors, low-rank matrices and tensors, and smooth\nmanifolds. In addition, we establish a new Johnson-Lindenstrauss embedding for\ndata sets taking the form of an infinite union of subspaces of a Hilbert space.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 11:51:15 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Dirksen", "Sjoerd", ""]]}, {"id": "1402.4053", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly and Martin Ehler", "title": "The Algebraic Approach to Phase Retrieval and Explicit Inversion at the\n  Identifiability Threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV cs.IT math.AG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study phase retrieval from magnitude measurements of an unknown signal as\nan algebraic estimation problem. Indeed, phase retrieval from rank-one and more\ngeneral linear measurements can be treated in an algebraic way. It is verified\nthat a certain number of generic rank-one or generic linear measurements are\nsufficient to enable signal reconstruction for generic signals, and slightly\nmore generic measurements yield reconstructability for all signals. Our results\nsolve a few open problems stated in the recent literature. Furthermore, we show\nhow the algebraic estimation problem can be solved by a closed-form algebraic\nestimation technique, termed ideal regression, providing non-asymptotic success\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 16:49:38 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Ehler", "Martin", ""]]}, {"id": "1402.4078", "submitter": "Alexander Jung", "authors": "Alexander Jung and Yonina C. Eldar and Norbert G\\\"ortz", "title": "Performance Limits of Dictionary Learning for Sparse Coding", "comments": "to appear in Proc. of EUSIPCO 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dictionary learning under the assumption that the\nobserved signals can be represented as sparse linear combinations of the\ncolumns of a single large dictionary matrix. In particular, we analyze the\nminimax risk of the dictionary learning problem which governs the mean squared\nerror (MSE) performance of any learning scheme, regardless of its computational\ncomplexity. By following an established information-theoretic method based on\nFanos inequality, we derive a lower bound on the minimax risk for a given\ndictionary learning problem. This lower bound yields a characterization of the\nsample-complexity, i.e., a lower bound on the required number of observations\nsuch that consistent dictionary learning schemes exist. Our bounds may be\ncompared with the performance of a given learning scheme, allowing to\ncharacterize how far the method is from optimal performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:39:24 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 16:55:51 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Jung", "Alexander", ""], ["Eldar", "Yonina C.", ""], ["G\u00f6rtz", "Norbert", ""]]}, {"id": "1402.4102", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Emily B. Fox, Carlos Guestrin", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "comments": "ICML 2014 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 19:57:59 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 06:38:21 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Chen", "Tianqi", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1402.4279", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster", "title": "A Bayesian Model of node interaction in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We are concerned with modeling the strength of links in networks by taking\ninto account how often those links are used. Link usage is a strong indicator\nof how closely two nodes are related, but existing network models in Bayesian\nStatistics and Machine Learning are able to predict only wether a link exists\nat all. As priors for latent attributes of network nodes we explore the Chinese\nRestaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.\nThe model is applied to a social network dataset and a word coocurrence\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:34:41 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 10:22:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Schuster", "Ingmar", ""]]}, {"id": "1402.4293", "submitter": "Alexander Davies", "authors": "Alex Davies, Zoubin Ghahramani", "title": "The Random Forest Kernel and other kernels for big data from random\n  partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Random Partition Kernels, a new class of kernels derived by\ndemonstrating a natural connection between random partitions of objects and\nkernels between those objects. We show how the construction can be used to\ncreate kernels from methods that would not normally be viewed as random\npartitions, such as Random Forest. To demonstrate the potential of this method,\nwe propose two new kernels, the Random Forest Kernel and the Fast Cluster\nKernel, and show that these kernels consistently outperform standard kernels on\nproblems involving real-world datasets. Finally, we show how the form of these\nkernels lend themselves to a natural approximation that is appropriate for\ncertain big data problems, allowing $O(N)$ inference in methods such as\nGaussian Processes, Support Vector Machines and Kernel PCA.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:13:45 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Davies", "Alex", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4304", "submitter": "James Lloyd", "authors": "James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum,\n  Zoubin Ghahramani", "title": "Automatic Construction and Natural-Language Description of Nonparametric\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the beginnings of an automatic statistician, focusing on\nregression problems. Our system explores an open-ended space of statistical\nmodels to discover a good explanation of a data set, and then produces a\ndetailed report with figures and natural-language text. Our approach treats\nunknown regression functions nonparametrically using Gaussian processes, which\nhas two important consequences. First, Gaussian processes can model functions\nin terms of high-level properties (e.g. smoothness, trends, periodicity,\nchangepoints). Taken together with the compositional structure of our language\nof models this allows us to automatically describe functions in simple terms.\nSecond, the use of flexible nonparametric models and a rich language for\ncomposing them in an open-ended manner also results in state-of-the-art\nextrapolation performance evaluated over 13 real time series data sets from\nvarious domains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:38:11 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 15:38:36 GMT"}, {"version": "v3", "created": "Thu, 24 Apr 2014 11:44:13 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Lloyd", "James Robert", ""], ["Duvenaud", "David", ""], ["Grosse", "Roger", ""], ["Tenenbaum", "Joshua B.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4306", "submitter": "Amar Shah", "authors": "Amar Shah, Andrew Gordon Wilson and Zoubin Ghahramani", "title": "Student-t Processes as Alternatives to Gaussian Processes", "comments": "13 pages, 6 figures, 1 table. To appear in \"The Seventeenth\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Student-t process as an alternative to the Gaussian\nprocess as a nonparametric prior over functions. We derive closed form\nexpressions for the marginal likelihood and predictive distribution of a\nStudent-t process, by integrating away an inverse Wishart process prior over\nthe covariance kernel of a Gaussian process model. We show surprising\nequivalences between different hierarchical Gaussian process models leading to\nStudent-t processes, and derive a new sampling scheme for the inverse Wishart\nprocess, which helps elucidate these equivalences. Overall, we show that a\nStudent-t process can retain the attractive properties of a Gaussian process --\na nonparametric representation, analytic marginal and predictive distributions,\nand easy model selection through covariance kernels -- but has enhanced\nflexibility, and predictive covariances that, unlike a Gaussian process,\nexplicitly depend on the values of training observations. We verify empirically\nthat a Student-t process is especially useful in situations where there are\nchanges in covariance structure, or in applications like Bayesian optimization,\nwhere accurate predictive covariances are critical for good performance. These\nadvantages come at no additional computational cost over Gaussian processes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:49:16 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Shah", "Amar", ""], ["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4354", "submitter": "Stefano Teso", "authors": "Stefano Teso and Roberto Sebastiani and Andrea Passerini", "title": "Hybrid SRL with Optimization Modulo Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally speaking, the goal of constructive learning could be seen as, given\nan example set of structured objects, to generate novel objects with similar\nproperties. From a statistical-relational learning (SRL) viewpoint, the task\ncan be interpreted as a constraint satisfaction problem, i.e. the generated\nobjects must obey a set of soft constraints, whose weights are estimated from\nthe data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL)\nas a description language, and on MAX-SAT solvers to perform inference. Alas,\nFOL is unsuited for con- structive problems where the objects contain a mixture\nof Boolean and numerical variables. It is in fact difficult to implement, e.g.\nlinear arithmetic constraints within the language of FOL. In this paper we\npropose a novel class of hybrid SRL methods that rely on Satisfiability Modulo\nTheories, an alternative class of for- mal languages that allow to describe,\nand reason over, mixed Boolean-numerical objects and constraints. The resulting\nmethods, which we call Learning Mod- ulo Theories, are formulated within the\nstructured output SVM framework, and employ a weighted SMT solver as an\noptimization oracle to perform efficient in- ference and discriminative max\nmargin weight learning. We also present a few examples of constructive learning\napplications enabled by our method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 14:35:30 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Teso", "Stefano", ""], ["Sebastiani", "Roberto", ""], ["Passerini", "Andrea", ""]]}, {"id": "1402.4371", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "A convergence proof of the split Bregman method for regularized\n  least-squares problems", "comments": "11 pages, 3 figures, submitted to SIAM J. Imaging Sci", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. Imaging\nSci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solves\nimage reconstruction problems with general l1, e.g., total-variation (TV) and\ncompressed sensing (CS), regularizations by introducing a single variable split\nto decouple the data-fitting term and the regularization term, yielding simple\nsubproblems that are separable (or partially separable) and easy to minimize.\nSeveral convergence proofs have been proposed, and these proofs either impose a\n\"full column rank\" assumption to the split or assume exact updates in all\nsubproblems. However, these assumptions are impractical in many applications\nsuch as the X-ray computed tomography (CT) image reconstructions, where the\ninner least-squares problem usually cannot be solved efficiently due to the\nhighly shift-variant Hessian. In this paper, we show that when the data-fitting\nterm is quadratic, the SB method is a convergent alternating direction method\nof multipliers (ADMM), and a straightforward convergence proof with inexact\nupdates is given using [J. Eckstein and D. P. Bertsekas, Mathematical\nProgramming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SB\nmethod is just a special case of an ADMM algorithm, it seems likely that the\nADMM algorithm will be faster than the SB method if the augmented Largangian\n(AL) penalty parameters are selected appropriately. To have a concrete example,\nwe conduct a convergence rate analysis of the ADMM algorithm using two splits\nfor image restoration problems with quadratic data-fitting term and\nregularization term. According to our analysis, we can show that the two-split\nADMM algorithm can be faster than the SB method if the AL penalty parameter of\nthe SB method is suboptimal. Numerical experiments were conducted to verify our\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 15:16:45 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1402.4381", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "Fast X-ray CT image reconstruction using the linearized augmented\n  Lagrangian method with ordered subsets", "comments": "21 pages (including the supplementary material), 12 figures,\n  submitted to IEEE Trans. Med. Imag", "journal-ref": "IEEE Trans. Medical Imaging, 34(2):388-99, Feb. 2015", "doi": "10.1109/TMI.2014.2358499", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The augmented Lagrangian (AL) method that solves convex optimization problems\nwith linear constraints has drawn more attention recently in imaging\napplications due to its decomposable structure for composite cost functions and\nempirical fast convergence rate under weak conditions. However, for problems\nsuch as X-ray computed tomography (CT) image reconstruction and large-scale\nsparse regression with \"big data\", where there is no efficient way to solve the\ninner least-squares problem, the AL method can be slow due to the inevitable\niterative inner updates. In this paper, we focus on solving regularized\n(weighted) least-squares problems using a linearized variant of the AL method\nthat replaces the quadratic AL penalty term in the scaled augmented Lagrangian\nwith its separable quadratic surrogate (SQS) function, thus leading to a much\nsimpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM,\nfor X-ray CT image reconstruction. To further accelerate the proposed\nalgorithm, we use a second-order recursive system analysis to design a\ndeterministic downward continuation approach that avoids tedious parameter\ntuning and provides fast convergence. Experimental results show that the\nproposed algorithm significantly accelerates the \"convergence\" of X-ray CT\nimage reconstruction with negligible overhead and greatly reduces the OS\nartifacts in the reconstructed image when using many subsets for OS\nacceleration.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 16:02:36 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1402.4419", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Incremental Majorization-Minimization Optimization with Application to\n  Large-Scale Machine Learning", "comments": "to appear in SIAM Journal on Optimization; final author's version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorization-minimization algorithms consist of successively minimizing a\nsequence of upper bounds of the objective function. These upper bounds are\ntight at the current estimate, and each iteration monotonically drives the\nobjective function downhill. Such a simple principle is widely applicable and\nhas been very popular in various scientific fields, especially in signal\nprocessing and statistics. In this paper, we propose an incremental\nmajorization-minimization scheme for minimizing a large sum of continuous\nfunctions, a problem of utmost importance in machine learning. We present\nconvergence guarantees for non-convex and convex optimization when the upper\nbounds approximate the objective up to a smooth error; we call such upper\nbounds \"first-order surrogate functions\". More precisely, we study asymptotic\nstationary point guarantees for non-convex problems, and for convex ones, we\nprovide convergence rates for the expected objective function value. We apply\nour scheme to composite optimization and obtain a new incremental proximal\ngradient algorithm with linear convergence rate for strongly convex functions.\nIn our experiments, we show that our method is competitive with the state of\nthe art for solving machine learning problems such as logistic regression when\nthe number of training samples is large enough, and we demonstrate its\nusefulness for sparse estimation with non-convex penalties.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 17:50:30 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 19:39:32 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 07:20:36 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1402.4501", "submitter": "Kacper Chwialkowski", "authors": "Kacper Chwialkowski and Arthur Gretton", "title": "A Kernel Independence Test for Random Processes", "comments": "In Proceedings of The 31st International Conference on Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A new non parametric approach to the problem of testing the independence of\ntwo random process is developed. The test statistic is the Hilbert Schmidt\nIndependence Criterion (HSIC), which was used previously in testing\nindependence for i.i.d pairs of variables. The asymptotic behaviour of HSIC is\nestablished when computed from samples drawn from random processes. It is shown\nthat earlier bootstrap procedures which worked in the i.i.d. case will fail for\nrandom processes, and an alternative consistent estimate of the p-values is\nproposed. Tests on artificial data and real-world Forex data indicate that the\nnew test procedure discovers dependence which is missed by linear approaches,\nwhile the earlier bootstrap procedure returns an elevated number of false\npositives. The code is available online:\nhttps://github.com/kacperChwialkowski/HSIC .\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 21:37:00 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 19:01:00 GMT"}, {"version": "v3", "created": "Tue, 17 Jun 2014 15:53:11 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Chwialkowski", "Kacper", ""], ["Gretton", "Arthur", ""]]}, {"id": "1402.4507", "submitter": "Fang Han", "authors": "Fang Han and Han Liu", "title": "High Dimensional Semiparametric Scale-Invariant Principal Component\n  Analysis", "comments": "Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPMAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new high dimensional semiparametric principal component analysis\n(PCA) method, named Copula Component Analysis (COCA). The semiparametric model\nassumes that, after unspecified marginally monotone transformations, the\ndistributions are multivariate Gaussian. COCA improves upon PCA and sparse PCA\nin three aspects: (i) It is robust to modeling assumptions; (ii) It is robust\nto outliers and data contamination; (iii) It is scale-invariant and yields more\ninterpretable results. We prove that the COCA estimators obtain fast estimation\nrates and are feature selection consistent when the dimension is nearly\nexponentially large relative to the sample size. Careful experiments confirm\nthat COCA outperforms sparse PCA on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 21:53:33 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Han", "Fang", ""], ["Liu", "Han", ""]]}, {"id": "1402.4512", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Robert Nowak, Christopher Cox and Timothy Rogers", "title": "Classification with Sparse Overlapping Groups", "comments": "Tighter result compared to the previous version. Some additional\n  details and justification on the problem being solved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification with a sparsity constraint on the solution plays a central\nrole in many high dimensional machine learning applications. In some cases, the\nfeatures can be grouped together so that entire subsets of features can be\nselected or not selected. In many applications, however, this can be too\nrestrictive. In this paper, we are interested in a less restrictive form of\nstructured sparse feature selection: we assume that while features can be\ngrouped according to some notion of similarity, not all features in a group\nneed be selected for the task at hand. When the groups are comprised of\ndisjoint sets of features, this is sometimes referred to as the \"sparse group\"\nlasso, and it allows for working with a richer class of models than traditional\ngroup lasso methods. Our framework generalizes conventional sparse group lasso\nfurther by allowing for overlapping groups, an additional flexiblity needed in\nmany applications and one that presents further challenges. The main\ncontribution of this paper is a new procedure called Sparse Overlapping Group\n(SOG) lasso, a convex optimization program that automatically selects similar\nfeatures for classification in high dimensions. We establish model selection\nerror bounds for SOGlasso classification problems under a fairly general\nsetting. In particular, the error bounds are the first such results for\nclassification using the sparse group lasso. Furthermore, the general SOGlasso\nbound specializes to results for the lasso and the group lasso, some known and\nsome new. The SOGlasso is motivated by multi-subject fMRI studies in which\nfunctional activity is classified using brain voxels as features, source\nlocalization problems in Magnetoencephalography (MEG), and analyzing gene\nactivation patterns in microarray data analysis. Experiments with real and\nsynthetic data demonstrate the advantages of SOGlasso compared to the lasso and\ngroup lasso.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 22:08:50 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 16:53:19 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Rao", "Nikhil", ""], ["Nowak", "Robert", ""], ["Cox", "Christopher", ""], ["Rogers", "Timothy", ""]]}, {"id": "1402.4539", "submitter": "Xingye Qiao", "authors": "Sungkyu Jung and Xingye Qiao", "title": "A Statistical Approach to Set Classification by Feature Selection with\n  Applications to Classification of Histopathology Images", "comments": "44 pages, 4 figures in the main paper", "journal-ref": null, "doi": "10.1111/biom.12164", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set classification problems arise when classification tasks are based on sets\nof observations as opposed to individual observations. In set classification, a\nclassification rule is trained with $N$ sets of observations, where each set is\nlabeled with class information, and the prediction of a class label is\nperformed also with a set of observations. Data sets for set classification\nappear, for example, in diagnostics of disease based on multiple cell nucleus\nimages from a single tissue. Relevant statistical models for set classification\nare introduced, which motivate a set classification framework based on\ncontext-free feature extraction. By understanding a set of observations as an\nempirical distribution, we employ a data-driven method to choose those features\nwhich contain information on location and major variation. In particular, the\nmethod of principal component analysis is used to extract the features of major\nvariation. Multidimensional scaling is used to represent features as\nvector-valued points on which conventional classifiers can be applied. The\nproposed set classification approaches achieve better classification results\nthan competing methods in a number of simulated data examples. The benefits of\nour method are demonstrated in an analysis of histopathology images of cell\nnuclei related to liver cancer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 01:27:43 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Jung", "Sungkyu", ""], ["Qiao", "Xingye", ""]]}, {"id": "1402.4542", "submitter": "Chunguo Li", "authors": "Chun-Guo Li, Xing Mei, Bao-Gang Hu", "title": "Unsupervised Ranking of Multi-Attribute Objects Based on Principal\n  Curves", "comments": "This paper has 14 pages and 9 figures. The paper has submitted to\n  IEEE Transactions on Knowledge and Data Engineering (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised ranking faces one critical challenge in evaluation applications,\nthat is, no ground truth is available. When PageRank and its variants show a\ngood solution in related subjects, they are applicable only for ranking from\nlink-structure data. In this work, we focus on unsupervised ranking from\nmulti-attribute data which is also common in evaluation tasks. To overcome the\nchallenge, we propose five essential meta-rules for the design and assessment\nof unsupervised ranking approaches: scale and translation invariance, strict\nmonotonicity, linear/nonlinear capacities, smoothness, and explicitness of\nparameter size. These meta-rules are regarded as high level knowledge for\nunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a\nranking principal curve (RPC) model, which learns a one-dimensional manifold\nfunction to perform unsupervised ranking tasks on multi-attribute observations.\nFurthermore, the RPC is modeled to be a cubic B\\'ezier curve with control\npoints restricted in the interior of a hypercube, thereby complying with all\nthe five meta-rules to infer a reasonable ranking list. With control points as\nthe model parameters, one is able to understand the learned manifold and to\ninterpret the ranking list semantically. Numerical experiments of the presented\nRPC model are conducted on two open datasets of different ranking applications.\nIn comparison with the state-of-the-art approaches, the new model is able to\nshow more reasonable ranking lists.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 01:29:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Li", "Chun-Guo", ""], ["Mei", "Xing", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1402.4566", "submitter": "Jaydeep De", "authors": "Jaydeep De and Xiaowei Zhang and Li Cheng", "title": "Transduction on Directed Graphs via Absorbing Random Walks", "comments": "The paper is withdrawn because of some violation in institute policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of graph-based transductive\nclassification, and we are particularly interested in the directed graph\nscenario which is a natural form for many real world applications. Different\nfrom existing research efforts that either only deal with undirected graphs or\ncircumvent directionality by means of symmetrization, we propose a novel random\nwalk approach on directed graphs using absorbing Markov chains, which can be\nregarded as maximizing the accumulated expected number of visits from the\nunlabeled transient states. Our algorithm is simple, easy to implement, and\nworks with large-scale graphs. In particular, it is capable of preserving the\ngraph structure even when the input graph is sparse and changes over time, as\nwell as retaining weak signals presented in the directed edges. We present its\nintimate connections to a number of existing methods, including graph kernels,\ngraph Laplacian based methods, and interestingly, spanning forest of graphs.\nIts computational complexity and the generalization error are also studied.\nEmpirically our algorithm is systematically evaluated on a wide range of\napplications, where it has shown to perform competitively comparing to a suite\nof state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 06:41:12 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 02:05:34 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["De", "Jaydeep", ""], ["Zhang", "Xiaowei", ""], ["Cheng", "Li", ""]]}, {"id": "1402.4624", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Anju Kambadur and Aurelie C. Lozano and Ronny\n  Luss", "title": "Sparse Quantile Huber Regression for Efficient and Robust Estimation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider new formulations and methods for sparse quantile regression in\nthe high-dimensional setting. Quantile regression plays an important role in\nmany applications, including outlier-robust exploratory analysis in gene\nselection. In addition, the sparsity consideration in quantile regression\nenables the exploration of the entire conditional distribution of the response\nvariable given the predictors and therefore yields a more comprehensive view of\nthe important predictors. We propose a generalized OMP algorithm for variable\nselection, taking the misfit loss to be either the traditional quantile loss or\na smooth version we call quantile Huber, and compare the resulting greedy\napproaches with convex sparsity-regularized formulations. We apply a recently\nproposed interior point methodology to efficiently solve all convex\nformulations as well as convex subproblems in the generalized OMP setting, pro-\nvide theoretical guarantees of consistent estimation, and demonstrate the\nperformance of our approach using empirical studies of simulated and genomic\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 11:18:32 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Kambadur", "Anju", ""], ["Lozano", "Aurelie C.", ""], ["Luss", "Ronny", ""]]}, {"id": "1402.4653", "submitter": "Sohan Seth", "authors": "Sohan Seth, John Shawe-Taylor, Samuel Kaski", "title": "Retrieval of Experiments by Efficient Estimation of Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of retrieving relevant experiments given a query\nexperiment. By experiment, we mean a collection of measurements from a set of\n`covariates' and the associated `outcomes'. While similar experiments can be\nretrieved by comparing available `annotations', this approach ignores the\nvaluable information available in the measurements themselves. To incorporate\nthis information in the retrieval task, we suggest employing a retrieval metric\nthat utilizes probabilistic models learned from the measurements. We argue that\nsuch a metric is a sensible measure of similarity between two experiments since\nit permits inclusion of experiment-specific prior knowledge. However, accurate\nmodels are often not analytical, and one must resort to storing posterior\nsamples which demands considerable resources. Therefore, we study strategies to\nselect informative posterior samples to reduce the computational load while\nmaintaining the retrieval performance. We demonstrate the efficacy of our\napproach on simulated data with simple linear regression as the models, and\nreal world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 13:21:40 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Seth", "Sohan", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1402.4732", "submitter": "Thomas Lasko", "authors": "Thomas A. Lasko", "title": "Efficient Inference of Gaussian Process Modulated Renewal Processes with\n  Application to Medical Event Data", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "VU-DBMI-2014-01-001", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The episodic, irregular and asynchronous nature of medical data render them\ndifficult substrates for standard machine learning algorithms. We would like to\nabstract away this difficulty for the class of time-stamped categorical\nvariables (or events) by modeling them as a renewal process and inferring a\nprobability density over continuous, longitudinal, nonparametric intensity\nfunctions modulating that process. Several methods exist for inferring such a\ndensity over intensity functions, but either their constraints and assumptions\nprevent their use with our potentially bursty event streams, or their time\ncomplexity renders their use intractable on our long-duration observations of\nhigh-resolution events, or both. In this paper we present a new and efficient\nmethod for inferring a distribution over intensity functions that uses direct\nnumeric integration and smooth interpolation over Gaussian processes. We\ndemonstrate that our direct method is up to twice as accurate and two orders of\nmagnitude more efficient than the best existing method (thinning). Importantly,\nthe direct method can infer intensity functions over the full range of bursty\nto memoryless to regular events, which thinning and many other methods cannot.\nFinally, we apply the method to clinical event data and demonstrate the\nface-validity of the abstraction, which is now amenable to standard learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:09:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Lasko", "Thomas A.", ""]]}, {"id": "1402.4746", "submitter": "Ananda Theertha Suresh", "authors": "Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha\n  Suresh", "title": "Near-optimal-sample estimators for spherical Gaussian mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical and machine-learning algorithms are frequently applied to\nhigh-dimensional data. In many of these applications data is scarce, and often\nmuch more costly than computation time. We provide the first sample-efficient\npolynomial-time estimator for high-dimensional spherical Gaussian mixtures.\n  For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive an\nintuitive spectral-estimator that uses\n$\\mathcal{O}_k\\bigl(\\frac{d\\log^2d}{\\epsilon^4}\\bigr)$ samples and runs in time\n$\\mathcal{O}_{k,\\epsilon}(d^3\\log^5 d)$, both significantly lower than\npreviously known. The constant factor $\\mathcal{O}_k$ is polynomial for sample\ncomplexity and is exponential for the time complexity, again much smaller than\nwhat was previously known. We also show that\n$\\Omega_k\\bigl(\\frac{d}{\\epsilon^2}\\bigr)$ samples are needed for any\nalgorithm. Hence the sample complexity is near-optimal in the number of\ndimensions.\n  We also derive a simple estimator for one-dimensional mixtures that uses\n$\\mathcal{O}\\bigl(\\frac{k \\log \\frac{k}{\\epsilon} }{\\epsilon^2} \\bigr)$ samples\nand runs in time\n$\\widetilde{\\mathcal{O}}\\left(\\bigl(\\frac{k}{\\epsilon}\\bigr)^{3k+1}\\right)$.\nOur other technical contributions include a faster algorithm for choosing a\ndensity estimate from a set of distributions, that minimizes the $\\ell_1$\ndistance to an unknown underlying distribution.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:59:55 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Acharya", "Jayadev", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1402.4844", "submitter": "Alon Gonen", "authors": "Alon Gonen, Dan Rosenbaum, Yonina Eldar, Shai Shalev-Shwartz", "title": "Subspace Learning with Partial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of subspace learning is to find a $k$-dimensional subspace of\n$\\mathbb{R}^d$, such that the expected squared distance between instance\nvectors and the subspace is as small as possible. In this paper we study\nsubspace learning in a partial information setting, in which the learner can\nonly observe $r \\le d$ attributes from each instance vector. We propose several\nefficient algorithms for this task, and analyze their sample complexity\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 22:57:03 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 14:06:50 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Gonen", "Alon", ""], ["Rosenbaum", "Dan", ""], ["Eldar", "Yonina", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1402.4862", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams and Ben Taskar", "title": "Learning the Parameters of Determinantal Point Process Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are well-suited for modeling repulsion\nand have proven useful in many applications where diversity is desired. While\nDPPs have many appealing properties, such as efficient sampling, learning the\nparameters of a DPP is still considered a difficult problem due to the\nnon-convex nature of the likelihood function. In this paper, we propose using\nBayesian methods to learn the DPP kernel parameters. These methods are\napplicable in large-scale and continuous DPP settings even when the exact form\nof the eigendecomposition is unknown. We demonstrate the utility of our DPP\nlearning methods in studying the progression of diabetic neuropathy based on\nspatial distribution of nerve fibers, and in studying human perception of\ndiversity in images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 01:54:37 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Fox", "Emily B.", ""], ["Adams", "Ryan P.", ""], ["Taskar", "Ben", ""]]}, {"id": "1402.4884", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen and Robert C. Williamson", "title": "Le Cam meets LeCun: Deficiency and Generic Feature Learning", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Deep Learning\" methods attempt to learn generic features in an unsupervised\nfashion from a large unlabelled data set. These generic features should perform\nas well as the best hand crafted features for any learning problem that makes\nuse of this data. We provide a definition of generic features, characterize\nwhen it is possible to learn them and provide methods closely related to the\nautoencoder and deep belief network of deep learning. In order to do so we use\nthe notion of deficiency and illustrate its value in studying certain general\nlearning problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:06:28 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 22:37:10 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1402.4888", "submitter": "Johnvictor D", "authors": "D. Johnvictor, G. Selvavinayagam", "title": "Survey on Sparse Coded Features for Content Based Face Image Retrieval", "comments": "4 pages,3 figures,1 table, Published with International Journal of\n  Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  8(1):30-33, February 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V8P106", "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based image retrieval, a technique which uses visual contents of\nimage to search images from large scale image databases according to users'\ninterests. This paper provides a comprehensive survey on recent technology used\nin the area of content based face image retrieval. Nowadays digital devices and\nphoto sharing sites are getting more popularity, large human face photos are\navailable in database. Multiple types of facial features are used to represent\ndiscriminality on large scale human facial image database. Searching and mining\nof facial images are challenging problems and important research issues. Sparse\nrepresentation on features provides significant improvement in indexing related\nimages to query image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:32:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Johnvictor", "D.", ""], ["Selvavinayagam", "G.", ""]]}, {"id": "1402.5077", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Group-sparse Matrix Recovery", "comments": "ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the OSCAR (octagonal selection and clustering algorithms for\nregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)\nfrom compressive measurements. We propose a 2D version of OSCAR (2OSCAR)\nconsisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which\nis convex but non-differentiable. We show that the proximity operator of 2OSCAR\ncan be computed based on that of OSCAR. The 2OSCAR problem can thus be\nefficiently solved by state-of-the-art proximal splitting algorithms.\nExperiments on group-sparse 2D array recovery show that 2OSCAR regularization\nsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm\n(with debiasing) yields the most accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 17:08:34 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1402.5131", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar and Edmond Jonckheere", "title": "Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse\n  Optimization and Noisy Matrix Decomposition", "comments": "appeared in Neural Information Processing Systems(NIPS) 2014. arXiv\n  admin note: text overlap with arXiv:1207.4421 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient ADMM method with guarantees for high-dimensional\nproblems. We provide explicit bounds for the sparse optimization problem and\nthe noisy matrix decomposition problem. For sparse optimization, we establish\nthat the modified ADMM method has an optimal convergence rate of\n$\\mathcal{O}(s\\log d/T)$, where $s$ is the sparsity level, $d$ is the data\ndimension and $T$ is the number of steps. This matches with the minimax lower\nbounds for sparse estimation. For matrix decomposition into sparse and low rank\ncomponents, we provide the first guarantees for any online method, and prove a\nconvergence rate of $\\tilde{\\mathcal{O}}((s+r)\\beta^2(p) /T) +\n\\mathcal{O}(1/p)$ for a $p\\times p$ matrix, where $s$ is the sparsity level,\n$r$ is the rank and $\\Theta(\\sqrt{p})\\leq \\beta(p)\\leq \\Theta(p)$. Our\nguarantees match the minimax lower bound with respect to $s,r$ and $T$. In\naddition, we match the minimax lower bound with respect to the matrix dimension\n$p$, i.e. $\\beta(p)=\\Theta(\\sqrt{p})$, for many important statistical models\nincluding the independent noise model, the linear Bayesian network and the\nlatent Gaussian graphical model under some conditions. Our ADMM method is based\non epoch-based annealing and consists of inexpensive steps which involve\nprojections on to simple norm balls. Experiments show that for both sparse\noptimization and matrix decomposition problems, our algorithm outperforms the\nstate-of-the-art methods. In particular, we reach higher accuracy with same\ntime complexity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 20:48:10 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2014 00:15:42 GMT"}, {"version": "v3", "created": "Wed, 19 Mar 2014 09:42:26 GMT"}, {"version": "v4", "created": "Mon, 16 Jun 2014 03:53:05 GMT"}, {"version": "v5", "created": "Sun, 7 Dec 2014 03:36:03 GMT"}, {"version": "v6", "created": "Tue, 7 Jul 2015 00:13:55 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""], ["Jonckheere", "Edmond", ""]]}, {"id": "1402.5176", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III", "title": "Pareto-depth for Multiple-query Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2378057", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most content-based image retrieval systems consider either one single query,\nor multiple queries that include the same object or represent the same semantic\ninformation. In this paper we consider the content-based image retrieval\nproblem for multiple query images corresponding to different image semantics.\nWe propose a novel multiple-query information retrieval algorithm that combines\nthe Pareto front method (PFM) with efficient manifold ranking (EMR). We show\nthat our proposed algorithm outperforms state of the art multiple-query\nretrieval algorithms on real-world image databases. We attribute this\nperformance improvement to concavity properties of the Pareto fronts, and prove\na theoretical result that characterizes the asymptotic concavity of the fronts.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 00:42:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1402.5180", "submitter": "Majid Janzamin", "authors": "Animashree Anandkumar and Rong Ge and Majid Janzamin", "title": "Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$\n  Updates", "comments": "We have added an additional sub-algorithm to remove the (approximate)\n  residual error left after the tensor power iteration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide local and global convergence guarantees for\nrecovering CP (Candecomp/Parafac) tensor decomposition. The main step of the\nproposed algorithm is a simple alternating rank-$1$ update which is the\nalternating version of the tensor power iteration adapted for asymmetric\ntensors. Local convergence guarantees are established for third order tensors\nof rank $k$ in $d$ dimensions, when $k=o \\bigl( d^{1.5} \\bigr)$ and the tensor\ncomponents are incoherent. Thus, we can recover overcomplete tensor\ndecomposition. We also strengthen the results to global convergence guarantees\nunder stricter rank condition $k \\le \\beta d$ (for arbitrary constant $\\beta >\n1$) through a simple initialization procedure where the algorithm is\ninitialized by top singular vectors of random tensor slices. Furthermore, the\napproximate local convergence guarantees for $p$-th order tensors are also\nprovided under rank condition $k=o \\bigl( d^{p/2} \\bigr)$. The guarantees also\ninclude tight perturbation analysis given noisy tensor.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 01:37:02 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 16:29:13 GMT"}, {"version": "v3", "created": "Sun, 3 Aug 2014 22:00:58 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 20:40:42 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1402.5360", "submitter": "Chanabasayya Vastrad M", "authors": "Doreswamy, Chanabasayya M. Vastrad", "title": "Important Molecular Descriptors Selection Using Self Tuned Reweighted\n  Sampling Method for Prediction of Antituberculosis Activity", "comments": "published 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, a new descriptor selection method for selecting an optimal\ncombination of important descriptors of sulfonamide derivatives data, named\nself tuned reweighted sampling (STRS), is developed. descriptors are defined as\nthe descriptors with large absolute coefficients in a multivariate linear\nregression model such as partial least squares(PLS). In this study, the\nabsolute values of regression coefficients of PLS model are used as an index\nfor evaluating the importance of each descriptor Then, based on the importance\nlevel of each descriptor, STRS sequentially selects N subsets of descriptors\nfrom N Monte Carlo (MC) sampling runs in an iterative and competitive manner.\nIn each sampling run, a fixed ratio (e.g. 80%) of samples is first randomly\nselected to establish a regresson model. Next, based on the regression\ncoefficients, a two-step procedure including rapidly decreasing function (RDF)\nbased enforced descriptor selection and self tuned sampling (STS) based\ncompetitive descriptor selection is adopted to select the important\ndescriptorss. After running the loops, a number of subsets of descriptors are\nobtained and root mean squared error of cross validation (RMSECV) of PLS models\nestablished with subsets of descriptors is computed. The subset of descriptors\nwith the lowest RMSECV is considered as the optimal descriptor subset. The\nperformance of the proposed algorithm is evaluated by sulfanomide derivative\ndataset. The results reveal an good characteristic of STRS that it can usually\nlocate an optimal combination of some important descriptors which are\ninterpretable to the biologically of interest. Additionally, our study shows\nthat better prediction is obtained by STRS when compared to full descriptor set\nPLS modeling, Monte Carlo uninformative variable elimination (MC-UVE).\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 17:24:53 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Doreswamy", "", ""], ["Vastrad", "Chanabasayya M.", ""]]}, {"id": "1402.5458", "submitter": "Sindhu Kutty", "authors": "Jacob Abernethy, Sindhu Kutty, S\\'ebastien Lahaie, Rahul Sami", "title": "Information Aggregation in Exponential Family Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of prediction market mechanisms known as automated\nmarket makers. We show that we can design these mechanisms via the mold of\n\\emph{exponential family distributions}, a popular and well-studied probability\ndistribution template used in statistics. We give a full development of this\nrelationship and explore a range of benefits. We draw connections between the\ninformation aggregation of market prices and the belief aggregation of learning\nagents that rely on exponential family distributions. We develop a very natural\nanalysis of the market behavior as well as the price equilibrium under the\nassumption that the traders exhibit risk aversion according to exponential\nutility. We also consider similar aspects under alternative models, such as\nwhen traders are budget constrained.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 01:11:05 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Abernethy", "Jacob", ""], ["Kutty", "Sindhu", ""], ["Lahaie", "S\u00e9bastien", ""], ["Sami", "Rahul", ""]]}, {"id": "1402.5473", "submitter": "Fritz Obermeyer", "authors": "Fritz Obermeyer and Jonathan Glidden and Eric Jonas", "title": "Scaling Nonparametric Bayesian Inference via Subsample-Annealing", "comments": "To appear in AISTATS 2014", "journal-ref": "PMLR 33:696-705 2014", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an adaptation of the simulated annealing algorithm to\nnonparametric clustering and related probabilistic models. This new algorithm\nlearns nonparametric latent structure over a growing and constantly churning\nsubsample of training data, where the portion of data subsampled can be\ninterpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs\nsampling at high temperature (i.e., with a very small subsample) can more\nquickly explore sketches of the final latent state by (a) making longer jumps\naround latent space (as in block Gibbs) and (b) lowering energy barriers (as in\nsimulated annealing). We prove subsample annealing speeds up mixing time N^2 ->\nN in a simple clustering model and exp(N) -> N in another class of models,\nwhere N is data size. Empirically subsample-annealing outperforms naive Gibbs\nsampling in accuracy-per-wallclock time, and can scale to larger datasets and\ndeeper hierarchical models. We demonstrate improved inference on million-row\nsubsamples of US Census data and network log data and a 307-row hospital rating\ndataset, using a Pitman-Yor generalization of the Cross Categorization model.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 03:44:04 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Obermeyer", "Fritz", ""], ["Glidden", "Jonathan", ""], ["Jonas", "Eric", ""]]}, {"id": "1402.5481", "submitter": "Nathan Kallus", "authors": "Dimitris Bertsimas, Nathan Kallus", "title": "From Predictive to Prescriptive Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we combine ideas from machine learning (ML) and operations\nresearch and management science (OR/MS) in developing a framework, along with\nspecific methods, for using data to prescribe optimal decisions in OR/MS\nproblems. In a departure from other work on data-driven optimization and\nreflecting our practical experience with the data available in applications of\nOR/MS, we consider data consisting, not only of observations of quantities with\ndirect effect on costs/revenues, such as demand or returns, but predominantly\nof observations of associated auxiliary quantities. The main problem of\ninterest is a conditional stochastic optimization problem, given imperfect\nobservations, where the joint probability distributions that specify the\nproblem are unknown. We demonstrate that our proposed solution methods, which\nare inspired by ML methods such as local regression, CART, and random forests,\nare generally applicable to a wide range of decision problems. We prove that\nthey are tractable and asymptotically optimal even when data is not iid and may\nbe censored. We extend this to the case where decision variables may directly\naffect uncertainty in unknown ways, such as pricing's effect on demand. As an\nanalogue to R^2, we develop a metric P termed the coefficient of\nprescriptiveness to measure the prescriptive content of data and the efficacy\nof a policy from an operations perspective. To demonstrate the power of our\napproach in a real-world setting we study an inventory management problem faced\nby the distribution arm of an international media conglomerate, which ships an\naverage of 1bil units per year. We leverage internal data and public online\ndata harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational\ndecisions that outperform baseline measures. Specifically, the data we collect,\nleveraged by our methods, accounts for an 88\\% improvement as measured by our\nP.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 05:10:56 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 17:25:20 GMT"}, {"version": "v3", "created": "Mon, 9 Feb 2015 20:07:10 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 15:36:29 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Kallus", "Nathan", ""]]}, {"id": "1402.5565", "submitter": "David M. Johnson", "authors": "David M. Johnson, Caiming Xiong and Jason J. Corso", "title": "Semi-Supervised Nonlinear Distance Metric Learning via Forests of\n  Max-Margin Cluster Hierarchies", "comments": "Manuscript submitted to SIGKDD on 21 Feb 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning is a key problem for many data mining and machine learning\napplications, and has long been dominated by Mahalanobis methods. Recent\nadvances in nonlinear metric learning have demonstrated the potential power of\nnon-Mahalanobis distance functions, particularly tree-based functions. We\npropose a novel nonlinear metric learning method that uses an iterative,\nhierarchical variant of semi-supervised max-margin clustering to construct a\nforest of cluster hierarchies, where each individual hierarchy can be\ninterpreted as a weak metric over the data. By introducing randomness during\nhierarchy training and combining the output of many of the resulting\nsemi-random weak hierarchy metrics, we can obtain a powerful and robust\nnonlinear metric model. This method has two primary contributions: first, it is\nsemi-supervised, incorporating information from both constrained and\nunconstrained points. Second, we take a relaxed approach to constraint\nsatisfaction, allowing the method to satisfy different subsets of the\nconstraints at different levels of the hierarchy rather than attempting to\nsimultaneously satisfy all of them. This leads to a more robust learning\nalgorithm. We compare our method to a number of state-of-the-art benchmarks on\n$k$-nearest neighbor classification, large-scale image retrieval and\nsemi-supervised clustering problems, and find that our algorithm yields results\ncomparable or superior to the state-of-the-art, and is significantly more\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 00:26:48 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Johnson", "David M.", ""], ["Xiong", "Caiming", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.5584", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats, Richard G. Baraniuk", "title": "Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse\n  Regression", "comments": "AISTATS 2014", "journal-ref": "Proceedings of the 17th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&CP\n  volume 33", "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging problem of selecting tuning\nparameters for high-dimensional sparse regression. We propose a simple and\ncomputationally efficient method, called path thresholding (PaTh), that\ntransforms any tuning parameter-dependent sparse regression algorithm into an\nasymptotically tuning-free sparse regression algorithm. More specifically, we\nprove that, as the problem size becomes large (in the number of variables and\nin the number of observations), PaTh performs accurate sparse regression, under\nappropriate conditions, without specifying a tuning parameter. In\nfinite-dimensional settings, we demonstrate that PaTh can alleviate the\ncomputational burden of model selection algorithms by significantly reducing\nthe search space of tuning parameters.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 07:23:39 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Vats", "Divyanshu", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1402.5596", "submitter": "Jason Lee", "authors": "Jason D Lee and Jonathan E Taylor", "title": "Exact Post Model Selection Inference for Marginal Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for post model selection inference, via marginal\nscreening, in linear regression. At the core of this framework is a result that\ncharacterizes the exact distribution of linear functions of the response $y$,\nconditional on the model being selected (``condition on selection\" framework).\nThis allows us to construct valid confidence intervals and hypothesis tests for\nregression coefficients that account for the selection procedure. In contrast\nto recent work in high-dimensional statistics, our results are exact\n(non-asymptotic) and require no eigenvalue-like assumptions on the design\nmatrix $X$. Furthermore, the computational cost of marginal regression,\nconstructing confidence intervals and hypothesis testing is negligible compared\nto the cost of linear regression, thus making our methods particularly suitable\nfor extremely large datasets. Although we focus on marginal screening to\nillustrate the applicability of the condition on selection framework, this\nframework is much more broadly applicable. We show how to apply the proposed\nframework to several other selection procedures including orthogonal matching\npursuit, non-negative least squares, and marginal screening+Lasso.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 10:30:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 00:28:21 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Lee", "Jason D", ""], ["Taylor", "Jonathan E", ""]]}, {"id": "1402.5715", "submitter": "Tejas Kulkarni", "authors": "Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman", "title": "Variational Particle Approximations", "comments": "First two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate inference in high-dimensional, discrete probabilistic models is a\ncentral problem in computational statistics and machine learning. This paper\ndescribes discrete particle variational inference (DPVI), a new approach that\ncombines key strengths of Monte Carlo, variational and search-based techniques.\nDPVI is based on a novel family of particle-based variational approximations\nthat can be fit using simple, fast, deterministic search techniques. Like Monte\nCarlo, DPVI can handle multiple modes, and yields exact results in a\nwell-defined limit. Like unstructured mean-field, DPVI is based on optimizing a\nlower bound on the partition function; when this quantity is not of intrinsic\ninterest, it facilitates convergence assessment and debugging. Like both Monte\nCarlo and combinatorial search, DPVI can take advantage of factorization,\nsequential structure, and custom search operators. This paper defines DPVI\nparticle-based approximation family and partition function lower bounds, along\nwith the sequential DPVI and local DPVI algorithm templates for optimizing\nthem. DPVI is illustrated and evaluated via experiments on lattice Markov\nRandom Fields, nonparametric Bayesian mixtures and block-models, and parametric\nas well as non-parametric hidden Markov models. Results include applications to\nreal-world spike-sorting and relational modeling problems, and show that DPVI\ncan offer appealing time/accuracy trade-offs as compared to multiple\nalternatives.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 03:58:16 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 00:07:30 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 04:40:24 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Kulkarni", "Tejas D", ""], ["Mansinghka", "Vikash", ""], ["Gershman", "Samuel", ""]]}, {"id": "1402.5728", "submitter": "Mathukumalli Vidyasagar", "authors": "Mathukumalli Vidyasagar", "title": "Machine Learning Methods in the Computational Biology of Cancer", "comments": "35 pages, three figures", "journal-ref": null, "doi": "10.1098/rspa.2014.0081", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objectives of this \"perspective\" paper are to review some recent advances\nin sparse feature selection for regression and classification, as well as\ncompressed sensing, and to discuss how these might be used to develop tools to\nadvance personalized cancer therapy. As an illustration of the possibilities, a\nnew algorithm for sparse regression is presented, and is applied to predict the\ntime to tumor recurrence in ovarian cancer. A new algorithm for sparse feature\nselection in classification problems is presented, and its validation in\nendometrial cancer is briefly discussed. Some open problems are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 06:07:56 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1402.5836", "submitter": "David Duvenaud", "authors": "David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani", "title": "Avoiding pathologies in very deep networks", "comments": "Fixed a typo regarding number of layers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Choosing appropriate architectures and regularization strategies for deep\nnetworks is crucial to good predictive performance. To shed light on this\nproblem, we analyze the analogous problem of constructing useful priors on\ncompositions of functions. Specifically, we study the deep Gaussian process, a\ntype of infinitely-wide, deep neural network. We show that in standard\narchitectures, the representational capacity of the network tends to capture\nfewer degrees of freedom as the number of layers increases, retaining only a\nsingle degree of freedom in the limit. We propose an alternate network\narchitecture which does not suffer from this pathology. We also examine deep\ncovariance functions, obtained by composing infinitely many feature transforms.\nLastly, we characterize the class of models obtained by performing dropout on\nGaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 14:27:40 GMT"}, {"version": "v2", "created": "Sun, 14 Sep 2014 21:50:47 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 22:59:45 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Duvenaud", "David", ""], ["Rippel", "Oren", ""], ["Adams", "Ryan P.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.5874", "submitter": "Mohammad Ghasemi Hamed", "authors": "Mohammad Ghasemi Hamed, Mathieu Serrurier, Nicolas Durand", "title": "Predictive Interval Models for Non-parametric Regression", "comments": "This paper has been withdrawn by the authors due to multiple errors\n  in the formulations and equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a regression model, we are interested in finding two-sided intervals\nthat are guaranteed to contain at least a desired proportion of the conditional\ndistribution of the response variable given a specific combination of\npredictors. We name such intervals predictive intervals. This work presents a\nnew method to find two-sided predictive intervals for non-parametric least\nsquares regression without the homoscedasticity assumption. Our predictive\nintervals are built by using tolerance intervals on prediction errors in the\nquery point's neighborhood. We proposed a predictive interval model test and we\nalso used it as a constraint in our hyper-parameter tuning algorithm. This\ngives an algorithm that finds the smallest reliable predictive intervals for a\ngiven dataset. We also introduce a measure for comparing different interval\nprediction methods yielding intervals having different size and coverage. These\nexperiments show that our methods are more reliable, effective and precise than\nother interval prediction methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:16:17 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 10:56:40 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Hamed", "Mohammad Ghasemi", ""], ["Serrurier", "Mathieu", ""], ["Durand", "Nicolas", ""]]}, {"id": "1402.5876", "submitter": "Roberto Calandra", "authors": "Roberto Calandra and Jan Peters and Carl Edward Rasmussen and Marc\n  Peter Deisenroth", "title": "Manifold Gaussian Processes for Regression", "comments": "8 pages, accepted to IJCNN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness\nassumptions on the structure of the function to be modeled. To model complex\nand non-differentiable functions, these smoothness assumptions are often too\nrestrictive. One way to alleviate this limitation is to find a different\nrepresentation of the data by introducing a feature space. This feature space\nis often learned in an unsupervised way, which might lead to data\nrepresentations that are not useful for the overall regression task. In this\npaper, we propose Manifold Gaussian Processes, a novel supervised method that\njointly learns a transformation of the data into a feature space and a GP\nregression from the feature space to observed space. The Manifold GP is a full\nGP and allows to learn data representations, which are useful for the overall\nregression task. As a proof-of-concept, we evaluate our approach on complex\nnon-smooth functions where standard GPs perform poorly, such as step functions\nand robotics tasks with contacts.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 16:19:51 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 14:59:49 GMT"}, {"version": "v3", "created": "Thu, 8 May 2014 10:02:40 GMT"}, {"version": "v4", "created": "Mon, 11 Apr 2016 11:07:31 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Calandra", "Roberto", ""], ["Peters", "Jan", ""], ["Rasmussen", "Carl Edward", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1402.5902", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, Shih-Fu\n  Chang", "title": "On Learning from Label Proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from Label Proportions (LLP) is a learning setting, where the\ntraining data is provided in groups, or \"bags\", and only the proportion of each\nclass in each bag is known. The task is to learn a model to predict the class\nlabels of the individual instances. LLP has broad applications in political\nscience, marketing, healthcare, and computer vision. This work answers the\nfundamental question, when and why LLP is possible, by introducing a general\nframework, Empirical Proportion Risk Minimization (EPRM). EPRM learns an\ninstance label classifier to match the given label proportions on the training\ndata. Our result is based on a two-step analysis. First, we provide a VC bound\non the generalization error of the bag proportions. We show that the bag sample\ncomplexity is only mildly sensitive to the bag size. Second, we show that under\nsome mild assumptions, good bag proportion prediction guarantees good instance\nlabel prediction. The results together provide a formal guarantee that the\nindividual labels can indeed be learned in the LLP setting. We discuss\napplications of the analysis, including justification of LLP algorithms,\nlearning with population proportions, and a paradigm for learning algorithms\nwith privacy guarantees. We also demonstrate the feasibility of LLP based on a\ncase study in real-world setting: predicting income based on census data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 17:40:09 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 23:38:42 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yu", "Felix X.", ""], ["Choromanski", "Krzysztof", ""], ["Kumar", "Sanjiv", ""], ["Jebara", "Tony", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1402.6076", "submitter": "Sergei Izrailev", "authors": "Sergei Izrailev and Jeremy M. Stanley", "title": "Machine Learning at Scale", "comments": "Submitted to KDD'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It takes skill to build a meaningful predictive model even with the abundance\nof implementations of modern machine learning algorithms and readily available\ncomputing resources. Building a model becomes challenging if hundreds of\nterabytes of data need to be processed to produce the training data set. In a\ndigital advertising technology setting, we are faced with the need to build\nthousands of such models that predict user behavior and power advertising\ncampaigns in a 24/7 chaotic real-time production environment. As data\nscientists, we also have to convince other internal departments critical to\nimplementation success, our management, and our customers that our machine\nlearning system works. In this paper, we present the details of the design and\nimplementation of an automated, robust machine learning platform that impacts\nbillions of advertising impressions monthly. This platform enables us to\ncontinuously optimize thousands of campaigns over hundreds of millions of\nusers, on multiple continents, against varying performance objectives.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 07:50:50 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Izrailev", "Sergei", ""], ["Stanley", "Jeremy M.", ""]]}, {"id": "1402.6133", "submitter": "Siddhant Sahu", "authors": "Siddhant Sahu, V. Sugumaran", "title": "Bayesian Sample Size Determination of Vibration Signals in Machine\n  Learning Approach to Fault Diagnosis of Roller Bearings", "comments": "14 pages, 1 table, 6 figures", "journal-ref": "Intentional Journal of Research in Mechanical Engineering, Volume\n  1, Issue 1, July-September, 2013, pp. 55-63, IASTER", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample size determination for a data set is an important statistical process\nfor analyzing the data to an optimum level of accuracy and using minimum\ncomputational work. The applications of this process are credible in every\ndomain which deals with large data sets and high computational work. This study\nuses Bayesian analysis for determination of minimum sample size of vibration\nsignals to be considered for fault diagnosis of a bearing using pre-defined\nparameters such as the inverse standard probability and the acceptable margin\nof error. Thus an analytical formula for sample size determination is\nintroduced. The fault diagnosis of the bearing is done using a machine learning\napproach using an entropy-based J48 algorithm. The following method will help\nresearchers involved in fault diagnosis to determine minimum sample size of\ndata for analysis for a good statistical stability and precision.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 11:11:28 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Sahu", "Siddhant", ""], ["Sugumaran", "V.", ""]]}, {"id": "1402.6262", "submitter": "Bahman Yari Saeed Khanloo", "authors": "Bahman Yari Saeed Khanloo", "title": "Novel Deviation Bounds for Mixture of Independent Bernoulli Variables\n  with Application to the Missing Mass", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with obtaining distribution-free\nconcentration inequalities for mixture of independent Bernoulli variables that\nincorporate a notion of variance. Missing mass is the total probability mass\nassociated to the outcomes that have not been seen in a given sample which is\nan important quantity that connects density estimates obtained from a sample to\nthe population for discrete distributions. Therefore, we are specifically\nmotivated to apply our method to study the concentration of missing mass -\nwhich can be expressed as a mixture of Bernoulli - in a novel way.\n  We not only derive - for the first time - Bernstein-like large deviation\nbounds for the missing mass whose exponents behave almost linearly with respect\nto deviation size, but also sharpen McAllester and Ortiz (2003) and Berend and\nKontorovich (2013) for large sample sizes in the case of small deviations which\nis the most interesting case in learning theory. In the meantime, our approach\nshows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is\nresolvable in the case of missing mass in the sense that one can use standard\ninequalities but it may not lead to strong results. Thus, we postulate that our\nresults are general and can be applied to provide potentially sharp\nBernstein-like bounds under some constraints.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 18:05:05 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 18:00:28 GMT"}, {"version": "v3", "created": "Fri, 7 Mar 2014 11:45:25 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 09:21:21 GMT"}, {"version": "v5", "created": "Wed, 4 Mar 2015 08:46:22 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Khanloo", "Bahman Yari Saeed", ""]]}, {"id": "1402.6455", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko\n  Shiroishi", "title": "Sparse principal component regression with adaptive loading", "comments": "24 pages", "journal-ref": "Computational Statistics & Data Analysis 89 (2015) 192-203", "doi": "10.1016/j.csda.2015.03.016", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a two-stage procedure that selects\nsome principal components and then constructs a regression model regarding them\nas new explanatory variables. Note that the principal components are obtained\nfrom only explanatory variables and not considered with the response variable.\nTo address this problem, we propose the sparse principal component regression\n(SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptively\nobtain sparse principal component loadings that are related to the response\nvariable and select the number of principal components simultaneously. SPCR can\nbe obtained by the convex optimization problem for each of parameters with the\ncoordinate descent algorithm. Monte Carlo simulations and real data analyses\nare performed to illustrate the effectiveness of SPCR.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 08:50:16 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 03:10:22 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 07:39:43 GMT"}, {"version": "v4", "created": "Sat, 1 Nov 2014 03:02:23 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kawano", "Shuichi", ""], ["Fujisawa", "Hironori", ""], ["Takada", "Toyoyuki", ""], ["Shiroishi", "Toshihiko", ""]]}, {"id": "1402.6636", "submitter": "Iain Rice Mr", "authors": "Iain Rice, Roger Benton, Les Hart and David Lowe", "title": "Analysis of Multibeam SONAR Data using Dissimilarity Representations", "comments": "Presented at IMA Mathematics in Defence 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of low-dimensional visualisation of very\nhigh dimensional information sources for the purpose of situation awareness in\nthe maritime environment. In response to the requirement for human decision\nsupport aids to reduce information overload (and specifically, data amenable to\ninter-point relative similarity measures) appropriate to the below-water\nmaritime domain, we are investigating a preliminary prototype topographic\nvisualisation model. The focus of the current paper is on the mathematical\nproblem of exploiting a relative dissimilarity representation of signals in a\nvisual informatics mapping model, driven by real-world sonar systems. An\nindependent source model is used to analyse the sonar beams from which a simple\nprobabilistic input model to represent uncertainty is mapped to a latent\nvisualisation space where data uncertainty can be accommodated. The use of\neuclidean and non-euclidean measures are used and the motivation for future use\nof non-euclidean measures is made. Concepts are illustrated using a simulated\n64 beam weak SNR dataset with realistic sonar targets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 10:21:34 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Rice", "Iain", ""], ["Benton", "Roger", ""], ["Hart", "Les", ""], ["Lowe", "David", ""]]}, {"id": "1402.6744", "submitter": "Paul McNicholas", "authors": "Katherine Morris, Antonio Punzo, Paul D. McNicholas and Ryan P. Browne", "title": "Asymmetric Clusters and Outliers: Mixtures of Multivariate Contaminated\n  Shifted Asymmetric Laplace Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of multivariate contaminated shifted asymmetric Laplace\ndistributions are developed for handling asymmetric clusters in the presence of\noutliers (also referred to as bad points herein). In addition to the parameters\nof the related non-contaminated mixture, for each (asymmetric) cluster, our\nmodel has one parameter controlling the proportion of outliers and one\nspecifying the degree of contamination. Crucially, these parameters do not have\nto be specified a priori, adding a flexibility to our approach that is absent\nfrom other approaches such as trimming. Moreover, each observation is given a\nposterior probability of belonging to a particular cluster, and of being an\noutlier or not; advantageously, this allows for the automatic detection of\noutliers. An expectation-conditional maximization algorithm is outlined for\nparameter estimation and various implementation issues are discussed. The\nbehaviour of the proposed model is investigated, and compared with\nwell-established finite mixtures, on artificial and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 23:29:54 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 00:31:37 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Morris", "Katherine", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1402.6863", "submitter": "Jack Kuipers", "authors": "Jack Kuipers, Giusi Moffa, David Heckerman", "title": "Addendum on the scoring of Gaussian directed acyclic graphical models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1217 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). Typo in definition of R and one sentence\n  corrected", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1689-1691", "doi": "10.1214/14-AOS1217", "report-no": "IMS-AOS-AOS1217", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a correction to the expression for scoring Gaussian directed\nacyclic graphical models derived in Geiger and Heckerman [Ann. Statist. 30\n(2002) 1414-1440] and discuss how to evaluate the score efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 11:27:44 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 08:09:08 GMT"}, {"version": "v3", "created": "Thu, 14 Aug 2014 09:11:18 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 08:44:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""], ["Heckerman", "David", ""]]}, {"id": "1402.6951", "submitter": "Drausin Wulsin", "authors": "Drausin F. Wulsin, Emily B. Fox, Brian Litt", "title": "Modeling the Complex Dynamics and Changing Correlations of Epileptic\n  Events", "comments": null, "journal-ref": null, "doi": "10.1016/j.artint.2014.05.006", "report-no": null, "categories": "stat.ML q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients with epilepsy can manifest short, sub-clinical epileptic \"bursts\" in\naddition to full-blown clinical seizures. We believe the relationship between\nthese two classes of events---something not previously studied\nquantitatively---could yield important insights into the nature and intrinsic\ndynamics of seizures. A goal of our work is to parse these complex epileptic\nevents into distinct dynamic regimes. A challenge posed by the intracranial EEG\n(iEEG) data we study is the fact that the number and placement of electrodes\ncan vary between patients. We develop a Bayesian nonparametric Markov switching\nprocess that allows for (i) shared dynamic regimes between a variable number of\nchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary\nof dynamic regimes. We encode a sparse and changing set of dependencies between\nthe channels using a Markov-switching Gaussian graphical model for the\ninnovations process driving the channel dynamics and demonstrate the importance\nof this model in parsing and out-of-sample predictions of iEEG data. We show\nthat our model produces intuitive state assignments that can help automate\nclinical analysis of seizures and enable the comparison of sub-clinical bursts\nand full clinical seizures.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 16:09:09 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 01:37:35 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Wulsin", "Drausin F.", ""], ["Fox", "Emily B.", ""], ["Litt", "Brian", ""]]}, {"id": "1402.6964", "submitter": "Austin Benson", "authors": "Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich", "title": "Scalable methods for nonnegative matrix factorizations of near-separable\n  tall-and-skinny matrices", "comments": null, "journal-ref": "Proceedings of Neural Information Processing Systems, 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous algorithms are used for nonnegative matrix factorization under the\nassumption that the matrix is nearly separable. In this paper, we show how to\nmake these algorithms efficient for data matrices that have many more rows than\ncolumns, so-called \"tall-and-skinny matrices\". One key component to these\nimproved methods is an orthogonal matrix transformation that preserves the\nseparability of the NMF problem. Our final methods need a single pass over the\ndata matrix and are suitable for streaming, multi-core, and MapReduce\narchitectures. We demonstrate the efficacy of these algorithms on\nterabyte-sized synthetic matrices and real-world matrices from scientific\ncomputing and bioinformatics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 16:41:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Lee", "Jason D.", ""], ["Rajwa", "Bartek", ""], ["Gleich", "David F.", ""]]}, {"id": "1402.7005", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas", "title": "Bayesian Multi-Scale Optimistic Optimization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a powerful global optimization technique for\nexpensive black-box functions. One of its shortcomings is that it requires\nauxiliary optimization of an acquisition function at each iteration. This\nauxiliary optimization can be costly and very hard to carry out in practice.\nMoreover, it creates serious theoretical concerns, as most of the convergence\nresults assume that the exact optimum of the acquisition function can be found.\nIn this paper, we introduce a new technique for efficient global optimization\nthat combines Gaussian process confidence bounds and treed simultaneous\noptimistic optimization to eliminate the need for auxiliary optimization of\nacquisition functions. The experiments with global optimization benchmarks and\na novel application to automatic information extraction demonstrate that the\nresulting technique is more efficient than the two approaches from which it\ndraws inspiration. Unlike most theoretical analyses of Bayesian optimization\nwith Gaussian processes, our finite-time convergence rate proofs do not require\nexact optimization of an acquisition function. That is, our approach eliminates\nthe unsatisfactory assumption that a difficult, potentially NP-hard, problem\nhas to be solved in order to obtain vanishing regret rates.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 18:38:02 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Wang", "Ziyu", ""], ["Shakibi", "Babak", ""], ["Jin", "Lin", ""], ["de Freitas", "Nando", ""]]}, {"id": "1402.7027", "submitter": "Rick Steinert", "authors": "Florian Ziel, Rick Steinert and Sven Husmann", "title": "Efficient Modeling and Forecasting of the Electricity Spot Price", "comments": null, "journal-ref": "Energy Economics, 47 (2015) 98-111", "doi": "10.1016/j.eneco.2014.10.012", "report-no": null, "categories": "stat.AP econ.EM q-fin.ST q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing importance of renewable energy, especially solar and wind\npower, has led to new forces in the formation of electricity prices. Hence,\nthis paper introduces an econometric model for the hourly time series of\nelectricity prices of the European Power Exchange (EPEX) which incorporates\nspecific features like renewable energy. The model consists of several\nsophisticated and established approaches and can be regarded as a periodic\nVAR-TARCH with wind power, solar power, and load as influences on the time\nseries. It is able to map the distinct and well-known features of electricity\nprices in Germany. An efficient iteratively reweighted lasso approach is used\nfor the estimation. Moreover, it is shown that several existing models are\noutperformed by the procedure developed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 19:18:21 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 11:45:13 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 10:33:59 GMT"}, {"version": "v4", "created": "Mon, 13 Oct 2014 09:46:09 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ziel", "Florian", ""], ["Steinert", "Rick", ""], ["Husmann", "Sven", ""]]}, {"id": "1402.7344", "submitter": "Menghan Wang", "authors": "Meera Sitharam, Mohamad Tarifi, Menghan Wang", "title": "An Incidence Geometry approach to Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Dictionary Learning (aka Sparse Coding) problem of obtaining a\nsparse representation of data points, by learning \\emph{dictionary vectors}\nupon which the data points can be written as sparse linear combinations. We\nview this problem from a geometry perspective as the spanning set of a subspace\narrangement, and focus on understanding the case when the underlying hypergraph\nof the subspace arrangement is specified. For this Fitted Dictionary Learning\nproblem, we completely characterize the combinatorics of the associated\nsubspace arrangements (i.e.\\ their underlying hypergraphs). Specifically, a\ncombinatorial rigidity-type theorem is proven for a type of geometric incidence\nsystem. The theorem characterizes the hypergraphs of subspace arrangements that\ngenerically yield (a) at least one dictionary (b) a locally unique dictionary\n(i.e.\\ at most a finite number of isolated dictionaries) of the specified size.\nWe are unaware of prior application of combinatorial rigidity techniques in the\nsetting of Dictionary Learning, or even in machine learning. We also provide a\nsystematic classification of problems related to Dictionary Learning together\nwith various algorithms, their assumptions and performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 18:54:07 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 03:15:39 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Sitharam", "Meera", ""], ["Tarifi", "Mohamad", ""], ["Wang", "Menghan", ""]]}, {"id": "1402.7349", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel,\n  and Daniela Witten", "title": "Learning Graphical Models With Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional graphical model in\nwhich certain hub nodes are highly-connected to many other nodes. Many authors\nhave studied the use of an l1 penalty in order to learn a sparse graph in\nhigh-dimensional setting. However, the l1 penalty implicitly assumes that each\nedge is equally likely and independent of all other edges. We propose a general\nframework to accommodate more realistic networks with hub nodes, using a convex\nformulation that involves a row-column overlap norm penalty. We apply this\ngeneral framework to three widely-used probabilistic graphical models: the\nGaussian graphical model, the covariance graph model, and the binary Ising\nmodel. An alternating direction method of multipliers algorithm is used to\nsolve the corresponding convex optimization problems. On synthetic data, we\ndemonstrate that our proposed framework outperforms competitors that do not\nexplicitly model hub nodes. We illustrate our proposal on a webpage data set\nand a gene expression data set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 19:09:11 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 18:33:43 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Tan", "Kean Ming", ""], ["London", "Palma", ""], ["Mohan", "Karthik", ""], ["Lee", "Su-In", ""], ["Fazel", "Maryam", ""], ["Witten", "Daniela", ""]]}]