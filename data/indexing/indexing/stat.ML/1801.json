[{"id": "1801.00025", "submitter": "Xiaodan Li", "authors": "Wangyan Feng, Shuning Wu, Xiaodan Li, Kevin Kunkle", "title": "A Deep Belief Network Based Machine Learning System for Risky Host\n  Detection", "comments": "10 pages, 10 figures. The paper is accepted by IEEE Conference on\n  Communications and Network Security 2017. However, it is not published\n  because either of the authors showed up in the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To assure cyber security of an enterprise, typically SIEM (Security\nInformation and Event Management) system is in place to normalize security\nevent from different preventive technologies and flag alerts. Analysts in the\nsecurity operation center (SOC) investigate the alerts to decide if it is truly\nmalicious or not. However, generally the number of alerts is overwhelming with\nmajority of them being false positive and exceeding the SOC's capacity to\nhandle all alerts. There is a great need to reduce the false positive rate as\nmuch as possible. While most previous research focused on network intrusion\ndetection, we focus on risk detection and propose an intelligent Deep Belief\nNetwork machine learning system. The system leverages alert information,\nvarious security logs and analysts' investigation results in a real enterprise\nenvironment to flag hosts that have high likelihood of being compromised. Text\nmining and graph based method are used to generate targets and create features\nfor machine learning. In the experiment, Deep Belief Network is compared with\nother machine learning algorithms, including multi-layer neural network, random\nforest, support vector machine and logistic regression. Results on real\nenterprise data indicate that the deep belief network machine learning system\nperforms better than other algorithms for our problem and is six times more\neffective than current rule-based system. We also implement the whole system\nfrom data collection, label creation, feature engineering to host score\ngeneration in a real enterprise production environment.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 19:46:09 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Feng", "Wangyan", ""], ["Wu", "Shuning", ""], ["Li", "Xiaodan", ""], ["Kunkle", "Kevin", ""]]}, {"id": "1801.00056", "submitter": "Boris Belousov", "authors": "Boris Belousov, Jan Peters", "title": "f-Divergence constrained policy improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure stability of learning, state-of-the-art generalized policy\niteration algorithms augment the policy improvement step with a trust region\nconstraint bounding the information loss. The size of the trust region is\ncommonly determined by the Kullback-Leibler (KL) divergence, which not only\ncaptures the notion of distance well but also yields closed-form solutions. In\nthis paper, we consider a more general class of f-divergences and derive the\ncorresponding policy update rules. The generic solution is expressed through\nthe derivative of the convex conjugate function to f and includes the KL\nsolution as a special case. Within the class of f-divergences, we further focus\non a one-parameter family of $\\alpha$-divergences to study effects of the\nchoice of divergence on policy improvement. Previously known as well as new\npolicy updates emerge for different values of $\\alpha$. We show that every type\nof policy update comes with a compatible policy evaluation resulting from the\nchosen f-divergence. Interestingly, the mean-squared Bellman error minimization\nis closely related to policy evaluation with the Pearson $\\chi^2$-divergence\npenalty, while the KL divergence results in the soft-max policy update and a\nlog-sum-exp critic. We carry out asymptotic analysis of the solutions for\ndifferent values of $\\alpha$ and demonstrate the effects of using different\ndivergence functions on a multi-armed bandit problem and on common standard\nreinforcement learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 23:07:26 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 11:36:12 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Belousov", "Boris", ""], ["Peters", "Jan", ""]]}, {"id": "1801.00085", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Chunyuan Li, Changyou Chen, Lawrence Carin", "title": "Learning Structural Weight Uncertainty for Sequential Decision-Making", "comments": "Accepted by AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probability distributions on the weights of neural networks (NNs)\nhas recently proven beneficial in many applications. Bayesian methods, such as\nStein variational gradient descent (SVGD), offer an elegant framework to reason\nabout NN model uncertainty. However, by assuming independent Gaussian priors\nfor the individual NN weights (as often applied), SVGD does not impose prior\nknowledge that there is often structural information (dependence) among\nweights. We propose efficient posterior learning of structural weight\nuncertainty, within an SVGD framework, by employing matrix variate Gaussian\npriors on NN parameters. We further investigate the learned structural\nuncertainty in sequential decision-making problems, including contextual\nbandits and reinforcement learning. Experiments on several synthetic and real\ndatasets indicate the superiority of our model, compared with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 04:34:34 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 01:06:13 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Li", "Chunyuan", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "1801.00101", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Satyen Kale, Mehryar Mohri, Karthik Sridharan", "title": "Parameter-free online learning via model selection", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient algorithmic framework for model selection in online\nlearning, also known as parameter-free online learning. Departing from previous\nwork, which has focused on highly structured function classes such as nested\nballs in Hilbert space, we propose a generic meta-algorithm framework that\nachieves online model selection oracle inequalities under minimal structural\nassumptions. We give the first computationally efficient parameter-free\nalgorithms that work in arbitrary Banach spaces under mild smoothness\nassumptions; previous results applied only to Hilbert spaces. We further derive\nnew oracle inequalities for matrix classes, non-nested convex sets, and\n$\\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these\nresults by providing oracle inequalities for arbitrary non-linear classes in\nthe online supervised learning model. These results are all derived through a\nunified meta-algorithm scheme using a novel \"multi-scale\" algorithm for\nprediction with expert advice based on random playout, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 08:21:19 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 22:25:05 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Foster", "Dylan J.", ""], ["Kale", "Satyen", ""], ["Mohri", "Mehryar", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1801.00171", "submitter": "Konstantinos Pitas", "authors": "Konstantinos Pitas, Mike Davies, Pierre Vandergheynst", "title": "PAC-Bayesian Margin Bounds for Convolutional Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1707.09564 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the generalization error of deep neural networks has been analyzed\nthrough the PAC-Bayesian framework, for the case of fully connected layers. We\nadapt this approach to the convolutional setting.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 18:11:59 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 09:27:47 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Pitas", "Konstantinos", ""], ["Davies", "Mike", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1801.00185", "submitter": "Piero Mazzarisi", "authors": "Piero Mazzarisi, Paolo Barucca, Fabrizio Lillo, Daniele Tantari", "title": "A dynamic network model with persistent links and node-specific latent\n  variables, with an application to the interbank market", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic network model where two mechanisms control the\nprobability of a link between two nodes: (i) the existence or absence of this\nlink in the past, and (ii) node-specific latent variables (dynamic fitnesses)\ndescribing the propensity of each node to create links. Assuming a Markov\ndynamics for both mechanisms, we propose an Expectation-Maximization algorithm\nfor model estimation and inference of the latent variables. The estimated\nparameters and fitnesses can be used to forecast the presence of a link in the\nfuture. We apply our methodology to the e-MID interbank network for which the\ntwo linkage mechanisms are associated with two different trading behaviors in\nthe process of network formation, namely preferential trading and trading\ndriven by node-specific characteristics. The empirical results allow to\nrecognise preferential lending in the interbank market and indicate how a\nmethod that does not account for time-varying network topologies tends to\noverestimate preferential linkage.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 19:57:35 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Mazzarisi", "Piero", ""], ["Barucca", "Paolo", ""], ["Lillo", "Fabrizio", ""], ["Tantari", "Daniele", ""]]}, {"id": "1801.00203", "submitter": "Andrew Ferguson", "authors": "Wei Chen and Andrew L Ferguson", "title": "Molecular enhanced sampling with autoencoders: On-the-fly collective\n  variable discovery and accelerated free energy landscape exploration", "comments": null, "journal-ref": "J. Chem. Theory Comput. 39 2079-2102 (2018)", "doi": "10.1002/jcc.25520", "report-no": null, "categories": "physics.bio-ph physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macromolecular and biomolecular folding landscapes typically contain high\nfree energy barriers that impede efficient sampling of configurational space by\nstandard molecular dynamics simulation. Biased sampling can artificially drive\nthe simulation along pre-specified collective variables (CVs), but success\ndepends critically on the availability of good CVs associated with the\nimportant collective dynamical motions. Nonlinear machine learning techniques\ncan identify such CVs but typically do not furnish an explicit relationship\nwith the atomic coordinates necessary to perform biased sampling. In this work,\nwe employ auto-associative artificial neural networks (\"autoencoders\") to learn\nnonlinear CVs that are explicit and differentiable functions of the atomic\ncoordinates. Our approach offers substantial speedups in exploration of\nconfigurational space, and is distinguished from exiting approaches by its\ncapacity to simultaneously discover and directly accelerate along data-driven\nCVs. We demonstrate the approach in simulations of alanine dipeptide and\nTrp-cage, and have developed an open-source and freely-available implementation\nwithin OpenMM.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 22:32:04 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 18:37:53 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Chen", "Wei", ""], ["Ferguson", "Andrew L", ""]]}, {"id": "1801.00209", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao and Liang Zhang and Long Xia and Zhuoye Ding and Dawei\n  Yin and Jiliang Tang", "title": "Deep Reinforcement Learning for List-wise Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedbacks. In particular, we introduce an online user-agent\ninteracting environment simulator, which can pre-train and evaluate model\nparameters offline before applying the model online. Moreover, we validate the\nimportance of list-wise recommendations during the interactions between users\nand agent, and develop a novel approach to incorporate them into the proposed\nframework LIRD for list-wide recommendations. The experimental results based on\na real-world e-commerce dataset demonstrate the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 23:30:36 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 14:57:45 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 06:29:27 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Zhang", "Liang", ""], ["Xia", "Long", ""], ["Ding", "Zhuoye", ""], ["Yin", "Dawei", ""], ["Tang", "Jiliang", ""]]}, {"id": "1801.00282", "submitter": "Zhou Honggang", "authors": "Jie Jia, Honggang Zhou, Yunchun Li", "title": "Using Deep Neural Network Approximate Bayesian Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to approximate posterior probabilities of Bayesian\nNetwork using Deep Neural Network. Experiment results on several public\nBayesian Network datasets shows that Deep Neural Network is capable of learning\njoint probability distri- bution of Bayesian Network by learning from a few\nobservation and posterior probability distribution pairs with high accuracy.\nCompared with traditional approximate method likelihood weighting sampling\nalgorithm, our method is much faster and gains higher accuracy in medium sized\nBayesian Network. Another advantage of our method is that our method can be\nparallelled much easier in GPU without extra effort. We also ex- plored the\nconnection between the accuracy of our model and the number of training\nexamples. The result shows that our model saturate as the number of training\nexamples grow and we don't need many training examples to get reasonably good\nresult. Another contribution of our work is that we have shown discriminative\nmodel like Deep Neural Network can approximate generative model like Bayesian\nNetwork.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 13:26:20 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 05:36:36 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Jia", "Jie", ""], ["Zhou", "Honggang", ""], ["Li", "Yunchun", ""]]}, {"id": "1801.00283", "submitter": "Klaus Broelemann", "authors": "Klaus Broelemann, Thomas Gottron and Gjergji Kasneci", "title": "Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of latent truth discovery, LTD for short, where the\ngoal is to discover the underlying true values of entity attributes in the\npresence of noisy, conflicting or incomplete information. Despite a multitude\nof algorithms to address the LTD problem that can be found in literature, only\nlittle is known about their overall performance with respect to effectiveness\n(in terms of truth discovery capabilities), efficiency and robustness. A\npractical LTD approach should satisfy all these characteristics so that it can\nbe applied to heterogeneous datasets of varying quality and degrees of\ncleanliness.\n  We propose a novel algorithm for LTD that satisfies the above requirements.\nThe proposed model is based on Restricted Boltzmann Machines, thus coined\nLTD-RBM. In extensive experiments on various heterogeneous and publicly\navailable datasets, LTD-RBM is superior to state-of-the-art LTD techniques in\nterms of an overall consideration of effectiveness, efficiency and robustness.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 13:26:51 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Broelemann", "Klaus", ""], ["Gottron", "Thomas", ""], ["Kasneci", "Gjergji", ""]]}, {"id": "1801.00315", "submitter": "E.M. Stoudenmire", "authors": "E.M. Stoudenmire", "title": "Learning Relevant Features of Data with Multi-scale Tensor Networks", "comments": "12 pages, 13 figures", "journal-ref": "Quantum Science and Technology 3, 034003 (2018)", "doi": "10.1088/2058-9565/aaba1a", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cond-mat.str-el cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by coarse-graining approaches used in physics, we show how similar\nalgorithms can be adapted for data. The resulting algorithms are based on\nlayered tree tensor networks and scale linearly with both the dimension of the\ninput and the training set size. Computing most of the layers with an\nunsupervised algorithm, then optimizing just the top layer for supervised\nclassification of the MNIST and fashion-MNIST data sets gives very good\nresults. We also discuss mixing a prior guess for supervised weights together\nwith an unsupervised representation of the data, yielding a smaller number of\nfeatures nevertheless able to give good performance.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 16:53:12 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Stoudenmire", "E. M.", ""]]}, {"id": "1801.00318", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification", "comments": "5 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 17:13:55 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:19:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1801.00329", "submitter": "Yang Yu", "authors": "Yu-Ren Liu, Yi-Qi Hu, Hong Qian, Yang Yu, Chao Qian", "title": "ZOOpt: Toolbox for Derivative-Free Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of derivative-free optimization allow efficient approximating\nthe global optimal solutions of sophisticated functions, such as functions with\nmany local optima, non-differentiable and non-continuous functions. This\narticle describes the ZOOpt (https://github.com/eyounx/ZOOpt) toolbox that\nprovides efficient derivative-free solvers and are designed easy to use. ZOOpt\nprovides a Python package for single-thread optimization, and a light-weighted\ndistributed version with the help of the Julia language for Python described\nfunctions. ZOOpt toolbox particularly focuses on optimization problems in\nmachine learning, addressing high-dimensional, noisy, and large-scale problems.\nThe toolbox is being maintained toward ready-to-use tool in real-world machine\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 18:06:25 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 21:11:13 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Liu", "Yu-Ren", ""], ["Hu", "Yi-Qi", ""], ["Qian", "Hong", ""], ["Yu", "Yang", ""], ["Qian", "Chao", ""]]}, {"id": "1801.00364", "submitter": "Martin Spindler", "authors": "Jannis Kueck, Ye Luo, Martin Spindler, Zigan Wang", "title": "Estimation and Inference of Treatment Effects with $L_2$-Boosting in\n  High-Dimensional Settings", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical researchers are increasingly faced with rich data sets containing\nmany controls or instrumental variables, making it essential to choose an\nappropriate approach to variable selection. In this paper, we provide results\nfor valid inference after post- or orthogonal $L_2$-Boosting is used for\nvariable selection. We consider treatment effects after selecting among many\ncontrol variables and instrumental variable models with potentially many\ninstruments. To achieve this, we establish new results for the rate of\nconvergence of iterated post-$L_2$-Boosting and orthogonal $L_2$-Boosting in a\nhigh-dimensional setting similar to Lasso, i.e., under approximate sparsity\nwithout assuming the beta-min condition. These results are extended to the 2SLS\nframework and valid inference is provided for treatment effect analysis. We\ngive extensive simulation results for the proposed methods and compare them\nwith Lasso. In an empirical application, we construct efficient IVs with our\nproposed methods to estimate the effect of pre-merger overlap of bank branch\nnetworks in the US on the post-merger stock returns of the acquirer bank.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 22:15:57 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:45:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kueck", "Jannis", ""], ["Luo", "Ye", ""], ["Spindler", "Martin", ""], ["Wang", "Zigan", ""]]}, {"id": "1801.00393", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Theoretical Analysis of Sparse Subspace Clustering with Missing Entries", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:4975-4984, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning\nmethod for clustering data lying close to an unknown union of low-dimensional\nlinear subspaces; a problem with numerous applications in pattern recognition\nand computer vision. Even though the behavior of SSC for complete data is by\nnow well-understood, little is known about its theoretical properties when\napplied to data with missing entries. In this paper we give theoretical\nguarantees for SSC with incomplete data, and analytically establish that\nprojecting the zero-filled data onto the observation pattern of the point being\nexpressed leads to a substantial improvement in performance. The main insight\nthat stems from our analysis is that even though the projection induces\nadditional missing entries, this is counterbalanced by the fact that the\nprojected and zero-filled data are in effect incomplete points associated with\nthe union of the corresponding projected subspaces, with respect to which the\npoint being expressed is complete. The significance of this phenomenon\npotentially extends to the entire class of self-expressive methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 04:51:46 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 08:43:52 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 17:27:43 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1801.00507", "submitter": "Alexander Zimin", "authors": "Alexander Zimin and Christoph Lampert", "title": "MACRO: A Meta-Algorithm for Conditional Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study conditional risk minimization (CRM), i.e. the problem of learning a\nhypothesis of minimal risk for prediction at the next step of sequentially\narriving dependent data. Despite it being a fundamental problem, successful\nlearning in the CRM sense has so far only been demonstrated using theoretical\nalgorithms that cannot be used for real problems as they would require storing\nall incoming data. In this work, we introduce MACRO, a meta-algorithm for CRM\nthat does not suffer from this shortcoming, but nevertheless offers learning\nguarantees. Instead of storing all data it maintains and iteratively updates a\nset of learning subroutines. With suitable approximations, MACRO applied to\nreal data, yielding improved prediction performance compared to traditional\nnon-conditional learning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 20:48:33 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 21:20:44 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zimin", "Alexander", ""], ["Lampert", "Christoph", ""]]}, {"id": "1801.00513", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller", "title": "An elementary derivation of the Chinese restaurant process from\n  Sethuraman's stick-breaking process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chinese restaurant process (CRP) and the stick-breaking process are the\ntwo most commonly used representations of the Dirichlet process. However, the\nusual proof of the connection between them is indirect, relying on abstract\nproperties of the Dirichlet process that are difficult for nonexperts to\nverify. This short note provides a direct proof that the stick-breaking process\nleads to the CRP, without using any measure theory. We also discuss how the\nstick-breaking representation arises naturally from the CRP.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 21:31:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 03:34:03 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 02:08:47 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Miller", "Jeffrey W.", ""]]}, {"id": "1801.00631", "submitter": "Gary Marcus", "authors": "Gary Marcus", "title": "Deep Learning: A Critical Appraisal", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 12:49:35 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Marcus", "Gary", ""]]}, {"id": "1801.00632", "submitter": "Cedric De Boom", "authors": "Cedric De Boom, Thomas Demeester, Bart Dhoedt", "title": "Character-level Recurrent Neural Networks in Practice: Comparing\n  Training and Sampling Schemes", "comments": "23 pages, 11 figures, 4 tables", "journal-ref": null, "doi": "10.1007/s00521-017-3322-z", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent neural networks are nowadays successfully used in an abundance of\napplications, going from text, speech and image processing to recommender\nsystems. Backpropagation through time is the algorithm that is commonly used to\ntrain these networks on specific tasks. Many deep learning frameworks have\ntheir own implementation of training and sampling procedures for recurrent\nneural networks, while there are in fact multiple other possibilities to choose\nfrom and other parameters to tune. In existing literature this is very often\noverlooked or ignored. In this paper we therefore give an overview of possible\ntraining and sampling schemes for character-level recurrent neural networks to\nsolve the task of predicting the next token in a given sequence. We test these\ndifferent schemes on a variety of datasets, neural network architectures and\nparameter settings, and formulate a number of take-home recommendations. The\nchoice of training and sampling scheme turns out to be subject to a number of\ntrade-offs, such as training stability, sampling time, model performance and\nimplementation effort, but is largely independent of the data. Perhaps the most\nsurprising result is that transferring hidden states for correctly initializing\nthe model on subsequences often leads to unstable training behavior depending\non the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 12:50:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 09:15:07 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["De Boom", "Cedric", ""], ["Demeester", "Thomas", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1801.00636", "submitter": "Mohammad Sultan", "authors": "Mohammad M. Sultan, Hannah K. Wayment-Steele, Vijay S. Pande", "title": "Transferable neural networks for enhanced sampling of protein dynamics", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational auto-encoder frameworks have demonstrated success in reducing\ncomplex nonlinear dynamics in molecular simulation to a single non-linear\nembedding. In this work, we illustrate how this non-linear latent embedding can\nbe used as a collective variable for enhanced sampling, and present a simple\nmodification that allows us to rapidly perform sampling in multiple related\nsystems. We first demonstrate our method is able to describe the effects of\nforce field changes in capped alanine dipeptide after learning a model using\nAMBER99. We further provide a simple extension to variational dynamics encoders\nthat allows the model to be trained in a more efficient manner on larger\nsystems by encoding the outputs of a linear transformation using time-structure\nbased independent component analysis (tICA). Using this technique, we show how\nsuch a model trained for one protein, the WW domain, can efficiently be\ntransferred to perform enhanced sampling on a related mutant protein, the GTT\nmutation. This method shows promise for its ability to rapidly sample related\nsystems using a single transferable collective variable and is generally\napplicable to sets of related simulations, enabling us to probe the effects of\nvariation in increasingly large systems of biophysical interest.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 13:19:11 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Sultan", "Mohammad M.", ""], ["Wayment-Steele", "Hannah K.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1801.00668", "submitter": "Sheng Zhang", "authors": "Jiashu Zhang, Sheng Zhang, and Defang Li", "title": "Random Euler Complex-Valued Nonlinear Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, both the neural network and kernel adaptive filter have\nsuccessfully been used for nonlinear signal processing. However, they suffer\nfrom high computational cost caused by their complex/growing network\nstructures. In this paper, we propose two random Euler filters for\ncomplex-valued nonlinear filtering problem, i.e., linear random Euler\ncomplex-valued filter (LRECF) and its widely-linear version (WLRECF), which\npossess a simple and fixed network structure. The transient and steady-state\nperformances are studied in a non-stationary environment. The analytical\nminimum mean square error (MSE) and optimum step-size are derived. Finally,\nnumerical simulations on complex-valued nonlinear system identification and\nnonlinear channel equalization are presented to show the effectiveness of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 14:38:16 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Zhang", "Jiashu", ""], ["Zhang", "Sheng", ""], ["Li", "Defang", ""]]}, {"id": "1801.00681", "submitter": "Guohao Li", "authors": "Shuheng Wang, Guohao Li, Yifan Bao", "title": "A novel improved fuzzy support vector machine based stock price trend\n  forecast model", "comments": "This paper is accepted by the International Conference on Innovations\n  in Economic Management and Social Science (IEMSS 2017) and International\n  Conference on Humanities, Management Engineering and Education Technology\n  (HMEET 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of fuzzy support vector machine in stock price forecast. Support\nvector machine is a new type of machine learning method proposed in 1990s. It\ncan deal with classification and regression problems very successfully. Due to\nthe excellent learning performance of support vector machine, the technology\nhas become a hot research topic in the field of machine learning, and it has\nbeen successfully applied in many fields. However, as a new technology, there\nare many limitations to support vector machines. There is a large amount of\nfuzzy information in the objective world. If the training of support vector\nmachine contains noise and fuzzy information, the performance of the support\nvector machine will become very weak and powerless. As the complexity of many\nfactors influence the stock price prediction, the prediction results of\ntraditional support vector machine cannot meet people with precision, this\nstudy improved the traditional support vector machine fuzzy prediction\nalgorithm is proposed to improve the new model precision. NASDAQ Stock Market,\nStandard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy\nsupport vector machine (NA-FSVM) is the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 15:17:37 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Wang", "Shuheng", ""], ["Li", "Guohao", ""], ["Bao", "Yifan", ""]]}, {"id": "1801.00711", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, Rongqing Huang, Ya Gao", "title": "Network-Scale Traffic Modeling and Forecasting with Graphical Lasso and\n  Neural Networks", "comments": null, "journal-ref": "Journal of Transportation Engineering, 2012, 138(11): 1358-1367", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic flow forecasting, especially the short-term case, is an important\ntopic in intelligent transportation systems (ITS). This paper does a lot of\nresearch on network-scale modeling and forecasting of short-term traffic flows.\nFirstly, we propose the concepts of single-link and multi-link models of\ntraffic flow forecasting. Secondly, we construct four prediction models by\ncombining the two models with single-task learning and multi-task learning. The\ncombination of the multi-link model and multi-task learning not only improves\nthe experimental efficiency but also the prediction accuracy. Moreover, a new\nmulti-link single-task approach that combines graphical lasso (GL) with neural\nnetwork (NN) is proposed. GL provides a general methodology for solving\nproblems involving lots of variables. Using L1 regularization, GL builds a\nsparse graphical model making use of the sparse inverse covariance matrix. In\naddition, Gaussian process regression (GPR) is a classic regression algorithm\nin Bayesian machine learning. Although there is wide research on GPR, there are\nfew applications of GPR in traffic flow forecasting. In this paper, we apply\nGPR to traffic flow forecasting and show its potential. Through sufficient\nexperiments, we compare all of the proposed approaches and make an overall\nassessment at last.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 03:08:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Sun", "Shiliang", ""], ["Huang", "Rongqing", ""], ["Gao", "Ya", ""]]}, {"id": "1801.00723", "submitter": "Pegah Karimi", "authors": "Pegah Karimi, Nicholas Davis, Kazjon Grace, Mary Lou Maher", "title": "Deep Learning for Identifying Potential Conceptual Shifts for\n  Co-creative Drawing", "comments": "This is an extended version of the paper presented at 31st Conference\n  on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n  Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for identifying conceptual shifts between visual\ncategories, which will form the basis for a co-creative drawing system to help\nusers draw more creative sketches. The system recognizes human sketches and\nmatches them to structurally similar sketches from categories to which they do\nnot belong. This would allow a co-creative drawing system to produce an\nambiguous sketch that blends features from both categories.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:51:22 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Karimi", "Pegah", ""], ["Davis", "Nicholas", ""], ["Grace", "Kazjon", ""], ["Maher", "Mary Lou", ""]]}, {"id": "1801.00753", "submitter": "Franz J. Kir\\'aly", "authors": "Frithjof Gressmann, Franz J. Kir\\'aly, Bilal Mateen and Harald\n  Oberhauser", "title": "Probabilistic supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modelling and supervised learning are central to modern data\nscience. With predictions from an ever-expanding number of supervised black-box\nstrategies - e.g., kernel methods, random forests, deep learning aka neural\nnetworks - being employed as a basis for decision making processes, it is\ncrucial to understand the statistical uncertainty associated with these\npredictions.\n  As a general means to approach the issue, we present an overarching framework\nfor black-box prediction strategies that not only predict the target but also\ntheir own predictions' uncertainty. Moreover, the framework allows for fair\nassessment and comparison of disparate prediction strategies. For this, we\nformally consider strategies capable of predicting full distributions from\nfeature variables, so-called probabilistic supervised learning strategies.\n  Our work draws from prior work including Bayesian statistics, information\ntheory, and modern supervised machine learning, and in a novel synthesis leads\nto (a) new theoretical insights such as a probabilistic bias-variance\ndecomposition and an entropic formulation of prediction, as well as to (b) new\nalgorithms and meta-algorithms, such as composite prediction strategies,\nprobabilistic boosting and bagging, and a probabilistic predictive independence\ntest.\n  Our black-box formulation also leads (c) to a new modular interface view on\nprobabilistic supervised learning and a modelling workflow API design, which we\nhave implemented in the newly released skpro machine learning toolbox,\nextending the familiar modelling interface and meta-modelling functionality of\nsklearn. The skpro package provides interfaces for construction, composition,\nand tuning of probabilistic supervised learning strategies, together with\norchestration features for validation and comparison of any such strategy - be\nit frequentist, Bayesian, or other.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 18:08:49 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 21:22:42 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 14:30:27 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Gressmann", "Frithjof", ""], ["Kir\u00e1ly", "Franz J.", ""], ["Mateen", "Bilal", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1801.00823", "submitter": "Thee Chanyaswad", "authors": "Thee Chanyaswad, Alex Dytso, H. Vincent Poor, Prateek Mittal", "title": "MVG Mechanism: Differential Privacy under Matrix-Valued Query", "comments": "Appeared in CCS'18", "journal-ref": "Thee Chanyaswad, Alex Dytso, H. Vincent Poor, and Prateek Mittal.\n  2018. MVG Mechanism: Differential Privacy under Matrix-Valued Query. In 2018\n  ACM SIGSAC Conference on Computer and Communications Security (CCS'18)", "doi": "10.1145/3243734.3243750", "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy mechanism design has traditionally been tailored for a\nscalar-valued query function. Although many mechanisms such as the Laplace and\nGaussian mechanisms can be extended to a matrix-valued query function by adding\ni.i.d. noise to each element of the matrix, this method is often suboptimal as\nit forfeits an opportunity to exploit the structural characteristics typically\nassociated with matrix analysis. To address this challenge, we propose a novel\ndifferential privacy mechanism called the Matrix-Variate Gaussian (MVG)\nmechanism, which adds a matrix-valued noise drawn from a matrix-variate\nGaussian distribution, and we rigorously prove that the MVG mechanism preserves\n$(\\epsilon,\\delta)$-differential privacy. Furthermore, we introduce the concept\nof directional noise made possible by the design of the MVG mechanism.\nDirectional noise allows the impact of the noise on the utility of the\nmatrix-valued query function to be moderated. Finally, we experimentally\ndemonstrate the performance of our mechanism using three matrix-valued queries\non three privacy-sensitive datasets. We find that the MVG mechanism notably\noutperforms four previous state-of-the-art approaches, and provides comparable\nutility to the non-private baseline.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:24:24 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 20:48:34 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 21:31:23 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Chanyaswad", "Thee", ""], ["Dytso", "Alex", ""], ["Poor", "H. Vincent", ""], ["Mittal", "Prateek", ""]]}, {"id": "1801.00857", "submitter": "Alireza Karbalayghareh", "authors": "Alireza Karbalayghareh, Xiaoning Qian, and Edward R. Dougherty", "title": "Optimal Bayesian Transfer Learning", "comments": "IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions On Signal Processing, Vol. 66, No. 14, July 15,\n  2018", "doi": "10.1109/TSP.2018.2839583", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has recently attracted significant research attention, as\nit simultaneously learns from different source domains, which have plenty of\nlabeled data, and transfers the relevant knowledge to the target domain with\nlimited labeled data to improve the prediction performance. We propose a\nBayesian transfer learning framework where the source and target domains are\nrelated through the joint prior density of the model parameters. The modeling\nof joint prior densities enables better understanding of the \"transferability\"\nbetween domains. We define a joint Wishart density for the precision matrices\nof the Gaussian feature-label distributions in the source and target domains to\nact like a bridge that transfers the useful information of the source domain to\nhelp classification in the target domain by improving the target posteriors.\nUsing several theorems in multivariate statistics, the posteriors and posterior\npredictive densities are derived in closed forms with hypergeometric functions\nof matrix argument, leading to our novel closed-form and fast Optimal Bayesian\nTransfer Learning (OBTL) classifier. Experimental results on both synthetic and\nreal-world benchmark data confirm the superb performance of the OBTL compared\nto the other state-of-the-art transfer learning and domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 23:15:56 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 17:30:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Karbalayghareh", "Alireza", ""], ["Qian", "Xiaoning", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1801.00885", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky, John D. Jakeman", "title": "Gradient-based Optimization for Regression in the Functional\n  Tensor-Train Format", "comments": "24 pages", "journal-ref": null, "doi": "10.1016/j.jcp.2018.08.010", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of low-multilinear-rank functional regression, i.e.,\nlearning a low-rank parametric representation of functions from scattered\nreal-valued data. Our first contribution is the development and analysis of an\nefficient gradient computation that enables gradient-based optimization\nprocedures, including stochastic gradient descent and quasi-Newton methods, for\nlearning the parameters of a functional tensor-train (FT). The functional\ntensor-train uses the tensor-train (TT) representation of low-rank arrays as an\nansatz for a class of low-multilinear-rank functions. The FT is represented by\na set of matrix-valued functions that contain a set of univariate functions,\nand the regression task is to learn the parameters of these univariate\nfunctions. Our second contribution demonstrates that using nonlinearly\nparameterized univariate functions, e.g., symmetric kernels with moving\ncenters, within each core can outperform the standard approach of using a\nlinear expansion of basis functions. Our final contributions are new rank\nadaptation and group-sparsity regularization procedures to minimize\noverfitting. We use several benchmark problems to demonstrate at least an order\nof magnitude lower accuracy with gradient-based optimization methods than\nstandard alternating least squares procedures in the low-sample number regime.\nWe also demonstrate an order of magnitude reduction in accuracy on a test\nproblem resulting from using nonlinear parameterizations over linear\nparameterizations. Finally we compare regression performance with 22 other\nnonparametric and parametric regression methods on 10 real-world data sets. We\nachieve top-five accuracy for seven of the data sets and best accuracy for two\nof the data sets. These rankings are the best amongst parametric models and\ncompetetive with the best non-parametric methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:34:14 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 04:53:22 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Jakeman", "John D.", ""]]}, {"id": "1801.00905", "submitter": "Mayank Singh", "authors": "Mayank Singh, Abhishek Sinha and Balaji Krishnamurthy", "title": "Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural networks have seen a huge surge in its adoption due to their\nability to provide high accuracy on various tasks. On the other hand, the\nexistence of adversarial examples have raised suspicions regarding the\ngeneralization capabilities of neural networks. In this work, we focus on the\nweight matrix learnt by the neural networks and hypothesize that ill\nconditioned weight matrix is one of the contributing factors in neural\nnetwork's susceptibility towards adversarial examples. For ensuring that the\nlearnt weight matrix's condition number remains sufficiently low, we suggest\nusing orthogonal regularizer. We show that this indeed helps in increasing the\nadversarial accuracy on MNIST and F-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 05:52:52 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Singh", "Mayank", ""], ["Sinha", "Abhishek", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1801.01019", "submitter": "Nghia (Andy) Nguyen", "authors": "Christine A. Liang, Lei Chen, Amer Wahed, Andy N.D. Nguyen", "title": "Proteomics Analysis of FLT3-ITD Mutation in Acute Myeloid Leukemia Using\n  Deep Learning Neural Network", "comments": "12 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning can significantly benefit cancer proteomics and genomics. In\nthis study, we attempt to determine a set of critical proteins that are\nassociated with the FLT3-ITD mutation in newly-diagnosed acute myeloid leukemia\npatients. A Deep Learning network consisting of autoencoders forming a\nhierarchical model from which high-level features are extracted without labeled\ntraining data. Dimensional reduction reduced the number of critical proteins\nfrom 231 to 20. Deep Learning found an excellent correlation between FLT3-ITD\nmutation with the levels of these 20 critical proteins (accuracy 97%,\nsensitivity 90%, specificity 100%). Our Deep Learning network could hone in on\n20 proteins with the strongest association with FLT3-ITD. The results of this\nstudy allow a novel approach to determine critical protein pathways in the\nFLT3-ITD mutation, and provide proof-of-concept for an accurate approach to\nmodel big data in cancer proteomics and genomics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 13:05:30 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Liang", "Christine A.", ""], ["Chen", "Lei", ""], ["Wahed", "Amer", ""], ["Nguyen", "Andy N. D.", ""]]}, {"id": "1801.01061", "submitter": "Mu Niu", "authors": "Mu Niu, Pokman Cheung, Lizhen Lin, Zhenwen Dai, Neil Lawrence, David\n  Dunson", "title": "Intrinsic Gaussian processes on complex constrained domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of intrinsic Gaussian processes (in-GPs) for\ninterpolation, regression and classification on manifolds with a primary focus\non complex constrained domains or irregular shaped spaces arising as subsets or\nsubmanifolds of R, R2, R3 and beyond. For example, in-GPs can accommodate\nspatial domains arising as complex subsets of Euclidean space. in-GPs respect\nthe potentially complex boundary or interior conditions as well as the\nintrinsic geometry of the spaces. The key novelty of the proposed approach is\nto utilise the relationship between heat kernels and the transition density of\nBrownian motion on manifolds for constructing and approximating valid and\ncomputationally feasible covariance kernels. This enables in-GPs to be\npractically applied in great generality, while existing approaches for\nsmoothing on constrained domains are limited to simple special cases. The broad\nutilities of the in-GP approach is illustrated through simulation studies and\ndata examples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 16:07:33 GMT"}], "update_date": "2018-01-05", "authors_parsed": [["Niu", "Mu", ""], ["Cheung", "Pokman", ""], ["Lin", "Lizhen", ""], ["Dai", "Zhenwen", ""], ["Lawrence", "Neil", ""], ["Dunson", "David", ""]]}, {"id": "1801.01220", "submitter": "Changshuai Wei", "authors": "Changshuai Wei and Qing Lu", "title": "Generalized Similarity U: A Non-parametric Test of Association Based on\n  Similarity", "comments": null, "journal-ref": "Bioinformatics (2017): btx103", "doi": "10.1093/bioinformatics/btx103", "report-no": null, "categories": "stat.ME q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second generation sequencing technologies are being increasingly used for\ngenetic association studies, where the main research interest is to identify\nsets of genetic variants that contribute to various phenotype. The phenotype\ncan be univariate disease status, multivariate responses and even\nhigh-dimensional outcomes. Considering the genotype and phenotype as two\ncomplex objects, this also poses a general statistical problem of testing\nassociation between complex objects. We here proposed a similarity-based test,\ngeneralized similarity U (GSU), that can test the association between complex\nobjects. We first studied the theoretical properties of the test in a general\nsetting and then focused on the application of the test to sequencing\nassociation studies. Based on theoretical analysis, we proposed to use\nLaplacian kernel based similarity for GSU to boost power and enhance\nrobustness. Through simulation, we found that GSU did have advantages over\nexisting methods in terms of power and robustness. We further performed a whole\ngenome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative\n(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with\nimaging phenotype. We developed a C++ package for analysis of whole genome\nsequencing data using GSU. The source codes can be downloaded at\nhttps://github.com/changshuaiwei/gsu.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 01:43:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wei", "Changshuai", ""], ["Lu", "Qing", ""]]}, {"id": "1801.01236", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, George Em Karniadakis", "title": "Multistep Neural Networks for Data-driven Discovery of Nonlinear\n  Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.NA nlin.CD physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of transforming observed data into predictive mathematical models\nof the physical world has always been paramount in science and engineering.\nAlthough data is currently being collected at an ever-increasing pace, devising\nmeaningful models out of such observations in an automated fashion still\nremains an open problem. In this work, we put forth a machine learning approach\nfor identifying nonlinear dynamical systems from data. Specifically, we blend\nclassical tools from numerical analysis, namely the multi-step time-stepping\nschemes, with powerful nonlinear function approximators, namely deep neural\nnetworks, to distill the mechanisms that govern the evolution of a given\ndata-set. We test the effectiveness of our approach for several benchmark\nproblems involving the identification of complex, nonlinear and chaotic\ndynamics, and we demonstrate how this allows us to accurately learn the\ndynamics, forecast future states, and identify basins of attraction. In\nparticular, we study the Lorenz system, the fluid flow behind a cylinder, the\nHopf bifurcation, and the Glycoltic oscillator model as an example of\ncomplicated nonlinear dynamics typical of biological systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 03:27:07 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1801.01253", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Max Simchowitz and Kannan Ramchandran and Martin\n  J. Wainwright", "title": "Approximate Ranking from Pairwise Comparisons", "comments": "AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in machine learning is to rank a set of n items based on\npairwise comparisons. Here ranking refers to partitioning the items into sets\nof pre-specified sizes according to their scores, which includes identification\nof the top-k items as the most prominent special case. The score of a given\nitem is defined as the probability that it beats a randomly chosen other item.\nFinding an exact ranking typically requires a prohibitively large number of\ncomparisons, but in practice, approximate rankings are often adequate.\nAccordingly, we study the problem of finding approximate rankings from pairwise\ncomparisons. We analyze an active ranking algorithm that counts the number of\ncomparisons won, and decides whether to stop or which pair of items to compare\nnext, based on confidence intervals computed from the data collected in\nprevious steps. We show that this algorithm succeeds in recovering approximate\nrankings using a number of comparisons that is close to optimal up to\nlogarithmic factors. We also present numerical results, showing that in\npractice, approximation can drastically reduce the number of comparisons\nrequired to estimate a ranking.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:18:39 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Heckel", "Reinhard", ""], ["Simchowitz", "Max", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1801.01258", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Jingu Kang, and Jong Chul Ye", "title": "Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For homeland and transportation security applications, 2D X-ray explosive\ndetection system (EDS) have been widely used, but they have limitations in\nrecognizing 3D shape of the hidden objects. Among various types of 3D computed\ntomography (CT) systems to address this issue, this paper is interested in a\nstationary CT using fixed X-ray sources and detectors. However, due to the\nlimited number of projection views, analytic reconstruction algorithms produce\nsevere streaking artifacts. Inspired by recent success of deep learning\napproach for sparse view CT reconstruction, here we propose a novel image and\nsinogram domain deep learning architecture for 3D reconstruction from very\nsparse view measurement. The algorithm has been tested with the real data from\na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner\ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior\nreconstruction performance over the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:35:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Han", "Yoseob", ""], ["Kang", "Jingu", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1801.01290", "submitter": "Tuomas Haarnoja", "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement\n  Learning with a Stochastic Actor", "comments": "ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code:\n  github.com/haarnoja/sac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free deep reinforcement learning (RL) algorithms have been demonstrated\non a range of challenging decision making and control tasks. However, these\nmethods typically suffer from two major challenges: very high sample complexity\nand brittle convergence properties, which necessitate meticulous hyperparameter\ntuning. Both of these challenges severely limit the applicability of such\nmethods to complex, real-world domains. In this paper, we propose soft\nactor-critic, an off-policy actor-critic deep RL algorithm based on the maximum\nentropy reinforcement learning framework. In this framework, the actor aims to\nmaximize expected reward while also maximizing entropy. That is, to succeed at\nthe task while acting as randomly as possible. Prior deep RL methods based on\nthis framework have been formulated as Q-learning methods. By combining\noff-policy updates with a stable stochastic actor-critic formulation, our\nmethod achieves state-of-the-art performance on a range of continuous control\nbenchmark tasks, outperforming prior on-policy and off-policy methods.\nFurthermore, we demonstrate that, in contrast to other off-policy algorithms,\nour approach is very stable, achieving very similar performance across\ndifferent random seeds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:50:50 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 21:27:08 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Haarnoja", "Tuomas", ""], ["Zhou", "Aurick", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1801.01401", "submitter": "Danica J. Sutherland", "authors": "Miko{\\l}aj Bi\\'nkowski, Danica J. Sutherland, Michael Arbel, Arthur\n  Gretton", "title": "Demystifying MMD GANs", "comments": "Published at ICLR 2018: https://openreview.net/forum?id=r1lUOzWCW", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the training and performance of generative adversarial\nnetworks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.\nAs our main theoretical contribution, we clarify the situation with bias in GAN\nloss functions raised by recent work: we show that gradient estimators used in\nthe optimization process for both MMD GANs and Wasserstein GANs are unbiased,\nbut learning a discriminator based on samples leads to biased gradients for the\ngenerator parameters. We also discuss the issue of kernel choice for the MMD\ncritic, and characterize the kernel corresponding to the energy distance used\nfor the Cramer GAN critic. Being an integral probability metric, the MMD\nbenefits from training strategies recently developed for Wasserstein GANs. In\nexperiments, the MMD GAN is able to employ a smaller critic network than the\nWasserstein GAN, resulting in a simpler and faster-training algorithm with\nmatching performance. We also propose an improved measure of GAN convergence,\nthe Kernel Inception Distance, and show how to use it to dynamically adapt\nlearning rates during GAN training.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 15:25:26 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 17:58:23 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 16:56:14 GMT"}, {"version": "v4", "created": "Wed, 21 Mar 2018 17:25:27 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 05:36:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bi\u0144kowski", "Miko\u0142aj", ""], ["Sutherland", "Danica J.", ""], ["Arbel", "Michael", ""], ["Gretton", "Arthur", ""]]}, {"id": "1801.01423", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, D\\'idac Sur\\'is, Marius Miron, Alexandros Karatzoglou", "title": "Overcoming catastrophic forgetting with hard attention to the task", "comments": "Includes appendix. Accepted for ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting occurs when a neural network loses the information\nlearned in a previous task after training on subsequent tasks. This problem\nremains a hurdle for artificial intelligence systems with sequential learning\ncapabilities. In this paper, we propose a task-based hard attention mechanism\nthat preserves previous tasks' information without affecting the current task's\nlearning. A hard attention mask is learned concurrently to every task, through\nstochastic gradient descent, and previous masks are exploited to condition such\nlearning. We show that the proposed mechanism is effective for reducing\ncatastrophic forgetting, cutting current rates by 45 to 80%. We also show that\nit is robust to different hyperparameter choices, and that it offers a number\nof monitoring capabilities. The approach features the possibility to control\nboth the stability and compactness of the learned knowledge, which we believe\nmakes it also attractive for online learning or network compression\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:22:22 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 09:01:55 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 09:00:04 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Sur\u00eds", "D\u00eddac", ""], ["Miron", "Marius", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1801.01455", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Mathews Jacob", "title": "Clustering of Data with Missing Entries", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.01870", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large datasets is often complicated by the presence of\nmissing entries, mainly because most of the current machine learning algorithms\nare designed to work with full data. The main focus of this work is to\nintroduce a clustering algorithm, that will provide good clustering even in the\npresence of missing data. The proposed technique solves an $\\ell_0$ fusion\npenalty based optimization problem to recover the clusters. We theoretically\nanalyze the conditions needed for the successful recovery of the clusters. We\nalso propose an algorithm to solve a relaxation of this problem using\nsaturating non-convex fusion penalties. The method is demonstrated on simulated\nand real datasets, and is observed to perform well in the presence of large\nfractions of missing entries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 15:45:52 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1801.01467", "submitter": "Hussain Kazmi", "authors": "Hussain Kazmi, Fahad Mehmood, Stefan Lodeweyckx, Johan Driesen", "title": "Deep Reinforcement Learning based Optimal Control of Hot Water Systems", "comments": null, "journal-ref": "Energy, Volume 144, 2018", "doi": "10.1016/j.energy.2017.12.019", "report-no": null, "categories": "cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption for hot water production is a major draw in high\nefficiency buildings. Optimizing this has typically been approached from a\nthermodynamics perspective, decoupled from occupant influence. Furthermore,\noptimization usually presupposes existence of a detailed dynamics model for the\nhot water system. These assumptions lead to suboptimal energy efficiency in the\nreal world. In this paper, we present a novel reinforcement learning based\nmethodology which optimizes hot water production. The proposed methodology is\ncompletely generalizable, and does not require an offline step or human domain\nknowledge to build a model for the hot water vessel or the heating element.\nOccupant preferences too are learnt on the fly. The proposed system is applied\nto a set of 32 houses in the Netherlands where it reduces energy consumption\nfor hot water production by roughly 20% with no loss of occupant comfort.\nExtrapolating, this translates to absolute savings of roughly 200 kWh for a\nsingle household on an annual basis. This performance can be replicated to any\ndomestic hot water system and optimization objective, given that the fairly\nminimal requirements on sensor data are met. With millions of hot water systems\noperational worldwide, the proposed framework has the potential to reduce\nenergy consumption in existing and new systems on a multi Gigawatt-hour scale\nin the years to come.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 17:37:50 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kazmi", "Hussain", ""], ["Mehmood", "Fahad", ""], ["Lodeweyckx", "Stefan", ""], ["Driesen", "Johan", ""]]}, {"id": "1801.01469", "submitter": "Florian H\\\"ase", "authors": "Florian H\\\"ase, Lo\\\"ic M. Roch, Christoph Kreisbeck, Al\\'an\n  Aspuru-Guzik", "title": "PHOENICS: A universal deep Bayesian optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce PHOENICS, a probabilistic global optimization\nalgorithm combining ideas from Bayesian optimization with concepts from\nBayesian kernel density estimation. We propose an inexpensive acquisition\nfunction balancing the explorative and exploitative behavior of the algorithm.\nThis acquisition function enables intuitive sampling strategies for an\nefficient parallel search of global minima. The performance of PHOENICS is\nassessed via an exhaustive benchmark study on a set of 15 discrete,\nquasi-discrete and continuous multidimensional functions. Unlike optimization\nmethods based on Gaussian processes (GP) and random forests (RF), we show that\nPHOENICS is less sensitive to the nature of the co-domain, and outperforms GP\nand RF optimizations. We illustrate the performance of PHOENICS on the\nOregonator, a difficult case-study describing a complex chemical reaction\nnetwork. We demonstrate that only PHOENICS was able to reproduce qualitatively\nand quantitatively the target dynamic behavior of this nonlinear reaction\ndynamics. We recommend PHOENICS for rapid optimization of scalar, possibly\nnon-convex, black-box unknown objective functions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 17:40:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["H\u00e4se", "Florian", ""], ["Roch", "Lo\u00efc M.", ""], ["Kreisbeck", "Christoph", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1801.01587", "submitter": "Uri Shaham", "authors": "Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, Yuval\n  Kluger", "title": "SpectralNet: Spectral Clustering using Deep Neural Networks", "comments": "Added citations. Accepted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a leading and popular technique in unsupervised data\nanalysis. Two of its major limitations are scalability and generalization of\nthe spectral embedding (i.e., out-of-sample-extension). In this paper we\nintroduce a deep learning approach to spectral clustering that overcomes the\nabove shortcomings. Our network, which we call SpectralNet, learns a map that\nembeds input data points into the eigenspace of their associated graph\nLaplacian matrix and subsequently clusters them. We train SpectralNet using a\nprocedure that involves constrained stochastic optimization. Stochastic\noptimization allows it to scale to large datasets, while the constraints, which\nare implemented using a special-purpose output layer, allow us to keep the\nnetwork output orthogonal. Moreover, the map learned by SpectralNet naturally\ngeneralizes the spectral embedding to unseen data points. To further improve\nthe quality of the clustering, we replace the standard pairwise Gaussian\naffinities with affinities leaned from unlabeled data using a Siamese network.\nAdditional improvement can be achieved by applying the network to code\nrepresentations produced, e.g., by standard autoencoders. Our end-to-end\nlearning procedure is fully unsupervised. In addition, we apply VC dimension\ntheory to derive a lower bound on the size of SpectralNet. State-of-the-art\nclustering results are reported on the Reuters dataset. Our implementation is\npublicly available at https://github.com/kstant0725/SpectralNet .\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 23:56:36 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 14:06:31 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 21:29:15 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 18:17:10 GMT"}, {"version": "v5", "created": "Thu, 29 Mar 2018 16:20:36 GMT"}, {"version": "v6", "created": "Wed, 4 Apr 2018 12:46:19 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Shaham", "Uri", ""], ["Stanton", "Kelly", ""], ["Li", "Henry", ""], ["Nadler", "Boaz", ""], ["Basri", "Ronen", ""], ["Kluger", "Yuval", ""]]}, {"id": "1801.01649", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn, Michael Chertkov, Jinwoo Shin and Adrian Weller", "title": "Gauged Mini-Bucket Elimination for Approximate Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the partition function $Z$ of a discrete graphical model is a\nfundamental inference challenge. Since this is computationally intractable,\nvariational approximations are often used in practice. Recently, so-called\ngauge transformations were used to improve variational lower bounds on $Z$. In\nthis paper, we propose a new gauge-variational approach, termed WMBE-G, which\ncombines gauge transformations with the weighted mini-bucket elimination (WMBE)\nmethod. WMBE-G can provide both upper and lower bounds on $Z$, and is easier to\noptimize than the prior gauge-variational algorithm. We show that WMBE-G\nstrictly improves the earlier WMBE approximation for symmetric models including\nIsing models with no magnetic field. Our experimental results demonstrate the\neffectiveness of WMBE-G even for generic, nonsymmetric models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 07:26:39 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 05:54:22 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""], ["Weller", "Adrian", ""]]}, {"id": "1801.01708", "submitter": "Olivier Gouvert", "authors": "Olivier Gouvert, Thomas Oberlin and C\\'edric F\\'evotte", "title": "Negative Binomial Matrix Factorization for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce negative binomial matrix factorization (NBMF), a matrix\nfactorization technique specially designed for analyzing over-dispersed count\ndata. It can be viewed as an extension of Poisson matrix factorization (PF)\nperturbed by a multiplicative term which models exposure. This term brings a\ndegree of freedom for controlling the dispersion, making NBMF more robust to\noutliers. We show that NBMF allows to skip traditional pre-processing stages,\nsuch as binarization, which lead to loss of information. Two estimation\napproaches are presented: maximum likelihood and variational Bayes inference.\nWe test our model with a recommendation task and show its ability to predict\nuser tastes with better precision than PF.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 11:00:46 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Gouvert", "Olivier", ""], ["Oberlin", "Thomas", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1801.01743", "submitter": "Alberto Fachechi", "authors": "Adriano Barra, Matteo Beccaria and Alberto Fachechi", "title": "A relativistic extension of Hopfield neural networks via the mechanical\n  analogy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modification of the cost function of the Hopfield model whose\nsalient features shine in its Taylor expansion and result in more than pairwise\ninteractions with alternate signs, suggesting a unified framework for handling\nboth with deep learning and network pruning. In our analysis, we heavily rely\non the Hamilton-Jacobi correspondence relating the statistical model with a\nmechanical system. In this picture, our model is nothing but the relativistic\nextension of the original Hopfield model (whose cost function is a quadratic\nform in the Mattis magnetization which mimics the non-relativistic Hamiltonian\nfor a free particle). We focus on the low-storage regime and solve the model\nanalytically by taking advantage of the mechanical analogy, thus obtaining a\ncomplete characterization of the free energy and the associated\nself-consistency equations in the thermodynamic limit. On the numerical side,\nwe test the performances of our proposal with MC simulations, showing that the\nstability of spurious states (limiting the capabilities of the standard Hebbian\nconstruction) is sensibly reduced due to presence of unlearning contributions\nin this extended framework.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 12:57:18 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Barra", "Adriano", ""], ["Beccaria", "Matteo", ""], ["Fachechi", "Alberto", ""]]}, {"id": "1801.01750", "submitter": "Melody Guan", "authors": "Melody Y. Guan, Heinrich Jiang", "title": "Nonparametric Stochastic Contextual Bandits", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the $K$-armed bandit problem where the reward for each arm is a\nnoisy realization based on an observed context under mild nonparametric\nassumptions. We attain tight results for top-arm identification and a sublinear\nregret of $\\widetilde{O}\\Big(T^{\\frac{1+D}{2+D}}\\Big)$, where $D$ is the\ncontext dimension, for a modified UCB algorithm that is simple to implement\n($k$NN-UCB). We then give global intrinsic dimension dependent and ambient\ndimension independent regret bounds. We also discuss recovering topological\nstructures within the context space based on expected bandit performance and\nprovide an extension to infinite-armed contextual bandits. Finally, we\nexperimentally show the improvement of our algorithm over existing multi-armed\nbandit approaches for both simulated tasks and MNIST image classification.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 13:27:42 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Guan", "Melody Y.", ""], ["Jiang", "Heinrich", ""]]}, {"id": "1801.01799", "submitter": "Louis Filstroff", "authors": "Louis Filstroff, Alberto Lumbreras, C\\'edric F\\'evotte", "title": "Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization", "comments": "Accepted for publication at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel understandings of the Gamma-Poisson (GaP) model, a\nprobabilistic matrix factorization model for count data. We show that GaP can\nbe rewritten free of the score/activation matrix. This gives us new insights\nabout the estimation of the topic/dictionary matrix by maximum marginal\nlikelihood estimation. In particular, this explains the robustness of this\nestimator to over-specified values of the factorization rank, especially its\nability to automatically prune irrelevant dictionary columns, as empirically\nobserved in previous work. The marginalization of the activation matrix leads\nin turn to a new Monte Carlo Expectation-Maximization algorithm with favorable\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 15:50:39 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 11:59:21 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Filstroff", "Louis", ""], ["Lumbreras", "Alberto", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1801.01899", "submitter": "Jun Li", "authors": "Hongfu Liu, Jun Li, Yue Wu and Yun Fu", "title": "Clustering with Outlier Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis and outlier detection are strongly coupled tasks in data\nmining area. Cluster structure can be easily destroyed by few outliers; on the\ncontrary, outliers are defined by the concept of cluster, which are recognized\nas the points belonging to none of the clusters. Unfortunately, most existing\nstudies do not notice the coupled relationship between these two task and\nhandle them separately. In light of this, we consider the joint cluster\nanalysis and outlier detection problem, and propose the Clustering with Outlier\nRemoval (COR) algorithm. Generally speaking, the original space is transformed\ninto the binary space via generating basic partitions in order to define\nclusters. Then an objective function based Holoentropy is designed to enhance\nthe compactness of each cluster with a few outliers removed. With further\nanalyses on the objective function, only partial of the problem can be handled\nby K-means optimization. To provide an integrated solution, an auxiliary binary\nmatrix is nontrivally introduced so that COR completely and efficiently solves\nthe challenging problem via a unified K-means-- with theoretical supports.\nExtensive experimental results on numerous data sets in various domains\ndemonstrate the effectiveness and efficiency of COR significantly over\nstate-of-the-art methods in terms of cluster validity and outlier detection.\nSome key factors in COR are further analyzed for practical use. Finally, an\napplication on flight trajectory is provided to demonstrate the effectiveness\nof COR in the real-world scenario.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 19:15:17 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 22:44:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Liu", "Hongfu", ""], ["Li", "Jun", ""], ["Wu", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "1801.01952", "submitter": "Lior Deutsch", "authors": "Lior Deutsch", "title": "Generating Neural Networks with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypernetworks are neural networks that generate weights for another neural\nnetwork. We formulate the hypernetwork training objective as a compromise\nbetween accuracy and diversity, where the diversity takes into account trivial\nsymmetry transformations of the target network. We explain how this simple\nformulation generalizes variational inference. We use multi-layered perceptrons\nto form the mapping from the low dimensional input random vector to the high\ndimensional weight space, and demonstrate how to reduce the number of\nparameters in this mapping by parameter sharing. We perform experiments and\nshow that the generated weights are diverse and lie on a non-trivial manifold.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 01:27:16 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 21:58:44 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 03:51:27 GMT"}, {"version": "v4", "created": "Fri, 6 Apr 2018 19:46:08 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Deutsch", "Lior", ""]]}, {"id": "1801.01953", "submitter": "Martin Gubri", "authors": "Martin Gubri", "title": "Adversarial Perturbation Intensity Achieving Chosen Intra-Technique\n  Transferability Level for Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine Learning models have been shown to be vulnerable to adversarial\nexamples, ie. the manipulation of data by a attacker to defeat a defender's\nclassifier at test time. We present a novel probabilistic definition of\nadversarial examples in perfect or limited knowledge setting using prior\nprobability distributions on the defender's classifier. Using the asymptotic\nproperties of the logistic regression, we derive a closed-form expression of\nthe intensity of any adversarial perturbation, in order to achieve a given\nexpected misclassification rate. This technique is relevant in a threat model\nof known model specifications and unknown training data. To our knowledge, this\nis the first method that allows an attacker to directly choose the probability\nof attack success. We evaluate our approach on two real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 01:37:30 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Gubri", "Martin", ""]]}, {"id": "1801.01961", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis and Xun Huan and Cosmin Safta and Khachik Sargsyan\n  and Guilhem Lacaze and Joseph C. Oefelein and Habib N. Najm and Roger G.\n  Ghanem", "title": "Compressive sensing adaptation for polynomial chaos expansions", "comments": "Submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2018.12.010", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basis adaptation in Homogeneous Chaos spaces rely on a suitable rotation of\nthe underlying Gaussian germ. Several rotations have been proposed in the\nliterature resulting in adaptations with different convergence properties. In\nthis paper we present a new adaptation mechanism that builds on compressive\nsensing algorithms, resulting in a reduced polynomial chaos approximation with\noptimal sparsity. The developed adaptation algorithm consists of a two-step\noptimization procedure that computes the optimal coefficients and the input\nprojection matrix of a low dimensional chaos expansion with respect to an\noptimally rotated basis. We demonstrate the attractive features of our\nalgorithm through several numerical examples including the application on\nLarge-Eddy Simulation (LES) calculations of turbulent combustion in a HIFiRE\nscramjet engine.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 03:52:52 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:14:08 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Huan", "Xun", ""], ["Safta", "Cosmin", ""], ["Sargsyan", "Khachik", ""], ["Lacaze", "Guilhem", ""], ["Oefelein", "Joseph C.", ""], ["Najm", "Habib N.", ""], ["Ghanem", "Roger G.", ""]]}, {"id": "1801.01973", "submitter": "Shane Barratt", "authors": "Shane Barratt, Rishi Sharma", "title": "A Note on the Inception Score", "comments": "Proc. ICML 2018 Workshop on Theoretical Foundations and Applications\n  of Deep Generative Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are powerful tools that have produced impressive\nresults in recent years. These advances have been for the most part empirically\ndriven, making it essential that we use high quality evaluation metrics. In\nthis paper, we provide new insights into the Inception Score, a recently\nproposed and widely used evaluation metric for generative models, and\ndemonstrate that it fails to provide useful guidance when comparing models. We\ndiscuss both suboptimalities of the metric itself and issues with its\napplication. Finally, we call for researchers to be more systematic and careful\nwhen evaluating and comparing generative models, as the advancement of the\nfield depends upon it.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 05:44:29 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 14:54:33 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Barratt", "Shane", ""], ["Sharma", "Rishi", ""]]}, {"id": "1801.02013", "submitter": "Joan Bruna", "authors": "Joan Bruna, Stephane Mallat", "title": "Multiscale Sparse Microcanonical Models", "comments": "Accepted to MSL (Mathematical Statistics and Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximations of non-Gaussian stationary processes having long\nrange correlations with microcanonical models. These models are conditioned by\nthe empirical value of an energy vector, evaluated on a single realization.\nAsymptotic properties of maximum entropy microcanonical and macrocanonical\nprocesses and their convergence to Gibbs measures are reviewed. We show that\nthe Jacobian of the energy vector controls the entropy rate of microcanonical\nprocesses.\n  Sampling maximum entropy processes through MCMC algorithms require too many\noperations when the number of constraints is large. We define microcanonical\ngradient descent processes by transporting a maximum entropy measure with a\ngradient descent algorithm which enforces the energy conditions. Convergence\nand symmetries are analyzed. Approximations of non-Gaussian processes with long\nrange interactions are defined with multiscale energy vectors computed with\nwavelet and scattering transforms. Sparsity properties are captured with $\\bf\nl^1$ norms. Approximations of Gaussian, Ising and point processes are studied,\nas well as image and audio texture synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 13:18:46 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 20:02:11 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 00:44:56 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "Stephane", ""]]}, {"id": "1801.02124", "submitter": "Xingyu Wang", "authors": "Xingyu Wang, Diego Klabjan", "title": "Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal\n  Demonstrations", "comments": "31 pages, to be presented at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inverse reinforcement learning in\nzero-sum stochastic games when expert demonstrations are known to be not\noptimal. Compared to previous works that decouple agents in the game by\nassuming optimality in expert strategies, we introduce a new objective function\nthat directly pits experts against Nash Equilibrium strategies, and we design\nan algorithm to solve for the reward function in the context of inverse\nreinforcement learning with deep neural networks as model approximations. In\nour setting the model and algorithm do not decouple by agent. In order to find\nNash Equilibrium in large-scale games, we also propose an adversarial training\nalgorithm for zero-sum stochastic games, and show the theoretical appeal of\nnon-existence of local optima in its objective function. In our numerical\nexperiments, we demonstrate that our Nash Equilibrium and inverse reinforcement\nlearning algorithms address games that are not amenable to previous approaches\nusing tabular representations. Moreover, with sub-optimal expert demonstrations\nour algorithms recover both reward functions and strategies with good quality.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 03:53:30 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 23:01:54 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Wang", "Xingyu", ""], ["Klabjan", "Diego", ""]]}, {"id": "1801.02125", "submitter": "Tsuyoshi Kato", "authors": "Yuya Onuma, Rachelle Rivero, Tsuyoshi Kato", "title": "Threshold Auto-Tuning Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been reported repeatedly that discriminative learning of distance\nmetric boosts the pattern recognition performance. A weak point of ITML-based\nmethods is that the distance threshold for similarity/dissimilarity constraints\nmust be determined manually and it is sensitive to generalization performance,\nalthough the ITML-based methods enjoy an advantage that the Bregman projection\nframework can be applied for optimization of distance metric. In this paper, we\npresent a new formulation of metric learning algorithm in which the distance\nthreshold is optimized together. Since the optimization is still in the Bregman\nprojection framework, the Dykstra algorithm can be applied for optimization. A\nnonlinear equation has to be solved to project the solution onto a half-space\nin each iteration. Na\\\"{i}ve method takes $O(LMn^{3})$ computational time to\nsolve the nonlinear equation. In this study, an efficient technique that can\nsolve the nonlinear equation in $O(Mn^{3})$ has been discovered. We have proved\nthat the root exists and is unique. We empirically show that the accuracy of\npattern recognition for the proposed metric learning algorithm is comparable to\nthe existing metric learning methods, yet the distance threshold is\nautomatically tuned for the proposed metric learning algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 04:01:24 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 03:20:43 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Onuma", "Yuya", ""], ["Rivero", "Rachelle", ""], ["Kato", "Tsuyoshi", ""]]}, {"id": "1801.02149", "submitter": "Amirreza Mahdavi-Shahri", "authors": "Amirreza Mahdavi-Shahri, Mahboobeh Houshmand, Mahdi Yaghoobi, Mehrdad\n  Jalali", "title": "Applying an Ensemble Learning Method for Improving Multi-label\n  Classification Performance", "comments": null, "journal-ref": null, "doi": "10.1109/ICSPIS.2016.7869900", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-label classification problem has become a\ncontroversial issue. In this kind of classification, each sample is associated\nwith a set of class labels. Ensemble approaches are supervised learning\nalgorithms in which an operator takes a number of learning algorithms, namely\nbase-level algorithms and combines their outcomes to make an estimation. The\nsimplest form of ensemble learning is to train the base-level algorithms on\nrandom subsets of data and then let them vote for the most popular\nclassifications or average the predictions of the base-level algorithms. In\nthis study, an ensemble learning method is proposed for improving multi-label\nclassification evaluation criteria. We have compared our method with well-known\nbase-level algorithms on some data sets. Experiment results show the proposed\napproach outperforms the base well-known classifiers for the multi-label\nclassification problem.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 06:43:46 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mahdavi-Shahri", "Amirreza", ""], ["Houshmand", "Mahboobeh", ""], ["Yaghoobi", "Mahdi", ""], ["Jalali", "Mehrdad", ""]]}, {"id": "1801.02171", "submitter": "Alexandre Attia", "authors": "Alexandre Attia, Sharone Dayan", "title": "Detection and segmentation of the Left Ventricle in Cardiac MRI using\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous\ntask that can vary depending on the patient, the Magnetic Resonance Images\n(MRI) cuts and the experts. Still today, we consider manual delineation done by\nexperts as being the ground truth for cardiac diagnosticians. Thus, we are\nreviewing the paper - written by Avendi and al. - who presents a combined\napproach with Convolutional Neural Networks, Stacked Auto-Encoders and\nDeformable Models, to try and automate the segmentation while performing more\naccurately. Furthermore, we have implemented parts of the paper (around three\nquarts) and experimented both the original method and slightly modified\nversions when changing the architecture and the parameters.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 11:22:12 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Attia", "Alexandre", ""], ["Dayan", "Sharone", ""]]}, {"id": "1801.02227", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda and Taiji Suzuki", "title": "Gradient Layer: Enhancing the Convergence of Adversarial Training for\n  Generative Models", "comments": "14 pages, 4 figures, AISTATS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique that boosts the convergence of training generative\nadversarial networks. Generally, the rate of training deep models reduces\nseverely after multiple iterations. A key reason for this phenomenon is that a\ndeep network is expressed using a highly non-convex finite-dimensional model,\nand thus the parameter gets stuck in a local optimum. Because of this, methods\noften suffer not only from degeneration of the convergence speed but also from\nlimitations in the representational power of the trained network. To overcome\nthis issue, we propose an additional layer called the gradient layer to seek a\ndescent direction in an infinite-dimensional space. Because the layer is\nconstructed in the infinite-dimensional space, we are not restricted by the\nspecific model structure of finite-dimensional models. As a result, we can get\nout of the local optima in finite-dimensional models and move towards the\nglobal optimal function more directly. In this paper, this phenomenon is\nexplained from the functional gradient method perspective of the gradient\nlayer. Interestingly, the optimization procedure using the gradient layer\nnaturally constructs the deep structure of the network. Moreover, we\ndemonstrate that this procedure can be regarded as a discretization method of\nthe gradient flow that naturally reduces the objective function. Finally, the\nmethod is tested using several numerical experiments, which show its fast\nconvergence.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 18:44:10 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 10:48:55 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Nitanda", "Atsushi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1801.02251", "submitter": "Siwei Feng", "authors": "Siwei Feng and Marco F.Duarte", "title": "Graph Autoencoder-Based Unsupervised Feature Selection with Broad and\n  Local Data Structure Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a dimensionality reduction technique that selects a\nsubset of representative features from high dimensional data by eliminating\nirrelevant and redundant features. Recently, feature selection combined with\nsparse learning has attracted significant attention due to its outstanding\nperformance compared with traditional feature selection methods that ignores\ncorrelation between features. These works first map data onto a low-dimensional\nsubspace and then select features by posing a sparsity constraint on the\ntransformation matrix. However, they are restricted by design to linear data\ntransformation, a potential drawback given that the underlying correlation\nstructures of data are often non-linear. To leverage a more sophisticated\nembedding, we propose an autoencoder-based unsupervised feature selection\napproach that leverages a single-layer autoencoder for a joint framework of\nfeature selection and manifold learning. More specifically, we enforce column\nsparsity on the weight matrix connecting the input layer and the hidden layer,\nas in previous work. Additionally, we include spectral graph analysis on the\nprojected data into the learning process to achieve local data geometry\npreservation from the original data space to the low-dimensional feature space.\nExtensive experiments are conducted on image, audio, text, and biological data.\nThe promising experimental results validate the superiority of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 21:14:01 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 03:14:38 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Feng", "Siwei", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1801.02257", "submitter": "Joani Mitro", "authors": "John Mitro and Derek Bridge and Steven Prestwich", "title": "Denoising Dictionary Learning Against Adversarial Perturbations", "comments": "8 pages, 10 figures, aaai18 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose denoising dictionary learning (DDL), a simple yet effective\ntechnique as a protection measure against adversarial perturbations. We\nexamined denoising dictionary learning on MNIST and CIFAR10 perturbed under two\ndifferent perturbation techniques, fast gradient sign (FGSM) and jacobian\nsaliency maps (JSMA). We evaluated it against five different deep neural\nnetworks (DNN) representing the building blocks of most recent architectures\nindicating a successive progression of model complexity of each other. We show\nthat each model tends to capture different representations based on their\narchitecture. For each model we recorded its accuracy both on the perturbed\ntest data previously misclassified with high confidence and on the denoised one\nafter the reconstruction using dictionary learning. The reconstruction quality\nof each data point is assessed by means of PSNR (Peak Signal to Noise Ratio)\nand Structure Similarity Index (SSI). We show that after applying (DDL) the\nreconstruction of the original data point from a noisy\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 22:03:20 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mitro", "John", ""], ["Bridge", "Derek", ""], ["Prestwich", "Steven", ""]]}, {"id": "1801.02261", "submitter": "Avi Ben-Cohen", "authors": "Avi Ben-Cohen, Eyal Klang, Michal Marianne Amitai, Jacob Goldberger,\n  Hayit Greenspan", "title": "Anatomical Data Augmentation For CNN based Pixel-wise Classification", "comments": "To be presented at IEEE ISBI 2018", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363762", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method for anatomical data augmentation that is\nbased on using slices of computed tomography (CT) examinations that are\nadjacent to labeled slices as another resource of labeled data for training the\nnetwork. The extended labeled data is used to train a U-net network for a\npixel-wise classification into different hepatic lesions and normal liver\ntissues. Our dataset contains CT examinations from 140 patients with 333 CT\nimages annotated by an expert radiologist. We tested our approach and compared\nit to the conventional training process. Results indicate superiority of our\nmethod. Using the anatomical data augmentation we achieved an improvement of 3%\nin the success rate, 5% in the classification accuracy, and 4% in Dice.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 23:00:02 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Klang", "Eyal", ""], ["Amitai", "Michal Marianne", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1801.02294", "submitter": "Han Zhu", "authors": "Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, Kun Gai", "title": "Learning Tree-based Deep Model for Recommender Systems", "comments": "Accepted by KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219826", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based methods for recommender systems have been studied extensively in\nrecent years. In systems with large corpus, however, the calculation cost for\nthe learnt model to predict all user-item preferences is tremendous, which\nmakes full corpus retrieval extremely difficult. To overcome the calculation\nbarriers, models such as matrix factorization resort to inner product form\n(i.e., model user-item preference as the inner product of user, item latent\nfactors) and indexes to facilitate efficient approximate k-nearest neighbor\nsearches. However, it still remains challenging to incorporate more expressive\ninteraction forms between user and item features, e.g., interactions through\ndeep neural networks, because of the calculation cost.\n  In this paper, we focus on the problem of introducing arbitrary advanced\nmodels to recommender systems with large corpus. We propose a novel tree-based\nmethod which can provide logarithmic complexity w.r.t. corpus size even with\nmore expressive models such as deep neural networks. Our main idea is to\npredict user interests from coarse to fine by traversing tree nodes in a\ntop-down fashion and making decisions for each user-node pair. We also show\nthat the tree structure can be jointly learnt towards better compatibility with\nusers' interest distribution and hence facilitate both training and prediction.\nExperimental evaluations with two large-scale real-world datasets show that the\nproposed method significantly outperforms traditional methods. Online A/B test\nresults in Taobao display advertising platform also demonstrate the\neffectiveness of the proposed method in production environments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 02:52:20 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 11:13:21 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 07:40:07 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 04:37:55 GMT"}, {"version": "v5", "created": "Fri, 21 Dec 2018 03:15:54 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zhu", "Han", ""], ["Li", "Xiang", ""], ["Zhang", "Pengye", ""], ["Li", "Guozheng", ""], ["He", "Jie", ""], ["Li", "Han", ""], ["Gai", "Kun", ""]]}, {"id": "1801.02309", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Yuansi Chen, Martin J. Wainwright, Bin Yu", "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast", "comments": "42 pages, 11 Figures; The first two authors contributed equally; A\n  subset of results were presented in an extended abstract at COLT 2018", "journal-ref": "Journal of Machine Learning Research, 2019", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of\nthe Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by\nsimulating a Markov chain obtained from the discretization of an appropriate\nLangevin diffusion, combined with an accept-reject step. Relative to known\nguarantees for the unadjusted Langevin algorithm (ULA), our bounds show that\nthe use of an accept-reject step in MALA leads to an exponentially improved\ndependence on the error-tolerance. Concretely, in order to obtain samples with\nTV error at most $\\delta$ for a density with condition number $\\kappa$, we show\nthat MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as\ncompared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established\nin past work on ULA. We also demonstrate the gains of MALA over ULA for weakly\nlog-concave densities. Furthermore, we derive mixing time bounds for the\nMetropolized random walk (MRW) and obtain $\\mathcal{O}(\\kappa)$ mixing time\nslower than MALA. We provide numerical examples that support our theoretical\nfindings, and demonstrate the benefits of Metropolis-Hastings adjustment for\nLangevin-type sampling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 05:39:14 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 04:51:16 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 21:20:49 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 01:26:44 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Chen", "Yuansi", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1801.02328", "submitter": "Yu Cheng", "authors": "Yu Cheng, Angus Wong, Kevin Hung, Zhizhong Li, Weitong Li, Jun Zhang", "title": "Deep Nearest Class Mean Model for Incremental Odor Classification", "comments": "17 pages, 6 figures", "journal-ref": "IEEE Transactions on Instrumentation and Measurement ( Volume: 68\n  , Issue: 4 , April 2019 ) 952 - 962", "doi": "10.1109/TIM.2018.2863438", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, more machine learning algorithms have been applied to odor\nclassification. These odor classification algorithms usually assume that the\ntraining datasets are static. However, for some odor recognition tasks, new\nodor classes continually emerge. That is, the odor datasets are dynamically\ngrowing while both training samples and number of classes are increasing over\ntime. Motivated by this concern, this paper proposes a Deep Nearest Class Mean\n(DNCM) model based on the deep learning framework and nearest class mean\nmethod. The proposed model not only leverages deep neural network to extract\ndeep features, but is also able to dynamically integrate new classes over time.\nIn our experiments, the DNCM model was initially trained with 10 classes, then\n25 new classes are integrated. Experiment results demonstrate that the proposed\nmodel is very efficient for incremental odor classification, especially for new\nclasses with only a small number of training examples.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 07:46:31 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 01:19:47 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Cheng", "Yu", ""], ["Wong", "Angus", ""], ["Hung", "Kevin", ""], ["Li", "Zhizhong", ""], ["Li", "Weitong", ""], ["Zhang", "Jun", ""]]}, {"id": "1801.02471", "submitter": "Meysam Golmohammadi", "authors": "Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Eva Von Weltin,\n  Christopher Campbell, Iyad Obeid and Joseph Picone", "title": "Gated Recurrent Networks for Seizure Detection", "comments": "Published in Dec 2017 publication In IEEE Signal Processing in\n  Medicine and Biology Symposium. Philadelphia, Pennsylvania, USA. arXiv admin\n  note: text overlap with arXiv:1712.09776", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) with sophisticated units that implement a\ngating mechanism have emerged as powerful technique for modeling sequential\nsignals such as speech or electroencephalography (EEG). The latter is the focus\non this paper. A significant big data resource, known as the TUH EEG Corpus\n(TUEEG), has recently become available for EEG research, creating a unique\nopportunity to evaluate these recurrent units on the task of seizure detection.\nIn this study, we compare two types of recurrent units: long short-term memory\nunits (LSTM) and gated recurrent units (GRU). These are evaluated using a state\nof the art hybrid architecture that integrates Convolutional Neural Networks\n(CNNs) with RNNs. We also investigate a variety of initialization methods and\nshow that initialization is crucial since poorly initialized networks cannot be\ntrained. Furthermore, we explore regularization of these convolutional gated\nrecurrent networks to address the problem of overfitting. Our experiments\nrevealed that convolutional LSTM networks can achieve significantly better\nperformance than convolutional GRU networks. The convolutional LSTM\narchitecture with proper initialization and regularization delivers 30%\nsensitivity at 6 false alarms per 24 hours.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:54:05 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Golmohammadi", "Meysam", ""], ["Ziyabari", "Saeedeh", ""], ["Shah", "Vinit", ""], ["Von Weltin", "Eva", ""], ["Campbell", "Christopher", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02472", "submitter": "Meysam Golmohammadi", "authors": "Vinit Shah, Meysam Golmohammadi, Saeedeh Ziyabari, Eva Von Weltin,\n  Iyad Obeid and Joseph Picone", "title": "Optimizing Channel Selection for Seizure Detection", "comments": "Published in Dec 2017 publication IEEE Signal Processing in Medicine\n  and Biology Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of electroencephalogram (EEG) signals can be complicated by\nobfuscating artifacts. Artifact detection plays an important role in the\nobservation and analysis of EEG signals. Spatial information contained in the\nplacement of the electrodes can be exploited to accurately detect artifacts.\nHowever, when fewer electrodes are used, less spatial information is available,\nmaking it harder to detect artifacts. In this study, we investigate the\nperformance of a deep learning algorithm, CNN-LSTM, on several channel\nconfigurations. Each configuration was designed to minimize the amount of\nspatial information lost compared to a standard 22-channel EEG. Systems using a\nreduced number of channels ranging from 8 to 20 achieved sensitivities between\n33% and 37% with false alarms in the range of [38, 50] per 24 hours. False\nalarms increased dramatically (e.g., over 300 per 24 hours) when the number of\nchannels was further reduced. Baseline performance of a system that used all 22\nchannels was 39% sensitivity with 23 false alarms. Since the 22-channel system\nwas the only system that included referential channels, the rapid increase in\nthe false alarm rate as the number of channels was reduced underscores the\nimportance of retaining referential channels for artifact reduction. This\ncautionary result is important because one of the biggest differences between\nvarious types of EEGs administered is the type of referential channel used.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:59:32 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Shah", "Vinit", ""], ["Golmohammadi", "Meysam", ""], ["Ziyabari", "Saeedeh", ""], ["Von Weltin", "Eva", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02474", "submitter": "Meysam Golmohammadi", "authors": "Silvia Lopez, Aaron Gross, Scott Yang, Meysam Golmohammadi, Iyad Obeid\n  and Joseph Picone", "title": "An Analysis of Two Common Reference Points for EEGs", "comments": "Published In IEEE Signal Processing in Medicine and Biology\n  Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": "S. Lopez, A. Gross, S. Yang, M. Golmohammadi, I. Obeid and J.\n  Picone, \"An analysis of two common reference points for EEGS,\" 2016 IEEE\n  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,\n  2016, pp. 1-5", "doi": "10.1109/SPMB.2016.7846854", "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical electroencephalographic (EEG) data varies significantly depending on\na number of operational conditions (e.g., the type and placement of electrodes,\nthe type of electrical grounding used). This investigation explores the\nstatistical differences present in two different referential montages: Linked\nEar (LE) and Averaged Reference (AR). Each of these accounts for approximately\n45% of the data in the TUH EEG Corpus. In this study, we explore the impact\nthis variability has on machine learning performance. We compare the\nstatistical properties of features generated using these two montages, and\nexplore the impact of performance on our standard Hidden Markov Model (HMM)\nbased classification system. We show that a system trained on LE data\nsignificantly outperforms one trained only on AR data (77.2% vs. 61.4%). We\nalso demonstrate that performance of a system trained on both data sets is\nsomewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data\nsuggests that mean, variance and channel normalization should be considered.\nHowever, cepstral mean subtraction failed to produce an improvement in\nperformance, suggesting that the impact of these statistical differences is\nsubtler.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 01:37:45 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Lopez", "Silvia", ""], ["Gross", "Aaron", ""], ["Yang", "Scott", ""], ["Golmohammadi", "Meysam", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02476", "submitter": "Meysam Golmohammadi", "authors": "Scott Yang, Silvia Lopez, Meysam Golmohammadi, Iyad Obeid and Joseph\n  Picone", "title": "Semi-automated Annotation of Signal Events in Clinical EEG Data", "comments": "Published in IEEE Signal Processing in Medicine and Biology\n  Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": "S. Yang, S. Lopez, M. Golmohammadi, I. Obeid and J. Picone,\n  \"Semi-automated annotation of signal events in clinical EEG data,\" 2016 IEEE\n  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,\n  2016, pp. 1-5", "doi": "10.1109/SPMB.2016.7846855", "report-no": null, "categories": "eess.SP cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be effective, state of the art machine learning technology needs large\namounts of annotated data. There are numerous compelling applications in\nhealthcare that can benefit from high performance automated decision support\nsystems provided by deep learning technology, but they lack the comprehensive\ndata resources required to apply sophisticated machine learning models.\nFurther, for economic reasons, it is very difficult to justify the creation of\nlarge annotated corpora for these applications. Hence, automated annotation\ntechniques become increasingly important. In this study, we investigated the\neffectiveness of using an active learning algorithm to automatically annotate a\nlarge EEG corpus. The algorithm is designed to annotate six types of EEG\nevents. Two model training schemes, namely threshold-based and volume-based,\nare evaluated. In the threshold-based scheme the threshold of confidence scores\nis optimized in the initial training iteration, whereas for the volume-based\nscheme only a certain amount of data is preserved after each iteration.\nRecognition performance is improved 2% absolute and the system is capable of\nautomatically annotating previously unlabeled data. Given that the\ninterpretation of clinical EEG data is an exceedingly difficult task, this\nstudy provides some evidence that the proposed method is a viable alternative\nto expensive manual annotation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 03:47:20 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yang", "Scott", ""], ["Lopez", "Silvia", ""], ["Golmohammadi", "Meysam", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02477", "submitter": "Meysam Golmohammadi", "authors": "Amir Harati, Meysam Golmohammadi, Silvia Lopez, Iyad Obeid and Joseph\n  Picone", "title": "Improved EEG Event Classification Using Differential Energy", "comments": "Published in IEEE Signal Processing in Medicine and Biology\n  Symposium. Philadelphia, Pennsylvania, USA", "journal-ref": "A. Harati, M. Golmohammadi, S. Lopez, I. Obeid and J. Picone,\n  \"Improved EEG event classification using differential energy,\" 2015 IEEE\n  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,\n  2015, pp. 1-4", "doi": "10.1109/SPMB.2015.7405421", "report-no": null, "categories": "eess.SP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction for automatic classification of EEG signals typically\nrelies on time frequency representations of the signal. Techniques such as\ncepstral-based filter banks or wavelets are popular analysis techniques in many\nsignal processing applications including EEG classification. In this paper, we\npresent a comparison of a variety of approaches to estimating and\npostprocessing features. To further aid in discrimination of periodic signals\nfrom aperiodic signals, we add a differential energy term. We evaluate our\napproaches on the TUH EEG Corpus, which is the largest publicly available EEG\ncorpus and an exceedingly challenging task due to the clinical nature of the\ndata. We demonstrate that a variant of a standard filter bank-based approach,\ncoupled with first and second derivatives, provides a substantial reduction in\nthe overall error rate. The combination of differential energy and derivatives\nproduces a 24% absolute reduction in the error rate and improves our ability to\ndiscriminate between signal events and background noise. This relatively simple\napproach proves to be comparable to other popular feature extraction approaches\nsuch as wavelets, but is much more computationally efficient.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 03:55:55 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Harati", "Amir", ""], ["Golmohammadi", "Meysam", ""], ["Lopez", "Silvia", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02567", "submitter": "Enrique Romero", "authors": "Enrique Romero Merino and Ferran Mazzanti Castrillejo and Jordi\n  Delgado Pin and David Buchaca Prats", "title": "Weighted Contrastive Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms for energy based Boltzmann architectures that rely on\ngradient descent are in general computationally prohibitive, typically due to\nthe exponential number of terms involved in computing the partition function.\nIn this way one has to resort to approximation schemes for the evaluation of\nthe gradient. This is the case of Restricted Boltzmann Machines (RBM) and its\nlearning algorithm Contrastive Divergence (CD). It is well-known that CD has a\nnumber of shortcomings, and its approximation to the gradient has several\ndrawbacks. Overcoming these defects has been the basis of much research and new\nalgorithms have been devised, such as persistent CD. In this manuscript we\npropose a new algorithm that we call Weighted CD (WCD), built from small\nmodifications of the negative phase in standard CD. However small these\nmodifications may be, experimental work reported in this paper suggest that WCD\nprovides a significant improvement over standard CD and persistent CD at a\nsmall additional computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 17:20:17 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 16:54:11 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Merino", "Enrique Romero", ""], ["Castrillejo", "Ferran Mazzanti", ""], ["Pin", "Jordi Delgado", ""], ["Prats", "David Buchaca", ""]]}, {"id": "1801.02610", "submitter": "Chaowei Xiao", "authors": "Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song", "title": "Generating Adversarial Examples with Adversarial Networks", "comments": "Accepted to IJCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples resulting from adding small-magnitude perturbations to inputs. Such\nadversarial examples can mislead DNNs to produce adversary-selected results.\nDifferent attack strategies have been proposed to generate adversarial\nexamples, but how to produce them with high perceptual quality and more\nefficiently requires more research efforts. In this paper, we propose AdvGAN to\ngenerate adversarial examples with generative adversarial networks (GANs),\nwhich can learn and approximate the distribution of original instances. For\nAdvGAN, once the generator is trained, it can generate adversarial\nperturbations efficiently for any instance, so as to potentially accelerate\nadversarial training as defenses. We apply AdvGAN in both semi-whitebox and\nblack-box attack settings. In semi-whitebox attacks, there is no need to access\nthe original target model after the generator is trained, in contrast to\ntraditional white-box attacks. In black-box attacks, we dynamically train a\ndistilled model for the black-box model and optimize the generator accordingly.\nAdversarial examples generated by AdvGAN on different target models have high\nattack success rate under state-of-the-art defenses compared to other attacks.\nOur attack has placed the first with 92.76% accuracy on a public MNIST\nblack-box attack challenge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:50:13 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 19:04:37 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 22:08:13 GMT"}, {"version": "v4", "created": "Tue, 12 Feb 2019 06:19:35 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 15:53:22 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Xiao", "Chaowei", ""], ["Li", "Bo", ""], ["Zhu", "Jun-Yan", ""], ["He", "Warren", ""], ["Liu", "Mingyan", ""], ["Song", "Dawn", ""]]}, {"id": "1801.02612", "submitter": "Chaowei Xiao", "authors": "Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song", "title": "Spatially Transformed Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that widely used deep neural networks (DNNs) are\nvulnerable to carefully crafted adversarial examples. Many advanced algorithms\nhave been proposed to generate adversarial examples by leveraging the\n$\\mathcal{L}_p$ distance for penalizing perturbations. Researchers have\nexplored different defense methods to defend against such adversarial attacks.\nWhile the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual\nquality remains an active research area, in this paper we will instead focus on\na different type of perturbation, namely spatial transformation, as opposed to\nmanipulating the pixel values directly as in prior works. Perturbations\ngenerated through spatial transformation could result in large $\\mathcal{L}_p$\ndistance measures, but our extensive experiments show that such spatially\ntransformed adversarial examples are perceptually realistic and more difficult\nto defend against with existing defense systems. This potentially provides a\nnew direction in adversarial example generation and the design of corresponding\ndefenses. We visualize the spatial transformation based perturbation for\ndifferent examples and show that our technique can produce realistic\nadversarial examples with smooth image deformation. Finally, we visualize the\nattention of deep networks with different types of adversarial examples to\nbetter understand how these examples are interpreted.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:51:59 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 19:03:32 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Xiao", "Chaowei", ""], ["Zhu", "Jun-Yan", ""], ["Li", "Bo", ""], ["He", "Warren", ""], ["Liu", "Mingyan", ""], ["Song", "Dawn", ""]]}, {"id": "1801.02642", "submitter": "Akshay Pai", "authors": "Marco Singh and Akshay Pai", "title": "Boundary Optimizing Network (BON)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite all the success that deep neural networks have seen in classifying\ncertain datasets, the challenge of finding optimal solutions that generalize\nstill remains. In this paper, we propose the Boundary Optimizing Network (BON),\na new approach to generalization for deep neural networks when used for\nsupervised learning. Given a classification network, we propose to use a\ncollaborative generative network that produces new synthetic data points in the\nform of perturbations of original data points. In this way, we create a data\nsupport around each original data point which prevents decision boundaries from\npassing too close to the original data points, i.e. prevents overfitting. We\nshow that BON improves convergence on CIFAR-10 using the state-of-the-art\nDensenet. We do however observe that the generative network suffers from\ncatastrophic forgetting during training, and we therefore propose to use a\nvariation of Memory Aware Synapses to optimize the generative network (called\nBON++). On the Iris dataset, we visualize the effect of BON++ when the\ngenerator does not suffer from catastrophic forgetting and conclude that the\napproach has the potential to create better boundaries in a higher dimensional\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 19:02:44 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 21:32:05 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 10:24:09 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Singh", "Marco", ""], ["Pai", "Akshay", ""]]}, {"id": "1801.02736", "submitter": "Brenden Petersen", "authors": "Brenden K. Petersen, Michael B. Mayhew, Kalvin O. E. Ogbuefi, John D.\n  Greene, Vincent X. Liu, Priyadip Ray", "title": "Modeling sepsis progression using hidden Markov models", "comments": "Accepted to NIPS ML4H 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing a patient's progression through stages of sepsis is critical\nfor enabling risk stratification and adaptive, personalized treatment. However,\ncommonly used sepsis diagnostic criteria fail to account for significant\nunderlying heterogeneity, both between patients as well as over time in a\nsingle patient. We introduce a hidden Markov model of sepsis progression that\nexplicitly accounts for patient heterogeneity. Benchmarked against two sepsis\ndiagnostic criteria, the model provides a useful tool to uncover a patient's\nlatent sepsis trajectory and to identify high-risk patients in whom more\naggressive therapy may be indicated.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 01:02:32 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Petersen", "Brenden K.", ""], ["Mayhew", "Michael B.", ""], ["Ogbuefi", "Kalvin O. E.", ""], ["Greene", "John D.", ""], ["Liu", "Vincent X.", ""], ["Ray", "Priyadip", ""]]}, {"id": "1801.02788", "submitter": "Ian Dewancker", "authors": "Ian Dewancker, Jakob Bauer, Michael McCourt", "title": "Sequential Preference-Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world engineering problems rely on human preferences to guide their\ndesign and optimization. We present PrefOpt, an open source package to simplify\nsequential optimization tasks that incorporate human preference feedback. Our\napproach extends an existing latent variable model for binary preferences to\nallow for observations of equivalent preference from users.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 04:13:11 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Dewancker", "Ian", ""], ["Bauer", "Jakob", ""], ["McCourt", "Michael", ""]]}, {"id": "1801.02850", "submitter": "Yongshuai Liu", "authors": "Yongshuai Liu, Jiyu Chen and Hao Chen", "title": "Less is More: Culling the Training Set to Improve Robustness of Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples. Prior defenses\nattempted to make deep networks more robust by either changing the network\narchitecture or augmenting the training set with adversarial examples, but both\nhave inherent limitations. Motivated by recent research that shows outliers in\nthe training set have a high negative influence on the trained model, we\nstudied the relationship between model robustness and the quality of the\ntraining set. We first show that outliers give the model better generalization\nability but weaker robustness. Next, we propose an adversarial example\ndetection framework, in which we design two methods for removing outliers from\ntraining set to obtain the sanitized model and then detect adversarial example\nby calculating the difference of outputs between the original and the sanitized\nmodel. We evaluated the framework on both MNIST and SVHN. Based on the\ndifference measured by Kullback-Leibler divergence, we could detect adversarial\nexamples with accuracy between 94.67% to 99.89%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 09:36:56 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 03:52:47 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liu", "Yongshuai", ""], ["Chen", "Jiyu", ""], ["Chen", "Hao", ""]]}, {"id": "1801.02858", "submitter": "Seth Flaxman", "authors": "Seth Flaxman and Michael Chirico and Pau Pereira and Charles Loeffler", "title": "Scalable high-resolution forecasting of sparse spatiotemporal events\n  with kernel methods: a winning solution to the NIJ \"Real-Time Crime\n  Forecasting Challenge\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic spatiotemporal event forecasting method, which we\ndeveloped for the National Institute of Justice's (NIJ) Real-Time Crime\nForecasting Challenge. Our method is a spatiotemporal forecasting model\ncombining scalable randomized Reproducing Kernel Hilbert Space (RKHS) methods\nfor approximating Gaussian processes with autoregressive smoothing kernels in a\nregularized supervised learning framework. While the smoothing kernels capture\nthe two main approaches in current use in the field of crime forecasting,\nkernel density estimation (KDE) and self-exciting point process (SEPP) models,\nthe RKHS component of the model can be understood as an approximation to the\npopular log-Gaussian Cox Process model. For inference, we discretize the\nspatiotemporal point pattern and learn a log-intensity function using the\nPoisson likelihood and highly efficient gradient-based optimization methods.\nModel hyperparameters including quality of RKHS approximation, spatial and\ntemporal kernel lengthscales, number of autoregressive lags, bandwidths for\nsmoothing kernels, as well as cell shape, size, and rotation, were learned\nusing crossvalidation. Resulting predictions significantly exceeded baseline\nKDE estimates and SEPP models for sparse events.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 10:04:17 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 13:00:24 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 11:38:24 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2019 08:10:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Flaxman", "Seth", ""], ["Chirico", "Michael", ""], ["Pereira", "Pau", ""], ["Loeffler", "Charles", ""]]}, {"id": "1801.02901", "submitter": "Han Xiao", "authors": "Han Xiao", "title": "Convexification of Neural Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, most complex intelligence architectures are extremely\nnon-convex, which could not be well performed by convex optimization. However,\nthis paper decomposes complex structures into three types of nodes: operators,\nalgorithms and functions. Iteratively, propagating from node to node along\nedge, we prove that \"regarding the tree-structured neural graph, it is nearly\nconvex in each variable, when the other variables are fixed.\" In fact, the\nnon-convex properties stem from circles and functions, which could be\ntransformed to be convex with our proposed \\textit{\\textbf{scale mechanism}}.\nExperimentally, we justify our theoretical analysis by two practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 11:57:41 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 07:13:16 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Xiao", "Han", ""]]}, {"id": "1801.02929", "submitter": "Hiroshi Inoue", "authors": "Hiroshi Inoue", "title": "Data Augmentation by Pairing Samples for Images Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a widely used technique in many machine learning tasks,\nsuch as image classification, to virtually enlarge the training dataset size\nand avoid overfitting. Traditional data augmentation techniques for image\nclassification tasks create new samples from the original training data by, for\nexample, flipping, distorting, adding a small amount of noise to, or cropping a\npatch from an original image. In this paper, we introduce a simple but\nsurprisingly effective data augmentation technique for image classification\ntasks. With our technique, named SamplePairing, we synthesize a new sample from\none image by overlaying another image randomly chosen from the training data\n(i.e., taking an average of two images for each pixel). By using two images\nrandomly selected from the training set, we can generate $N^2$ new samples from\n$N$ training samples. This simple data augmentation technique significantly\nimproved classification accuracy for all the tested datasets; for example, the\ntop-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset\nwith GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show\nthat our SamplePairing technique largely improved accuracy when the number of\nsamples in the training set was very small. Therefore, our technique is more\nvaluable for tasks with a limited amount of training data, such as medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 13:37:11 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 13:28:07 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Inoue", "Hiroshi", ""]]}, {"id": "1801.02937", "submitter": "Masud Moshtaghi", "authors": "Masud Moshtaghi, James C. Bezdek, Sarah M. Erfani, Christopher Leckie,\n  James Bailey", "title": "Online Cluster Validity Indices for Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster analysis is used to explore structure in unlabeled data sets in a\nwide range of applications. An important part of cluster analysis is validating\nthe quality of computationally obtained clusters. A large number of different\ninternal indices have been developed for validation in the offline setting.\nHowever, this concept has not been extended to the online setting. A key\nchallenge is to find an efficient incremental formulation of an index that can\ncapture both cohesion and separation of the clusters over potentially infinite\ndata streams. In this paper, we develop two online versions (with and without\nforgetting factors) of the Xie-Beni and Davies-Bouldin internal validity\nindices, and analyze their characteristics, using two streaming clustering\nalgorithms (sk-means and online ellipsoidal clustering), and illustrate their\nuse in monitoring evolving clusters in streaming data. We also show that\nincremental cluster validity indices are capable of sending a distress signal\nto online monitors when evolving clusters go awry. Our numerical examples\nindicate that the incremental Xie-Beni index with forgetting factor is superior\nto the other three indices tested.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:43:00 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Moshtaghi", "Masud", ""], ["Bezdek", "James C.", ""], ["Erfani", "Sarah M.", ""], ["Leckie", "Christopher", ""], ["Bailey", "James", ""]]}, {"id": "1801.02939", "submitter": "Marton Havasi", "authors": "Marton Havasi, Jos\\'e Miguel Hern\\'andez-Lobato, Juan Jos\\'e\n  Murillo-Fuentes", "title": "Deep Gaussian Processes with Decoupled Inducing Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian Processes (DGP) are hierarchical generalizations of Gaussian\nProcesses (GP) that have proven to work effectively on a multiple supervised\nregression tasks. They combine the well calibrated uncertainty estimates of GPs\nwith the great flexibility of multilayer models. In DGPs, given the inputs, the\noutputs of the layers are Gaussian distributions parameterized by their means\nand covariances. These layers are realized as Sparse GPs where the training\ndata is approximated using a small set of pseudo points. In this work, we show\nthat the computational cost of DGPs can be reduced with no loss in performance\nby using a separate, smaller set of pseudo points when calculating the\nlayerwise variance while using a larger set of pseudo points when calculating\nthe layerwise mean. This enabled us to train larger models that have lower cost\nand better predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 14:01:53 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Havasi", "Marton", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""]]}, {"id": "1801.02949", "submitter": "Marco Capo MSc", "authors": "Marco Cap\\'o, Aritz P\\'erez and Jose A. Lozano", "title": "An efficient K -means clustering algorithm for massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of continously larger datasets is a task of major importance in\na wide variety of scientific fields. In this sense, cluster analysis algorithms\nare a key element of exploratory data analysis, due to their easiness in the\nimplementation and relatively low computational cost. Among these algorithms,\nthe K -means algorithm stands out as the most popular approach, besides its\nhigh dependency on the initial conditions, as well as to the fact that it might\nnot scale well on massive datasets. In this article, we propose a recursive and\nparallel approximation to the K -means algorithm that scales well on both the\nnumber of instances and dimensionality of the problem, without affecting the\nquality of the approximation. In order to achieve this, instead of analyzing\nthe entire dataset, we work on small weighted sets of points that mostly intend\nto extract information from those regions where it is harder to determine the\ncorrect cluster assignment of the original instances. In addition to different\ntheoretical properties, which deduce the reasoning behind the algorithm,\nexperimental results indicate that our method outperforms the state-of-the-art\nin terms of the trade-off between number of distance computations and the\nquality of the solution obtained.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 14:32:06 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Cap\u00f3", "Marco", ""], ["P\u00e9rez", "Aritz", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1801.02950", "submitter": "Abdullah Al-Dujaili", "authors": "Abdullah Al-Dujaili and Alex Huang and Erik Hemberg and Una-May\n  O'Reilly", "title": "Adversarial Deep Learning for Robust Detection of Binary Encoded Malware", "comments": "1ST Deep Learning and Security Workshop (co-located with the 39th\n  IEEE Symposium on Security and Privacy)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware is constantly adapting in order to avoid detection. Model based\nmalware detectors, such as SVM and neural networks, are vulnerable to so-called\nadversarial examples which are modest changes to detectable malware that allows\nthe resulting malware to evade detection. Continuous-valued methods that are\nrobust to adversarial examples of images have been developed using saddle-point\noptimization formulations. We are inspired by them to develop similar methods\nfor the discrete, e.g. binary, domain which characterizes the features of\nmalware. A specific extra challenge of malware is that the adversarial examples\nmust be generated in a way that preserves their malicious functionality. We\nintroduce methods capable of generating functionally preserved adversarial\nmalware examples in the binary domain. Using the saddle-point formulation, we\nincorporate the adversarial examples into the training of models that are\nrobust to them. We evaluate the effectiveness of the methods and others in the\nliterature on a set of Portable Execution~(PE) files. Comparison prompts our\nintroduction of an online measure computed during training to assess general\nexpectation of robustness.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 14:32:30 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 20:29:25 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 14:17:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Al-Dujaili", "Abdullah", ""], ["Huang", "Alex", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1801.02961", "submitter": "Milad Zafar Nezhad", "authors": "Najibesadat Sadati, Milad Zafar Nezhad, Ratna Babu Chinnam, Dongxiao\n  Zhu", "title": "Representation Learning with Autoencoders for Electronic Health Records:\n  A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing volume of Electronic Health Records (EHR) in recent years provides\ngreat opportunities for data scientists to collaborate on different aspects of\nhealthcare research by applying advanced analytics to these EHR clinical data.\nA key requirement however is obtaining meaningful insights from high\ndimensional, sparse and complex clinical data. Data science approaches\ntypically address this challenge by performing feature learning in order to\nbuild more reliable and informative feature representations from clinical data\nfollowed by supervised learning. In this paper, we propose a predictive\nmodeling approach based on deep learning based feature representations and word\nembedding techniques. Our method uses different deep architectures (stacked\nsparse autoencoders, deep belief network, adversarial autoencoders and\nvariational autoencoders) for feature representation in higher-level\nabstraction to obtain effective and robust features from EHRs, and then build\nprediction models on top of them. Our approach is particularly useful when the\nunlabeled data is abundant whereas labeled data is scarce. We investigate the\nperformance of representation learning through a supervised learning approach.\nOur focus is to present a comparative study to evaluate the performance of\ndifferent deep architectures through supervised learning and provide insights\nin the choice of deep feature representation techniques. Our experiments\ndemonstrate that for small data sets, stacked sparse autoencoder demonstrates a\nsuperior generality performance in prediction due to sparsity regularization\nwhereas variational autoencoders outperform the competing approaches for large\ndata sets due to its capability of learning the representation distribution.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 23:17:24 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 14:01:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Sadati", "Najibesadat", ""], ["Nezhad", "Milad Zafar", ""], ["Chinnam", "Ratna Babu", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "1801.02982", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and\n  Nonconvex SGD", "comments": "V2 added two applications to nonconvex stochastic optimization, and\n  V3 corrects a citation. arXiv admin note: text overlap with arXiv:1708.08694", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n  If $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n  If $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30].\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 10:26:50 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 08:27:48 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 22:02:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1801.03050", "submitter": "V\\'ictor Gallego", "authors": "V\\'ictor Gallego, Pablo Su\\'arez-Garc\\'ia, Pablo Angulo, David\n  G\\'omez-Ullate", "title": "Assessing the effect of advertising expenditures upon sales: a Bayesian\n  structural time series model", "comments": "Published at Applied Stochastic Models in Business and Industry,\n  https://onlinelibrary.wiley.com/doi/full/10.1002/asmb.2460", "journal-ref": "Appl Stochastic Models Bus Ind. 2019; 1-13", "doi": "10.1002/asmb.2460", "report-no": null, "categories": "stat.ML econ.EM q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust implementation of the Nerlove--Arrow model using a\nBayesian structural time series model to explain the relationship between\nadvertising expenditures of a country-wide fast-food franchise network with its\nweekly sales. Thanks to the flexibility and modularity of the model, it is well\nsuited to generalization to other markets or situations. Its Bayesian nature\nfacilitates incorporating \\emph{a priori} information (the manager's views),\nwhich can be updated with relevant data. This aspect of the model will be used\nto present a strategy of budget scheduling across time and channels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:39:51 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 08:15:51 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 08:36:35 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gallego", "V\u00edctor", ""], ["Su\u00e1rez-Garc\u00eda", "Pablo", ""], ["Angulo", "Pablo", ""], ["G\u00f3mez-Ullate", "David", ""]]}, {"id": "1801.03132", "submitter": "Chen Wang", "authors": "Chen Wang, Suzhen Wang, Fuyan Shi, Zaixiang Wang", "title": "Robust Propensity Score Computation Method based on Machine Learning\n  with Label-corrupted Data", "comments": "26 pages, 4 figures, 8tables, to be submitted to peer-review journals\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In biostatistics, propensity score is a common approach to analyze the\nimbalance of covariate and process confounding covariates to eliminate\ndifferences between groups. While there are an abundant amount of methods to\ncompute propensity score, a common issue of them is the corrupted labels in the\ndataset. For example, the data collected from the patients could contain\nsamples that are treated mistakenly, and the computing methods could\nincorporate them as a misleading information. In this paper, we propose a\nMachine Learning-based method to handle the problem. Specifically, we utilize\nthe fact that the majority of sample should be labeled with the correct\ninstance and design an approach to first cluster the data with spectral\nclustering and then sample a new dataset with a distribution processed from the\nclustering results. The propensity score is computed by Xgboost, and a\nmathematical justification of our method is provided in this paper. The\nexperimental results illustrate that xgboost propensity scores computing with\nthe data processed by our method could outperform the same method with original\ndata, and the advantages of our method increases as we add some artificial\ncorruptions to the dataset. Meanwhile, the implementation of xgboost to compute\npropensity score for multiple treatments is also a pioneering work in the area.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:35:31 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Suzhen", ""], ["Shi", "Fuyan", ""], ["Wang", "Zaixiang", ""]]}, {"id": "1801.03137", "submitter": "Ben Parr", "authors": "Igor Gitman, Deepak Dilipkumar, Ben Parr", "title": "Convergence Analysis of Gradient Descent Algorithms with Proportional\n  Updates", "comments": "Source code (uses TensorFlow): https://github.com/bparr/lars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of deep learning in recent years has brought with it increasingly\nclever optimization methods to deal with complex, non-linear loss functions.\nThese methods are often designed with convex optimization in mind, but have\nbeen shown to work well in practice even for the highly non-convex optimization\nassociated with neural networks. However, one significant drawback of these\nmethods when they are applied to deep learning is that the magnitude of the\nupdate step is sometimes disproportionate to the magnitude of the weights (much\nsmaller or larger), leading to training instabilities such as vanishing and\nexploding gradients. An idea to combat this issue is gradient descent with\nproportional updates. Gradient descent with proportional updates was introduced\nin 2017. It was independently developed by You et al (Layer-wise Adaptive Rate\nScaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The\nbasic idea of both of these algorithms is to make each step of the gradient\ndescent proportional to the current weight norm and independent of the gradient\nmagnitude. It is common in the context of new optimization methods to prove\nconvergence or derive regret bounds under the assumption of Lipschitz\ncontinuity and convexity. However, even though LARS and PercentDelta were shown\nto work well in practice, there is no theoretical analysis of the convergence\nproperties of these algorithms. Thus it is not clear if the idea of gradient\ndescent with proportional updates is used in the optimal way, or if it could be\nimproved by using a different norm or specific learning rate schedule, for\nexample. Moreover, it is not clear if these algorithms can be extended to other\nproblems, besides neural networks. We attempt to answer these questions by\nestablishing the theoretical analysis of gradient descent with proportional\nupdates, and verifying this analysis with empirical examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:51:28 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Gitman", "Igor", ""], ["Dilipkumar", "Deepak", ""], ["Parr", "Ben", ""]]}, {"id": "1801.03143", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong, Kettip Kriangchaivech, Austin Lanari, Supui Lam,\n  Panthong Wangperawong", "title": "Comparing heterogeneous entities using artificial neural networks of\n  trainable weighted structural components and machine-learned activation\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To compare entities of differing types and structural components, the\nartificial neural network paradigm was used to cross-compare structural\ncomponents between heterogeneous documents. Trainable weighted structural\ncomponents were input into machine-learned activation functions of the neurons.\nThe model was used for matching news articles and videos, where the inputs and\nactivation functions respectively consisted of term vectors and cosine\nsimilarity measures between the weighted structural components. The model was\ntested with different weights, achieving as high as 59.2% accuracy for matching\nvideos to news articles. A mobile application user interface for recommending\nrelated videos for news articles was developed to demonstrate consumer value,\nincluding its potential usefulness for cross-selling products from unrelated\ncategories.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:20:08 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wangperawong", "Artit", ""], ["Kriangchaivech", "Kettip", ""], ["Lanari", "Austin", ""], ["Lam", "Supui", ""], ["Wangperawong", "Panthong", ""]]}, {"id": "1801.03164", "submitter": "Justin Gottschlich", "authors": "Justin Gottschlich", "title": "Paranom: A Parallel Anomaly Dataset Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Paranom, a parallel anomaly dataset generator. We\ndiscuss its design and provide brief experimental results demonstrating its\nusefulness in improving the classification correctness of LSTM-AD, a\nstate-of-the-art anomaly detection model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 22:33:51 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Gottschlich", "Justin", ""]]}, {"id": "1801.03222", "submitter": "Ning Ning", "authors": "S. Rao Jammalamadaka, Jinwen Qiu and Ning Ning", "title": "Multivariate Bayesian Structural Time Series Model", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with inference and prediction for multiple correlated time\nseries, where one has also the choice of using a candidate pool of\ncontemporaneous predictors for each target series. Starting with a structural\nmodel for the time-series, Bayesian tools are used for model fitting,\nprediction, and feature selection, thus extending some recent work along these\nlines for the univariate case. The Bayesian paradigm in this multivariate\nsetting helps the model avoid overfitting as well as capture correlations among\nthe multiple time series with the various state components. The model provides\nneeded flexibility to choose a different set of components and available\npredictors for each target series. The cyclical component in the model can\nhandle large variations in the short term, which may be caused by external\nshocks. We run extensive simulations to investigate properties such as\nestimation accuracy and performance in forecasting. We then run an empirical\nstudy with one-step-ahead prediction on the max log return of a portfolio of\nstocks that involve four leading financial institutions. Both the simulation\nstudies and the extensive empirical study confirm that this multivariate model\noutperforms three other benchmark models, viz. a model that treats each target\nseries as independent, the autoregressive integrated moving average model with\nregression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 02:48:34 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 00:46:48 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Jammalamadaka", "S. Rao", ""], ["Qiu", "Jinwen", ""], ["Ning", "Ning", ""]]}, {"id": "1801.03226", "submitter": "Ruoyu Li", "authors": "Ruoyu Li, Sheng Wang, Feiyun Zhu, Junzhou Huang", "title": "Adaptive Graph Convolutional Neural Networks", "comments": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-18), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Neural Networks (Graph CNNs) are generalizations of\nclassical CNNs to handle graph data such as molecular data, point could and\nsocial networks. Current filters in graph CNNs are built for fixed and shared\ngraph structure. However, for most real data, the graph structures varies in\nboth size and connectivity. The paper proposes a generalized and flexible graph\nCNN taking data of arbitrary graph structure as input. In that way a\ntask-driven adaptive graph is learned for each graph data while training. To\nefficiently learn the graph, a distance metric learning is proposed. Extensive\nexperiments on nine graph-structured datasets have demonstrated the superior\nperformance improvement on both convergence speed and predictive accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:17:45 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Li", "Ruoyu", ""], ["Wang", "Sheng", ""], ["Zhu", "Feiyun", ""], ["Huang", "Junzhou", ""]]}, {"id": "1801.03265", "submitter": "Chen-Yu Wei", "authors": "Chen-Yu Wei and Haipeng Luo", "title": "More Adaptive Algorithms for Adversarial Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel and generic algorithm for the adversarial multi-armed\nbandit problem (or more generally the combinatorial semi-bandit problem). When\ninstantiated differently, our algorithm achieves various new data-dependent\nregret bounds improving previous work. Examples include: 1) a regret bound\ndepending on the variance of only the best arm; 2) a regret bound depending on\nthe first-order path-length of only the best arm; 3) a regret bound depending\non the sum of first-order path-lengths of all arms as well as an important\nnegative term, which together lead to faster convergence rates for some normal\nform games with partial feedback; 4) a regret bound that simultaneously implies\nsmall regret when the best arm has small loss and logarithmic regret when there\nexists an arm whose expected loss is always smaller than those of others by a\nfixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last\ntwo results, our algorithm is completely parameter-free.\n  The main idea of our algorithm is to apply the optimism and adaptivity\ntechniques to the well-known Online Mirror Descent framework with a special\nlog-barrier regularizer. The challenges are to come up with appropriate\noptimistic predictions and correction terms in this framework. Some of our\nresults also crucially rely on using a sophisticated increasing learning rate\nschedule.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 08:28:00 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 00:23:45 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 16:57:01 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Wei", "Chen-Yu", ""], ["Luo", "Haipeng", ""]]}, {"id": "1801.03326", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and Shimon Whiteson", "title": "Expected Policy Gradients for Reinforcement Learning", "comments": "36 pages, submitted for review to JMLR. This is an extended version\n  of our paper in the AAAI-18 conference (arXiv:1706.05374)", "journal-ref": "Journal of Machine Learning Research, Vol. 21, (52):1-51, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose expected policy gradients (EPG), which unify stochastic policy\ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement\nlearning. Inspired by expected sarsa, EPG integrates (or sums) across actions\nwhen estimating the gradient, instead of relying only on the action in the\nsampled trajectory. For continuous action spaces, we first derive a practical\nresult for Gaussian policies and quadratic critics and then extend it to a\nuniversal analytical method, covering a broad class of actors and critics,\nincluding Gaussian, exponential families, and policies with bounded support.\nFor Gaussian policies, we introduce an exploration method that uses covariance\nproportional to the matrix exponential of the scaled Hessian of the critic with\nrespect to the actions. For discrete action spaces, we derive a variant of EPG\nbased on softmax policies. We also establish a new general policy gradient\ntheorem, of which the stochastic and deterministic policy gradient theorems are\nspecial cases. Furthermore, we prove that EPG reduces the variance of the\ngradient estimates without requiring deterministic policies and with little\ncomputational overhead. Finally, we provide an extensive experimental\nevaluation of EPG and show that it outperforms existing approaches on multiple\nchallenging control domains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 11:59:59 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 07:20:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ciosek", "Kamil", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1801.03329", "submitter": "Gil Keren", "authors": "Gil Keren, Maximilian Schmitt, Thomas Kehrenberg, Bj\\\"orn Schuller", "title": "Weakly Supervised One-Shot Detection with Attention Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models that are not conditioned on class identities were shown\nto facilitate knowledge transfer between classes and to be well-suited for\none-shot learning tasks. Following this motivation, we further explore and\nestablish such models and present a novel neural network architecture for the\ntask of weakly supervised one-shot detection. Our model is only conditioned on\na single exemplar of an unseen class and a larger target example that may or\nmay not contain an instance of the same class as the exemplar. By pairing a\nSiamese similarity network with an attention mechanism, we design a model that\nmanages to simultaneously identify and localise instances of classes unseen at\ntraining time. In experiments with datasets from the computer vision and audio\ndomains, the proposed method considerably outperforms the baseline methods for\nthe weakly supervised one-shot detection task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 12:10:24 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 13:57:08 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 09:12:33 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Keren", "Gil", ""], ["Schmitt", "Maximilian", ""], ["Kehrenberg", "Thomas", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1801.03437", "submitter": "Mikhail Belkin", "authors": "Mikhail Belkin", "title": "Approximation beats concentration? An approximation view on inference\n  with smooth radial kernels", "comments": null, "journal-ref": "Conference on Computational Learning Theory (COLT) 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive definite kernels and their associated Reproducing Kernel Hilbert\nSpaces provide a mathematically compelling and practically competitive\nframework for learning from data.\n  In this paper we take the approximation theory point of view to explore\nvarious aspects of smooth kernels related to their inferential properties. We\nanalyze eigenvalue decay of kernels operators and matrices, properties of\neigenfunctions/eigenvectors and \"Fourier\" coefficients of functions in the\nkernel space restricted to a discrete set of data points. We also investigate\nthe fitting capacity of kernels, giving explicit bounds on the fat shattering\ndimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the\nsame properties that make kernels very effective approximators for functions in\ntheir \"native\" kernel space, also limit their capacity to represent arbitrary\nfunctions. We discuss various implications, including those for gradient\ndescent type methods.\n  It is important to note that most of our bounds are measure independent.\nMoreover, at least in moderate dimension, the bounds for eigenvalues are much\ntighter than the bounds which can be obtained from the usual matrix\nconcentration results. For example, we see that the eigenvalues of kernel\nmatrices show nearly exponential decay with constants depending only on the\nkernel and the domain. We call this \"approximation beats concentration\"\nphenomenon as even when the data are sampled from a probability distribution,\nsome of their aspects are better understood in terms of approximation theory.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 16:13:15 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 21:03:29 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Belkin", "Mikhail", ""]]}, {"id": "1801.03454", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters\n  in Deep Neural Networks", "comments": "Camera-Ready for CVPR18; supplementary materials:\n  http://ruthcfong.github.io/files/net2vec_supps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to understand the meaning of the intermediate representations\ncaptured by deep networks, recent papers have tried to associate specific\nsemantic concepts to individual neural network filter responses, where\ninteresting correlations are often found, largely by focusing on extremal\nfilter responses. In this paper, we show that this approach can favor\neasy-to-interpret cases that are not necessarily representative of the average\nbehavior of a representation.\n  A more realistic but harder-to-study hypothesis is that semantic\nrepresentations are distributed, and thus filters must be studied in\nconjunction. In order to investigate this idea while enabling systematic\nvisualization and quantification of multiple filter responses, we introduce the\nNet2Vec framework, in which semantic concepts are mapped to vectorial\nembeddings based on corresponding filter responses. By studying such\nembeddings, we are able to show that 1., in most cases, multiple filters are\nrequired to code for a concept, that 2., often filters are not concept specific\nand help encode multiple concepts, and that 3., compared to single filter\nactivations, filter embeddings are able to better characterize the meaning of a\nrepresentation and its relationship to other concepts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 17:01:36 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:00:02 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1801.03523", "submitter": "Fernando Fernandes Neto", "authors": "Fernando Fernandes Neto", "title": "Generative Models for Stochastic Processes Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE physics.comp-ph q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper aims to demonstrate the usage of Convolutional Neural\nNetworks as a generative model for stochastic processes, enabling researchers\nfrom a wide range of fields (such as quantitative finance and physics) to\ndevelop a general tool for forecasts and simulations without the need to\nidentify/assume a specific system structure or estimate its parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 03:35:20 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Neto", "Fernando Fernandes", ""]]}, {"id": "1801.03533", "submitter": "Manish Raghavan", "authors": "Jon Kleinberg, Manish Raghavan", "title": "Selection Problems in the Presence of Implicit Bias", "comments": "ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, the notion of implicit bias has come to serve as\nan important component in our understanding of discrimination in activities\nsuch as hiring, promotion, and school admissions. Research on implicit bias\nposits that when people evaluate others -- for example, in a hiring context --\ntheir unconscious biases about membership in particular groups can have an\neffect on their decision-making, even when they have no deliberate intention to\ndiscriminate against members of these groups. A growing body of experimental\nwork has pointed to the effect that implicit bias can have in producing adverse\noutcomes.\n  Here we propose a theoretical model for studying the effects of implicit bias\non selection decisions, and a way of analyzing possible procedural remedies for\nimplicit bias within this model. A canonical situation represented by our model\nis a hiring setting: a recruiting committee is trying to choose a set of\nfinalists to interview among the applicants for a job, evaluating these\napplicants based on their future potential, but their estimates of potential\nare skewed by implicit bias against members of one group. In this model, we\nshow that measures such as the Rooney Rule, a requirement that at least one of\nthe finalists be chosen from the affected group, can not only improve the\nrepresentation of this affected group, but also lead to higher payoffs in\nabsolute terms for the organization performing the recruiting. However,\nidentifying the conditions under which such measures can lead to improved\npayoffs involves subtle trade-offs between the extent of the bias and the\nunderlying distribution of applicant characteristics, leading to novel\ntheoretical questions about order statistics in the presence of probabilistic\nside information.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:53:58 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Kleinberg", "Jon", ""], ["Raghavan", "Manish", ""]]}, {"id": "1801.03558", "submitter": "Chris Cremer", "authors": "Chris Cremer, Xuechen Li, David Duvenaud", "title": "Inference Suboptimality in Variational Autoencoders", "comments": "ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amortized inference allows latent-variable models trained via variational\nlearning to scale to large datasets. The quality of approximate inference is\ndetermined by two factors: a) the capacity of the variational distribution to\nmatch the true posterior and b) the ability of the recognition network to\nproduce good variational parameters for each datapoint. We examine approximate\ninference in variational autoencoders in terms of these factors. We find that\ndivergence from the true posterior is often due to imperfect recognition\nnetworks, rather than the limited complexity of the approximating distribution.\nWe show that this is due partly to the generator learning to accommodate the\nchoice of approximation. Furthermore, we show that the parameters used to\nincrease the expressiveness of the approximation play a role in generalizing\ninference rather than simply improving the complexity of the approximation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 21:24:59 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 19:53:47 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 18:04:22 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Cremer", "Chris", ""], ["Li", "Xuechen", ""], ["Duvenaud", "David", ""]]}, {"id": "1801.03714", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar, Mahdi Barzegar Khalilsarai, and Giuseppe Caire", "title": "Multi-Band Covariance Interpolation with Applications in Massive MIMO", "comments": "A short version of this paper was submitted to IEEE International\n  Symposium on Information Theory (ISIT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of multi-band (frequency-variant)\ncovariance interpolation with a particular emphasis towards massive MIMO\napplications. In a massive MIMO system, the communication between each BS with\n$M \\gg 1$ antennas and each single-antenna user occurs through a collection of\nscatterers in the environment, where the channel vector of each user at BS\nantennas consists in a weighted linear combination of the array responses of\nthe scatterers, where each scatterer has its own angle of arrival (AoA) and\ncomplex channel gain. The array response at a given AoA depends on the\nwavelength of the incoming planar wave and is naturally frequency dependent.\nThis results in a frequency-dependent distortion where the second order\nstatistics, i.e., the covariance matrix, of the channel vectors varies with\nfrequency. In this paper, we show that although this effect is generally\nnegligible for a small number of antennas $M$, it results in a considerable\ndistortion of the covariance matrix and especially its dominant signal subspace\nin the massive MIMO regime where $M \\to \\infty$, and can generally incur a\nserious degradation of the performance especially in frequency division\nduplexing (FDD) massive MIMO systems where the uplink (UL) and the downlink\n(DL) communication occur over different frequency bands. We propose a novel\nUL-DL covariance interpolation technique that is able to recover the covariance\nmatrix in the DL from an estimate of the covariance matrix in the UL under a\nmild reciprocity condition on the angular power spread function (PSF) of the\nusers. We analyze the performance of our proposed scheme mathematically and\nprove its robustness under a sufficiently large spatial oversampling of the\narray. We also propose several simple off-the-shelf algorithms for UL-DL\ncovariance interpolation and evaluate their performance via numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 11:21:03 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Haghighatshoar", "Saeid", ""], ["Khalilsarai", "Mahdi Barzegar", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1801.03744", "submitter": "Boris Hanin", "authors": "Boris Hanin", "title": "Which Neural Net Architectures Give Rise To Exploding and Vanishing\n  Gradients?", "comments": "v3. 18p. 1 fig. Accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a rigorous analysis of the statistical behavior of gradients in a\nrandomly initialized fully connected network N with ReLU activations. Our\nresults show that the empirical variance of the squares of the entries in the\ninput-output Jacobian of N is exponential in a simple architecture-dependent\nconstant beta, given by the sum of the reciprocals of the hidden layer widths.\nWhen beta is large, the gradients computed by N at initialization vary wildly.\nOur approach complements the mean field theory analysis of random networks.\nFrom this point of view, we rigorously compute finite width corrections to the\nstatistics of gradients at the edge of chaos.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 13:17:22 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 13:25:08 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 01:02:57 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hanin", "Boris", ""]]}, {"id": "1801.03749", "submitter": "R\\'emi Leblond", "authors": "R\\'emi Leblond and Fabian Pedregosa and Simon Lacoste-Julien", "title": "Improved asynchronous parallel optimization analysis for stochastic\n  incremental methods", "comments": "67 pages, published in JMLR, can be found online at\n  http://jmlr.org/papers/v19/17-650.html. arXiv admin note: substantial text\n  overlap with arXiv:1606.04809", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets continue to increase in size and multi-core computer\narchitectures are developed, asynchronous parallel optimization algorithms\nbecome more and more essential to the field of Machine Learning. Unfortunately,\nconducting the theoretical analysis asynchronous methods is difficult, notably\ndue to the introduction of delay and inconsistency in inherently sequential\nalgorithms. Handling these issues often requires resorting to simplifying but\nunrealistic assumptions. Through a novel perspective, we revisit and clarify a\nsubtle but important technical issue present in a large fraction of the recent\nconvergence rate proofs for asynchronous parallel optimization algorithms, and\npropose a simplification of the recently introduced \"perturbed iterate\"\nframework that resolves it. We demonstrate the usefulness of our new framework\nby analyzing three distinct asynchronous parallel incremental optimization\nalgorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and\nASAGA, a novel asynchronous parallel version of the incremental gradient\nalgorithm SAGA that enjoys fast linear convergence rates. We are able to both\nremove problematic assumptions and obtain better theoretical results. Notably,\nwe prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on\nmulti-core systems even without sparsity assumptions. We present results of an\nimplementation on a 40-core architecture illustrating the practical speedups as\nwell as the hardware overhead. Finally, we investigate the overlap constant, an\nill-understood but central quantity for the theoretical analysis of\nasynchronous parallel algorithms. We find that it encompasses much more\ncomplexity than suggested in previous work, and often is order-of-magnitude\nbigger than traditionally thought.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 13:31:33 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 10:55:58 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 21:52:08 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Leblond", "R\u00e9mi", ""], ["Pedregosa", "Fabian", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1801.03765", "submitter": "Dirk Lorenz", "authors": "Dirk A. Lorenz, Quoc Tran-Dinh", "title": "Non-stationary Douglas-Rachford and alternating direction method of\n  multipliers: adaptive stepsizes and convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical Douglas-Rachford (DR) method for finding a zero of\nthe sum of two maximal monotone operators. Since the practical performance of\nthe DR method crucially depends on the stepsizes, we aim at developing an\nadaptive stepsize rule. To that end, we take a closer look at a linear case of\nthe problem and use our findings to develop a stepsize strategy that eliminates\nthe need for stepsize tuning. We analyze a general non-stationary DR scheme and\nprove its convergence for a convergent sequence of stepsizes with summable\nincrements. This, in turn, proves the convergence of the method with the new\nadaptive stepsize rule. We also derive the related non-stationary alternating\ndirection method of multipliers (ADMM) from such a non-stationary DR method. We\nillustrate the efficiency of the proposed methods on several numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 14:13:16 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 08:08:20 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lorenz", "Dirk A.", ""], ["Tran-Dinh", "Quoc", ""]]}, {"id": "1801.03773", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Polar $n$-Complex and $n$-Bicomplex Singular Value Decomposition and\n  Principal Component Pursuit", "comments": "12 pages, 2 figures", "journal-ref": "IEEE Trans. Signal Process., vol. 64, no. 24, pp. 6533-6544, Dec.\n  2016", "doi": "10.1109/TSP.2016.2612171", "report-no": null, "categories": "eess.SP cs.MM cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informed by recent work on tensor singular value decomposition and circulant\nalgebra matrices, this paper presents a new theoretical bridge that unifies the\nhypercomplex and tensor-based approaches to singular value decomposition and\nrobust principal component analysis. We begin our work by extending the\nprincipal component pursuit to Olariu's polar $n$-complex numbers as well as\ntheir bicomplex counterparts. In so doing, we have derived the polar\n$n$-complex and $n$-bicomplex proximity operators for both the $\\ell_1$- and\ntrace-norm regularizers, which can be used by proximal optimization methods\nsuch as the alternating direction method of multipliers. Experimental results\non two sets of audio data show that our algebraically-informed formulation\noutperforms tensor robust principal component analysis. We conclude with the\nmessage that an informed definition of the trace norm can bridge the gap\nbetween the hypercomplex and tensor-based approaches. Our approach can be seen\nas a general methodology for generating other principal component pursuit\nalgorithms with proper algebraic structures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:45:01 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.03815", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Informed Group-Sparse Representation for Singing Voice Separation", "comments": "5 pages, 1 figure", "journal-ref": "IEEE Signal Process. Lett., vol. 24, no. 2, pp. 156-160, Feb. 2017", "doi": "10.1109/LSP.2017.2647810", "report-no": null, "categories": "eess.AS cs.IR cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice separation attempts to separate the vocal and instrumental\nparts of a music recording, which is a fundamental problem in music information\nretrieval. Recent work on singing voice separation has shown that the low-rank\nrepresentation and informed separation approaches are both able to improve\nseparation quality. However, low-rank optimizations are computationally\ninefficient due to the use of singular value decompositions. Therefore, in this\npaper, we propose a new linear-time algorithm called informed group-sparse\nrepresentation, and use it to separate the vocals from music using pitch\nannotations as side information. Experimental results on the iKala dataset\nconfirm the efficacy of our approach, suggesting that the music accompaniment\nfollows a group-sparse structure given a pre-trained instrumental dictionary.\nWe also show how our work can be easily extended to accommodate multiple\ndictionaries using the DSD100 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:45:30 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.03816", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Complex and Quaternionic Principal Component Pursuit and Its Application\n  to Audio Separation", "comments": "5 pages, 1 figure", "journal-ref": "IEEE Signal Process. Lett., vol. 23, no. 2, pp. 287-291, Feb. 2016", "doi": "10.1109/LSP.2016.2514845", "report-no": null, "categories": "eess.SP cs.MM cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the principal component pursuit has received increasing attention\nin signal processing research ranging from source separation to video\nsurveillance. So far, all existing formulations are real-valued and lack the\nconcept of phase, which is inherent in inputs such as complex spectrograms or\ncolor images. Thus, in this letter, we extend principal component pursuit to\nthe complex and quaternionic cases to account for the missing phase\ninformation. Specifically, we present both complex and quaternionic proximity\noperators for the $\\ell_1$- and trace-norm regularizers. These operators can be\nused in conjunction with proximal minimization methods such as the inexact\naugmented Lagrange multiplier algorithm. The new algorithms are then applied to\nthe singing voice separation problem, which aims to separate the singing voice\nfrom the instrumental accompaniment. Results on the iKala and MSD100 datasets\nconfirmed the usefulness of phase information in principal component pursuit.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:39:50 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.03851", "submitter": "Chris Williams", "authors": "Christopher K. I. Williams, Charlie Nash, Alfredo Naz\\'abal", "title": "Autoencoders and Probabilistic Inference with Missing Data: An Exact\n  Solution for The Factor Analysis Case", "comments": "7 pages, 2 figures, Adding ref to Ilin and Raiko (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models can be used to probabilistically \"fill-in\" missing\ndata entries. The variational autoencoder architecture (Kingma and Welling,\n2014; Rezende et al., 2014) includes a \"recognition\" or \"encoder\" network that\ninfers the latent variables given the data variables. However, it is not clear\nhow to handle missing data variables in this network. The factor analysis (FA)\nmodel is a basic autoencoder, using linear encoder and decoder networks. We\nshow how to calculate exactly the latent posterior distribution for the factor\nanalysis (FA) model in the presence of missing data, and note that this\nsolution implies that a different encoder network is required for each pattern\nof missingness. We also discuss various approximations to the exact solution.\nExperiments compare the effectiveness of various approaches to filling in the\nmissing data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 16:22:07 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 13:50:26 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 15:59:07 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Williams", "Christopher K. I.", ""], ["Nash", "Charlie", ""], ["Naz\u00e1bal", "Alfredo", ""]]}, {"id": "1801.03911", "submitter": "Sahil Garg", "authors": "Sahil Garg and Greg Ver Steeg and Aram Galstyan", "title": "Stochastic Learning of Nonstationary Kernels for Natural Language\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing often involves computations with semantic or\nsyntactic graphs to facilitate sophisticated reasoning based on structural\nrelationships. While convolution kernels provide a powerful tool for comparing\ngraph structure based on node (word) level relationships, they are difficult to\ncustomize and can be computationally expensive. We propose a generalization of\nconvolution kernels, with a nonstationary model, for better expressibility of\nnatural languages in supervised settings. For a scalable learning of the\nparameters introduced with our model, we propose a novel algorithm that\nleverages stochastic sampling on k-nearest neighbor graphs, along with\napproximations based on locality-sensitive hashing. We demonstrate the\nadvantages of our approach on a challenging real-world (structured inference)\nproblem of automatically extracting biological models from the text of\nscientific papers.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:24:02 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 21:41:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Garg", "Sahil", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1801.04016", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "Theoretical Impediments to Machine Learning With Seven Sparks from the\n  Causal Revolution", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "R-475", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current machine learning systems operate, almost exclusively, in a\nstatistical, or model-free mode, which entails severe theoretical limits on\ntheir power and performance. Such systems cannot reason about interventions and\nretrospection and, therefore, cannot serve as the basis for strong AI. To\nachieve human level intelligence, learning machines need the guidance of a\nmodel of reality, similar to the ones used in causal inference tasks. To\ndemonstrate the essential role of such models, I will present a summary of\nseven tasks which are beyond reach of current machine learning systems and\nwhich have been accomplished using the tools of causal modeling.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 23:37:48 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1801.04053", "submitter": "Osonde Osoba Ph.D.", "authors": "Osonde Osoba, Bart Kosko", "title": "Noisy Expectation-Maximization: Applications and Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a noise-injected version of the Expectation-Maximization (EM)\nalgorithm: the Noisy Expectation Maximization (NEM) algorithm. The NEM\nalgorithm uses noise to speed up the convergence of the EM algorithm. The NEM\ntheorem shows that injected noise speeds up the average convergence of the EM\nalgorithm to a local maximum of the likelihood surface if a positivity\ncondition holds. The generalized form of the noisy expectation-maximization\n(NEM) algorithm allow for arbitrary modes of noise injection including adding\nand multiplying noise to the data.\n  We demonstrate these noise benefits on EM algorithms for the Gaussian mixture\nmodel (GMM) with both additive and multiplicative NEM noise injection. A\nseparate theorem (not presented here) shows that the noise benefit for\nindependent identically distributed additive noise decreases with sample size\nin mixture models. This theorem implies that the noise benefit is most\npronounced if the data is sparse. Injecting blind noise only slowed\nconvergence.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 04:09:48 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Osoba", "Osonde", ""], ["Kosko", "Bart", ""]]}, {"id": "1801.04055", "submitter": "Aristide Baratin", "authors": "Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien", "title": "A3T: Adversarially Augmented Adversarial Training", "comments": "accepted for an oral presentation in Machine Deception Workshop, NIPS\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research showed that deep neural networks are highly sensitive to\nso-called adversarial perturbations, which are tiny perturbations of the input\ndata purposely designed to fool a machine learning classifier. Most\nclassification models, including deep learning models, are highly vulnerable to\nadversarial attacks. In this work, we investigate a procedure to improve\nadversarial robustness of deep neural networks through enforcing representation\ninvariance. The idea is to train the classifier jointly with a discriminator\nattached to one of its hidden layer and trained to filter the adversarial\nnoise. We perform preliminary experiments to test the viability of the approach\nand to compare it to other standard adversarial training methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 04:34:17 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Erraqabi", "Akram", ""], ["Baratin", "Aristide", ""], ["Bengio", "Yoshua", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1801.04062", "submitter": "Mohamed Ishmael Belghazi", "authors": "Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil\n  Ozair, Yoshua Bengio, Aaron Courville, R Devon Hjelm", "title": "MINE: Mutual Information Neural Estimation", "comments": "19 pages, 6 figures", "journal-ref": "ICML 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the estimation of mutual information between high dimensional\ncontinuous random variables can be achieved by gradient descent over neural\nnetworks. We present a Mutual Information Neural Estimator (MINE) that is\nlinearly scalable in dimensionality as well as in sample size, trainable\nthrough back-prop, and strongly consistent. We present a handful of\napplications on which MINE can be used to minimize or maximize mutual\ninformation. We apply MINE to improve adversarially trained generative models.\nWe also use MINE to implement Information Bottleneck, applying it to supervised\nclassification; our results demonstrate substantial improvement in flexibility\nand performance in these settings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 05:42:58 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 03:31:35 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 15:56:13 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 20:13:11 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Belghazi", "Mohamed Ishmael", ""], ["Baratin", "Aristide", ""], ["Rajeswar", "Sai", ""], ["Ozair", "Sherjil", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Hjelm", "R Devon", ""]]}, {"id": "1801.04140", "submitter": "Sadegh Movahed", "authors": "A. Vafaei Sadr, M. Farhang, S. M. S. Movahed, B. Bassett, M. Kunz", "title": "Cosmic String Detection with Tree-Based Machine Learning", "comments": "7 pages, 3 figures, 2 tables, Comments are welcome", "journal-ref": "Monthly Notices of the Royal Astronomical Society 478.1 (2018):\n  1132-1140", "doi": "10.1093/mnras/sty1055", "report-no": null, "categories": "astro-ph.CO astro-ph.IM physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of random forest and gradient boosting, two powerful\ntree-based machine learning algorithms, for the detection of cosmic strings in\nmaps of the cosmic microwave background (CMB), through their unique\nGott-Kaiser-Stebbins effect on the temperature anisotropies.The information in\nthe maps is compressed into feature vectors before being passed to the learning\nunits. The feature vectors contain various statistical measures of processed\nCMB maps that boost the cosmic string detectability. Our proposed classifiers,\nafter training, give results improved over or similar to the claimed\ndetectability levels of the existing methods for string tension, $G\\mu$. They\ncan make $3\\sigma$ detection of strings with $G\\mu \\gtrsim 2.1\\times 10^{-10}$\nfor noise-free, $0.9'$-resolution CMB observations. The minimum detectable\ntension increases to $G\\mu \\gtrsim 3.0\\times 10^{-8}$ for a more realistic, CMB\nS4-like (II) strategy, still a significant improvement over the previous\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 11:57:22 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Sadr", "A. Vafaei", ""], ["Farhang", "M.", ""], ["Movahed", "S. M. S.", ""], ["Bassett", "B.", ""], ["Kunz", "M.", ""]]}, {"id": "1801.04153", "submitter": "Francois-Xavier Briol", "authors": "Xiaoyue Xi, Fran\\c{c}ois-Xavier Briol, Mark Girolami", "title": "Bayesian Quadrature for Multiple Related Integrals", "comments": "Proceedings of the 35th International Conference on Machine Learning\n  (ICML), PMLR 80:5369-5378, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian probabilistic numerical methods are a set of tools providing\nposterior distributions on the output of numerical methods. The use of these\nmethods is usually motivated by the fact that they can represent our\nuncertainty due to incomplete/finite information about the continuous\nmathematical problem being approximated. In this paper, we demonstrate that\nthis paradigm can provide additional advantages, such as the possibility of\ntransferring information between several numerical methods. This allows users\nto represent uncertainty in a more faithful manner and, as a by-product,\nprovide increased numerical efficiency. We propose the first such numerical\nmethod by extending the well-known Bayesian quadrature algorithm to the case\nwhere we are interested in computing the integral of several related functions.\nWe then prove convergence rates for the method in the well-specified and\nmisspecified cases, and demonstrate its efficiency in the context of\nmulti-fidelity models for complex engineering systems and a problem of global\nillumination in computer graphics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 12:49:32 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 11:16:00 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 10:53:01 GMT"}, {"version": "v4", "created": "Wed, 7 Mar 2018 15:03:05 GMT"}, {"version": "v5", "created": "Wed, 6 Jun 2018 12:48:30 GMT"}, {"version": "v6", "created": "Thu, 19 Jul 2018 10:29:51 GMT"}, {"version": "v7", "created": "Mon, 30 Jul 2018 18:44:13 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Xi", "Xiaoyue", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Girolami", "Mark", ""]]}, {"id": "1801.04159", "submitter": "Lucas Maystre", "authors": "Ali Batuhan Yard{\\i}m, Victor Kristof, Lucas Maystre, Matthias\n  Grossglauser", "title": "Can Who-Edits-What Predict Edit Survival?", "comments": "Accepted at KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219979", "report-no": null, "categories": "stat.AP cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of contributors to online peer-production systems grows, it\nbecomes increasingly important to predict whether the edits that users make\nwill eventually be beneficial to the project. Existing solutions either rely on\na user reputation system or consist of a highly specialized predictor that is\ntailored to a specific peer-production system. In this work, we explore a\ndifferent point in the solution space that goes beyond user reputation but does\nnot involve any content-based feature of the edits. We view each edit as a game\nbetween the editor and the component of the project. We posit that the\nprobability that an edit is accepted is a function of the editor's skill, of\nthe difficulty of editing the component and of a user-component interaction\nterm. Our model is broadly applicable, as it only requires observing data about\nwho makes an edit, what the edit affects and whether the edit survives or not.\nWe apply our model on Wikipedia and the Linux kernel, two examples of\nlarge-scale peer-production systems, and we seek to understand whether it can\neffectively predict edit survival: in both cases, we provide a positive answer.\nOur approach significantly outperforms those based solely on user reputation\nand bridges the gap with specialized predictors that use content-based\nfeatures. It is simple to implement, computationally inexpensive, and in\naddition it enables us to discover interesting structure in the data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 13:26:57 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 07:37:47 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Yard\u0131m", "Ali Batuhan", ""], ["Kristof", "Victor", ""], ["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1801.04211", "submitter": "Felix Horger", "authors": "Felix Horger, Tobias W\\\"urfl, Vincent Christlein, Andreas Maier", "title": "Towards Arbitrary Noise Augmentation - Deep Learning for Sampling from\n  Arbitrary Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate noise modelling is important for training of deep learning\nreconstruction algorithms. While noise models are well known for traditional\nimaging techniques, the noise distribution of a novel sensor may be difficult\nto determine a priori. Therefore, we propose learning arbitrary noise\ndistributions. To do so, this paper proposes a fully connected neural network\nmodel to map samples from a uniform distribution to samples of any explicitly\nknown probability density function. During the training, the Jensen-Shannon\ndivergence between the distribution of the model's output and the target\ndistribution is minimized. We experimentally demonstrate that our model\nconverges towards the desired state. It provides an alternative to existing\nsampling methods such as inversion sampling, rejection sampling, Gaussian\nmixture models and Markov-Chain-Monte-Carlo. Our model has high sampling\nefficiency and is easily applied to any probability distribution, without the\nneed of further analytical or numerical calculations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:03:21 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 08:55:22 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Horger", "Felix", ""], ["W\u00fcrfl", "Tobias", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""]]}, {"id": "1801.04212", "submitter": "Mor Absa Loum", "authors": "Mor Absa Loum (LM-Orsay), Marie-Anne Poursat (LM-Orsay), Abdourahmane\n  Sow, Amadou Sall, Cheikh Loucoubar (G4-IPD), Elisabeth Gassiat (LM-Orsay)", "title": "Multinomial logistic model for coinfection diagnosis between arbovirus\n  and malaria in Kedougou", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tropical regions, populations continue to suffer morbidity and mortality\nfrom malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are\nall endemic due to the climate and its geographical position. The\nco-circulation of malaria parasites and arboviruses can explain the observation\nof coinfected cases. Indeed there is strong resemblance in symptoms between\nthese diseases making problematic targeted medical care of coinfected cases.\nThis is due to the fact that the origin of illness is not obviously known. Some\ncases could be immunized against one or the other of the pathogens, immunity\ntypically acquired with factors like age and exposure as usual for endemic\narea. Then, coinfection needs to be better diagnosed. Using data collected from\npatients in Kedougou region, from 2009 to 2013, we adjusted a multinomial\nlogistic model and selected relevant variables in explaining coinfection\nstatus. We observed specific sets of variables explaining each of the diseases\nexclusively and the coinfection. We tested the independence between arboviral\nand malaria infections and derived coinfection probabilities from the model\nfitting. In case of a coinfection probability greater than a threshold value to\nbe calibrated on the data, duration of illness above 3 days and age above 10\nyears-old are mostly indicative of arboviral disease while body temperature\nhigher than 40{\\textdegree}C and presence of nausea or vomiting symptoms during\nthe rainy season are mostly indicative of malaria disease.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:05:56 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:47:17 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Loum", "Mor Absa", "", "LM-Orsay"], ["Poursat", "Marie-Anne", "", "LM-Orsay"], ["Sow", "Abdourahmane", "", "G4-IPD"], ["Sall", "Amadou", "", "G4-IPD"], ["Loucoubar", "Cheikh", "", "G4-IPD"], ["Gassiat", "Elisabeth", "", "LM-Orsay"]]}, {"id": "1801.04289", "submitter": "Saad Mohamad", "authors": "Saad Mohamad, Abdelhamid Bouchachia, Moamar Sayed-Mouchaweh", "title": "Asynchronous Stochastic Variational Inference", "comments": "7 pages, 8 figures, 1 table, 2 algorithms, The paper has been\n  submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) employs stochastic optimization to\nscale up Bayesian computation to massive data. Since SVI is at its core a\nstochastic gradient-based algorithm, horizontal parallelism can be harnessed to\nallow larger scale inference. We propose a lock-free parallel implementation\nfor SVI which allows distributed computations over multiple slaves in an\nasynchronous style. We show that our implementation leads to linear speed-up\nwhile guaranteeing an asymptotic ergodic convergence rate $O(1/\\sqrt(T)$ )\ngiven that the number of slaves is bounded by $\\sqrt(T)$ ($T$ is the total\nnumber of iterations). The implementation is done in a high-performance\ncomputing (HPC) environment using message passing interface (MPI) for python\n(MPI4py). The extensive empirical evaluation shows that our parallel SVI is\nlossless, performing comparably well to its counterpart serial SVI with linear\nspeed-up.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 19:05:09 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Mohamad", "Saad", ""], ["Bouchachia", "Abdelhamid", ""], ["Sayed-Mouchaweh", "Moamar", ""]]}, {"id": "1801.04295", "submitter": "Ankit Pensia", "authors": "Ankit Pensia, Varun Jog and Po-Ling Loh", "title": "Generalization Error Bounds for Noisy, Iterative Algorithms", "comments": "A shorter version of this paper was submitted to ISIT 2018. 14 pages,\n  1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical learning theory, generalization error is used to quantify the\ndegree to which a supervised machine learning algorithm may overfit to training\ndata. Recent work [Xu and Raginsky (2017)] has established a bound on the\ngeneralization error of empirical risk minimization based on the mutual\ninformation $I(S;W)$ between the algorithm input $S$ and the algorithm output\n$W$, when the loss function is sub-Gaussian. We leverage these results to\nderive generalization error bounds for a broad class of iterative algorithms\nthat are characterized by bounded, noisy updates with Markovian structure. Our\nbounds are very general and are applicable to numerous settings of interest,\nincluding stochastic gradient Langevin dynamics (SGLD) and variants of the\nstochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm. Furthermore, our\nerror bounds hold for any output function computed over the path of iterates,\nincluding the last iterate of the algorithm or the average of subsets of\niterates, and also allow for non-uniform sampling of data in successive updates\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 19:26:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Pensia", "Ankit", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1801.04339", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Yihong Wu", "title": "Estimating the Number of Connected Components in a Graph via Subgraph\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning properties of large graphs from samples has been an important\nproblem in statistical network analysis since the early work of Goodman\n\\cite{Goodman1949} and Frank \\cite{Frank1978}. We revisit a problem formulated\nby Frank \\cite{Frank1978} of estimating the number of connected components in a\nlarge graph based on the subgraph sampling model, in which we randomly sample a\nsubset of the vertices and observe the induced subgraph. The key question is\nwhether accurate estimation is achievable in the \\emph{sublinear} regime where\nonly a vanishing fraction of the vertices are sampled. We show that it is\nimpossible if the parent graph is allowed to contain high-degree vertices or\nlong induced cycles. For the class of chordal graphs, where induced cycles of\nlength four or above are forbidden, we characterize the optimal sample\ncomplexity within constant factors and construct linear-time estimators that\nprovably achieve these bounds. This significantly expands the scope of previous\nresults which have focused on unbiased estimators and special classes of graphs\nsuch as forests or cliques.\n  Both the construction and the analysis of the proposed methodology rely on\ncombinatorial properties of chordal graphs and identities of induced subgraph\ncounts. They, in turn, also play a key role in proving minimax lower bounds\nbased on construction of random instances of graphs with matching structures of\nsmall subgraphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:13:48 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 21:28:29 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 21:13:30 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Wu", "Yihong", ""]]}, {"id": "1801.04342", "submitter": "Forough Arabshahi", "authors": "Forough Arabshahi, Sameer Singh, Animashree Anandkumar", "title": "Combining Symbolic Expressions and Black-box Function Evaluations in\n  Neural Programs", "comments": "Published as a conference paper at the sixth International Conference\n  on Learning Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural programming involves training neural networks to learn programs,\nmathematics, or logic from data. Previous works have failed to achieve good\ngeneralization performance, especially on problems and programs with high\ncomplexity or on large domains. This is because they mostly rely either on\nblack-box function evaluations that do not capture the structure of the\nprogram, or on detailed execution traces that are expensive to obtain, and\nhence the training data has poor coverage of the domain under consideration. We\npresent a novel framework that utilizes black-box function evaluations, in\nconjunction with symbolic expressions that define relationships between the\ngiven functions. We employ tree LSTMs to incorporate the structure of the\nsymbolic expression trees. We use tree encoding for numbers present in function\nevaluation data, based on their decimal representation. We present an\nevaluation benchmark for this task to demonstrate our proposed model combines\nsymbolic reasoning and function evaluation in a fruitful manner, obtaining high\naccuracies in our experiments. Our framework generalizes significantly better\nto expressions of higher depth and is able to fill partial equations with valid\ncompletions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:24:42 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 22:45:02 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 23:36:01 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Arabshahi", "Forough", ""], ["Singh", "Sameer", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1801.04378", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Sajad Khodadadian, Negar Kiyavash", "title": "Fairness in Supervised Learning: An Information Theoretic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated decision making systems are increasingly being used in real-world\napplications. In these systems for the most part, the decision rules are\nderived by minimizing the training error on the available historical data.\nTherefore, if there is a bias related to a sensitive attribute such as gender,\nrace, religion, etc. in the data, say, due to cultural/historical\ndiscriminatory practices against a certain demographic, the system could\ncontinue discrimination in decisions by including the said bias in its decision\nrule. We present an information theoretic framework for designing fair\npredictors from data, which aim to prevent discrimination against a specified\nsensitive attribute in a supervised learning setting. We use equalized odds as\nthe criterion for discrimination, which demands that the prediction should be\nindependent of the protected attribute conditioned on the actual label. To\nensure fairness and generalization simultaneously, we compress the data to an\nauxiliary variable, which is used for the prediction task. This auxiliary\nvariable is chosen such that it is decontaminated from the discriminatory\nattribute in the sense of equalized odds. The final predictor is obtained by\napplying a Bayesian decision rule to the auxiliary variable.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 04:03:04 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 21:49:01 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Khodadadian", "Sajad", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1801.04503", "submitter": "Fazle Karim", "authors": "Fazle Karim, Somshubra Majumdar, Houshang Darabi, Samuel Harford", "title": "Multivariate LSTM-FCNs for Time Series Classification", "comments": "18 pages, 2 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.neunet.2019.04.014", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, multivariate time series classification has received\ngreat attention. We propose transforming the existing univariate time series\nclassification models, the Long Short Term Memory Fully Convolutional Network\n(LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series\nclassification model by augmenting the fully convolutional block with a\nsqueeze-and-excitation block to further improve accuracy. Our proposed models\noutperform most state-of-the-art models while requiring minimum preprocessing.\nThe proposed models work efficiently on various complex multivariate time\nseries classification tasks such as activity recognition or action recognition.\nFurthermore, the proposed models are highly efficient at test time and small\nenough to deploy on memory constrained systems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 03:00:53 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 23:51:37 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Karim", "Fazle", ""], ["Majumdar", "Somshubra", ""], ["Darabi", "Houshang", ""], ["Harford", "Samuel", ""]]}, {"id": "1801.04510", "submitter": "Jia Wu", "authors": "Chenglong Dai, Jia Wu, Dechang Pi, Lin Cui", "title": "Brain EEG Time Series Selection: A Novel Graph-Based Approach for\n  Classification", "comments": "9 pages, 5 figures, Accepted by SDM-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain Electroencephalography (EEG) classification is widely applied to\nanalyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs\ndegrade the diagnosis performance and most previously developed methods ignore\nthe necessity of EEG selection for classification. To this end, this paper\nproposes a novel maximum weight clique-based EEG selection approach, named\nmwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques\nfrom an improved Fr\\'{e}chet distance-weighted undirected EEG graph\nsimultaneously considering edge weights and vertex weights. Our mwcEEGs\nimproves the classification performance by selecting intra-clique pairwise\nsimilar and inter-clique discriminative EEGs with similarity threshold\n$\\delta$. Experimental results demonstrate the algorithm effectiveness compared\nwith the state-of-the-art time series selection algorithms on real-world EEG\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 04:51:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 06:19:21 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Dai", "Chenglong", ""], ["Wu", "Jia", ""], ["Pi", "Dechang", ""], ["Cui", "Lin", ""]]}, {"id": "1801.04540", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Fix your classifier: the marginal value of training the last weight\n  layer", "comments": "https://openreview.net/forum?id=S1Dh8Tg0-", "journal-ref": "International Conference on Learning Representations 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are commonly used as models for classification for a wide\nvariety of tasks. Typically, a learned affine transformation is placed at the\nend of such models, yielding a per-class value used for classification. This\nclassifier can have a vast number of parameters, which grows linearly with the\nnumber of possible classes, thus requiring increasingly more resources. In this\nwork we argue that this classifier can be fixed, up to a global scale constant,\nwith little or no loss of accuracy for most tasks, allowing memory and\ncomputational benefits. Moreover, we show that by initializing the classifier\nwith a Hadamard matrix we can speed up inference as well. We discuss the\nimplications for current understanding of neural network models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 12:00:43 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 08:56:25 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Soudry", "Daniel", ""]]}, {"id": "1801.04590", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Raviteja Vemulapalli and Matthew Brown", "title": "Frame-Recurrent Video Super-Resolution", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video super-resolution have shown that convolutional\nneural networks combined with motion compensation are able to merge information\nfrom multiple low-resolution (LR) frames to generate high-quality images.\nCurrent state-of-the-art methods process a batch of LR frames to generate a\nsingle high-resolution (HR) frame and run this scheme in a sliding window\nfashion over the entire video, effectively treating the problem as a large\nnumber of separate multi-frame super-resolution tasks. This approach has two\nmain weaknesses: 1) Each input frame is processed and warped multiple times,\nincreasing the computational cost, and 2) each output frame is estimated\nindependently conditioned on the input frames, limiting the system's ability to\nproduce temporally consistent results.\n  In this work, we propose an end-to-end trainable frame-recurrent video\nsuper-resolution framework that uses the previously inferred HR estimate to\nsuper-resolve the subsequent frame. This naturally encourages temporally\nconsistent results and reduces the computational cost by warping only one image\nin each step. Furthermore, due to its recurrent nature, the proposed method has\nthe ability to assimilate a large number of previous frames without increased\ncomputational demands. Extensive evaluations and comparisons with previous\nmethods validate the strengths of our approach and demonstrate that the\nproposed framework is able to significantly outperform the current state of the\nart.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 17:53:53 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 12:28:58 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 00:38:35 GMT"}, {"version": "v4", "created": "Sun, 25 Mar 2018 17:24:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Vemulapalli", "Raviteja", ""], ["Brown", "Matthew", ""]]}, {"id": "1801.04693", "submitter": "Bo Luo", "authors": "Bo Luo, Yannan Liu, Lingxiao Wei, Qiang Xu", "title": "Towards Imperceptible and Robust Adversarial Example Attacks against\n  Neural Networks", "comments": "Adversarial example attacks, Robust and Imperceptible, Human\n  perceptual system, Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems based on deep neural networks, being able to produce\nstate-of-the-art results on various perception tasks, have gained mainstream\nadoption in many applications. However, they are shown to be vulnerable to\nadversarial example attack, which generates malicious output by adding slight\nperturbations to the input. Previous adversarial example crafting methods,\nhowever, use simple metrics to evaluate the distances between the original\nexamples and the adversarial ones, which could be easily detected by human\neyes. In addition, these attacks are often not robust due to the inevitable\nnoises and deviation in the physical world. In this work, we present a new\nadversarial example attack crafting method, which takes the human perceptual\nsystem into consideration and maximizes the noise tolerance of the crafted\nadversarial example. Experimental results demonstrate the efficacy of the\nproposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 08:15:33 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Luo", "Bo", ""], ["Liu", "Yannan", ""], ["Wei", "Lingxiao", ""], ["Xu", "Qiang", ""]]}, {"id": "1801.04695", "submitter": "Soorya Gopalakrishnan", "authors": "Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, Ramtin Pedarsani", "title": "Sparsity-based Defense against Adversarial Attacks on Linear Classifiers", "comments": "Published in IEEE International Symposium on Information Theory\n  (ISIT) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks represent the state of the art in machine learning in a\ngrowing number of fields, including vision, speech and natural language\nprocessing. However, recent work raises important questions about the\nrobustness of such architectures, by showing that it is possible to induce\nclassification errors through tiny, almost imperceptible, perturbations.\nVulnerability to such \"adversarial attacks\", or \"adversarial examples\", has\nbeen conjectured to be due to the excessive linearity of deep networks. In this\npaper, we study this phenomenon in the setting of a linear classifier, and show\nthat it is possible to exploit sparsity in natural data to combat\n$\\ell_{\\infty}$-bounded adversarial perturbations. Specifically, we demonstrate\nthe efficacy of a sparsifying front end via an ensemble averaged analysis, and\nexperimental results for the MNIST handwritten digit database. To the best of\nour knowledge, this is the first work to show that sparsity provides a\ntheoretically rigorous framework for defense against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 08:18:33 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 23:21:58 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 07:16:51 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Marzi", "Zhinus", ""], ["Gopalakrishnan", "Soorya", ""], ["Madhow", "Upamanyu", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "1801.04701", "submitter": "Ao Zhang", "authors": "Ao Zhang, Nan Li, Jian Pu, Jun Wang, Junchi Yan, Hongyuan Zha", "title": "tau-FPL: Tolerance-Constrained Learning in Linear Time", "comments": "32 pages, 3 figures. This is an extended version of our paper\n  published in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a classifier with control on the false-positive rate plays a\ncritical role in many machine learning applications. Existing approaches either\nintroduce prior knowledge dependent label cost or tune parameters based on\ntraditional classifiers, which lack consistency in methodology because they do\nnot strictly adhere to the false-positive rate constraint. In this paper, we\npropose a novel scoring-thresholding approach, tau-False Positive Learning\n(tau-FPL) to address this problem. We show the scoring problem which takes the\nfalse-positive rate tolerance into accounts can be efficiently solved in linear\ntime, also an out-of-bootstrap thresholding method can transform the learned\nranking function into a low false-positive classifier. Both theoretical\nanalysis and experimental results show superior performance of the proposed\ntau-FPL over existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 08:56:49 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Zhang", "Ao", ""], ["Li", "Nan", ""], ["Pu", "Jian", ""], ["Wang", "Jun", ""], ["Yan", "Junchi", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1801.04813", "submitter": "Quan Hoang", "authors": "Quan Hoang", "title": "Predicting Movie Genres Based on Plot Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project explores several Machine Learning methods to predict movie\ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent\nNeural Networks are used for text classification, while K-binary\ntransformation, rank method and probabilistic classification with learned\nprobability threshold are employed for the multi-label problem involved in the\ngenre tagging task.Experiments with more than 250,000 movies show that\nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic\nclassification with learned probability threshold approach achieves the best\nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score\nof 0.56, and a hit rate of 80.5%.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 14:11:57 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Hoang", "Quan", ""]]}, {"id": "1801.04856", "submitter": "Hao Peng", "authors": "Hao Peng, Xiaoli Bai", "title": "Improving Orbit Prediction Accuracy through Supervised Machine Learning", "comments": "30 pages, 21 figures, 4 tables, Preprint submitted to Advances in\n  Space Research, on December 14, 2017", "journal-ref": null, "doi": "10.1016/j.asr.2018.03.001", "report-no": null, "categories": "astro-ph.EP cs.CE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the lack of information such as the space environment condition and\nresident space objects' (RSOs') body characteristics, current orbit predictions\nthat are solely grounded on physics-based models may fail to achieve required\naccuracy for collision avoidance and have led to satellite collisions already.\nThis paper presents a methodology to predict RSOs' trajectories with higher\naccuracy than that of the current methods. Inspired by the machine learning\n(ML) theory through which the models are learned based on large amounts of\nobserved data and the prediction is conducted without explicitly modeling space\nobjects and space environment, the proposed ML approach integrates\nphysics-based orbit prediction algorithms with a learning-based process that\nfocuses on reducing the prediction errors. Using a simulation-based space\ncatalog environment as the test bed, the paper demonstrates three types of\ngeneralization capability for the proposed ML approach: 1) the ML model can be\nused to improve the same RSO's orbit information that is not available during\nthe learning process but shares the same time interval as the training data; 2)\nthe ML model can be used to improve predictions of the same RSO at future\nepochs; and 3) the ML model based on a RSO can be applied to other RSOs that\nshare some common features.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 15:56:36 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Peng", "Hao", ""], ["Bai", "Xiaoli", ""]]}, {"id": "1801.04929", "submitter": "Mario Michael Krell", "authors": "Mario Michael Krell", "title": "Generalizing, Decoding, and Optimizing Support Vector Machine\n  Classification", "comments": null, "journal-ref": "PhD Thesis, University of Bremen, Bremen, 1-236, 2015", "doi": null, "report-no": "urn:nbn:de:gbv:46-00104380-12", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of complex data usually requires the composition of\nprocessing steps. Here, a major challenge is the selection of optimal\nalgorithms for preprocessing and classification (including parameterizations).\nNowadays, parts of the optimization process are automized but expert knowledge\nand manual work are still required. We present three steps to face this process\nand ease the optimization. Namely, we take a theoretical view on classical\nclassifiers, provide an approach to interpret the classifier together with the\npreprocessing, and integrate both into one framework which enables a\nsemiautomatic optimization of the processing chain and which interfaces\nnumerous algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 16:49:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Krell", "Mario Michael", ""]]}, {"id": "1801.04958", "submitter": "Robert Giaquinto", "authors": "Robert Giaquinto and Arindam Banerjee", "title": "Topic Modeling on Health Journals with Regularized Variational Inference", "comments": "Published in Thirty-Second AAAI Conference on Artificial\n  Intelligence, February 2018, New Orleans, Louisiana, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling enables exploration and compact representation of a corpus.\nThe CaringBridge (CB) dataset is a massive collection of journals written by\npatients and caregivers during a health crisis. Topic modeling on the CB\ndataset, however, is challenging due to the asynchronous nature of multiple\nauthors writing about their health journeys. To overcome this challenge we\nintroduce the Dynamic Author-Persona topic model (DAP), a probabilistic\ngraphical model designed for temporal corpora with multiple authors. The\nnovelty of the DAP model lies in its representation of authors by a persona ---\nwhere personas capture the propensity to write about certain topics over time.\nFurther, we present a regularized variational inference algorithm, which we use\nto encourage the DAP model's personas to be distinct. Our results show\nsignificant improvements over competing topic models --- particularly after\nregularization, and highlight the DAP model's unique ability to capture common\njourneys shared by different authors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 19:23:21 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Giaquinto", "Robert", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1801.04987", "submitter": "Jos\\'e Bento", "authors": "Jose Bento, Ralph Furmaniak, Surjyendu Ray", "title": "On the Complexity of the Weighted Fused Lasso", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2867800", "report-no": null, "categories": "cs.LG cs.CC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solution path of the 1D fused lasso for an $n$-dimensional input is\npiecewise linear with $\\mathcal{O}(n)$ segments (Hoefling et al. 2010 and\nTibshirani et al 2011). However, existing proofs of this bound do not hold for\nthe weighted fused lasso. At the same time, results for the generalized lasso,\nof which the weighted fused lasso is a special case, allow $\\Omega(3^n)$\nsegments (Mairal et al. 2012). In this paper, we prove that the number of\nsegments in the solution path of the weighted fused lasso is\n$\\mathcal{O}(n^2)$, and that, for some instances, it is $\\Omega(n^2)$. We also\ngive a new, very simple, proof of the $\\mathcal{O}(n)$ bound for the fused\nlasso.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:20:53 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 16:03:24 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 20:02:47 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Bento", "Jose", ""], ["Furmaniak", "Ralph", ""], ["Ray", "Surjyendu", ""]]}, {"id": "1801.05007", "submitter": "Qi Liu", "authors": "Qi Liu, Anindya Bhadra, and William S. Cleveland", "title": "Divide and Recombine for Large and Complex Data: Model Likelihood\n  Functions using MCMC", "comments": "6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Divide & Recombine (D&R), big data are divided into subsets, each analytic\nmethod is applied to subsets, and the outputs are recombined. This enables deep\nanalysis and practical computational performance. An innovate D\\&R procedure is\nproposed to compute likelihood functions of data-model (DM) parameters for big\ndata. The likelihood-model (LM) is a parametric probability density function of\nthe DM parameters. The density parameters are estimated by fitting the density\nto MCMC draws from each subset DM likelihood function, and then the fitted\ndensities are recombined. The procedure is illustrated using normal and\nskew-normal LMs for the logistic regression DM.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:53:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Liu", "Qi", ""], ["Bhadra", "Anindya", ""], ["Cleveland", "William S.", ""]]}, {"id": "1801.05039", "submitter": "Rong Ge", "authors": "Maryam Fazel, Rong Ge, Sham M. Kakade, Mehran Mesbahi", "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic\n  Regulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct policy gradient methods for reinforcement learning and continuous\ncontrol problems are a popular approach for a variety of reasons: 1) they are\neasy to implement without explicit knowledge of the underlying model 2) they\nare an \"end-to-end\" approach, directly optimizing the performance metric of\ninterest 3) they inherently allow for richly parameterized policies. A notable\ndrawback is that even in the most basic continuous control problem (that of\nlinear quadratic regulators), these methods must solve a non-convex\noptimization problem, where little is understood about their efficiency from\nboth computational and statistical perspectives. In contrast, system\nidentification and model based planning in optimal control theory have a much\nmore solid theoretical footing, where much is known with regards to their\ncomputational and statistical properties. This work bridges this gap showing\nthat (model free) policy gradient methods globally converge to the optimal\nsolution and are efficient (polynomially so in relevant problem dependent\nquantities) with regards to their sample and computational complexities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 21:40:50 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 13:15:27 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2019 20:29:16 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Fazel", "Maryam", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "1801.05055", "submitter": "Edward Raff", "authors": "Edward Raff and Charles Nicholas", "title": "Toward Metric Indexes for Incremental Insertion and Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the use of metric index structures, which accelerate\nnearest neighbor queries, in the scenario where we need to interleave\ninsertions and queries during deployment. This use-case is inspired by a\nreal-life need in malware analysis triage, and is surprisingly understudied.\nExisting literature tends to either focus on only final query efficiency, often\ndoes not support incremental insertion, or does not support arbitrary distance\nmetrics. We modify and improve three algorithms to support our scenario of\nincremental insertion and querying with arbitrary metrics, and evaluate them on\nmultiple datasets and distance metrics while varying the value of $k$ for the\ndesired number of nearest neighbors. In doing so we determine that our improved\nVantage-Point tree of Minimum-Variance performs best for this scenario.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:25:16 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""]]}, {"id": "1801.05062", "submitter": "Xinyuan Zhang", "authors": "Xinyuan Zhang, Ricardo Henao, Zhe Gan, Yitong Li, Lawrence Carin", "title": "Multi-Label Learning from Medical Plain Text with Convolutional Residual\n  Models", "comments": "Machine Learning for Healthcare 2018 spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting diagnoses from Electronic Health Records (EHRs) is an important\nmedical application of multi-label learning. We propose a convolutional\nresidual model for multi-label classification from doctor notes in EHR data. A\ngiven patient may have multiple diagnoses, and therefore multi-label learning\nis required. We employ a Convolutional Neural Network (CNN) to encode plain\ntext into a fixed-length sentence embedding vector. Since diagnoses are\ntypically correlated, a deep residual network is employed on top of the CNN\nencoder, to capture label (diagnosis) dependencies and incorporate information\ndirectly from the encoded sentence vector. A real EHR dataset is considered,\nand we compare the proposed model with several well-known baselines, to predict\ndiagnoses based on doctor notes. Experimental results demonstrate the\nsuperiority of the proposed convolutional residual model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 22:59:17 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 19:36:06 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Zhang", "Xinyuan", ""], ["Henao", "Ricardo", ""], ["Gan", "Zhe", ""], ["Li", "Yitong", ""], ["Carin", "Lawrence", ""]]}, {"id": "1801.05134", "submitter": "Xiang Li", "authors": "Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang", "title": "Understanding the Disharmony between Dropout and Batch Normalization by\n  Variance Shift", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper first answers the question \"why do the two most powerful\ntechniques Dropout and Batch Normalization (BN) often lead to a worse\nperformance when they are combined together?\" in both theoretical and\nstatistical aspects. Theoretically, we find that Dropout would shift the\nvariance of a specific neural unit when we transfer the state of that network\nfrom train to test. However, BN would maintain its statistical variance, which\nis accumulated from the entire learning procedure, in the test phase. The\ninconsistency of that variance (we name this scheme as \"variance shift\") causes\nthe unstable numerical behavior in inference that leads to more erroneous\npredictions finally, when applying Dropout before BN. Thorough experiments on\nDenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to\nthe uncovered mechanism, we next explore several strategies that modifies\nDropout and try to overcome the limitations of their combination by avoiding\nthe variance shift risks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 06:47:59 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Li", "Xiang", ""], ["Chen", "Shuo", ""], ["Hu", "Xiaolin", ""], ["Yang", "Jian", ""]]}, {"id": "1801.05141", "submitter": "Kazi Nazmul Haque", "authors": "Kazi Nazmul Haque, Mohammad Abu Yousuf, Rajib Rana", "title": "Image denoising and restoration with CNN-LSTM Encoder Decoder with\n  Direct Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is always a challenging task in the field of computer vision\nand image processing. In this paper, we have proposed an encoder-decoder model\nwith direct attention, which is capable of denoising and reconstruct highly\ncorrupted images. Our model consists of an encoder and a decoder, where the\nencoder is a convolutional neural network and decoder is a multilayer Long\nShort-Term memory network. In the proposed model, the encoder reads an image\nand catches the abstraction of that image in a vector, where decoder takes that\nvector as well as the corrupted image to reconstruct a clean image. We have\ntrained our model on MNIST handwritten digit database after making lower half\nof every image as black as well as adding noise top of that. After a massive\ndestruction of the images where it is hard for a human to understand the\ncontent of those images, our model can retrieve that image with minimal error.\nOur proposed model has been compared with convolutional encoder-decoder, where\nour model has performed better at generating missing part of the images than\nconvolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 07:27:46 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Haque", "Kazi Nazmul", ""], ["Yousuf", "Mohammad Abu", ""], ["Rana", "Rajib", ""]]}, {"id": "1801.05236", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks, Juan Miguel L. Andres, Ryan Baker", "title": "MORF: A Framework for Predictive Modeling and Replication At Scale With\n  Privacy-Restricted MOOC Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data repositories from online learning platforms such as Massive Open\nOnline Courses (MOOCs) represent an unprecedented opportunity to advance\nresearch on education at scale and impact a global population of learners. To\ndate, such research has been hindered by poor reproducibility and a lack of\nreplication, largely due to three types of barriers: experimental, inferential,\nand data. We present a novel system for large-scale computational research, the\nMOOC Replication Framework (MORF), to jointly address these barriers. We\ndiscuss MORF's architecture, an open-source platform-as-a-service (PaaS) which\nincludes a simple, flexible software API providing for multiple modes of\nresearch (predictive modeling or production rule analysis) integrated with a\nhigh-performance computing environment. All experiments conducted on MORF use\nexecutable Docker containers which ensure complete reproducibility while\nallowing for the use of any software or language which can be installed in the\nlinux-based Docker container. Each experimental artifact is assigned a DOI and\nmade publicly available. MORF has the potential to accelerate and democratize\nresearch on its massive data repository, which currently includes over 200\nMOOCs, as demonstrated by initial research conducted on the platform. We also\nhighlight ways in which MORF represents a solution template to a more general\nclass of problems faced by computational researchers in other domains.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 13:06:12 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 13:10:38 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 22:16:21 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""], ["Andres", "Juan Miguel L.", ""], ["Baker", "Ryan", ""]]}, {"id": "1801.05394", "submitter": "Wei-Han Lee", "authors": "Wei-Han Lee, Jorge Ortiz, Bongjun Ko, Ruby Lee", "title": "Time Series Segmentation through Automatic Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of things (IoT) applications have become increasingly popular in\nrecent years, with applications ranging from building energy monitoring to\npersonal health tracking and activity recognition. In order to leverage these\ndata, automatic knowledge extraction - whereby we map from observations to\ninterpretable states and transitions - must be done at scale. As such, we have\nseen many recent IoT data sets include annotations with a human expert\nspecifying states, recorded as a set of boundaries and associated labels in a\ndata sequence. These data can be used to build automatic labeling algorithms\nthat produce labels as an expert would. Here, we refer to human-specified\nboundaries as breakpoints. Traditional changepoint detection methods only look\nfor statistically-detectable boundaries that are defined as abrupt variations\nin the generative parameters of a data sequence. However, we observe that\nbreakpoints occur on more subtle boundaries that are non-trivial to detect with\nthese statistical methods. In this work, we propose a new unsupervised\napproach, based on deep learning, that outperforms existing techniques and\nlearns the more subtle, breakpoint boundaries with a high accuracy. Through\nextensive experiments on various real-world data sets - including\nhuman-activity sensing data, speech signals, and electroencephalogram (EEG)\nactivity traces - we demonstrate the effectiveness of our algorithm for\npractical applications. Furthermore, we show that our approach achieves\nsignificantly better performance than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:05:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 11:51:31 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Lee", "Wei-Han", ""], ["Ortiz", "Jorge", ""], ["Ko", "Bongjun", ""], ["Lee", "Ruby", ""]]}, {"id": "1801.05398", "submitter": "Hao Wang", "authors": "Hao Wang, Berk Ustun, Flavio P. Calmon", "title": "On the Direction of Discrimination: An Information-Theoretic Analysis of\n  Disparate Impact in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of machine learning, disparate impact refers to a form of\nsystematic discrimination whereby the output distribution of a model depends on\nthe value of a sensitive attribute (e.g., race or gender). In this paper, we\npropose an information-theoretic framework to analyze the disparate impact of a\nbinary classification model. We view the model as a fixed channel, and quantify\ndisparate impact as the divergence in output distributions over two groups. Our\naim is to find a correction function that can perturb the input distributions\nof each group to align their output distributions. We present an optimization\nproblem that can be solved to obtain a correction function that will make the\noutput distributions statistically indistinguishable. We derive closed-form\nexpressions to efficiently compute the correction function, and demonstrate the\nbenefits of our framework on a recidivism prediction problem based on the\nProPublica COMPAS dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:26:56 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 00:19:56 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 17:57:11 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Wang", "Hao", ""], ["Ustun", "Berk", ""], ["Calmon", "Flavio P.", ""]]}, {"id": "1801.05407", "submitter": "Neil Mallinar", "authors": "Neil Mallinar and Corbin Rosset", "title": "Deep Canonically Correlated LSTMs", "comments": "8 pages, 3 figures, accepted as the undergraduate honors thesis for\n  Neil Mallinar by The Johns Hopkins University", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\ntransformations of variable length sequences and embed them into a correlated,\nfixed dimensional space. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the data. We then\nperform correlation analysis on the outputs of these neural networks to find a\ncorrelated subspace through which we get our final representation via\nprojection. This work follows from previous work done on Deep Canonical\nCorrelation (DCCA), in which deep feed-forward neural networks were used to\nlearn nonlinear transformations of data while maximizing correlation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:44:31 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Mallinar", "Neil", ""], ["Rosset", "Corbin", ""]]}, {"id": "1801.05413", "submitter": "Thomas M\\\"ollenhoff", "authors": "Thomas M\\\"ollenhoff, Zhenzhang Ye, Tao Wu, Daniel Cremers", "title": "Combinatorial Preconditioners for Proximal Algorithms on Graphs", "comments": "Published as a conference paper at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel preconditioning technique for proximal optimization\nmethods that relies on graph algorithms to construct effective preconditioners.\nSuch combinatorial preconditioners arise from partitioning the graph into\nforests. We prove that certain decompositions lead to a theoretically optimal\ncondition number. We also show how ideal decompositions can be realized using\nmatroid partitioning and propose efficient greedy variants thereof for\nlarge-scale problems. Coupled with specialized solvers for the resulting scaled\nproximal subproblems, the preconditioned algorithm achieves competitive\nperformance in machine learning and vision applications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:50:13 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 11:18:24 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Ye", "Zhenzhang", ""], ["Wu", "Tao", ""], ["Cremers", "Daniel", ""]]}, {"id": "1801.05453", "submitter": "William Murdoch", "authors": "W. James Murdoch, Peter J. Liu, Bin Yu", "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions\n  from LSTMs", "comments": "Oral presentation at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The driving force behind the recent success of LSTMs has been their ability\nto learn complex and non-linear relationships. Consequently, our inability to\ndescribe these relationships has led to LSTMs being characterized as black\nboxes. To this end, we introduce contextual decomposition (CD), an\ninterpretation algorithm for analysing individual predictions made by standard\nLSTMs, without any changes to the underlying model. By decomposing the output\nof a LSTM, CD captures the contributions of combinations of words or variables\nto the final prediction of an LSTM. On the task of sentiment analysis with the\nYelp and SST data sets, we show that CD is able to reliably identify words and\nphrases of contrasting sentiment, and how they are combined to yield the LSTM's\nfinal prediction. Using the phrase-level labels in SST, we also demonstrate\nthat CD is able to successfully extract positive and negative negations from an\nLSTM, something which has not previously been done.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 19:21:48 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 22:25:53 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Murdoch", "W. James", ""], ["Liu", "Peter J.", ""], ["Yu", "Bin", ""]]}, {"id": "1801.05504", "submitter": "Fady Medhat", "authors": "Fady Medhat, David Chesmore and John Robinson", "title": "Automatic Classification of Music Genre using Masked Conditional Neural\n  Networks", "comments": "Restricted Boltzmann Machine; RBM; Conditional RBM; CRBM; Deep Belief\n  Net; DBN; Conditional Neural Network; CLNN; Masked Conditional Neural\n  Network; MCLNN; Music Information Retrieval; MIR. IEEE International\n  Conference on Data Mining (ICDM), 2017", "journal-ref": "IEEE International Conference on Data Mining (ICDM) Year: 2017\n  Pages: 979 - 984", "doi": "10.1109/ICDM.2017.125", "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based architectures used for sound recognition are usually\nadapted from other application domains such as image recognition, which may not\nharness the time-frequency representation of a signal. The ConditionaL Neural\nNetworks (CLNN) and its extension the Masked ConditionaL Neural Networks\n(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN\nis trained over a window of frames to preserve the inter-frame relation, and\nthe MCLNN enforces a systematic sparseness over the network's links that mimics\na filterbank-like behavior. The masking operation induces the network to learn\nin frequency bands, which decreases the network susceptibility to\nfrequency-shifts in time-frequency representations. Additionally, the mask\nallows an exploration of a range of feature combinations concurrently analogous\nto the manual handcrafting of the optimum collection of features for a\nrecognition task. MCLNN have achieved competitive performance on the Ballroom\nmusic dataset compared to several hand-crafted attempts and outperformed models\nbased on state-of-the-art Convolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 23:43:34 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 09:00:47 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Medhat", "Fady", ""], ["Chesmore", "David", ""], ["Robinson", "John", ""]]}, {"id": "1801.05512", "submitter": "Stephane Fotso", "authors": "Stephane Fotso", "title": "Deep Neural Networks for Survival Analysis Based on a Multi-Task\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis/time-to-event models are extremely useful as they can help\ncompanies predict when a customer will buy a product, churn or default on a\nloan, and therefore help them improve their ROI. In this paper, we introduce a\nnew method to calculate survival functions using the Multi-Task Logistic\nRegression (MTLR) model as its base and a deep learning architecture as its\ncore. Based on the Concordance index (C-index) and Brier score, this method\noutperforms the MTLR in all the experiments disclosed in this paper as well as\nthe Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are\nfound.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 00:53:35 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Fotso", "Stephane", ""]]}, {"id": "1801.05558", "submitter": "Yoonho Lee", "authors": "Yoonho Lee and Seungjin Choi", "title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based meta-learning methods leverage gradient descent to learn the\ncommonalities among various tasks. While previous such methods have been\nsuccessful in meta-learning tasks, they resort to simple gradient descent\nduring meta-testing. Our primary contribution is the {\\em MT-net}, which\nenables the meta-learner to learn on each layer's activation space a subspace\nthat the task-specific learner performs gradient descent on. Additionally, a\ntask-specific learner of an {\\em MT-net} performs gradient descent with respect\nto a meta-learned distance metric, which warps the activation space to be more\nsensitive to task identity. We demonstrate that the dimension of this learned\nsubspace reflects the complexity of the task-specific learner's adaptation\ntask, and also that our model is less sensitive to the choice of initial\nlearning rates than previous gradient-based meta-learning methods. Our method\nachieves state-of-the-art or comparable performance on few-shot classification\nand regression tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 05:34:08 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 07:40:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 12:33:23 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Lee", "Yoonho", ""], ["Choi", "Seungjin", ""]]}, {"id": "1801.05566", "submitter": "Jiaming Song", "authors": "Jiaming Song and Yuhuai Wu", "title": "An Empirical Analysis of Proximal Policy Optimization with\n  Kronecker-factored Natural Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we consider an approach that combines the PPO\nobjective and K-FAC natural gradient optimization, for which we call PPOKFAC.\nWe perform a range of empirical analysis on various aspects of the algorithm,\nsuch as sample complexity, training speed, and sensitivity to batch size and\ntraining epochs. We observe that PPOKFAC is able to outperform PPO in terms of\nsample complexity and speed in a range of MuJoCo environments, while being\nscalable in terms of batch size. In spite of this, it seems that adding more\nepochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to\nbe worse than its A2C counterpart, ACKTR.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 06:09:09 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Song", "Jiaming", ""], ["Wu", "Yuhuai", ""]]}, {"id": "1801.05574", "submitter": "Ying Lu", "authors": "Ying Lu, Liming Chen, Alexandre Saidi, Xianfeng Gu", "title": "Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 07:06:21 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Lu", "Ying", ""], ["Chen", "Liming", ""], ["Saidi", "Alexandre", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1801.05589", "submitter": "Franck Iutzeler", "authors": "Franck Iutzeler (1), Jerome Malick (1) ((1) DAO)", "title": "On the Proximal Gradient Algorithm with Alternated Inertia", "comments": "Journal of Optimization Theory and Applications, Springer Verlag, A\n  Para{\\^i}tre", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the attractive properties of the proximal\ngradient algorithm with inertia. Notably, we show that using alternated inertia\nyields monotonically decreasing functional values, which contrasts with usual\naccelerated proximal gradient methods. We also provide convergence rates for\nthe algorithm with alternated inertia based on local geometric properties of\nthe objective function. The results are put into perspective by discussions on\nseveral extensions and illustrations on common regularized problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 08:42:33 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Iutzeler", "Franck", "", "DAO"], ["Malick", "Jerome", "", "DAO"]]}, {"id": "1801.05772", "submitter": "Mastane Achab", "authors": "Stephan Cl\\'emen\\c{c}on, Mastane Achab", "title": "Ranking Data with Continuous Labels through Oriented Recursive\n  Partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a supervised learning problem, referred to as continuous\nranking, where a continuous real-valued label Y is assigned to an observable\nr.v. X taking its values in a feature space $\\mathcal{X}$ and the goal is to\norder all possible observations x in $\\mathcal{X}$ by means of a scoring\nfunction $s:\\mathcal{X}\\rightarrow \\mathbb{R}$ so that s(X) and Y tend to\nincrease or decrease together with highest probability. This problem\ngeneralizes bi/multi-partite ranking to a certain extent and the task of\nfinding optimal scoring functions s(x) can be naturally cast as optimization of\na dedicated functional criterion, called the IROC curve here, or as\nmaximization of the Kendall ${\\tau}$ related to the pair (s(X), Y ). From the\ntheoretical side, we describe the optimal elements of this problem and provide\nstatistical guarantees for empirical Kendall ${\\tau}$ maximization under\nappropriate conditions for the class of scoring function candidates. We also\npropose a recursive statistical learning algorithm tailored to empirical IROC\ncurve optimization and producing a piecewise constant scoring function that is\nfully described by an oriented binary tree. Preliminary numerical experiments\nhighlight the difference in nature between regression and continuous ranking\nand provide strong empirical evidence of the performance of empirical\noptimizers of the criteria proposed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 17:44:48 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Cl\u00e9men\u00e7on", "Stephan", ""], ["Achab", "Mastane", ""]]}, {"id": "1801.05787", "submitter": "Lucas Theis", "authors": "Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Husz\\'ar", "title": "Faster gaze prediction with dense networks and Fisher pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human fixations from images has recently seen large improvements\nby leveraging deep representations which were pretrained for object\nrecognition. However, as we show in this paper, these networks are highly\noverparameterized for the task of fixation prediction. We first present a\nsimple yet principled greedy pruning method which we call Fisher pruning.\nThrough a combination of knowledge distillation and Fisher pruning, we obtain\nmuch more runtime-efficient architectures for saliency prediction, achieving a\n10x speedup for the same AUC performance as a state of the art network on the\nCAT2000 dataset. Speeding up single-image gaze prediction is important for many\nreal-world applications, but it is also a crucial step in the development of\nvideo saliency models, where the amount of data to be processed is\nsubstantially larger.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 18:34:33 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 10:38:35 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Theis", "Lucas", ""], ["Korshunova", "Iryna", ""], ["Tejani", "Alykhan", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1801.05852", "submitter": "Daokun Zhang", "authors": "Daokun Zhang, Jie Yin, Xingquan Zhu and Chengqi Zhang", "title": "Network Representation Learning: A Survey", "comments": "Accepted by IEEE transactions on Big Data; 25 pages, 10 tables, 6\n  figures and 127 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the widespread use of information technologies, information networks are\nbecoming increasingly popular to capture complex relationships across various\ndisciplines, such as social networks, citation networks, telecommunication\nnetworks, and biological networks. Analyzing these networks sheds light on\ndifferent aspects of social life such as the structure of societies,\ninformation diffusion, and communication patterns. In reality, however, the\nlarge scale of information networks often makes network analytic tasks\ncomputationally expensive or intractable. Network representation learning has\nbeen recently proposed as a new learning paradigm to embed network vertices\ninto a low-dimensional vector space, by preserving network topology structure,\nvertex content, and other side information. This facilitates the original\nnetwork to be easily handled in the new vector space for further analysis. In\nthis survey, we perform a comprehensive review of the current literature on\nnetwork representation learning in the data mining and machine learning field.\nWe propose new taxonomies to categorize and summarize the state-of-the-art\nnetwork representation learning techniques according to the underlying learning\nmechanisms, the network information intended to preserve, as well as the\nalgorithmic designs and methodologies. We summarize evaluation protocols used\nfor validating network representation learning including published benchmark\ndatasets, evaluation methods, and open source algorithms. We also perform\nempirical studies to compare the performance of representative algorithms on\ncommon datasets, and analyze their computational complexity. Finally, we\nsuggest promising research directions to facilitate future study.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 03:28:54 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 05:16:49 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 09:11:08 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Zhang", "Daokun", ""], ["Yin", "Jie", ""], ["Zhu", "Xingquan", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1801.05856", "submitter": "Dan Kushnir", "authors": "Dan Kushnir, Benjamin Mirabelli", "title": "Active Community Detection with Maximal Expected Model Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel active learning algorithm for community detection on\nnetworks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)\ncriterion for querying network nodes label assignments. MEMC detects nodes that\nmaximally change the community assignment likelihood model following a query.\nOur method is inspired by detection in the benchmark Stochastic Block Model\n(SBM), where we provide sample complexity analysis and empirical study with SBM\nand real network data for binary as well as for the multi-class settings. The\nanalysis also covers the most challenging case of sparse degree and\nbelow-detection-threshold SBMs, where we observe a super-linear error\nreduction. MEMC is shown to be superior to the random selection baseline and\nother state-of-the-art active learners.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:26:16 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 19:39:14 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Kushnir", "Dan", ""], ["Mirabelli", "Benjamin", ""]]}, {"id": "1801.05894", "submitter": "Desmond Higham J", "authors": "Catherine F. Higham and Desmond J. Higham", "title": "Deep Learning: An Introduction for Applied Mathematicians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayered artificial neural networks are becoming a pervasive tool in a\nhost of application fields. At the heart of this deep learning revolution are\nfamiliar concepts from applied and computational mathematics; notably, in\ncalculus, approximation theory, optimization and linear algebra. This article\nprovides a very brief introduction to the basic ideas that underlie deep\nlearning from an applied mathematics perspective. Our target audience includes\npostgraduate and final year undergraduate students in mathematics who are keen\nto learn about the area. The article may also be useful for instructors in\nmathematics who wish to enliven their classes with references to the\napplication of deep learning techniques. We focus on three fundamental\nquestions: what is a deep neural network? how is a network trained? what is the\nstochastic gradient method? We illustrate the ideas with a short MATLAB code\nthat sets up and trains a network. We also show the use of state-of-the art\nsoftware on a large scale image classification problem. We finish with\nreferences to the current literature.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:05:25 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Higham", "Catherine F.", ""], ["Higham", "Desmond J.", ""]]}, {"id": "1801.05922", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Nata\\v{s}a Jonoska, Denys Kukushkin, Masahico Saito", "title": "Graph Based Analysis for Gene Segment Organization In a Scrambled Genome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA rearrangement processes recombine gene segments that are organized on the\nchromosome in a variety of ways. The segments can overlap, interleave or one\nmay be a subsegment of another. We use directed graphs to represent segment\norganizations on a given locus where contigs containing rearranged segments\nrepresent vertices and the edges correspond to the segment relationships. Using\ngraph properties we associate a point in a higher dimensional Euclidean space\nto each graph such that cluster formations and analysis can be performed with\nmethods from topological data analysis. The method is applied to a recently\nsequenced model organism \\textit{Oxytricha trifallax}, a species of ciliate\nwith highly scrambled genome that undergoes massive rearrangement process after\nconjugation. The analysis shows some emerging star-like graph structures\nindicating that segments of a single gene can interleave, or even contain all\nof the segments from fifteen or more other genes in between its segments. We\nalso observe that as many as six genes can have their segments mutually\ninterleaving or overlapping.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 03:30:13 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 15:37:01 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hajij", "Mustafa", ""], ["Jonoska", "Nata\u0161a", ""], ["Kukushkin", "Denys", ""], ["Saito", "Masahico", ""]]}, {"id": "1801.05931", "submitter": "Vinod Kumar Chauhan", "authors": "Vinod Kumar Chauhan, Anuj Sharma and Kalpana Dahiya", "title": "Faster Learning by Reduction of Data Access Time", "comments": "80 figures, final journal version", "journal-ref": "Applied Intelligence, Springer, 2018", "doi": "10.1007/s10489-018-1235-x", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the major challenge in machine learning is the Big Data challenge.\nThe big data problems due to large number of data points or large number of\nfeatures in each data point, or both, the training of models have become very\nslow. The training time has two major components: Time to access the data and\ntime to process (learn from) the data. So far, the research has focused only on\nthe second part, i.e., learning from the data. In this paper, we have proposed\none possible solution to handle the big data problems in machine learning. The\nidea is to reduce the training time through reducing data access time by\nproposing systematic sampling and cyclic/sequential sampling to select\nmini-batches from the dataset. To prove the effectiveness of proposed sampling\ntechniques, we have used Empirical Risk Minimization, which is commonly used\nmachine learning problem, for strongly convex and smooth case. The problem has\nbeen solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each\nusing two step determination techniques, namely, constant step size and\nbacktracking line search method. Theoretical results prove the same convergence\nfor systematic sampling, cyclic sampling and the widely used random sampling\ntechnique, in expectation. Experimental results with bench marked datasets\nprove the efficacy of the proposed sampling techniques and show up to six times\nfaster training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 04:31:40 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 09:18:20 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 07:38:28 GMT"}, {"version": "v4", "created": "Wed, 25 Jul 2018 04:27:05 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Chauhan", "Vinod Kumar", ""], ["Sharma", "Anuj", ""], ["Dahiya", "Kalpana", ""]]}, {"id": "1801.05935", "submitter": "Koulik Khamaru", "authors": "Koulik Khamaru, Rahul Mazumder", "title": "Computation of the Maximum Likelihood estimator in low-rank Factor\n  Analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis, a classical multivariate statistical technique is popularly\nused as a fundamental tool for dimensionality reduction in statistics,\neconometrics and data science. Estimation is often carried out via the Maximum\nLikelihood (ML) principle, which seeks to maximize the likelihood under the\nassumption that the positive definite covariance matrix can be decomposed as\nthe sum of a low rank positive semidefinite matrix and a diagonal matrix with\nnonnegative entries. This leads to a challenging rank constrained nonconvex\noptimization problem. We reformulate the low rank ML Factor Analysis problem as\na nonlinear nonsmooth semidefinite optimization problem, study various\nstructural properties of this reformulation and propose fast and scalable\nalgorithms based on difference of convex (DC) optimization. Our approach has\ncomputational guarantees, gracefully scales to large problems, is applicable to\nsituations where the sample covariance matrix is rank deficient and adapts to\nvariants of the ML problem with additional constraints on the problem\nparameters. Our numerical experiments demonstrate the significant usefulness of\nour approach over existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 04:50:42 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Khamaru", "Koulik", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1801.06024", "submitter": "Gino Brunner", "authors": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Michael Weigelt", "title": "Natural Language Multitasking: Analyzing and Improving Syntactic\n  Saliency of Hidden Representations", "comments": "The 31st Annual Conference on Neural Information Processing (NIPS) -\n  Workshop on Learning Disentangled Features: from Perception to Control, Long\n  Beach, CA, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train multi-task autoencoders on linguistic tasks and analyze the learned\nhidden sentence representations. The representations change significantly when\ntranslation and part-of-speech decoders are added. The more decoders a model\nemploys, the better it clusters sentences according to their syntactic\nsimilarity, as the representation space becomes less entangled. We explore the\nstructure of the representation space by interpolating between sentences, which\nyields interesting pseudo-English sentences, many of which have recognizable\nsyntactic structure. Lastly, we point out an interesting property of our\nmodels: The difference-vector between two sentences can be added to change a\nthird sentence with similar features in a meaningful way.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 14:10:37 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Brunner", "Gino", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""], ["Weigelt", "Michael", ""]]}, {"id": "1801.06043", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried, Farzana Yusuf", "title": "Optimal Weighting for Exam Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem faced by many instructors is that of designing exams that\naccurately assess the abilities of the students. Typically these exams are\nprepared several days in advance, and generic question scores are used based on\nrough approximation of the question difficulty and length. For example, for a\nrecent class taught by the author, there were 30 multiple choice questions\nworth 3 points, 15 true/false with explanation questions worth 4 points, and 5\nanalytical exercises worth 10 points. We describe a novel framework where\nalgorithms from machine learning are used to modify the exam question weights\nin order to optimize the exam scores, using the overall class grade as a proxy\nfor a student's true ability. We show that significant error reduction can be\nobtained by our approach over standard weighting schemes, and we make several\nnew observations regarding the properties of the \"good\" and \"bad\" exam\nquestions that can have impact on the design of improved future evaluation\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 05:35:47 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ganzfried", "Sam", ""], ["Yusuf", "Farzana", ""]]}, {"id": "1801.06146", "submitter": "Sebastian Ruder", "authors": "Jeremy Howard, Sebastian Ruder", "title": "Universal Language Model Fine-tuning for Text Classification", "comments": "ACL 2018, fixed denominator in Equation 3, line 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 17:54:52 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 13:57:04 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 22:02:11 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 17:46:49 GMT"}, {"version": "v5", "created": "Wed, 23 May 2018 09:23:47 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Howard", "Jeremy", ""], ["Ruder", "Sebastian", ""]]}, {"id": "1801.06147", "submitter": "Brendan Tracey", "authors": "Brendan D. Tracey and David H. Wolpert", "title": "Upgrading from Gaussian Processes to Student's-T Processes", "comments": "2018 AIAA Non-Deterministic Approaches Conference", "journal-ref": null, "doi": "10.2514/6.2018-1659", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process priors are commonly used in aerospace design for performing\nBayesian optimization. Nonetheless, Gaussian processes suffer two significant\ndrawbacks: outliers are a priori assumed unlikely, and the posterior variance\nconditioned on observed data depends only on the locations of those data, not\nthe associated sample values. Student's-T processes are a generalization of\nGaussian processes, founded on the Student's-T distribution instead of the\nGaussian distribution. Student's-T processes maintain the primary advantages of\nGaussian processes (kernel function, analytic update rule) with additional\nbenefits beyond Gaussian processes. The Student's-T distribution has higher\nKurtosis than a Gaussian distribution and so outliers are much more likely, and\nthe posterior variance increases or decreases depending on the variance of\nobserved data sample values. Here, we describe Student's-T processes, and\ndiscuss their advantages in the context of aerospace optimization. We show how\nto construct a Student's-T process using a kernel function and how to update\nthe process given new samples. We provide a clear derivation of\noptimization-relevant quantities such as expected improvement, and contrast\nwith the related computations for Gaussian processes. Finally, we compare the\nperformance of Student's-T processes against Gaussian process on canonical test\nproblems in Bayesian optimization, and apply the Student's-T process to the\noptimization of an aerostructural design problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 17:56:03 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Tracey", "Brendan D.", ""], ["Wolpert", "David H.", ""]]}, {"id": "1801.06159", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Nam H. Nguyen, Dzung T. Phan, Jayant R. Kalagnanam,\n  Katya Scheinberg", "title": "When Does Stochastic Gradient Algorithm Work Well?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a general stochastic optimization problem which is\noften at the core of supervised learning, such as deep learning and linear\nclassification. We consider a standard stochastic gradient descent (SGD) method\nwith a fixed, large step size and propose a novel assumption on the objective\nfunction, under which this method has the improved convergence rates (to a\nneighborhood of the optimal solutions). We then empirically demonstrate that\nthese assumptions hold for logistic regression and standard deep neural\nnetworks on classical data sets. Thus our analysis helps to explain when\nefficient behavior can be expected from the SGD method in training\nclassification models and deep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 18:23:02 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 04:35:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Nguyen", "Nam H.", ""], ["Phan", "Dzung T.", ""], ["Kalagnanam", "Jayant R.", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1801.06202", "submitter": "Yunchuan Kong", "authors": "Yunchuan Kong and Tianwei Yu", "title": "A graph-embedded deep feedforward network for disease outcome\n  classification and feature selection using gene expression data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression data represents a unique challenge in predictive model\nbuilding, because of the small number of samples $(n)$ compared to the huge\namount of features $(p)$. This \"$n<<p$\" property has hampered application of\ndeep learning techniques for disease outcome classification. Sparse learning by\nincorporating external gene network information could be a potential solution\nto this issue. Still, the problem is very challenging because (1) there are\ntens of thousands of features and only hundreds of training samples, (2) the\nscale-free structure of the gene network is unfriendly to the setup of\nconvolutional neural networks. To address these issues and build a robust\nclassification model, we propose the Graph-Embedded Deep Feedforward Networks\n(GEDFN), to integrate external relational information of features into the deep\nneural network architecture. The method is able to achieve sparse connection\nbetween network layers to prevent overfitting. To validate the method's\ncapability, we conducted both simulation experiments and a real data analysis\nusing a breast cancer RNA-seq dataset from The Cancer Genome Atlas (TCGA). The\nresulting high classification accuracy and easily interpretable feature\nselection results suggest the method is a useful addition to the current\nclassification models and feature selection procedures. The method is available\nat https://github.com/yunchuankong/NetworkNeuralNetwork.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 19:14:03 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:56:32 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kong", "Yunchuan", ""], ["Yu", "Tianwei", ""]]}, {"id": "1801.06230", "submitter": "Brian Trippe", "authors": "Brian Trippe and Richard Turner", "title": "Overpruning in Variational Bayesian Neural Networks", "comments": "Presented the Advances in Approximate Bayesian Inference workshop at\n  NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivations for using variational inference (VI) in neural networks\ndiffer significantly from those in latent variable models. This has a\ncounter-intuitive consequence; more expressive variational approximations can\nprovide significantly worse predictions as compared to those with less\nexpressive families. In this work we make two contributions. First, we identify\na cause of this performance gap, variational over-pruning. Second, we introduce\na theoretically grounded explanation for this phenomenon. Our perspective sheds\nlight on several related published results and provides intuition into the\ndesign of effective variational approximations of neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 20:33:26 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Trippe", "Brian", ""], ["Turner", "Richard", ""]]}, {"id": "1801.06287", "submitter": "Linyuan Gong", "authors": "Linyuan Gong, Ruyi Ji", "title": "What Does a TextCNN Learn?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TextCNN, the convolutional neural network for text, is a useful deep learning\nalgorithm for sentence classification tasks such as sentiment analysis and\nquestion classification. However, neural networks have long been known as black\nboxes because interpreting them is a challenging task. Researchers have\ndeveloped several tools to understand a CNN for image classification by deep\nvisualization, but research about deep TextCNNs is still insufficient. In this\npaper, we are trying to understand what a TextCNN learns on two classical NLP\ndatasets. Our work focuses on functions of different convolutional kernels and\ncorrelations between convolutional kernels.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 04:02:04 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Gong", "Linyuan", ""], ["Ji", "Ruyi", ""]]}, {"id": "1801.06296", "submitter": "Rico Krueger", "authors": "Rico Krueger, Akshay Vij, Taha H. Rashidi", "title": "A Dirichlet Process Mixture Model of Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixed multinomial logit (MNL) model, which leverages the\ntruncated stick-breaking process representation of the Dirichlet process as a\nflexible nonparametric mixing distribution. The proposed model is a Dirichlet\nprocess mixture model and accommodates discrete representations of\nheterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL\nmodel, the proposed discrete choice model does not require the analyst to fix\nthe number of mixture components prior to estimation, as the complexity of the\ndiscrete mixing distribution is inferred from the evidence. For posterior\ninference in the proposed Dirichlet process mixture model of discrete choice,\nwe derive an expectation maximisation algorithm. In a simulation study, we\ndemonstrate that the proposed model framework can flexibly capture\ndifferently-shaped taste parameter distributions. Furthermore, we empirically\nvalidate the model framework in a case study on motorists' route choice\npreferences and find that the proposed Dirichlet process mixture model of\ndiscrete choice outperforms a latent class MNL model and mixed MNL models with\ncommon parametric mixing distributions in terms of both in-sample fit and\nout-of-sample predictive ability. Compared to extant modelling approaches, the\nproposed discrete choice model substantially abbreviates specification\nsearches, as it relies on less restrictive parametric assumptions and does not\nrequire the analyst to specify the complexity of the discrete mixing\ndistribution prior to estimation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:12:16 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Krueger", "Rico", ""], ["Vij", "Akshay", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1801.06309", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Composite Functional Gradient Learning of Generative Adversarial Models", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper first presents a theory for generative adversarial methods that\ndoes not rely on the traditional minimax formulation. It shows that with a\nstrong discriminator, a good generator can be learned so that the KL divergence\nbetween the distributions of real data and generated data improves after each\nfunctional gradient step until it converges to zero. Based on the theory, we\npropose a new stable generative adversarial method. A theoretical insight into\nthe original GAN from this new viewpoint is also provided. The experiments on\nimage generation show the effectiveness of our new method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 06:20:56 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 13:21:18 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1801.06378", "submitter": "Grigori Fursin", "authors": "Thierry Moreau, Anton Lokhmotov and Grigori Fursin", "title": "Introducing ReQuEST: an Open Platform for Reproducible and\n  Quality-Efficient Systems-ML Tournaments", "comments": "ReQuEST tournament website: http://cKnowledge.org/request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-designing efficient machine learning based systems across the whole\nhardware/software stack to trade off speed, accuracy, energy and costs is\nbecoming extremely complex and time consuming. Researchers often struggle to\nevaluate and compare different published works across rapidly evolving software\nframeworks, heterogeneous hardware platforms, compilers, libraries, algorithms,\ndata sets, models, and environments.\n  We present our community effort to develop an open co-design tournament\nplatform with an online public scoreboard. It will gradually incorporate best\nresearch practices while providing a common way for multidisciplinary\nresearchers to optimize and compare the quality vs. efficiency Pareto\noptimality of various workloads on diverse and complete hardware/software\nsystems. We want to leverage the open-source Collective Knowledge framework and\nthe ACM artifact evaluation methodology to validate and share the complete\nmachine learning system implementations in a standardized, portable, and\nreproducible fashion. We plan to hold regular multi-objective optimization and\nco-design tournaments for emerging workloads such as deep learning, starting\nwith ASPLOS'18 (ACM conference on Architectural Support for Programming\nLanguages and Operating Systems - the premier forum for multidisciplinary\nsystems research spanning computer architecture and hardware, programming\nlanguages and compilers, operating systems and networking) to build a public\nrepository of the most efficient machine learning algorithms and systems which\ncan be easily customized, reused and built upon.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 12:22:51 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Moreau", "Thierry", ""], ["Lokhmotov", "Anton", ""], ["Fursin", "Grigori", ""]]}, {"id": "1801.06397", "submitter": "Nikolaus Mayer", "authors": "Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel\n  Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "What Makes Good Synthetic Training Data for Learning Disparity and\n  Optical Flow Estimation?", "comments": "added references (UCL dataset); added IJCV copyright information", "journal-ref": null, "doi": "10.1007/s11263-018-1082-6", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finding that very large networks can be trained efficiently and reliably\nhas led to a paradigm shift in computer vision from engineered solutions to\nlearning formulations. As a result, the research challenge shifts from devising\nalgorithms to creating suitable and abundant training data for supervised\nlearning. How to efficiently create such training data? The dominant data\nacquisition method in visual recognition is based on web data and manual\nannotation. Yet, for many computer vision problems, such as stereo or optical\nflow estimation, this approach is not feasible because humans cannot manually\nenter a pixel-accurate flow field. In this paper, we promote the use of\nsynthetically generated data for the purpose of training deep networks on such\ntasks.We suggest multiple ways to generate such data and evaluate the influence\nof dataset properties on the performance and generalization properties of the\nresulting networks. We also demonstrate the benefit of learning schedules that\nuse different types of data at selected stages of the training process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 13:21:07 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 10:28:01 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 10:26:58 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["Fischer", "Philipp", ""], ["Hazirbas", "Caner", ""], ["Cremers", "Daniel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1801.06423", "submitter": "Rafa\\\"el Pinot", "authors": "Rafael Pinot", "title": "Minimum spanning tree release under differential privacy constraints", "comments": "Thesis of Master Degree of Statistics, Universit\\'e Paris 6 Pierre et\n  Marie Curie", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of nodes clustering under privacy constraints when\nrepresenting a dataset as a graph. Our contribution is threefold. First we\nformally define the concept of differential privacy for structured databases\nsuch as graphs, and give an alternative definition based on a new neighborhood\nnotion between graphs. This definition is adapted to particular frameworks that\ncan be met in various application fields such as genomics, world wide web,\npopulation survey, etc. Second, we introduce a new algorithm to tackle the\nissue of privately releasing an approximated minimum spanning tree topology for\na simple-undirected-weighted graph. It provides a simple way of producing the\ntopology of a private almost minimum spanning tree which outperforms, in most\ncases, the state of the art \"Laplace mechanism\" in terms of\nweight-approximation error.\n  Finally, we propose a theoretically motivated method combining a sanitizing\nmechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree\n(MST)-based clustering algorithm. It provides an accurate method for nodes\nclustering in a graph while keeping the sensitive information contained in the\nedges weights of the private graph. We provide some theoretical results on the\nrobustness of an almost minimum spanning tree construction for Laplace\nsanitizing mechanisms. These results exhibit which conditions the graph weights\nshould respect in order to consider that the nodes form well separated clusters\nboth for Laplace and our algorithm as sanitizing mechanism. The method has been\nexperimentally evaluated on simulated data, and preliminary results show the\ngood behavior of the algorithm while identifying well separated clusters.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 14:45:06 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Pinot", "Rafael", ""]]}, {"id": "1801.06432", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou", "title": "Robust Kronecker Component Analysis", "comments": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Special Issue on Compact and Efficient Feature Representation and Learning in\n  Computer Vision, 2018. Contains appendices. arXiv admin note: text overlap\n  with arXiv:1703.07886", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning and component analysis models are fundamental for\nlearning compact representations that are relevant to a given task (feature\nextraction, dimensionality reduction, denoising, etc.). The model complexity is\nencoded by means of specific structure, such as sparsity, low-rankness, or\nnonnegativity. Unfortunately, approaches like K-SVD - that learn dictionaries\nfor sparse coding via Singular Value Decomposition (SVD) - are hard to scale to\nhigh-volume and high-dimensional visual data, and fragile in the presence of\noutliers. Conversely, robust component analysis methods such as the Robust\nPrincipal Component Analysis (RPCA) are able to recover low-complexity (e.g.,\nlow-rank) representations from data corrupted with noise of unknown magnitude\nand support, but do not provide a dictionary that respects the structure of the\ndata (e.g., images), and also involve expensive computations. In this paper, we\npropose a novel Kronecker-decomposable component analysis model, coined as\nRobust Kronecker Component Analysis (RKCA), that combines ideas from sparse\ndictionary learning and robust component analysis. RKCA has several appealing\nproperties, including robustness to gross corruption; it can be used for\nlow-rank modeling, and leverages separability to solve significantly smaller\nproblems. We design an efficient learning algorithm by drawing links with a\nrestricted form of tensor factorization, and analyze its optimality and\nlow-rankness properties. The effectiveness of the proposed approach is\ndemonstrated on real-world applications, namely background subtraction and\nimage denoising and completion, by performing a thorough comparison with the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 18:01:50 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 20:55:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Bahri", "Mehdi", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1801.06481", "submitter": "Chen Liang", "authors": "Chen Liang, Jianbo Ye, Han Zhao, Bart Pursel, C. Lee Giles", "title": "Active Learning of Strict Partial Orders: A Case Study on Concept\n  Prerequisite Relations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict partial order is a mathematical structure commonly seen in relational\ndata. One obstacle to extracting such type of relations at scale is the lack of\nlarge-scale labels for building effective data-driven solutions. We develop an\nactive learning framework for mining such relations subject to a strict order.\nOur approach incorporates relational reasoning not only in finding new\nunlabeled pairs whose labels can be deduced from an existing label set, but\nalso in devising new query strategies that consider the relational structure of\nlabels. Our experiments on concept prerequisite relations show our proposed\nframework can substantially improve the classification performance with the\nsame query budget compared to other baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 16:26:18 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Liang", "Chen", ""], ["Ye", "Jianbo", ""], ["Zhao", "Han", ""], ["Pursel", "Bart", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.06490", "submitter": "Pankaj Pansari", "authors": "Pankaj Pansari, Chris Russell, M.Pawan Kumar", "title": "Worst-case Optimal Submodular Extensions for Marginal Estimation", "comments": "Accepted to AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular extensions of an energy function can be used to efficiently\ncompute approximate marginals via variational inference. The accuracy of the\nmarginals depends crucially on the quality of the submodular extension. To\nidentify the best possible extension, we show an equivalence between the\nsubmodular extensions of the energy and the objective functions of linear\nprogramming (LP) relaxations for the corresponding MAP estimation problem. This\nallows us to (i) establish the worst-case optimality of the submodular\nextension for Potts model used in the literature; (ii) identify the worst-case\noptimal submodular extension for the more general class of metric labeling; and\n(iii) efficiently compute the marginals for the widely used dense CRF model\nwith the help of a recently proposed Gaussian filtering method. Using synthetic\nand real data, we show that our approach provides comparable upper bounds on\nthe log-partition function to those obtained using tree-reweighted message\npassing (TRW) in cases where the latter is computationally feasible.\nImportantly, unlike TRW, our approach provides the first practical algorithm to\ncompute an upper bound on the dense CRF model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 14:36:57 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Pansari", "Pankaj", ""], ["Russell", "Chris", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1801.06503", "submitter": "Alexandre Attia", "authors": "Alexandre Attia, Sharone Dayan", "title": "Global overview of Imitation Learning", "comments": "9 pages, 5 figures, 5 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation Learning is a sequential task where the learner tries to mimic an\nexpert's action in order to achieve the best performance. Several algorithms\nhave been proposed recently for this task. In this project, we aim at proposing\na wide review of these algorithms, presenting their main features and comparing\nthem on their performance and their regret bounds.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 17:40:09 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Attia", "Alexandre", ""], ["Dayan", "Sharone", ""]]}, {"id": "1801.06504", "submitter": "Alexandre Attia", "authors": "Alexandre Attia, Sharone Dayan", "title": "Detecting and counting tiny faces", "comments": "4 pages, 10 figures, 2 appendix page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding Tiny Faces (by Hu and Ramanan) proposes a novel approach to find\nsmall objects in an image. Our contribution consists in deeply understanding\nthe choices of the paper together with applying and extending a similar method\nto a real world subject which is the counting of people in a public\ndemonstration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 17:41:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 16:04:15 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Attia", "Alexandre", ""], ["Dayan", "Sharone", ""]]}, {"id": "1801.06637", "submitter": "Maziar Raissi", "authors": "Maziar Raissi", "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 08:02:09 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Raissi", "Maziar", ""]]}, {"id": "1801.06700", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng\n  Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath\n  Chandar, Nan Rosemary Ke, Sai Rajeswar, Alexandre de Brebisson, Jose M. R.\n  Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau,\n  Yoshua Bengio", "title": "A Deep Reinforcement Learning Chatbot (Short Version)", "comments": "9 pages, 1 figure, 2 tables; presented at NIPS 2017, Conversational\n  AI: \"Today's Practice and Tomorrow's Potential\" Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MILABOT: a deep reinforcement learning chatbot developed by the\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\ncompetition. MILABOT is capable of conversing with humans on popular small talk\ntopics through both speech and text. The system consists of an ensemble of\nnatural language generation and retrieval models, including neural network and\ntemplate-based models. By applying reinforcement learning to crowdsourced data\nand real-world user interactions, the system has been trained to select an\nappropriate response from the models in its ensemble. The system has been\nevaluated through A/B testing with real-world users, where it performed\nsignificantly better than other systems. The results highlight the potential of\ncoupling ensemble systems with deep reinforcement learning as a fruitful path\nfor developing real-world, open-domain conversational agents.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 17:22:06 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sankar", "Chinnadhurai", ""], ["Germain", "Mathieu", ""], ["Zhang", "Saizheng", ""], ["Lin", "Zhouhan", ""], ["Subramanian", "Sandeep", ""], ["Kim", "Taesup", ""], ["Pieper", "Michael", ""], ["Chandar", "Sarath", ""], ["Ke", "Nan Rosemary", ""], ["Rajeswar", "Sai", ""], ["de Brebisson", "Alexandre", ""], ["Sotelo", "Jose M. R.", ""], ["Suhubdy", "Dendi", ""], ["Michalski", "Vincent", ""], ["Nguyen", "Alexandre", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1801.06720", "submitter": "Junhong Lin", "authors": "Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, Volkan Cevher", "title": "Optimal Rates for Spectral Algorithms with Least-Squares Regression over\n  Hilbert Spaces", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis 2018", "doi": "10.1016/j.acha.2018.09.009", "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study regression problems over a separable Hilbert space\nwith the square loss, covering non-parametric regression over a reproducing\nkernel Hilbert space. We investigate a class of spectral-regularized\nalgorithms, including ridge regression, principal component analysis, and\ngradient methods. We prove optimal, high-probability convergence results in\nterms of variants of norms for the studied algorithms, considering a capacity\nassumption on the hypothesis space and a general source condition on the target\nfunction. Consequently, we obtain almost sure convergence results with optimal\nrates. Our results improve and generalize previous results, filling a\ntheoretical gap for the non-attainable cases.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 20:02:00 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 20:46:30 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 15:26:26 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Lin", "Junhong", ""], ["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""], ["Cevher", "Volkan", ""]]}, {"id": "1801.06727", "submitter": "Denisa Roberts", "authors": "Denisa Roberts and Douglas Patterson", "title": "A Second Order Cumulant Spectrum Test That a Stochastic Process is\n  Strictly Stationary and a Step Toward a Test for Graph Signal Strict\n  Stationarity", "comments": "6 pages", "journal-ref": "NeurIPS 2018 Workshop for the Spatiotemporal Domain", "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a statistical test for the null hypothesis of strict\nstationarity of a discrete time stochastic process in the frequency domain.\nWhen the null hypothesis is true, the second order cumulant spectrum is zero at\nall the discrete Fourier frequency pairs in the principal domain. The test uses\na window averaged sample estimate of the second order cumulant spectrum to\nbuild a test statistic with an asymptotic complex standard normal distribution.\nWe derive the test statistic, study the properties of the test and demonstrate\nits application using 137Cs gamma ray decay data. Future areas of research\ninclude testing for strict stationarity of graph signals, with applications in\nlearning convolutional neural networks on graphs, denoising, and inpainting.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 20:47:27 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 18:46:52 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Roberts", "Denisa", ""], ["Patterson", "Douglas", ""]]}, {"id": "1801.06797", "submitter": "Luis Herranz", "authors": "Xinhang Song, Luis Herranz, Shuqiang Jiang", "title": "Depth CNNs for RGB-D scene recognition: learning from scratch better\n  than transferring from RGB-CNNs", "comments": "AAAI Conference on Artificial Intelligence 2017", "journal-ref": "AAAI Conference on Artificial Intelligence 2017, 4271-4277", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition with RGB images has been extensively studied and has\nreached very remarkable recognition levels, thanks to convolutional neural\nnetworks (CNN) and large scene datasets. In contrast, current RGB-D scene data\nis much more limited, so often leverages RGB large datasets, by transferring\npretrained RGB CNN models and fine-tuning with the target RGB-D dataset.\nHowever, we show that this approach has the limitation of hardly reaching\nbottom layers, which is key to learn modality-specific features. In contrast,\nwe focus on the bottom layers, and propose an alternative strategy to learn\ndepth features combining local weakly supervised training from patches followed\nby global fine tuning with images. This strategy is capable of learning very\ndiscriminative depth-specific features with limited depth images, without\nresorting to Places-CNN. In addition we propose a modified CNN architecture to\nfurther match the complexity of the model and the amount of data available. For\nRGB-D scene recognition, depth and RGB features are combined by projecting them\nin a common space and further leaning a multilayer classifier, which is jointly\noptimized in an end-to-end network. Our framework achieves state-of-the-art\naccuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 09:38:50 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Song", "Xinhang", ""], ["Herranz", "Luis", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1801.06805", "submitter": "Weichang Wu", "authors": "Weichang Wu, Junchi Yan, Xiaokang Yang, Hongyuan Zha", "title": "Decoupled Learning for Factorial Marked Temporal Point Processes", "comments": "9 pages, 8 figures, submitted to TNNLS, 21 Jan, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the factorial marked temporal point process model and\npresents efficient learning methods. In conventional (multi-dimensional) marked\ntemporal point process models, event is often encoded by a single discrete\nvariable i.e. a marker. In this paper, we describe the factorial marked point\nprocesses whereby time-stamped event is factored into multiple markers.\nAccordingly the size of the infectivity matrix modeling the effect between\npairwise markers is in power order w.r.t. the number of the discrete marker\nspace. We propose a decoupled learning method with two learning procedures: i)\ndirectly solving the model based on two techniques: Alternating Direction\nMethod of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii)\ninvolving a reformulation that transforms the original problem into a Logistic\nRegression model for more efficient learning. Moreover, a sparse group\nregularizer is added to identify the key profile features and event labels.\nEmpirical results on real world datasets demonstrate the efficiency of our\ndecoupled and reformulated method. The source code is available online.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 11:13:29 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Wu", "Weichang", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1801.06816", "submitter": "Bruce Hajek", "authors": "Bruce Hajek and Suryanarayana Sankagiri", "title": "Preferential Attachment Graphs with Planted Communities", "comments": "Discovered large overlap with J. Jordan, Geometric preferential\n  attachment in non-uniform metric spaces (2013) Electronic J. Prob, Vol. 18,\n  no. 8, pp 1-15. New aspects of our approach will be moved to: Recovering a\n  Hidden Community in a Preferential Attachment Graph, arXiv:1801.06818", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variation of the preferential attachment random graph model of Barab\\'asi\nand Albert is defined that incorporates planted communities. The graph is built\nprogressively, with new vertices attaching to the existing ones one-by-one. At\nevery step, the incoming vertex is randomly assigned a label, which represents\na community it belongs to. This vertex then chooses certain vertices as its\nneighbors, with the choice of each vertex being proportional to the degree of\nthe vertex multiplied by an affinity depending on the labels of the new vertex\nand a potential neighbor. It is shown that the fraction of half-edges attached\nto vertices with a given label converges almost surely for some classes of\naffinity matrices. In addition, the empirical degree distribution for the set\nof vertices with a given label converges to a heavy tailed distribution, such\nthat the tail decay parameter can be different for different communities. Our\nproof method may be of independent interest, both for the classical Barab\\'asi\n-Albert model and for other possible extensions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 13:29:57 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 05:56:48 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hajek", "Bruce", ""], ["Sankagiri", "Suryanarayana", ""]]}, {"id": "1801.06818", "submitter": "Bruce Hajek", "authors": "Bruce Hajek and Suryanarayana Sankagiri", "title": "Community Recovery in a Preferential Attachment Graph", "comments": "arXiv admin note: text overlap with arXiv:1801.06816", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A message passing algorithm is derived for recovering communities within a\ngraph generated by a variation of the Barab\\'{a}si-Albert preferential\nattachment model. The estimator is assumed to know the arrival times, or order\nof attachment, of the vertices. The derivation of the algorithm is based on\nbelief propagation under an independence assumption. Two precursors to the\nmessage passing algorithm are analyzed: the first is a degree thresholding (DT)\nalgorithm and the second is an algorithm based on the arrival times of the\nchildren (C) of a given vertex, where the children of a given vertex are the\nvertices that attached to it. Comparison of the performance of the algorithms\nshows it is beneficial to know the arrival times, not just the number, of the\nchildren. The probability of correct classification of a vertex is\nasymptotically determined by the fraction of vertices arriving before it. Two\nextensions of Algorithm C are given: the first is based on joint likelihood of\nthe children of a fixed set of vertices; it can sometimes be used to seed the\nmessage passing algorithm. The second is the message passing algorithm.\nSimulation results are given.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 13:36:50 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 16:30:04 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 14:31:30 GMT"}, {"version": "v4", "created": "Fri, 1 Jun 2018 02:43:21 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 19:52:08 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hajek", "Bruce", ""], ["Sankagiri", "Suryanarayana", ""]]}, {"id": "1801.06845", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Lorenzo Livi, Alberto Ferrante, Jelena\n  Milosevic, Miroslaw Malek", "title": "Time series kernel similarities for predicting Paroxysmal Atrial\n  Fibrillation from ECGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of classifying Electrocardiography (ECG) signals with\nthe aim of predicting the onset of Paroxysmal Atrial Fibrillation (PAF). Atrial\nfibrillation is the most common type of arrhythmia, but in many cases PAF\nepisodes are asymptomatic. Therefore, in order to help diagnosing PAF, it is\nimportant to design procedures for detecting and, more importantly, predicting\nPAF episodes. We propose a method for predicting PAF events whose first step\nconsists of a feature extraction procedure that represents each ECG as a\nmulti-variate time series. Successively, we design a classification framework\nbased on kernel similarities for multi-variate time series, capable of handling\nmissing data. We consider different approaches to perform classification in the\noriginal space of the multi-variate time series and in an embedding space,\ndefined by the kernel similarity measure. We achieve a classification accuracy\ncomparable with state of the art methods, with the additional advantage of\ndetecting the PAF onset up to 15 minutes in advance.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 16:28:23 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 20:03:05 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Livi", "Lorenzo", ""], ["Ferrante", "Alberto", ""], ["Milosevic", "Jelena", ""], ["Malek", "Miroslaw", ""]]}, {"id": "1801.06879", "submitter": "Yinhao Zhu", "authors": "Yinhao Zhu, Nicholas Zabaras", "title": "Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate\n  Modeling and Uncertainty Quantification", "comments": "52 pages, 28 figures, submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2018.04.018", "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the development of surrogate models for uncertainty\nquantification and propagation in problems governed by stochastic PDEs using a\ndeep convolutional encoder-decoder network in a similar fashion to approaches\nconsidered in deep learning for image-to-image regression tasks. Since normal\nneural networks are data intensive and cannot provide predictive uncertainty,\nwe propose a Bayesian approach to convolutional neural nets. A recently\nintroduced variational gradient descent algorithm based on Stein's method is\nscaled to deep convolutional networks to perform approximate Bayesian inference\non millions of uncertain network parameters. This approach achieves state of\nthe art performance in terms of predictive accuracy and uncertainty\nquantification in comparison to other approaches in Bayesian neural networks as\nwell as techniques that include Gaussian processes and ensemble methods even\nwhen the training data size is relatively small. To evaluate the performance of\nthis approach, we consider standard uncertainty quantification benchmark\nproblems including flow in heterogeneous media defined in terms of limited\ndata-driven permeability realizations. The performance of the surrogate model\ndeveloped is very good even though there is no underlying structure shared\nbetween the input (permeability) and output (flow/pressure) fields as is often\nthe case in the image-to-image regression models used in computer vision\nproblems. Studies are performed with an underlying stochastic input\ndimensionality up to $4,225$ where most other uncertainty quantification\nmethods fail. Uncertainty propagation tasks are considered and the predictive\noutput Bayesian statistics are compared to those obtained with Monte Carlo\nestimates.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 19:18:13 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhu", "Yinhao", ""], ["Zabaras", "Nicholas", ""]]}, {"id": "1801.06889", "submitter": "Fred Hohman", "authors": "Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers", "comments": "Under review for IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 20:13:07 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 01:09:33 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 04:59:24 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Pienta", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1801.06920", "submitter": "Girish Joshi", "authors": "Girish Joshi, Girish Chowdhary", "title": "Cross-Domain Transfer in Reinforcement Learning using Target Apprentice", "comments": "To appear as conference paper in ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach to Transfer Learning (TL) in\nReinforcement Learning (RL) for cross-domain tasks. Many of the available\ntechniques approach the transfer architecture as a method of speeding up the\ntarget task learning. We propose to adapt and reuse the mapped source task\noptimal-policy directly in related domains. We show the optimal policy from a\nrelated source task can be near optimal in target domain provided an adaptive\npolicy accounts for the model error between target and source. The main benefit\nof this policy augmentation is generalizing policies across multiple related\ndomains without having to re-learn the new tasks. Our results show that this\narchitecture leads to better sample efficiency in the transfer, reducing sample\ncomplexity of target task learning to target apprentice learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 00:39:19 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Joshi", "Girish", ""], ["Chowdhary", "Girish", ""]]}, {"id": "1801.06934", "submitter": "Linbo Qiao", "authors": "Linbo Qiao, Tianyi Lin, Qi Qin, Xicheng Lu", "title": "On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid\n  Gradient Approach with High Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a stochastic Primal-Dual Hybrid Gradient (PDHG)\napproach for solving a wide spectrum of regularized stochastic minimization\nproblems, where the regularization term is composite with a linear function. It\nhas been recognized that solving this kind of problem is challenging since the\nclosed-form solution of the proximal mapping associated with the regularization\nterm is not available due to the imposed linear composition, and the\nper-iteration cost of computing the full gradient of the expected objective\nfunction is extremely high when the number of input data samples is\nconsiderably large.\n  Our new approach overcomes these issues by exploring the special structure of\nthe regularization term and sampling a few data points at each iteration.\nRather than analyzing the convergence in expectation, we provide the detailed\niteration complexity analysis for the cases of both uniformly and non-uniformly\naveraged iterates with high probability. This strongly supports the good\npractical performance of the proposed approach. Numerical experiments\ndemonstrate that the efficiency of stochastic PDHG, which outperforms other\ncompeting algorithms, as expected by the high-probability convergence analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 02:09:20 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 05:14:41 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Qiao", "Linbo", ""], ["Lin", "Tianyi", ""], ["Qin", "Qi", ""], ["Lu", "Xicheng", ""]]}, {"id": "1801.06975", "submitter": "Feng Li", "authors": "Feng Li, Sibo Yang, Huanhuan Huang, and Wei Wu", "title": "Extreme Learning Machine with Local Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the sparsification of the input-hidden weights\nof ELM (Extreme Learning Machine). For ordinary feedforward neural networks,\nthe sparsification is usually done by introducing certain regularization\ntechnique into the learning process of the network. But this strategy can not\nbe applied for ELM, since the input-hidden weights of ELM are supposed to be\nrandomly chosen rather than to be learned. To this end, we propose a modified\nELM, called ELM-LC (ELM with local connections), which is designed for the\nsparsification of the input-hidden weights as follows: The hidden nodes and the\ninput nodes are divided respectively into several corresponding groups, and an\ninput node group is fully connected with its corresponding hidden node group,\nbut is not connected with any other hidden node group. As in the usual ELM, the\nhidden-input weights are randomly given, and the hidden-output weights are\nobtained through a least square learning. In the numerical simulations on some\nbenchmark problems, the new ELM-CL behaves better than the traditional ELM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:54:22 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Feng", ""], ["Yang", "Sibo", ""], ["Huang", "Huanhuan", ""], ["Wu", "Wei", ""]]}, {"id": "1801.07030", "submitter": "Thomas Nedelec", "authors": "Alexandre Gilotte, Cl\\'ement Calauz\\`enes, Thomas Nedelec, Alexandre\n  Abraham and Simon Doll\\'e", "title": "Offline A/B testing for Recommender Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3159652.3159687", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before A/B testing online a new version of a recommender system, it is usual\nto perform some offline evaluations on historical data. We focus on evaluation\nmethods that compute an estimator of the potential uplift in revenue that could\ngenerate this new technology. It helps to iterate faster and to avoid losing\nmoney by detecting poor policies. These estimators are known as counterfactual\nor off-policy estimators. We show that traditional counterfactual estimators\nsuch as capped importance sampling and normalised importance sampling are\nexperimentally not having satisfying bias-variance compromises in the context\nof personalised product recommendation for online advertising. We propose two\nvariants of counterfactual estimates with different modelling of the bias that\nprove to be accurate in real-world conditions. We provide a benchmark of these\nestimators by showing their correlation with business metrics observed by\nrunning online A/B tests on a commercial recommender system.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 10:31:56 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Gilotte", "Alexandre", ""], ["Calauz\u00e8nes", "Cl\u00e9ment", ""], ["Nedelec", "Thomas", ""], ["Abraham", "Alexandre", ""], ["Doll\u00e9", "Simon", ""]]}, {"id": "1801.07047", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Julius Gordon", "title": "News-based forecasts of macroeconomic indicators: A semantic path model\n  for interpretable predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The macroeconomic climate influences operations with regard to, e.g., raw\nmaterial prices, financing, supply chain utilization and demand quotas. In\norder to adapt to the economic environment, decision-makers across the public\nand private sectors require accurate forecasts of the economic outlook.\nExisting predictive frameworks base their forecasts primarily on time series\nanalysis, as well as the judgments of experts. As a consequence, current\napproaches are often biased and prone to error. In order to reduce forecast\nerrors, this paper presents an innovative methodology that extends lag\nvariables with unstructured data in the form of financial news: (1) we apply a\nvariety of models from machine learning to word counts as a high-dimensional\ninput. However, this approach suffers from low interpretability and\noverfitting, motivating the following remedies. (2) We follow the intuition\nthat the economic climate is driven by general sentiments and suggest a\nprojection of words onto latent semantic structures as a means of feature\nengineering. (3) We propose a semantic path model, together with estimation\ntechnique based on regularization, in order to yield full interpretability of\nthe forecasts. We demonstrate the predictive performance of our approach by\nutilizing 80,813 ad hoc announcements in order to make long-term forecasts of\nup to 24 months ahead regarding key macroeconomic indicators. Back-testing\nreveals a considerable reduction in forecast errors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 11:26:30 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 20:24:03 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Gordon", "Julius", ""]]}, {"id": "1801.07145", "submitter": "Eric Alcaide", "authors": "Eric Alcaide", "title": "E-swish: Adjusting Activations to Different Network Depths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions have a notorious impact on neural networks on both\ntraining and testing the models against the desired problem. Currently, the\nmost used activation function is the Rectified Linear Unit (ReLU). This paper\nintroduces a new and novel activation function, closely related with the new\nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which\ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We\nshow that E-swish outperforms many other well-known activations including both\nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy\nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when\ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The\ncode to reproduce all our experiments can be found at\nhttps://github.com/EricAlcaide/E-swish\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:40:29 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Alcaide", "Eric", ""]]}, {"id": "1801.07172", "submitter": "Shotaro Shiba", "authors": "Satoshi Iso, Shotaro Shiba and Sumito Yokoo", "title": "Scale-invariant Feature Extraction of Neural Network and Renormalization\n  Group Flow", "comments": "32 pages, 17 figures", "journal-ref": "Phys. Rev. E 97, 053304 (2018)", "doi": "10.1103/PhysRevE.97.053304", "report-no": "KEK-TH-2029", "categories": "hep-th cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical understanding of how deep neural network (DNN) extracts features\nfrom input images is still unclear, but it is widely believed that the\nextraction is performed hierarchically through a process of coarse-graining. It\nreminds us of the basic concept of renormalization group (RG) in statistical\nphysics. In order to explore possible relations between DNN and RG, we use the\nRestricted Boltzmann machine (RBM) applied to Ising model and construct a flow\nof model parameters (in particular, temperature) generated by the RBM. We show\nthat the unsupervised RBM trained by spin configurations at various\ntemperatures from $T=0$ to $T=6$ generates a flow along which the temperature\napproaches the critical value $T_c=2.27$. This behavior is opposite to the\ntypical RG flow of the Ising model. By analyzing various properties of the\nweight matrices of the trained RBM, we discuss why it flows towards $T_c$ and\nhow the RBM learns to extract features of spin configurations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 16:18:04 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Iso", "Satoshi", ""], ["Shiba", "Shotaro", ""], ["Yokoo", "Sumito", ""]]}, {"id": "1801.07194", "submitter": "Davide Falessi", "authors": "Sean Bayley, Davide Falessi", "title": "Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:05:20 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bayley", "Sean", ""], ["Falessi", "Davide", ""]]}, {"id": "1801.07222", "submitter": "Louis Faury", "authors": "Louis Faury, Flavian Vasile", "title": "Rover Descent: Learning to optimize by learning to navigate on\n  prototypical loss surfaces", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to optimize - the idea that we can learn from data algorithms that\noptimize a numerical criterion - has recently been at the heart of a growing\nnumber of research efforts. One of the most challenging issues within this\napproach is to learn a policy that is able to optimize over classes of\nfunctions that are fairly different from the ones that it was trained on. We\npropose a novel way of framing learning to optimize as a problem of learning a\ngood navigation policy on a partially observable loss surface. To this end, we\ndevelop Rover Descent, a solution that allows us to learn a fairly broad\noptimization policy from training on a small set of prototypical\ntwo-dimensional surfaces that encompasses the classically hard cases such as\nvalleys, plateaus, cliffs and saddles and by using strictly zero-order\ninformation. We show that, without having access to gradient or curvature\ninformation, we achieve state-of-the-art convergence speed on optimization\nproblems not presented at training time such as the Rosenbrock function and\nother hard cases in two dimensions. We extend our framework to optimize over\nhigh dimensional landscapes, while still handling only two-dimensional local\nlandscape information and show good preliminary results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:13:46 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 10:17:04 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 14:34:19 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Faury", "Louis", ""], ["Vasile", "Flavian", ""]]}, {"id": "1801.07226", "submitter": "Junhong Lin", "authors": "Junhong Lin and Volkan Cevher", "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient\n  Methods and Spectral Algorithms", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalization properties of distributed algorithms in the setting\nof nonparametric regression over a reproducing kernel Hilbert space (RKHS). We\nfirst investigate distributed stochastic gradient methods (SGM), with\nmini-batches and multi-passes over the data. We show that optimal\ngeneralization error bounds can be retained for distributed SGM provided that\nthe partition level is not too large. We then extend our results to\nspectral-regularization algorithms (SRA), including kernel ridge regression\n(KRR), kernel principal component analysis, and gradient methods. Our results\nare superior to the state-of-the-art theory. Particularly, our results show\nthat distributed SGM has a smaller theoretical computational complexity,\ncompared with distributed KRR and classic SGM. Moreover, even for\nnon-distributed SRA, they provide the first optimal, capacity-dependent\nconvergence rates, considering the case that the regression function may not be\nin the RKHS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:14:11 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 15:21:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lin", "Junhong", ""], ["Cevher", "Volkan", ""]]}, {"id": "1801.07292", "submitter": "Ching-An Cheng", "authors": "Ching-An Cheng, Byron Boots", "title": "Convergence of Value Aggregation for Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value aggregation is a general framework for solving imitation learning\nproblems. Based on the idea of data aggregation, it generates a policy sequence\nby iteratively interleaving policy optimization and evaluation in an online\nlearning setting. While the existence of a good policy in the policy sequence\ncan be guaranteed non-asymptotically, little is known about the convergence of\nthe sequence or the performance of the last policy. In this paper, we debunk\nthe common belief that value aggregation always produces a convergent policy\nsequence with improving performance. Moreover, we identify a critical stability\ncondition for convergence and provide a tight non-asymptotic bound on the\nperformance of the last policy. These new theoretical insights let us stabilize\nproblems with regularization, which removes the inconvenient process of\nidentifying the best policy in the policy sequence in stochastic problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 19:47:34 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Cheng", "Ching-An", ""], ["Boots", "Byron", ""]]}, {"id": "1801.07316", "submitter": "Robert Kosar", "authors": "Robert Kosar and David W. Scott", "title": "The Hybrid Bootstrap: A Drop-in Replacement for Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is an important component of predictive model building. The\nhybrid bootstrap is a regularization technique that functions similarly to\ndropout except that features are resampled from other training points rather\nthan replaced with zeros. We show that the hybrid bootstrap offers superior\nperformance to dropout. We also present a sampling based technique to simplify\nhyperparameter choice. Next, we provide an alternative sampling technique for\nconvolutional neural networks. Finally, we demonstrate the efficacy of the\nhybrid bootstrap on non-image tasks using tree-based models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:50:26 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Kosar", "Robert", ""], ["Scott", "David W.", ""]]}, {"id": "1801.07318", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Seth R. Flaxman, Daniel E. Runcie, Mike West", "title": "Variable Prioritization in Nonlinear Black Box Methods: A Genetic\n  Association Case Study", "comments": "28 pages, 5 figures, 1 tables; Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The central aim in this paper is to address variable selection questions in\nnonlinear and nonparametric regression. Motivated by statistical genetics,\nwhere nonlinear interactions are of particular interest, we introduce a novel\nand interpretable way to summarize the relative importance of predictor\nvariables. Methodologically, we develop the \"RelATive cEntrality\" (RATE)\nmeasure to prioritize candidate genetic variants that are not just marginally\nimportant, but whose associations also stem from significant covarying\nrelationships with other variants in the data. We illustrate RATE through\nBayesian Gaussian process regression, but the methodological innovations apply\nto other \"black box\" methods. It is known that nonlinear models often exhibit\ngreater predictive accuracy than linear models, particularly for phenotypes\ngenerated by complex genetic architectures. With detailed simulations and two\nreal data association mapping studies, we show that applying RATE enables an\nexplanation for this improved performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:57:39 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 23:32:04 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 00:36:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Crawford", "Lorin", ""], ["Flaxman", "Seth R.", ""], ["Runcie", "Daniel E.", ""], ["West", "Mike", ""]]}, {"id": "1801.07353", "submitter": "Hokchhay Tann", "authors": "Hokchhay Tann, Soheil Hashemi, Sherief Reda", "title": "Flexible Deep Neural Network Processing", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of Deep Neural Networks (DNNs) has drastically improved\nthe state of the art for many application domains. While achieving high\naccuracy performance, deploying state-of-the-art DNNs is a challenge since they\ntypically require billions of expensive arithmetic computations. In addition,\nDNNs are typically deployed in ensemble to boost accuracy performance, which\nfurther exacerbates the system requirements. This computational overhead is an\nissue for many platforms, e.g. data centers and embedded systems, with tight\nlatency and energy budgets. In this article, we introduce flexible DNNs\nensemble processing technique, which achieves large reduction in average\ninference latency while incurring small to negligible accuracy drop. Our\ntechnique is flexible in that it allows for dynamic adaptation between quality\nof results (QoR) and execution runtime. We demonstrate the effectiveness of the\ntechnique on AlexNet and ResNet-50 using the ImageNet dataset. This technique\ncan also easily handle other types of networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 00:02:57 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Tann", "Hokchhay", ""], ["Hashemi", "Soheil", ""], ["Reda", "Sherief", ""]]}, {"id": "1801.07384", "submitter": "Hugh Chen", "authors": "Hugh Chen, Scott Lundberg, Su-In Lee", "title": "Hybrid Gradient Boosting Trees and Neural Networks for Forecasting\n  Operating Room Data", "comments": "Presented at Machine Learning for Health Workshop: 31st Conference on\n  Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data constitutes a distinct and growing problem in machine\nlearning. As the corpus of time series data grows larger, deep models that\nsimultaneously learn features and classify with these features can be\nintractable or suboptimal. In this paper, we present feature learning via long\nshort term memory (LSTM) networks and prediction via gradient boosting trees\n(XGB). Focusing on the consequential setting of electronic health record data,\nwe predict the occurrence of hypoxemia five minutes into the future based on\npast features. We make two observations: 1) long short term memory networks are\neffective at capturing long term dependencies based on a single feature and 2)\ngradient boosting trees are capable of tractably combining a large number of\nfeatures including static features like height and weight. With these\nobservations in mind, we generate features by performing \"supervised\"\nrepresentation learning with LSTM networks. Augmenting the original XGB model\nwith these features gives significantly better performance than either\nindividual method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 03:18:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 02:11:40 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Chen", "Hugh", ""], ["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1801.07389", "submitter": "Tao Sun", "authors": "Tao Sun, Linbo Qiao, Dongsheng Li", "title": "Non-ergodic Complexity of Convex Proximal Inertial Gradient Descents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proximal inertial gradient descent is efficient for the composite\nminimization and applicable for broad of machine learning problems. In this\npaper, we revisit the computational complexity of this algorithm and present\nother novel results, especially on the convergence rates of the objective\nfunction values. The non-ergodic O(1/k) rate is proved for proximal inertial\ngradient descent with constant stepzise when the objective function is\ncoercive. When the objective function fails to promise coercivity, we prove the\nsublinear rate with diminishing inertial parameters. In the case that the\nobjective function satisfies optimal strong convexity condition (which is much\nweaker than the strong convexity), the linear convergence is proved with much\nlarger and general stepsize than previous literature. We also extend our\nresults to the multi-block version and present the computational complexity.\nBoth cyclic and stochastic index selection strategies are considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 04:03:56 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 01:53:38 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 08:17:01 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sun", "Tao", ""], ["Qiao", "Linbo", ""], ["Li", "Dongsheng", ""]]}, {"id": "1801.07426", "submitter": "Chunna Li", "authors": "Chun-Na Li, Yuan-Hai Shao, Wei-Jie Chen, Zhen Wang and Nai-Yang Deng", "title": "Generalized two-dimensional linear discriminant analysis with\n  regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances show that two-dimensional linear discriminant analysis\n(2DLDA) is a successful matrix based dimensionality reduction method. However,\n2DLDA may encounter the singularity issue theoretically and the sensitivity to\noutliers. In this paper, a generalized Lp-norm 2DLDA framework with\nregularization for an arbitrary $p>0$ is proposed, named G2DLDA. There are\nmainly two contributions of G2DLDA: one is G2DLDA model uses an arbitrary\nLp-norm to measure the between-class and within-class scatter, and hence a\nproper $p$ can be selected to achieve the robustness. The other one is that by\nintroducing an extra regularization term, G2DLDA achieves better generalization\nperformance, and solves the singularity problem. In addition, G2DLDA can be\nsolved through a series of convex problems with equality constraint, and it has\nclosed solution for each single problem. Its convergence can be guaranteed\ntheoretically when $1\\leq p\\leq2$. Preliminary experimental results on three\ncontaminated human face databases show the effectiveness of the proposed\nG2DLDA.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:03:25 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 00:45:27 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Li", "Chun-Na", ""], ["Shao", "Yuan-Hai", ""], ["Chen", "Wei-Jie", ""], ["Wang", "Zhen", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1801.07508", "submitter": "Gael Sent\\'is", "authors": "Shang Yu, Chang-Jiang Huang, Jian-Shun Tang, Zhih-Ahn Jia, Yi-Tao\n  Wang, Zhi-Jin Ke, Wei Liu, Xiao Liu, Zong-Quan Zhou, Ze-Di Cheng, Jin-Shi Xu,\n  Yu-Chun Wu, Yuan-Yuan Zhao, Guo-Yong Xiang, Chuan-Feng Li, Guang-Can Guo,\n  Gael Sent\\'is, and Ramon Mu\\~noz-Tapia", "title": "Experimentally detecting a quantum change point via Bayesian inference", "comments": null, "journal-ref": "Phys. Rev. A 98, 040301(R) (2018)", "doi": "10.1103/PhysRevA.98.040301", "report-no": null, "categories": "quant-ph physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting a change point is a crucial task in statistics that has been\nrecently extended to the quantum realm. A source state generator that emits a\nseries of single photons in a default state suffers an alteration at some point\nand starts to emit photons in a mutated state. The problem consists in\nidentifying the point where the change took place. In this work, we consider a\nlearning agent that applies Bayesian inference on experimental data to solve\nthis problem. This learning machine adjusts the measurement over each photon\naccording to the past experimental results finds the change position in an\nonline fashion. Our results show that the local-detection success probability\ncan be largely improved by using such a machine learning technique. This\nprotocol provides a tool for improvement in many applications where a sequence\nof identical quantum states is required.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 12:27:12 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Yu", "Shang", ""], ["Huang", "Chang-Jiang", ""], ["Tang", "Jian-Shun", ""], ["Jia", "Zhih-Ahn", ""], ["Wang", "Yi-Tao", ""], ["Ke", "Zhi-Jin", ""], ["Liu", "Wei", ""], ["Liu", "Xiao", ""], ["Zhou", "Zong-Quan", ""], ["Cheng", "Ze-Di", ""], ["Xu", "Jin-Shi", ""], ["Wu", "Yu-Chun", ""], ["Zhao", "Yuan-Yuan", ""], ["Xiang", "Guo-Yong", ""], ["Li", "Chuan-Feng", ""], ["Guo", "Guang-Can", ""], ["Sent\u00eds", "Gael", ""], ["Mu\u00f1oz-Tapia", "Ramon", ""]]}, {"id": "1801.07606", "submitter": "Qimai Li", "authors": "Qimai Li, Zhichao Han, Xiao-Ming Wu", "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised\n  Learning", "comments": "AAAI-2018 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting problems in machine learning are being revisited with new\ndeep learning tools. For graph-based semisupervised learning, a recent\nimportant development is graph convolutional networks (GCNs), which nicely\nintegrate local vertex features and graph topology in the convolutional layers.\nAlthough the GCN model compares favorably with other state-of-the-art methods,\nits mechanisms are not clear and it still requires a considerable amount of\nlabeled data for validation and model selection. In this paper, we develop\ndeeper insights into the GCN model and address its fundamental limits. First,\nwe show that the graph convolution of the GCN model is actually a special form\nof Laplacian smoothing, which is the key reason why GCNs work, but it also\nbrings potential concerns of over-smoothing with many convolutional layers.\nSecond, to overcome the limits of the GCN model with shallow architectures, we\npropose both co-training and self-training approaches to train GCNs. Our\napproaches significantly improve GCNs in learning with very few labels, and\nexempt them from requiring additional labels for validation. Extensive\nexperiments on benchmarks have verified our theory and proposals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:24:24 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Li", "Qimai", ""], ["Han", "Zhichao", ""], ["Wu", "Xiao-Ming", ""]]}, {"id": "1801.07615", "submitter": "J\\\"org Herbel", "authors": "J\\\"org Herbel, Tomasz Kacprzak, Adam Amara, Alexandre Refregier,\n  Aurelien Lucchi (ETH Zurich)", "title": "Fast Point Spread Function Modeling with Deep Learning", "comments": "25 pages, 8 figures, 1 table", "journal-ref": null, "doi": "10.1088/1475-7516/2018/07/054", "report-no": null, "categories": "astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the Point Spread Function (PSF) of wide-field surveys is vital for\nmany astrophysical applications and cosmological probes including weak\ngravitational lensing. The PSF smears the image of any recorded object and\ntherefore needs to be taken into account when inferring properties of galaxies\nfrom astronomical images. In the case of cosmic shear, the PSF is one of the\ndominant sources of systematic errors and must be treated carefully to avoid\nbiases in cosmological parameters. Recently, forward modeling approaches to\ncalibrate shear measurements within the Monte-Carlo Control Loops ($MCCL$)\nframework have been developed. These methods typically require simulating a\nlarge amount of wide-field images, thus, the simulations need to be very fast\nyet have realistic properties in key features such as the PSF pattern. Hence,\nsuch forward modeling approaches require a very flexible PSF model, which is\nquick to evaluate and whose parameters can be estimated reliably from survey\ndata. We present a PSF model that meets these requirements based on a fast\ndeep-learning method to estimate its free parameters. We demonstrate our\napproach on publicly available SDSS data. We extract the most important\nfeatures of the SDSS sample via principal component analysis. Next, we\nconstruct our model based on perturbations of a fixed base profile, ensuring\nthat it captures these features. We then train a Convolutional Neural Network\nto estimate the free parameters of the model from noisy images of the PSF. This\nallows us to render a model image of each star, which we compare to the SDSS\nstars to evaluate the performance of our method. We find that our approach is\nable to accurately reproduce the SDSS PSF at the pixel level, which, due to the\nspeed of both the model evaluation and the parameter estimation, offers good\nprospects for incorporating our method into the $MCCL$ framework.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 15:31:33 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:20:54 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Herbel", "J\u00f6rg", "", "ETH Zurich"], ["Kacprzak", "Tomasz", "", "ETH Zurich"], ["Amara", "Adam", "", "ETH Zurich"], ["Refregier", "Alexandre", "", "ETH Zurich"], ["Lucchi", "Aurelien", "", "ETH Zurich"]]}, {"id": "1801.07644", "submitter": "Hao Zhou", "authors": "Hao Henry Zhou and Garvesh Raskutti", "title": "Non-parametric Sparse Additive Auto-regressive Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \\in\n\\mathbb{R}^d$ which may represent spike train responses for multiple neurons in\na brain, crime event data across multiple regions, and many others. An\nimportant challenge associated with these time series models is to estimate an\ninfluence network between the $d$ variables, especially when the number of\nvariables $d$ is large meaning we are in the high-dimensional setting. Prior\nwork has focused on parametric vector auto-regressive models. However,\nparametric approaches are somewhat restrictive in practice. In this paper, we\nuse the non-parametric sparse additive model (SpAM) framework to address this\nchallenge. Using a combination of $\\beta$ and $\\phi$-mixing properties of\nMarkov chains and empirical process techniques for reproducing kernel Hilbert\nspaces (RKHSs), we provide upper bounds on mean-squared error in terms of the\nsparsity $s$, logarithm of the dimension $\\log d$, number of time points $T$,\nand the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in\nmany cases. We also provide numerical experiments that support our theoretical\nresults and display potential advantages of using our non-parametric SpAM\nframework for a Chicago crime dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:34:11 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 20:16:18 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Zhou", "Hao Henry", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1801.07648", "submitter": "Elie Aljalbout", "authors": "Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel,\n  Daniel Cremers", "title": "Clustering with Deep Learning: Taxonomy and New Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods based on deep neural networks have proven promising for\nclustering real-world data because of their high representational power. In\nthis paper, we propose a systematic taxonomy of clustering methods that utilize\ndeep neural networks. We base our taxonomy on a comprehensive review of recent\nwork and validate the taxonomy in a case study. In this case study, we show\nthat the taxonomy enables researchers and practitioners to systematically\ncreate new clustering methods by selectively recombining and replacing distinct\naspects of previous methods with the goal of overcoming their individual\nlimitations. The experimental evaluation confirms this and shows that the\nmethod created for the case study achieves state-of-the-art clustering quality\nand surpasses it in some cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:41:03 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:41:22 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Aljalbout", "Elie", ""], ["Golkov", "Vladimir", ""], ["Siddiqui", "Yawar", ""], ["Strobel", "Maximilian", ""], ["Cremers", "Daniel", ""]]}, {"id": "1801.07650", "submitter": "Shinichi Shirakawa", "authors": "Shinichi Shirakawa, Yasushi Iwata, Youhei Akimoto", "title": "Dynamic Optimization of Neural Network Structures Using Probabilistic\n  Modeling", "comments": "To appear in the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18), 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful machine learning models and have\nsucceeded in various artificial intelligence tasks. Although various\narchitectures and modules for the DNNs have been proposed, selecting and\ndesigning the appropriate network structure for a target problem is a\nchallenging task. In this paper, we propose a method to simultaneously optimize\nthe network structure and weight parameters during neural network training. We\nconsider a probability distribution that generates network structures, and\noptimize the parameters of the distribution instead of directly optimizing the\nnetwork structure. The proposed method can apply to the various network\nstructure optimization problems under the same framework. We apply the proposed\nmethod to several structure optimization problems such as selection of layers,\nselection of unit types, and selection of connections using the MNIST,\nCIFAR-10, and CIFAR-100 datasets. The experimental results show that the\nproposed method can find the appropriate and competitive network structures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:43:59 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Shirakawa", "Shinichi", ""], ["Iwata", "Yasushi", ""], ["Akimoto", "Youhei", ""]]}, {"id": "1801.07654", "submitter": "Pablo Barros", "authors": "Pablo Barros, German I. Parisi, Di Fu, Xun Liu, and Stefan Wermter", "title": "Expectation Learning for Adaptive Crossmodal Stimuli Association", "comments": "3 pages 2017 EUCog meeting abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human brain is able to learn, generalize, and predict crossmodal stimuli.\nLearning by expectation fine-tunes crossmodal processing at different levels,\nthus enhancing our power of generalization and adaptation in highly dynamic\nenvironments. In this paper, we propose a deep neural architecture trained by\nusing expectation learning accounting for unsupervised learning tasks. Our\nlearning model exhibits a self-adaptable behavior, setting the first steps\ntowards the development of deep learning architectures for crossmodal stimuli\nassociation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:47:32 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Barros", "Pablo", ""], ["Parisi", "German I.", ""], ["Fu", "Di", ""], ["Liu", "Xun", ""], ["Wermter", "Stefan", ""]]}, {"id": "1801.07668", "submitter": "Ivo Gon\\c{c}alves", "authors": "Mauro Castelli, Ivo Gon\\c{c}alves, Luca Manzoni, Leonardo Vanneschi", "title": "Pruning Techniques for Mixed Ensembles of Genetic Programming Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-77553-1_4", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to define an effective strategy for building\nan ensemble of Genetic Programming (GP) models. Ensemble methods are widely\nused in machine learning due to their features: they average out biases, they\nreduce the variance and they usually generalize better than single models.\nDespite these advantages, building ensemble of GP models is not a\nwell-developed topic in the evolutionary computation community. To fill this\ngap, we propose a strategy that blends individuals produced by standard\nsyntax-based GP and individuals produced by geometric semantic genetic\nprogramming, one of the newest semantics-based method developed in GP. In fact,\nrecent literature showed that combining syntax and semantics could improve the\ngeneralization ability of a GP model. Additionally, to improve the diversity of\nthe GP models used to build up the ensemble, we propose different pruning\ncriteria that are based on correlation and entropy, a commonly used measure in\ninformation theory. Experimental results,obtained over different complex\nproblems, suggest that the pruning criteria based on correlation and entropy\ncould be effective in improving the generalization ability of the ensemble\nmodel and in reducing the computational burden required to build it.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 17:29:22 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Castelli", "Mauro", ""], ["Gon\u00e7alves", "Ivo", ""], ["Manzoni", "Luca", ""], ["Vanneschi", "Leonardo", ""]]}, {"id": "1801.07691", "submitter": "Junfeng Liu", "authors": "Yicheng He, Junfeng Liu and Xia Ning", "title": "Drug Selection via Joint Push and Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the right drugs for the right patients is a primary goal of\nprecision medicine. In this manuscript, we consider the problem of cancer drug\nselection in a learning-to-rank framework. We have formulated the cancer drug\nselection problem as to accurately predicting 1). the ranking positions of\nsensitive drugs and 2). the ranking orders among sensitive drugs in cancer cell\nlines based on their responses to cancer drugs. We have developed a new\nlearning-to-rank method, denoted as pLETORg , that predicts drug ranking\nstructures in each cell line via using drug latent vectors and cell line latent\nvectors. The pLETORg method learns such latent vectors through explicitly\nenforcing that, in the drug ranking list of each cell line, the sensitive drugs\nare pushed above insensitive drugs, and meanwhile the ranking orders among\nsensitive drugs are correct. Genomics information on cell lines is leveraged in\nlearning the latent vectors. Our experimental results on a benchmark cell\nline-drug response dataset demonstrate that the new pLETORg significantly\noutperforms the state-of-the-art method in prioritizing new sensitive drugs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 18:26:54 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 22:50:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["He", "Yicheng", ""], ["Liu", "Junfeng", ""], ["Ning", "Xia", ""]]}, {"id": "1801.07710", "submitter": "Vikram Mullachery", "authors": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "title": "Bayesian Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1111.4246 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and discusses Bayesian Neural Network (BNN). The paper\nshowcases a few different applications of them for classification and\nregression problems. BNNs are comprised of a Probabilistic Model and a Neural\nNetwork. The intent of such a design is to combine the strengths of Neural\nNetworks and Stochastic modeling. Neural Networks exhibit continuous function\napproximator capabilities. Stochastic models allow direct specification of a\nmodel with known interaction between parameters to generate data. During the\nprediction phase, stochastic models generate a complete posterior distribution\nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique\ncombination of neural network and stochastic models with the stochastic model\nforming the core of this integration. BNNs can then produce probabilistic\nguarantees on it's predictions and also generate the distribution of parameters\nthat it has learnt from the observations. That means, in the parameter space,\none can deduce the nature and shape of the neural network's learnt parameters.\nThese two characteristics makes them highly attractive to theoreticians as well\nas practitioners. Recently there has been a lot of activity in this area, with\nthe advent of numerous probabilistic programming libraries such as: PyMC3,\nEdward, Stan etc. Further this area is rapidly gaining ground as a standard\nmachine learning approach for numerous problems\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 20:52:44 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 15:30:26 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Mullachery", "Vikram", ""], ["Khera", "Aniruddh", ""], ["Husain", "Amir", ""]]}, {"id": "1801.07736", "submitter": "William Fedus", "authors": "William Fedus, Ian Goodfellow and Andrew M. Dai", "title": "MaskGAN: Better Text Generation via Filling in the______", "comments": "16 pages, ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation models are often autoregressive language models or\nseq2seq models. These models generate text by sampling words sequentially, with\neach word conditioned on the previous word, and are state-of-the-art for\nseveral machine translation and summarization benchmarks. These benchmarks are\noften defined by validation perplexity even though this is not a direct measure\nof the quality of the generated text. Additionally, these models are typically\ntrained via maxi- mum likelihood and teacher forcing. These methods are\nwell-suited to optimizing perplexity but can result in poor sample quality\nsince generating text requires conditioning on sequences of words that may have\nnever been observed at training time. We propose to improve sample quality\nusing Generative Adversarial Networks (GANs), which explicitly train the\ngenerator to produce high quality samples and have shown a lot of success in\nimage generation. GANs were originally designed to output differentiable\nvalues, so discrete language generation is challenging for them. We claim that\nvalidation perplexity alone is not indicative of the quality of text generated\nby a model. We introduce an actor-critic conditional GAN that fills in missing\ntext conditioned on the surrounding context. We show qualitatively and\nquantitatively, evidence that this produces more realistic conditional and\nunconditional text samples compared to a maximum likelihood trained model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:22:21 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:26:04 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 15:30:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Fedus", "William", ""], ["Goodfellow", "Ian", ""], ["Dai", "Andrew M.", ""]]}, {"id": "1801.07756", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Cheikh Latyr Fall, Alexandre Drouin, Alexandre\n  Campeau-Lecours, Cl\\'ement Gosselin, Kyrre Glette, Fran\\c{c}ois Laviolette,\n  Benoit Gosselin", "title": "Deep Learning for Electromyographic Hand Gesture Signal Classification\n  Using Transfer Learning", "comments": "Source code and datasets available:\n  https://github.com/Giguelingueling/MyoArmbandDataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning algorithms have become increasingly more\nprominent for their unparalleled ability to automatically learn discriminant\nfeatures from large amounts of data. However, within the field of\nelectromyography-based gesture recognition, deep learning algorithms are seldom\nemployed as they require an unreasonable amount of effort from a single person,\nto generate tens of thousands of examples.\n  This work's hypothesis is that general, informative features can be learned\nfrom the large amounts of data generated by aggregating the signals of multiple\nusers, thus reducing the recording burden while enhancing gesture recognition.\nConsequently, this paper proposes applying transfer learning on aggregated data\nfrom multiple users, while leveraging the capacity of deep learning algorithms\nto learn discriminant features from large datasets. Two datasets comprised of\n19 and 17 able-bodied participants respectively (the first one is employed for\npre-training) were recorded for this work, using the Myo Armband. A third Myo\nArmband dataset was taken from the NinaPro database and is comprised of 10\nable-bodied participants. Three different deep learning networks employing\nthree different modalities as input (raw EMG, Spectrograms and Continuous\nWavelet Transform (CWT)) are tested on the second and third dataset. The\nproposed transfer learning scheme is shown to systematically and significantly\nenhance the performance for all three networks on the two datasets, achieving\nan offline accuracy of 98.31% for 7 gestures over 17 participants for the\nCWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw\nEMG-based ConvNet. Finally, a use-case study employing eight able-bodied\nparticipants suggests that real-time feedback allows users to adapt their\nmuscle activation strategy which reduces the degradation in accuracy normally\nexperienced over time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 11:42:30 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 14:29:42 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 09:25:44 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2018 16:48:25 GMT"}, {"version": "v5", "created": "Sat, 26 Jan 2019 04:00:47 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Fall", "Cheikh Latyr", ""], ["Drouin", "Alexandre", ""], ["Campeau-Lecours", "Alexandre", ""], ["Gosselin", "Cl\u00e9ment", ""], ["Glette", "Kyrre", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Gosselin", "Benoit", ""]]}, {"id": "1801.07807", "submitter": "Ishanu Chattopadhyay", "authors": "Jaideep Dhanoa, Balaji Manicassamy and Ishanu Chattopadhyay", "title": "Algorithmic Bio-surveillance For Precise Spatio-temporal Prediction of\n  Zoonotic Emergence", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viral zoonoses have emerged as the key drivers of recent pandemics. Human\ninfection by zoonotic viruses are either spillover events -- isolated\ninfections that fail to cause a widespread contagion -- or species jumps, where\nsuccessful adaptation to the new host leads to a pandemic. Despite expensive\nbio-surveillance efforts, historically emergence response has been reactive,\nand post-hoc. Here we use machine inference to demonstrate a high accuracy\npredictive bio-surveillance capability, designed to pro-actively localize an\nimpending species jump via automated interrogation of massive sequence\ndatabases of viral proteins. Our results suggest that a jump might not purely\nbe the result of an isolated unfortunate cross-infection localized in space and\ntime; there are subtle yet detectable patterns of genotypic changes\naccumulating in the global viral population leading up to emergence. Using tens\nof thousands of protein sequences simultaneously, we train models that track\nmaximum achievable accuracy for disambiguating host tropism from the primary\nstructure of surface proteins, and show that the inverse classification\naccuracy is a quantitative indicator of jump risk. We validate our claim in the\ncontext of the 2009 swine flu outbreak, and the 2004 emergence of H5N1\nsubspecies of Influenza A from avian reservoirs; illustrating that\ninterrogation of the global viral population can unambiguously track a near\nmonotonic risk elevation over several preceding years leading to eventual\nemergence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 23:27:31 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Dhanoa", "Jaideep", ""], ["Manicassamy", "Balaji", ""], ["Chattopadhyay", "Ishanu", ""]]}, {"id": "1801.07826", "submitter": "Susan Athey", "authors": "Susan Athey, David Blei, Robert Donnelly, Francisco Ruiz, and Tobias\n  Schmidt", "title": "Estimating Heterogeneous Consumer Preferences for Restaurants and Travel\n  Time Using Mobile Location Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes consumer choices over lunchtime restaurants using data\nfrom a sample of several thousand anonymous mobile phone users in the San\nFrancisco Bay Area. The data is used to identify users' approximate typical\nmorning location, as well as their choices of lunchtime restaurants. We build a\nmodel where restaurants have latent characteristics (whose distribution may\ndepend on restaurant observables, such as star ratings, food category, and\nprice range), each user has preferences for these latent characteristics, and\nthese preferences are heterogeneous across users. Similarly, each item has\nlatent characteristics that describe users' willingness to travel to the\nrestaurant, and each user has individual-specific preferences for those latent\ncharacteristics. Thus, both users' willingness to travel and their base utility\nfor each restaurant vary across user-restaurant pairs. We use a Bayesian\napproach to estimation. To make the estimation computationally feasible, we\nrely on variational inference to approximate the posterior distribution, as\nwell as stochastic gradient descent as a computational approach. Our model\nperforms better than more standard competing models such as multinomial logit\nand nested logit models, in part due to the personalization of the estimates.\nWe analyze how consumers re-allocate their demand after a restaurant closes to\nnearby restaurants versus more distant restaurants with similar\ncharacteristics, and we compare our predictions to actual outcomes. Finally, we\nshow how the model can be used to analyze counterfactual questions such as what\ntype of restaurant would attract the most consumers in a given location.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 23:55:42 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Athey", "Susan", ""], ["Blei", "David", ""], ["Donnelly", "Robert", ""], ["Ruiz", "Francisco", ""], ["Schmidt", "Tobias", ""]]}, {"id": "1801.07827", "submitter": "Ming Zeng", "authors": "Ming Zeng, Tong Yu, Xiao Wang, Le T. Nguyen, Ole J. Mengshoel, Ian\n  Lane", "title": "Semi-Supervised Convolutional Neural Networks for Human Activity\n  Recognition", "comments": "Accepted by BigData2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled data used for training activity recognition classifiers are usually\nlimited in terms of size and diversity. Thus, the learned model may not\ngeneralize well when used in real-world use cases. Semi-supervised learning\naugments labeled examples with unlabeled examples, often resulting in improved\nperformance. However, the semi-supervised methods studied in the activity\nrecognition literatures assume that feature engineering is already done. In\nthis paper, we lift this assumption and present two semi-supervised methods\nbased on convolutional neural networks (CNNs) to learn discriminative hidden\nfeatures. Our semi-supervised CNNs learn from both labeled and unlabeled data\nwhile also performing feature learning on raw sensor data. In experiments on\nthree real world datasets, we show that our CNNs outperform supervised methods\nand traditional semi-supervised learning methods by up to 18% in mean F1-score\n(Fm).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:18:16 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Zeng", "Ming", ""], ["Yu", "Tong", ""], ["Wang", "Xiao", ""], ["Nguyen", "Le T.", ""], ["Mengshoel", "Ole J.", ""], ["Lane", "Ian", ""]]}, {"id": "1801.07873", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, David J. Nott, Robert Kohn", "title": "Gaussian variational approximation for high-dimensional state space\n  models", "comments": "Significantly revised, especially the multivariate stochastic\n  volatility model example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article considers a Gaussian variational approximation of the posterior\ndensity in a high-dimensional state space model. The variational parameters to\nbe optimized are the mean vector and the covariance matrix of the\napproximation. The number of parameters in the covariance matrix grows as the\nsquare of the number of model parameters, so it is necessary to find simple yet\neffective parameterizations of the covariance structure when the number of\nmodel parameters is large. We approximate the joint posterior distribution over\nthe high-dimensional state vectors by a dynamic factor model, having Markovian\ntime dependence and a factor covariance structure for the states. This gives a\nreduced description of the dependence structure for the states, as well as a\ntemporal conditional independence structure similar to that in the true\nposterior. The usefulness of the approach is illustrated for prediction in two\nhigh-dimensional applications that are challenging for Markov chain Monte Carlo\nsampling. The first is a spatio-temporal model for the spread of the Eurasian\nCollared-Dove across North America; the second is a Wishart-based multivariate\nstochastic volatility model for financial returns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 06:30:46 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 05:05:24 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 01:28:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Quiroz", "Matias", ""], ["Nott", "David J.", ""], ["Kohn", "Robert", ""]]}, {"id": "1801.07875", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood", "title": "Support Vector Machine Active Learning Algorithms with\n  Query-by-Committee versus Closest-to-Hyperplane Selection", "comments": "8 pages, 7 figures, 3 tables; published in Proceedings of the IEEE\n  12th International Conference on Semantic Computing (ICSC 2018), Laguna\n  Hills, CA, USA, pages 148-155, January 2018", "journal-ref": "In Proceedings of the 2018 IEEE 12th International Conference on\n  Semantic Computing (ICSC), pages 148-155, Laguna Hills, CA, USA, January\n  2018. IEEE", "doi": "10.1109/ICSC.2018.00029", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates and evaluates support vector machine active learning\nalgorithms for use with imbalanced datasets, which commonly arise in many\napplications such as information extraction applications. Algorithms based on\nclosest-to-hyperplane selection and query-by-committee selection are combined\nwith methods for addressing imbalance such as positive amplification based on\nprevalence statistics from initial random samples. Three algorithms (ClosestPA,\nQBagPA, and QBoostPA) are presented and carefully evaluated on datasets for\ntext classification and relation extraction. The ClosestPA algorithm is shown\nto consistently outperform the other two in a variety of ways and insights are\nprovided as to why this is the case.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 06:38:06 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 19:16:47 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Bloodgood", "Michael", ""]]}, {"id": "1801.07883", "submitter": "Lei Zhang", "authors": "Lei Zhang, Shuai Wang, Bing Liu", "title": "Deep Learning for Sentiment Analysis : A Survey", "comments": "34 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 07:32:29 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 07:20:41 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhang", "Lei", ""], ["Wang", "Shuai", ""], ["Liu", "Bing", ""]]}, {"id": "1801.07887", "submitter": "Michael Bloodgood", "authors": "Garrett Beatty, Ethan Kochis and Michael Bloodgood", "title": "Impact of Batch Size on Stopping Active Learning for Text Classification", "comments": "2 pages, 1 table; published in Proceedings of the IEEE 12th\n  International Conference on Semantic Computing (ICSC 2018), Laguna Hills, CA,\n  USA, pages 306-307, January 2018", "journal-ref": "In Proceedings of the 2018 IEEE 12th International Conference on\n  Semantic Computing (ICSC), pages 306-307, Laguna Hills, CA, USA, January\n  2018. IEEE", "doi": "10.1109/ICSC.2018.00059", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using active learning, smaller batch sizes are typically more efficient\nfrom a learning efficiency perspective. However, in practice due to speed and\nhuman annotator considerations, the use of larger batch sizes is necessary.\nWhile past work has shown that larger batch sizes decrease learning efficiency\nfrom a learning curve perspective, it remains an open question how batch size\nimpacts methods for stopping active learning. We find that large batch sizes\ndegrade the performance of a leading stopping method over and above the\ndegradation that results from reduced learning efficiency. We analyze this\ndegradation and find that it can be mitigated by changing the window size\nparameter of how many past iterations of learning are taken into account when\nmaking the stopping decision. We find that when using larger batch sizes,\nstopping methods are more effective when smaller window sizes are used.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 07:47:05 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 18:31:45 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Beatty", "Garrett", ""], ["Kochis", "Ethan", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1801.07889", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Francesco Cricri, Lixin Fan and Emre Aksu", "title": "A Theoretical Investigation of Graph Degree as an Unsupervised Normality\n  Measure", "comments": "Submitted to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph representation of a dataset, a straightforward normality measure\nfor a sample can be its graph degree. Considering a weighted graph, degree of a\nsample is the sum of the corresponding row's values in a similarity matrix. The\nmeasure is intuitive given the abnormal samples are usually rare and they are\ndissimilar to the rest of the data. In order to have an in-depth theoretical\nunderstanding, in this manuscript, we investigate the graph degree in spectral\ngraph clustering based and kernel based point of views and draw connections to\na recent kernel method for the two sample problem. We show that our analyses\nguide us to choose fully-connected graphs whose edge weights are calculated via\nuniversal kernels. We show that a simple graph degree based unsupervised\nanomaly detection method with the above properties, achieves higher accuracy\ncompared to other unsupervised anomaly detection methods on average over 10\nwidely used datasets. We also provide an extensive analysis on the effect of\nthe kernel parameter on the method's accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 07:58:34 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 08:40:18 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 11:19:39 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Aytekin", "Caglar", ""], ["Cricri", "Francesco", ""], ["Fan", "Lixin", ""], ["Aksu", "Emre", ""]]}, {"id": "1801.08019", "submitter": "Xuezhou Zhang", "authors": "Xuezhou Zhang, Xiaojin Zhu and Stephen J. Wright", "title": "Training Set Debugging Using Trusted Items", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training set bugs are flaws in the data that adversely affect machine\nlearning. The training set is usually too large for man- ual inspection, but\none may have the resources to verify a few trusted items. The set of trusted\nitems may not by itself be adequate for learning, so we propose an algorithm\nthat uses these items to identify bugs in the training set and thus im- proves\nlearning. Specifically, our approach seeks the smallest set of changes to the\ntraining set labels such that the model learned from this corrected training\nset predicts labels of the trusted items correctly. We flag the items whose\nlabels are changed as potential bugs, whose labels can be checked for veracity\nby human experts. To find the bugs in this way is a challenging combinatorial\nbilevel optimization problem, but it can be relaxed into a continuous\noptimization problem. Ex- periments on toy and real data demonstrate that our\napproach can identify training set bugs effectively and suggest appro- priate\nchanges to the labels. Our algorithm is a step toward trustworthy machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 15:21:57 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Zhang", "Xuezhou", ""], ["Zhu", "Xiaojin", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1801.08085", "submitter": "Meysam Golmohammadi", "authors": "Vinit Shah, Eva von Weltin, Silvia Lopez, James Riley McHugh, Lily\n  Veloso, Meysam Golmohammadi, Iyad Obeid and Joseph Picone", "title": "The Temple University Hospital Seizure Detection Corpus", "comments": "Under review in Frontiers in Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the TUH EEG Seizure Corpus (TUSZ), which is the largest open\nsource corpus of its type, and represents an accurate characterization of\nclinical conditions. In this paper, we describe the techniques used to develop\nTUSZ, evaluate their effectiveness, and present some descriptive statistics on\nthe resulting corpus.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 01:16:26 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Shah", "Vinit", ""], ["von Weltin", "Eva", ""], ["Lopez", "Silvia", ""], ["McHugh", "James Riley", ""], ["Veloso", "Lily", ""], ["Golmohammadi", "Meysam", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.08094", "submitter": "Kui Zhao", "authors": "Kui Zhao, Yuechuan Li, Chi Zhang, Cheng Yang, Huan Xu", "title": "Adaptive Recurrent Neural Network Based on Mixture Layer", "comments": "7 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although Recurrent Neural Network (RNN) has been a powerful tool for modeling\nsequential data, its performance is inadequate when processing sequences with\nmultiple patterns. In this paper, we address this challenge by introducing a\nnovel mixture layer and constructing an adaptive RNN. The mixture layer\naugmented RNN (termed as M-RNN) partitions patterns in training sequences into\nseveral clusters and stores the principle patterns as prototype vectors of\ncomponents in a mixture model. By leveraging the mixture layer, the proposed\nmethod can adaptively update states according to the similarities between\nencoded inputs and prototype vectors, leading to a stronger capacity in\nassimilating sequences with multiple patterns. Moreover, our approach can be\nfurther extended by taking advantage of prior knowledge about data. Experiments\non both synthetic and real datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:38:48 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 09:19:59 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 09:45:30 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 01:54:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Zhao", "Kui", ""], ["Li", "Yuechuan", ""], ["Zhang", "Chi", ""], ["Yang", "Cheng", ""], ["Xu", "Huan", ""]]}, {"id": "1801.08196", "submitter": "Baichuan Zhang", "authors": "Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan", "title": "Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory\n  and Applications", "comments": "Accept to publish in Social Network Analysis and Mining. arXiv admin\n  note: text overlap with arXiv:1512.07349", "journal-ref": "Social Network Analysis and Mining, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)\nof a graph Laplacian matrix have been widely used in spectral clustering and\ncommunity detection. However, in real-life applications the number of clusters\nor communities (say, $K$) is generally unknown a-priori. Consequently, the\nmajority of the existing methods either choose $K$ heuristically or they repeat\nthe clustering method with different choices of $K$ and accept the best\nclustering result. The first option, more often, yields suboptimal result,\nwhile the second option is computationally expensive. In this work, we propose\nan incremental method for constructing the eigenspectrum of the graph Laplacian\nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to\nobtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection\nof all previously computed $K-1$ smallest eigenpairs. Our proposed method\nadapts the Laplacian matrix such that the batch eigenvalue decomposition\nproblem transforms into an efficient sequential leading eigenpair computation\nproblem. As a practical application, we consider user-guided spectral\nclustering. Specifically, we demonstrate that users can utilize the proposed\nincremental method for effective eigenpair computation and for determining the\ndesired number of clusters based on multiple clustering metrics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 19:04:35 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Zhang", "Baichuan", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1801.08227", "submitter": "Haolei Weng", "authors": "Rahul Mazumder, Diego F. Saldana, Haolei Weng", "title": "Matrix Completion with Nonconvex Regularization: Spectral Operators and\n  Scalable Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the popularly dubbed matrix completion problem, where\nthe task is to \"fill in\" the unobserved entries of a matrix from a small subset\nof observed entries, under the assumption that the underlying matrix is of\nlow-rank. Our contributions herein, enhance our prior work on nuclear norm\nregularized problems for matrix completion (Mazumder et al., 2010) by\nincorporating a continuum of nonconvex penalty functions between the convex\nnuclear norm and nonconvex rank functions. Inspired by SOFT-IMPUTE (Mazumder et\nal., 2010; Hastie et al., 2016), we propose NC-IMPUTE- an EM-flavored\nalgorithmic framework for computing a family of nonconvex penalized matrix\ncompletion problems with warm-starts. We present a systematic study of the\nassociated spectral thresholding operators, which play an important role in the\noverall algorithm. We study convergence properties of the algorithm. Using\nstructured low-rank SVD computations, we demonstrate the computational\nscalability of our proposal for problems up to the Netflix size (approximately,\na $500,000 \\times 20, 000$ matrix with $10^8$ observed entries). We demonstrate\nthat on a wide range of synthetic and real data instances, our proposed\nnonconvex regularization framework leads to low-rank solutions with better\npredictive performance when compared to those obtained from nuclear norm\nproblems. Implementations of algorithms proposed herein, written in the R\nprogramming language, are made available on github.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 22:42:36 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 18:07:47 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Mazumder", "Rahul", ""], ["Saldana", "Diego F.", ""], ["Weng", "Haolei", ""]]}, {"id": "1801.08256", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay", "title": "A Hilbert Space of Stationary Ergodic Processes", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying meaningful signal buried in noise is a problem of interest\narising in diverse scenarios of data-driven modeling. We present here a\ntheoretical framework for exploiting intrinsic geometry in data that resists\nnoise corruption, and might be identifiable under severe obfuscation. Our\napproach is based on uncovering a valid complete inner product on the space of\nergodic stationary finite valued processes, providing the latter with the\nstructure of a Hilbert space on the real field. This rigorous construction,\nbased on non-standard generalizations of the notions of sum and scalar\nmultiplication of finite dimensional probability vectors, allows us to\nmeaningfully talk about \"angles\" between data streams and data sources, and,\nmake precise the notion of orthogonal stochastic processes. In particular, the\nrelative angles appear to be preserved, and identifiable, under severe noise,\nand will be developed in future as the underlying principle for robust\nclassification, clustering and unsupervised featurization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 02:18:49 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Chattopadhyay", "Ishanu", ""]]}, {"id": "1801.08273", "submitter": "Yingxiang Yang", "authors": "Yingxiang Yang, Jalal Etesami, Niao He, Negar Kiyavash", "title": "Nonparametric Hawkes Processes: Online Estimation and Generalization\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a nonparametric online algorithm for estimating the\ntriggering functions of multivariate Hawkes processes. Unlike parametric\nestimation, where evolutionary dynamics can be exploited for fast computation\nof the gradient, and unlike typical function learning, where representer\ntheorem is readily applicable upon proper regularization of the objective\nfunction, nonparametric estimation faces the challenges of (i) inefficient\nevaluation of the gradient, (ii) lack of representer theorem, and (iii)\ncomputationally expensive projection necessary to guarantee positivity of the\ntriggering functions. In this paper, we offer solutions to the above\nchallenges, and design an online estimation algorithm named NPOLE-MHP that\noutputs estimations with a $\\mathcal{O}(1/T)$ regret, and a $\\mathcal{O}(1/T)$\nstability. Furthermore, we design an algorithm, NPOLE-MMHP, for estimation of\nmultivariate marked Hawkes processes. We test the performance of NPOLE-MHP on\nvarious synthetic and real datasets, and demonstrate, under different\nevaluation metrics, that NPOLE-MHP performs as good as the optimal maximum\nlikelihood estimation (MLE), while having a run time as little as parametric\nonline algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 04:47:38 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Yang", "Yingxiang", ""], ["Etesami", "Jalal", ""], ["He", "Niao", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1801.08284", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo", "title": "DKN: Deep Knowledge-Aware Network for News Recommendation", "comments": "The 27th International Conference on World Wide Web (WWW'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online news recommender systems aim to address the information explosion of\nnews and make personalized recommendation for users. In general, news language\nis highly condensed, full of knowledge entities and common sense. However,\nexisting methods are unaware of such external knowledge and cannot fully\ndiscover latent knowledge-level connections among news. The recommended results\nfor a user are consequently limited to simple patterns and cannot be extended\nreasonably. Moreover, news recommendation also faces the challenges of high\ntime-sensitivity of news and dynamic diversity of users' interests. To solve\nthe above problems, in this paper, we propose a deep knowledge-aware network\n(DKN) that incorporates knowledge graph representation into news\nrecommendation. DKN is a content-based deep recommendation framework for\nclick-through rate prediction. The key component of DKN is a multi-channel and\nword-entity-aligned knowledge-aware convolutional neural network (KCNN) that\nfuses semantic-level and knowledge-level representations of news. KCNN treats\nwords and entities as multiple channels, and explicitly keeps their alignment\nrelationship during convolution. In addition, to address users' diverse\ninterests, we also design an attention module in DKN to dynamically aggregate a\nuser's history with respect to current candidate news. Through extensive\nexperiments on a real online news platform, we demonstrate that DKN achieves\nsubstantial gains over state-of-the-art deep recommendation models. We also\nvalidate the efficacy of the usage of knowledge in DKN.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 06:15:16 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 02:42:56 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Fuzheng", ""], ["Xie", "Xing", ""], ["Guo", "Minyi", ""]]}, {"id": "1801.08310", "submitter": "Matthieu Boussard", "authors": "Antonin Leroux, Matthieu Boussard, Remi D\\`es", "title": "Information gain ratio correction: Improving prediction with more\n  balanced decision tree splits", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees algorithms use a gain function to select the best split during\nthe tree's induction. This function is crucial to obtain trees with high\npredictive accuracy. Some gain functions can suffer from a bias when it\ncompares splits of different arities. Quinlan proposed a gain ratio in C4.5's\ninformation gain function to fix this bias. In this paper, we present an\nupdated version of the gain ratio that performs better as it tries to fix the\ngain ratio's bias for unbalanced trees and some splits with low predictive\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 08:45:43 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Leroux", "Antonin", ""], ["Boussard", "Matthieu", ""], ["D\u00e8s", "Remi", ""]]}, {"id": "1801.08383", "submitter": "Carl Andersson", "authors": "Carl Andersson, Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on", "title": "Data-Driven Impulse Response Regularization via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of impulse response estimation of stable linear\nsingle-input single-output systems. It is a well-studied problem where flexible\nnon-parametric models recently offered a leap in performance compared to the\nclassical finite-dimensional model structures. Inspired by this development and\nthe success of deep learning we propose a new flexible data-driven model. Our\nexperiments indicate that the new model is capable of exploiting even more of\nthe hidden patterns that are present in the input-output data as compared to\nthe non-parametric models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 12:48:41 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 08:21:04 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Andersson", "Carl", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1801.08454", "submitter": "Diego Mesa", "authors": "Diego A. Mesa, Justin Tantiongloc, Marcela Mendoza, Todd P. Coleman", "title": "A Distributed Framework for the Construction of Transport Maps", "comments": "Submitted to Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to reason about uncertainty in large, complex, and multi-modal\ndatasets has become increasingly common across modern scientific environments.\nThe ability to transform samples from one distribution $P$ to another\ndistribution $Q$ enables the solution to many problems in machine learning\n(e.g. Bayesian inference, generative modeling) and has been actively pursued\nfrom theoretical, computational, and application perspectives across the fields\nof information theory, computer science, and biology. Performing such\ntransformations, in general, still leads to computational difficulties,\nespecially in high dimensions. Here, we consider the problem of computing such\n\"measure transport maps\" with efficient and parallelizable methods. Under the\nmild assumptions that $P$ need not be known but can be sampled from, and that\nthe density of $Q$ is known up to a proportionality constant, and that $Q$ is\nlog-concave, we provide in this work a convex optimization problem pertaining\nto relative entropy minimization. We show how an empirical minimization\nformulation and polynomial chaos map parameterization can allow for learning a\ntransport map between $P$ and $Q$ with distributed and scalable methods. We\nalso leverage findings from nonequilibrium thermodynamics to represent the\ntransport map as a composition of simpler maps, each of which is learned\nsequentially with a transport cost regularized version of the aforementioned\nproblem formulation. We provide examples of our framework within the context of\nBayesian inference for the Boston housing dataset and generative modeling for\nhandwritten digit images from the MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 15:37:01 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:37:40 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 22:03:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Mesa", "Diego A.", ""], ["Tantiongloc", "Justin", ""], ["Mendoza", "Marcela", ""], ["Coleman", "Todd P.", ""]]}, {"id": "1801.08570", "submitter": "Alexandr A. Kalinin", "authors": "Alexandr A. Kalinin, Gerald A. Higgins, Narathip Reamaroon, S.M. Reza\n  Soroushmehr, Ari Allyn-Feuer, Ivo D. Dinov, Kayvan Najarian, Brian D. Athey", "title": "Deep Learning in Pharmacogenomics: From Gene Regulation to Patient\n  Stratification", "comments": "Alexandr A. Kalinin and Gerald A. Higgins contributed equally to this\n  work. Corresponding author: Brian D. Athey, <bleu@umich.edu>", "journal-ref": null, "doi": "10.2217/pgs-2018-0008", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Perspective provides examples of current and future applications of deep\nlearning in pharmacogenomics, including: (1) identification of novel regulatory\nvariants located in noncoding domains and their function as applied to\npharmacoepigenomics; (2) patient stratification from medical records; and (3)\nprediction of drugs, targets, and their interactions. Deep learning\nencapsulates a family of machine learning algorithms that over the last decade\nhas transformed many important subfields of artificial intelligence (AI) and\nhas demonstrated breakthrough performance improvements on a wide range of tasks\nin biomedicine. We anticipate that in the future deep learning will be widely\nused to predict personalized drug response and optimize medication selection\nand dosing, using knowledge extracted from large and complex molecular,\nepidemiological, clinical, and demographic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:21:15 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 00:25:37 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kalinin", "Alexandr A.", ""], ["Higgins", "Gerald A.", ""], ["Reamaroon", "Narathip", ""], ["Soroushmehr", "S. M. Reza", ""], ["Allyn-Feuer", "Ari", ""], ["Dinov", "Ivo D.", ""], ["Najarian", "Kayvan", ""], ["Athey", "Brian D.", ""]]}, {"id": "1801.08577", "submitter": "Jayanta Dutta", "authors": "Jayanta K Dutta, Jiayi Liu, Unmesh Kurup and Mohak Shah", "title": "Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:40:44 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Dutta", "Jayanta K", ""], ["Liu", "Jiayi", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "1801.08600", "submitter": "Zois Boukouvalas", "authors": "Zois Boukouvalas", "title": "Development of ICA and IVA Algorithms with Application to Medical Image\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a widely used BSS method that can\nuniquely achieve source recovery, subject to only scaling and permutation\nambiguities, through the assumption of statistical independence on the part of\nthe latent sources. Independent vector analysis (IVA) extends the applicability\nof ICA by jointly decomposing multiple datasets through the exploitation of the\ndependencies across datasets. Though both ICA and IVA algorithms cast in the\nmaximum likelihood (ML) framework enable the use of all available statistical\ninformation in reality, they often deviate from their theoretical optimality\nproperties due to improper estimation of the probability density function\n(PDF). This motivates the development of flexible ICA and IVA algorithms that\nclosely adhere to the underlying statistical description of the data. Although\nit is attractive minimize the assumptions, important prior information about\nthe data, such as sparsity, is usually available. If incorporated into the ICA\nmodel, use of this additional information can relax the independence\nassumption, resulting in an improvement in the overall separation performance.\nTherefore, the development of a unified mathematical framework that can take\ninto account both statistical independence and sparsity is of great interest.\nIn this work, we first introduce a flexible ICA algorithm that uses an\neffective PDF estimator to accurately capture the underlying statistical\nproperties of the data. We then discuss several techniques to accurately\nestimate the parameters of the multivariate generalized Gaussian distribution,\nand how to integrate them into the IVA model. Finally, we provide a\nmathematical framework that enables direct control over the influence of\nstatistical independence and sparsity, and use this framework to develop an\neffective ICA algorithm that can jointly exploit these two forms of diversity.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 21:34:47 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Boukouvalas", "Zois", ""]]}, {"id": "1801.08639", "submitter": "Thang Huynh", "authors": "Thang Huynh, Rayan Saab", "title": "Fast binary embeddings, and quantized compressed sensing with structured\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with two related problems, namely distance-preserving binary\nembeddings and quantization for compressed sensing . First, we propose fast\nmethods to replace points from a subset $\\mathcal{X} \\subset \\mathbb{R}^n$,\nassociated with the Euclidean metric, with points in the cube $\\{\\pm 1\\}^m$ and\nwe associate the cube with a pseudo-metric that approximates Euclidean distance\namong points in $\\mathcal{X}$. Our methods rely on quantizing fast\nJohnson-Lindenstrauss embeddings based on bounded orthonormal systems and\npartial circulant ensembles, both of which admit fast transforms. Our\nquantization methods utilize noise-shaping, and include Sigma-Delta schemes and\ndistributed noise-shaping schemes. The resulting approximation errors decay\npolynomially and exponentially fast in $m$, depending on the embedding method.\nThis dramatically outperforms the current decay rates associated with binary\nembeddings and Hamming distances. Additionally, it is the first such binary\nembedding result that applies to fast Johnson-Lindenstrauss maps while\npreserving $\\ell_2$ norms.\n  Second, we again consider noise-shaping schemes, albeit this time to quantize\ncompressed sensing measurements arising from bounded orthonormal ensembles and\npartial circulant matrices. We show that these methods yield a reconstruction\nerror that again decays with the number of measurements (and bits), when using\nconvex optimization for reconstruction. Specifically, for Sigma-Delta schemes,\nthe error decays polynomially in the number of measurements, and it decays\nexponentially for distributed noise-shaping schemes based on beta encoding.\nThese results are near optimal and the first of their kind dealing with bounded\northonormal systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 00:16:53 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 17:25:07 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Huynh", "Thang", ""], ["Saab", "Rayan", ""]]}, {"id": "1801.08640", "submitter": "Sarah Tan", "authors": "Sarah Tan, Rich Caruana, Giles Hooker, Paul Koch, Albert Gordo", "title": "Learning Global Additive Explanations for Neural Nets Using Model\n  Distillation", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/214", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability has largely focused on local explanations, i.e. explaining\nwhy a model made a particular prediction for a sample. These explanations are\nappealing due to their simplicity and local fidelity. However, they do not\nprovide information about the general behavior of the model. We propose to\nleverage model distillation to learn global additive explanations that describe\nthe relationship between input features and model predictions. These global\nexplanations take the form of feature shapes, which are more expressive than\nfeature attributions. Through careful experimentation, we show qualitatively\nand quantitatively that global additive explanations are able to describe model\nbehavior and yield insights about models such as neural nets. A visualization\nof our approach applied to a neural net as it is trained is available at\nhttps://youtu.be/ErQYwNqzEdc.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 00:23:20 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 02:45:29 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Tan", "Sarah", ""], ["Caruana", "Rich", ""], ["Hooker", "Giles", ""], ["Koch", "Paul", ""], ["Gordo", "Albert", ""]]}, {"id": "1801.08660", "submitter": "Angli Liu", "authors": "Angli Liu, Katrin Kirchhoff", "title": "Context Models for OOV Word Translation in Low-Resource Languages", "comments": "to be published at AMTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-vocabulary word translation is a major problem for the translation of\nlow-resource languages that suffer from a lack of parallel training data. This\npaper evaluates the contributions of target-language context models towards the\ntranslation of OOV words, specifically in those cases where OOV translations\nare derived from external knowledge sources, such as dictionaries. We develop\nboth neural and non-neural context models and evaluate them within both\nphrase-based and self-attention based neural machine translation systems. Our\nresults show that neural language models that integrate additional context\nbeyond the current sentence are the most effective in disambiguating possible\nOOV word translations. We present an efficient second-pass lattice-rescoring\nmethod for wide-context neural language models and demonstrate performance\nimprovements over state-of-the-art self-attention based neural MT systems in\nfive out of six low-resource language pairs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 02:50:03 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Liu", "Angli", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "1801.08694", "submitter": "Kalyan Ram Ayyalasomayajula", "authors": "Kalyan Ram Ayyalasomayajula, Filip Malmberg, Anders Brun", "title": "PDNet: Semantic Segmentation integrated with a Primal-Dual Network for\n  Document binarization", "comments": "Under consideration for Pattern Recognition Letters Special Issue on\n  Graphonomics for e-citizens: e-health, e-society, e-education 11 pages, 10\n  figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.patrec.2018.05.011", "report-no": "PRLETTERS-D-17-01108", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization of digital documents is the task of classifying each pixel in an\nimage of the document as belonging to the background (parchment/paper) or\nforeground (text/ink). Historical documents are often subjected to\ndegradations, that make the task challenging. In the current work a deep neural\nnetwork architecture is proposed that combines a fully convolutional network\nwith an unrolled primal-dual network that can be trained end-to-end to achieve\nstate of the art binarization on four out of seven datasets. Document\nbinarization is formulated as an energy minimization problem. A fully\nconvolutional neural network is trained for semantic segmentation of pixels\nthat provides labeling cost associated with each pixel. This cost estimate is\nrefined along the edges to compensate for any over or under estimation of the\nforeground class using a primal-dual approach. We provide necessary overview on\nproximal operator that facilitates theoretical underpinning required to train a\nprimal-dual network using a gradient descent algorithm. Numerical instabilities\nencountered due to the recurrent nature of primal-dual approach are handled. We\nprovide experimental results on document binarization competition dataset along\nwith network changes and hyperparameter tuning required for stability and\nperformance of the network. The network when pre-trained on synthetic dataset\nperforms better as per the competition metrics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 07:07:07 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 07:08:03 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 10:33:54 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ayyalasomayajula", "Kalyan Ram", ""], ["Malmberg", "Filip", ""], ["Brun", "Anders", ""]]}, {"id": "1801.08702", "submitter": "Masahiro Suzuki", "authors": "Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo", "title": "Improving Bi-directional Generation between Different Modalities with\n  Variational Autoencoders", "comments": "Updated version of arXiv:1611.01891", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. A major approach to achieve this objective is to train a model that\nintegrates all the information of different modalities into a joint\nrepresentation and then to generate one modality from the corresponding other\nmodality via this joint representation. We simply applied this approach to\nvariational autoencoders (VAEs), which we call a joint multimodal variational\nautoencoder (JMVAE). However, we found that when this model attempts to\ngenerate a large dimensional modality missing at the input, the joint\nrepresentation collapses and this modality cannot be generated successfully.\nFurthermore, we confirmed that this difficulty cannot be resolved even using a\nknown solution. Therefore, in this study, we propose two models to prevent this\ndifficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that\nthese methods can prevent the difficulty above and that they generate\nmodalities bi-directionally with equal or higher likelihood than conventional\nVAE methods, which generate in only one direction. Moreover, we confirm that\nthese methods can obtain the joint representation appropriately, so that they\ncan generate various variations of modality by moving over the joint\nrepresentation or changing the value of another modality.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:06:34 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Suzuki", "Masahiro", ""], ["Nakayama", "Kotaro", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1801.08712", "submitter": "Atanas Mirchev", "authors": "Atanas Mirchev, Seyed-Ahmad Ahmadi", "title": "Classification of sparsely labeled spatio-temporal data through\n  semi-supervised adversarial learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Generative Adversarial Networks (GAN) have emerged as a\npowerful method for learning the mapping from noisy latent spaces to realistic\ndata samples in high-dimensional space. So far, the development and application\nof GANs have been predominantly focused on spatial data such as images. In this\nproject, we aim at modeling of spatio-temporal sensor data instead, i.e.\ndynamic data over time. The main goal is to encode temporal data into a global\nand low-dimensional latent vector that captures the dynamics of the\nspatio-temporal signal. To this end, we incorporate auto-regressive RNNs,\nWasserstein GAN loss, spectral norm weight constraints and a semi-supervised\nlearning scheme into InfoGAN, a method for retrieval of meaningful latents in\nadversarial learning. To demonstrate the modeling capability of our method, we\nencode full-body skeletal human motion from a large dataset representing 60\nclasses of daily activities, recorded in a multi-Kinect setup. Initial results\nindicate competitive classification performance of the learned latent\nrepresentations, compared to direct CNN/RNN inference. In future work, we plan\nto apply this method on a related problem in the medical domain, i.e. on\nrecovery of meaningful latents in gait analysis of patients with vertigo and\nbalance disorders.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:32:34 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 10:13:35 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Mirchev", "Atanas", ""], ["Ahmadi", "Seyed-Ahmad", ""]]}, {"id": "1801.08788", "submitter": "Marko Nagode", "authors": "Marko Nagode", "title": "Multivariate normal mixture modeling, clustering and classification with\n  the rebmix package", "comments": "15 pages, 6 figures, R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rebmix package provides R functions for random univariate and\nmultivariate finite mixture model generation, estimation, clustering and\nclassification. The paper is focused on multivariate normal mixture models with\nunrestricted variance-covariance matrices. The objective is to show how to\ngenerate datasets for a known number of components, numbers of observations and\ncomponent parameters, how to estimate the number of components, component\nweights and component parameters and how to predict cluster and class\nmembership based upon a model trained by the REBMIX algorithm. The accompanying\nplotting, bootstrapping and other features of the package are dealt with, too.\nFor demonstration purpose a multivariate normal dataset with unrestricted\nvariance-covariance matrices is studied.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 12:52:39 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Nagode", "Marko", ""]]}, {"id": "1801.08881", "submitter": "Lucas Parra", "authors": "Lucas C. Parra, Stefan Haufe, Jacek P. Dmochowski", "title": "Correlated Components Analysis - Extracting Reliable Dimensions in\n  Multivariate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How does one find dimensions in multivariate data that are reliably expressed\nacross repetitions? For example, in a brain imaging study one may want to\nidentify combinations of neural signals that are reliably expressed across\nmultiple trials or subjects. For a behavioral assessment with multiple ratings,\none may want to identify an aggregate score that is reliably reproduced across\nraters. Correlated Components Analysis (CorrCA) addresses this problem by\nidentifying components that are maximally correlated between repetitions (e.g.\ntrials, subjects, raters). Here we formalize this as the maximization of the\nratio of between-repetition to within-repetition covariance. We show that this\ncriterion maximizes repeat-reliability, defined as mean over variance across\nrepeats, and that it leads to CorrCA or to multi-set Canonical Correlation\nAnalysis, depending on the constraints. Surprisingly, we also find that CorrCA\nis equivalent to Linear Discriminant Analysis for zero-mean signals, which\nprovides an unexpected link between classic concepts of multivariate analysis.\nWe present an exact parametric test of statistical significance based on the\nF-statistic for normally distributed independent samples, and present and\nvalidate shuffle statistics for the case of dependent samples. Regularization\nand extension to non-linear mappings using kernels are also presented. The\nalgorithms are demonstrated on a series of data analysis applications, and we\nprovide all code and data required to reproduce the results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 16:12:07 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 19:19:10 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 16:05:30 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 22:02:19 GMT"}, {"version": "v5", "created": "Sun, 20 Jan 2019 21:15:39 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Parra", "Lucas C.", ""], ["Haufe", "Stefan", ""], ["Dmochowski", "Jacek P.", ""]]}, {"id": "1801.09049", "submitter": "Ran Zhao", "authors": "Qidi Peng, Nan Rao, Ran Zhao", "title": "Covariance-based Dissimilarity Measures Applied to Clustering Wide-sense\n  Stationary Ergodic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new unsupervised learning problem: clustering wide-sense\nstationary ergodic stochastic processes. A covariance-based dissimilarity\nmeasure together with asymptotically consistent algorithms is designed for\nclustering offline and online datasets, respectively. We also suggest a formal\ncriterion on the efficiency of dissimilarity measures, and discuss of some\napproach to improve the efficiency of our clustering algorithms, when they are\napplied to cluster particular type of processes, such as self-similar processes\nwith wide-sense stationary ergodic increments. Clustering synthetic data and\nreal-world data are provided as examples of applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 07:37:41 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 17:44:02 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 23:09:13 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 22:46:24 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Peng", "Qidi", ""], ["Rao", "Nan", ""], ["Zhao", "Ran", ""]]}, {"id": "1801.09055", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Solving for multi-class using orthogonal coding matrices", "comments": null, "journal-ref": "SN Applied Sciences 2019, 1 (11): 1451", "doi": "10.1007/s42452-019-1437-9", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common method of generalizing binary to multi-class classification is the\nerror correcting code (ECC). ECCs may be optimized in a number of ways, for\ninstance by making them orthogonal. Here we test two types of orthogonal ECCs\non seven different datasets using three types of binary classifier and compare\nthem with three other multi-class methods: 1 vs. 1, one-versus-the-rest and\nrandom ECCs. The first type of orthogonal ECC, in which the codes contain no\nzeros, admits a fast and simple method of solving for the probabilities.\nOrthogonal ECCs are always more accurate than random ECCs as predicted by\nrecent literature. Improvments in uncertainty coefficient (U.C.) range between\n0.4--17.5% (0.004--0.139, absolute), while improvements in Brier score between\n0.7--10.7%. Unfortunately, orthogonal ECCs are rarely more accurate than 1 vs.\n1. Disparities are worst when the methods are paired with logistic regression,\nwith orthogonal ECCs never beating 1 vs. 1. When the methods are paired with\nSVM, the losses are less significant, peaking at 1.5%, relative, 0.011 absolute\nin uncertainty coefficient and 6.5% in Brier scores. Orthogonal ECCs are always\nthe fastest of the five multi-class methods when paired with linear\nclassifiers. When paired with a piecewise linear classifier, whose\nclassification speed does not depend on the number of training samples,\nclassifications using orthogonal ECCs were always more accurate than the the\nremaining three methods and also faster than 1 vs. 1. Losses against 1 vs. 1\nhere were higher, peaking at 1.9% (0.017, absolute), in U.C. and 39% in Brier\nscore. Gains in speed ranged between 1.1% and over 100%. Whether the speed\nincrease is worth the penalty in accuracy will depend on the application.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 08:45:07 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 04:10:47 GMT"}, {"version": "v3", "created": "Sat, 15 Sep 2018 14:46:39 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 04:58:07 GMT"}, {"version": "v5", "created": "Mon, 28 Oct 2019 03:51:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1801.09065", "submitter": "Luca Martino", "authors": "Luca Martino", "title": "A Review of Multiple Try MCMC algorithms for Signal Processing", "comments": "Digital Signal Processing, 2018", "journal-ref": null, "doi": "10.1016/j.dsp.2018.01.004", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in signal processing require the estimation of some\nparameters of interest given a set of observed data. More specifically,\nBayesian inference needs the computation of {\\it a-posteriori} estimators which\nare often expressed as complicated multi-dimensional integrals. Unfortunately,\nanalytical expressions for these estimators cannot be found in most real-world\napplications, and Monte Carlo methods are the only feasible approach. A very\npowerful class of Monte Carlo techniques is formed by the Markov Chain Monte\nCarlo (MCMC) algorithms. They generate a Markov chain such that its stationary\ndistribution coincides with the target posterior density. In this work, we\nperform a thorough review of MCMC methods using multiple candidates in order to\nselect the next state of the chain, at each iteration. With respect to the\nclassical Metropolis-Hastings method, the use of multiple try techniques foster\nthe exploration of the sample space. We present different Multiple Try\nMetropolis schemes, Ensemble MCMC methods, Particle Metropolis-Hastings\nalgorithms and the Delayed Rejection Metropolis technique. We highlight\nlimitations, benefits, connections and differences among the different methods,\nand compare them by numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 10:27:19 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Martino", "Luca", ""]]}, {"id": "1801.09070", "submitter": "Andres C Rodriguez", "authors": "Andres C. Rodriguez, Tomasz Kacprzak, Aurelien Lucchi, Adam Amara,\n  Raphael Sgier, Janis Fluri, Thomas Hofmann and Alexandre R\\'efr\\'egier", "title": "Fast cosmic web simulations with generative adversarial networks", "comments": null, "journal-ref": "Rodriguez, A.C., Kacprzak, T., Lucchi, A. et al. Computational\n  Astrophysics and Cosmology (2018) 5: 4", "doi": "10.1186/s40668-018-0026-4", "report-no": null, "categories": "astro-ph.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark matter in the universe evolves through gravity to form a complex network\nof halos, filaments, sheets and voids, that is known as the cosmic web.\nComputational models of the underlying physical processes, such as classical\nN-body simulations, are extremely resource intensive, as they track the action\nof gravity in an expanding universe using billions of particles as tracers of\nthe cosmic matter distribution. Therefore, upcoming cosmology experiments will\nface a computational bottleneck that may limit the exploitation of their full\nscientific potential. To address this challenge, we demonstrate the application\nof a machine learning technique called Generative Adversarial Networks (GAN) to\nlearn models that can efficiently generate new, physically realistic\nrealizations of the cosmic web. Our training set is a small, representative\nsample of 2D image snapshots from N-body simulations of size 500 and 100 Mpc.\nWe show that the GAN-generated samples are qualitatively and quantitatively\nvery similar to the originals. For the larger boxes of size 500 Mpc, it is very\ndifficult to distinguish them visually. The agreement of the power spectrum\n$P_k$ is 1-2\\% for most of the range, between $k=0.06$ and $k=0.4$. An\nimportant advantage of generating cosmic web realizations with a GAN is the\nconsiderable gains in terms of computation time. Each new sample generated by a\nGAN takes a fraction of a second, compared to the many hours needed by\ntraditional N-body techniques. We anticipate that the use of generative models\nsuch as GANs will therefore play an important role in providing extremely fast\nand precise simulations of cosmic web in the era of large cosmological surveys,\nsuch as Euclid and Large Synoptic Survey Telescope (LSST).\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 10:52:40 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 15:18:37 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 07:17:42 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 10:12:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Rodriguez", "Andres C.", ""], ["Kacprzak", "Tomasz", ""], ["Lucchi", "Aurelien", ""], ["Amara", "Adam", ""], ["Sgier", "Raphael", ""], ["Fluri", "Janis", ""], ["Hofmann", "Thomas", ""], ["R\u00e9fr\u00e9gier", "Alexandre", ""]]}, {"id": "1801.09125", "submitter": "Morteza Noshad Iranzad", "authors": "Morteza Noshad, Yu Zeng, Alfred O. Hero III", "title": "Scalable Mutual Information Estimation using Dependence Graphs", "comments": "19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mutual Information (MI) is an often used measure of dependency between\ntwo random variables utilized in information theory, statistics and machine\nlearning. Recently several MI estimators have been proposed that can achieve\nparametric MSE convergence rate. However, most of the previously proposed\nestimators have the high computational complexity of at least $O(N^2)$. We\npropose a unified method for empirical non-parametric estimation of general MI\nfunction between random vectors in $\\mathbb{R}^d$ based on $N$ i.i.d. samples.\nThe reduced complexity MI estimator, called the ensemble dependency graph\nestimator (EDGE), combines randomized locality sensitive hashing (LSH),\ndependency graphs, and ensemble bias-reduction methods. We prove that EDGE\nachieves optimal computational complexity $O(N)$, and can achieve the optimal\nparametric MSE rate of $O(1/N)$ if the density is $d$ times differentiable. To\nthe best of our knowledge EDGE is the first non-parametric MI estimator that\ncan achieve parametric MSE rates with linear time complexity. We illustrate the\nutility of EDGE for the analysis of the information plane (IP) in deep\nlearning. Using EDGE we shed light on a controversy on whether or not the\ncompression property of information bottleneck (IB) in fact holds for ReLu and\nother rectification functions in deep neural networks (DNN).\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 19:04:33 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 16:45:50 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Noshad", "Morteza", ""], ["Zeng", "Yu", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1801.09136", "submitter": "Mathieu Ravaut", "authors": "Mathieu Ravaut, Satya Gorti", "title": "Gradient descent revisited via an adaptive online learning rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any gradient descent optimization requires to choose a learning rate. With\ndeeper and deeper models, tuning that learning rate can easily become tedious\nand does not necessarily lead to an ideal convergence. We propose a variation\nof the gradient descent algorithm in the which the learning rate is not fixed.\nInstead, we learn the learning rate itself, either by another gradient descent\n(first-order method), or by Newton's method (second-order). This way, gradient\ndescent for any machine learning algorithm can be optimized.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 20:39:24 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 15:17:02 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ravaut", "Mathieu", ""], ["Gorti", "Satya", ""]]}, {"id": "1801.09144", "submitter": "Vadim Smolyakov", "authors": "Vadim Smolyakov and Qiang Liu and John W. Fisher III", "title": "Adaptive Scan Gibbs Sampler for Large Scale Inference Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large scale on-line inference problems the update strategy is critical\nfor performance. We derive an adaptive scan Gibbs sampler that optimizes the\nupdate frequency by selecting an optimum mini-batch size. We demonstrate\nperformance of our adaptive batch-size Gibbs sampler by comparing it against\nthe collapsed Gibbs sampler for Bayesian Lasso, Dirichlet Process Mixture\nModels (DPMM) and Latent Dirichlet Allocation (LDA) graphical models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 22:19:56 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Smolyakov", "Vadim", ""], ["Liu", "Qiang", ""], ["Fisher", "John W.", "III"]]}, {"id": "1801.09150", "submitter": "Vadim Smolyakov", "authors": "Vadim Smolyakov and Julian Straub and Sue Zheng and John W. Fisher III", "title": "Bayesian Nonparametric Modeling of Driver Behavior using HDP Split-Merge\n  Sampling Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vehicles are equipped with increasingly complex sensors. These sensors\ngenerate large volumes of data that provide opportunities for modeling and\nanalysis. Here, we are interested in exploiting this data to learn aspects of\nbehaviors and the road network associated with individual drivers. Our dataset\nis collected on a standard vehicle used to commute to work and for personal\ntrips. A Hidden Markov Model (HMM) trained on the GPS position and orientation\ndata is utilized to compress the large amount of position information into a\nsmall amount of road segment states. Each state has a set of observations, i.e.\ncar signals, associated with it that are quantized and modeled as draws from a\nHierarchical Dirichlet Process (HDP). The inference for the topic distributions\nis carried out using HDP split-merge sampling algorithm. The topic\ndistributions over joint quantized car signals characterize the driving\nsituation in the respective road state. In a novel manner, we demonstrate how\nthe sparsity of the personal road network of a driver in conjunction with a\nhierarchical topic model allows data driven predictions about destinations as\nwell as likely road conditions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 23:25:08 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Smolyakov", "Vadim", ""], ["Straub", "Julian", ""], ["Zheng", "Sue", ""], ["Fisher", "John W.", "III"]]}, {"id": "1801.09185", "submitter": "Stephen L. France", "authors": "Stephen L. France and Sanjoy Ghose", "title": "Marketing Analytics: Methods, Practice, Implementation, and Links to\n  Other Fields", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2018.11.002", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marketing analytics is a diverse field, with both academic researchers and\npractitioners coming from a range of backgrounds including marketing, expert\nsystems, statistics, and operations research. This paper provides an\nintegrative review at the boundary of these areas. The aim is to give\nresearchers in the intelligent and expert systems community the opportunity to\ngain a broad view of the marketing analytics area and provide a starting point\nfor future interdisciplinary collaboration. The topics of visualization,\nsegmentation, and class prediction are featured. Links between the disciplines\nare emphasized. For each of these topics, a historical overview is given,\nstarting with initial work in the 1960s and carrying through to the present\nday. Recent innovations for modern, large, and complex \"big data\" sets are\ndescribed. Practical implementation advice is given, along with a directory of\nopen source R routines for implementing marketing analytics techniques.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 05:53:44 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 16:49:26 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["France", "Stephen L.", ""], ["Ghose", "Sanjoy", ""]]}, {"id": "1801.09197", "submitter": "Markus Lange-Hegermann", "authors": "Markus Lange-Hegermann", "title": "Algorithmic Linearly Constrained Gaussian Processes", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SC math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We algorithmically construct multi-output Gaussian process priors which\nsatisfy linear differential equations. Our approach attempts to parametrize all\nsolutions of the equations using Gr\\\"obner bases. If successful, a push forward\nGaussian process along the paramerization is the desired prior. We consider\nseveral examples from physics, geomathematics and control, among them the full\ninhomogeneous system of Maxwell's equations. By bringing together stochastic\nlearning and computer algebra in a novel way, we combine noisy observations\nwith precise algebraic computations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 09:07:05 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 13:51:01 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 17:33:23 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Lange-Hegermann", "Markus", ""]]}, {"id": "1801.09271", "submitter": "Ying Liu", "authors": "Ning Liu and Ying Liu and Brent Logan and Zhiyuan Xu and Jian Tang and\n  Yanzhi Wang", "title": "Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical\n  Registry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first deep reinforcement learning (DRL) framework to\nestimate the optimal Dynamic Treatment Regimes from observational medical data.\nThis framework is more flexible and adaptive for high dimensional action and\nstate spaces than existing reinforcement learning methods to model real-life\ncomplexity in heterogeneous disease progression and treatment choices, with the\ngoal of providing doctor and patients the data-driven personalized decision\nrecommendations. The proposed DRL framework comprises (i) a supervised learning\nstep to predict the most possible expert actions, and (ii) a deep reinforcement\nlearning step to estimate the long-term value function of Dynamic Treatment\nRegimes. Both steps depend on deep neural networks.\n  As a key motivational example, we have implemented the proposed framework on\na data set from the Center for International Bone Marrow Transplant Research\n(CIBMTR) registry database, focusing on the sequence of prevention and\ntreatments for acute and chronic graft versus host disease after\ntransplantation. In the experimental results, we have demonstrated promising\naccuracy in predicting human experts' decisions, as well as the high expected\nreward function in the DRL-based dynamic treatment regimes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 19:29:50 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Liu", "Ning", ""], ["Liu", "Ying", ""], ["Logan", "Brent", ""], ["Xu", "Zhiyuan", ""], ["Tang", "Jian", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1801.09303", "submitter": "Nesreen Ahmed", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao,\n  Yasin Abbasi Yadkori", "title": "HONE: Higher-Order Network Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a general framework for learning Higher-Order Network\nEmbeddings (HONE) from graph data based on network motifs. The HONE framework\nis highly expressive and flexible with many interchangeable components. The\nexperimental results demonstrate the effectiveness of learning higher-order\nnetwork representations. In all cases, HONE outperforms recent embedding\nmethods that are unable to capture higher-order structures with a mean relative\ngain in AUC of $19\\%$ (and up to $75\\%$ gain) across a wide variety of networks\nand embedding methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 21:59:49 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 01:34:46 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""], ["Rao", "Anup", ""], ["Yadkori", "Yasin Abbasi", ""]]}, {"id": "1801.09319", "submitter": "Olexandr Isayev", "authors": "Justin S. Smith, Ben Nebgen, Nicholas Lubbers, Olexandr Isayev, Adrian\n  E. Roitberg", "title": "Less is more: sampling chemical space with active learning", "comments": "Accepted at J. Chem. Phys", "journal-ref": "J. Chem. Phys. 148, 241733 (2018)", "doi": "10.1063/1.5023802", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of accurate and transferable machine learning (ML) potentials\nfor predicting molecular energetics is a challenging task. The process of data\ngeneration to train such ML potentials is a task neither well understood nor\nresearched in detail. In this work, we present a fully automated approach for\nthe generation of datasets with the intent of training universal ML potentials.\nIt is based on the concept of active learning (AL) via Query by Committee\n(QBC), which uses the disagreement between an ensemble of ML potentials to\ninfer the reliability of the ensemble's prediction. QBC allows the presented AL\nalgorithm to automatically sample regions of chemical space where the ML\npotential fails to accurately predict the potential energy. AL improves the\noverall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test\ncases by mitigating human biases in deciding what new training data to use. AL\nalso reduces the training set size to a fraction of the data required when\nusing naive random sampling techniques. To provide validation of our AL\napproach we develop the COMP6 benchmark (publicly available on GitHub), which\ncontains a diverse set of organic molecules. Through the AL process, it is\nshown that the AL-based potentials perform as well as the ANI-1 potential on\nCOMP6 with only 10% of the data, and vastly outperforms ANI-1 with 25% the\namount of data. Finally, we show that our proposed AL technique develops a\nuniversal ANI potential (ANI-1x) that provides accurate energy and force\npredictions on the entire COMP6 benchmark. This universal ML potential achieves\na level of accuracy on par with the best ML potentials for single molecule or\nmaterials, while remaining applicable to the general class of organic molecules\ncomprised of the elements CHNO.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 23:48:01 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 15:51:41 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Smith", "Justin S.", ""], ["Nebgen", "Ben", ""], ["Lubbers", "Nicholas", ""], ["Isayev", "Olexandr", ""], ["Roitberg", "Adrian E.", ""]]}, {"id": "1801.09326", "submitter": "Guang Cheng", "authors": "Botao Hao, Anru Zhang, Guang Cheng", "title": "Sparse and Low-rank Tensor Estimation via Cubic Sketchings", "comments": "Accepted at IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework for sparse and low-rank tensor\nestimation from cubic sketchings. A two-stage non-convex implementation is\ndeveloped based on sparse tensor decomposition and thresholded gradient\ndescent, which ensures exact recovery in the noiseless case and stable recovery\nin the noisy case with high probability. The non-asymptotic analysis sheds\nlight on an interplay between optimization error and statistical error. The\nproposed procedure is shown to be rate-optimal under certain conditions. As a\ntechnical by-product, novel high-order concentration inequalities are derived\nfor studying high-moment sub-Gaussian tensors. An interesting tensor\nformulation illustrates the potential application to high-order interaction\npursuit in high-dimensional linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 00:26:39 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 03:09:42 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 04:05:03 GMT"}, {"version": "v4", "created": "Sun, 15 Mar 2020 01:07:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hao", "Botao", ""], ["Zhang", "Anru", ""], ["Cheng", "Guang", ""]]}, {"id": "1801.09367", "submitter": "Hiroshi Kera", "authors": "Hiroshi Kera and Yoshihiko Hasegawa", "title": "Approximate Vanishing Ideal via Data Knotting", "comments": "11 pages; AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanishing ideal is a set of polynomials that takes zero value on the\ngiven data points. Originally proposed in computer algebra, the vanishing ideal\nhas been recently exploited for extracting the nonlinear structures of data in\nmany applications. To avoid overfitting to noisy data, the polynomials are\noften designed to approximately rather than exactly equal zero on the\ndesignated data. Although such approximations empirically demonstrate high\nperformance, the sound algebraic structure of the vanishing ideal is lost. The\npresent paper proposes a vanishing ideal that is tolerant to noisy data and\nalso pursued to have a better algebraic structure. As a new problem, we\nsimultaneously find a set of polynomials and data points for which the\npolynomials approximately vanish on the input data points, and almost exactly\nvanish on the discovered data points. In experimental classification tests, our\nmethod discovered much fewer and lower-degree polynomials than an existing\nstate-of-the-art method. Consequently, our method accelerated the runtime of\nthe classification tasks without degrading the classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 05:42:36 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kera", "Hiroshi", ""], ["Hasegawa", "Yoshihiko", ""]]}, {"id": "1801.09386", "submitter": "Ileana Montoya Perez", "authors": "Ileana Montoya Perez, Antti Airola, Peter J. Bostr\\\"om, Ivan Jambor\n  and Tapio Pahikkala", "title": "Tournament Leave-pair-out Cross-validation for Receiver Operating\n  Characteristic (ROC) Analysis", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Receiver operating characteristic (ROC) analysis is widely used for\nevaluating diagnostic systems. Recent studies have shown that estimating an\narea under ROC curve (AUC) with standard cross-validation methods suffers from\na large bias. The leave-pair-out (LPO) cross-validation has been shown to\ncorrect this bias. However, while LPO produces an almost unbiased estimate of\nAUC, it does not provide a ranking of the data needed for plotting and\nanalyzing the ROC curve. In this study, we propose a new method called\ntournament leave-pair-out (TLPO) cross-validation. This method extends LPO by\ncreating a tournament from pair comparisons to produce a ranking for the data.\nTLPO preserves the advantage of LPO for estimating AUC, while it also allows\nperforming ROC analysis. We have shown using both synthetic and real world data\nthat TLPO is as reliable as LPO for AUC estimation and confirmed the bias in\nleave-one-out cross-validation on low-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 07:56:55 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Perez", "Ileana Montoya", ""], ["Airola", "Antti", ""], ["Bostr\u00f6m", "Peter J.", ""], ["Jambor", "Ivan", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1801.09390", "submitter": "Yanning Shen", "authors": "Yanning Shen, Panagiotis A. Traganitis, Georgios B. Giannakis", "title": "Nonlinear Dimensionality Reduction on Graphs", "comments": "Dimensionality reduction, nonlinear modeling, signal processing over\n  graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of data deluge, many signal processing and machine learning tasks\nare faced with high-dimensional datasets, including images, videos, as well as\ntime series generated from social, commercial and brain network interactions.\nTheir efficient processing calls for dimensionality reduction techniques\ncapable of properly compressing the data while preserving task-related\ncharacteristics, going beyond pairwise data correlations. The present paper\nputs forth a nonlinear dimensionality reduction framework that accounts for\ndata lying on known graphs. The novel framework encompasses most of the\nexisting dimensionality reduction methods, but it is also capable of capturing\nand preserving possibly nonlinear correlations that are ignored by linear\nmethods. Furthermore, it can take into account information from multiple\ngraphs. The proposed algorithms were tested on synthetic as well as real\ndatasets to corroborate their effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 08:11:04 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 06:56:17 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Shen", "Yanning", ""], ["Traganitis", "Panagiotis A.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1801.09597", "submitter": "Per-Arne Andersen", "authors": "Per-Arne Andersen", "title": "Deep Reinforcement Learning using Capsules in Advanced Game Environments", "comments": "Master Thesis in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling\nalgorithms to extract useful information from noisy environments. Capsule\nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group\nand has only barely begun to be explored. The network is an architecture for\nimage classification, with superior performance for classification of the MNIST\ndataset. CapsNets have not been explored beyond image classification.\n  This thesis introduces the use of CapsNet for Q-Learning based game\nalgorithms. To successfully apply CapsNet in advanced game play, three main\ncontributions follow. First, the introduction of four new game environments as\nframeworks for RL research with increasing complexity, namely Flash RL, Deep\nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between\nrelatively simple and more complex game environments available for RL research\nand are in the thesis used to test and explore the CapsNet behavior.\n  Second, the thesis introduces a generative modeling approach to produce\nartificial training data for use in Deep Learning models including CapsNets. We\nempirically show that conditional generative modeling can successfully generate\ngame data of sufficient quality to train a Deep Q-Network well.\n  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning\nbased algorithms for game AI. A capsule is a group of neurons that determine\nthe presence of objects in the data and is in the literature shown to increase\nthe robustness of training and predictions while lowering the amount training\ndata needed. It should, therefore, be ideally suited for game plays.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:04:25 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Andersen", "Per-Arne", ""]]}, {"id": "1801.09626", "submitter": "Bhavya Kailkhura", "authors": "Aditya Vempaty, Bhavya Kailkhura, Pramod K. Varshney", "title": "Human-Machine Inference Networks For Smart Decision Making:\n  Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines\ncomplementary cognitive strengths of humans and machines in an intelligent\nmanner to tackle various inference tasks and achieves higher performance than\neither humans or machines by themselves. While inference performance\noptimization techniques for human-only or sensor-only networks are quite\nmature, HuMaINs require novel signal processing and machine learning solutions.\nIn this paper, we present an overview of the HuMaINs architecture with a focus\non three main issues that include architecture design, inference algorithms\nincluding security/privacy challenges, and application areas/use cases.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:03:34 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Vempaty", "Aditya", ""], ["Kailkhura", "Bhavya", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1801.09667", "submitter": "Johannes Kirschner", "authors": "Johannes Kirschner, Andreas Krause", "title": "Information Directed Sampling and Bandits with Heteroscedastic Noise", "comments": "Figure 1a,2a updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic bandit problem, the goal is to maximize an unknown function\nvia a sequence of noisy evaluations. Typically, the observation noise is\nassumed to be independent of the evaluation point and to satisfy a tail bound\nuniformly on the domain; a restrictive assumption for many applications. In\nthis work, we consider bandits with heteroscedastic noise, where we explicitly\nallow the noise distribution to depend on the evaluation point. We show that\nthis leads to new trade-offs for information and regret, which are not taken\ninto account by existing approaches like upper confidence bound algorithms\n(UCB) or Thompson Sampling. To address these shortcomings, we introduce a\nfrequentist regret analysis framework, that is similar to the Bayesian\nframework of Russo and Van Roy (2014), and we prove a new high-probability\nregret bound for general, possibly randomized policies, which depends on a\nquantity we refer to as regret-information ratio. From this bound, we define a\nfrequentist version of Information Directed Sampling (IDS) to minimize the\nregret-information ratio over all possible action sampling distributions. This\nfurther relies on concentration inequalities for online least squares\nregression in separable Hilbert spaces, which we generalize to the case of\nheteroscedastic noise. We then formulate several variants of IDS for linear and\nreproducing kernel Hilbert space response functions, yielding novel algorithms\nfor Bayesian optimization. We also prove frequentist regret bounds, which in\nthe homoscedastic case recover known bounds for UCB, but can be much better\nwhen the noise is heteroscedastic. Empirically, we demonstrate in a linear\nsetting with heteroscedastic noise, that some of our methods can outperform UCB\nand Thompson Sampling, while staying competitive when the noise is\nhomoscedastic.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 18:52:38 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 17:13:56 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Kirschner", "Johannes", ""], ["Krause", "Andreas", ""]]}, {"id": "1801.09797", "submitter": "{\\L}ukasz Kaiser", "authors": "{\\L}ukasz Kaiser and Samy Bengio", "title": "Discrete Autoencoders for Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 23:36:11 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Kaiser", "\u0141ukasz", ""], ["Bengio", "Samy", ""]]}, {"id": "1801.09819", "submitter": "Avinava Dubey", "authors": "Junier B. Oliva, Avinava Dubey, Manzil Zaheer, Barnab\\'as P\\'oczos,\n  Ruslan Salakhutdinov, Eric P. Xing, Jeff Schneider", "title": "Transformation Autoregressive Networks", "comments": null, "journal-ref": "ICML 2018", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental task of general density estimation $p(x)$ has been of keen\ninterest to machine learning. In this work, we attempt to systematically\ncharacterize methods for density estimation. Broadly speaking, most of the\nexisting methods can be categorized into either using: \\textit{a})\nautoregressive models to estimate the conditional factors of the chain rule,\n$p(x_{i}\\, |\\, x_{i-1}, \\ldots)$; or \\textit{b}) non-linear transformations of\nvariables of a simple base distribution. Based on the study of the\ncharacteristics of these categories, we propose multiple novel methods for each\ncategory. For example we proposed RNN based transformations to model\nnon-Markovian dependencies. Further, through a comprehensive study over both\nreal world and synthetic data, we show for that jointly leveraging\ntransformations of variables and autoregressive conditional models, results in\na considerable improvement in performance. We illustrate the use of our models\nin outlier detection and image modeling. Finally we introduce a novel data\ndriven framework for learning a family of distributions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 01:39:38 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 18:13:41 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 02:13:46 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 20:56:56 GMT"}, {"version": "v5", "created": "Tue, 23 Oct 2018 14:30:22 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Oliva", "Junier B.", ""], ["Dubey", "Avinava", ""], ["Zaheer", "Manzil", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""], ["Schneider", "Jeff", ""]]}, {"id": "1801.09827", "submitter": "Pingping Zhang", "authors": "Jie Yang and Pingping Zhang and Yan Liu", "title": "Robustness of classification ability of spiking neural networks", "comments": "Aceppted by Nonlinear Dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the robustness of artificial neural networks (ANNs) is\nimportant for their wide ranges of applications. In this paper, we focus on the\nrobustness of the classification ability of a spiking neural network which\nreceives perturbed inputs. Actually, the perturbation is allowed to be\narbitrary styles. However, Gaussian perturbation and other regular ones have\nbeen rarely investigated. For classification problems, the closer to the\ndesired point, the more perturbed points there are in the input space. In\naddition, the perturbation may be periodic. Based on these facts, we only\nconsider sinusoidal and Gaussian perturbations in this paper. With the\nSpikeProp algorithm, we perform extensive experiments on the classical XOR\nproblem and other three benchmark datasets. The numerical results show that\nthere is not significant reduction in the classification ability of the network\nif the input signals are subject to sinusoidal and Gaussian perturbations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 02:34:38 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Yang", "Jie", ""], ["Zhang", "Pingping", ""], ["Liu", "Yan", ""]]}, {"id": "1801.09829", "submitter": "Pan Zhang", "authors": "Cheng Shi, Yanchen Liu and Pan Zhang", "title": "Weighted Community Detection and Data Clustering Using Message Passing", "comments": "21 pages, 13 figures, to appear in Journal of Statistical Mechanics:\n  Theory and Experiment", "journal-ref": null, "doi": "10.1088/1742-5468/aaa8f5", "report-no": null, "categories": "physics.soc-ph cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping objects into clusters based on similarities or weights between them\nis one of the most important problems in science and engineering. In this work,\nby extending message passing algorithms and spectral algorithms proposed for\nunweighted community detection problem, we develop a non-parametric method\nbased on statistical physics, by mapping the problem to Potts model at the\ncritical temperature of spin glass transition and applying belief propagation\nto solve the marginals corresponding to the Boltzmann distribution. Our\nalgorithm is robust to over-fitting and gives a principled way to determine\nwhether there are significant clusters in the data and how many clusters there\nare. We apply our method to different clustering tasks and use extensive\nnumerical experiments to illustrate the advantage of our method over existing\nalgorithms. In the community detection problem in weighted and directed\nnetworks, we show that our algorithm significantly outperforms existing\nalgorithms. In the clustering problem when the data was generated by mixture\nmodels in the sparse regime we show that our method works to the theoretical\nlimit of detectability and gives accuracy very close to that of the optimal\nBayesian inference. In the semi-supervised clustering problem, our method only\nneeds several labels to work perfectly in classic datasets. Finally, we further\ndevelop Thouless-Anderson-Palmer equations which reduce heavily the computation\ncomplexity in dense-networks but gives almost the same performance as belief\npropagation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 02:42:14 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Shi", "Cheng", ""], ["Liu", "Yanchen", ""], ["Zhang", "Pan", ""]]}, {"id": "1801.09834", "submitter": "James Long", "authors": "Zhenfeng Lin and James P. Long", "title": "A Flexible Procedure for Mixture Proportion Estimation in\n  Positive-Unlabeled Learning", "comments": "28 pages (including 9 pages of Technical Notes), 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive--unlabeled (PU) learning considers two samples, a positive set P\nwith observations from only one class and an unlabeled set U with observations\nfrom two classes. The goal is to classify observations in U. Class mixture\nproportion estimation (MPE) in U is a key step in PU learning. Blanchard et al.\n[2010] showed that MPE in PU learning is a generalization of the problem of\nestimating the proportion of true null hypotheses in multiple testing problems.\nMotivated by this idea, we propose reducing the problem to one dimension via\nconstruction of a probabilistic classifier trained on the P and U data sets\nfollowed by application of a one--dimensional mixture proportion method from\nthe multiple testing literature to the observation class probabilities. The\nflexibility of this framework lies in the freedom to choose the classifier and\nthe one--dimensional MPE method. We prove consistency of two mixture proportion\nestimators using bounds from empirical process theory, develop tuning parameter\nfree implementations, and demonstrate that they have competitive performance on\nsimulated waveform data and a protein signaling problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 03:02:12 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 22:23:11 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 21:12:26 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 03:30:27 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lin", "Zhenfeng", ""], ["Long", "James P.", ""]]}, {"id": "1801.09848", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto,\n  Thomas R. Shultz", "title": "Over-representation of Extreme Events in Decision-Making: A Rational\n  Metacognitive Account", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Availability bias, manifested in the over-representation of extreme\neventualities in decision-making, is a well-known cognitive bias, and is\ngenerally taken as evidence of human irrationality. In this work, we present\nthe first rational, metacognitive account of the Availability bias, formally\narticulated at Marr's algorithmic level of analysis. Concretely, we present a\nnormative, metacognitive model of how a cognitive system should over-represent\nextreme eventualities, depending on the amount of time available at its\ndisposal for decision-making. Our model also accounts for two well-known\nframing effects in human decision-making under risk---the fourfold pattern of\nrisk preferences in outcome probability (Tversky & Kahneman, 1992) and in\noutcome magnitude (Markovitz, 1952)---thereby providing the first\nmetacognitively-rational basis for those effects. Empirical evidence,\nfurthermore, confirms an important prediction of our model. Surprisingly, our\nmodel is unimaginably robust with respect to its focal parameter. We discuss\nthe implications of our work for studies on human decision-making, and conclude\nby presenting a counterintuitive prediction of our model, which, if confirmed,\nwould have intriguing implications for human decision-making under risk. To our\nknowledge, our model is the first metacognitive, resource-rational process\nmodel of cognitive biases in decision-making.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:33:25 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Nobandegani", "Ardavan S.", ""], ["Castanheira", "Kevin da Silva", ""], ["Otto", "A. Ross", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1801.09851", "submitter": "Xuan Wang", "authors": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo\n  Shang, Curtis Langlotz and Jiawei Han", "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task\n  Learning", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: State-of-the-art biomedical named entity recognition (BioNER)\nsystems often require handcrafted features specific to each entity type, such\nas genes, chemicals and diseases. Although recent studies explored using neural\nnetwork models for BioNER to free experts from manual feature engineering, the\nperformance remains limited by the available training data for each entity\ntype. Results: We propose a multi-task learning framework for BioNER to\ncollectively use the training data of different types of entities and improve\nthe performance on each of them. In experiments on 15 benchmark BioNER\ndatasets, our multi-task model achieves substantially better performance\ncompared with state-of-the-art BioNER systems and baseline neural sequence\nlabeling models. Further analysis shows that the large performance gains come\nfrom sharing character- and word-level information among relevant biomedical\nentities across differently labeled corpora.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:44:14 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 04:37:50 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 19:32:10 GMT"}, {"version": "v4", "created": "Mon, 8 Oct 2018 01:51:11 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Wang", "Xuan", ""], ["Zhang", "Yu", ""], ["Ren", "Xiang", ""], ["Zhang", "Yuhao", ""], ["Zitnik", "Marinka", ""], ["Shang", "Jingbo", ""], ["Langlotz", "Curtis", ""], ["Han", "Jiawei", ""]]}, {"id": "1801.09856", "submitter": "Hu Wang", "authors": "Hu Wang", "title": "ReNN: Rule-embedded Neural Networks", "comments": "poster paper in ICPR, 6 pages, 4 figures, and 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artificial neural network shows powerful ability of inference, but it is\nstill criticized for lack of interpretability and prerequisite needs of big\ndataset. This paper proposes the Rule-embedded Neural Network (ReNN) to\novercome the shortages. ReNN first makes local-based inferences to detect local\npatterns, and then uses rules based on domain knowledge about the local\npatterns to generate rule-modulated map. After that, ReNN makes global-based\ninferences that synthesizes the local patterns and the rule-modulated map. To\nsolve the optimization problem caused by rules, we use a two-stage optimization\nstrategy to train the ReNN model. By introducing rules into ReNN, we can\nstrengthen traditional neural networks with long-term dependencies which are\ndifficult to learn with limited empirical dataset, thus improving inference\naccuracy. The complexity of neural networks can be reduced since long-term\ndependencies are not modeled with neural connections, and thus the amount of\ndata needed to optimize the neural networks can be reduced. Besides, inferences\nfrom ReNN can be analyzed with both local patterns and rules, and thus have\nbetter interpretability. In this paper, ReNN has been validated with a\ntime-series detection problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 05:47:01 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 06:57:05 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Wang", "Hu", ""]]}, {"id": "1801.09870", "submitter": "Benjamin Donnot", "authors": "Benjamin Donnot (1, 2), Isabelle Guyon (1), Marc Schoenauer (1),\n  Antoine Marot, Patrick Panciatici ((1) TAU, (2) LRI)", "title": "Fast Power system security analysis with Guided Dropout", "comments": "European Symposium on Artificial Neural Networks, Apr 2018, Bruges,\n  Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to efficiently compute load-flows (the steady-state\nof the power-grid for given productions, consumptions and grid topology),\nsubstituting conventional simulators based on differential equation solvers. We\nuse a deep feed-forward neural network trained with load-flows precomputed by\nsimulation. Our architecture permits to train a network on so-called \"n-1\"\nproblems, in which load flows are evaluated for every possible line\ndisconnection, then generalize to \"n-2\" problems without retraining (a clear\nadvantage because of the combinatorial nature of the problem). To that end, we\ndeveloped a technique bearing similarity with \"dropout\", which we named \"guided\ndropout\".\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 07:10:36 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Donnot", "Benjamin", "", "TAU"], ["Guyon", "Isabelle", "", "TAU"], ["Schoenauer", "Marc", "", "TAU"], ["Marot", "Antoine", ""], ["Panciatici", "Patrick", ""]]}, {"id": "1801.09955", "submitter": "Toon Van Craenendonck", "authors": "Toon Van Craenendonck, Sebastijan Dumancic, Hendrik Blockeel", "title": "COBRA: A Fast and Simple Method for Active Clustering with Pairwise\n  Constraints", "comments": "Presented at IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is inherently ill-posed: there often exist multiple valid\nclusterings of a single dataset, and without any additional information a\nclustering system has no way of knowing which clustering it should produce.\nThis motivates the use of constraints in clustering, as they allow users to\ncommunicate their interests to the clustering system. Active constraint-based\nclustering algorithms select the most useful constraints to query, aiming to\nproduce a good clustering using as few constraints as possible. We propose\nCOBRA, an active method that first over-clusters the data by running K-means\nwith a $K$ that is intended to be too large, and subsequently merges the\nresulting small clusters into larger ones based on pairwise constraints. In its\nmerging step, COBRA is able to keep the number of pairwise queries low by\nmaximally exploiting constraint transitivity and entailment. We experimentally\nshow that COBRA outperforms the state of the art in terms of clustering quality\nand runtime, without requiring the number of clusters in advance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 12:30:50 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Van Craenendonck", "Toon", ""], ["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1801.10033", "submitter": "Masun Nabhan Homsi", "authors": "Philip Warrick (1) and Masun Nabhan Homsi (2) ((1) PeriGen. Inc.,\n  Montreal, Canada, (2) Simon Bolivar University, Caracas, Venezuela)", "title": "Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long\n  Short-Term Memory Networks", "comments": "Computing in Cardiology 2017, 4 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder\nassociated with deadly and debilitating consequences including heart failure,\nstroke, poor mental health, reduced quality of life and death. Having an\nautomatic system that diagnoses various types of cardiac arrhythmias would\nassist cardiologists to initiate appropriate preventive measures and to improve\nthe analysis of cardiac disease. To this end, this paper introduces a new\napproach to detect and classify automatically cardiac arrhythmias in\nelectrocardiograms (ECG) recordings.\n  Methods: The proposed approach used a combination of Convolution Neural\nNetworks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with\npooling, dropout and normalization techniques to improve their accuracy. The\nnetwork predicted a classification at every 18th input sample and we selected\nthe final prediction for classification. Results were cross-validated on the\nPhysionet Challenge 2017 training dataset, which contains 8,528 single lead ECG\nrecordings lasting from 9s to just over 60s.\n  Results: Using the proposed structure and no explicit feature selection,\n10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015\non the held-out test data (mean-standard deviation over all folds) and 0.80 on\nthe hidden dataset of the Challenge entry server.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 14:59:57 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Warrick", "Philip", ""], ["Homsi", "Masun Nabhan", ""]]}, {"id": "1801.10108", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Moritz Gerlach, Matthias Hein, Dejan Slepcev", "title": "Error estimates for spectral convergence of the graph Laplacian on\n  random geometric graphs towards the Laplace--Beltrami operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.AP math.DG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence of the graph Laplacian of a random geometric graph\ngenerated by an i.i.d. sample from a $m$-dimensional submanifold $M$ in $R^d$\nas the sample size $n$ increases and the neighborhood size $h$ tends to zero.\nWe show that eigenvalues and eigenvectors of the graph Laplacian converge with\na rate of $O\\Big(\\big(\\frac{\\log n}{n}\\big)^\\frac{1}{2m}\\Big)$ to the\neigenvalues and eigenfunctions of the weighted Laplace-Beltrami operator of\n$M$.\n  No information on the submanifold $M$ is needed in the construction of the\ngraph or the \"out-of-sample extension\" of the eigenvectors. Of independent\ninterest is a generalization of the rate of convergence of empirical measures\non submanifolds in $R^d$ in infinity transportation distance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 17:23:27 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Gerlach", "Moritz", ""], ["Hein", "Matthias", ""], ["Slepcev", "Dejan", ""]]}, {"id": "1801.10123", "submitter": "Philip Mansfield", "authors": "Philip Andrew Mansfield, Quan Wang, Carlton Downey, Li Wan, Ignacio\n  Lopez Moreno", "title": "Links: A High-Dimensional Online Clustering Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm, called Links, designed to perform online\nclustering on unit vectors in a high-dimensional Euclidean space. The algorithm\nis appropriate when it is necessary to cluster data efficiently as it streams\nin, and is to be contrasted with traditional batch clustering algorithms that\nhave access to all data at once. For example, Links has been successfully\napplied to embedding vectors generated from face images or voice recordings for\nthe purpose of recognizing people, thereby providing real-time identification\nduring video or audio capture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:15:02 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Mansfield", "Philip Andrew", ""], ["Wang", "Quan", ""], ["Downey", "Carlton", ""], ["Wan", "Li", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "1801.10130", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Mario Geiger, Jonas Koehler, Max Welling", "title": "Spherical CNNs", "comments": "Proceedings of the 6th International Conference on Learning\n  Representations (ICLR), 2018", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the method of choice for\nlearning problems involving 2D planar images. However, a number of problems of\nrecent interest have created a demand for models that can analyze spherical\nimages. Examples include omnidirectional vision for drones, robots, and\nautonomous cars, molecular regression problems, and global weather and climate\nmodelling. A naive application of convolutional networks to a planar projection\nof the spherical signal is destined to fail, because the space-varying\ndistortions introduced by such a projection will make translational weight\nsharing ineffective.\n  In this paper we introduce the building blocks for constructing spherical\nCNNs. We propose a definition for the spherical cross-correlation that is both\nexpressive and rotation-equivariant. The spherical correlation satisfies a\ngeneralized Fourier theorem, which allows us to compute it efficiently using a\ngeneralized (non-commutative) Fast Fourier Transform (FFT) algorithm. We\ndemonstrate the computational efficiency, numerical accuracy, and effectiveness\nof spherical CNNs applied to 3D model recognition and atomization energy\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:28:30 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 08:06:34 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 13:43:49 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Geiger", "Mario", ""], ["Koehler", "Jonas", ""], ["Welling", "Max", ""]]}, {"id": "1801.10158", "submitter": "Patrick Komiske", "authors": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, and Matthew D.\n  Schwartz", "title": "Learning to Classify from Impure Samples with High-Dimensional Data", "comments": "6 pages, 2 tables, 2 figures. v2: updated to match PRD version", "journal-ref": "Phys. Rev. D 98, 011502 (2018)", "doi": "10.1103/PhysRevD.98.011502", "report-no": "MIT-CTP 4968", "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A persistent challenge in practical classification tasks is that labeled\ntraining sets are not always available. In particle physics, this challenge is\nsurmounted by the use of simulations. These simulations accurately reproduce\nmost features of data, but cannot be trusted to capture all of the complex\ncorrelations exploitable by modern machine learning methods. Recent work in\nweakly supervised learning has shown that simple, low-dimensional classifiers\ncan be trained using only the impure mixtures present in data. Here, we\ndemonstrate that complex, high-dimensional classifiers can also be trained on\nimpure mixtures using weak supervision techniques, with performance comparable\nto what could be achieved with pure samples. Using weak supervision will\ntherefore allow us to avoid relying exclusively on simulations for\nhigh-dimensional classification. This work opens the door to a new regime\nwhereby complex models are trained directly on data, providing direct access to\nprobe the underlying physics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 19:00:00 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 10:37:21 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Komiske", "Patrick T.", ""], ["Metodiev", "Eric M.", ""], ["Nachman", "Benjamin", ""], ["Schwartz", "Matthew D.", ""]]}, {"id": "1801.10186", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan S. Nobandegani, Ioannis N. Psaromiligkos", "title": "A Rational Distributed Process-level Account of Independence Judgment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is inconceivable how chaotic the world would look to humans, faced with\ninnumerable decisions a day to be made under uncertainty, had they been lacking\nthe capacity to distinguish the relevant from the irrelevant---a capacity which\ncomputationally amounts to handling probabilistic independence relations. The\nhighly parallel and distributed computational machinery of the brain suggests\nthat a satisfying process-level account of human independence judgment should\nalso mimic these features. In this work, we present the first rational,\ndistributed, message-passing, process-level account of independence judgment,\ncalled $\\mathcal{D}^\\ast$. Interestingly, $\\mathcal{D}^\\ast$ shows a curious,\nbut normatively-justified tendency for quick detection of dependencies,\nwhenever they hold. Furthermore, $\\mathcal{D}^\\ast$ outperforms all the\npreviously proposed algorithms in the AI literature in terms of worst-case\nrunning time, and a salient aspect of it is supported by recent work in\nneuroscience investigating possible implementations of Bayes nets at the neural\nlevel. $\\mathcal{D}^\\ast$ nicely exemplifies how the pursuit of cognitive\nplausibility can lead to the discovery of state-of-the-art algorithms with\nappealing properties, and its simplicity makes $\\mathcal{D}^\\ast$ potentially a\ngood candidate for pedagogical purposes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 19:42:45 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Nobandegani", "Ardavan S.", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1801.10193", "submitter": "Hakime \\\"Ozt\\\"urk", "authors": "Hakime \\\"Ozt\\\"urk, Elif Ozkirimli, Arzucan \\\"Ozg\\\"ur", "title": "DeepDTA: Deep Drug-Target Binding Affinity Prediction", "comments": "extended version", "journal-ref": null, "doi": "10.1093/bioinformatics/bty593", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of novel drug-target (DT) interactions is a substantial\npart of the drug discovery process. Most of the computational methods that have\nbeen proposed to predict DT interactions have focused on binary classification,\nwhere the goal is to determine whether a DT pair interacts or not. However,\nprotein-ligand interactions assume a continuum of binding strength values, also\ncalled binding affinity and predicting this value still remains a challenge.\nThe increase in the affinity data available in DT knowledge-bases allows the\nuse of advanced learning techniques such as deep learning architectures in the\nprediction of binding affinities. In this study, we propose a deep-learning\nbased model that uses only sequence information of both targets and drugs to\npredict DT interaction binding affinities. The few studies that focus on DT\nbinding affinity prediction use either 3D structures of protein-ligand\ncomplexes or 2D features of compounds. One novel approach used in this work is\nthe modeling of protein sequences and compound 1D representations with\nconvolutional neural networks (CNNs). The results show that the proposed deep\nlearning based model that uses the 1D representations of targets and drugs is\nan effective approach for drug target binding affinity prediction. The model in\nwhich high-level representations of a drug and a target are constructed via\nCNNs achieved the best Concordance Index (CI) performance in one of our larger\nbenchmark data sets, outperforming the KronRLS algorithm and SimBoost, a\nstate-of-the-art method for DT binding affinity prediction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 19:58:35 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 06:34:46 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["\u00d6zt\u00fcrk", "Hakime", ""], ["Ozkirimli", "Elif", ""], ["\u00d6zg\u00fcr", "Arzucan", ""]]}, {"id": "1801.10199", "submitter": "Hakime \\\"Ozt\\\"urk", "authors": "Hakime \\\"Ozt\\\"urk, Elif Ozkirimli, Arzucan \\\"Ozg\\\"ur", "title": "A novel methodology on distributed representations of proteins using\n  their interacting ligands", "comments": null, "journal-ref": "Bioinformatics 2018", "doi": "10.1093/bioinformatics/bty287", "report-no": null, "categories": "stat.ML q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective representation of proteins is a crucial task that directly\naffects the performance of many bioinformatics problems. Related proteins\nusually bind to similar ligands. Chemical characteristics of ligands are known\nto capture the functional and mechanistic properties of proteins suggesting\nthat a ligand based approach can be utilized in protein representation. In this\nstudy, we propose SMILESVec, a SMILES-based method to represent ligands and a\nnovel method to compute similarity of proteins by describing them based on\ntheir ligands. The proteins are defined utilizing the word-embeddings of the\nSMILES strings of their ligands. The performance of the proposed protein\ndescription method is evaluated in protein clustering task using TransClust and\nMCL algorithms. Two other protein representation methods that utilize protein\nsequence, BLAST and ProtVec, and two compound fingerprint based protein\nrepresentation methods are compared. We showed that ligand-based protein\nrepresentation, which uses only SMILES strings of the ligands that proteins\nbind to, performs as well as protein-sequence based representation methods in\nprotein clustering. The results suggest that ligand-based protein description\ncan be an alternative to the traditional sequence or structure based\nrepresentation of proteins and this novel approach can be applied to different\nbioinformatics problems such as prediction of new protein-ligand interactions\nand protein function annotation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 20:07:35 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["\u00d6zt\u00fcrk", "Hakime", ""], ["Ozkirimli", "Elif", ""], ["\u00d6zg\u00fcr", "Arzucan", ""]]}, {"id": "1801.10242", "submitter": "Jonas Mueller", "authors": "Jonas Mueller, Vasilis Syrgkanis, Matt Taddy", "title": "Low-Rank Bandit Methods for High-Dimensional Dynamic Pricing", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic pricing with many products under an evolving but\nlow-dimensional demand model. Assuming the temporal variation in\ncross-elasticities exhibits low-rank structure based on fixed (latent) features\nof the products, we show that the revenue maximization problem reduces to an\nonline bandit convex optimization with side information given by the observed\ndemands. We design dynamic pricing algorithms whose revenue approaches that of\nthe best fixed price vector in hindsight, at a rate that only depends on the\nintrinsic rank of the demand model and not the number of products. Our approach\napplies a bandit convex optimization algorithm in a projected low-dimensional\nspace spanned by the latent product features, while simultaneously learning\nthis span via online singular value decomposition of a carefully-crafted matrix\ncontaining the observed demands.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 22:19:13 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 03:54:27 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Mueller", "Jonas", ""], ["Syrgkanis", "Vasilis", ""], ["Taddy", "Matt", ""]]}, {"id": "1801.10273", "submitter": "Congzheng Song", "authors": "Congzheng Song, Yiming Sun", "title": "Kernel Distillation for Fast Gaussian Processes Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are flexible models that can capture complex\nstructure in large-scale dataset due to their non-parametric nature. However,\nthe usage of GPs in real-world application is limited due to their high\ncomputational cost at inference time. In this paper, we introduce a new\nframework, \\textit{kernel distillation}, to approximate a fully trained teacher\nGP model with kernel matrix of size $n\\times n$ for $n$ training points. We\ncombine inducing points method with sparse low-rank approximation in the\ndistillation procedure. The distilled student GP model only costs $O(m^2)$\nstorage for $m$ inducing points where $m \\ll n$ and improves the inference time\ncomplexity. We demonstrate empirically that kernel distillation provides better\ntrade-off between the prediction time and the test performance compared to the\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 01:59:58 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 16:54:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Song", "Congzheng", ""], ["Sun", "Yiming", ""]]}, {"id": "1801.10395", "submitter": "Andreas Doerr", "authors": "Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong,\n  Stefan Schaal, Marc Toussaint, Sebastian Trimpe", "title": "Probabilistic Recurrent State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models (SSMs) are a highly expressive model class for learning\npatterns in time series data and for system identification. Deterministic\nversions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex\ntime series data. Fully probabilistic SSMs, however, are often found hard to\ntrain, even for smaller problems. To overcome this limitation, we propose a\nnovel model formulation and a scalable training algorithm based on doubly\nstochastic variational inference and Gaussian processes. In contrast to\nexisting work, the proposed variational approximation allows one to fully\ncapture the latent state temporal correlations. These correlations are the key\nto robust training. The effectiveness of the proposed PR-SSM is evaluated on a\nset of real-world benchmark datasets in comparison to state-of-the-art\nprobabilistic model learning methods. Scalability and robustness are\ndemonstrated on a high dimensional problem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 10:53:02 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 16:29:58 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Doerr", "Andreas", ""], ["Daniel", "Christian", ""], ["Schiegg", "Martin", ""], ["Nguyen-Tuong", "Duy", ""], ["Schaal", "Stefan", ""], ["Toussaint", "Marc", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "1801.10402", "submitter": "Guanqun Cao Dr", "authors": "Guanqun Cao and Alexandros Iosifidis and Moncef Gabbouj and Vijay\n  Raghavan and Raju Gottumukkala", "title": "Deep Multi-view Learning to Rank", "comments": "Published at IEEE TKDE", "journal-ref": null, "doi": "10.1109/TKDE.2019.2942590", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to rank from multiple information sources.\nThough multi-view learning and learning to rank have been studied extensively\nleading to a wide range of applications, multi-view learning to rank as a\nsynergy of both topics has received little attention. The aim of the paper is\nto propose a composite ranking method while keeping a close correlation with\nthe individual rankings simultaneously. We present a generic framework for\nmulti-view subspace learning to rank (MvSL2R), and two novel solutions are\nintroduced under the framework. The first solution captures information of\nfeature mappings from within each view as well as across views using\nautoencoder-like networks. Novel feature embedding methods are formulated in\nthe optimization of multi-view unsupervised and discriminant autoencoders.\nMoreover, we introduce an end-to-end solution to learning towards both the\njoint ranking objective and the individual rankings. The proposed solution\nenhances the joint ranking with minimum view-specific ranking loss, so that it\ncan achieve the maximum global view agreements in a single optimization\nprocess. The proposed method is evaluated on three different ranking problems,\ni.e. university ranking, multi-view lingual text ranking and image data\nranking, providing superior results compared to related methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 11:13:49 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 06:57:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cao", "Guanqun", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""], ["Raghavan", "Vijay", ""], ["Gottumukkala", "Raju", ""]]}, {"id": "1801.10459", "submitter": "Xiaoqin Zhang", "authors": "Xiaoqin Zhang, Huimin Ma", "title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With\n  Expert Demonstrations", "comments": "Added acknowledgements, modified references. 7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining with expert demonstrations have been found useful in speeding up\nthe training process of deep reinforcement learning algorithms since less\nonline simulation data is required. Some people use supervised learning to\nspeed up the process of feature learning, others pretrain the policies by\nimitating expert demonstrations. However, these methods are unstable and not\nsuitable for actor-critic reinforcement learning algorithms. Also, some\nexisting methods rely on the global optimum assumption, which is not true in\nmost scenarios. In this paper, we employ expert demonstrations in a\nactor-critic reinforcement learning framework, and meanwhile ensure that the\nperformance is not affected by the fact that expert demonstrations are not\nglobal optimal. We theoretically derive a method for computing policy gradients\nand value estimators with only expert demonstrations. Our method is\ntheoretically plausible for actor-critic reinforcement learning algorithms that\npretrains both policy and value functions. We apply our method to two of the\ntypical actor-critic reinforcement learning algorithms, DDPG and ACER, and\ndemonstrate with experiments that our method not only outperforms the RL\nalgorithms without pretraining process, but also is more simulation efficient.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 14:30:00 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 06:36:09 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Zhang", "Xiaoqin", ""], ["Ma", "Huimin", ""]]}, {"id": "1801.10562", "submitter": "Min Xu", "authors": "Bo Zhou, Qiang Guo, Xiangrui Zeng, Min Xu", "title": "Feature Decomposition Based Saliency Detection in Electron\n  Cryo-Tomograms", "comments": "14 pages", "journal-ref": "IEEE International Conference on Bioinformatics & Biomedicine,\n  Workshop on Machine Learning in High Resolution Microscopy (BIBM-MLHRM 2018)", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular\nstructures at the submolecular resolution in close to the native state.\nHowever, due to the high degree of structural complexity and imaging limits,\nthe automatic segmentation of cellular components from ECT images is very\ndifficult. To complement and speed up existing segmentation methods, it is\ndesirable to develop a generic cell component segmentation method that is 1)\nnot specific to particular types of cellular components, 2) able to segment\nunknown cellular components, 3) fully unsupervised and does not rely on the\navailability of training data. As an important step towards this goal, in this\npaper, we propose a saliency detection method that computes the likelihood that\na subregion in a tomogram stands out from the background. Our method consists\nof four steps: supervoxel over-segmentation, feature extraction, feature matrix\ndecomposition, and computation of saliency. The method produces a distribution\nmap that represents the regions' saliency in tomograms. Our experiments show\nthat our method can successfully label most salient regions detected by a human\nobserver, and able to filter out regions not containing cellular components.\nTherefore, our method can remove the majority of the background region, and\nsignificantly speed up the subsequent processing of segmentation and\nrecognition of cellular components captured by ECT.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:25:14 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Zhou", "Bo", ""], ["Guo", "Qiang", ""], ["Zeng", "Xiangrui", ""], ["Xu", "Min", ""]]}, {"id": "1801.10578", "submitter": "Tsui-Wei Weng", "authors": "Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng\n  Gao, Cho-Jui Hsieh, Luca Daniel", "title": "Evaluating the Robustness of Neural Networks: An Extreme Value Theory\n  Approach", "comments": "Accepted by Sixth International Conference on Learning\n  Representations (ICLR 2018). Tsui-Wei Weng and Huan Zhang contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of neural networks to adversarial examples has received great\nattention due to security implications. Despite various attack approaches to\ncrafting visually imperceptible adversarial examples, little has been developed\ntowards a comprehensive measure of robustness. In this paper, we provide a\ntheoretical justification for converting robustness analysis into a local\nLipschitz constant estimation problem, and propose to use the Extreme Value\nTheory for efficient evaluation. Our analysis yields a novel robustness metric\ncalled CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork\nRobustness. The proposed CLEVER score is attack-agnostic and computationally\nfeasible for large neural networks. Experimental results on various networks,\nincluding ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned\nwith the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms\nof adversarial examples from powerful attacks, and (ii) defended networks using\ndefensive distillation or bounded ReLU indeed achieve better CLEVER scores. To\nthe best of our knowledge, CLEVER is the first attack-independent robustness\nmetric that can be applied to any neural network classifier.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:51:32 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Weng", "Tsui-Wei", ""], ["Zhang", "Huan", ""], ["Chen", "Pin-Yu", ""], ["Yi", "Jinfeng", ""], ["Su", "Dong", ""], ["Gao", "Yupeng", ""], ["Hsieh", "Cho-Jui", ""], ["Daniel", "Luca", ""]]}, {"id": "1801.10579", "submitter": "Natasa Tagasovska", "authors": "Natasa Tagasovska, Val\\'erie Chavez-Demoulin, Thibault Vatter", "title": "Distinguishing Cause from Effect Using Quantiles: Bivariate Quantile\n  Causal Discovery", "comments": "To appear ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference using observational data is challenging, especially in the\nbivariate case. Through the minimum description length principle, we link the\npostulate of independence between the generating mechanisms of the cause and of\nthe effect given the cause to quantile regression. Based on this theory, we\ndevelop Bivariate Quantile Causal Discovery (bQCD), a new method to distinguish\ncause from effect assuming no confounding, selection bias or feedback. Because\nit uses multiple quantile levels instead of the conditional mean only, bQCD is\nadaptive not only to additive, but also to multiplicative or even\nlocation-scale generating mechanisms. To illustrate the effectiveness of our\napproach, we perform an extensive empirical comparison on both synthetic and\nreal datasets. This study shows that bQCD is robust across different\nimplementations of the method (i.e., the quantile regression), computationally\nefficient, and compares favorably to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:54:17 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 13:15:49 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 18:05:54 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2020 08:54:54 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Tagasovska", "Natasa", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""], ["Vatter", "Thibault", ""]]}, {"id": "1801.10597", "submitter": "Min Xu", "authors": "Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu", "title": "Model compression for faster structural separation of macromolecules\n  captured by Cellular Electron Cryo-Tomography", "comments": "8 pages", "journal-ref": "International Conference on Image Analysis and Recognition (ICIAR)\n  2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule\nstructure inside single cells. Macromolecule classification approaches based on\nconvolutional neural networks (CNN) were developed to separate millions of\nmacromolecules captured from ECT systematically. However, given the fast\naccumulation of ECT data, it will soon become necessary to use CNN models to\nefficiently and accurately separate substantially more macromolecules at the\nprediction stage, which requires additional computational costs. To speed up\nthe prediction, we compress classification models into compact neural networks\nwith little in accuracy for deployment. Specifically, we propose to perform\nmodel compression through knowledge distillation. Firstly, a complex teacher\nnetwork is trained to generate soft labels with better classification\nfeasibility followed by training of customized student networks with simple\narchitectures using the soft label to compress model complexity. Our tests\ndemonstrate that our compressed models significantly reduce the number of\nparameters and time cost while maintaining similar classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 18:39:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Guo", "Jialiang", ""], ["Zhou", "Bo", ""], ["Zeng", "Xiangrui", ""], ["Freyberg", "Zachary", ""], ["Xu", "Min", ""]]}]