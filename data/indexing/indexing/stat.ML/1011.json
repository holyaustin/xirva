[{"id": "1011.0057", "submitter": "Julien Cornebise", "authors": "Luke Bornn, Julien Cornebise, Gareth W. Peters", "title": "Discussion of \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods'' by M. Girolami and B. Calderhead", "comments": "To appear in Journal of the Royal Statitistical Society Series B, in\n  the discussion of the read paper by Calderhead and Girolami 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report is the union of two contributions to the discussion of\nthe Read Paper \"Riemann manifold Langevin and Hamiltonian Monte Carlo methods\"\nby B. Calderhead and M. Girolami, presented in front of the Royal Statistical\nSociety on October 13th 2010 and to appear in the Journal of the Royal\nStatistical Society Series B. The first comment establishes a parallel and\npossible interactions with Adaptive Monte Carlo methods. The second comment\nexposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)\nfor a weakly identifiable model presenting a strong ridge in its geometry.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 08:38:15 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Bornn", "Luke", ""], ["Cornebise", "Julien", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1011.0096", "submitter": "Matthieu Cornec", "authors": "Matthieu Cornec", "title": "Concentration inequalities of the cross-validation estimator for\n  Empirical Risk Minimiser", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for empirical risk\nminimizers. In the general setting, we prove sanity-check bounds in the spirit\nof \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case\nerror of this estimate is not much worse that of training error estimate}\n\\textquotedblright . General loss functions and class of predictors with finite\nVC-dimension are considered. We closely follow the formalism introduced by\n\\cite{DUD03} to cover a large variety of cross-validation procedures including\nleave-one-out cross-validation, $k$% -fold cross-validation, hold-out\ncross-validation (or split sample), and the leave-$\\upsilon$-out\ncross-validation.\n  In particular, we focus on proving the consistency of the various\ncross-validation procedures. We point out the interest of each cross-validation\nprocedure in terms of rate of convergence. An estimation curve with transition\nphases depending on the cross-validation procedure and not only on the\npercentage of observations in the test sample gives a simple rule on how to\nchoose the cross-validation. An interesting consequence is that the size of the\ntest sample is not required to grow to infinity for the consistency of the\ncross-validation procedure.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 17:48:04 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Cornec", "Matthieu", ""]]}, {"id": "1011.0097", "submitter": "Shiqian Ma", "authors": "Katya Scheinberg, Shiqian Ma, Donald Goldfarb", "title": "Sparse Inverse Covariance Selection via Alternating Linearization\n  Methods", "comments": null, "journal-ref": "NIPS 2010", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are of great interest in statistical learning.\nBecause the conditional independencies between different nodes correspond to\nzero entries in the inverse covariance matrix of the Gaussian distribution, one\ncan learn the structure of the graph by estimating a sparse inverse covariance\nmatrix from sample data, by solving a convex maximum likelihood problem with an\n$\\ell_1$-regularization term. In this paper, we propose a first-order method\nbased on an alternating linearization technique that exploits the problem's\nspecial structure; in particular, the subproblems solved in each iteration have\nclosed-form solutions. Moreover, our algorithm obtains an $\\epsilon$-optimal\nsolution in $O(1/\\epsilon)$ iterations. Numerical experiments on both synthetic\nand real data from gene association networks show that a practical version of\nthis algorithm outperforms other competitive algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 18:30:43 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Scheinberg", "Katya", ""], ["Ma", "Shiqian", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1011.0413", "submitter": "Michael Mahoney", "authors": "Jacob Bien and Ya Xu and Michael W. Mahoney", "title": "CUR from a Sparse Optimization Viewpoint", "comments": "9 pages; in NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR decomposition provides an approximation of a matrix $X$ that has low\nreconstruction error and that is sparse in the sense that the resulting\napproximation lies in the span of only a few columns of $X$. In this regard, it\nappears to be similar to many sparse PCA methods. However, CUR takes a\nrandomized algorithmic approach, whereas most sparse PCA methods are framed as\nconvex optimization problems. In this paper, we try to understand CUR from a\nsparse optimization viewpoint. We show that CUR is implicitly optimizing a\nsparse regression objective and, furthermore, cannot be directly cast as a\nsparse PCA method. We also observe that the sparsity attained by CUR possesses\nan interesting structure, which leads us to formulate a sparse PCA method that\nachieves a CUR-like sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 19:07:15 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Bien", "Jacob", ""], ["Xu", "Ya", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1011.0450", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos and Georgios B. Giannakis", "title": "From Sparse Signals to Sparse Residuals for Robust Sensing", "comments": "Under review for publication in the IEEE Transactions on Signal\n  Processing (revised version)", "journal-ref": null, "doi": "10.1109/TSP.2011.2141661", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in sensor networks is the extraction of information\nby fusing data from a multitude of distinct, but possibly unreliable sensors.\nRecovering information from the maximum number of dependable sensors while\nspecifying the unreliable ones is critical for robust sensing. This sensing\ntask is formulated here as that of finding the maximum number of feasible\nsubsystems of linear equations, and proved to be NP-hard. Useful links are\nestablished with compressive sampling, which aims at recovering vectors that\nare sparse. In contrast, the signals here are not sparse, but give rise to\nsparse residuals. Capitalizing on this form of sparsity, four sensing schemes\nwith complementary strengths are developed. The first scheme is a convex\nrelaxation of the original problem expressed as a second-order cone program\n(SOCP). It is shown that when the involved sensing matrices are Gaussian and\nthe reliable measurements are sufficiently many, the SOCP can recover the\noptimal solution with overwhelming probability. The second scheme is obtained\nby replacing the initial objective function with a concave one. The third and\nfourth schemes are tailored for noisy sensor data. The noisy case is cast as a\ncombinatorial problem that is subsequently surrogated by a (weighted) SOCP.\nInterestingly, the derived cost functions fall into the framework of robust\nmultivariate linear regression, while an efficient block-coordinate descent\nalgorithm is developed for their minimization. The robust sensing capabilities\nof all schemes are verified by simulated tests.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 20:59:12 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2011 20:05:18 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1011.0506", "submitter": "Suren Rathnayake", "authors": "Vladimir Nikulin, Tian-Hsiang Huang, Shu-Kay Ng, Suren I Rathnayake,\n  and Geoffrey J McLachlan", "title": "A Very Fast Algorithm for Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very fast algorithm for general matrix factorization of a data\nmatrix for use in the statistical analysis of high-dimensional data via latent\nfactors. Such data are prevalent across many application areas and generate an\never-increasing demand for methods of dimension reduction in order to undertake\nthe statistical analysis of interest. Our algorithm uses a gradient-based\napproach which can be used with an arbitrary loss function provided the latter\nis differentiable. The speed and effectiveness of our algorithm for dimension\nreduction is demonstrated in the context of supervised classification of some\nreal high-dimensional data sets from the bioinformatics literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 03:36:52 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Nikulin", "Vladimir", ""], ["Huang", "Tian-Hsiang", ""], ["Ng", "Shu-Kay", ""], ["Rathnayake", "Suren I", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1011.0686", "submitter": "Stephane Ross", "authors": "Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret\n  Online Learning", "comments": "Appearing in the 14th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential prediction problems such as imitation learning, where future\nobservations depend on previous predictions (actions), violate the common\ni.i.d. assumptions made in statistical learning. This leads to poor performance\nin theory and often in practice. Some recent approaches provide stronger\nguarantees in this setting, but remain somewhat unsatisfactory as they train\neither non-stationary or stochastic policies and require a large number of\niterations. In this paper, we propose a new iterative algorithm, which trains a\nstationary deterministic policy, that can be seen as a no regret algorithm in\nan online learning setting. We show that any such no regret algorithm, combined\nwith additional reduction assumptions, must find a policy with good performance\nunder the distribution of observations it induces in such sequential settings.\nWe demonstrate that this new approach outperforms previous approaches on two\nchallenging imitation learning problems and a benchmark sequence labeling\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 17:55:55 GMT"}, {"version": "v2", "created": "Wed, 3 Nov 2010 15:59:19 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2011 18:51:21 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ross", "Stephane", ""], ["Gordon", "Geoffrey J.", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1011.0774", "submitter": "Tauhid Zaman", "authors": "Dhruv Parthasarathy, Devavrat Shah and Tauhid Zaman", "title": "Leaders, Followers, and Community Detection", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communities in social networks or graphs are sets of well-connected,\noverlapping vertices. The effectiveness of a community detection algorithm is\ndetermined by accuracy in finding the ground-truth communities and ability to\nscale with the size of the data. In this work, we provide three contributions.\nFirst, we show that a popular measure of accuracy known as the F1 score, which\nis between 0 and 1, with 1 being perfect detection, has an information lower\nbound is 0.5. We provide a trivial algorithm that produces communities with an\nF1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular\nalgorithms such as modularity optimization, BigClam and CESNA have F1 scores\nless than 0.5 for the popular IMDB graph. To rectify this, as the second\ncontribution we propose a generative model for community formation, the\nsequential community graph, which is motivated by the formation of social\nnetworks. Third, motivated by our generative model, we propose the\nleader-follower algorithm (LFA). We prove that it recovers all communities for\nsequential community graphs by establishing a structural result that sequential\ncommunity graphs are chordal. For a large number of popular social networks, it\nrecovers communities with a much higher F1 score than other popular algorithms.\nFor the IMDB graph, it obtains an F1 score of 0.81. We also propose a\nmodification to the LFA called the fast leader-follower algorithm (FLFA) which\nin addition to being highly accurate, is also fast, with a scaling that is\nalmost linear in the network size.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 23:46:19 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 03:06:39 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 04:30:02 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Parthasarathy", "Dhruv", ""], ["Shah", "Devavrat", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1011.0997", "submitter": "Blake Hunter", "authors": "Blake Hunter and Thomas Strohmer", "title": "Performance Analysis of Spectral Clustering on Compressed, Incomplete\n  and Inaccurate Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most widely used techniques for extracting\nthe underlying global structure of a data set. Compressed sensing and matrix\ncompletion have emerged as prevailing methods for efficiently recovering sparse\nand partially observed signals respectively. We combine the distance preserving\nmeasurements of compressed sensing and matrix completion with the power of\nrobust spectral clustering. Our analysis provides rigorous bounds on how small\nerrors in the affinity matrix can affect the spectral coordinates and\nclusterability. This work generalizes the current perturbation results of\ntwo-class spectral clustering to incorporate multi-class clustering with k\neigenvectors. We thoroughly track how small perturbation from using compressed\nsensing and matrix completion affect the affinity matrix and in succession the\nspectral coordinates. These perturbation results for multi-class clustering\nrequire an eigengap between the kth and (k+1)th eigenvalues of the affinity\nmatrix, which naturally occurs in data with k well-defined clusters. Our\ntheoretical guarantees are complemented with numerical results along with a\nnumber of examples of the unsupervised organization and clustering of image\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 20:02:35 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Hunter", "Blake", ""], ["Strohmer", "Thomas", ""]]}, {"id": "1011.1026", "submitter": "Jinzhu Jia", "authors": "Jinzhu Jia, Karl Rohe and Bin Yu", "title": "The Lasso under Heteroscedasticity", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the Lasso is well understood under the assumptions of the\nstandard linear model with homoscedastic noise. However, in several\napplications, the standard model does not describe the important features of\nthe data. This paper examines how the Lasso performs on a non-standard model\nthat is motivated by medical imaging applications. In these applications, the\nvariance of the noise scales linearly with the expectation of the observation.\nLike all heteroscedastic models, the noise terms in this Poisson-like model are\n\\textit{not} independent of the design matrix.\n  More specifically, this paper studies the sign consistency of the Lasso under\na sparse Poisson-like model. In addition to studying sufficient conditions for\nthe sign consistency of the Lasso estimate, this paper also gives necessary\nconditions for sign consistency. Both sets of conditions are comparable to\nresults for the homoscedastic model, showing that when a measure of the signal\nto noise ratio is large, the Lasso performs well on both Poisson-like data and\nhomoscedastic data.\n  Simulations reveal that the Lasso performs equally well in terms of model\nselection performance on both Poisson-like data and homoscedastic data (with\nproperly scaled noise variance), across a range of parameterizations. Taken as\na whole, these results suggest that the Lasso is robust to the Poisson-like\nheteroscedastic noise.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 22:26:10 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Jia", "Jinzhu", ""], ["Rohe", "Karl", ""], ["Yu", "Bin", ""]]}, {"id": "1011.1373", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran", "title": "The Loss Rank Criterion for Variable Selection in Linear Regression\n  Analysis", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": "10.1111/j.1467-9469.2011.00732.x", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Lasso and other regularization procedures are attractive methods for variable\nselection, subject to a proper choice of shrinkage parameter. Given a set of\npotential subsets produced by a regularization algorithm, a consistent model\nselection criterion is proposed to select the best one among this preselected\nset. The approach leads to a fast and efficient procedure for variable\nselection, especially in high-dimensional settings. Model selection consistency\nof the suggested criterion is proven when the number of covariates d is fixed.\nSimulation studies suggest that the criterion still enjoys model selection\nconsistency when d is much larger than the sample size. The simulations also\nshow that our approach for variable selection works surprisingly well in\ncomparison with existing competitors. The method is also applied to a real data\nset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 11:54:24 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Tran", "Minh-Ngoc", ""]]}, {"id": "1011.1379", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran and Marcus Hutter", "title": "Model Selection by Loss Rank for Classification and Unsupervised\n  Learning", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Hutter (2007) recently introduced the loss rank principle (LoRP) as a\ngeneralpurpose principle for model selection. The LoRP enjoys many attractive\nproperties and deserves further investigations. The LoRP has been well-studied\nfor regression framework in Hutter and Tran (2010). In this paper, we study the\nLoRP for classification framework, and develop it further for model selection\nproblems in unsupervised learning where the main interest is to describe the\nassociations between input measurements, like cluster analysis or graphical\nmodelling. Theoretical properties and simulation studies are presented.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 12:21:12 GMT"}], "update_date": "2010-11-08", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Hutter", "Marcus", ""]]}, {"id": "1011.1518", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade, Tong Zhang", "title": "Robust Matrix Decomposition with Outliers", "comments": "Corrected comparisons to previous work of Candes et al (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a given observation matrix can be decomposed as the sum of a low-rank\nmatrix and a sparse matrix (outliers), and the goal is to recover these\nindividual components from the observed sum. Such additive decompositions have\napplications in a variety of numerical problems including system\nidentification, latent variable graphical modeling, and principal components\nanalysis. We study conditions under which recovering such a decomposition is\npossible via a combination of $\\ell_1$ norm and trace norm minimization. We are\nspecifically interested in the question of how many outliers are allowed so\nthat convex programming can still achieve accurate recovery, and we obtain\nstronger recovery guarantees than previous studies. Moreover, we do not assume\nthat the spatial pattern of outliers is random, which stands in contrast to\nrelated analyses under such assumptions via matrix completion.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 21:43:02 GMT"}, {"version": "v2", "created": "Fri, 12 Nov 2010 22:07:12 GMT"}, {"version": "v3", "created": "Sat, 4 Dec 2010 01:44:01 GMT"}], "update_date": "2010-12-07", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1011.1581", "submitter": "James P. Crutchfield", "authors": "Nicholas F. Travers and James P. Crutchfield", "title": "Asymptotic Synchronization for Finite-State Sources", "comments": "13 pages, 4 figures:\n  http://cse.ucdavis.edu/~cmg/compmech/pubs/asfs.htm; updates and corrections\n  added", "journal-ref": null, "doi": "10.1007/s10955-011-0349-x", "report-no": null, "categories": "nlin.CD cs.IT math.DS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a recent synchronization analysis of exact finite-state sources to\nnonexact sources for which synchronization occurs only asymptotically. Although\nthe proof methods are quite different, the primary results remain the same. We\nfind that an observer's average uncertainty in the source state vanishes\nexponentially fast and, as a consequence, an observer's average uncertainty in\npredicting future output converges exponentially fast to the source entropy\nrate.\n", "versions": [{"version": "v1", "created": "Sat, 6 Nov 2010 19:06:51 GMT"}, {"version": "v2", "created": "Mon, 3 Jan 2011 22:19:39 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Travers", "Nicholas F.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1011.1761", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Arnaud Doucet", "title": "Efficient Bayesian Inference for Generalized Bradley-Terry Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bradley-Terry model is a popular approach to describe probabilities of\nthe possible outcomes when elements of a set are repeatedly compared with one\nanother in pairs. It has found many applications including animal behaviour,\nchess ranking and multiclass classification. Numerous extensions of the basic\nmodel have also been proposed in the literature including models with ties,\nmultiple comparisons, group comparisons and random graphs. From a computational\npoint of view, Hunter (2004) has proposed efficient iterative MM\n(minorization-maximization) algorithms to perform maximum likelihood estimation\nfor these generalized Bradley-Terry models whereas Bayesian inference is\ntypically performed using MCMC (Markov chain Monte Carlo) algorithms based on\ntailored Metropolis-Hastings (M-H) proposals. We show here that these MM\\\nalgorithms can be reinterpreted as special instances of\nExpectation-Maximization (EM) algorithms associated to suitable sets of latent\nvariables and propose some original extensions. These latent variables allow us\nto derive simple Gibbs samplers for Bayesian inference. We demonstrate\nexperimentally the efficiency of these algorithms on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 10:40:19 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Doucet", "Arnaud", ""]]}, {"id": "1011.1970", "submitter": "Aaron Francis McDaid", "authors": "Aaron F. McDaid, Neil J. Hurley", "title": "Using Model-based Overlapping Seed Expansion to detect highly\n  overlapping community structure", "comments": "based on work accepted at ASONAM 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As research into community finding in social networks progresses, there is a\nneed for algorithms capable of detecting overlapping community structure. Many\nalgorithms have been proposed in recent years that are capable of assigning\neach node to more than a single community. The performance of these algorithms\ntends to degrade when the ground-truth contains a more highly overlapping\ncommunity structure, with nodes assigned to more than two communities. Such\nhighly overlapping structure is likely to exist in many social networks, such\nas Facebook friendship networks. In this paper we present a scalable algorithm,\nMOSES, based on a statistical model of community structure, which is capable of\ndetecting highly overlapping community structure, especially when there is\nvariance in the number of communities each node is in. In evaluation on\nsynthetic data MOSES is found to be superior to existing algorithms, especially\nat high levels of overlap. We demonstrate MOSES on real social network data by\nanalyzing the networks of friendship links between students of five US\nuniversities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 04:05:25 GMT"}, {"version": "v2", "created": "Mon, 15 Nov 2010 22:31:14 GMT"}, {"version": "v3", "created": "Wed, 17 Nov 2010 16:38:36 GMT"}], "update_date": "2010-11-18", "authors_parsed": [["McDaid", "Aaron F.", ""], ["Hurley", "Neil J.", ""]]}, {"id": "1011.2234", "submitter": "Rob Tibshirani", "authors": "Robert Tibshirani, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah\n  Simon, Jonathan Taylor, and Ryan J. Tibshirani,", "title": "Strong rules for discarding predictors in lasso-type problems", "comments": "5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider rules for discarding predictors in lasso regression and related\nproblems, for computational efficiency. El Ghaoui et al (2010) propose \"SAFE\"\nrules that guarantee that a coefficient will be zero in the solution, based on\nthe inner products of each predictor with the outcome. In this paper we propose\nstrong rules that are not foolproof but rarely fail in practice. These can be\ncomplemented with simple checks of the Karush- Kuhn-Tucker (KKT) conditions to\nprovide safe rules that offer substantial speed and space savings in a variety\nof statistical convex optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 23:22:24 GMT"}, {"version": "v2", "created": "Wed, 24 Nov 2010 16:42:42 GMT"}], "update_date": "2010-11-25", "authors_parsed": [["Tibshirani", "Robert", ""], ["Bien", "Jacob", ""], ["Friedman", "Jerome", ""], ["Hastie", "Trevor", ""], ["Simon", "Noah", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1011.2771", "submitter": "Alessandro Rinaldo", "authors": "Alessandro Rinaldo, Aarti Singh, Rebecca Nugent, Larry Wasserman", "title": "Stability of Density-Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High density clusters can be characterized by the connected components of a\nlevel set $L(\\lambda) = \\{x:\\ p(x)>\\lambda\\}$ of the underlying probability\ndensity function $p$ generating the data, at some appropriate level\n$\\lambda\\geq 0$. The complete hierarchical clustering can be characterized by a\ncluster tree ${\\cal T}= \\bigcup_{\\lambda} L(\\lambda)$. In this paper, we study\nthe behavior of a density level set estimate $\\widehat L(\\lambda)$ and cluster\ntree estimate $\\widehat{\\cal{T}}$ based on a kernel density estimator with\nkernel bandwidth $h$. We define two notions of instability to measure the\nvariability of $\\widehat L(\\lambda)$ and $\\widehat{\\cal{T}}$ as a function of\n$h$, and investigate the theoretical properties of these instability measures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 21:08:35 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Nugent", "Rebecca", ""], ["Wasserman", "Larry", ""]]}, {"id": "1011.2952", "submitter": "Boumediene Hamzi", "authors": "Jake Bouvrie and Boumediene Hamzi", "title": "Balanced Reduction of Nonlinear Control Systems in Reproducing Kernel\n  Hilbert Space", "comments": null, "journal-ref": "Proc. 48th Annual Allerton Conference on Communication, Control,\n  and Computing, 2010, pp. 294 - 301", "doi": null, "report-no": null, "categories": "math.OC math.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel data-driven order reduction method for nonlinear control\nsystems, drawing on recent progress in machine learning and statistical\ndimensionality reduction. The method rests on the assumption that the nonlinear\nsystem behaves linearly when lifted into a high (or infinite) dimensional\nfeature space where balanced truncation may be carried out implicitly. This\nleads to a nonlinear reduction map which can be combined with a representation\nof the system belonging to a reproducing kernel Hilbert space to give a closed,\nreduced order dynamical system which captures the essential input-output\ncharacteristics of the original model. Empirical simulations illustrating the\napproach are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 12 Nov 2010 15:34:22 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Bouvrie", "Jake", ""], ["Hamzi", "Boumediene", ""]]}, {"id": "1011.3090", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki", "title": "Regularization Strategies and Empirical Bayesian Learning for MKL", "comments": "19pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple kernel learning (MKL), structured sparsity, and multi-task learning\nhave recently received considerable attention. In this paper, we show how\ndifferent MKL algorithms can be understood as applications of either\nregularization on the kernel weights or block-norm-based regularization, which\nis more common in structured sparsity and multi-task learning. We show that\nthese two regularization strategies can be systematically mapped to each other\nthrough a concave conjugate operation. When the kernel-weight-based regularizer\nis separable into components, we can naturally consider a generative\nprobabilistic model behind MKL. Based on this model, we propose learning\nalgorithms for the kernel weights through the maximization of marginal\nlikelihood. We show through numerical experiments that $\\ell_2$-norm MKL and\nElastic-net MKL achieve comparable accuracy to uniform kernel combination.\nAlthough uniform kernel combination might be preferable from its simplicity,\n$\\ell_2$-norm MKL and Elastic-net MKL can learn the usefulness of the\ninformation sources represented as kernels. In particular, Elastic-net MKL\nachieves sparsity in the kernel weights.\n", "versions": [{"version": "v1", "created": "Sat, 13 Nov 2010 02:40:14 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 08:19:07 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1011.3168", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari", "title": "Online Learning: Beyond Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learnability of a wide class of problems, extending the\nresults of (Rakhlin, Sridharan, Tewari, 2010) to general notions of performance\nmeasure well beyond external regret. Our framework simultaneously captures such\nwell-known notions as internal and general Phi-regret, learning with\nnon-additive global cost functions, Blackwell's approachability, calibration of\nforecasters, adaptive regret, and more. We show that learnability in all these\nsituations is due to control of the same three quantities: a martingale\nconvergence term, a term describing the ability to perform well if future is\nknown, and a generalization of sequential Rademacher complexity, studied in\n(Rakhlin, Sridharan, Tewari, 2010). Since we directly study complexity of the\nproblem instead of focusing on efficient algorithms, we are able to improve and\nextend many known results which have been previously derived via an algorithmic\nconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Nov 2010 00:17:02 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 15:45:21 GMT"}], "update_date": "2011-03-25", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1011.3494", "submitter": "Jason Johnson Dr.", "authors": "Jason K. Johnson, Praneeth Netrapalli and Michael Chertkov", "title": "Learning Planar Ising Models", "comments": "11 pages, 4 figures, Submitted to 14th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2011)", "journal-ref": null, "doi": null, "report-no": "LANL LA-UR 10-07656", "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference and learning of graphical models are both well-studied problems in\nstatistics and machine learning that have found many applications in science\nand engineering. However, exact inference is intractable in general graphical\nmodels, which suggests the problem of seeking the best approximation to a\ncollection of random variables within some tractable family of graphical\nmodels. In this paper, we focus our attention on the class of planar Ising\nmodels, for which inference is tractable using techniques of statistical\nphysics [Kac and Ward; Kasteleyn]. Based on these techniques and recent methods\nfor planarity testing and planar embedding [Chrobak and Payne], we propose a\nsimple greedy algorithm for learning the best planar Ising model to approximate\nan arbitrary collection of binary random variables (possibly from sample data).\nGiven the set of all pairwise correlations among variables, we select a planar\ngraph and optimal planar Ising model defined on this graph to best approximate\nthat set of correlations. We demonstrate our method in some simulations and for\nthe application of modeling senate voting records.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 20:27:42 GMT"}], "update_date": "2010-11-16", "authors_parsed": [["Johnson", "Jason K.", ""], ["Netrapalli", "Praneeth", ""], ["Chertkov", "Michael", ""]]}, {"id": "1011.3728", "submitter": "Curzio Basso", "authors": "Curzio Basso and Matteo Santoro and Alessandro Verri and Silvia Villa", "title": "PADDLE: Proximal Algorithm for Dual Dictionaries LEarning", "comments": null, "journal-ref": null, "doi": null, "report-no": "DISI-TR-2010-06", "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable research efforts have been devoted to the design of\nmethods to learn from data overcomplete dictionaries for sparse coding.\nHowever, learned dictionaries require the solution of an optimization problem\nfor coding new data. In order to overcome this drawback, we propose an\nalgorithm aimed at learning both a dictionary and its dual: a linear mapping\ndirectly performing the coding. By leveraging on proximal methods, our\nalgorithm jointly minimizes the reconstruction error of the dictionary and the\ncoding error of its dual; the sparsity of the representation is induced by an\n$\\ell_1$-based penalty on its coefficients. The results obtained on synthetic\ndata and real images show that the algorithm is capable of recovering the\nexpected dictionaries. Furthermore, on a benchmark dataset, we show that the\nimage features obtained from the dual matrix yield state-of-the-art\nclassification performance while being much less computational intensive.\n", "versions": [{"version": "v1", "created": "Tue, 16 Nov 2010 15:31:25 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["Basso", "Curzio", ""], ["Santoro", "Matteo", ""], ["Verri", "Alessandro", ""], ["Villa", "Silvia", ""]]}, {"id": "1011.3805", "submitter": "Rodrigo Labouriau", "authors": "Gabriel C. G. de Abreu and Rodrigo Labouriau", "title": "Characterization of differentially expressed genes using\n  high-dimensional co-expression networks", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to characterize differentially expressed genes in\nterms of their position in a high-dimensional co-expression network. The set-up\nof Gaussian graphical models is used to construct representations of the\nco-expression network in such a way that redundancy and the propagation of\nspurious information along the network are avoided. The proposed inference\nprocedure is based on the minimization of the Bayesian Information Criterion\n(BIC) in the class of decomposable graphical models. This class of models can\nbe used to represent complex relationships and has suitable properties that\nallow to make effective inference in problems with high degree of complexity\n(e.g. several thousands of genes) and small number of observations (e.g.\n10-100) as typically occurs in high throughput gene expression studies. Taking\nadvantage of the internal structure of decomposable graphical models, we\nconstruct a compact representation of the co-expression network that allows to\nidentify the regions with high concentration of differentially expressed genes.\nIt is argued that differentially expressed genes located in highly\ninterconnected regions of the co-expression network are less informative than\ndifferentially expressed genes located in less interconnected regions. Based on\nthat idea, a measure of uncertainty that resembles the notion of relative\nentropy is proposed. Our methods are illustrated with three publically\navailable data sets on microarray experiments (the larger involving more than\n50,000 genes and 64 patients) and a short simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Nov 2010 13:37:45 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["de Abreu", "Gabriel C. G.", ""], ["Labouriau", "Rodrigo", ""]]}, {"id": "1011.4058", "submitter": "Kilian Koepsell", "authors": "Charles F. Cadieu and Kilian Koepsell", "title": "Modeling Image Structure with Factorized Phase-Coupled Boltzmann\n  Machines", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a model for capturing the statistical structure of local\namplitude and local spatial phase in natural images. The model is based on a\nrecently developed, factorized third-order Boltzmann machine that was shown to\nbe effective at capturing higher-order structure in images by modeling\ndependencies among squared filter outputs (Ranzato and Hinton, 2010). Here, we\nextend this model to $L_p$-spherically symmetric subspaces. In order to model\nlocal amplitude and phase structure in images, we focus on the case of two\ndimensional subspaces, and the $L_2$-norm. When trained on natural images the\nmodel learns subspaces resembling quadrature-pair Gabor filters. We then\nintroduce an additional set of hidden units that model the dependencies among\nsubspace phases. These hidden units form a combinatorial mixture of phase\ncoupling distributions, concentrated in the sum and difference of phase pairs.\nWhen adapted to natural images, these distributions capture local spatial phase\nstructure in natural images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Nov 2010 20:55:02 GMT"}], "update_date": "2010-11-18", "authors_parsed": [["Cadieu", "Charles F.", ""], ["Koepsell", "Kilian", ""]]}, {"id": "1011.4071", "submitter": "Jure Leskovec", "authors": "L. Backstrom, J. Leskovec", "title": "Supervised Random Walks: Predicting and Recommending Links in Social\n  Networks", "comments": null, "journal-ref": "Proceedings of the Fourth ACM International Conference on Web\n  Search and Data Mining (WSDM '11), February, 2011", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the occurrence of links is a fundamental problem in networks. In\nthe link prediction problem we are given a snapshot of a network and would like\nto infer which interactions among existing members are likely to occur in the\nnear future or which existing interactions are we missing. Although this\nproblem has been extensively studied, the challenge of how to effectively\ncombine the information from the network structure with rich node and edge\nattribute data remains largely open.\n  We develop an algorithm based on Supervised Random Walks that naturally\ncombines the information from the network structure with node and edge level\nattributes. We achieve this by using these attributes to guide a random walk on\nthe graph. We formulate a supervised learning task where the goal is to learn a\nfunction that assigns strengths to edges in the network such that a random\nwalker is more likely to visit the nodes to which new links will be created in\nthe future. We develop an efficient training algorithm to directly learn the\nedge strength estimation function.\n  Our experiments on the Facebook social graph and large collaboration networks\nshow that our approach outperforms state-of-the-art unsupervised approaches as\nwell as approaches that are based on feature extraction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Nov 2010 21:01:46 GMT"}], "update_date": "2010-11-19", "authors_parsed": [["Backstrom", "L.", ""], ["Leskovec", "J.", ""]]}, {"id": "1011.4088", "submitter": "Charles Sutton", "authors": "Charles Sutton and Andrew McCallum", "title": "An Introduction to Conditional Random Fields", "comments": "90 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.\n", "versions": [{"version": "v1", "created": "Wed, 17 Nov 2010 22:14:50 GMT"}], "update_date": "2010-11-19", "authors_parsed": [["Sutton", "Charles", ""], ["McCallum", "Andrew", ""]]}, {"id": "1011.4339", "submitter": "Srikanth Jagabathula", "authors": "Vivek F. Farias and Srikanth Jagabathula and Devavrat Shah", "title": "Sparse Choice Models", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice models, which capture popular preferences over objects of interest,\nplay a key role in making decisions whose eventual outcome is impacted by human\nchoice behavior. In most scenarios, the choice model, which can effectively be\nviewed as a distribution over permutations, must be learned from observed data.\nThe observed data, in turn, may frequently be viewed as (partial, noisy)\ninformation about marginals of this distribution over permutations. As such,\nthe search for an appropriate choice model boils down to learning a\ndistribution over permutations that is (near-)consistent with observed\ninformation about this distribution.\n  In this work, we pursue a non-parametric approach which seeks to learn a\nchoice model (i.e. a distribution over permutations) with {\\em sparsest}\npossible support, and consistent with observed data. We assume that the data\nobserved consists of noisy information pertaining to the marginals of the\nchoice model we seek to learn. We establish that {\\em any} choice model admits\na `very' sparse approximation in the sense that there exists a choice model\nwhose support is small relative to the dimension of the observed data and whose\nmarginals approximately agree with the observed marginal information. We\nfurther show that under, what we dub, `signature' conditions, such a sparse\napproximation can be found in a computationally efficiently fashion relative to\na brute force approach. An empirical study using the American Psychological\nAssociation election data-set suggests that our approach manages to unearth\nuseful structural properties of the underlying choice model using the sparse\napproximation found. Our results further suggest that the signature condition\nis a potential alternative to the recently popularized Restricted Null Space\ncondition for efficient recovery of sparse models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 03:51:01 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2011 07:05:07 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Farias", "Vivek F.", ""], ["Jagabathula", "Srikanth", ""], ["Shah", "Devavrat", ""]]}, {"id": "1011.4644", "submitter": "Patrick J. Wolfe", "authors": "David S. Choi, Patrick J. Wolfe, Edoardo M. Airoldi", "title": "Stochastic blockmodels with growing number of classes", "comments": "12 pages, 3 figures; revised version", "journal-ref": "Biometrika, 99:273--284, 2012", "doi": "10.1093/biomet/asr053", "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present asymptotic and finite-sample results on the use of stochastic\nblockmodels for the analysis of network data. We show that the fraction of\nmisclassified network nodes converges in probability to zero under maximum\nlikelihood fitting when the number of classes is allowed to grow as the root of\nthe network size and the average network degree grows at least\npoly-logarithmically in this size. We also establish finite-sample confidence\nbounds on maximum-likelihood blockmodel parameter estimates from data\ncomprising independent Bernoulli random variates; these results hold uniformly\nover class assignment. We provide simulations verifying the conditions\nsufficient for our results, and conclude by fitting a logit parameterization of\na stochastic blockmodel with covariates to a network data example comprising a\ncollection of Facebook profiles, resulting in block estimates that reveal\nresidual structure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Nov 2010 07:44:03 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2011 19:29:57 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Choi", "David S.", ""], ["Wolfe", "Patrick J.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1011.4832", "submitter": "Minh-Ngoc Tran", "authors": "David J. Nott, Minh-Ngoc Tran, Chenlei Leng", "title": "Variational approximation for heteroscedastic linear models and matching\n  pursuit algorithms", "comments": "35 pages, 3 figures, 4 tables", "journal-ref": null, "doi": "10.1007/s11222-011-9243-2", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modern statistical applications involving large data sets have focused\nattention on statistical methodologies which are both efficient computationally\nand able to deal with the screening of large numbers of different candidate\nmodels. Here we consider computationally efficient variational Bayes approaches\nto inference in high-dimensional heteroscedastic linear regression, where both\nthe mean and variance are described in terms of linear functions of the\npredictors and where the number of predictors can be larger than the sample\nsize. We derive a closed form variational lower bound on the log marginal\nlikelihood useful for model selection, and propose a novel fast greedy search\nalgorithm on the model space which makes use of one step optimization updates\nto the variational lower bound in the current model for screening large numbers\nof candidate predictor variables for inclusion/exclusion in a computationally\nthrifty way. We show that the model search strategy we suggest is related to\nwidely used orthogonal matching pursuit algorithms for model search but yields\na framework for potentially extending these algorithms to more complex models.\nThe methodology is applied in simulations and in two real examples involving\nprediction for food constituents using NIR technology and prediction of disease\nprogression in diabetes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 14:55:45 GMT"}, {"version": "v2", "created": "Wed, 24 Nov 2010 05:52:55 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2011 15:46:40 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Nott", "David J.", ""], ["Tran", "Minh-Ngoc", ""], ["Leng", "Chenlei", ""]]}, {"id": "1011.5053", "submitter": "Sivan Sabato", "authors": "Sivan Sabato, Nathan Srebro, Naftali Tishby", "title": "Tight Sample Complexity of Large-Margin Learning", "comments": "Appearing in Neural Information Processing Systems (NIPS) 2010; This\n  is the full version, including appendix with proofs; Also with some\n  corrections", "journal-ref": "Advances in Neural Information Processing Systems 23 (NIPS),\n  2038-2046, 2010", "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a tight distribution-specific characterization of the sample\ncomplexity of large-margin classification with L_2 regularization: We introduce\nthe \\gamma-adapted-dimension, which is a simple function of the spectrum of a\ndistribution's covariance matrix, and show distribution-specific upper and\nlower bounds on the sample complexity, both governed by the\n\\gamma-adapted-dimension of the source distribution. We conclude that this new\nquantity tightly characterizes the true sample complexity of large-margin\nclassification. The bounds hold for a rich family of sub-Gaussian\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 10:44:21 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 16:40:03 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sabato", "Sivan", ""], ["Srebro", "Nathan", ""], ["Tishby", "Naftali", ""]]}, {"id": "1011.5096", "submitter": "Florian Markowetz", "authors": "Roland F. Schwarz, William Fletcher, Frank F\\\"orster, Benjamin Merget,\n  Matthias Wolf, J\\\"org Schultz, Florian Markowetz", "title": "Evolutionary distances in the twilight zone -- a rational kernel\n  approach", "comments": "to appear in PLoS ONE", "journal-ref": "PLoS One. 2010 Dec 31;5(12):e15788", "doi": "10.1371/journal.pone.0015788", "report-no": null, "categories": "q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic tree reconstruction is traditionally based on multiple sequence\nalignments (MSAs) and heavily depends on the validity of this information\nbottleneck. With increasing sequence divergence, the quality of MSAs decays\nquickly. Alignment-free methods, on the other hand, are based on abstract\nstring comparisons and avoid potential alignment problems. However, in general\nthey are not biologically motivated and ignore our knowledge about the\nevolution of sequences. Thus, it is still a major open question how to define\nan evolutionary distance metric between divergent sequences that makes use of\nindel information and known substitution models without the need for a multiple\nalignment. Here we propose a new evolutionary distance metric to close this\ngap. It uses finite-state transducers to create a biologically motivated\nsimilarity score which models substitutions and indels, and does not depend on\na multiple sequence alignment. The sequence similarity score is defined in\nanalogy to pairwise alignments and additionally has the positive semi-definite\nproperty. We describe its derivation and show in simulation studies and\nreal-world examples that it is more accurate in reconstructing phylogenies than\ncompeting methods. The result is a new and accurate way of determining\nevolutionary distances in and beyond the twilight zone of sequence alignments\nthat is suitable for large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 13:40:56 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Schwarz", "Roland F.", ""], ["Fletcher", "William", ""], ["F\u00f6rster", "Frank", ""], ["Merget", "Benjamin", ""], ["Wolf", "Matthias", ""], ["Schultz", "J\u00f6rg", ""], ["Markowetz", "Florian", ""]]}, {"id": "1011.5133", "submitter": "Matthieu Cornec", "authors": "Matthieu Cornec", "title": "Concentration inequalities of the cross-validation estimate for stable\n  predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for stable predictors in\nthe context of risk assessment. The notion of stability has been first\nintroduced by \\cite{DEWA79} and extended by \\cite{KEA95}, \\cite{BE01} and\n\\cite{KUNIY02} to characterize class of predictors with infinite VC dimension.\nIn particular, this covers $k$-nearest neighbors rules, bayesian algorithm\n(\\cite{KEA95}), boosting,... General loss functions and class of predictors are\nconsidered. We use the formalism introduced by \\cite{DUD03} to cover a large\nvariety of cross-validation procedures including leave-one-out\ncross-validation, $k$-fold cross-validation, hold-out cross-validation (or\nsplit sample), and the leave-$\\upsilon$-out cross-validation.\n  In particular, we give a simple rule on how to choose the cross-validation,\ndepending on the stability of the class of predictors. In the special case of\nuniform stability, an interesting consequence is that the number of elements in\nthe test set is not required to grow to infinity for the consistency of the\ncross-validation procedure. In this special case, the particular interest of\nleave-one-out cross-validation is emphasized.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 15:31:57 GMT"}], "update_date": "2010-11-24", "authors_parsed": [["Cornec", "Matthieu", ""]]}, {"id": "1011.5142", "submitter": "Matthieu Cornec", "authors": "Matthieu CORNEC", "title": "Estimating Subagging by cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive concentration inequalities for the\ncross-validation estimate of the generalization error for subagged estimators,\nboth for classification and regressor. General loss functions and class of\npredictors with both finite and infinite VC-dimension are considered. We\nslightly generalize the formalism introduced by \\cite{DUD03} to cover a large\nvariety of cross-validation procedures including leave-one-out\ncross-validation, $k$-fold cross-validation, hold-out cross-validation (or\nsplit sample), and the leave-$\\upsilon$-out cross-validation.\n  \\bigskip\n  \\noindent An interesting consequence is that the probability upper bound is\nbounded by the minimum of a Hoeffding-type bound and a Vapnik-type bounds, and\nthus is smaller than 1 even for small learning set. Finally, we give a simple\nrule on how to subbag the predictor. \\bigskip\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 16:12:33 GMT"}], "update_date": "2010-11-24", "authors_parsed": [["CORNEC", "Matthieu", ""]]}, {"id": "1011.5270", "submitter": "Facundo Memoli", "authors": "Gunnar Carlsson and Facundo Memoli", "title": "Classifying Clustering Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering schemes are defined by optimizing an objective function\ndefined on the partitions of the underlying set of a finite metric space. In\nthis paper, we construct a framework for studying what happens when we instead\nimpose various structural conditions on the clustering schemes, under the\ngeneral heading of functoriality. Functoriality refers to the idea that one\nshould be able to compare the results of clustering algorithms as one varies\nthe data set, for example by adding points or by applying functions to it. We\nshow that within this framework, one can prove a theorems analogous to one of\nJ. Kleinberg, in which for example one obtains an existence and uniqueness\ntheorem instead of a non-existence result.\n  We obtain a full classification of all clustering schemes satisfying a\ncondition we refer to as excisiveness. The classification can be changed by\nvarying the notion of maps of finite metric spaces. The conditions occur\nnaturally when one considers clustering as the statistical version of the\ngeometric notion of connected components. By varying the degree of\nfunctoriality that one requires from the schemes it is possible to construct\nricher families of clustering schemes that exhibit sensitivity to density.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 01:51:00 GMT"}, {"version": "v2", "created": "Mon, 29 Nov 2010 22:11:55 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Carlsson", "Gunnar", ""], ["Memoli", "Facundo", ""]]}, {"id": "1011.5395", "submitter": "Daniel Vainsencher", "authors": "Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein", "title": "The Sample Complexity of Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.specom.2013.01.005", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large set of signals can sometimes be described sparsely using a\ndictionary, that is, every element can be represented as a linear combination\nof few elements from the dictionary. Algorithms for various signal processing\napplications, including classification, denoising and signal separation, learn\na dictionary from a set of signals to be represented. Can we expect that the\nrepresentation found by such a dictionary for a previously unseen example from\nthe same source will have L_2 error of the same magnitude as those for the\ngiven examples? We assume signals are generated from a fixed distribution, and\nstudy this questions from a statistical learning theory perspective.\n  We develop generalization bounds on the quality of the learned dictionary for\ntwo types of constraints on the coefficient selection, as measured by the\nexpected L_2 error in representation when the dictionary is used. For the case\nof l_1 regularized coefficient selection we provide a generalization bound of\nthe order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the\nnumber of elements in the dictionary, lambda is a bound on the l_1 norm of the\ncoefficient vector and m is the number of samples, which complements existing\nresults. For the case of representing a new signal as a combination of at most\nk dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))\nunder an assumption on the level of orthogonality of the dictionary (low Babel\nfunction). We further show that this assumption holds for most dictionaries in\nhigh dimensions in a strong probabilistic sense. Our results further yield fast\nrates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher\ncomplexity. We provide similar results in a general setting using kernels with\nweak smoothness requirements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 15:18:42 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Vainsencher", "Daniel", ""], ["Mannor", "Shie", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1011.6086", "submitter": "Lucas Theis", "authors": "Lucas Theis, Sebastian Gerwinn, Fabian Sinz and Matthias Bethge", "title": "In All Likelihood, Deep Belief Is Not Enough", "comments": null, "journal-ref": "Journal of Machine Learning Research 12, 3071-3096, 2011", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models of natural stimuli provide an important tool for\nresearchers in the fields of machine learning and computational neuroscience. A\ncanonical way to quantitatively assess and compare the performance of\nstatistical models is given by the likelihood. One class of statistical models\nwhich has recently gained increasing popularity and has been applied to a\nvariety of complex data are deep belief networks. Analyses of these models,\nhowever, have been typically limited to qualitative analyses based on samples\ndue to the computationally intractable nature of the model likelihood.\nMotivated by these circumstances, the present article provides a consistent\nestimator for the likelihood that is both computationally tractable and simple\nto apply in practice. Using this estimator, a deep belief network which has\nbeen suggested for the modeling of natural image patches is quantitatively\ninvestigated and compared to other models of natural image patches. Contrary to\nearlier claims based on qualitative results, the results presented in this\narticle provide evidence that the model under investigation is not a\nparticularly good model for natural images\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 20:54:58 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Theis", "Lucas", ""], ["Gerwinn", "Sebastian", ""], ["Sinz", "Fabian", ""], ["Bethge", "Matthias", ""]]}, {"id": "1011.6095", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng and Xin Tong", "title": "A ROAD to Classification in High Dimensional Space", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high-dimensional classification, it is well known that naively performing\nthe Fisher discriminant rule leads to poor results due to diverging spectra and\nnoise accumulation. Therefore, researchers proposed independence rules to\ncircumvent the diverse spectra, and sparse independence rules to mitigate the\nissue of noise accumulation. However, in biological applications, there are\noften a group of correlated genes responsible for clinical outcomes, and the\nuse of the covariance information can significantly reduce misclassification\nrates. The extent of such error rate reductions is unveiled by comparing the\nmisclassification rates of the Fisher discriminant rule and the independence\nrule. To materialize the gain based on finite samples, a Regularized Optimal\nAffine Discriminant (ROAD) is proposed based on a covariance penalty. ROAD\nselects an increasing number of features as the penalization relaxes. Further\nbenefits can be achieved when a screening method is employed to narrow the\nfeature pool before hitting the ROAD. An efficient Constrained Coordinate\nDescent algorithm (CCD) is also developed to solve the associated optimization\nproblems. Sampling properties of oracle type are established. Simulation\nstudies and real data analysis support our theoretical results and demonstrate\nthe advantages of the new classification procedure under a variety of\ncorrelation structures. A delicate result on continuous piecewise linear\nsolution path for the ROAD optimization problem at the population level\njustifies the linear interpolation of the CCD algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 22:04:28 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2011 17:31:30 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1011.6224", "submitter": "Markward Britsch", "authors": "Markward Britsch (1), Nikolai Gagunashvili (2), Michael Schmelling (1)\n  ((1) Max-Planck-Institut f\\\"ur Kernphysik, (2) University of Akureyri)", "title": "Classifying extremely imbalanced data sets", "comments": null, "journal-ref": "PoS ACAT2010:047,2010", "doi": null, "report-no": null, "categories": "physics.data-an cs.LG hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalanced data sets containing much more background than signal instances\nare very common in particle physics, and will also be characteristic for the\nupcoming analyses of LHC data. Following up the work presented at ACAT 2008, we\nuse the multivariate technique presented there (a rule growing algorithm with\nthe meta-methods bagging and instance weighting) on much more imbalanced data\nsets, especially a selection of D0 decays without the use of particle\nidentification. It turns out that the quality of the result strongly depends on\nthe number of background instances used for training. We discuss methods to\nexploit this in order to improve the results significantly, and how to handle\nand reduce the size of large training sets without loss of result quality in\ngeneral. We will also comment on how to take into account statistical\nfluctuation in receiver operation characteristic curves (ROC) for comparing\nclassifier methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 13:34:02 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Britsch", "Markward", "", "Max-Planck-Institut f\u00fcr Kernphysik"], ["Gagunashvili", "Nikolai", "", "University of Akureyri"], ["Schmelling", "Michael", "", "Max-Planck-Institut f\u00fcr Kernphysik"]]}, {"id": "1011.6256", "submitter": "Karim Lounici", "authors": "Vladimir Koltchinskii, Alexandre B. Tsybakov, Karim Lounici", "title": "Nuclear norm penalization and optimal rates for noisy low rank matrix\n  completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the trace regression model where $n$ entries or linear\ncombinations of entries of an unknown $m_1\\times m_2$ matrix $A_0$ corrupted by\nnoise are observed. We propose a new nuclear norm penalized estimator of $A_0$\nand establish a general sharp oracle inequality for this estimator for\narbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation.\nThen this method is applied to the matrix completion problem. In this case, the\nestimator admits a simple explicit form and we prove that it satisfies oracle\ninequalities with faster rates of convergence than in the previous works. They\nare valid, in particular, in the high-dimensional setting $m_1m_2\\gg n$. We\nshow that the obtained rates are optimal up to logarithmic factors in a minimax\nsense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound on\nthe rate of convergence of our estimator, which coincides with the upper bound\nup to a constant factor. Finally, we show that our procedure provides an exact\nrecovery of the rank of $A_0$ with probability close to 1. We also discuss the\nstatistical learning setting where there is no underlying model determined by\n$A_0$ and the aim is to find the best trace regression model approximating the\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 15:20:14 GMT"}, {"version": "v2", "created": "Fri, 10 Dec 2010 15:17:02 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2011 22:32:52 GMT"}, {"version": "v4", "created": "Wed, 23 Mar 2016 17:27:31 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["Tsybakov", "Alexandre B.", ""], ["Lounici", "Karim", ""]]}, {"id": "1011.6293", "submitter": "David Knowles", "authors": "David Knowles, Zoubin Ghahramani", "title": "Nonparametric Bayesian sparse factor models with application to gene\n  expression modeling", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS435 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1534-1552", "doi": "10.1214/10-AOAS435", "report-no": "IMS-AOAS-AOAS435", "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where\nobserved data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$,\nof a potentially infinite number of hidden factors, $\\mathbf{X}$. The Indian\nBuffet Process (IBP) is used as a prior on $\\mathbf{G}$ to incorporate sparsity\nand to allow the number of latent features to be inferred. The model's utility\nfor modeling gene expression data is investigated using randomly generated data\nsets based on a known sparse connectivity matrix for E. Coli, and on three\nbiological data sets of increasing complexity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 17:06:41 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2011 12:20:24 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Knowles", "David", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1011.6326", "submitter": "Samet Oymak", "authors": "Samet Oymak, Babak Hassibi", "title": "New Null Space Results and Recovery Thresholds for Matrix Rank\n  Minimization", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear norm minimization (NNM) has recently gained significant attention for\nits use in rank minimization problems. Similar to compressed sensing, using\nnull space characterizations, recovery thresholds for NNM have been studied in\n\\cite{arxiv,Recht_Xu_Hassibi}. However simulations show that the thresholds are\nfar from optimal, especially in the low rank region. In this paper we apply the\nrecent analysis of Stojnic for compressed sensing \\cite{mihailo} to the null\nspace conditions of NNM. The resulting thresholds are significantly better and\nin particular our weak threshold appears to match with simulation results.\nFurther our curves suggest for any rank growing linearly with matrix size $n$\nwe need only three times of oversampling (the model complexity) for weak\nrecovery. Similar to \\cite{arxiv} we analyze the conditions for weak, sectional\nand strong thresholds. Additionally a separate analysis is given for special\ncase of positive semidefinite matrices. We conclude by discussing simulation\nresults and future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 18:46:08 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Oymak", "Samet", ""], ["Hassibi", "Babak", ""]]}]