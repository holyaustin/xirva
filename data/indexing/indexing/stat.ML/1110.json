[{"id": "1110.0107", "submitter": "Roland Memisevic", "authors": "Roland Memisevic", "title": "Learning to relate images: Mapping units, complex cells and simultaneous\n  eigenspaces", "comments": "Revised argument in sections 4 and 3.3. Added illustration of\n  subspaces (Figure 13). Added inference Equation (Eq. 17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI nlin.AO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental operation in many vision tasks, including motion understanding,\nstereopsis, visual odometry, or invariant recognition, is establishing\ncorrespondences between images or between images and data from other\nmodalities. We present an analysis of the role that multiplicative interactions\nplay in learning such correspondences, and we show how learning and inferring\nrelationships between images can be viewed as detecting rotations in the\neigenspaces shared among a set of orthogonal matrices. We review a variety of\nrecent multiplicative sparse coding methods in light of this observation. We\nalso review how the squaring operation performed by energy models and by models\nof complex cells can be thought of as a way to implement multiplicative\ninteractions. This suggests that the main utility of including complex cells in\ncomputational models of vision may be that they can encode relations not\ninvariances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2011 15:14:16 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 21:55:29 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Memisevic", "Roland", ""]]}, {"id": "1110.0413", "submitter": "Guillaume Obozinski", "authors": "Guillaume Obozinski (LIENS, INRIA Paris - Rocquencourt), Laurent\n  Jacob, Jean-Philippe Vert (CBIO)", "title": "Group Lasso with Overlaps: the Latent Group Lasso approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a norm for structured sparsity which leads to sparse linear\npredictors whose supports are unions of prede ned overlapping groups of\nvariables. We call the obtained formulation latent group Lasso, since it is\nbased on applying the usual group Lasso penalty on a set of latent variables. A\ndetailed analysis of the norm and its properties is presented and we\ncharacterize conditions under which the set of groups associated with latent\nvariables are correctly identi ed. We motivate and discuss the delicate choice\nof weights associated to each group, and illustrate this approach on simulated\ndata and on the problem of breast cancer prognosis from gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 16:49:45 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jacob", "Laurent", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1110.0641", "submitter": "Vladimir Nikulin", "authors": "Vladimir Nikulin", "title": "Identifying relationships between drugs and medical conditions: winning\n  experience in the Challenge 2 of the OMOP 2010 Cup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in using a longitudinal observational databases\nto detect drug safety signal. In this paper we present a novel method, which we\nused online during the OMOP Cup. We consider homogeneous ensembling, which is\nbased on random re-sampling (known, also, as bagging) as a main innovation\ncompared to the previous publications in the related field. This study is based\non a very large simulated database of the 10 million patients records, which\nwas created by the Observational Medical Outcomes Partnership (OMOP). Compared\nto the traditional classification problem, the given data are unlabelled. The\nobjective of this study is to discover hidden associations between drugs and\nconditions. The main idea of the approach, which we used during the OMOP Cup is\nto compare the numbers of observed and expected patterns. This comparison may\nbe organised in several different ways, and the outcomes (base learners) may be\nquite different as well. It is proposed to construct the final decision\nfunction as an ensemble of the base learners. Our method was recognised\nformally by the Organisers of the OMOP Cup as a top performing method for the\nChallenge N2.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 11:17:04 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Nikulin", "Vladimir", ""]]}, {"id": "1110.1757", "submitter": "Michael Mahoney", "authors": "Patrick O. Perry and Michael W. Mahoney", "title": "Regularized Laplacian Estimation and Fast Eigenvector Approximation", "comments": "13 pages and 3 figures. A more detailed version of a paper appearing\n  in the 2011 NIPS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Mahoney and Orecchia demonstrated that popular diffusion-based\nprocedures to compute a quick \\emph{approximation} to the first nontrivial\neigenvector of a data graph Laplacian \\emph{exactly} solve certain regularized\nSemi-Definite Programs (SDPs). In this paper, we extend that result by\nproviding a statistical interpretation of their approximation procedure. Our\ninterpretation will be analogous to the manner in which $\\ell_2$-regularized or\n$\\ell_1$-regularized $\\ell_2$-regression (often called Ridge regression and\nLasso regression, respectively) can be interpreted in terms of a Gaussian prior\nor a Laplace prior, respectively, on the coefficient vector of the regression\nproblem. Our framework will imply that the solutions to the Mahoney-Orecchia\nregularized SDP can be interpreted as regularized estimates of the\npseudoinverse of the graph Laplacian. Conversely, it will imply that the\nsolution to this regularized estimation problem can be computed very quickly by\nrunning, e.g., the fast diffusion-based PageRank procedure for computing an\napproximation to the first nontrivial eigenvector of the graph Laplacian.\nEmpirical results are also provided to illustrate the manner in which\napproximate eigenvector computation \\emph{implicitly} performs statistical\nregularization, relative to running the corresponding exact algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 18:43:52 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2011 21:31:02 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Perry", "Patrick O.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1110.1769", "submitter": "Jose Bento", "authors": "Jos\\'e Bento, Andrea Montanari", "title": "On the trade-off between complexity and correlation decay in structural\n  learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms often fail when the Markov random field develops\nlong-range correlations. More precisely, this phenomenon appears to be related\nto the Ising model phase transition (although it does not coincide with it).\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 21:24:36 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Montanari", "Andrea", ""]]}, {"id": "1110.1773", "submitter": "Suvrit Sra", "authors": "Suvrit Sra", "title": "Positive definite matrices and the S-divergence", "comments": "24 pages with several new results; a fraction of this paper also\n  appeared at the Neural Information Processing Systems (NIPS) Conference, Dec.\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive definite matrices abound in a dazzling variety of applications. This\nubiquity can be in part attributed to their rich geometric structure: positive\ndefinite matrices form a self-dual convex cone whose strict interior is a\nRiemannian manifold. The manifold view is endowed with a \"natural\" distance\nfunction while the conic view is not. Nevertheless, drawing motivation from the\nconic view, we introduce the S-Divergence as a \"natural\" distance-like function\non the open cone of positive definite matrices. We motivate the S-divergence\nvia a sequence of results that connect it to the Riemannian distance. In\nparticular, we show that (a) this divergence is the square of a distance; and\n(b) that it has several geometric properties similar to those of the Riemannian\ndistance, though without being computationally as demanding. The S-divergence\nis even more intriguing: although nonconvex, we can still compute matrix means\nand medians using it to global optimality. We complement our results with some\nnumerical experiments illustrating our theorems and our optimization algorithm\nfor computing matrix medians.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 22:16:36 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2012 00:14:04 GMT"}, {"version": "v3", "created": "Sun, 8 Dec 2013 07:01:20 GMT"}, {"version": "v4", "created": "Fri, 27 Dec 2013 23:38:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Sra", "Suvrit", ""]]}, {"id": "1110.1880", "submitter": "Konstantin Zuev M", "authors": "James L. Beck and Konstantin M. Zuev", "title": "Asymptotically Independent Markov Sampling: a new MCMC scheme for\n  Bayesian Inference", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian statistics, many problems can be expressed as the evaluation of\nthe expectation of a quantity of interest with respect to the posterior\ndistribution. Standard Monte Carlo method is often not applicable because the\nencountered posterior distributions cannot be sampled directly. In this case,\nthe most popular strategies are the importance sampling method, Markov chain\nMonte Carlo, and annealing. In this paper, we introduce a new scheme for\nBayesian inference, called Asymptotically Independent Markov Sampling (AIMS),\nwhich is based on the above methods. We derive important ergodic properties of\nAIMS. In particular, it is shown that, under certain conditions, the AIMS\nalgorithm produces a uniformly ergodic Markov chain. The choice of the free\nparameters of the algorithm is discussed and recommendations are provided for\nthis choice, both theoretically and heuristically based. The efficiency of AIMS\nis demonstrated with three numerical examples, which include both multi-modal\nand higher-dimensional target posterior distributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 20:00:33 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Beck", "James L.", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "1110.2058", "submitter": "Eduardo Mendes", "authors": "Eduardo F. Mendes and Wenxin Jiang", "title": "Convergence Rates for Mixture-of-Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixtures-of-experts (ME) model, where a number of submodels (experts) are\ncombined, there have been two longstanding problems: (i) how many experts\nshould be chosen, given the size of the training data? (ii) given the total\nnumber of parameters, is it better to use a few very complex experts, or is it\nbetter to combine many simple experts? In this paper, we try to provide some\ninsights to these problems through a theoretic study on a ME structure where\n$m$ experts are mixed, with each expert being related to a polynomial\nregression model of order $k$. We study the convergence rate of the maximum\nlikelihood estimator (MLE), in terms of how fast the Kullback-Leibler\ndivergence of the estimated density converges to the true density, when the\nsample size $n$ increases. The convergence rate is found to be dependent on\nboth $m$ and $k$, and certain choices of $m$ and $k$ are found to produce\noptimal convergence rates. Therefore, these results shed light on the two\naforementioned important problems: on how to choose $m$, and on how $m$ and $k$\nshould be compromised, for achieving good convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 14:43:02 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 11:39:30 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Mendes", "Eduardo F.", ""], ["Jiang", "Wenxin", ""]]}, {"id": "1110.2227", "submitter": "Raif Rustamov", "authors": "Raif M. Rustamov", "title": "Average Interpolating Wavelets on Point Clouds and Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new wavelet transform suitable for analyzing functions on\npoint clouds and graphs. Our construction is based on a generalization of the\naverage interpolating refinement scheme of Donoho. The most important\ningredient of the original scheme that needs to be altered is the choice of the\ninterpolant. Here, we define the interpolant as the minimizer of a smoothness\nfunctional, namely a generalization of the Laplacian energy, subject to the\naveraging constraints. In the continuous setting, we derive a formula for the\noptimal solution in terms of the poly-harmonic Green's function. The form of\nthis solution is used to motivate our construction in the setting of graphs and\npoint clouds. We highlight the empirical convergence of our refinement scheme\nand the potential applications of the resulting wavelet transform through\nexperiments on a number of data stets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 22:56:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rustamov", "Raif M.", ""]]}, {"id": "1110.2306", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, David Avis", "title": "Ground Metric Learning", "comments": "32 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research, 15, 533-564. 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation distances have been used for more than a decade now in machine\nlearning to compare histograms of features. They have one parameter: the ground\nmetric, which can be any metric between the features themselves. As is the case\nfor all parameterized distances, transportation distances can only prove useful\nin practice when this parameter is carefully chosen. To date, the only option\navailable to practitioners to set the ground metric parameter was to rely on a\npriori knowledge of the features, which limited considerably the scope of\napplication of transportation distances. We propose to lift this limitation and\nconsider instead algorithms that can learn the ground metric using only a\ntraining set of labeled histograms. We call this approach ground metric\nlearning. We formulate the problem of learning the ground metric as the\nminimization of the difference of two polyhedral convex functions over a convex\nset of distance matrices. We follow the presentation of our algorithms with\npromising experimental results on binary classification tasks using GIST\ndescriptors of images taken in the Caltech-256 set.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 09:04:56 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Cuturi", "Marco", ""], ["Avis", "David", ""]]}, {"id": "1110.2436", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ram\\'irez and Guillermo Sapiro (University of Minnesota)", "title": "An MDL framework for sparse coding and dictionary learning", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2187203", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power of sparse signal modeling with learned over-complete dictionaries\nhas been demonstrated in a variety of applications and fields, from signal\nprocessing to statistical inference and machine learning. However, the\nstatistical properties of these models, such as under-fitting or over-fitting\ngiven sets of data, are still not well characterized in the literature. As a\nresult, the success of sparse modeling depends on hand-tuning critical\nparameters for each data and application. This work aims at addressing this by\nproviding a practical and objective characterization of sparse models by means\nof the Minimum Description Length (MDL) principle -- a well established\ninformation-theoretic approach to model selection in statistical inference. The\nresulting framework derives a family of efficient sparse coding and dictionary\nlearning algorithms which, by virtue of the MDL principle, are completely\nparameter free. Furthermore, such framework allows to incorporate additional\nprior information to existing models, such as Markovian dependencies, or to\ndefine completely new problem formulations, including in the matrix analysis\narea, in a natural way. These virtues will be demonstrated with parameter-free\nalgorithms for the classic image denoising and classification problems, and for\nlow-rank matrix recovery in video applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 17:06:37 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Ram\u00edrez", "Ignacio", "", "University of Minnesota"], ["Sapiro", "Guillermo", "", "University of Minnesota"]]}, {"id": "1110.2529", "submitter": "John Duchi", "authors": "Alekh Agarwal and John C. Duchi", "title": "The Generalization Ability of Online Algorithms for Dependent Data", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization performance of online learning algorithms trained\non samples coming from a dependent source of data. We show that the\ngeneralization error of any stable online algorithm concentrates around its\nregret--an easily computable statistic of the online performance of the\nalgorithm--when the underlying ergodic process is $\\beta$- or $\\phi$-mixing. We\nshow high probability error bounds assuming the loss function is convex, and we\nalso establish sharp convergence rates and deviation bounds for strongly convex\nlosses and several linear prediction problems such as linear and logistic\nregression, least-squares SVM, and boosting on dependent data. In addition, our\nresults have straightforward applications to stochastic optimization with\ndependent data, and our analysis requires only martingale convergence\narguments; we need not rely on more powerful statistical tools such as\nempirical process theory.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 23:27:42 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2012 03:12:48 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Agarwal", "Alekh", ""], ["Duchi", "John C.", ""]]}, {"id": "1110.2855", "submitter": "Louise Benoit", "authors": "Louise Beno\\^it (INRIA Paris - Rocquencourt, LIENS, INRIA Paris -\n  Rocquencourt), Julien Mairal (INRIA Paris - Rocquencourt, LIENS), Francis\n  Bach (INRIA Paris - Rocquencourt), Jean Ponce (INRIA Paris - Rocquencourt)", "title": "Sparse Image Representation with Epitomes", "comments": "Computer Vision and Pattern Recognition, Colorado Springs : United\n  States (2011)", "journal-ref": "Computer Vision and Pattern Recognition, Colorado Springs :\n  \\'Etats-Unis (2011)", "doi": "10.1109/CVPR.2011.5995636", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding, which is the decomposition of a vector using only a few basis\nelements, is widely used in machine learning and image processing. The basis\nset, also called dictionary, is learned to adapt to specific data. This\napproach has proven to be very effective in many image processing tasks.\nTraditionally, the dictionary is an unstructured \"flat\" set of atoms. In this\npaper, we study structured dictionaries which are obtained from an epitome, or\na set of epitomes. The epitome is itself a small image, and the atoms are all\nthe patches of a chosen size inside this image. This considerably reduces the\nnumber of parameters to learn and provides sparse image decompositions with\nshiftinvariance properties. We propose a new formulation and an algorithm for\nlearning the structured dictionaries associated with epitomes, and illustrate\ntheir use in image denoising tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 07:35:05 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Beno\u00eet", "Louise", "", "INRIA Paris - Rocquencourt, LIENS, INRIA Paris -\n  Rocquencourt"], ["Mairal", "Julien", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt"], ["Ponce", "Jean", "", "INRIA Paris - Rocquencourt"]]}, {"id": "1110.2899", "submitter": "Ryota Tomioka", "authors": "Toshimitsu Takahashi, Ryota Tomioka, Kenji Yamanishi", "title": "Discovering Emerging Topics in Social Streams via Link Anomaly Detection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of emerging topics are now receiving renewed interest motivated by\nthe rapid growth of social networks. Conventional term-frequency-based\napproaches may not be appropriate in this context, because the information\nexchanged are not only texts but also images, URLs, and videos. We focus on the\nsocial aspects of theses networks. That is, the links between users that are\ngenerated dynamically intentionally or unintentionally through replies,\nmentions, and retweets. We propose a probability model of the mentioning\nbehaviour of a social network user, and propose to detect the emergence of a\nnew topic from the anomaly measured through the model. We combine the proposed\nmention anomaly score with a recently proposed change-point detection technique\nbased on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or\nwith Kleinberg's burst model. Aggregating anomaly scores from hundreds of\nusers, we show that we can detect emerging topics only based on the\nreply/mention relationships in social network posts. We demonstrate our\ntechnique in a number of real data sets we gathered from Twitter. The\nexperiments show that the proposed mention-anomaly-based approaches can detect\nnew topics at least as early as the conventional term-frequency-based approach,\nand sometimes much earlier when the keyword is ill-defined.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:34:21 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Takahashi", "Toshimitsu", ""], ["Tomioka", "Ryota", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "1110.2997", "submitter": "Philip Graff", "authors": "Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony Lasenby", "title": "BAMBI: blind accelerated multimodal Bayesian inference", "comments": "12 pages, 8 tables, 17 figures; accepted by MNRAS; v2 to reflect\n  minor changes in published version", "journal-ref": "MNRAS, Vol. 421, Issue 1, pg. 169-180 (2012)", "doi": "10.1111/j.1365-2966.2011.20288.x", "report-no": null, "categories": "astro-ph.IM astro-ph.CO physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm for rapid Bayesian analysis that\ncombines the benefits of nested sampling and artificial neural networks. The\nblind accelerated multimodal Bayesian inference (BAMBI) algorithm implements\nthe MultiNest package for nested sampling as well as the training of an\nartificial neural network (NN) to learn the likelihood function. In the case of\ncomputationally expensive likelihoods, this allows the substitution of a much\nmore rapid approximation in order to increase significantly the speed of the\nanalysis. We begin by demonstrating, with a few toy examples, the ability of a\nNN to learn complicated likelihood surfaces. BAMBI's ability to decrease\nrunning time for Bayesian inference is then demonstrated in the context of\nestimating cosmological parameters from Wilkinson Microwave Anisotropy Probe\nand other observations. We show that valuable speed increases are achieved in\naddition to obtaining NNs trained on the likelihood functions for the different\nmodel and data combinations. These NNs can then be used for an even faster\nfollow-up analysis using the same likelihood and different priors. This is a\nfully general algorithm that can be applied, without any pre-processing, to\nother problems with computationally expensive likelihood functions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 17:04:59 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 17:04:58 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Graff", "Philip", ""], ["Feroz", "Farhan", ""], ["Hobson", "Michael P.", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1110.3076", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye, Yuanfeng Wang, Yifei Chen, and Xiaohui Xie", "title": "Efficient Latent Variable Graphical Model Selection via Split Bregman\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of covariance matrix estimation in the presence of\nlatent variables. Under suitable conditions, it is possible to learn the\nmarginal covariance matrix of the observed variables via a tractable convex\nprogram, where the concentration matrix of the observed variables is decomposed\ninto a sparse matrix (representing the graphical structure of the observed\nvariables) and a low rank matrix (representing the marginalization effect of\nlatent variables). We present an efficient first-order method based on split\nBregman to solve the convex problem. The algorithm is guaranteed to converge\nunder mild conditions. We show that our algorithm is significantly faster than\nthe state-of-the-art algorithm on both artificial and real-world data. Applying\nthe algorithm to a gene expression data involving thousands of genes, we show\nthat most of the correlation between observed variables can be explained by\nonly a few dozen latent factors.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 21:48:04 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Wang", "Yuanfeng", ""], ["Chen", "Yifei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1110.3204", "submitter": "Arto Klami", "authors": "Seppo Virtanen, Arto Klami, Suleiman A. Khan, Samuel Kaski", "title": "Bayesian Group Factor Analysis", "comments": "9 pages, 5 figures", "journal-ref": "Proceedings of the 15th AISTATS, JMLR W&CP 22: 1269-1277, 2012", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a factor analysis model that summarizes the dependencies between\nobserved variable groups, instead of dependencies between individual variables\nas standard factor analysis does. A group may correspond to one view of the\nsame set of objects, one of many data sets tied by co-occurrence, or a set of\nalternative variables collected from statistics tables to measure one property\nof interest. We show that by assuming group-wise sparse factors, active in a\nsubset of the sets, the variation can be decomposed into factors explaining\nrelationships between the sets and factors explaining away set-specific\nvariation. We formulate the assumptions in a Bayesian model which provides the\nfactors, and apply the model to two data analysis tasks, in neuroimaging and\nchemical systems biology.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 13:26:09 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Virtanen", "Seppo", ""], ["Klami", "Arto", ""], ["Khan", "Suleiman A.", ""], ["Kaski", "Samuel", ""]]}, {"id": "1110.3239", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Cassio P. De Campos", "title": "Improving parameter learning of Bayesian nets from incomplete data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of parameters of a Bayesian network from\nincomplete data. The task is usually tackled by running the\nExpectation-Maximization (EM) algorithm several times in order to obtain a high\nlog-likelihood estimate. We argue that choosing the maximum log-likelihood\nestimate (as well as the maximum penalized log-likelihood and the maximum a\nposteriori estimate) has severe drawbacks, being affected both by overfitting\nand model uncertainty. Two ideas are discussed to overcome these issues: a\nmaximum entropy approach and a Bayesian model averaging approach. Both ideas\ncan be easily applied on top of EM, while the entropy idea can be also\nimplemented in a more sophisticated way, through a dedicated non-linear solver.\nA vast set of experiments shows that these ideas produce significantly better\nestimates and inferences than the traditional and widely used maximum\n(penalized) log-likelihood and maximum a posteriori estimates. In particular,\nif EM is adopted as optimization engine, the model averaging approach is the\nbest performing one; its performance is matched by the entropy approach when\nimplemented using the non-linear solver. The results suggest that the\napplicability of these ideas is immediate (they are easy to implement and to\nintegrate in currently available inference engines) and that they constitute a\nbetter way to learn Bayesian network parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 12:17:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Corani", "Giorgio", ""], ["De Campos", "Cassio P.", ""]]}, {"id": "1110.3556", "submitter": "Florentina Bunea", "authors": "Florentina Bunea, Yiyuan She, Marten H. Wegkamp", "title": "Joint variable and rank selection for parsimonious estimation of\n  high-dimensional matrices", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1039 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 5, 2359-2388", "doi": "10.1214/12-AOS1039", "report-no": "IMS-AOS-AOS1039", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose dimension reduction methods for sparse, high-dimensional\nmultivariate response regression models. Both the number of responses and that\nof the predictors may exceed the sample size. Sometimes viewed as\ncomplementary, predictor selection and rank reduction are the most popular\nstrategies for obtaining lower-dimensional approximations of the parameter\nmatrix in such models. We show in this article that important gains in\nprediction accuracy can be obtained by considering them jointly. We motivate a\nnew class of sparse multivariate regression models, in which the coefficient\nmatrix has low rank and zero rows or can be well approximated by such a matrix.\nNext, we introduce estimators that are based on penalized least squares, with\nnovel penalties that impose simultaneous row and rank restrictions on the\ncoefficient matrix. We prove that these estimators indeed adapt to the unknown\nmatrix sparsity and have fast rates of convergence. We support our theoretical\nresults with an extensive simulation study and two data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 02:11:00 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2012 17:40:09 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2012 23:10:29 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2013 08:42:52 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Bunea", "Florentina", ""], ["She", "Yiyuan", ""], ["Wegkamp", "Marten H.", ""]]}, {"id": "1110.3564", "submitter": "Sewoong Oh", "authors": "David R. Karger and Sewoong Oh and Devavrat Shah", "title": "Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems", "comments": "38 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems, in which numerous tasks are electronically distributed\nto numerous \"information piece-workers\", have emerged as an effective paradigm\nfor human-powered solving of large scale problems in domains such as image\nclassification, data entry, optical character recognition, recommendation, and\nproofreading. Because these low-paid workers can be unreliable, nearly all such\nsystems must devise schemes to increase confidence in their answers, typically\nby assigning each task multiple times and combining the answers in an\nappropriate manner, e.g. majority voting.\n  In this paper, we consider a general model of such crowdsourcing tasks and\npose the problem of minimizing the total price (i.e., number of task\nassignments) that must be paid to achieve a target overall reliability. We give\na new algorithm for deciding which tasks to assign to which workers and for\ninferring correct answers from the workers' answers. We show that our\nalgorithm, inspired by belief propagation and low-rank matrix approximation,\nsignificantly outperforms majority voting and, in fact, is optimal through\ncomparison to an oracle that knows the reliability of every worker. Further, we\ncompare our approach with a more general class of algorithms which can\ndynamically assign tasks. By adaptively deciding which questions to ask to the\nnext arriving worker, one might hope to reduce uncertainty more efficiently. We\nshow that, perhaps surprisingly, the minimum price necessary to achieve a\ntarget reliability scales in the same manner under both adaptive and\nnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under\nboth scenarios. This strongly relies on the fact that workers are fleeting and\ncan not be exploited. Therefore, architecturally, our results suggest that\nbuilding a reliable worker-reputation system is essential to fully harnessing\nthe potential of adaptive designs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 02:52:20 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 18:49:14 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 21:23:24 GMT"}, {"version": "v4", "created": "Tue, 26 Mar 2013 07:28:04 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Karger", "David R.", ""], ["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1110.3592", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Information, learning and falsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are (at least) three approaches to quantifying information. The first,\nalgorithmic information or Kolmogorov complexity, takes events as strings and,\ngiven a universal Turing machine, quantifies the information content of a\nstring as the length of the shortest program producing it. The second, Shannon\ninformation, takes events as belonging to ensembles and quantifies the\ninformation resulting from observing the given event in terms of the number of\nalternate events that have been ruled out. The third, statistical learning\ntheory, has introduced measures of capacity that control (in part) the expected\nrisk of classifiers. These capacities quantify the expectations regarding\nfuture data that learning algorithms embed into classifiers.\n  This note describes a new method of quantifying information, effective\ninformation, that links algorithmic information to Shannon information, and\nalso links both to capacities arising in statistical learning theory. After\nintroducing the measure, we show that it provides a non-universal analog of\nKolmogorov complexity. We then apply it to derive basic capacities in\nstatistical learning theory: empirical VC-entropy and empirical Rademacher\ncomplexity. A nice byproduct of our approach is an interpretation of the\nexplanatory power of a learning algorithm in terms of the number of hypotheses\nit falsifies, counted in two different ways for the two capacities. We also\ndiscuss how effective information relates to information gain, Shannon and\nmutual information.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 07:51:59 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2011 06:56:52 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1110.3741", "submitter": "Kevin Xu", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, and Alfred O. Hero III", "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis", "comments": "Removed an unnecessary line from Algorithm 1", "journal-ref": "Advances in Neural Information Processing Systems 25 (2012)\n  854-862", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. In most anomaly\ndetection algorithms, the dissimilarity between data samples is calculated by a\nsingle criterion, such as Euclidean distance. However, in many cases there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such a case, multiple criteria can be defined, and one can test\nfor anomalies by scalarizing the multiple criteria using a linear combination\nof them. If the importance of the different criteria are not known in advance,\nthe algorithm may need to be executed multiple times with different choices of\nweights in the linear combination. In this paper, we introduce a novel\nnon-parametric multi-criteria anomaly detection method using Pareto depth\nanalysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies\nunder multiple criteria without having to run an algorithm multiple times with\ndifferent choices of weights. The proposed PDA approach scales linearly in the\nnumber of criteria and is provably better than linear combinations of the\ncriteria.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 17:48:22 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2013 17:18:42 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1110.3907", "submitter": "Peng Sun", "authors": "Peng Sun, Mark D. Reid, Jie Zhou", "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem", "comments": "8-pages camera ready version for ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improvement to model learning when using multi-class\nLogitBoost for classification. Motivated by the statistical view, LogitBoost\ncan be seen as additive tree regression. Two important factors in this setting\nare: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the\ndense Hessian matrices that arise when computing tree node split gain and node\nvalue fittings. In general, this setting is too complicated for a tractable\nmodel learning algorithm. However, too aggressive simplification of the setting\nmay lead to degraded performance. For example, the original LogitBoost is\noutperformed by ABC-LogitBoost due to the latter's more careful treatment of\nthe above two factors.\n  In this paper we propose techniques to address the two main difficulties of\nthe LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is\nvector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block\ncoordinate descent that exploits the dense Hessian when computing tree split\ngain and node values. Higher classification accuracy and faster convergence\nrates are observed for a range of public data sets when compared to both the\noriginal and the ABC-LogitBoost implementations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 08:26:59 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 19:43:06 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2012 07:14:17 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Sun", "Peng", ""], ["Reid", "Mark D.", ""], ["Zhou", "Jie", ""]]}, {"id": "1110.4088", "submitter": "Harry Crane", "authors": "Harry Crane", "title": "Infinitely exchangeable random graphs generated from a Poisson point\n  process on monotone sets and applications to cluster analysis for networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an infinitely exchangeable process on the set $\\cate$ of subsets\nof the power set of the natural numbers $\\mathbb{N}$ via a Poisson point\nprocess with mean measure $\\Lambda$ on the power set of $\\mathbb{N}$. Each\n$E\\in\\cate$ has a least monotone cover in $\\catf$, the collection of monotone\nsubsets of $\\cate$, and every monotone subset maps to an undirected graph\n$G\\in\\catg$, the space of undirected graphs with vertex set $\\mathbb{N}$. We\nshow a natural mapping $\\cate\\rightarrow\\catf\\rightarrow\\catg$ which induces an\ninfinitely exchangeable measure on the projective system $\\catg^{\\rest}$ of\ngraphs $\\catg$ under permutation and restriction mappings given an infinitely\nexchangeable family of measures on the projective system $\\cate^{\\rest}$ of\nsubsets with permutation and restriction maps. We show potential connections of\nthis process to applications in cluster analysis, machine learning,\nclassification and Bayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 18:57:56 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2011 14:07:45 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Crane", "Harry", ""]]}, {"id": "1110.4168", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi", "title": "Stable mixed graphs", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ454 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5B, 2330-2358", "doi": "10.3150/12-BEJ454", "report-no": "IMS-BEJ-BEJ454", "categories": "stat.OT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study classes of graphs with three types of edges that\ncapture the modified independence structure of a directed acyclic graph (DAG)\nafter marginalisation over unobserved variables and conditioning on selection\nvariables using the $m$-separation criterion. These include MC, summary, and\nancestral graphs. As a modification of MC graphs, we define the class of\nribbonless graphs (RGs) that permits the use of the $m$-separation criterion.\nRGs contain summary and ancestral graphs as subclasses, and each RG can be\ngenerated by a DAG after marginalisation and conditioning. We derive simple\nalgorithms to generate RGs, from given DAGs or RGs, and also to generate\nsummary and ancestral graphs in a simple way by further extension of the\nRG-generating algorithm. This enables us to develop a parallel theory on these\nthree classes and to study the relationships between them as well as the use of\neach class.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 03:18:24 GMT"}, {"version": "v2", "created": "Mon, 28 May 2012 10:47:31 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2013 08:25:13 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Sadeghi", "Kayvan", ""]]}, {"id": "1110.4198", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Olivier Chapelle, Miroslav Dudik, John Langford", "title": "A Reliable Effective Terascale Linear Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system and a set of techniques for learning linear predictors\nwith convex losses on terascale datasets, with trillions of features, {The\nnumber of features here refers to the number of non-zero entries in the data\nmatrix.} billions of training examples and millions of parameters in an hour\nusing a cluster of 1000 machines. Individually none of the component techniques\nare new, but the careful synthesis required to obtain an efficient\nimplementation is. The result is, up to our knowledge, the most scalable and\nefficient linear learning system reported in the literature (as of 2011 when\nour experiments were conducted). We describe and thoroughly evaluate the\ncomponents of the system, showing the importance of the various design choices.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 07:34:19 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2012 18:31:21 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2013 03:28:17 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Agarwal", "Alekh", ""], ["Chapelle", "Olivier", ""], ["Dudik", "Miroslav", ""], ["Langford", "John", ""]]}, {"id": "1110.4300", "submitter": "Samory Kpotufe", "authors": "Samory Kpotufe", "title": "k-NN Regression Adapts to Local Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many nonparametric regressors were recently shown to converge at rates that\ndepend only on the intrinsic dimension of data. These regressors thus escape\nthe curse of dimension when high-dimensional data has low intrinsic dimension\n(e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic\ndimension. In particular our rates are local to a query x and depend only on\nthe way masses of balls centered at x vary with radius.\n  Furthermore, we show a simple way to choose k = k(x) locally at any x so as\nto nearly achieve the minimax rate at x in terms of the unknown intrinsic\ndimension in the vicinity of x. We also establish that the minimax rate does\nnot depend on a particular choice of metric space or distribution, but rather\nthat this minimax rate holds for any metric space and doubling measure.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 14:49:04 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Kpotufe", "Samory", ""]]}, {"id": "1110.4304", "submitter": "J\\'an Dolinsk\\'y", "authors": "J\\'an Dolinsk\\'y and Kei Hirose and Sadanori Konishi", "title": "Readouts for Echo-state Networks Built using Locally Regularized\n  Orthogonal Forward Regression", "comments": "12 pages, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state network (ESN) is viewed as a temporal non-orthogonal expansion\nwith pseudo-random parameters. Such expansions naturally give rise to\nregressors of various relevance to a teacher output. We illustrate that often\nonly a certain amount of the generated echo-regressors effectively explain the\nvariance of the teacher output and also that sole local regularization is not\nable to provide in-depth information concerning the importance of the generated\nregressors. The importance is therefore determined by a joint calculation of\nthe individual variance contributions and Bayesian relevance using locally\nregularized orthogonal forward regression (LROFR) algorithm. This information\ncan be advantageously used in a variety of ways for an in-depth analysis of an\nESN structure and its state-space parameters in relation to the unknown\ndynamics of the underlying problem. We present locally regularized linear\nreadout built using LROFR. The readout may have a different dimensionality than\nan ESN model itself, and besides improving robustness and accuracy of an ESN it\nrelates the echo-regressors to different features of the training data and may\ndetermine what type of an additional readout is suitable for a task at hand.\nMoreover, as flexibility of the linear readout has limitations and might\nsometimes be insufficient for certain tasks, we also present a radial basis\nfunction (RBF) readout built using LROFR. It is a flexible and parsimonious\nreadout with excellent generalization abilities and is a viable alternative to\nreadouts based on a feed-forward neural network (FFNN) or an RBF net built\nusing relevance vector machine (RVM).\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 14:59:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 14:27:53 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2012 10:06:26 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Dolinsk\u00fd", "J\u00e1n", ""], ["Hirose", "Kei", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1110.4322", "submitter": "Nicol\\`o Cesa-Bianchi", "authors": "Nicol\\`o Cesa-Bianchi and Sham Kakade", "title": "An Optimal Algorithm for Linear Bandits", "comments": "This paper is superseded by S. Bubeck, N. Cesa-Bianchi, and S.M.\n  Kakade, \"Towards minimax policies for online linear optimization with bandit\n  feedback\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first algorithm for online bandit linear optimization whose\nregret after T rounds is of order sqrt{Td ln N} on any finite class X of N\nactions in d dimensions, and of order d*sqrt{T} (up to log factors) when X is\ninfinite. These bounds are not improvable in general. The basic idea utilizes\ntools from convex geometry to construct what is essentially an optimal\nexploration basis. We also present an application to a model of linear bandits\nwith expert advice. Interestingly, these results show that bandit linear\noptimization with expert advice in d dimensions is no more difficult (in terms\nof the achievable regret) than the online d-armed bandit problem with expert\nadvice (where EXP4 is optimal).\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 15:57:27 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 14:30:27 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2012 16:14:39 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Kakade", "Sham", ""]]}, {"id": "1110.4347", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Is the k-NN classifier in high dimensions affected by the curse of\n  dimensionality?", "comments": "24 pages, 6 figures, 1 table, latex2e with Elsevier macros. The\n  Introduction is somewhat expanded, one figure added. To appear in a special\n  issue of Computers & Mathematics with Applications", "journal-ref": "Comput. Math. Appl. 65 (2013), no. 10, 1427-1437", "doi": null, "report-no": "DPA-11341", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing body of evidence suggesting that exact nearest\nneighbour search in high-dimensional spaces is affected by the curse of\ndimensionality at a fundamental level. Does it necessarily mean that the same\nis true for k nearest neighbours based learning algorithms such as the k-NN\nclassifier? We analyse this question at a number of levels and show that the\nanswer is different at each of them. As our first main observation, we show the\nconsistency of a k approximate nearest neighbour classifier. However, the\nperformance of the classifier in very high dimensions is provably unstable. As\nour second main observation, we point out that the existing model for\nstatistical learning is oblivious of dimension of the domain and so every\nlearning problem admits a universally consistent deterministic reduction to the\none-dimensional case by means of a Borel isomorphism.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 18:25:27 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 17:19:59 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2012 01:34:27 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1110.4411", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, David A. Knowles, Zoubin Ghahramani", "title": "Gaussian Process Regression Networks", "comments": "17 pages, 3 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new regression framework, Gaussian process regression networks\n(GPRN), which combines the structural properties of Bayesian neural networks\nwith the non-parametric flexibility of Gaussian processes. This model\naccommodates input dependent signal and noise correlations between multiple\nresponse variables, input dependent length-scales and amplitudes, and\nheavy-tailed predictive distributions. We derive both efficient Markov chain\nMonte Carlo and variational Bayes inference procedures for this model. We apply\nGPRN as a multiple output regression and multivariate volatility model,\ndemonstrating substantially improved performance over eight popular multiple\noutput (multi-task) Gaussian process models and three multivariate volatility\nmodels on benchmark datasets, including a 1000 dimensional gene expression\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 22:18:03 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1110.4531", "submitter": "Franz J. Kir\\'aly", "authors": "Franz Johannes Kir\\'aly, Paul von B\\\"unau, Jan Saputra M\\\"uller,\n  Duncan Blythe, Frank Meinecke, Klaus-Robert M\\\"uller", "title": "Regression for sets of polynomial equations", "comments": "arXiv admin note: substantial text overlap with arXiv:1108.1483", "journal-ref": "Journal of Machine Learning Research Workshop and Conference\n  Proceedings Vol.22: Proceedings on the Fifteenth International Conference on\n  Artificial Intelligence and Statistics, 22:628-637. 2012", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method called ideal regression for approximating an arbitrary\nsystem of polynomial equations by a system of a particular type. Using\ntechniques from approximate computational algebraic geometry, we show how we\ncan solve ideal regression directly without resorting to numerical\noptimization. Ideal regression is useful whenever the solution to a learning\nproblem can be described by a system of polynomial equations. As an example, we\ndemonstrate how to formulate Stationary Subspace Analysis (SSA), a source\nseparation problem, in terms of ideal regression, which also yields a\nconsistent estimator for SSA. We then compare this estimator in simulations\nwith previous optimization-based approaches for SSA.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 13:51:16 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2011 15:12:58 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2012 10:06:54 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2013 16:39:38 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Kir\u00e1ly", "Franz Johannes", ""], ["von B\u00fcnau", "Paul", ""], ["M\u00fcller", "Jan Saputra", ""], ["Blythe", "Duncan", ""], ["Meinecke", "Frank", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1110.4539", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi", "title": "Markov Equivalences for Subclasses of Loopless Mixed Graphs", "comments": "22 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss four problems regarding Markov equivalences for\nsubclasses of loopless mixed graphs. We classify these four problems as finding\nconditions for internal Markov equivalence, which is Markov equivalence within\na subclass, for external Markov equivalence, which is Markov equivalence\nbetween subclasses, for representational Markov equivalence, which is the\npossibility of a graph from a subclass being Markov equivalent to a graph from\nanother subclass, and finding algorithms to generate a graph from a certain\nsubclass that is Markov equivalent to a given graph. We particularly focus on\nthe class of maximal ancestral graphs and its subclasses, namely regression\ngraphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and\npresent novel results for representational Markov equivalence and algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 14:30:15 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Sadeghi", "Kayvan", ""]]}, {"id": "1110.4713", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig, David Stern, Ralf Herbrich and Thore Graepel", "title": "Kernel Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation models discrete data as a mixture of discrete\ndistributions, using Dirichlet beliefs over the mixture weights. We study a\nvariation of this concept, in which the documents' mixture weight beliefs are\nreplaced with squashed Gaussian distributions. This allows documents to be\nassociated with elements of a Hilbert space, admitting kernel topic models\n(KTM), modelling temporal, spatial, hierarchical, social and other structure\nbetween documents. The main challenge is efficient approximate inference on the\nlatent Gaussian. We present an approximate algorithm cast around a Laplace\napproximation in a transformed basis. The KTM can also be interpreted as a type\nof Gaussian process latent variable model, or as a topic model conditional on\ndocument features, uncovering links between earlier work in these areas.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 07:29:36 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Hennig", "Philipp", ""], ["Stern", "David", ""], ["Herbrich", "Ralf", ""], ["Graepel", "Thore", ""]]}, {"id": "1110.5238", "submitter": "Cedric Archambeau", "authors": "Cedric Archambeau and Francis Bach", "title": "Multiple Gaussian Process Models", "comments": "NIPS 2010 Workshop: New Directions in Multiple Kernel Learning;\n  Videolectures: http://videolectures.net/nipsworkshops2010_archambeau_mgp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Gaussian process formulation of the multiple kernel learning\nproblem. The goal is to select the convex combination of kernel matrices that\nbest explains the data and by doing so improve the generalisation on unseen\ndata. Sparsity in the kernel weights is obtained by adopting a hierarchical\nBayesian approach: Gaussian process priors are imposed over the latent\nfunctions and generalised inverse Gaussians on their associated weights. This\nconstruction is equivalent to imposing a product of heavy-tailed process priors\nover function space. A variational inference algorithm is derived for\nregression and binary classification.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 14:01:26 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Archambeau", "Cedric", ""], ["Bach", "Francis", ""]]}, {"id": "1110.5383", "submitter": "Hyokun Yun", "authors": "Hyokun Yun, S. V. N. Vishwanathan", "title": "Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative\n  Attribute Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first sub-quadratic sampling algorithm for the Multiplicative\nAttribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close\nconnection between MAGM and the Kronecker Product Graph Model (KPGM) of\nLeskovec et al. (2010), and show that to sample a graph from a MAGM it suffices\nto sample small number of KPGM graphs and \\emph{quilt} them together. Under a\nrestricted set of technical conditions our algorithm runs in $O((\\log_2(n))^3\n|E|)$ time, where $n$ is the number of nodes and $|E|$ is the number of edges\nin the sampled graph. We demonstrate the scalability of our algorithm via\nextensive empirical evaluation; we can sample a MAGM graph with 8 million nodes\nand 20 billion edges in under 6 hours.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 23:47:21 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 13:54:17 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1110.5454", "submitter": "Samuel Gershman", "authors": "Samuel J. Gershman, Peter I. Frazier, David M. Blei", "title": "Distance Dependent Infinite Latent Feature Models", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent feature models are widely used to decompose data into a small number\nof components. Bayesian nonparametric variants of these models, which use the\nIndian buffet process (IBP) as a prior over latent features, allow the number\nof features to be determined from the data. We present a generalization of the\nIBP, the distance dependent Indian buffet process (dd-IBP), for modeling\nnon-exchangeable data. It relies on distances defined between data points,\nbiasing nearby data to share more features. The choice of distance measure\nallows for many kinds of dependencies, including temporal and spatial. Further,\nthe original IBP is a special case of the dd-IBP. In this paper, we develop the\ndd-IBP and theoretically characterize its feature-sharing properties. We derive\na Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP\nprior and study its performance on several non-exchangeable data sets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 10:11:44 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2012 18:19:22 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Gershman", "Samuel J.", ""], ["Frazier", "Peter I.", ""], ["Blei", "David M.", ""]]}, {"id": "1110.5508", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder, Deepak K. Agarwal", "title": "A Flexible, Scalable and Efficient Algorithmic Framework for Primal\n  Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable, efficient and statistically motivated computational\nframework for Graphical Lasso (Friedman et al., 2007b) - a covariance\nregularization framework that has received significant attention in the\nstatistics community over the past few years. Existing algorithms have trouble\nin scaling to dimensions larger than a thousand. Our proposal significantly\nenhances the state-of-the-art for such moderate sized problems and gracefully\nscales to larger problems where other algorithms become practically infeasible.\nThis requires a few key new ideas. We operate on the primal problem and use a\nsubtle variation of block-coordinate-methods which drastically reduces the\ncomputational complexity by orders of magnitude. We provide rigorous\ntheoretical guarantees on the convergence and complexity of our algorithm and\ndemonstrate the effectiveness of our proposal via experiments. We believe that\nour framework extends the applicability of Graphical Lasso to large-scale\nmodern applications like bioinformatics, collaborative filtering and social\nnetworks, among others.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 14:07:40 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Mazumder", "Rahul", ""], ["Agarwal", "Deepak K.", ""]]}, {"id": "1110.5847", "submitter": "Joseph Wang", "authors": "Joseph Wang, Venkatesh Saligrama, David A. Casta\\~n\\'on", "title": "Structural Similarity and Distance in Learning", "comments": "Based on work presented at Allerton 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method of introducing structure into existing machine\nlearning techniques by developing structure-based similarity and distance\nmeasures. To learn structural information, low-dimensional structure of the\ndata is captured by solving a non-linear, low-rank representation problem. We\nshow that this low-rank representation can be kernelized, has a closed-form\nsolution, allows for separation of independent manifolds, and is robust to\nnoise. From this representation, similarity between observations based on\nnon-linear structure is computed and can be incorporated into existing feature\ntransformations, dimensionality reduction techniques, and machine learning\nmethods. Experimental results on both synthetic and real data sets show\nperformance improvements for clustering, and anomaly detection through the use\nof structural similarity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 17:21:18 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""], ["Casta\u00f1\u00f3n", "David A.", ""]]}, {"id": "1110.6084", "submitter": "Vianney Perchet", "authors": "Vianney Perchet, Philippe Rigollet", "title": "The multi-armed bandit problem with covariates", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1101 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 693-721", "doi": "10.1214/13-AOS1101", "report-no": "IMS-AOS-AOS1101", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-armed bandit problem in a setting where each arm produces\na noisy reward realization which depends on an observable random covariate. As\nopposed to the traditional static multi-armed bandit problem, this setting\nallows for dynamically changing rewards that better describe applications where\nside information is available. We adopt a nonparametric model where the\nexpected rewards are smooth functions of the covariate and where the hardness\nof the problem is captured by a margin parameter. To maximize the expected\ncumulative reward, we introduce a policy called Adaptively Binned Successive\nElimination (abse) that adaptively decomposes the global problem into suitably\n\"localized\" static bandit problems. This policy constructs an adaptive\npartition using a variant of the Successive Elimination (se) policy. Our\nresults include sharper regret bounds for the se policy in a static bandit\nproblem and minimax optimal regret bounds for the abse policy in the dynamic\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 14:09:12 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2012 21:38:07 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 09:35:28 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1110.6228", "submitter": "Kirill Vaninsky", "authors": "A. Lykov, S.Muzychka and K. Vaninsky", "title": "The AdaBoost Flow", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamical system which we call the AdaBoost flow. The flow is\ndefined by a system of ODEs with control. We show that three algorithms of the\nAdaBoost family (i) the AdaBoost algorithm of Schapire and Freund (ii) the\narc-gv algorithm of Breiman (iii) the confidence rated prediction of Schapire\nand Singer can be can be embedded in the AdaBoost flow.\n  The nontrivial part of the AdaBoost flow equations coincides with the\nequations of dynamics of nonperiodic Toda system written in terms of spectral\nvariables. We provide a novel invariant geometrical description of the AdaBoost\nalgorithm as a gradient flow on a foliation defined by level sets of the\npotential function.\n  We propose a new approach for constructing boosting algorithms as a\ncontinuous time gradient flow on measures defined by various metrics and\npotential functions. Finally we explain similarity of the AdaBoost algorithm\nwith the Perelman's construction for the Ricci flow.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 01:15:41 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2012 03:57:42 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2012 22:50:43 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2013 01:12:08 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Lykov", "A.", ""], ["Muzychka", "S.", ""], ["Vaninsky", "K.", ""]]}, {"id": "1110.6317", "submitter": "Yun Shen", "authors": "Yun Shen and Wilhelm Stannat and Klaus Obermayer", "title": "Risk-sensitive Markov control processes", "comments": "21 pages", "journal-ref": "SIAM J. Control Optim., 51(5), 3652-3672, 2013", "doi": "10.1137/120899005", "report-no": null, "categories": "math.OC cs.CE math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for measuring risk in the context of Markov\ncontrol processes with risk maps on general Borel spaces that generalize known\nconcepts of risk measures in mathematical finance, operations research and\nbehavioral economics. Within the framework, applying weighted norm spaces to\nincorporate also unbounded costs, we study two types of infinite-horizon\nrisk-sensitive criteria, discounted total risk and average risk, and solve the\nassociated optimization problems by dynamic programming. For the discounted\ncase, we propose a new discount scheme, which is different from the\nconventional form but consistent with the existing literature, while for the\naverage risk criterion, we state Lyapunov-like stability conditions that\ngeneralize known conditions for Markov chains to ensure the existence of\nsolutions to the optimality equation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 12:37:44 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2011 00:13:07 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2013 14:34:38 GMT"}, {"version": "v4", "created": "Sun, 17 Nov 2013 10:07:22 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2014 21:43:23 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Shen", "Yun", ""], ["Stannat", "Wilhelm", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1110.6416", "submitter": "Tim van Erven", "authors": "Tim van Erven, Peter Gr\\\"unwald, Wouter M. Koolen and Steven de Rooij", "title": "Adaptive Hedge", "comments": "This is the full version of the paper with the same name that will\n  appear in Advances in Neural Information Processing Systems 24 (NIPS 2011),\n  2012. The two papers are identical, except that this version contains an\n  extra section of Additional Material", "journal-ref": "Advances in Neural Information Processing Systems 24, pages\n  1656-1664, December 2011", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for decision-theoretic online learning are based on the Hedge\nalgorithm, which takes a parameter called the learning rate. In most previous\nanalyses the learning rate was carefully tuned to obtain optimal worst-case\nperformance, leading to suboptimal performance on easy instances, for example\nwhen there exists an action that is significantly better than all others. We\npropose a new way of setting the learning rate, which adapts to the difficulty\nof the learning problem: in the worst case our procedure still guarantees\noptimal performance, but on easy instances it achieves much smaller regret. In\nparticular, our adaptive method achieves constant regret in a probabilistic\nsetting, when there exists an action that on average obtains strictly smaller\nloss than all other actions. We also provide a simulation study comparing our\napproach to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 18:09:50 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["van Erven", "Tim", ""], ["Gr\u00fcnwald", "Peter", ""], ["Koolen", "Wouter M.", ""], ["de Rooij", "Steven", ""]]}, {"id": "1110.6497", "submitter": "Ziyu Wang", "authors": "Nimalan Mahendran, Ziyu Wang, Firas Hamze, Nando de Freitas", "title": "Bayesian Optimization for Adaptive MCMC", "comments": "This paper contains 12 pages and 6 figures. A similar version of this\n  paper has been submitted to AISTATS 2012 and is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new randomized strategy for adaptive MCMC using\nBayesian optimization. This approach applies to non-differentiable objective\nfunctions and trades off exploration and exploitation to reduce the number of\npotentially costly objective function evaluations. We demonstrate the strategy\nin the complex setting of sampling from constrained, discrete and densely\nconnected probabilistic graphical models where, for each variation of the\nproblem, one needs to adjust the parameters of the proposal mechanism\nautomatically to ensure efficient mixing of the Markov chains.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2011 05:23:36 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Mahendran", "Nimalan", ""], ["Wang", "Ziyu", ""], ["Hamze", "Firas", ""], ["de Freitas", "Nando", ""]]}, {"id": "1110.6546", "submitter": "Andrea Schirru Mr", "authors": "Andrea Schirru, Simone Pampuri, Giuseppe De Nicolao, Sean McLoone", "title": "Efficient Marginal Likelihood Computation for Gaussian Process\n  Regression", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Bayesian learning setting, the posterior distribution of a predictive\nmodel arises from a trade-off between its prior distribution and the\nconditional likelihood of observed data. Such distribution functions usually\nrely on additional hyperparameters which need to be tuned in order to achieve\noptimum predictive performance; this operation can be efficiently performed in\nan Empirical Bayes fashion by maximizing the posterior marginal likelihood of\nthe observed data. Since the score function of this optimization problem is in\ngeneral characterized by the presence of local optima, it is necessary to\nresort to global optimization strategies, which require a large number of\nfunction evaluations. Given that the evaluation is usually computationally\nintensive and badly scaled with respect to the dataset size, the maximum number\nof observations that can be treated simultaneously is quite limited. In this\npaper, we consider the case of hyperparameter tuning in Gaussian process\nregression. A straightforward implementation of the posterior log-likelihood\nfor this model requires O(N^3) operations for every iteration of the\noptimization procedure, where N is the number of examples in the input dataset.\nWe derive a novel set of identities that allow, after an initial overhead of\nO(N^3), the evaluation of the score function, as well as the Jacobian and\nHessian matrices, in O(N) operations. We prove how the proposed identities,\nthat follow from the eigendecomposition of the kernel matrix, yield a reduction\nof several orders of magnitude in the computation time for the hyperparameter\noptimization problem. Notably, the proposed solution provides computational\nadvantages even with respect to state of the art approximations that rely on\nsparse kernel matrices.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2011 18:36:00 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Schirru", "Andrea", ""], ["Pampuri", "Simone", ""], ["De Nicolao", "Giuseppe", ""], ["McLoone", "Sean", ""]]}, {"id": "1110.6886", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin, Fran\\c{c}ois Laviolette, Nicol\\`o Cesa-Bianchi, John\n  Shawe-Taylor, Peter Auer", "title": "PAC-Bayesian Inequalities for Martingales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of high-probability inequalities that control the\nconcentration of weighted averages of multiple (possibly uncountably many)\nsimultaneously evolving and interdependent martingales. Our results extend the\nPAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales\nopening the way for its application to importance weighted sampling,\nreinforcement learning, and other interactive learning domains, as well as many\nother domains in probability theory and statistics, where martingales are\nencountered.\n  We also present a comparison inequality that bounds the expectation of a\nconvex function of a martingale difference sequence shifted to the [0,1]\ninterval by the expectation of the same function of independent Bernoulli\nvariables. This inequality is applied to derive a tighter analog of\nHoeffding-Azuma's inequality.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 18:22:24 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2012 11:56:07 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2012 14:02:53 GMT"}], "update_date": "2012-07-31", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shawe-Taylor", "John", ""], ["Auer", "Peter", ""]]}]