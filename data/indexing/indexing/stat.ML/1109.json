[{"id": "1109.0105", "submitter": "Abhradeep Thakurta", "authors": "Prateek Jain, Pravesh Kothari, Abhradeep Thakurta", "title": "Differentially Private Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of preserving privacy in the online\nlearning setting. We study the problem in the online convex programming (OCP)\nframework---a popular online learning setting with several interesting\ntheoretical and practical implications---while using differential privacy as\nthe formal privacy measure. For this problem, we distill two critical\nattributes that a private OCP algorithm should have in order to provide\nreasonable privacy as well as utility guarantees: 1) linearly decreasing\nsensitivity, i.e., as new data points arrive their effect on the learning model\ndecreases, 2) sub-linear regret bound---regret bound is a popular\ngoodness/utility measure of an online learning algorithm.\n  Given an OCP algorithm that satisfies these two conditions, we provide a\ngeneral framework to convert the given algorithm into a privacy preserving OCP\nalgorithm with good (sub-linear) regret. We then illustrate our approach by\nconverting two popular online learning algorithms into their differentially\nprivate variants while guaranteeing sub-linear regret ($O(\\sqrt{T})$). Next, we\nconsider the special case of online linear regression problems, a practically\nimportant class of online learning problems, for which we generalize an\napproach by Dwork et al. to provide a differentially private algorithm with\njust $O(\\log^{1.5} T)$ regret. Finally, we show that our online learning\nframework can be used to provide differentially private algorithms for offline\nlearning as well. For the offline learning problem, our approach obtains better\nerror bounds as well as can handle larger class of problems than the existing\nstate-of-the-art methods Chaudhuri et al.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 06:43:23 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2011 17:10:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jain", "Prateek", ""], ["Kothari", "Pravesh", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1109.0258", "submitter": "Suvrit Sra", "authors": "Suvrit Sra", "title": "Nonconvex proximal splitting: batch and incremental algorithms", "comments": "revised version 12 pages, 2 figures; superset of shorter counterpart\n  in NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the unmanageably large class of nonconvex optimization, we consider\nthe rich subclass of nonsmooth problems that have composite objectives---this\nalready includes the extensively studied convex, composite objective problems\nas a special case. For this subclass, we introduce a powerful, new framework\nthat permits asymptotically non-vanishing perturbations. In particular, we\ndevelop perturbation-based batch and incremental (online like) nonconvex\nproximal splitting algorithms. To our knowledge, this is the first time that\nsuch perturbation-based nonconvex splitting algorithms are being proposed and\nanalyzed. While the main contribution of the paper is the theoretical\nframework, we complement our results by presenting some empirical results on\nmatrix factorization.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 18:51:19 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 09:58:53 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Sra", "Suvrit", ""]]}, {"id": "1109.0322", "submitter": "Lauren Hannah", "authors": "Lauren A. Hannah and David B. Dunson", "title": "Bayesian nonparametric multivariate convex regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as economics, operations research and\nreinforcement learning, one often needs to estimate a multivariate regression\nfunction f subject to a convexity constraint. For example, in sequential\ndecision processes the value of a state under optimal subsequent decisions may\nbe known to be convex or concave. We propose a new Bayesian nonparametric\nmultivariate approach based on characterizing the unknown regression function\nas the max of a random collection of unknown hyperplanes. This specification\ninduces a prior with large support in a Kullback-Leibler sense on the space of\nconvex functions, while also leading to strong posterior consistency. Although\nwe assume that f is defined over R^p, we show that this model has a convergence\nrate of log(n)^{-1} n^{-1/(d+2)} under the empirical L2 norm when f actually\nmaps a d dimensional linear subspace to R. We design an efficient reversible\njump MCMC algorithm for posterior computation and demonstrate the methods\nthrough application to value function approximation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 22:51:34 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Hannah", "Lauren A.", ""], ["Dunson", "David B.", ""]]}, {"id": "1109.0343", "submitter": "John Paisley", "authors": "John Paisley, David Blei and Michael I. Jordan", "title": "The Stick-Breaking Construction of the Beta Process as a Poisson Process", "comments": "We've updated with the conference version of the paper. Please note\n  that the title has changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the stick-breaking construction of the beta process due to\nPaisley, et al. (2010) can be obtained from the characterization of the beta\nprocess as a Poisson process. Specifically, we show that the mean measure of\nthe underlying Poisson process is equal to that of the beta process. We use\nthis underlying representation to derive error bounds on truncated beta\nprocesses that are tighter than those in the literature. We also develop a new\nMCMC inference algorithm for beta processes, based in part on our new Poisson\nprocess construction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 02:24:45 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2012 13:27:25 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Paisley", "John", ""], ["Blei", "David", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1109.0455", "submitter": "Kenji Fukumizu", "authors": "Kenji Fukumizu and Chenlei Leng", "title": "Gradient-based kernel dimension reduction for supervised learning", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel kernel approach to linear dimension reduction for\nsupervised learning. The purpose of the dimension reduction is to find\ndirections in the input space to explain the output as effectively as possible.\nThe proposed method uses an estimator for the gradient of regression function,\nbased on the covariance operators on reproducing kernel Hilbert spaces. In\ncomparison with other existing methods, the proposed one has wide applicability\nwithout strong assumptions on the distributions or the type of variables, and\nuses computationally simple eigendecomposition. Experimental results show that\nthe proposed method successfully finds the effective directions with efficient\ncomputation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 14:27:25 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Fukumizu", "Kenji", ""], ["Leng", "Chenlei", ""]]}, {"id": "1109.0730", "submitter": "Antony Joseph", "authors": "Antony Joseph", "title": "Variable Selection in High Dimensions with Random Designs and Orthogonal\n  Matching Pursuit", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Orthogonal Matching Pursuit (OMP) for variable selection\nis analyzed for random designs. When contrasted with the deterministic case,\nsince the performance is here measured after averaging over the distribution of\nthe design matrix, one can have far less stringent sparsity constraints on the\ncoefficient vector. We demonstrate that for exact sparse vectors, the\nperformance of the OMP is similar to known results on the Lasso algorithm\n[\\textit{IEEE Trans. Inform. Theory} \\textbf{55} (2009) 2183--2202]. Moreover,\nvariable selection under a more relaxed sparsity assumption on the coefficient\nvector, whereby one has only control on the $\\ell_1$ norm of the smaller\ncoefficients, is also analyzed. As a consequence of these results, we also show\nthat the coefficient estimate satisfies strong oracle type inequalities.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 17:03:00 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Joseph", "Antony", ""]]}, {"id": "1109.0820", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Yonatan Wexler and Amnon Shashua", "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiclass prediction is the problem of classifying an object into a relevant\ntarget class. We consider the problem of learning a multiclass predictor that\nuses only few features, and in particular, the number of used features should\nincrease sub-linearly with the number of possible classes. This implies that\nfeatures should be shared by several classes. We describe and analyze the\nShareBoost algorithm for learning a multiclass predictor that uses few shared\nfeatures. We prove that ShareBoost efficiently finds a predictor that uses few\nshared features (if such a predictor exists) and that it has a small\ngeneralization error. We also describe how to use ShareBoost for learning a\nnon-linear predictor that has a fast evaluation time. In a series of\nexperiments with natural data sets we demonstrate the benefits of ShareBoost\nand evaluate its success relatively to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 07:52:17 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Wexler", "Yonatan", ""], ["Shashua", "Amnon", ""]]}, {"id": "1109.0887", "submitter": "Tong Zhang", "authors": "Rie Johnson and Tong Zhang", "title": "Learning Nonlinear Functions Using Regularized Greedy Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a forest of nonlinear decision rules with\ngeneral loss functions. The standard methods employ boosted decision trees such\nas Adaboost for exponential loss and Friedman's gradient boosting for general\nloss. In contrast to these traditional boosting algorithms that treat a tree\nlearner as a black box, the method we propose directly learns decision forests\nvia fully-corrective regularized greedy search using the underlying forest\nstructure. Our method achieves higher accuracy and smaller models than gradient\nboosting (and Adaboost with exponential loss) on many datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 13:17:08 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2011 16:39:08 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2012 22:20:21 GMT"}, {"version": "v4", "created": "Mon, 5 Mar 2012 19:02:41 GMT"}, {"version": "v5", "created": "Mon, 24 Sep 2012 16:28:00 GMT"}, {"version": "v6", "created": "Sat, 12 Oct 2013 12:13:10 GMT"}, {"version": "v7", "created": "Sat, 28 Jun 2014 23:46:39 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1109.0895", "submitter": "Anis Charrada", "authors": "Anis Charrada, Abdelaziz Samet", "title": "Nonlinear Channel Estimation for OFDM System by Complex LS-SVM under\n  High Mobility Conditions", "comments": "11 pages", "journal-ref": "International Journal of Wireless & Mobile Networks (IJWMN) Vol.\n  3, No. 4, August 2011", "doi": "10.5121/ijwmn.2011.3412", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A nonlinear channel estimator using complex Least Square Support Vector\nMachines (LS-SVM) is proposed for pilot-aided OFDM system and applied to Long\nTerm Evolution (LTE) downlink under high mobility conditions. The estimation\nalgorithm makes use of the reference signals to estimate the total frequency\nresponse of the highly selective multipath channel in the presence of\nnon-Gaussian impulse noise interfering with pilot signals. Thus, the algorithm\nmaps trained data into a high dimensional feature space and uses the structural\nrisk minimization (SRM) principle to carry out the regression estimation for\nthe frequency response function of the highly selective channel. The\nsimulations show the effectiveness of the proposed method which has good\nperformance and high precision to track the variations of the fading channels\ncompared to the conventional LS method and it is robust at high speed mobility.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 21:40:00 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Charrada", "Anis", ""], ["Samet", "Abdelaziz", ""]]}, {"id": "1109.1032", "submitter": "Emanuele Coviello", "authors": "Emanuele Coviello, Antoni B. Chan, Gert R.G. Lanckriet", "title": "Tech Report A Variational HEM Algorithm for Clustering Hidden Markov\n  Models", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hidden Markov model (HMM) is a generative model that treats sequential\ndata under the assumption that each observation is conditioned on the state of\na discrete hidden variable that evolves in time as a Markov chain. In this\npaper, we derive a novel algorithm to cluster HMMs through their probability\ndistributions. We propose a hierarchical EM algorithm that i) clusters a given\ncollection of HMMs into groups of HMMs that are similar, in terms of the\ndistributions they represent, and ii) characterizes each group by a \"cluster\ncenter\", i.e., a novel HMM that is representative for the group. We present\nseveral empirical studies that illustrate the benefits of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 00:12:55 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Coviello", "Emanuele", ""], ["Chan", "Antoni B.", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "1109.1077", "submitter": "Deepayan Chakrabarti", "authors": "Purnamrita Sarkar and Deepayan Chakrabarti and Michael Jordan", "title": "Nonparametric Link Prediction in Large Scale Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric approach to link prediction in large-scale dynamic\nnetworks. Our model uses graph-based features of pairs of nodes as well as\nthose of their local neighborhoods to predict whether those nodes will be\nlinked at each time step. The model allows for different types of evolution in\ndifferent parts of the graph (e.g, growing or shrinking communities). We focus\non large-scale graphs and present an implementation of our model that makes use\nof locality-sensitive hashing to allow it to be scaled to large problems.\nExperiments with simulated data as well as five real-world dynamic graphs show\nthat we outperform the state of the art, especially when sharp fluctuations or\nnonlinearities are present. We also establish theoretical properties of our\nestimator, in particular consistency and weak convergence, the latter making\nuse of an elaboration of Stein's method for dependency graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 06:11:03 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 00:37:07 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2013 05:29:47 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Sarkar", "Purnamrita", ""], ["Chakrabarti", "Deepayan", ""], ["Jordan", "Michael", ""]]}, {"id": "1109.1754", "submitter": "Denis Mau\\'a", "authors": "Denis Deratani Mau\\'a, Cassio Polpo de Campos, Marco Zaffalon", "title": "Solving Limited Memory Influence Diagrams", "comments": "43 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for exactly solving decision making problems\nrepresented as influence diagrams. We do not require the usual assumptions of\nno forgetting and regularity; this allows us to solve problems with\nsimultaneous decisions and limited information. The algorithm is empirically\nshown to outperform a state-of-the-art algorithm on randomly generated problems\nof up to 150 variables and $10^{64}$ solutions. We show that the problem is\nNP-hard even if the underlying graph structure of the problem has small\ntreewidth and the variables take on a bounded number of states, but that a\nfully polynomial time approximation scheme exists for these cases. Moreover, we\nshow that the bound on the number of states is a necessary condition for any\nefficient approximation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 16:17:07 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 10:30:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mau\u00e1", "Denis Deratani", ""], ["de Campos", "Cassio Polpo", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1109.1990", "submitter": "Francis Bach", "authors": "Edouard Grave (LIENS, INRIA Paris - Rocquencourt), Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt), Francis Bach (LIENS, INRIA Paris -\n  Rocquencourt)", "title": "Trace Lasso: a trace norm regularization for correlated designs", "comments": null, "journal-ref": "Neural Information Processing Systems, Spain (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the $\\ell_1$-norm to regularize the estimation of the parameter vector\nof a linear model leads to an unstable estimator when covariates are highly\ncorrelated. In this paper, we introduce a new penalty function which takes into\naccount the correlation of the design matrix to stabilize the estimation. This\nnorm, called the trace Lasso, uses the trace norm, which is a convex surrogate\nof the rank, of the selected covariates as the criterion of model complexity.\nWe analyze the properties of our norm, describe an optimization algorithm based\non reweighted least-squares, and illustrate the behavior of this norm on\nsynthetic data, showing that it is more adapted to strong correlations than\ncompeting methods such as the elastic net.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 13:01:41 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Grave", "Edouard", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris -\n  Rocquencourt"]]}, {"id": "1109.2279", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott and Jesse Windle", "title": "The Bayesian Bridge", "comments": "Supplemental files are available from the second author's website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Bayesian bridge estimator for regularized regression and\nclassification. Two key mixture representations for the Bayesian bridge model\nare developed: (1) a scale mixture of normals with respect to an alpha-stable\nrandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle\ndensities) with respect to a two-component mixture of gamma random variables.\nBoth lead to MCMC methods for posterior simulation, and these methods turn out\nto have complementary domains of maximum efficiency. The first representation\nis a well known result due to West (1987), and is the better choice for\ncollinear design matrices. The second representation is new, and is more\nefficient for orthogonal problems, largely because it avoids the need to deal\nwith exponentially tilted stable random variables. It also provides insight\ninto the multimodality of the joint posterior distribution, a feature of the\nbridge model that is notably absent under ridge or lasso-type priors. We prove\na theorem that extends this representation to a wider class of densities\nrepresentable as scale mixtures of betas, and provide an explicit inversion\nformula for the mixing distribution. The connections with slice sampling and\nscale mixtures of normals are explored. On the practical side, we find that the\nBayesian bridge model outperforms its classical cousin in estimation and\nprediction across a variety of data sets, both simulated and real. We also show\nthat the MCMC for fitting the bridge model exhibits excellent mixing\nproperties, particularly for the global scale parameter. This makes for a\nfavorable contrast with analogous MCMC algorithms for other sparse Bayesian\nmodels. All methods described in this paper are implemented in the R package\nBayesBridge. An extensive set of simulation results are provided in two\nsupplemental files.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 04:57:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2012 21:01:16 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1109.2378", "submitter": "Daniel M\\\"ullner", "authors": "Daniel M\\\"ullner", "title": "Modern hierarchical, agglomerative clustering algorithms", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents algorithms for hierarchical, agglomerative clustering\nwhich perform most efficiently in the general-purpose setup that is given in\nmodern standard software. Requirements are: (1) the input data is given by\npairwise dissimilarities between data points, but extensions to vector data are\nalso discussed (2) the output is a \"stepwise dendrogram\", a data structure\nwhich is shared by all implementations in current standard software. We present\nalgorithms (old and new) which perform clustering in this setting efficiently,\nboth in an asymptotic worst-case analysis and from a practical point of view.\nThe main contributions of this paper are: (1) We present a new algorithm which\nis suitable for any distance update scheme and performs significantly better\nthan the existing algorithms. (2) We prove the correctness of two algorithms by\nRohlf and Murtagh, which is necessary in each case for different reasons. (3)\nWe give well-founded recommendations for the best current algorithms for the\nvarious agglomerative clustering schemes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 05:49:11 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["M\u00fcllner", "Daniel", ""]]}, {"id": "1109.2397", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt), Rodolphe Jenatton\n  (LIENS, INRIA Paris - Rocquencourt), Julien Mairal, Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Structured sparsity through convex optimization", "comments": "Statistical Science (2012) To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods are aimed at using or obtaining parsimonious\nrepresentations of data or models. While naturally cast as a combinatorial\noptimization problem, variable or feature selection admits a convex relaxation\nthrough the regularization by the $\\ell_1$-norm. In this paper, we consider\nsituations where we are not only interested in sparsity, but where some\nstructural prior knowledge is available as well. We show that the $\\ell_1$-norm\ncan then be extended to structured norms built on either disjoint or\noverlapping groups of variables, leading to a flexible framework that can deal\nwith various structures. We present applications to unsupervised learning, for\nstructured sparse principal component analysis and hierarchical dictionary\nlearning, and to supervised learning in the context of non-linear variable\nselection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 08:23:02 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2012 13:20:27 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1109.2411", "submitter": "Kei Hirose", "authors": "Kei Hirose, Shohei Tateishi and Sadanori Konishi", "title": "Efficient algorithm to select tuning parameters in sparse regression\n  modeling with regularization", "comments": "24pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse regression modeling via regularization such as the lasso, it is\nimportant to select appropriate values of tuning parameters including\nregularization parameters. The choice of tuning parameters can be viewed as a\nmodel selection and evaluation problem. Mallows' $C_p$ type criteria may be\nused as a tuning parameter selection tool in lasso-type regularization methods,\nfor which the concept of degrees of freedom plays a key role. In the present\npaper, we propose an efficient algorithm that computes the degrees of freedom\nby extending the generalized path seeking algorithm. Our procedure allows us to\nconstruct model selection criteria for evaluating models estimated by\nregularization with a wide variety of convex and non-convex penalties. Monte\nCarlo simulations demonstrate that our methodology performs well in various\nsituations. A real data example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 09:33:12 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 01:01:26 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2012 04:41:51 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Hirose", "Kei", ""], ["Tateishi", "Shohei", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1109.2553", "submitter": "Xiaogang (Steven) Wang", "authors": "Wenxue Huang, Yong Shi and Xiaogang Wang", "title": "Nominal Association Vector and Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When response variables are nominal and populations are cross-classified with\nrespect to multiple polytomies, questions often arise about the degree of\nassociation of the responses with explanatory variables. When populations are\nknown, we introduce a nominal association vector and matrix to evaluate the\ndependence of a response variable with an explanatory variable. These measures\nprovide detailed evaluations of nominal associations at both local and global\nlevels. We also define a general class of global association measures which\nembraces the well known association measure by Goodman-Kruskal (1954). The\nproposed association matrix also gives rise to the expected generalized\nconfusion matrix in classification. The hierarchy of equivalence relations\ndefined by the association vector and matrix are also shown.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 18:12:09 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2012 22:15:57 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Huang", "Wenxue", ""], ["Shi", "Yong", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1109.2618", "submitter": "O. Anatole von Lilienfeld", "authors": "Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert M\\\"uller, O. Anatole\n  von Lilienfeld", "title": "Fast and Accurate Modeling of Molecular Atomization Energies with\n  Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevLett.108.058301", "report-no": null, "categories": "physics.chem-ph cond-mat.dis-nn cond-mat.mtrl-sci stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a machine learning model to predict atomization energies of a\ndiverse set of organic molecules, based on nuclear charges and atomic positions\nonly. The problem of solving the molecular Schr\\\"odinger equation is mapped\nonto a non-linear statistical regression problem of reduced complexity.\nRegression models are trained on and compared to atomization energies computed\nwith hybrid density-functional theory. Cross-validation over more than seven\nthousand small organic molecules yields a mean absolute error of ~10 kcal/mol.\nApplicability is demonstrated for the prediction of molecular atomization\npotential energy curves.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 20:33:46 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Rupp", "Matthias", ""], ["Tkatchenko", "Alexandre", ""], ["M\u00fcller", "Klaus-Robert", ""], ["von Lilienfeld", "O. Anatole", ""]]}, {"id": "1109.3240", "submitter": "Yaojia Zhu", "authors": "Cristopher Moore, Xiaoran Yan, Yaojia Zhu, Jean-Baptiste Rouquier,\n  Terran Lane", "title": "Active Learning for Node Classification in Assortative and\n  Disassortative Networks", "comments": "9 pages, 7 figures, KDD 2011: The 17th ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world networks, nodes have class labels, attributes, or\nvariables that affect the network's topology. If the topology of the network is\nknown but the labels of the nodes are hidden, we would like to select a small\nsubset of nodes such that, if we knew their labels, we could accurately predict\nthe labels of all the other nodes. We develop an active learning algorithm for\nthis problem which uses information-theoretic techniques to choose which nodes\nto explore. We test our algorithm on networks from three different domains: a\nsocial network, a network of English words that appear adjacently in a novel,\nand a marine food web. Our algorithm makes no initial assumptions about how the\ngroups connect, and performs well even when faced with quite general types of\nnetwork structure. In particular, we do not assume that nodes of the same class\nare more likely to be connected to each other---only that they connect to the\nrest of the network in similar ways.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 02:10:26 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Moore", "Cristopher", ""], ["Yan", "Xiaoran", ""], ["Zhu", "Yaojia", ""], ["Rouquier", "Jean-Baptiste", ""], ["Lane", "Terran", ""]]}, {"id": "1109.3248", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Reconstruction of sequential data with density models", "comments": "30 pages, 9 figures. Original manuscript dated January 27, 2004 and\n  not updated since. Current author's email address:\n  mcarreira-perpinan@ucmerced.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of reconstructing a sequence of multidimensional\nreal vectors where some of the data are missing. This problem contains\nregression and mapping inversion as particular cases where the pattern of\nmissing data is independent of the sequence index. The problem is hard because\nit involves possibly multivalued mappings at each vector in the sequence, where\nthe missing variables can take more than one value given the present variables;\nand the set of missing variables can vary from one vector to the next. To solve\nthis problem, we propose an algorithm based on two redundancy assumptions:\nvector redundancy (the data live in a low-dimensional manifold), so that the\npresent variables constrain the missing ones; and sequence redundancy (e.g.\ncontinuity), so that consecutive vectors constrain each other. We capture the\nlow-dimensional nature of the data in a probabilistic way with a joint density\nmodel, here the generative topographic mapping, which results in a Gaussian\nmixture. Candidate reconstructions at each vector are obtained as all the modes\nof the conditional distribution of missing variables given present variables.\nThe reconstructed sequence is obtained by minimising a global constraint, here\nthe sequence length, by dynamic programming. We present experimental results\nfor a toy problem and for inverse kinematics of a robot arm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 03:12:36 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1109.3250", "submitter": "XuanLong Nguyen", "authors": "XuanLong Nguyen", "title": "Convergence of latent mixing measures in finite and infinite mixture\n  models", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1065 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 1, 370-400", "doi": "10.1214/12-AOS1065", "report-no": "IMS-AOS-AOS1065", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convergence behavior of latent mixing measures that arise\nin finite and infinite mixture models, using transportation distances (i.e.,\nWasserstein metrics). The relationship between Wasserstein distances on the\nspace of mixing measures and f-divergence functionals such as Hellinger and\nKullback-Leibler distances on the space of mixture distributions is\ninvestigated in detail using various identifiability conditions. Convergence in\nWasserstein metrics for discrete measures implies convergence of individual\natoms that provide support for the measures, thereby providing a natural\ninterpretation of convergence of clusters in clustering applications where\nmixture models are typically employed. Convergence rates of posterior\ndistributions for latent mixing measures are established, for both finite\nmixtures of multivariate distributions and infinite mixtures based on the\nDirichlet process.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 03:26:59 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 16:40:05 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2012 00:09:56 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2012 21:19:24 GMT"}, {"version": "v5", "created": "Tue, 9 Apr 2013 05:24:55 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Nguyen", "XuanLong", ""]]}, {"id": "1109.3336", "submitter": "Arash A. Amini", "authors": "Arash A. Amini, Martin J. Wainwright", "title": "Sampled forms of functional PCA in reproducing kernel Hilbert spaces", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1033 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 5, 2483-2510", "doi": "10.1214/12-AOS1033", "report-no": "IMS-AOS-AOS1033", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sampling problem for functional PCA (fPCA), where the\nsimplest example is the case of taking time samples of the underlying\nfunctional components. More generally, we model the sampling operation as a\ncontinuous linear map from $\\mathcal{H}$ to $\\mathbb{R}^m$, where the\nfunctional components to lie in some Hilbert subspace $\\mathcal{H}$ of $L^2$,\nsuch as a reproducing kernel Hilbert space of smooth functions. This model\nincludes time and frequency sampling as special cases. In contrast to classical\napproach in fPCA in which access to entire functions is assumed, having a\nlimited number m of functional samples places limitations on the performance of\nstatistical procedures. We study these effects by analyzing the rate of\nconvergence of an M-estimator for the subspace spanned by the leading\ncomponents in a multi-spiked covariance model. The estimator takes the form of\nregularized PCA, and hence is computationally attractive. We analyze the\nbehavior of this estimator within a nonasymptotic framework, and provide bounds\nthat hold with high probability as a function of the number of statistical\nsamples n and the number of functional samples m. We also derive lower bounds\nshowing that the rates obtained are minimax optimal.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 12:52:47 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2013 12:28:11 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Amini", "Arash A.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1109.3701", "submitter": "Kevin Jamieson", "authors": "Kevin G. Jamieson and Robert D. Nowak", "title": "Active Ranking using Pairwise Comparisons", "comments": "17 pages, an extended version of our NIPS 2011 paper. The new version\n  revises the argument of the robust section and slightly modifies the result\n  there to give it more impact", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of ranking a collection of objects using\npairwise comparisons (rankings of two objects). In general, the ranking of $n$\nobjects can be identified by standard sorting methods using $n log_2 n$\npairwise comparisons. We are interested in natural situations in which\nrelationships among the objects may allow for ranking using far fewer pairwise\ncomparisons. Specifically, we assume that the objects can be embedded into a\n$d$-dimensional Euclidean space and that the rankings reflect their relative\ndistances from a common reference point in $R^d$. We show that under this\nassumption the number of possible rankings grows like $n^{2d}$ and demonstrate\nan algorithm that can identify a randomly selected ranking using just slightly\nmore than $d log n$ adaptively selected pairwise comparisons, on average. If\ninstead the comparisons are chosen at random, then almost all pairwise\ncomparisons must be made in order to identify any ranking. In addition, we\npropose a robust, error-tolerant algorithm that only requires that the pairwise\ncomparisons are probably correct. Experimental studies with synthetic and real\ndatasets support the conclusions of our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 19:35:13 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2011 01:02:14 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Jamieson", "Kevin G.", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1109.3714", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh, Martin J. Wainwright", "title": "High-dimensional regression with noisy and missing data: Provable\n  guarantees with nonconvexity", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1018 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 3, 1637-1664", "doi": "10.1214/12-AOS1018", "report-no": "IMS-AOS-AOS1018", "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the standard formulations of prediction problems involve\nfully-observed and noiseless data drawn in an i.i.d. manner, many applications\ninvolve noisy and/or missing data, possibly involving dependence, as well. We\nstudy these issues in the context of high-dimensional sparse linear regression,\nand propose novel estimators for the cases of noisy, missing and/or dependent\ndata. Many standard approaches to noisy or missing data, such as those using\nthe EM algorithm, lead to optimization problems that are inherently nonconvex,\nand it is difficult to establish theoretical guarantees on practical\nalgorithms. While our approach also involves optimizing nonconvex programs, we\nare able to both analyze the statistical error associated with any global\noptimum, and more surprisingly, to prove that a simple algorithm based on\nprojected gradient descent will converge in polynomial time to a small\nneighborhood of the set of all global minimizers. On the statistical side, we\nprovide nonasymptotic bounds that hold with high probability for the cases of\nnoisy, missing and/or dependent data. On the computational side, we prove that\nunder the same types of conditions required for statistical consistency, the\nprojected gradient descent algorithm is guaranteed to converge at a geometric\nrate to a near-global minimizer. We illustrate these theoretical predictions\nwith simulations, showing close agreement with the predicted scalings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 20:02:47 GMT"}, {"version": "v2", "created": "Thu, 10 May 2012 23:22:28 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2012 17:04:24 GMT"}, {"version": "v4", "created": "Tue, 25 Sep 2012 06:58:01 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1109.3827", "submitter": "Jun He", "authors": "Jun He, Laura Balzano, John C.S. Lui", "title": "Online Robust Subspace Tracking from Partial Information", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GRASTA (Grassmannian Robust Adaptive Subspace Tracking\nAlgorithm), an efficient and robust online algorithm for tracking subspaces\nfrom highly incomplete information. The algorithm uses a robust $l^1$-norm cost\nfunction in order to estimate and track non-stationary subspaces when the\nstreaming data vectors are corrupted with outliers. We apply GRASTA to the\nproblems of robust matrix completion and real-time separation of background\nfrom foreground in video. In this second application, we show that GRASTA\nperforms high-quality separation of moving objects from background at\nexceptional speeds: In one popular benchmark video example, GRASTA achieves a\nrate of 57 frames per second, even when run in MATLAB on a personal laptop.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2011 00:53:53 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2011 13:02:04 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["He", "Jun", ""], ["Balzano", "Laura", ""], ["Lui", "John C. S.", ""]]}, {"id": "1109.3940", "submitter": "Yuan Shi", "authors": "Yuan Shi, Yung-Kyun Noh, Fei Sha, Daniel D. Lee", "title": "Learning Discriminative Metrics via Generative Models and Kernel\n  Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics specifying distances between data points can be learned in a\ndiscriminative manner or from generative models. In this paper, we show how to\nunify generative and discriminative learning of metrics via a kernel learning\nframework. Specifically, we learn local metrics optimized from parametric\ngenerative models. These are then used as base kernels to construct a global\nkernel that minimizes a discriminative training criterion. We consider both\nlinear and nonlinear combinations of local metric kernels. Our empirical\nresults show that these combinations significantly improve performance on\nclassification tasks. The proposed learning algorithm is also very efficient,\nachieving order of magnitude speedup in training time compared to previous\ndiscriminative baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 04:19:30 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Shi", "Yuan", ""], ["Noh", "Yung-Kyun", ""], ["Sha", "Fei", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1109.4347", "submitter": "Yohji Akama", "authors": "Yohji Akama and Kei Irie", "title": "VC dimension of ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will establish that the VC dimension of the class of d-dimensional\nellipsoids is (d^2+3d)/2, and that maximum likelihood estimate with N-component\nd-dimensional Gaussian mixture models induces a geometric class having VC\ndimension at least N(d^2+3d)/2.\n  Keywords: VC dimension; finite dimensional ellipsoid; Gaussian mixture model\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 16:53:29 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Akama", "Yohji", ""], ["Irie", "Kei", ""]]}, {"id": "1109.4389", "submitter": "Lucas Theis", "authors": "Lucas Theis, Reshad Hosseini, Matthias Bethge", "title": "Mixtures of conditional Gaussian scale mixtures applied to multiscale\n  image representations", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0039857", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for natural images which is based on\nGaussian scale mixtures and a simple multiscale representation. In contrast to\nthe dominant approach to modeling whole images focusing on Markov random\nfields, we formulate our model in terms of a directed graphical model. We show\nthat it is able to generate images with interesting higher-order correlations\nwhen trained on natural images or samples from an occlusion based model. More\nimportantly, the directed model enables us to perform a principled evaluation.\nWhile it is easy to generate visually appealing images, we demonstrate that our\nmodel also yields the best performance reported to date when evaluated with\nrespect to the cross-entropy rate, a measure tightly linked to the average\nlog-likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 18:39:31 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Theis", "Lucas", ""], ["Hosseini", "Reshad", ""], ["Bethge", "Matthias", ""]]}, {"id": "1109.4540", "submitter": "Christopher R. Genovese", "authors": "Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli,\n  Larry Wasserman", "title": "Manifold estimation and singular deconvolution under Hausdorff loss", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS994 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 941-963", "doi": "10.1214/12-AOS994", "report-no": "IMS-AOS-AOS994", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find lower and upper bounds for the risk of estimating a manifold in\nHausdorff distance under several models. We also show that there are close\nconnections between manifold estimation and the problem of deconvolving a\nsingular measure.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 14:29:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2012 13:37:56 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Genovese", "Christopher R.", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1109.4928", "submitter": "Leo Lahti", "authors": "Leo Lahti, Laura L. Elo, Tero Aittokallio, Samuel Kaski", "title": "RPA: Probabilistic analysis of probe performance and robust\n  summarization", "comments": "Replaced by extended work which forms an independent publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Probe-level models have led to improved performance in microarray studies but\nthe various sources of probe-level contamination are still poorly understood.\nData-driven analysis of probe performance can be used to quantify the\nuncertainty in individual probes and to highlight the relative contribution of\ndifferent noise sources. Improved understanding of the probe-level effects can\nlead to improved preprocessing techniques and microarray design.\n  We have implemented probabilistic tools for probe performance analysis and\nsummarization on short oligonucleotide arrays. In contrast to standard\npreprocessing approaches, the methods provide quantitative estimates of\nprobe-specific noise and affinity terms and tools to investigate these\nparameters. Tools to incorporate prior information of the probes in the\nanalysis are provided as well. Comparisons to known probe-level error sources\nand spike-in data sets validate the approach.\n  Implementation is freely available in R/BioConductor:\nhttp://www.bioconductor.org/packages/release/bioc/html/RPA.html\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 19:46:02 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2013 09:39:10 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lahti", "Leo", ""], ["Elo", "Laura L.", ""], ["Aittokallio", "Tero", ""], ["Kaski", "Samuel", ""]]}, {"id": "1109.5311", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Bias Plus Variance Decomposition for Survival Analysis Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias - variance decomposition of the expected error defined for regression\nand classification problems is an important tool to study and compare different\nalgorithms, to find the best areas for their application. Here the\ndecomposition is introduced for the survival analysis problem. In our\nexperiments, we study bias -variance parts of the expected error for two\nalgorithms: original Cox proportional hazard regression and CoxPath, path\nalgorithm for L1-regularized Cox regression, on the series of increased\ntraining sets. The experiments demonstrate that, contrary expectations, CoxPath\ndoes not necessarily have an advantage over Cox regression.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2011 22:14:46 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1109.5404", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Towards Optimal Learning of Chain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend Meek's conjecture (Meek 1997) from directed and\nacyclic graphs to chain graphs, and prove that the extended conjecture is true.\nSpecifically, we prove that if a chain graph H is an independence map of the\nindependence model induced by another chain graph G, then (i) G can be\ntransformed into H by a sequence of directed and undirected edge additions and\nfeasible splits and mergings, and (ii) after each operation in the sequence H\nremains an independence map of the independence model induced by G. Our result\nhas the same important consequence for learning chain graphs from data as the\nproof of Meek's conjecture in (Chickering 2002) had for learning Bayesian\nnetworks from data: It makes it possible to develop efficient and\nasymptotically correct learning algorithms under mild assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 21:53:44 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1109.5894", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Yee Whye Teh", "title": "Learning Item Trees for Probabilistic Modelling of Implicit Feedback", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preferences for items can be inferred from either explicit feedback,\nsuch as item ratings, or implicit feedback, such as rental histories. Research\nin collaborative filtering has concentrated on explicit feedback, resulting in\nthe development of accurate and scalable models. However, since explicit\nfeedback is often difficult to collect it is important to develop effective\nmodels that take advantage of the more widely available implicit feedback. We\nintroduce a probabilistic approach to collaborative filtering with implicit\nfeedback based on modelling the user's item selection process. In the interests\nof scalability, we restrict our attention to tree-structured distributions over\nitems and develop a principled and efficient algorithm for learning item trees\nfrom data. We also identify a problem with a widely used protocol for\nevaluating implicit feedback models and propose a way of addressing it using a\nsmall quantity of explicit feedback data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 13:58:39 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mnih", "Andriy", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1109.5909", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi, Steffen Lauritzen", "title": "Markov properties for mixed graphs", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ502 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2014, Vol. 20, No. 2, 676-696", "doi": "10.3150/12-BEJ502", "report-no": "IMS-BEJ-BEJ502", "categories": "stat.OT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we unify the Markov theory of a variety of different types of\ngraphs used in graphical Markov models by introducing the class of loopless\nmixed graphs, and show that all independence models induced by $m$-separation\non such graphs are compositional graphoids. We focus in particular on the\nsubclass of ribbonless graphs which as special cases include undirected graphs,\nbidirected graphs, and directed acyclic graphs, as well as ancestral graphs and\nsummary graphs. We define maximality of such graphs as well as a pairwise and a\nglobal Markov property. We prove that the global and pairwise Markov properties\nof a maximal ribbonless graph are equivalent for any independence model that is\na compositional graphoid.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 14:26:46 GMT"}, {"version": "v2", "created": "Mon, 28 May 2012 09:58:43 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2012 09:05:28 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2012 09:59:11 GMT"}, {"version": "v5", "created": "Wed, 12 Mar 2014 06:59:12 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Lauritzen", "Steffen", ""]]}, {"id": "1109.6090", "submitter": "Eric Chi", "authors": "Eric C. Chi and David W. Scott", "title": "Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion", "comments": "41 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 23(1):111-128,\n  2014", "doi": "10.1080/10618600.2012.737296", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 04:09:37 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 04:29:12 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2012 23:15:18 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chi", "Eric C.", ""], ["Scott", "David W.", ""]]}, {"id": "1109.6297", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ram\\'irez and Guillermo Sapiro", "title": "Low-rank data modeling via the Minimum Description Length principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MM math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust low-rank matrix estimation is a topic of increasing interest, with\npromising applications in a variety of fields, from computer vision to data\nmining and recommender systems. Recent theoretical results establish the\nability of such data models to recover the true underlying low-rank matrix when\na large portion of the measured matrix is either missing or arbitrarily\ncorrupted. However, if low rank is not a hypothesis about the true nature of\nthe data, but a device for extracting regularity from it, no current guidelines\nexist for choosing the rank of the estimated matrix. In this work we address\nthis problem by means of the Minimum Description Length (MDL) principle -- a\nwell established information-theoretic approach to statistical inference -- as\na guideline for selecting a model for the data at hand. We demonstrate the\npractical usefulness of our formal approach with results for complex background\nextraction in video sequences.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 18:56:22 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Ram\u00edrez", "Ignacio", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1109.6804", "submitter": "Athina Spiliopoulou", "authors": "Athina Spiliopoulou and Amos Storkey", "title": "Comparing Probabilistic Models for Melodic Sequences", "comments": "in Proceedings of the ECML-PKDD 2011. Lecture Notes in Computer\n  Science, vol. 6913, pp. 289-304. Springer (2011)", "journal-ref": "Proceedings of the ECML-PKDD 2011. Lecture Notes in Computer\n  Science, vol. 6913, pp. 289-304. Springer (2011)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the real world complexity of music is a challenge for machine\nlearning. We address the task of modeling melodic sequences from the same music\ngenre. We perform a comparative analysis of two probabilistic models; a\nDirichlet Variable Length Markov Model (Dirichlet-VMM) and a Time Convolutional\nRestricted Boltzmann Machine (TC-RBM). We show that the TC-RBM learns\ndescriptive music features, such as underlying chords and typical melody\ntransitions and dynamics. We assess the models for future prediction and\ncompare their performance to a VMM, which is the current state of the art in\nmelody generation. We show that both models perform significantly better than\nthe VMM, with the Dirichlet-VMM marginally outperforming the TC-RBM. Finally,\nwe evaluate the short order statistics of the models, using the\nKullback-Leibler divergence between test sequences and model samples, and show\nthat our proposed methods match the statistics of the music genre significantly\nbetter than the VMM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 12:17:50 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Spiliopoulou", "Athina", ""], ["Storkey", "Amos", ""]]}]