[{"id": "1407.0044", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Frank Wood", "title": "Infinite Structured Hidden Semi-Markov Models", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent advances in Bayesian nonparametric techniques for\nconstructing and performing inference in infinite hidden Markov models. We\nfocus on variants of Bayesian nonparametric hidden Markov models that enhance a\nposteriori state-persistence in particular. This paper also introduces a new\nBayesian nonparametric framework for generating left-to-right and other\nstructured, explicit-duration infinite hidden Markov models that we call the\ninfinite structured hidden semi-Markov model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 20:18:18 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Wood", "Frank", ""]]}, {"id": "1407.0067", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Sanjoy Dasgupta", "title": "Rates of Convergence for Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor methods are a popular class of nonparametric estimators with\nseveral desirable properties, such as adaptivity to different distance scales\nin different regions of space. Prior work on convergence rates for nearest\nneighbor classification has not fully reflected these subtle properties. We\nanalyze the behavior of these estimators in metric spaces and provide\nfinite-sample, distribution-dependent rates of convergence under minimal\nassumptions. As a by-product, we are able to establish the universal\nconsistency of nearest neighbor in a broader range of data spaces than was\npreviously known. We illustrate our upper and lower bounds by introducing\nsmoothness classes that are customized for nearest neighbor classification.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 22:00:57 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 00:44:29 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1407.0179", "submitter": "Novi Quadrianto", "authors": "Daniel Hern\\'andez-Lobato, Viktoriia Sharmanska, Kristian Kersting,\n  Christoph H. Lampert, Novi Quadrianto", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged\n  Noise", "comments": "14 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning with privileged information setting has recently attracted a lot\nof attention within the machine learning community, as it allows the\nintegration of additional knowledge into the training process of a classifier,\neven when this comes in the form of a data modality that is not available at\ntest time. Here, we show that privileged information can naturally be treated\nas noise in the latent function of a Gaussian Process classifier (GPC). That\nis, in contrast to the standard GPC setting, the latent function is not just a\nnuisance but a feature: it becomes a natural measure of confidence about the\ntraining data by modulating the slope of the GPC sigmoid likelihood function.\nExtensive experiments on public datasets show that the proposed GPC method\nusing privileged noise, called GPC+, improves over a standard GPC without\nprivileged knowledge, and also over the current state-of-the-art SVM-based\nmethod, SVM+. Moreover, we show that advanced neural networks and deep learning\nmethods can be compressed as privileged information.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 10:44:49 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Sharmanska", "Viktoriia", ""], ["Kersting", "Kristian", ""], ["Lampert", "Christoph H.", ""], ["Quadrianto", "Novi", ""]]}, {"id": "1407.0202", "submitter": "Francis Bach", "authors": "Aaron Defazio, Francis Bach (INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA), Simon Lacoste-Julien (INRIA Paris - Rocquencourt, LIENS, MSR - INRIA)", "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly\n  Convex Composite Objectives", "comments": "Advances In Neural Information Processing Systems, Nov 2014,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 11:47:56 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 06:57:50 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 08:44:27 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Defazio", "Aaron", "", "INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS, MSR -\n  INRIA"], ["Lacoste-Julien", "Simon", "", "INRIA Paris - Rocquencourt, LIENS, MSR - INRIA"]]}, {"id": "1407.0208", "submitter": "Roi Weiss", "authors": "Aryeh Kontorovich and Roi Weiss", "title": "A Bayes consistent 1-NN classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple modification of the 1-nearest neighbor classifier\nyields a strongly Bayes consistent learner. Prior to this work, the only\nstrongly Bayes consistent proximity-based method was the k-nearest neighbor\nclassifier, for k growing appropriately with sample size. We will argue that a\nmargin-regularized 1-NN enjoys considerable statistical and algorithmic\nadvantages over the k-NN classifier. These include user-friendly finite-sample\nerror bounds, as well as time- and memory-efficient learning and test-point\nevaluation algorithms with a principled speed-accuracy tradeoff. Encouraging\nempirical results are reported.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 12:08:10 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 10:35:28 GMT"}, {"version": "v3", "created": "Sat, 1 Apr 2017 17:10:50 GMT"}, {"version": "v4", "created": "Fri, 17 Aug 2018 09:09:37 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Weiss", "Roi", ""]]}, {"id": "1407.0286", "submitter": "Hoai An Le Thi", "authors": "Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo", "title": "DC approximation approaches for sparse optimization", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse optimization refers to an optimization problem involving the zero-norm\nin objective or constraints. In this paper, nonconvex approximation approaches\nfor sparse optimization have been studied with a unifying point of view in DC\n(Difference of Convex functions) programming framework. Considering a common DC\napproximation of the zero-norm including all standard sparse inducing penalty\nfunctions, we studied the consistency between global minimums (resp. local\nminimums) of approximate and original problems. We showed that, in several\ncases, some global minimizers (resp. local minimizers) of the approximate\nproblem are also those of the original problem. Using exact penalty techniques\nin DC programming, we proved stronger results for some particular\napproximations, namely, the approximate problem, with suitable parameters, is\nequivalent to the original problem. The efficiency of several sparse inducing\npenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes\nwere developed that cover all standard algorithms in nonconvex sparse\napproximation approaches as special versions. They can be viewed as, an $\\ell\n_{1}$-perturbed algorithm / reweighted-$\\ell _{1}$ algorithm / reweighted-$\\ell\n_{1}$ algorithm. We offer a unifying nonconvex approximation approach, with\nsolid theoretical tools as well as efficient algorithms based on DC programming\nand DCA, to tackle the zero-norm and sparse optimization. As an application, we\nimplemented our methods for the feature selection in SVM (Support Vector\nMachine) problem and performed empirical comparative numerical experiments on\nthe proposed algorithms with various approximation functions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 15:45:05 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:28:33 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Thi", "Hoai An Le", ""], ["Dinh", "Tao Pham", ""], ["Le", "Hoai Minh", ""], ["Vo", "Xuan Thanh", ""]]}, {"id": "1407.0312", "submitter": "Xingguo Li", "authors": "Xingguo Li and Jarvis Haupt", "title": "Identifying Outliers in Large Matrices via Randomized Adaptive\n  Compressive Sampling", "comments": "16 pages, 7 figures, 2 tables, IEEE Transactions on Signal Processing\n  (submitted)", "journal-ref": null, "doi": "10.1109/TSP.2015.2401536", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank, matrix. We propose a simple two-step adaptive sensing and\ninference approach and establish theoretical guarantees for its performance;\nour results show that accurate outlier identification is achievable using very\nfew linear summaries of the original data matrix -- as few as the squared rank\nof the low-rank component plus the number of outliers, times constant and\nlogarithmic factors. We demonstrate the performance of our approach\nexperimentally in two stylized applications, one motivated by robust\ncollaborative filtering tasks, and the other by saliency map estimation tasks\narising in computer vision and automated surveillance, and also investigate\nextensions to settings where the data are noisy, or possibly incomplete.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:37:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 20:51:44 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 02:08:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1407.0316", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama, Felipe Llinares L\\'opez, Niklas Kasenburg, Karsten M.\n  Borgwardt", "title": "Significant Subgraph Mining with Multiple Testing Correction", "comments": "18 pages, 5 figure, accepted to the 2015 SIAM International\n  Conference on Data Mining (SDM15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding itemsets that are statistically significantly enriched\nin a class of transactions is complicated by the need to correct for multiple\nhypothesis testing. Pruning untestable hypotheses was recently proposed as a\nstrategy for this task of significant itemset mining. It was shown to lead to\ngreater statistical power, the discovery of more truly significant itemsets,\nthan the standard Bonferroni correction on real-world datasets. An open\nquestion, however, is whether this strategy of excluding untestable hypotheses\nalso leads to greater statistical power in subgraph mining, in which the number\nof hypotheses is much larger than in itemset mining. Here we answer this\nquestion by an empirical investigation on eight popular graph benchmark\ndatasets. We propose a new efficient search strategy, which always returns the\nsame solution as the state-of-the-art approach and is approximately two orders\nof magnitude faster. Moreover, we exploit the dependence between subgraphs by\nconsidering the effective number of tests and thereby further increase the\nstatistical power.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:53:51 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 13:39:21 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 16:11:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Sugiyama", "Mahito", ""], ["L\u00f3pez", "Felipe Llinares", ""], ["Kasenburg", "Niklas", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1407.0449", "submitter": "Amir-massoud Farahmand", "authors": "Amir-massoud Farahmand, Doina Precup, Andr\\'e M.S. Barreto, Mohammad\n  Ghavamzadeh", "title": "Classification-based Approximate Policy Iteration: Experiments and\n  Extended Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling large approximate dynamic programming or reinforcement learning\nproblems requires methods that can exploit regularities, or intrinsic\nstructure, of the problem in hand. Most current methods are geared towards\nexploiting the regularities of either the value function or the policy. We\nintroduce a general classification-based approximate policy iteration (CAPI)\nframework, which encompasses a large class of algorithms that can exploit\nregularities of both the value function and the policy space, depending on what\nis advantageous. This framework has two main components: a generic value\nfunction estimator and a classifier that learns a policy based on the estimated\nvalue function. We establish theoretical guarantees for the sample complexity\nof CAPI-style algorithms, which allow the policy evaluation step to be\nperformed by a wide variety of algorithms (including temporal-difference-style\nmethods), and can handle nonparametric representations of policies. Our bounds\non the estimation error of the performance loss are tighter than existing\nresults. We also illustrate this approach empirically on several problems,\nincluding a large HIV control task.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 03:19:43 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Farahmand", "Amir-massoud", ""], ["Precup", "Doina", ""], ["Barreto", "Andr\u00e9 M. S.", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1407.0581", "submitter": "Song Liu Dr.", "authors": "Song Liu, Taiji Suzuki, Raissa Relator, Jun Sese, Masashi Sugiyama,\n  Kenji Fukumizu", "title": "Support Consistency of Direct Sparse-Change Learning in Markov Networks", "comments": "Rerun experiments, added a new image change detection experiment.\n  Changed some typos in the proof of Proposition 6 and 11", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning sparse structure changes between two Markov\nnetworks $P$ and $Q$. Rather than fitting two Markov networks separately to two\nsets of data and figuring out their differences, a recent work proposed to\nlearn changes \\emph{directly} via estimating the ratio between two Markov\nnetwork models. In this paper, we give sufficient conditions for\n\\emph{successful change detection} with respect to the sample size $n_p, n_q$,\nthe dimension of data $m$, and the number of changed edges $d$. When using an\nunbounded density ratio model we prove that the true sparse changes can be\nconsistently identified for $n_p = \\Omega(d^2 \\log \\frac{m^2+m}{2})$ and $n_q =\n\\Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error.\nSuch sample complexity can be improved to $\\min(n_p, n_q) = \\Omega(d^2 \\log\n\\frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed.\nOur theoretical guarantee can be applied to a wide range of discrete/continuous\nMarkov networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 14:27:48 GMT"}, {"version": "v10", "created": "Fri, 8 Apr 2016 16:06:57 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 06:31:41 GMT"}, {"version": "v3", "created": "Mon, 27 Oct 2014 15:20:12 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 15:39:25 GMT"}, {"version": "v5", "created": "Fri, 4 Sep 2015 00:46:15 GMT"}, {"version": "v6", "created": "Mon, 4 Jan 2016 13:49:52 GMT"}, {"version": "v7", "created": "Wed, 10 Feb 2016 20:55:01 GMT"}, {"version": "v8", "created": "Sun, 14 Feb 2016 17:55:10 GMT"}, {"version": "v9", "created": "Tue, 16 Feb 2016 20:57:19 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Liu", "Song", ""], ["Suzuki", "Taiji", ""], ["Relator", "Raissa", ""], ["Sese", "Jun", ""], ["Sugiyama", "Masashi", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1407.0611", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (SAMM)", "title": "How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?", "comments": null, "journal-ref": "10th International Workshop on Self Organizing Maps, WSSOM 2014,\n  Mittweida : Germany (2014)", "doi": "10.1007/978-3-319-07695-9_1", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerous applicative contexts, data are too rich and too complex to be\nrepresented by numerical vectors. A general approach to extend machine learning\nand data mining techniques to such data is to really on a dissimilarity or on a\nkernel that measures how different or similar two objects are. This approach\nhas been used to define several variants of the Self Organizing Map (SOM). This\npaper reviews those variants in using a common set of notations in order to\noutline differences and similarities between them. It discusses the advantages\nand drawbacks of the variants, as well as the actual relevance of the\ndissimilarity/kernel SOM for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:31:20 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0612", "submitter": "Fabrice Rossi", "authors": "Marc Boull\\'e, Romain Guigour\\`es (SAMM), Fabrice Rossi (SAMM)", "title": "Nonparametric Hierarchical Clustering of Functional Data", "comments": null, "journal-ref": "Advances in Knowledge Discovery and Management, Guillet, Fabrice\n  and Pinaud, Bruno and Venturini, Gilles and Zighed, Djamel Abdelkader (Ed.)\n  (2014) 15-35", "doi": "10.1007/978-3-319-02999-3_2", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of curves clustering. We propose a\nnonparametric method which partitions the curves into clusters and discretizes\nthe dimensions of the curve points into intervals. The cross-product of these\npartitions forms a data-grid which is obtained using a Bayesian model selection\napproach while making no assumptions regarding the curves. Finally, a\npost-processing technique, aiming at reducing the number of clusters in order\nto improve the interpretability of the clustering, is proposed. It consists in\noptimally merging the clusters step by step, which corresponds to an\nagglomerative hierarchical classification whose dissimilarity measure is the\nvariation of the criterion. Interestingly this measure is none other than the\nsum of the Kullback-Leibler divergences between clusters distributions before\nand after the merges. The practical interest of the approach for functional\ndata exploratory analysis is presented and compared with an alternative\napproach on an artificial and a real world data set.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:32:10 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Boull\u00e9", "Marc", "", "SAMM"], ["Guigour\u00e8s", "Romain", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0726", "submitter": "Yao Xie", "authors": "Yang Cao and Yao Xie", "title": "Fast Algorithm for Low-rank matrix recovery in Poisson noise", "comments": "Presented at IEEE GLOBALSIP2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast algorithm for recovering low-rank matrices from\ntheir linear measurements contaminated with Poisson noise: the Poisson noise\nMaximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose a\nconvex optimization formulation with a cost function consisting of the sum of a\nlikelihood function and a regularization function which the nuclear norm of the\nmatrix. Instead of solving the optimization problem directly by semi-definite\nprogram (SDP), we derive an iterative singular value thresholding algorithm by\nexpanding the likelihood function. We demonstrate the good performance of the\nproposed algorithm on recovery of solar flare images with Poisson noise: the\nalgorithm is more efficient than solving SDP using the interior-point algorithm\nand it generates a good approximate solution compared to that solved from SDP.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 21:27:23 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:11:13 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1407.0731", "submitter": "Yao Xie", "authors": "Gabor Braun, Sebastian Pokutta, and Yao Xie", "title": "Info-Greedy sequential adaptive compressed sensing", "comments": "Preliminary results presented at Allerton Conference 2014. To appear\n  in IEEE Journal Selected Topics on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an information-theoretic framework for sequential adaptive\ncompressed sensing, Info-Greedy Sensing, where measurements are chosen to\nmaximize the extracted information conditioned on the previous measurements. We\nshow that the widely used bisection approach is Info-Greedy for a family of\n$k$-sparse signals by connecting compressed sensing and blackbox complexity of\nsequential query algorithms, and present Info-Greedy algorithms for Gaussian\nand Gaussian Mixture Model (GMM) signals, as well as ways to design sparse\nInfo-Greedy measurements. Numerical examples demonstrate the good performance\nof the proposed algorithms using simulated and real data: Info-Greedy Sensing\nshows significant improvement over random projection for signals with sparse\nand low-rank covariance matrices, and adaptivity brings robustness when there\nis a mismatch between the assumed and the true distributions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 22:03:28 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 03:34:11 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 02:20:52 GMT"}, {"version": "v4", "created": "Mon, 2 Feb 2015 08:10:38 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Braun", "Gabor", ""], ["Pokutta", "Sebastian", ""], ["Xie", "Yao", ""]]}, {"id": "1407.0733", "submitter": "Davide Barbieri", "authors": "Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti", "title": "Cortical spatio-temporal dimensionality reduction for visual grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual systems of many mammals, including humans, is able to integrate\nthe geometric information of visual stimuli and to perform cognitive tasks\nalready at the first stages of the cortical processing. This is thought to be\nthe result of a combination of mechanisms, which include feature extraction at\nsingle cell level and geometric processing by means of cells connectivity. We\npresent a geometric model of such connectivities in the space of detected\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\nused to obtain low-level object segmentation. The main idea is that of defining\na spectral clustering procedure with anisotropic affinities over datasets\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\nNeural plausibility of the proposed arguments will be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 22:07:06 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 16:46:41 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Cocci", "Giacomo", ""], ["Barbieri", "Davide", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1407.0749", "submitter": "Justin Domke", "authors": "Justin Domke and Xianghang Liu", "title": "Projecting Ising Model Parameters for Fast Mixing", "comments": "Advances in Neural Information Processing Systems 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in general Ising models is difficult, due to high treewidth making\ntree-based algorithms intractable. Moreover, when interactions are strong,\nGibbs sampling may take exponential time to converge to the stationary\ndistribution. We present an algorithm to project Ising model parameters onto a\nparameter set that is guaranteed to be fast mixing, under several divergences.\nWe find that Gibbs sampling using the projected parameters is more accurate\nthan with the original parameters when interaction strengths are strong and\nwhen limited time is available for sampling.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:19:08 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 06:30:20 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Domke", "Justin", ""], ["Liu", "Xianghang", ""]]}, {"id": "1407.0753", "submitter": "Ting Kei Pong", "authors": "Guoyin Li, Ting Kei Pong", "title": "Global convergence of splitting methods for nonconvex composite\n  optimization", "comments": "To appear in SIOPT", "journal-ref": null, "doi": "10.1137/140998135", "report-no": null, "categories": "math.OC cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the sum of a smooth function $h$ with a\nbounded Hessian, and a nonsmooth function. We assume that the latter function\nis a composition of a proper closed function $P$ and a surjective linear map\n$\\cal M$, with the proximal mappings of $\\tau P$, $\\tau > 0$, simple to\ncompute. This problem is nonconvex in general and encompasses many important\napplications in engineering and machine learning. In this paper, we examined\ntwo types of splitting methods for solving this nonconvex optimization problem:\nalternating direction method of multipliers and proximal gradient algorithm.\nFor the direct adaptation of the alternating direction method of multipliers,\nwe show that, if the penalty parameter is chosen sufficiently large and the\nsequence generated has a cluster point, then it gives a stationary point of the\nnonconvex problem. We also establish convergence of the whole sequence under an\nadditional assumption that the functions $h$ and $P$ are semi-algebraic.\nFurthermore, we give simple sufficient conditions to guarantee boundedness of\nthe sequence generated. These conditions can be satisfied for a wide range of\napplications including the least squares problem with the $\\ell_{1/2}$\nregularization. Finally, when $\\cal M$ is the identity so that the proximal\ngradient algorithm can be efficiently applied, we show that any cluster point\nis stationary under a slightly more flexible constant step-size rule than what\nis known in the literature for a nonconvex $h$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:29:25 GMT"}, {"version": "v2", "created": "Sat, 2 Aug 2014 13:09:55 GMT"}, {"version": "v3", "created": "Wed, 3 Dec 2014 00:59:38 GMT"}, {"version": "v4", "created": "Mon, 4 May 2015 11:51:54 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 11:32:31 GMT"}, {"version": "v6", "created": "Wed, 4 Nov 2015 02:27:46 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Li", "Guoyin", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1407.0754", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Structured Learning via Logistic Regression", "comments": "Advances in Neural Information Processing Systems 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful approach to structured learning is to write the learning\nobjective as a joint function of linear parameters and inference messages, and\niterate between updates to each. This paper observes that if the inference\nproblem is \"smoothed\" through the addition of entropy terms, for fixed\nmessages, the learning objective reduces to a traditional (non-structured)\nlogistic regression problem with respect to parameters. In these logistic\nregression problems, each training example has a bias term determined by the\ncurrent set of messages. Based on this insight, the structured energy function\ncan be extended from linear factors to any function class where an \"oracle\"\nexists to minimize a logistic loss.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 00:48:34 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1407.0822", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM), B\\'en\\'edicte Le Grand (CRI), Boris\n  Golden (Viadeo), Fabrice Rossi (SAMM)", "title": "Reducing Offline Evaluation Bias in Recommendation Systems", "comments": "23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn\n  2014), Bruxelles : Belgium (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems. They tailor those systems to individual users by filtering and ranking\ninformation according to user profiles. This adaptation process influences the\nway users interact with the system and, as a consequence, increases the\ndifficulty of evaluating a recommendation algorithm with historical data (via\noffline evaluation). This paper analyses this evaluation bias and proposes a\nsimple item weighting solution that reduces its impact. The efficiency of the\nproposed solution is evaluated on real world data extracted from Viadeo\nprofessional social network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 09:05:33 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Golden", "Boris", "", "Viadeo"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.0880", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Anomaly Detection Based on Aggregation of Indicators", "comments": "23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn\n  2014), Bruxelles : Belgium (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic anomaly detection is a major issue in various areas. Beyond mere\ndetection, the identification of the origin of the problem that produced the\nanomaly is also essential. This paper introduces a general methodology that can\nassist human operators who aim at classifying monitoring signals. The main idea\nis to leverage expert knowledge by generating a very large number of\nindicators. A feature selection method is used to keep only the most\ndiscriminant indicators which are used as inputs of a Naive Bayes classifier.\nThe parameters of the classifier have been optimized indirectly by the\nselection process. Simulated data designed to reproduce some of the anomaly\ntypes observed in real world engines.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 12:16:50 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 19:43:54 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.1097", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula, Cynthia Rudin", "title": "Robust Optimization using Machine Learning for Uncertainty Sets", "comments": "28 pages, 2 figures; a shorter preliminary version appeared in ISAIM\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to build robust optimization problems for making decisions based\non complex data from the past. In robust optimization (RO) generally, the goal\nis to create a policy for decision-making that is robust to our uncertainty\nabout the future. In particular, we want our policy to best handle the the\nworst possible situation that could arise, out of an uncertainty set of\npossible situations. Classically, the uncertainty set is simply chosen by the\nuser, or it might be estimated in overly simplistic ways with strong\nassumptions; whereas in this work, we learn the uncertainty set from data\ncollected in the past. The past data are drawn randomly from an (unknown)\npossibly complicated high-dimensional distribution. We propose a new\nuncertainty set design and show how tools from statistical learning theory can\nbe employed to provide probabilistic guarantees on the robustness of the\npolicy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 00:39:00 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1407.1123", "submitter": "Mehrtash Harandi", "authors": "Mehrtash T. Harandi and Mathieu Salzmann and Sadeep Jayasumana and\n  Richard Hartley and Hongdong Li", "title": "Expanding the Family of Grassmannian Kernels: An Embedding Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling videos and image-sets as linear subspaces has proven beneficial for\nmany visual recognition tasks. However, it also incurs challenges arising from\nthe fact that linear subspaces do not obey Euclidean geometry, but lie on a\nspecial type of Riemannian manifolds known as Grassmannian. To leverage the\ntechniques developed for Euclidean spaces (e.g, support vector machines) with\nsubspaces, several recent studies have proposed to embed the Grassmannian into\na Hilbert space by making use of a positive definite kernel. Unfortunately,\nonly two Grassmannian kernels are known, none of which -as we will show- is\nuniversal, which limits their ability to approximate a target function\narbitrarily well. Here, we introduce several positive definite Grassmannian\nkernels, including universal ones, and demonstrate their superiority over\npreviously-known kernels in various tasks, such as classification, clustering,\nsparse coding and hashing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:34:38 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Salzmann", "Mathieu", ""], ["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Li", "Hongdong", ""]]}, {"id": "1407.1176", "submitter": "Felipe Llinares", "authors": "Felipe Llinares, Mahito Sugiyama, Karsten M. Borgwardt", "title": "Identifying Higher-order Combinations of Binary Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding statistically significant interactions between binary variables is\ncomputationally and statistically challenging in high-dimensional settings, due\nto the combinatorial explosion in the number of hypotheses. Terada et al.\nrecently showed how to elegantly address this multiple testing problem by\nexcluding non-testable hypotheses. Still, it remains unclear how their approach\nscales to large datasets.\n  We here proposed strategies to speed up the approach by Terada et al. and\nevaluate them thoroughly in 11 real-world benchmark datasets. We observe that\none approach, incremental search with early stopping, is orders of magnitude\nfaster than the current state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 10:17:43 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Llinares", "Felipe", ""], ["Sugiyama", "Mahito", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1407.1339", "submitter": "Tejas Kulkarni", "authors": "Tejas D. Kulkarni and Vikash K. Mansinghka and Pushmeet Kohli and\n  Joshua B. Tenenbaum", "title": "Inverse Graphics with Probabilistic CAD Models", "comments": "For correspondence, contact tejask@mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recently, multiple formulations of vision problems as probabilistic\ninversions of generative models based on computer graphics have been proposed.\nHowever, applications to 3D perception from natural images have focused on\nlow-dimensional latent scenes, due to challenges in both modeling and\ninference. Accounting for the enormous variability in 3D object shape and 2D\nappearance via realistic generative models seems intractable, as does inverting\neven simple versions of the many-to-many computations that link 3D scenes to 2D\nimages. This paper proposes and evaluates an approach that addresses key\naspects of both these challenges. We show that it is possible to solve\nchallenging, real-world 3D vision problems by approximate inference in\ngenerative models for images based on rendering the outputs of probabilistic\nCAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D\nmeshes corresponding to plausible objects and apply affine transformations to\nplace them in a scene. Image likelihoods are based on similarity in a feature\nspace based on standard mid-level image representations from the vision\nliterature. Our inference algorithm integrates single-site and locally blocked\nMetropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative\ndata-driven proposals learned from training data generated from our models. We\napply this approach to 3D human pose estimation and object shape reconstruction\nfrom single images, achieving quantitative and qualitative performance\nimprovements over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 23:03:25 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Mansinghka", "Vikash K.", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1407.1537", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "comments": "A new section added; polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order methods play a central role in large-scale machine learning. Even\nthough many variations exist, each suited to a particular problem, almost all\nsuch methods fundamentally rely on two types of algorithmic steps: gradient\ndescent, which yields primal progress, and mirror descent, which yields dual\nprogress.\n  We observe that the performances of gradient and mirror descent are\ncomplementary, so that faster algorithms can be designed by LINEARLY COUPLING\nthe two. We show how to reconstruct Nesterov's accelerated gradient methods\nusing linear coupling, which gives a cleaner interpretation than Nesterov's\noriginal proofs. We also discuss the power of linear coupling by extending it\nto many other settings that Nesterov's methods cannot apply to.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:11:48 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 01:48:01 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 06:59:10 GMT"}, {"version": "v4", "created": "Fri, 2 Jan 2015 17:41:24 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 19:30:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.1543", "submitter": "David Steurer", "authors": "Boaz Barak, Jonathan A. Kelner, David Steurer", "title": "Dictionary Learning and Tensor Decomposition via the Sum-of-Squares\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new approach to the dictionary learning (also known as \"sparse\ncoding\") problem of recovering an unknown $n\\times m$ matrix $A$ (for $m \\geq\nn$) from examples of the form \\[ y = Ax + e, \\] where $x$ is a random vector in\n$\\mathbb R^m$ with at most $\\tau m$ nonzero coordinates, and $e$ is a random\nnoise vector in $\\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,\nour algorithm recovers every column of $A$ within arbitrarily good constant\naccuracy in time $m^{O(\\log m/\\log(\\tau^{-1}))}$, in particular achieving\npolynomial time if $\\tau = m^{-\\delta}$ for any $\\delta>0$, and time $m^{O(\\log\nm)}$ if $\\tau$ is (a sufficiently small) constant. Prior algorithms with\ncomparable assumptions on the distribution required the vector $x$ to be much\nsparser---at most $\\sqrt{n}$ nonzero coordinates---and there were intrinsic\nbarriers preventing these algorithms from applying for denser $x$.\n  We achieve this by designing an algorithm for noisy tensor decomposition that\ncan recover, under quite general conditions, an approximate rank-one\ndecomposition of a tensor $T$, given access to a tensor $T'$ that is\n$\\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our\nknowledge, this is the first algorithm for tensor decomposition that works in\nthe constant spectral-norm noise regime, where there is no guarantee that the\nlocal optima of $T$ and $T'$ have similar structures.\n  Our algorithm is based on a novel approach to using and analyzing the Sum of\nSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and\nit can be viewed as an indication of the utility of this very general and\npowerful tool for unsupervised learning problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:42:05 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 21:32:44 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Barak", "Boaz", ""], ["Kelner", "Jonathan A.", ""], ["Steurer", "David", ""]]}, {"id": "1407.1598", "submitter": "Gabriel Peyre", "authors": "Samuel Vaiter (CEREMADE), Gabriel Peyr\\'e (CEREMADE), Jalal M. Fadili\n  (GREYC)", "title": "Low Complexity Regularization of Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems and regularization theory is a central theme in contemporary\nsignal processing, where the goal is to reconstruct an unknown signal from\npartial indirect, and possibly noisy, measurements of it. A now standard method\nfor recovering the unknown signal is to solve a convex optimization problem\nthat enforces some prior knowledge about its structure. This has proved\nefficient in many problems routinely encountered in imaging sciences,\nstatistics and machine learning. This chapter delivers a review of recent\nadvances in the field where the regularization prior promotes solutions\nconforming to some notion of simplicity/low-complexity. These priors encompass\nas popular examples sparsity and group sparsity (to capture the compressibility\nof natural signals and images), total variation and analysis sparsity (to\npromote piecewise regularity), and low-rank (as natural extension of sparsity\nto matrix-valued data). Our aim is to provide a unified treatment of all these\nregularizations under a single umbrella, namely the theory of partial\nsmoothness. This framework is very general and accommodates all low-complexity\nregularizers just mentioned, as well as many others. Partial smoothness turns\nout to be the canonical way to encode low-dimensional models that can be linear\nspaces or more general smooth manifolds. This review is intended to serve as a\none stop shop toward the understanding of the theoretical properties of the\nso-regularized solutions. It covers a large spectrum including: (i) recovery\nguarantees and stability to noise, both in terms of $\\ell^2$-stability and\nmodel (manifold) identification; (ii) sensitivity analysis to perturbations of\nthe parameters involved (in particular the observations), with applications to\nunbiased risk estimation ; (iii) convergence properties of the forward-backward\nproximal splitting scheme, that is particularly well suited to solve the\ncorresponding large-scale regularized optimization problem.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 06:59:19 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 14:05:43 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Vaiter", "Samuel", "", "CEREMADE"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"], ["Fadili", "Jalal M.", "", "GREYC"]]}, {"id": "1407.1870", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka and Taiji Suzuki", "title": "Spectral norm of random tensors", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the spectral norm of a random $n_1\\times n_2\\times \\cdots \\times\nn_K$ tensor (or higher-order array) scales as\n$O\\left(\\sqrt{(\\sum_{k=1}^{K}n_k)\\log(K)}\\right)$ under some sub-Gaussian\nassumption on the entries. The proof is based on a covering number argument.\nSince the spectral norm is dual to the tensor nuclear norm (the tightest convex\nrelaxation of the set of rank one tensors), the bound implies that the convex\nrelaxation yields sample complexity that is linear in (the sum of) the number\nof dimensions, which is much smaller than other recently proposed convex\nrelaxations of tensor rank that use unfolding.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 20:30:06 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1407.1890", "submitter": "Michael Smith", "authors": "Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, Tony\n  Martinez", "title": "Recommending Learning Algorithms and Their Associated Hyperparameters", "comments": "Short paper--2 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of machine learning on a given task dependson, among other\nthings, which learning algorithm is selected and its associated\nhyperparameters. Selecting an appropriate learning algorithm and setting its\nhyperparameters for a given data set can be a challenging task, especially for\nusers who are not experts in machine learning. Previous work has examined using\nmeta-features to predict which learning algorithm and hyperparameters should be\nused. However, choosing a set of meta-features that are predictive of algorithm\nperformance is difficult. Here, we propose to apply collaborative filtering\ntechniques to learning algorithm and hyperparameter selection, and find that\ndoing so avoids determining which meta-features to use and outperforms\ntraditional meta-learning approaches in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 21:23:42 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Smith", "Michael R.", ""], ["Mitchell", "Logan", ""], ["Giraud-Carrier", "Christophe", ""], ["Martinez", "Tony", ""]]}, {"id": "1407.2256", "submitter": "Rafael Chaves", "authors": "R. Chaves, L. Luft, T. O. Maciel, D. Gross, D. Janzing, B. Sch\\\"olkopf", "title": "Inferring latent structures via information inequalities", "comments": "10 pages + appendix, 5 figures. To appear in Proceedings of the 30th\n  Conference on Uncertainty in Artificial Intelligence (UAI2014)", "journal-ref": "Proceedings of the 30th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2014), pp. 112 - 121, AUAI Press, 2014", "doi": null, "report-no": null, "categories": "stat.ML quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the goals of probabilistic inference is to decide whether an\nempirically observed distribution is compatible with a candidate Bayesian\nnetwork. However, Bayesian networks with hidden variables give rise to highly\nnon-trivial constraints on the observed distribution. Here, we propose an\ninformation-theoretic approach, based on the insight that conditions on\nentropies of Bayesian networks take the form of simple linear inequalities. We\ndescribe an algorithm for deriving entropic tests for latent structures. The\nwell-known conditional independence tests appear as a special case. While the\napproach applies for generic Bayesian networks, we presently adopt the causal\nview, and show the versatility of the framework by treating several relevant\nproblems from that domain: detecting common ancestors, quantifying the strength\nof causal influence, and inferring the direction of causation from two-variable\nmarginals.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 20:00:17 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Chaves", "R.", ""], ["Luft", "L.", ""], ["Maciel", "T. O.", ""], ["Gross", "D.", ""], ["Janzing", "D.", ""], ["Sch\u00f6lkopf", "B.", ""]]}, {"id": "1407.2433", "submitter": "Peter Foster", "authors": "Peter Foster, Simon Dixon, Anssi Klapuri", "title": "Identifying Cover Songs Using Information-Theoretic Measures of\n  Similarity", "comments": "13 pages, 5 figures, 4 tables. v3: Accepted version", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 23 no. 6, pp. 993-1005, 2015", "doi": "10.1109/TASLP.2015.2416655", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates methods for quantifying similarity between audio\nsignals, specifically for the task of of cover song detection. We consider an\ninformation-theoretic approach, where we compute pairwise measures of\npredictability between time series. We compare discrete-valued approaches\noperating on quantised audio features, to continuous-valued approaches. In the\ndiscrete case, we propose a method for computing the normalised compression\ndistance, where we account for correlation between time series. In the\ncontinuous case, we propose to compute information-based measures of similarity\nas statistics of the prediction error between time series. We evaluate our\nmethods on two cover song identification tasks using a data set comprised of\n300 Jazz standards and using the Million Song Dataset. For both datasets, we\nobserve that continuous-valued approaches outperform discrete-valued\napproaches. We consider approaches to estimating the normalised compression\ndistance (NCD) based on string compression and prediction, where we observe\nthat our proposed normalised compression distance with alignment (NCDA)\nimproves average performance over NCD, for sequential compression algorithms.\nFinally, we demonstrate that continuous-valued distances may be combined to\nimprove performance with respect to baseline approaches. Using a large-scale\nfilter-and-refine approach, we demonstrate state-of-the-art performance for\ncover song identification using the Million Song Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 11:04:15 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 00:53:47 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 15:53:43 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Foster", "Peter", ""], ["Dixon", "Simon", ""], ["Klapuri", "Anssi", ""]]}, {"id": "1407.2483", "submitter": "Shyam Visweswaran", "authors": "Shyam Visweswaran and Gregory F. Cooper", "title": "Counting Markov Blanket Structures", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Learning Markov blanket (MB) structures has proven useful in performing\nfeature selection, learning Bayesian networks (BNs), and discovering causal\nrelationships. We present a formula for efficiently determining the number of\nMB structures given a target variable and a set of other variables. As\nexpected, the number of MB structures grows exponentially. However, we show\nquantitatively that there are many fewer MB structures that contain the target\nvariable than there are BN structures that contain it. In particular, the ratio\nof BN structures to MB structures appears to increase exponentially in the\nnumber of variables.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 14:02:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2014 17:23:57 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Visweswaran", "Shyam", ""], ["Cooper", "Gregory F.", ""]]}, {"id": "1407.2490", "submitter": "Zai Yang", "authors": "Zai Yang and Lihua Xie", "title": "On Gridless Sparse Methods for Line Spectral Estimation From Complete\n  and Incomplete Data", "comments": "15 pages, double-column, 7 figures, accepted by IEEE Transaction on\n  Signal Processing in March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned about sparse, continuous frequency estimation in line\nspectral estimation, and focused on developing gridless sparse methods which\novercome grid mismatches and correspond to limiting scenarios of existing\ngrid-based approaches, e.g., $\\ell_1$ optimization and SPICE, with an\ninfinitely dense grid. We generalize AST (atomic-norm soft thresholding) to the\ncase of nonconsecutively sampled data (incomplete data) inspired by recent\natomic norm based techniques. We present a gridless version of SPICE (gridless\nSPICE, or GLS), which is applicable to both complete and incomplete data\nwithout the knowledge of noise level. We further prove the equivalence between\nGLS and atomic norm-based techniques under different assumptions of noise.\nMoreover, we extend GLS to a systematic framework consisting of model order\nselection and robust frequency estimation, and present feasible algorithms for\nAST and GLS. Numerical simulations are provided to validate our theoretical\nanalysis and demonstrate performance of our methods compared to existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 14:22:22 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 14:51:31 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yang", "Zai", ""], ["Xie", "Lihua", ""]]}, {"id": "1407.2646", "submitter": "Yura Perov N", "authors": "Yura N. Perov, Frank D. Wood", "title": "Learning Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for generalising from data in which models are\nsamplers represented as program text. We establish encouraging empirical\nresults that suggest that Markov chain Monte Carlo probabilistic programming\ninference techniques coupled with higher-order probabilistic programming\nlanguages are now sufficiently powerful to enable successful inference of this\nkind in nontrivial domains. We also introduce a new notion of probabilistic\nprogram compilation and show how the same machinery might be used in the future\nto compile probabilistic programs for efficient reusable predictive inference.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 22:06:18 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Perov", "Yura N.", ""], ["Wood", "Frank D.", ""]]}, {"id": "1407.2657", "submitter": "Chicheng Zhang", "authors": "Chicheng Zhang and Kamalika Chaudhuri", "title": "Beyond Disagreement-based Agnostic Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study agnostic active learning, where the goal is to learn a classifier in\na pre-specified hypothesis class interactively with as few label queries as\npossible, while making no assumptions on the true function generating the\nlabels. The main algorithms for this problem are {\\em{disagreement-based active\nlearning}}, which has a high label requirement, and {\\em{margin-based active\nlearning}}, which only applies to fairly restricted settings. A major challenge\nis to find an algorithm which achieves better label complexity, is consistent\nin an agnostic setting, and applies to general classification problems.\n  In this paper, we provide such an algorithm. Our solution is based on two\nnovel contributions -- a reduction from consistent active learning to\nconfidence-rated prediction with guaranteed error, and a novel confidence-rated\npredictor.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 00:34:16 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 23:35:49 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Zhang", "Chicheng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1407.2674", "submitter": "Kobbi Nissim", "authors": "Amos Beimel, Kobbi Nissim, Uri Stemmer", "title": "Private Learning and Sanitization: Pure vs. Approximate Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the sample complexity of private learning [Kasiviswanathan et al.\n2008] and sanitization~[Blum et al. 2008] under pure $\\epsilon$-differential\nprivacy [Dwork et al. TCC 2006] and approximate\n$(\\epsilon,\\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We show\nthat the sample complexity of these tasks under approximate differential\nprivacy can be significantly lower than that under pure differential privacy.\n  We define a family of optimization problems, which we call Quasi-Concave\nPromise Problems, that generalizes some of our considered tasks. We observe\nthat a quasi-concave promise problem can be privately approximated using a\nsolution to a smaller instance of a quasi-concave promise problem. This allows\nus to construct an efficient recursive algorithm solving such problems\nprivately. Specifically, we construct private learners for point functions,\nthreshold functions, and axis-aligned rectangles in high dimension. Similarly,\nwe construct sanitizers for point functions and threshold functions.\n  We also examine the sample complexity of label-private learners, a relaxation\nof private learning where the learner is required to only protect the privacy\nof the labels in the sample. We show that the VC dimension completely\ncharacterizes the sample complexity of such learners, that is, the sample\ncomplexity of learning with label privacy is equal (up to constants) to\nlearning without privacy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 01:42:44 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1407.2676", "submitter": "Peter Frazier", "authors": "Ilya O. Ryzhov and Peter I. Frazier and Warren B. Powell", "title": "A New Optimal Stepsize For Approximate Dynamic Programming", "comments": "Matlab files are included with the paper source", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate dynamic programming (ADP) has proven itself in a wide range of\napplications spanning large-scale transportation problems, health care, revenue\nmanagement, and energy systems. The design of effective ADP algorithms has many\ndimensions, but one crucial factor is the stepsize rule used to update a value\nfunction approximation. Many operations research applications are\ncomputationally intensive, and it is important to obtain good results quickly.\nFurthermore, the most popular stepsize formulas use tunable parameters and can\nproduce very poor results if tuned improperly. We derive a new stepsize rule\nthat optimizes the prediction error in order to improve the short-term\nperformance of an ADP algorithm. With only one, relatively insensitive tunable\nparameter, the new rule adapts to the level of noise in the problem and\nproduces faster convergence in numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 02:34:15 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 00:24:14 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Ryzhov", "Ilya O.", ""], ["Frazier", "Peter I.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1407.2697", "submitter": "Aaron Defazio Mr", "authors": "Aaron J. Defazio and Tiberio S. Caetano", "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular\n  Relaxation", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 25 (NIPS 2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in statistics and machine learning is the determination of\nnetwork structure from data. We consider the case where the structure of the\ngraph to be reconstructed is known to be scale-free. We show that in such cases\nit is natural to formulate structured sparsity inducing priors using submodular\nfunctions, and we use their Lov\\'asz extension to obtain a convex relaxation.\nFor tractable classes such as Gaussian graphical models, this leads to a convex\noptimization problem that can be efficiently solved. We show that our method\nresults in an improvement in the accuracy of reconstructed networks for\nsynthetic data. We also show how our prior encourages scale-free\nreconstructions on a bioinfomatics dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 05:45:17 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Defazio", "Aaron J.", ""], ["Caetano", "Tiberio S.", ""]]}, {"id": "1407.2710", "submitter": "Aaron Defazio Mr", "authors": "Aaron J. Defazio and Tib\\'erio S. Caetano and Justin Domke", "title": "Finito: A Faster, Permutable Incremental Gradient Method for Big Data\n  Problems", "comments": null, "journal-ref": "International Conference on Machine Learning 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in optimization theory have shown that smooth strongly convex\nfinite sums can be minimized faster than by treating them as a black box\n\"batch\" problem. In this work we introduce a new method in this class with a\ntheoretical convergence rate four times faster than existing methods, for sums\nwith sufficiently many terms. This method is also amendable to a sampling\nwithout replacement scheme that in practice gives further speed-ups. We give\nempirical results showing state of the art performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 07:01:31 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Defazio", "Aaron J.", ""], ["Caetano", "Tib\u00e9rio S.", ""], ["Domke", "Justin", ""]]}, {"id": "1407.2724", "submitter": "Jonathan Rosenblatt", "authors": "Jonathan Rosenblatt, Boaz Nadler", "title": "On the Optimality of Averaging in Distributed Statistical Learning", "comments": "Major changes from previous version. Particularly on the second order\n  error approximation and implications", "journal-ref": null, "doi": "10.1093/imaiai/iaw013", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to statistical learning with big-data is to randomly split\nit among $m$ machines and learn the parameter of interest by averaging the $m$\nindividual estimates. In this paper, focusing on empirical risk minimization,\nor equivalently M-estimation, we study the statistical error incurred by this\nstrategy. We consider two large-sample settings: First, a classical setting\nwhere the number of parameters $p$ is fixed, and the number of samples per\nmachine $n\\to\\infty$. Second, a high-dimensional regime where both\n$p,n\\to\\infty$ with $p/n \\to \\kappa \\in (0,1)$. For both regimes and under\nsuitable assumptions, we present asymptotically exact expressions for this\nestimation error. In the fixed-$p$ setting, under suitable assumptions, we\nprove that to leading order averaging is as accurate as the centralized\nsolution. We also derive the second order error terms, and show that these can\nbe non-negligible, notably for non-linear models. The high-dimensional setting,\nin contrast, exhibits a qualitatively different behavior: data splitting incurs\na first-order accuracy loss, which to leading order increases linearly with the\nnumber of machines. The dependence of our error approximations on the number of\nmachines traces an interesting accuracy-complexity tradeoff, allowing the\npractitioner an informed choice on the number of machines to deploy. Finally,\nwe confirm our theoretical analysis with several simulations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 08:25:49 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 15:14:47 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Rosenblatt", "Jonathan", ""], ["Nadler", "Boaz", ""]]}, {"id": "1407.2806", "submitter": "Preux Philippe", "authors": "J\\'er\\'emie Mary (INRIA Lille - Nord Europe, LIFL), Romaric Gaudel\n  (INRIA Lille - Nord Europe, LIFL), Preux Philippe (INRIA Lille - Nord Europe,\n  LIFL)", "title": "Bandits Warm-up Cold Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8563", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the cold start problem in recommendation systems assuming no\ncontextual information is available neither about users, nor items. We consider\nthe case in which we only have access to a set of ratings of items by users.\nMost of the existing works consider a batch setting, and use cross-validation\nto tune parameters. The classical method consists in minimizing the root mean\nsquare error over a training subset of the ratings which provides a\nfactorization of the matrix of ratings, interpreted as a latent representation\nof items and users. Our contribution in this paper is 5-fold. First, we\nexplicit the issues raised by this kind of batch setting for users or items\nwith very few ratings. Then, we propose an online setting closer to the actual\nuse of recommender systems; this setting is inspired by the bandit framework.\nThe proposed methodology can be used to turn any recommender system dataset\n(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit a\nstrong and insightful link between contextual bandit algorithms and matrix\nfactorization; this leads us to a new algorithm that tackles the\nexploration/exploitation dilemma associated to the cold start problem in a\nstrikingly new perspective. Finally, experimental evidence confirm that our\nalgorithm is effective in dealing with the cold start problem on publicly\navailable datasets. Overall, the goal of this paper is to bridge the gap\nbetween recommender systems based on matrix factorizations and those based on\ncontextual bandits.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 14:32:37 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Mary", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe, LIFL"], ["Gaudel", "Romaric", "", "INRIA Lille - Nord Europe, LIFL"], ["Philippe", "Preux", "", "INRIA Lille - Nord Europe,\n  LIFL"]]}, {"id": "1407.2812", "submitter": "Ming Yuan", "authors": "T. Tony Cai and Ming Yuan", "title": "Rate-Optimal Detection of Very Short Signal Segments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a range of applications in engineering and genomics, we consider\nin this paper detection of very short signal segments in three settings:\nsignals with known shape, arbitrary signals, and smooth signals. Optimal rates\nof detection are established for the three cases and rate-optimal detectors are\nconstructed. The detectors are easily implementable and are based on scanning\nwith linear and quadratic statistics. Our analysis reveals both similarities\nand differences in the strategy and fundamental difficulty of detection among\nthese three settings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 14:48:03 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Cai", "T. Tony", ""], ["Yuan", "Ming", ""]]}, {"id": "1407.2864", "submitter": "Brooks Paige", "authors": "Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh", "title": "Asynchronous Anytime Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sequential Monte Carlo algorithm we call the particle\ncascade. The particle cascade is an asynchronous, anytime alternative to\ntraditional particle filtering algorithms. It uses no barrier synchronizations\nwhich leads to improved particle throughput and memory efficiency. It is an\nanytime algorithm in the sense that it can be run forever to emit an unbounded\nnumber of particles while keeping within a fixed memory budget. We prove that\nthe particle cascade is an unbiased marginal likelihood estimator which means\nthat it can be straightforwardly plugged into existing pseudomarginal methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 17:04:38 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Paige", "Brooks", ""], ["Wood", "Frank", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1407.2904", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "An eigenanalysis of data centering in machine learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many pattern recognition methods rely on statistical information from\ncentered data, with the eigenanalysis of an empirical central moment, such as\nthe covariance matrix in principal component analysis (PCA), as well as partial\nleast squares regression, canonical-correlation analysis and Fisher\ndiscriminant analysis. Recently, many researchers advocate working on\nnon-centered data. This is the case for instance with the singular value\ndecomposition approach, with the (kernel) entropy component analysis, with the\ninformation-theoretic learning framework, and even with nonnegative matrix\nfactorization. Moreover, one can also consider a non-centered PCA by using the\nsecond-order non-central moment.\n  The main purpose of this paper is to bridge the gap between these two\nviewpoints in designing machine learning methods. To provide a study at the\ncornerstone of kernel-based machines, we conduct an eigenanalysis of the inner\nproduct matrices from centered and non-centered data. We derive several results\nconnecting their eigenvalues and their eigenvectors. Furthermore, we explore\nthe outer product matrices, by providing several results connecting the largest\neigenvectors of the covariance matrix and its non-centered counterpart. These\nresults lay the groundwork to several extensions beyond conventional centering,\nwith the weighted mean shift, the rank-one update, and the multidimensional\nscaling. Experiments conducted on simulated and real data illustrate the\nrelevance of this work.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:04:49 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1407.3010", "submitter": "Eric Bair", "authors": "Qian Liu, Guanhua Chen, Michael R. Kosorok, and Eric Bair", "title": "Biclustering Via Sparse Clustering", "comments": "40 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations it is desirable to identify clusters that differ with\nrespect to only a subset of features. Such clusters may represent homogeneous\nsubgroups of patients with a disease, such as cancer or chronic pain. We define\na bicluster to be a submatrix U of a larger data matrix X such that the\nfeatures and observations in U differ from those not contained in U. For\nexample, the observations in U could have different means or variances with\nrespect to the features in U. We propose a general framework for biclustering\nbased on the sparse clustering method of Witten and Tibshirani (2010). We\ndevelop a method for identifying features that belong to biclusters. This\nframework can be used to identify biclusters that differ with respect to the\nmeans of the features, the variance of the features, or more general\ndifferences. We apply these methods to several simulated and real-world data\nsets and compare the results of our method with several previously published\nmethods. The results of our method compare favorably with existing methods with\nrespect to both predictive accuracy and computing time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 02:56:40 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Liu", "Qian", ""], ["Chen", "Guanhua", ""], ["Kosorok", "Michael R.", ""], ["Bair", "Eric", ""]]}, {"id": "1407.3242", "submitter": "Marcello La Rocca", "authors": "Marcello La Rocca", "title": "Density Adaptive Parallel Clustering", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are going to introduce a new nearest neighbours based\napproach to clustering, and compare it with previous solutions; the resulting\nalgorithm, which takes inspiration from both DBscan and minimum spanning tree\napproaches, is deterministic but proves simpler, faster and doesnt require to\nset in advance a value for k, the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 18:24:15 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["La Rocca", "Marcello", ""]]}, {"id": "1407.3289", "submitter": "Stefan Wager", "authors": "Stefan Wager, William Fithian, Sida Wang, and Percy Liang", "title": "Altitude Training: Strong Bounds for Single-Layer Dropout", "comments": "Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout training, originally designed for deep neural networks, has been\nsuccessful on high-dimensional single-layer natural language tasks. This paper\nproposes a theoretical explanation for this phenomenon: we show that, under a\ngenerative Poisson topic model with long documents, dropout training improves\nthe exponent in the generalization bound for empirical risk minimization.\nDropout achieves this gain much like a marathon runner who practices at\naltitude: once a classifier learns to perform reasonably well on training\nexamples that have been artificially corrupted by dropout, it will do very well\non the uncorrupted test set. We also show that, under similar conditions,\ndropout preserves the Bayes decision boundary and should therefore induce\nminimal bias in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 20:32:34 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 18:30:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Wager", "Stefan", ""], ["Fithian", "William", ""], ["Wang", "Sida", ""], ["Liang", "Percy", ""]]}, {"id": "1407.3422", "submitter": "Igor Melnyk", "authors": "Igor Melnyk and Arindam Banerjee", "title": "A Spectral Algorithm for Inference in Hidden Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden semi-Markov models (HSMMs) are latent variable models which allow\nlatent state persistence and can be viewed as a generalization of the popular\nhidden Markov models (HMMs). In this paper, we introduce a novel spectral\nalgorithm to perform inference in HSMMs. Unlike expectation maximization (EM),\nour approach correctly estimates the probability of given observation sequence\nbased on a set of training sequences. Our approach is based on estimating\nmoments from the sample, whose number of dimensions depends only\nlogarithmically on the maximum length of the hidden state persistence.\nMoreover, the algorithm requires only a few matrix inversions and is therefore\ncomputationally efficient. Empirical evaluations on synthetic and real data\ndemonstrate the advantage of the algorithm over EM in terms of speed and\naccuracy, especially for large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 23:57:07 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 19:29:03 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 00:29:23 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1407.3619", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy and Aarti Singh", "title": "On the Power of Adaptivity in Matrix Completion and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the related tasks of matrix completion and matrix approximation\nfrom missing data and propose adaptive sampling procedures for both problems.\nWe show that adaptive sampling allows one to eliminate standard incoherence\nassumptions on the matrix row space that are necessary for passive sampling\nprocedures. For exact recovery of a low-rank matrix, our algorithm judiciously\nselects a few columns to observe in full and, with few additional measurements,\nprojects the remaining columns onto their span. This algorithm exactly recovers\nan $n \\times n$ rank $r$ matrix using $O(nr\\mu_0 \\log^2(r))$ observations,\nwhere $\\mu_0$ is a coherence parameter on the column space of the matrix. In\naddition to completely eliminating any row space assumptions that have pervaded\nthe literature, this algorithm enjoys a better sample complexity than any\nexisting matrix completion algorithm. To certify that this improvement is due\nto adaptive sampling, we establish that row space coherence is necessary for\npassive sampling algorithms to achieve non-trivial sample complexity bounds.\n  For constructing a low-rank approximation to a high-rank input matrix, we\npropose a simple algorithm that thresholds the singular values of a zero-filled\nversion of the input matrix. The algorithm computes an approximation that is\nnearly as good as the best rank-$r$ approximation using $O(nr\\mu \\log^2(n))$\nsamples, where $\\mu$ is a slightly different coherence parameter on the matrix\ncolumns. Again we eliminate assumptions on the row space.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 12:14:08 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1407.3716", "submitter": "Mohammadreza Malek-Mohammadi", "authors": "Mohammadreza Malek-Mohammadi, Massoud Babaie-Zadeh, and Mikael\n  Skoglund", "title": "Performance Guarantees for Schatten-$p$ Quasi-Norm Minimization in\n  Recovery of Low-Rank Matrices", "comments": "Submitted to Elsevier Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address some theoretical guarantees for Schatten-$p$ quasi-norm\nminimization ($p \\in (0,1]$) in recovering low-rank matrices from compressed\nlinear measurements. Firstly, using null space properties of the measurement\noperator, we provide a sufficient condition for exact recovery of low-rank\nmatrices. This condition guarantees unique recovery of matrices of ranks equal\nor larger than what is guaranteed by nuclear norm minimization. Secondly, this\nsufficient condition leads to a theorem proving that all restricted isometry\nproperty (RIP) based sufficient conditions for $\\ell_p$ quasi-norm minimization\ngeneralize to Schatten-$p$ quasi-norm minimization. Based on this theorem, we\nprovide a few RIP-based recovery conditions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 16:37:51 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 15:59:54 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Malek-Mohammadi", "Mohammadreza", ""], ["Babaie-Zadeh", "Massoud", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1407.4070", "submitter": "Mary Wootters", "authors": "Moritz Hardt and Mary Wootters", "title": "Fast matrix completion without the condition number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for Matrix Completion whose running time and\nsample complexity is polynomial in the rank of the unknown target matrix,\nlinear in the dimension of the matrix, and logarithmic in the condition number\nof the matrix. To the best of our knowledge, all previous algorithms either\nincurred a quadratic dependence on the condition number of the unknown matrix\nor a quadratic dependence on the dimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which\nwe show has theoretical guarantees under standard assumptions even in the\npresence of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 17:47:44 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hardt", "Moritz", ""], ["Wootters", "Mary", ""]]}, {"id": "1407.4137", "submitter": "Eric Jonas", "authors": "Eric Jonas and Konrad Kording", "title": "Automatic discovery of cell types and microcircuitry from neural\n  connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural connectomics has begun producing massive amounts of data,\nnecessitating new analysis methods to discover the biological and computational\nstructure. It has long been assumed that discovering neuron types and their\nrelation to microcircuitry is crucial to understanding neural function. Here we\ndeveloped a nonparametric Bayesian technique that identifies neuron types and\nmicrocircuitry patterns in connectomics data. It combines the information\ntraditionally used by biologists, including connectivity, cell body location\nand the spatial distribution of synapses, in a principled and\nprobabilistically-coherent manner. We show that the approach recovers known\nneuron types in the retina and enables predictions of connectivity, better than\nsimpler algorithms. It also can reveal interesting structure in the nervous\nsystem of C. elegans, and automatically discovers the structure of a\nmicroprocessor. Our approach extracts structural meaning from connectomics,\nenabling new approaches of automatically deriving anatomical insights from\nthese emerging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 20:14:05 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Jonas", "Eric", ""], ["Kording", "Konrad", ""]]}, {"id": "1407.4139", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega", "title": "Subjectivity, Bayesianism, and Causality", "comments": "21 pages, 21 figures. Submitted to Special Issue of Pattern\n  Recognition Letters on \"Philosophical aspects of pattern recognition\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian probability theory is one of the most successful frameworks to model\nreasoning under uncertainty. Its defining property is the interpretation of\nprobabilities as degrees of belief in propositions about the state of the world\nrelative to an inquiring subject. This essay examines the notion of\nsubjectivity by drawing parallels between Lacanian theory and Bayesian\nprobability theory, and concludes that the latter must be enriched with causal\ninterventions to model agency. The central contribution of this work is an\nabstract model of the subject that accommodates causal interventions in a\nmeasure-theoretic formalisation. This formalisation is obtained through a\ngame-theoretic Ansatz based on modelling the inside and outside of the subject\nas an extensive-form game with imperfect information between two players.\nFinally, I illustrate the expressiveness of this model with an example of\ncausal induction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 20:16:10 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 03:51:42 GMT"}, {"version": "v3", "created": "Mon, 16 Feb 2015 21:27:16 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 19:59:32 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Ortega", "Pedro A.", ""]]}, {"id": "1407.4211", "submitter": "Maria Lomeli Miss", "authors": "Mar\\'ia Lomel\\'i, Stefano Favaro, Yee Whye Teh", "title": "A marginal sampler for $\\sigma$-Stable Poisson-Kingman mixture models", "comments": "New algorithmic performance comparisons were added", "journal-ref": null, "doi": "10.1080/10618600.2015.1110526", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the class of $\\sigma$-stable Poisson-Kingman random\nprobability measures (RPMs) in the context of Bayesian nonparametric mixture\nmodeling. This is a large class of discrete RPMs which encompasses most of the\nthe popular discrete RPMs used in Bayesian nonparametrics, such as the\nDirichlet process, Pitman-Yor process, the normalized inverse Gaussian process\nand the normalized generalized Gamma process. We show how certain sampling\nproperties and marginal characterizations of $\\sigma$-stable Poisson-Kingman\nRPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC)\nalgorithm for making inference in Bayesian nonparametric mixture modeling.\nSpecifically, we introduce a novel and efficient MCMC sampling scheme in an\naugmented space that has a fixed number of auxiliary variables per iteration.\nWe apply our sampling scheme for a density estimation and clustering tasks with\nunidimensional and multidimensional datasets, and we compare it against\ncompeting sampling schemes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 07:06:10 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 23:53:20 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 14:37:43 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Lomel\u00ed", "Mar\u00eda", ""], ["Favaro", "Stefano", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1407.4373", "submitter": "Gerard Biau", "authors": "G\\'erard Biau (LSTA, LPMA, DMA, INRIA Paris - Rocquencourt), Ryad\n  Zenine (LSTA)", "title": "Online Asynchronous Distributed Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing offers a high degree of flexibility to accommodate\nmodern learning constraints and the ever increasing size of datasets involved\nin massive data issues. Drawing inspiration from the theory of distributed\ncomputation models developed in the context of gradient-type optimization\nalgorithms, we present a consensus-based asynchronous distributed approach for\nnonparametric online regression and analyze some of its asymptotic properties.\nSubstantial numerical evidence involving up to 28 parallel processors is\nprovided on synthetic datasets to assess the excellent performance of our\nmethod, both in terms of computation time and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 16:24:30 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, LPMA, DMA, INRIA Paris - Rocquencourt"], ["Zenine", "Ryad", "", "LSTA"]]}, {"id": "1407.4416", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "In Defense of MinHash Over SimHash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:27:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1407.4420", "submitter": "Paul Honeine", "authors": "Fei Zhu, Paul Honeine, Maya Kallas", "title": "Kernel Nonnegative Matrix Factorization Without the Curse of the\n  Pre-image - Application to Unmixing Hyperspectral Images", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonnegative matrix factorization (NMF) is widely used in signal and image\nprocessing, including bio-informatics, blind source separation and\nhyperspectral image analysis in remote sensing. A great challenge arises when\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\nmachines, the models suggested in the literature do not allow the\nrepresentation of the factorization matrices, which is a fallout of the curse\nof the pre-image. In this paper, we propose a novel kernel-based model for the\nNMF that does not suffer from the pre-image problem, by investigating the\nestimation of the factorization matrices directly in the input space. For\ndifferent kernel functions, we describe two schemes for iterative algorithms:\nan additive update rule based on a gradient descent scheme and a multiplicative\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\nproposed framework, we develop several extensions to incorporate constraints,\nincluding sparseness, smoothness, and spatial regularization with a\ntotal-variation-like penalty. The effectiveness of the proposed method is\ndemonstrated with the problem of unmixing hyperspectral images, using\nwell-known real images and results with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:46:41 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 20:44:42 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Zhu", "Fei", ""], ["Honeine", "Paul", ""], ["Kallas", "Maya", ""]]}, {"id": "1407.4430", "submitter": "Zhaoyi Kang", "authors": "Zhaoyi Kang and Costas J. Spanos", "title": "Sequential Logistic Principal Component Analysis (SLPCA): Dimensional\n  Reduction in Streaming Multivariate Binary-State System", "comments": "6 pages, 4 figures, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sequential or online dimensional reduction is of interests due to the\nexplosion of streaming data based applications and the requirement of adaptive\nstatistical modeling, in many emerging fields, such as the modeling of energy\nend-use profile. Principal Component Analysis (PCA), is the classical way of\ndimensional reduction. However, traditional Singular Value Decomposition (SVD)\nbased PCA fails to model data which largely deviates from Gaussian\ndistribution. The Bregman Divergence was recently introduced to achieve a\ngeneralized PCA framework. If the random variable under dimensional reduction\nfollows Bernoulli distribution, which occurs in many emerging fields, the\ngeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend the\nbatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex\noptimization theory. The convergence property of this algorithm is discussed\ncompared to the batch version of LPCA (i.e. BLPCA), as well as its performance\nin reducing the dimension for multivariate binary-state systems. Its\napplication in building energy end-use profile modeling is also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:05:55 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Kang", "Zhaoyi", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1407.4443", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien\n  Garivier (IMT)", "title": "On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models", "comments": "arXiv admin note: text overlap with arXiv:1405.3224", "journal-ref": "Journal of Machine Learning Research, Journal of Machine Learning\n  Research, 2016, 17, pp.1-42", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic multi-armed bandit model is a simple abstraction that has\nproven useful in many different contexts in statistics and machine learning.\nWhereas the achievable limit in terms of regret minimization is now well known,\nour aim is to contribute to a better understanding of the performance in terms\nof identifying the m best arms. We introduce generic notions of complexity for\nthe two dominant frameworks considered in the literature: fixed-budget and\nfixed-confidence settings. In the fixed-confidence setting, we provide the\nfirst known distribution-dependent lower bound on the complexity that involves\ninformation-theoretic quantities and holds when m is larger than 1 under\ngeneral assumptions. In the specific case of two armed-bandits, we derive\nrefined lower bounds in both the fixed-confidence and fixed-budget settings,\nalong with matching algorithms for Gaussian and Bernoulli bandit models. These\nresults show in particular that the complexity of the fixed-budget setting may\nbe smaller than the complexity of the fixed-confidence setting, contradicting\nthe familiar behavior observed when testing fully specified alternatives. In\naddition, we also provide improved sequential stopping rules that have\nguaranteed error probabilities and shorter average running times. The proofs\nrely on two technical results that are of independent interest : a deviation\nlemma for self-normalized sums (Lemma 19) and a novel change of measure\ninequality for bandit models (Lemma 1).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:44:15 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 12:38:45 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1407.4446", "submitter": "Weidong Han", "authors": "Weidong Han, Purnima Rajan, Peter I. Frazier, Bruno M. Jedynak", "title": "Probabilistic Group Testing under Sum Observations: A Parallelizable\n  2-Approximation for Entropy Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of group testing with sum observations and noiseless\nanswers, in which we aim to locate multiple objects by querying the number of\nobjects in each of a sequence of chosen sets. We study a probabilistic setting\nwith entropy loss, in which we assume a joint Bayesian prior density on the\nlocations of the objects and seek to choose the sets queried to minimize the\nexpected entropy of the Bayesian posterior distribution after a fixed number of\nquestions. We present a new non-adaptive policy, called the dyadic policy, show\nit is optimal among non-adaptive policies, and is within a factor of two of\noptimal among adaptive policies. This policy is quick to compute, its\nnonadaptive nature makes it easy to parallelize, and our bounds show it\nperforms well even when compared with adaptive policies. We also study an\nadaptive greedy policy, which maximizes the one-step expected reduction in\nentropy, and show that it performs at least as well as the dyadic policy,\noffering greater query efficiency but reduced parallelism. Numerical\nexperiments demonstrate that both procedures outperform a divide-and-conquer\nbenchmark policy from the literature, called sequential bifurcation, and show\nhow these procedures may be applied in a stylized computer vision problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 19:55:51 GMT"}, {"version": "v2", "created": "Sat, 26 Jul 2014 15:28:35 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 03:32:33 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Han", "Weidong", ""], ["Rajan", "Purnima", ""], ["Frazier", "Peter I.", ""], ["Jedynak", "Bruno M.", ""]]}, {"id": "1407.4508", "submitter": "Yichao Lu", "authors": "Yichao Lu and Dean P. Foster", "title": "Large scale canonical correlation analysis with iterative least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is a widely used statistical tool with\nboth well established theory and favorable performance for a wide range of\nmachine learning problems. However, computing CCA for huge datasets can be very\nslow since it involves implementing QR decomposition or singular value\ndecomposition of huge matrices. In this paper we introduce L-CCA, a iterative\nalgorithm which can compute CCA fast on huge sparse datasets. Theory on both\nthe asymptotic convergence and finite time accuracy of L-CCA are established.\nThe experiments also show that L-CCA outperform other fast CCA approximation\nschemes on two real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 21:51:07 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 16:55:45 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Lu", "Yichao", ""], ["Foster", "Dean P.", ""]]}, {"id": "1407.4543", "submitter": "Ya Le", "authors": "Ya Le, Trevor Hastie", "title": "Sparse Quadratic Discriminant Analysis and Community Bayes", "comments": "Revised version (adding more experiments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of rules spanning the range between quadratic discriminant\nanalysis and naive Bayes, through a path of sparse graphical models. A group\nlasso penalty is used to introduce shrinkage and encourage a similar pattern of\nsparsity across precision matrices. It gives sparse estimates of interactions\nand produces interpretable models. Inspired by the connected-components\nstructure of the estimated precision matrices, we propose the community Bayes\nmodel, which partitions features into several conditional independent\ncommunities and splits the classification problem into separate smaller ones.\nThe community Bayes idea is quite general and can be applied to non-Gaussian\ndata and likelihood-based classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 03:11:19 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 06:13:35 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Le", "Ya", ""], ["Hastie", "Trevor", ""]]}, {"id": "1407.4596", "submitter": "Shenglong Zhou", "authors": "Shenglong Zhou, Naihua Xiu, Ziyan Luo, Lingchen Kong", "title": "Sparse and Low-Rank Covariance Matrices Estimation", "comments": "arXiv admin note: text overlap with arXiv:1208.5702 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper aims at achieving a simultaneously sparse and low-rank estimator\nfrom the semidefinite population covariance matrices. We first benefit from a\nconvex optimization which develops $l_1$-norm penalty to encourage the sparsity\nand nuclear norm to favor the low-rank property. For the proposed estimator, we\nthen prove that with large probability, the Frobenious norm of the estimation\nrate can be of order $O(\\sqrt{s(\\log{r})/n})$ under a mild case, where $s$ and\n$r$ denote the number of sparse entries and the rank of the population\ncovariance respectively, $n$ notes the sample capacity. Finally an efficient\nalternating direction method of multipliers with global convergence is proposed\nto tackle this problem, and meantime merits of the approach are also\nillustrated by practicing numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 08:28:57 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 01:51:01 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Zhou", "Shenglong", ""], ["Xiu", "Naihua", ""], ["Luo", "Ziyan", ""], ["Kong", "Lingchen", ""]]}, {"id": "1407.4729", "submitter": "Yin Lou", "authors": "Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke", "title": "Sparse Partially Linear Additive Models", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized partially linear additive model (GPLAM) is a flexible and\ninterpretable approach to building predictive models. It combines features in\nan additive manner, allowing each to have either a linear or nonlinear effect\non the response. However, the choice of which features to treat as linear or\nnonlinear is typically assumed known. Thus, to make a GPLAM a viable approach\nin situations in which little is known $a~priori$ about the features, one must\novercome two primary model selection challenges: deciding which features to\ninclude in the model and determining which of these features to treat\nnonlinearly. We introduce the sparse partially linear additive model (SPLAM),\nwhich combines model fitting and $both$ of these model selection challenges\ninto a single convex optimization problem. SPLAM provides a bridge between the\nlasso and sparse additive models. Through a statistical oracle inequality and\nthorough simulation, we demonstrate that SPLAM can outperform other methods\nacross a broad spectrum of statistical regimes, including the high-dimensional\n($p\\gg N$) setting. We develop efficient algorithms that are applied to real\ndata sets with half a million samples and over 45,000 features with excellent\npredictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 16:27:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 19:17:59 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 19:02:45 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Lou", "Yin", ""], ["Bien", "Jacob", ""], ["Caruana", "Rich", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1407.4916", "submitter": "Andre Beinrucker", "authors": "Andre Beinrucker, \\\"Ur\\\"un Dogan, Gilles Blanchard", "title": "Extensions of stability selection using subsamples of observations and\n  covariates", "comments": "accepted for publication in Statistics and Computing", "journal-ref": "Statistics and Computing 26 (5): 1059-1077 (2016)", "doi": "10.1007/s11222-015-9589-y", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce extensions of stability selection, a method to stabilise\nvariable selection methods introduced by Meinshausen and B\\\"uhlmann (J R Stat\nSoc 72:417-473, 2010). We propose to apply a base selection method repeatedly\nto random observation subsamples and covariate subsets under scrutiny, and to\nselect covariates based on their selection frequency. We analyse the effects\nand benefits of these extensions. Our analysis generalizes the theoretical\nresults of Meinshausen and B\\\"uhlmann (J R Stat Soc 72:417-473, 2010) from the\ncase of half-samples to subsamples of arbitrary size. We study, in a\ntheoretical manner, the effect of taking random covariate subsets using a\nsimplified score model. Finally we validate these extensions on numerical\nexperiments on both synthetic and real datasets, and compare the obtained\nresults in detail to the original stability selection method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 08:52:41 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 16:53:44 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2015 08:21:08 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Beinrucker", "Andre", ""], ["Dogan", "\u00dcr\u00fcn", ""], ["Blanchard", "Gilles", ""]]}, {"id": "1407.4981", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander", "title": "Likelihood-free inference via classification", "comments": "Accepted for publication in Statistics and Computing (Feb 13, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex generative models are being used across disciplines as\nthey allow for realistic characterization of data, but a common difficulty with\nthem is the prohibitively large computational cost to evaluate the likelihood\nfunction and thus to perform likelihood-based statistical inference. A\nlikelihood-free inference framework has emerged where the parameters are\nidentified by finding values that yield simulated data resembling the observed\ndata. While widely applicable, a major difficulty in this framework is how to\nmeasure the discrepancy between the simulated and observed data. Transforming\nthe original problem into a problem of classifying the data into simulated\nversus observed, we find that classification accuracy can be used to assess the\ndiscrepancy. The complete arsenal of classification methods becomes thereby\navailable for inference of intractable generative models. We validate our\napproach using theory and simulations for both point estimation and Bayesian\ninference, and demonstrate its use on real data by inferring an\nindividual-based epidemiological model for bacterial infections in child care\ncenters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 13:17:30 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2015 12:09:27 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 11:14:02 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""], ["Corander", "Jukka", ""]]}, {"id": "1407.5017", "submitter": "Pablo G. Moreno", "authors": "Pablo G. Moreno and Yee Whye Teh and Fernando Perez-Cruz and Antonio\n  Art\\'es-Rodr\\'iguez", "title": "Bayesian Nonparametric Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been proven to be an effective and efficient tool to\nannotate large datasets. User annotations are often noisy, so methods to\ncombine the annotations to produce reliable estimates of the ground truth are\nnecessary. We claim that considering the existence of clusters of users in this\ncombination step can improve the performance. This is especially important in\nearly stages of crowdsourcing implementations, where the number of annotations\nis low. At this stage there is not enough information to accurately estimate\nthe bias introduced by each annotator separately, so we have to resort to\nmodels that consider the statistical links among them. In addition, finding\nthese clusters is interesting in itself as knowing the behavior of the pool of\nannotators allows implementing efficient active learning strategies. Based on\nthis, we propose in this paper two new fully unsupervised models based on a\nChinese Restaurant Process (CRP) prior and a hierarchical structure that allows\ninferring these groups jointly with the ground truth and the properties of the\nusers. Efficient inference algorithms based on Gibbs sampling with auxiliary\nvariables are proposed. Finally, we perform experiments, both on synthetic and\nreal databases, to show the advantages of our models over state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 14:51:34 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Moreno", "Pablo G.", ""], ["Teh", "Yee Whye", ""], ["Perez-Cruz", "Fernando", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "1407.5155", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (PANAMA), Rodolphe Jenatton (CMAP), Francis Bach\n  (SIERRA, LIENS)", "title": "Sparse and spurious: dictionary learning with noise and outliers", "comments": "This is a substantially revised version of a first draft that\n  appeared as a preprint titled \"Local stability and robustness of sparse\n  dictionary learning in the presence of noise\",\n  http://hal.inria.fr/hal-00737152, IEEE Transactions on Information Theory,\n  Institute of Electrical and Electronics Engineers (IEEE), 2015, pp.22", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach within the signal processing and machine learning\ncommunities consists in modelling signals as sparse linear combinations of\natoms selected from a learned dictionary. While this paradigm has led to\nnumerous empirical successes in various fields ranging from image to audio\nprocessing, there have only been a few theoretical arguments supporting these\nevidences. In particular, sparse coding, or sparse dictionary learning, relies\non a non-convex procedure whose local minima have not been fully analyzed yet.\nIn this paper, we consider a probabilistic model of sparse signals, and show\nthat, with high probability, sparse coding admits a local minimum around the\nreference dictionary generating the signals. Our study takes into account the\ncase of over-complete dictionaries, noisy signals, and possible outliers, thus\nextending previous work limited to noiseless settings and/or under-complete\ndictionaries. The analysis we conduct is non-asymptotic and makes it possible\nto understand how the key quantities of the problem, such as the coherence or\nthe level of noise, can scale with respect to the dimension of the signals, the\nnumber of atoms, the sparsity and the number of observations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 06:50:19 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 15:39:15 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 18:39:21 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2015 12:46:49 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "PANAMA"], ["Jenatton", "Rodolphe", "", "CMAP"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1407.5158", "submitter": "Jean-Philippe Vert", "authors": "Emile Richard, Guillaume Obozinski (LIGM), Jean-Philippe Vert (CBIO)", "title": "Tight convex relaxations for sparse matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a new atomic norm, we propose a new convex formulation for sparse\nmatrix factorization problems in which the number of nonzero elements of the\nfactors is assumed fixed and known. The formulation counts sparse PCA with\nmultiple factors, subspace clustering and low-rank sparse bilinear regression\nas potential applications. We compute slow rates and an upper bound on the\nstatistical dimension of the suggested norm for rank 1 matrices, showing that\nits statistical dimension is an order of magnitude smaller than the usual\n$\\ell\\_1$-norm, trace norm and their combinations. Even though our convex\nformulation is in theory hard and does not lead to provably polynomial time\nalgorithmic schemes, we propose an active set algorithm leveraging the\nstructure of the convex problem to solve it and show promising numerical\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 07:04:08 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 11:19:07 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Richard", "Emile", "", "LIGM"], ["Obozinski", "Guillaume", "", "LIGM"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1407.5358", "submitter": "Andr\\'e Barreto", "authors": "Andr\\'e M. S. Barreto, Doina Precup, and Joelle Pineau", "title": "Practical Kernel-Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based reinforcement learning (KBRL) stands out among reinforcement\nlearning algorithms for its strong theoretical guarantees. By casting the\nlearning problem as a local kernel approximation, KBRL provides a way of\ncomputing a decision policy which is statistically consistent and converges to\na unique solution. Unfortunately, the model constructed by KBRL grows with the\nnumber of sample transitions, resulting in a computational cost that precludes\nits application to large-scale or on-line domains. In this paper we introduce\nan algorithm that turns KBRL into a practical reinforcement learning tool.\nKernel-based stochastic factorization (KBSF) builds on a simple idea: when a\ntransition matrix is represented as the product of two stochastic matrices, one\ncan swap the factors of the multiplication to obtain another transition matrix,\npotentially much smaller, which retains some fundamental properties of its\nprecursor. KBSF exploits such an insight to compress the information contained\nin KBRL's model into an approximator of fixed size. This makes it possible to\nbuild an approximation that takes into account both the difficulty of the\nproblem and the associated computational cost. KBSF's computational complexity\nis linear in the number of sample transitions, which is the best one can do\nwithout discarding data. Moreover, the algorithm's simple mechanics allow for a\nfully incremental implementation that makes the amount of memory used\nindependent of the number of sample transitions. The result is a kernel-based\nreinforcement learning algorithm that can be applied to large-scale problems in\nboth off-line and on-line regimes. We derive upper bounds for the distance\nbetween the value functions computed by KBRL and KBSF using the same data. We\nalso illustrate the potential of our algorithm in an extensive empirical study\nin which KBSF is applied to difficult tasks based on real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 01:20:45 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Barreto", "Andr\u00e9 M. S.", ""], ["Precup", "Doina", ""], ["Pineau", "Joelle", ""]]}, {"id": "1407.5599", "submitter": "Bo Dai", "authors": "Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina\n  Balcan, Le Song", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "comments": "32 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general perception is that kernel methods are not scalable, and neural\nnets are the methods of choice for nonlinear learning problems. Or have we\nsimply not tried hard enough for kernel methods? Here we propose an approach\nthat scales up kernel methods using a novel concept called \"doubly stochastic\nfunctional gradients\". Our approach relies on the fact that many kernel methods\ncan be expressed as convex optimization problems, and we solve the problems by\nmaking two unbiased stochastic approximations to the functional gradient, one\nusing random training points and another using random functions associated with\nthe kernel, and then descending using this noisy functional gradient. We show\nthat a function produced by this procedure after $t$ iterations converges to\nthe optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$,\nand achieves a generalization performance of $O(1/\\sqrt{t})$. This doubly\nstochasticity also allows us to avoid keeping the support vectors and to\nimplement the algorithm in a small memory footprint, which is linear in number\nof iterations and independent of data dimension. Our approach can readily scale\nkernel methods up to the regimes which are dominated by neural nets. We show\nthat our method can achieve competitive performance to neural nets in datasets\nsuch as 8 million handwritten digits from MNIST, 2.3 million energy materials\nfrom MolecularSpace, and 1 million photos from ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 19:05:47 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 17:58:57 GMT"}, {"version": "v3", "created": "Tue, 23 Sep 2014 15:39:03 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2015 16:40:45 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Dai", "Bo", ""], ["Xie", "Bo", ""], ["He", "Niao", ""], ["Liang", "Yingyu", ""], ["Raj", "Anant", ""], ["Balcan", "Maria-Florina", ""], ["Song", "Le", ""]]}, {"id": "1407.5602", "submitter": "Edouard Duchesnay", "authors": "Mathieu Dubois (NEUROSPIN), Fouad Hadj-Selem (NEUROSPIN), Tommy\n  Lofstedt (NEUROSPIN), Matthieu Perrot (NEUROSPIN), Clara Fischer, Vincent\n  Frouin (NEUROSPIN), Edouard Duchesnay (NEUROSPIN)", "title": "Predictive support recovery with TV-Elastic Net penalty and logistic\n  regression: an application to structural MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine-learning in neuroimaging offers new perspectives in early\ndiagnosis and prognosis of brain diseases. Although such multivariate methods\ncan capture complex relationships in the data, traditional approaches provide\nirregular (l2 penalty) or scattered (l1 penalty) predictive pattern with a very\nlimited relevance. A penalty like Total Variation (TV) that exploits the\nnatural 3D structure of the images can increase the spatial coherence of the\nweight map. However, TV penalization leads to non-smooth optimization problems\nthat are hard to minimize. We propose an optimization framework that minimizes\nany combination of l1, l2, and TV penalties while preserving the exact l1\npenalty. This algorithm uses Nesterov's smoothing technique to approximate the\nTV penalty with a smooth function such that the loss and the penalties are\nminimized with an exact accelerated proximal gradient algorithm. We propose an\noriginal continuation algorithm that uses successively smaller values of the\nsmoothing parameter to reach a prescribed precision while achieving the best\npossible convergence rate. This algorithm can be used with other losses or\npenalties. The algorithm is applied on a classification problem on the ADNI\ndataset. We observe that the TV penalty does not necessarily improve the\nprediction but provides a major breakthrough in terms of support recovery of\nthe predictive brain regions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 19:12:59 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Dubois", "Mathieu", "", "NEUROSPIN"], ["Hadj-Selem", "Fouad", "", "NEUROSPIN"], ["Lofstedt", "Tommy", "", "NEUROSPIN"], ["Perrot", "Matthieu", "", "NEUROSPIN"], ["Fischer", "Clara", "", "NEUROSPIN"], ["Frouin", "Vincent", "", "NEUROSPIN"], ["Duchesnay", "Edouard", "", "NEUROSPIN"]]}, {"id": "1407.5807", "submitter": "Andrea Carron", "authors": "Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schenato,\n  Gianluigi Pillonetto", "title": "Multi-agents adaptive estimation and coverage control using Gaussian\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a scenario where the aim of a group of agents is to perform the\noptimal coverage of a region according to a sensory function. In particular,\ncentroidal Voronoi partitions have to be computed. The difficulty of the task\nis that the sensory function is unknown and has to be reconstructed on line\nfrom noisy measurements. Hence, estimation and coverage needs to be performed\nat the same time. We cast the problem in a Bayesian regression framework, where\nthe sensory function is seen as a Gaussian random field. Then, we design a set\nof control inputs which try to well balance coverage and estimation, also\ndiscussing convergence properties of the algorithm. Numerical experiments show\nthe effectivness of the new approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 10:12:51 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Carron", "Andrea", ""], ["Todescato", "Marco", ""], ["Carli", "Ruggero", ""], ["Schenato", "Luca", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1407.5820", "submitter": "Niclas Blomberg", "authors": "Niclas Blomberg, Cristian R. Rojas, and Bo Wahlberg", "title": "Approximate Regularization Path for Nuclear Norm Based H2 Model\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns model reduction of dynamical systems using the nuclear\nnorm of the Hankel matrix to make a trade-off between model fit and model\ncomplexity. This results in a convex optimization problem where this trade-off\nis determined by one crucial design parameter. The main contribution is a\nmethodology to approximately calculate all solutions up to a certain tolerance\nto the model reduction problem as a function of the design parameter. This is\ncalled the regularization path in sparse estimation and is a very important\ntool in order to find the appropriate balance between fit and complexity. We\nextend this to the more complicated nuclear norm case. The key idea is to\ndetermine when to exactly calculate the optimal solution using an upper bound\nbased on the so-called duality gap. Hence, by solving a fixed number of\noptimization problems the whole regularization path up to a given tolerance can\nbe efficiently computed. We illustrate this approach on some numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 11:20:11 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Blomberg", "Niclas", ""], ["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1407.5924", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven and Elena Marchiori", "title": "Resolution-limit-free and local Non-negative Matrix Factorization\n  quality functions for graph clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph clustering quality functions suffer from a resolution limit, the\ninability to find small clusters in large graphs. So called\nresolution-limit-free quality functions do not have this limit. This property\nwas previously introduced for hard clustering, that is, graph partitioning.\n  We investigate the resolution-limit-free property in the context of\nNon-negative Matrix Factorization (NMF) for hard and soft graph clustering. To\nuse NMF in the hard clustering setting, a common approach is to assign each\nnode to its highest membership cluster. We show that in this case symmetric NMF\nis not resolution-limit-free, but that it becomes so when hardness constraints\nare used as part of the optimization. The resulting function is strongly linked\nto the Constant Potts Model. In soft clustering, nodes can belong to more than\none cluster, with varying degrees of membership. In this setting\nresolution-limit-free turns out to be too strong a property. Therefore we\nintroduce locality, which roughly states that changing one part of the graph\ndoes not affect the clustering of other parts of the graph. We argue that this\nis a desirable property, provide conditions under which NMF quality functions\nare local, and propose a novel class of local probabilistic NMF quality\nfunctions for soft graph clustering.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 16:21:59 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1407.5978", "submitter": "Yao Xie", "authors": "David Marangoni-Simonsen and Yao Xie", "title": "Sequential Changepoint Approach for Online Community Detection", "comments": "Submitted to 2014 INFORMS Workshop on Data Mining and Analytics and\n  an IEEE journal", "journal-ref": null, "doi": "10.1109/LSP.2014.2381553", "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for detecting the emergence of a community in large\nnetworks from sequential observations. The networks are modeled using\nErdos-Renyi random graphs with edges forming between nodes in the community\nwith higher probability. Based on statistical changepoint detection\nmethodology, we develop three algorithms: the Exhaustive Search (ES), the\nmixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these\nmethods is evaluated by the average run length (ARL), which captures the\nfrequency of false alarms, and the detection delay. Numerical comparisons show\nthat the ES method performs the best; however, it is exponentially complex. The\nmixture method is polynomially complex by exploiting the fact that the size of\nthe community is typically small in a large network. However, it may react to a\ngroup of active edges that do not form a community. This issue is resolved by\nthe H-Mix method, which is based on a dendrogram decomposition of the network.\nWe present an asymptotic analytical expression for ARL of the mixture method\nwhen the threshold is large. Numerical simulation verifies that our\napproximation is accurate even in the non-asymptotic regime. Hence, it can be\nused to determine a desired threshold efficiently. Finally, numerical examples\nshow that the mixture and the H-Mix methods can both detect a community quickly\nwith a lower complexity than the ES method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 19:16:01 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 19:54:17 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 06:27:05 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Marangoni-Simonsen", "David", ""], ["Xie", "Yao", ""]]}, {"id": "1407.6089", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Rank Functionals: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a key aspect of many applications, such as information retrieval,\nquestion answering, ad placement and recommender systems. Learning to rank has\nthe goal of estimating a ranking model automatically from training data. In\npractical settings, the task often reduces to estimating a rank functional of\nan object with respect to a query. In this paper, we investigate key issues in\ndesigning an effective learning to rank algorithm. These include data\nrepresentation, the choice of rank functionals, the design of the loss function\nso that it is correlated with the rank metrics used in evaluation. For the loss\nfunction, we study three techniques: approximating the rank metric by a smooth\nfunction, decomposition of the loss into a weighted sum of element-wise losses\nand into a weighted sum of pairwise losses. We then present derivations of\npiecewise losses using the theory of high-order Markov chains and Markov random\nfields. In experiments, we evaluate these design aspects on two tasks: answer\nranking in a Social Question Answering site, and Web Information Retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 01:54:31 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 23:50:43 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6094", "submitter": "Shivapratap Gopakumar", "authors": "Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Stabilizing Sparse Cox Model using Clinical Structures in Electronic\n  Medical Records", "comments": "Submitted to International Workshop on Pattern Recognition for\n  Healthcare Analytics 2014, Sweden. Contains 4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability in clinical prediction models is crucial for transferability\nbetween studies, yet has received little attention. The problem is paramount in\nhigh dimensional data which invites sparse models with feature selection\ncapability. We introduce an effective method to stabilize sparse Cox model of\ntime-to-events using clinical structures inherent in Electronic Medical\nRecords. Model estimation is stabilized using a feature graph derived from two\ntypes of EMR structures: temporal structure of disease and intervention\nrecurrences, and hierarchical structure of medical knowledge and practices. We\ndemonstrate the efficacy of the method in predicting time-to-readmission of\nheart failure patients. On two stability measures - the Jaccard index and the\nConsistency index - the use of clinical structures significantly increased\nfeature stability without hurting discriminative power. Our model reported a\ncompetitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 02:47:47 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Gopakumar", "Shivapratap", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6128", "submitter": "Truyen Tran", "authors": "Truyen Tran and Svetha Venkatesh", "title": "Permutation Models for Collaborative Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of collaborative filtering where ranking information is\navailable. Focusing on the core of the collaborative ranking process, the user\nand their community, we propose new models for representation of the underlying\npermutations and prediction of ranks. The first approach is based on the\nassumption that the user makes successive choice of items in a stage-wise\nmanner. In particular, we extend the Plackett-Luce model in two ways -\nintroducing parameter factoring to account for user-specific contribution, and\nmodelling the latent community in a generative setting. The second approach\nrelies on log-linear parameterisation, which relaxes the discrete-choice\nassumption, but makes learning and inference much more involved. We propose\nMCMC-based learning and inference methods and derive linear-time prediction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 08:20:09 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6288", "submitter": "Yuejie Chi", "authors": "Yuejie Chi, Haoyu Fu", "title": "Subspace Learning From Bits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networked sensing, where the goal is to perform complex inference using a\nlarge number of inexpensive and decentralized sensors, has become an\nincreasingly attractive research topic due to its applications in wireless\nsensor networks and internet-of-things. To reduce the communication, sensing\nand storage complexity, this paper proposes a simple sensing and estimation\nframework to faithfully recover the principal subspace of high-dimensional data\nstreams using a collection of binary measurements from distributed sensors,\nwithout transmitting the whole data. The binary measurements are designed to\nindicate comparison outcomes of aggregated energy projections of the data\nsamples over pairs of randomly selected directions. When the covariance matrix\nis a low-rank matrix, we propose a spectral estimator that recovers the\nprincipal subspace of the covariance matrix as the subspace spanned by the top\neigenvectors of a properly designed surrogate matrix, which is provably\naccurate as soon as the number of binary measurements is sufficiently large. An\nadaptive rank selection strategy based on soft thresholding is also presented.\nFurthermore, we propose a tailored spectral estimator when the covariance\nmatrix is additionally Toeplitz, and show reliable estimation can be obtained\nfrom a substantially smaller number of binary measurements. Our results hold\neven when a constant fraction of the binary measurements is randomly flipped.\nFinally, we develop a low-complexity online algorithm to track the principal\nsubspace when new measurements arrive sequentially. Numerical examples are\nprovided to validate the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 16:48:19 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 01:09:15 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 18:08:45 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Chi", "Yuejie", ""], ["Fu", "Haoyu", ""]]}, {"id": "1407.6432", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Structured Outputs from Partial Labels using Forest Ensemble", "comments": "Conference version appeared in Truyen et al, AdaBoost.MRF: Boosted\n  Markov random forests and application to multilevel activity recognition.\n  CVPR'06", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structured outputs with general structures is computationally\nchallenging, except for tree-structured models. Thus we propose an efficient\nboosting-based algorithm AdaBoost.MRF for this task. The idea is based on the\nrealization that a graph is a superimposition of trees. Different from most\nexisting work, our algorithm can handle partial labelling, and thus is\nparticularly attractive in practice where reliable labels are often sparsely\nobserved. In addition, our method works exclusively on trees and thus is\nguaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video\nsurveillance scenario, where activities are modelled at multiple levels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 02:53:52 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6810", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar, Guillermo Sapiro and S. Shankar Sastry", "title": "Dissimilarity-based Sparse Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an informative subset of a large collection of data points or models\nis at the center of many problems in computer vision, recommender systems,\nbio/health informatics as well as image and natural language processing. Given\npairwise dissimilarities between the elements of a `source set' and a `target\nset,' we consider the problem of finding a subset of the source set, called\nrepresentatives or exemplars, that can efficiently describe the target set. We\nformulate the problem as a row-sparsity regularized trace minimization problem.\nSince the proposed formulation is, in general, NP-hard, we consider a convex\nrelaxation. The solution of our optimization finds representatives and the\nassignment of each element of the target set to each representative, hence,\nobtaining a clustering. We analyze the solution of our proposed optimization as\na function of the regularization parameter. We show that when the two sets\njointly partition into multiple groups, our algorithm finds representatives\nfrom all groups and reveals clustering of the sets. In addition, we show that\nthe proposed framework can effectively deal with outliers. Our algorithm works\nwith arbitrary dissimilarities, which can be asymmetric or violate the triangle\ninequality. To efficiently implement our algorithm, we consider an Alternating\nDirection Method of Multipliers (ADMM) framework, which results in quadratic\ncomplexity in the problem size. We show that the ADMM implementation allows to\nparallelize the algorithm, hence further reducing the computational time.\nFinally, by experiments on real-world datasets, we show that our proposed\nalgorithm improves the state of the art on the two problems of scene\ncategorization using representative images and time-series modeling and\nsegmentation using representative~models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 08:30:04 GMT"}, {"version": "v2", "created": "Sat, 9 Apr 2016 03:09:18 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Sapiro", "Guillermo", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1407.6949", "submitter": "Tom Gunter", "authors": "Tom Gunter, Chris Lloyd, Michael A. Osborne, Stephen J. Roberts", "title": "Efficient Bayesian Nonparametric Modelling of Structured Point Processes", "comments": "Presented at UAI 2014. Bibtex: @inproceedings{structcoxpp14_UAI,\n  Author = {Tom Gunter and Chris Lloyd and Michael A. Osborne and Stephen J.\n  Roberts}, Title = {Efficient Bayesian Nonparametric Modelling of Structured\n  Point Processes}, Booktitle = {Uncertainty in Artificial Intelligence (UAI)},\n  Year = {2014}}", "journal-ref": "Proceedings of Uncertainty in Artificial Intelligence (UAI) 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian generative model for dependent Cox point\nprocesses, alongside an efficient inference scheme which scales as if the point\nprocesses were modelled independently. We can handle missing data naturally,\ninfer latent structure, and cope with large numbers of observed processes. A\nfurther novel contribution enables the model to work effectively in higher\ndimensional spaces. Using this method, we achieve vastly improved predictive\nperformance on both 2D and 1D real data, validating our structured approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 15:57:49 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Gunter", "Tom", ""], ["Lloyd", "Chris", ""], ["Osborne", "Michael A.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1407.7094", "submitter": "Bruno Gon\\c{c}alves", "authors": "Bruno Gon\\c{c}alves and David S\\'anchez", "title": "Crowdsourcing Dialect Characterization through Twitter", "comments": "10 pages, 5 figures", "journal-ref": "PLoS One 9, E112074 (2014)", "doi": "10.1371/journal.pone.0112074", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a large-scale analysis of language diatopic variation using\ngeotagged microblogging datasets. By collecting all Twitter messages written in\nSpanish over more than two years, we build a corpus from which a carefully\nselected list of concepts allows us to characterize Spanish varieties on a\nglobal scale. A cluster analysis proves the existence of well defined\nmacroregions sharing common lexical properties. Remarkably enough, we find that\nSpanish language is split into two superdialects, namely, an urban speech used\nacross major American and Spanish citites and a diverse form that encompasses\nrural areas and small towns. The latter can be further clustered into smaller\nvarieties with a stronger regional character.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 04:16:31 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1407.7299", "submitter": "Carl Meyer Dr.", "authors": "Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David\n  Duling", "title": "Algorithms, Initializations, and Convergence for the Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that good initializations can improve the speed and accuracy\nof the solutions of many nonnegative matrix factorization (NMF) algorithms.\nMany NMF algorithms are sensitive with respect to the initialization of W or H\nor both. This is especially true of algorithms of the alternating least squares\n(ALS) type, including the two new ALS algorithms that we present in this paper.\nWe compare the results of six initialization procedures (two standard and four\nnew) on our ALS algorithms. Lastly, we discuss the practical issue of choosing\nan appropriate convergence criterion.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 00:41:12 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Langville", "Amy N.", ""], ["Meyer", "Carl D.", ""], ["Albright", "Russell", ""], ["Cox", "James", ""], ["Duling", "David", ""]]}, {"id": "1407.7502", "submitter": "Gilles Louppe", "authors": "Gilles Louppe", "title": "Understanding Random Forests: From Theory to Practice", "comments": "PhD thesis. Source code available at\n  https://github.com/glouppe/phd-thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and machine learning have become an integrative part of the\nmodern scientific methodology, offering automated procedures for the prediction\nof a phenomenon based on past observations, unraveling underlying patterns in\ndata and providing insights about the problem. Yet, caution should avoid using\nmachine learning as a black-box tool, but rather consider it as a methodology,\nwith a rational thought process that is entirely dependent on the problem under\nstudy. In particular, the use of algorithms should ideally require a reasonable\nunderstanding of their mechanisms, properties and limitations, in order to\nbetter apprehend and interpret their results.\n  Accordingly, the goal of this thesis is to provide an in-depth analysis of\nrandom forests, consistently calling into question each and every part of the\nalgorithm, in order to shed new light on its learning capabilities, inner\nworkings and interpretability. The first part of this work studies the\ninduction of decision trees and the construction of ensembles of randomized\ntrees, motivating their design and purpose whenever possible. Our contributions\nfollow with an original complexity analysis of random forests, showing their\ngood computational performance and scalability, along with an in-depth\ndiscussion of their implementation details, as contributed within Scikit-Learn.\n  In the second part of this work, we analyse and discuss the interpretability\nof random forests in the eyes of variable importance measures. The core of our\ncontributions rests in the theoretical characterization of the Mean Decrease of\nImpurity variable importance measure, from which we prove and derive some of\nits properties in the case of multiway totally randomized trees and in\nasymptotic conditions. In consequence of this work, our analysis demonstrates\nthat variable importances [...].\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 19:16:02 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 17:55:01 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 19:04:07 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Louppe", "Gilles", ""]]}, {"id": "1407.7508", "submitter": "Zhenqiu  Liu", "authors": "Zhenqiu Liu and Gang Li", "title": "Efficient Regularized Regression for Variable Selection with L0 Penalty", "comments": "26 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable (feature, gene, model, which we use interchangeably) selections for\nregression with high-dimensional BIGDATA have found many applications in\nbioinformatics, computational biology, image processing, and engineering. One\nappealing approach is the L0 regularized regression which penalizes the number\nof nonzero features in the model directly. L0 is known as the most essential\nsparsity measure and has nice theoretical properties, while the popular L1\nregularization is only a best convex relaxation of L0. Therefore, it is natural\nto expect that L0 regularized regression performs better than LASSO. However,\nit is well-known that L0 optimization is NP-hard and computationally\nchallenging. Instead of solving the L0 problems directly, most publications so\nfar have tried to solve an approximation problem that closely resembles L0\nregularization.\n  In this paper, we propose an efficient EM algorithm (L0EM) that directly\nsolves the L0 optimization problem. $L_0$EM is efficient with high dimensional\ndata. It also provides a natural solution to all Lp p in [0,2] problems. The\nregularized parameter can be either determined through cross-validation or AIC\nand BIC. Theoretical properties of the L0-regularized estimator are given under\nmild conditions that permit the number of variables to be much larger than the\nsample size. We demonstrate our methods through simulation and high-dimensional\ngenomic data. The results indicate that L0 has better performance than LASSO\nand L0 with AIC or BIC has similar performance as computationally intensive\ncross-validation. The proposed algorithms are efficient in identifying the\nnon-zero variables with less-bias and selecting biologically important genes\nand pathways with high dimensional BIGDATA.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 19:28:26 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Liu", "Zhenqiu", ""], ["Li", "Gang", ""]]}, {"id": "1407.7556", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz", "title": "Entropic one-class classifiers", "comments": "To appear in IEEE-TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2418332", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-class classification problem is a well-known research endeavor in\npattern recognition. The problem is also known under different names, such as\noutlier and novelty/anomaly detection. The core of the problem consists in\nmodeling and recognizing patterns belonging only to a so-called target class.\nAll other patterns are termed non-target, and therefore they should be\nrecognized as such. In this paper, we propose a novel one-class classification\nsystem that is based on an interplay of different techniques. Primarily, we\nfollow a dissimilarity representation based approach; we embed the input data\ninto the dissimilarity space by means of an appropriate parametric\ndissimilarity measure. This step allows us to process virtually any type of\ndata. The dissimilarity vectors are then represented through a weighted\nEuclidean graphs, which we use to (i) determine the entropy of the data\ndistribution in the dissimilarity space, and at the same time (ii) derive\neffective decision regions that are modeled as clusters of vertices. Since the\ndissimilarity measure for the input data is parametric, we optimize its\nparameters by means of a global optimization scheme, which considers both\nmesoscopic and structural characteristics of the data represented through the\ngraphs. The proposed one-class classifier is designed to provide both hard\n(Boolean) and soft decisions about the recognition of test patterns, allowing\nan accurate description of the classification process. We evaluate the\nperformance of the system on different benchmarking datasets, containing either\nfeature-based or structured patterns. Experimental results demonstrate the\neffectiveness of the proposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:26:24 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 20:46:21 GMT"}, {"version": "v3", "created": "Sun, 11 Jan 2015 16:27:23 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Livi", "Lorenzo", ""], ["Sadeghian", "Alireza", ""], ["Pedrycz", "Witold", ""]]}, {"id": "1407.7566", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Shyam Visweswaran", "title": "Dependence versus Conditional Dependence in Local Causal Discovery from\n  Gene Expression Data", "comments": "11 pages, 2 algorithms, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Algorithms that discover variables which are causally related to\na target may inform the design of experiments. With observational gene\nexpression data, many methods discover causal variables by measuring each\nvariable's degree of statistical dependence with the target using dependence\nmeasures (DMs). However, other methods measure each variable's ability to\nexplain the statistical dependence between the target and the remaining\nvariables in the data using conditional dependence measures (CDMs), since this\nstrategy is guaranteed to find the target's direct causes, direct effects, and\ndirect causes of the direct effects in the infinite sample limit. In this\npaper, we design a new algorithm in order to systematically compare the\nrelative abilities of DMs and CDMs in discovering causal variables from gene\nexpression data.\n  Results: The proposed algorithm using a CDM is sample efficient, since it\nconsistently outperforms other state-of-the-art local causal discovery\nalgorithms when samples sizes are small. However, the proposed algorithm using\na CDM outperforms the proposed algorithm using a DM only when sample sizes are\nabove several hundred. These results suggest that accurate causal discovery\nfrom gene expression data using current CDM-based algorithms requires datasets\nwith at least several hundred samples.\n  Availability: The proposed algorithm is freely available at\nhttps://github.com/ericstrobl/DvCD.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:52:18 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Strobl", "Eric V.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1407.7584", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala", "title": "Dynamic Feature Scaling for Online Learning of Binary Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling feature values is an important step in numerous machine learning\ntasks. Different features can have different value ranges and some form of a\nfeature scaling is often required in order to learn an accurate classifier.\nHowever, feature scaling is conducted as a preprocessing task prior to\nlearning. This is problematic in an online setting because of two reasons.\nFirst, it might not be possible to accurately determine the value range of a\nfeature at the initial stages of learning when we have observed only a few\nnumber of training instances. Second, the distribution of data can change over\nthe time, which render obsolete any feature scaling that we perform in a\npre-processing step. We propose a simple but an effective method to dynamically\nscale features at train time, thereby quickly adapting to any changes in the\ndata stream. We compare the proposed dynamic feature scaling method against\nmore complex methods for estimating scaling parameters using several benchmark\ndatasets for binary classification. Our proposed feature scaling method\nconsistently outperforms more complex methods on all of the benchmark datasets\nand improves classification accuracy of a state-of-the-art online binary\nclassifier algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 21:59:06 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Bollegala", "Danushka", ""]]}, {"id": "1407.7644", "submitter": "Ariel Jaffe", "authors": "Ariel Jaffe, Boaz Nadler and Yuval Kluger", "title": "Estimating the Accuracies of Multiple Classifiers Without Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various situations one is given only the predictions of multiple\nclassifiers over a large unlabeled test data. This scenario raises the\nfollowing questions: Without any labeled data and without any a-priori\nknowledge about the reliability of these different classifiers, is it possible\nto consistently and computationally efficiently estimate their accuracies?\nFurthermore, also in a completely unsupervised manner, can one construct a more\naccurate unsupervised ensemble classifier? In this paper, focusing on the\nbinary case, we present simple, computationally efficient algorithms to solve\nthese questions. Furthermore, under standard classifier independence\nassumptions, we prove our methods are consistent and study their asymptotic\nerror. Our approach is spectral, based on the fact that the off-diagonal\nentries of the classifiers' covariance matrix and 3-d tensor are rank-one. We\nillustrate the competitive performance of our algorithms via extensive\nexperiments on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 07:19:08 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 11:23:37 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Jaffe", "Ariel", ""], ["Nadler", "Boaz", ""], ["Kluger", "Yuval", ""]]}, {"id": "1407.7691", "submitter": "J\\'er\\'emy Rapin", "authors": "J\\'er\\'emy Rapin and J\\'er\\^ome Bobin and Anthony Larue and Jean-Luc\n  Starck", "title": "NMF with Sparse Regularizations in Transformed Domains", "comments": "26 pages, 19 figures, accepted in SIAM Journal on Imaging Sciences", "journal-ref": "SIAM J. Imaging Sci., 7(4), 2020-2047. (28 pages)", "doi": "10.1137/140952314", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative blind source separation (non-negative BSS), which is also\nreferred to as non-negative matrix factorization (NMF), is a very active field\nin domains as different as astrophysics, audio processing or biomedical signal\nprocessing. In this context, the efficient retrieval of the sources requires\nthe use of signal priors such as sparsity. If NMF has now been well studied\nwith sparse constraints in the direct domain, only very few algorithms can\nencompass non-negativity together with sparsity in a transformed domain since\nsimultaneously dealing with two priors in two different domains is challenging.\nIn this article, we show how a sparse NMF algorithm coined non-negative\ngeneralized morphological component analysis (nGMCA) can be extended to impose\nnon-negativity in the direct domain along with sparsity in a transformed\ndomain, with both analysis and synthesis formulations. To our knowledge, this\nwork presents the first comparison of analysis and synthesis priors ---as well\nas their reweighted versions--- in the context of blind source separation.\nComparisons with state-of-the-art NMF algorithms on realistic data show the\nefficiency as well as the robustness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 11:09:59 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Rapin", "J\u00e9r\u00e9my", ""], ["Bobin", "J\u00e9r\u00f4me", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1407.7819", "submitter": "Rui Song", "authors": "Shikai Luo, Rui Song, Daniela Witten", "title": "Sure Screening for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose {graphical sure screening}, or GRASS, a very simple and\ncomputationally-efficient screening procedure for recovering the structure of a\nGaussian graphical model in the high-dimensional setting. The GRASS estimate of\nthe conditional dependence graph is obtained by thresholding the elements of\nthe sample covariance matrix. The proposed approach possesses the sure\nscreening property: with very high probability, the GRASS estimated edge set\ncontains the true edge set. Furthermore, with high probability, the size of the\nestimated edge set is controlled. We provide a choice of threshold for GRASS\nthat can control the expected false positive rate. We illustrate the\nperformance of GRASS in a simulation study and on a gene expression data set,\nand show that in practice it performs quite competitively with more complex and\ncomputationally-demanding techniques for graph estimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 18:37:15 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Luo", "Shikai", ""], ["Song", "Rui", ""], ["Witten", "Daniela", ""]]}, {"id": "1407.7840", "submitter": "Cody Severinski", "authors": "Cody Severinski, Ruslan Salakhutdinov", "title": "Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) has become a common approach to collaborative\nfiltering, due to ease of implementation and scalability to large data sets.\nTwo existing drawbacks of the basic model is that it does not incorporate side\ninformation on either users or items, and assumes a common variance for all\nusers. We extend the work of constrained probabilistic matrix factorization by\nderiving the Gibbs updates for the side feature vectors for items\n(Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to the\nconstrained PMF model outperforms simple MAP estimation. We also consider\nextensions to heteroskedastic precision introduced in the literature\n(Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tends\nresult in overfitting for deterministic approximation algorithms (ex:\nVariational inference) when the observed entries in the user / item matrix are\ndistributed in an non-uniform manner. In light of this, we propose a truncated\nprecision model. Our experimental results suggest that this model tends to\ndelay overfitting.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 19:38:32 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Severinski", "Cody", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1407.7969", "submitter": "Michael Osborne", "authors": "Thomas Nickson, Michael A Osborne, Steven Reece, Stephen J Roberts", "title": "Automated Machine Learning on Big Data using Stochastic Algorithm Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a means of automating machine learning (ML) for big data tasks,\nby performing scalable stochastic Bayesian optimisation of ML algorithm\nparameters and hyper-parameters. More often than not, the critical tuning of ML\nalgorithm parameters has relied on domain expertise from experts, along with\nlaborious hand-tuning, brute search or lengthy sampling runs. Against this\nbackground, Bayesian optimisation is finding increasing use in automating\nparameter tuning, making ML algorithms accessible even to non-experts. However,\nthe state of the art in Bayesian optimisation is incapable of scaling to the\nlarge number of evaluations of algorithm performance required to fit realistic\nmodels to complex, big data. We here describe a stochastic, sparse, Bayesian\noptimisation strategy to solve this problem, using many thousands of noisy\nevaluations of algorithm performance on subsets of data in order to effectively\ntrain algorithms for big data. We provide a comprehensive benchmarking of\npossible sparsification strategies for Bayesian optimisation, concluding that a\nNystrom approximation offers the best scaling and performance for real tasks.\nOur proposed algorithm demonstrates substantial improvement over the state of\nthe art in tuning the parameters of a Gaussian Process time series prediction\ntask on real, big data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 08:29:38 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Nickson", "Thomas", ""], ["Osborne", "Michael A", ""], ["Reece", "Steven", ""], ["Roberts", "Stephen J", ""]]}, {"id": "1407.8042", "submitter": "Lewis Evans Mr", "authors": "Lewis P. G. Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "Targeting Optimal Active Learning via Example Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems unlabelled data is abundant and a subset can\nbe chosen for labelling. This defines the context of active learning (AL),\nwhere methods systematically select that subset, to improve a classifier by\nretraining. Given a classification problem, and a classifier trained on a small\nnumber of labelled examples, consider the selection of a single further\nexample. This example will be labelled by the oracle and then used to retrain\nthe classifier. This example selection raises a central question: given a fully\nspecified stochastic description of the classification problem, which example\nis the optimal selection? If optimality is defined in terms of loss, this\ndefinition directly produces expected loss reduction (ELR), a central quantity\nwhose maximum yields the optimal example selection. This work presents a new\ntheoretical approach to AL, example quality, which defines optimal AL behaviour\nin terms of ELR. Once optimal AL behaviour is defined mathematically, reasoning\nabout this abstraction provides insights into AL. In a theoretical context the\noptimal selection is compared to existing AL methods, showing that heuristics\ncan make sub-optimal selections. Algorithms are constructed to estimate example\nquality directly. A large-scale experimental study shows these algorithms to be\ncompetitive with standard AL methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 13:54:58 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Evans", "Lewis P. G.", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1407.8067", "submitter": "Fei Yu", "authors": "Fei Yu, Michal Rybar, Caroline Uhler, Stephen E. Fienberg", "title": "Differentially-Private Logistic Regression for Detecting Multiple-SNP\n  Association in GWAS Databases", "comments": "To appear in Proceedings of the 2014 International Conference on\n  Privacy in Statistical Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the publication of an attack on genome-wide association studies\n(GWAS) data proposed by Homer et al., considerable attention has been given to\ndeveloping methods for releasing GWAS data in a privacy-preserving way. Here,\nwe develop an end-to-end differentially private method for solving regression\nproblems with convex penalty functions and selecting the penalty parameters by\ncross-validation. In particular, we focus on penalized logistic regression with\nelastic-net regularization, a method widely used to in GWAS analyses to\nidentify disease-causing genes. We show how a differentially private procedure\nfor penalized logistic regression with elastic-net regularization can be\napplied to the analysis of GWAS data and evaluate our method's performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 14:51:19 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Yu", "Fei", ""], ["Rybar", "Michal", ""], ["Uhler", "Caroline", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1407.8187", "submitter": "Charles Fisher", "authors": "Charles K. Fisher, Pankaj Mehta", "title": "Fast Bayesian Feature Selection for High Dimensional Linear Regression\n  in Genomics via the Ising Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection, identifying a subset of variables that are relevant for\npredicting a response, is an important and challenging component of many\nmethods in statistics and machine learning. Feature selection is especially\ndifficult and computationally intensive when the number of variables approaches\nor exceeds the number of samples, as is often the case for many genomic\ndatasets. Here, we introduce a new approach -- the Bayesian Ising Approximation\n(BIA) -- to rapidly calculate posterior probabilities for feature relevance in\nL2 penalized linear regression. In the regime where the regression problem is\nstrongly regularized by the prior, we show that computing the marginal\nposterior probabilities for features is equivalent to computing the\nmagnetizations of an Ising model. Using a mean field approximation, we show it\nis possible to rapidly compute the feature selection path described by the\nposterior probabilities as a function of the L2 penalty. We present simulations\nand analytical results illustrating the accuracy of the BIA on some simple\nregression problems. Finally, we demonstrate the applicability of the BIA to\nhigh dimensional regression by analyzing a gene expression dataset with nearly\n30,000 features.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 20:00:14 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Fisher", "Charles K.", ""], ["Mehta", "Pankaj", ""]]}]