[{"id": "1208.0129", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Peter L. Bartlett, John C. Duchi", "title": "Oracle inequalities for computationally adaptive model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze general model selection procedures using penalized empirical loss\nminimization under computational constraints. While classical model selection\napproaches do not consider computational aspects of performing model selection,\nwe argue that any practical model selection procedure must not only trade off\nestimation and approximation error, but also the computational effort required\nto compute empirical minimizers for different function classes. We provide a\nframework for analyzing such problems, and we give algorithms for model\nselection under a computational budget. These algorithms satisfy oracle\ninequalities that show that the risk of the selected model is not much worse\nthan if we had devoted all of our omputational budget to the optimal function\nclass.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 07:57:53 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bartlett", "Peter L.", ""], ["Duchi", "John C.", ""]]}, {"id": "1208.0378", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Fast Planar Correlation Clustering for Image Segmentation", "comments": "This is the extended version of a paper to appear at the 12th\n  European Conference on Computer Vision (ECCV 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 00:54:02 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1208.0402", "submitter": "Yun Jiang", "authors": "Yun Jiang, Marcus Lim and Ashutosh Saxena", "title": "Multidimensional Membership Mixture Models", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multidimensional membership mixture (M3) models where every\ndimension of the membership represents an independent mixture model and each\ndata point is generated from the selected mixture components jointly. This is\nhelpful when the data has a certain shared structure. For example, three unique\nmeans and three unique variances can effectively form a Gaussian mixture model\nwith nine components, while requiring only six parameters to fully describe it.\nIn this paper, we present three instantiations of M3 models (together with the\nlearning and inference algorithms): infinite, finite, and hybrid, depending on\nwhether the number of mixtures is fixed or not. They are built upon Dirichlet\nprocess mixture models, latent Dirichlet allocation, and a combination\nrespectively. We then consider two applications: topic modeling and learning 3D\nobject arrangements. Our experiments show that our M3 models achieve better\nperformance using fewer topics than many classic topic models. We also observe\nthat topics from the different dimensions of M3 models are meaningful and\northogonal to each other.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 05:20:01 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Jiang", "Yun", ""], ["Lim", "Marcus", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1208.0432", "submitter": "Ju Sun", "authors": "Ju Sun and Yuqian Zhang and John Wright", "title": "Efficient Point-to-Subspace Query in $\\ell^1$ with Application to Robust\n  Object Instance Recognition", "comments": "Revised based on reviewers' feedback; one new experiment on\n  synthesized data added; one section discussing the speed up added", "journal-ref": "SIAM Journal on Imaging Sciences, 7(4):2105 - 2138, 2014", "doi": "10.1137/130936166", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space, and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. In contrast to the naive exhaustive search which entails large-scale\nlinear programs, we show that the computational burden can be cut down\nsignificantly by a simple two-stage algorithm: (1) projecting the query and\ndata-base subspaces into lower-dimensional space by random Cauchy matrix, and\nsolving small-scale distance evaluations (linear programs) in the projection\nspace to locate candidate nearest; (2) with few candidates upon independent\nrepetition of (1), getting back to the high-dimensional space and performing\nexhaustive search. To preserve the identity of the nearest subspace with\nnontrivial probability, the projection dimension typically is low-order\npolynomial of the subspace dimension multiplied by logarithm of number of the\nsubspaces (Theorem 2.1). The reduced dimensionality and hence complexity\nrenders the proposed algorithm particularly relevant to vision application such\nas robust face and object instance recognition that we investigate empirically.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 08:43:45 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2013 19:59:11 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 06:12:11 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sun", "Ju", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""]]}, {"id": "1208.0628", "submitter": "Pantelis - Zenon Hadjipantelis", "authors": "Pantelis Z. Hadjipantelis, Nick S. Jones, John Moriarty, David\n  Springate, Christopher G. Knight", "title": "Ancestral Inference from Functional Data: Statistical Methods and\n  Numerical Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological characteristics of evolutionary interest are not scalar\nvariables but continuous functions. Here we use phylogenetic Gaussian process\nregression to model the evolution of simulated function-valued traits. Given\nfunction-valued data only from the tips of an evolutionary tree and utilising\nindependent principal component analysis (IPCA) as a method for dimension\nreduction, we construct distributional estimates of ancestral function-valued\ntraits, and estimate parameters describing their evolutionary dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 23:15:26 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Hadjipantelis", "Pantelis Z.", ""], ["Jones", "Nick S.", ""], ["Moriarty", "John", ""], ["Springate", "David", ""], ["Knight", "Christopher G.", ""]]}, {"id": "1208.0645", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Zhi-Hua Zhou", "title": "On the Consistency of AUC Pairwise Optimization", "comments": null, "journal-ref": "IJCAI 2015", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC (area under ROC curve) is an important evaluation criterion, which has\nbeen popularly used in many learning tasks such as class-imbalance learning,\ncost-sensitive learning, learning to rank, etc. Many learning approaches try to\noptimize AUC, while owing to the non-convexity and discontinuousness of AUC,\nalmost all approaches work with surrogate loss functions. Thus, the consistency\nof AUC is crucial; however, it has been almost untouched before. In this paper,\nwe provide a sufficient condition for the asymptotic consistency of learning\napproaches based on surrogate loss functions. Based on this result, we prove\nthat exponential loss and logistic loss are consistent with AUC, but hinge loss\nis inconsistent. Then, we derive the $q$-norm hinge loss and general hinge loss\nthat are consistent with AUC. We also derive the consistent bounds for\nexponential loss and logistic loss, and obtain the consistent bounds for many\nsurrogate loss functions under the non-noise setting. Further, we disclose an\nequivalence between the exponential surrogate loss of AUC and exponential\nsurrogate loss of accuracy, and one straightforward consequence of such finding\nis that AdaBoost and RankBoost are equivalent.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 02:37:44 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2012 08:35:28 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2012 07:00:09 GMT"}, {"version": "v4", "created": "Wed, 2 Jul 2014 14:46:59 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gao", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1208.0651", "submitter": "Muhammad Salman Asif", "authors": "M. Salman Asif, Justin Romberg", "title": "Fast and Accurate Algorithms for Re-Weighted L1-Norm Minimization", "comments": "Submitted to IEEE Trans. Signal Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  To recover a sparse signal from an underdetermined system, we often solve a\nconstrained L1-norm minimization problem. In many cases, the signal sparsity\nand the recovery performance can be further improved by replacing the L1 norm\nwith a \"weighted\" L1 norm. Without any prior information about nonzero elements\nof the signal, the procedure for selecting weights is iterative in nature.\nCommon approaches update the weights at every iteration using the solution of a\nweighted L1 problem from the previous iteration.\n  In this paper, we present two homotopy-based algorithms that efficiently\nsolve reweighted L1 problems. First, we present an algorithm that quickly\nupdates the solution of a weighted L1 problem as the weights change. Since the\nsolution changes only slightly with small changes in the weights, we develop a\nhomotopy algorithm that replaces the old weights with the new ones in a small\nnumber of computationally inexpensive steps. Second, we propose an algorithm\nthat solves a weighted L1 problem by adaptively selecting the weights while\nestimating the signal. This algorithm integrates the reweighting into every\nstep along the homotopy path by changing the weights according to the changes\nin the solution and its support, allowing us to achieve a high quality signal\nreconstruction by solving a single homotopy problem. We compare the performance\nof both algorithms, in terms of reconstruction accuracy and computational\ncomplexity, against state-of-the-art solvers and show that our methods have\nsmaller computational cost. In addition, we will show that the adaptive\nselection of the weights inside the homotopy often yields reconstructions of\nhigher quality.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 04:06:32 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Asif", "M. Salman", ""], ["Romberg", "Justin", ""]]}, {"id": "1208.0806", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Cross-conformal predictors", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduces the method of cross-conformal prediction, which is a\nhybrid of the methods of inductive conformal prediction and cross-validation,\nand studies its validity and predictive efficiency empirically.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 18:01:52 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1208.0848", "submitter": "Qiang Wu", "authors": "Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou", "title": "Learning Theory Approach to Minimum Error Entropy Criterion", "comments": null, "journal-ref": "JMLR 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum error entropy (MEE) criterion and an empirical risk\nminimization learning algorithm in a regression setting. A learning theory\napproach is presented for this MEE algorithm and explicit error bounds are\nprovided in terms of the approximation ability and capacity of the involved\nhypothesis space when the MEE scaling parameter is large. Novel asymptotic\nanalysis is conducted for the generalization error associated with Renyi's\nentropy and a Parzen window function, to overcome technical difficulties arisen\nfrom the essential differences between the classical least squares problems and\nthe MEE setting. A semi-norm and the involved symmetrized least squares error\nare introduced, which is related to some ranking algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 21:15:19 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 21:09:57 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Hu", "Ting", ""], ["Fan", "Jun", ""], ["Wu", "Qiang", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1208.0959", "submitter": "Misha Denil", "authors": "Misha Denil and Nando de Freitas", "title": "Recklessly Approximate Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been observed that certain extremely simple feature encoding\ntechniques are able to achieve state of the art performance on several standard\nimage classification benchmarks including deep belief networks, convolutional\nnets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and\nseveral others. Moreover, these \"triangle\" or \"soft threshold\" encodings are\nex- tremely efficient to compute. Several intuitive arguments have been put\nforward to explain this remarkable performance, yet no mathematical\njustification has been offered.\n  The main result of this report is to show that these features are realized as\nan approximate solution to the a non-negative sparse coding problem. Using this\nconnection we describe several variants of the soft threshold features and\ndemonstrate their effectiveness on two image classification benchmark tasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2012 21:48:52 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2013 19:00:48 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1208.1237", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Stephen A. Vavasis", "title": "Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix\n  Factorization", "comments": "30 pages, 2 figures, 7 tables. Main change: Improvement of the bound\n  of the main theorem (Th. 3), replacing r with sqrt(r)", "journal-ref": "IEEE Trans. on Pattern Analysis and Machine Intelligence 36 (4),\n  pp. 698-714, 2014", "doi": "10.1109/TPAMI.2013.226", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonnegative matrix factorization problem under\nthe separability assumption (that is, there exists a cone spanned by a small\nsubset of the columns of the input nonnegative data matrix containing all\ncolumns), which is equivalent to the hyperspectral unmixing problem under the\nlinear mixing model and the pure-pixel assumption. We present a family of fast\nrecursive algorithms, and prove they are robust under any small perturbations\nof the input data matrix. This family generalizes several existing\nhyperspectral unmixing algorithms and hence provides for the first time a\ntheoretical justification of their better practical performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 18:49:07 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2012 20:57:52 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2013 14:15:01 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1208.1259", "submitter": "Ping Li", "authors": "Ping Li and Art Owen and Cun-Hui Zhang", "title": "One Permutation Hashing for Efficient Search and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the method of b-bit minwise hashing has been applied to large-scale\nlinear learning and sublinear time near-neighbor search. The major drawback of\nminwise hashing is the expensive preprocessing cost, as the method requires\napplying (e.g.,) k=200 to 500 permutations on the data. The testing time can\nalso be expensive if a new data point (e.g., a new document or image) has not\nbeen processed, which might be a significant issue in user-facing applications.\n  We develop a very simple solution based on one permutation hashing.\nConceptually, given a massive binary data matrix, we permute the columns only\nonce and divide the permuted columns evenly into k bins; and we simply store,\nfor each data vector, the smallest nonzero location in each bin. The\ninteresting probability analysis (which is validated by experiments) reveals\nthat our one permutation scheme should perform very similarly to the original\n(k-permutation) minwise hashing. In fact, the one permutation scheme can be\neven slightly more accurate, due to the \"sample-without-replacement\" effect.\n  Our experiments with training linear SVM and logistic regression on the\nwebspam dataset demonstrate that this one permutation hashing scheme can\nachieve the same (or even slightly better) accuracies compared to the original\nk-permutation scheme. To test the robustness of our method, we also experiment\nwith the small news20 dataset which is very sparse and has merely on average\n500 nonzeros in each data vector. Interestingly, our one permutation scheme\nnoticeably outperforms the k-permutation scheme when k is not too small on the\nnews20 dataset. In summary, our method can achieve at least the same accuracy\nas the original k-permutation scheme, at merely 1/k of the original\npreprocessing cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 12:28:06 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Li", "Ping", ""], ["Owen", "Art", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1208.2043", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats", "title": "High-Dimensional Screening Using Multiple Grouping of Variables", "comments": "This paper will appear in the IEEE Transactions on Signal Processing.\n  See http://www.ima.umn.edu/~dvats/MuGScreening.html for more details", "journal-ref": null, "doi": "10.1109/TSP.2013.2294591", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening is the problem of finding a superset of the set of non-zero entries\nin an unknown p-dimensional vector \\beta* given n noisy observations.\nNaturally, we want this superset to be as small as possible. We propose a novel\nframework for screening, which we refer to as Multiple Grouping (MuG), that\ngroups variables, performs variable selection over the groups, and repeats this\nprocess multiple number of times to estimate a sequence of sets that contains\nthe non-zero entries in \\beta*. Screening is done by taking an intersection of\nall these estimated sets. The MuG framework can be used in conjunction with any\ngroup based variable selection algorithm. In the high-dimensional setting,\nwhere p >> n, we show that when MuG is used with the group Lasso estimator,\nscreening can be consistently performed without using any tuning parameter. Our\nnumerical simulations clearly show the merits of using the MuG framework in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 21:22:04 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2013 06:46:13 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2013 17:03:53 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Vats", "Divyanshu", ""]]}, {"id": "1208.2278", "submitter": "Peter Van de Ven", "authors": "Kush R. Varshney and Peter M. van de Ven", "title": "Balancing Lifetime and Classification Accuracy of Wireless Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor networks are composed of distributed sensors that can be used\nfor signal detection or classification. The likelihood functions of the\nhypotheses are often not known in advance, and decision rules have to be\nlearned via supervised learning. A specific such algorithm is Fisher\ndiscriminant analysis (FDA), the classification accuracy of which has been\npreviously studied in the context of wireless sensor networks. Previous work,\nhowever, does not take into account the communication protocol or battery\nlifetime of the sensor networks; in this paper we extend the existing studies\nby proposing a model that captures the relationship between battery lifetime\nand classification accuracy. In order to do so we combine the FDA with a model\nthat captures the dynamics of the Carrier-Sense Multiple-Access (CSMA)\nalgorithm, the random-access algorithm used to regulate communications in\nsensor networks. This allows us to study the interaction between the\nclassification accuracy, battery lifetime and effort put towards learning, as\nwell as the impact of the back-off rates of CSMA on the accuracy. We\ncharacterize the tradeoff between the length of the training stage and\naccuracy, and show that accuracy is non-monotone in the back-off rate due to\nchanges in the training sample size and overfitting.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 20:18:00 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Varshney", "Kush R.", ""], ["van de Ven", "Peter M.", ""]]}, {"id": "1208.2417", "submitter": "Assaf Hallak", "authors": "Assaf Hallak and Shie Mannor", "title": "How to sample if you must: on optimal functional sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a fundamental problem that models various active sampling setups,\nsuch as network tomography. We analyze sampling of a multivariate normal\ndistribution with an unknown expectation that needs to be estimated: in our\nsetup it is possible to sample the distribution from a given set of linear\nfunctionals, and the difficulty addressed is how to optimally select the\ncombinations to achieve low estimation error. Although this problem is in the\nheart of the field of optimal design, no efficient solutions for the case with\nmany functionals exist. We present some bounds and an efficient sub-optimal\nsolution for this problem for more structured sets such as binary functionals\nthat are induced by graph walks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2012 10:12:48 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Hallak", "Assaf", ""], ["Mannor", "Shie", ""]]}, {"id": "1208.2523", "submitter": "Konrad Rawlik", "authors": "Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar", "title": "Path Integral Control by Reproducing Kernel Hilbert Space Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an embedding of stochastic optimal control problems, of the so\ncalled path integral form, into reproducing kernel Hilbert spaces. Using\nconsistent, sample based estimates of the embedding leads to a model free,\nnon-parametric approach for calculation of an approximate solution to the\ncontrol problem. This formulation admits a decomposition of the problem into an\ninvariant and task dependent component. Consequently, we make much more\nefficient use of the sample data compared to previous sample based approaches\nin this domain, e.g., by allowing sample re-use across tasks. Numerical\nexamples on test problems, which illustrate the sample efficiency, are\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 08:30:14 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Rawlik", "Konrad", ""], ["Toussaint", "Marc", ""], ["Vijayakumar", "Sethu", ""]]}, {"id": "1208.2572", "submitter": "Silvia Villa", "authors": "Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro\n  verri", "title": "Nonparametric sparsity and regularization", "comments": "45 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we are interested in the problems of supervised learning and\nvariable selection when the input-output dependence is described by a nonlinear\nfunction depending on a few variables. Our goal is to consider a sparse\nnonparametric model, hence avoiding linear or additive models. The key idea is\nto measure the importance of each variable in the model by making use of\npartial derivatives. Based on this intuition we propose a new notion of\nnonparametric sparsity and a corresponding least squares regularization scheme.\nUsing concepts and results from the theory of reproducing kernel Hilbert spaces\nand proximal methods, we show that the proposed learning algorithm corresponds\nto a minimization problem which can be provably solved by an iterative\nprocedure. The consistency properties of the obtained estimator are studied\nboth in terms of prediction and selection performance. An extensive empirical\nanalysis shows that the proposed method performs favorably with respect to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 13:02:33 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""], ["Mosci", "Sofia", ""], ["Santoro", "Matteo", ""], ["verri", "Alessandro", ""]]}, {"id": "1208.2651", "submitter": "Manuel J. A. Eugster", "authors": "Anne-Laure Boulesteix and Manuel J. A. Eugster", "title": "A Plea for Neutral Comparison Studies in Computational Sciences", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0061562", "report-no": null, "categories": "stat.CO cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context where most published articles are devoted to the development of\n\"new methods\", comparison studies are generally appreciated by readers but\nsurprisingly given poor consideration by many scientific journals. In\nconnection with recent articles on over-optimism and epistemology published in\nBioinformatics, this letter stresses the importance of neutral comparison\nstudies for the objective evaluation of existing methods and the establishment\nof standards by drawing parallels with clinical research.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:01:17 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Boulesteix", "Anne-Laure", ""], ["Eugster", "Manuel J. A.", ""]]}, {"id": "1208.2873", "submitter": "Vasileios Lampos", "authors": "Vasileios Lampos", "title": "Detecting Events and Patterns in Large-Scale User Generated Textual\n  Streams with Statistical Learning Methods", "comments": "PhD thesis, 238 pages, 9 chapters, 2 appendices, 58 figures, 49\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A vast amount of textual web streams is influenced by events or phenomena\nemerging in the real world. The social web forms an excellent modern paradigm,\nwhere unstructured user generated content is published on a regular basis and\nin most occasions is freely distributed. The present Ph.D. Thesis deals with\nthe problem of inferring information - or patterns in general - about events\nemerging in real life based on the contents of this textual stream. We show\nthat it is possible to extract valuable information about social phenomena,\nsuch as an epidemic or even rainfall rates, by automatic analysis of the\ncontent published in Social Media, and in particular Twitter, using Statistical\nMachine Learning methods. An important intermediate task regards the formation\nand identification of features which characterise a target event; we select and\nuse those textual features in several linear, non-linear and hybrid inference\napproaches achieving a significantly good performance in terms of the applied\nloss function. By examining further this rich data set, we also propose methods\nfor extracting various types of mood signals revealing how affective norms - at\nleast within the social web's population - evolve during the day and how\nsignificant events emerging in the real world are influencing them. Lastly, we\npresent some preliminary findings showing several spatiotemporal\ncharacteristics of this textual information as well as the potential of using\nit to tackle tasks such as the prediction of voting intentions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:59:54 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Lampos", "Vasileios", ""]]}, {"id": "1208.3014", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Eric P. Xing", "title": "Efficient Algorithm for Extremely Large Multi-task Regression with\n  Massive Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a highly scalable optimization method called \"hierarchical\ngroup-thresholding\" for solving a multi-task regression model with complex\nstructured sparsity constraints on both input and output spaces. Despite the\nrecent emergence of several efficient optimization algorithms for tackling\ncomplex sparsity-inducing regularizers, true scalability in practical\nhigh-dimensional problems where a huge amount (e.g., millions) of sparsity\npatterns need to be enforced remains an open challenge, because all existing\nalgorithms must deal with ALL such patterns exhaustively in every iteration,\nwhich is computationally prohibitive. Our proposed algorithm addresses the\nscalability problem by screening out multiple groups of coefficients\nsimultaneously and systematically. We employ a hierarchical tree representation\nof group constraints to accelerate the process of removing irrelevant\nconstraints by taking advantage of the inclusion relationships between group\nsparsities, thereby avoiding dealing with all constraints in every optimization\nstep, and necessitating optimization operation only on a small number of\noutstanding coefficients. In our experiments, we demonstrate the efficiency of\nour method on simulation datasets, and in an application of detecting genetic\nvariants associated with gene expression traits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 01:54:31 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1208.3030", "submitter": "Dacheng Tao", "authors": "Wei Bian and Dacheng Tao", "title": "Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's linear discriminant analysis (FLDA) is an important dimension\nreduction method in statistical pattern recognition. It has been shown that\nFLDA is asymptotically Bayes optimal under the homoscedastic Gaussian\nassumption. However, this classical result has the following two major\nlimitations: 1) it holds only for a fixed dimensionality $D$, and thus does not\napply when $D$ and the training sample size $N$ are proportionally large; 2) it\ndoes not provide a quantitative description on how the generalization ability\nof FLDA is affected by $D$ and $N$. In this paper, we present an asymptotic\ngeneralization analysis of FLDA based on random matrix theory, in a setting\nwhere both $D$ and $N$ increase and $D/N\\longrightarrow\\gamma\\in[0,1)$. The\nobtained lower bound of the generalization discrimination power overcomes both\nlimitations of the classical result, i.e., it is applicable when $D$ and $N$\nare proportionally large and provides a quantitative description of the\ngeneralization ability of FLDA in terms of the ratio $\\gamma=D/N$ and the\npopulation discrimination power. Besides, the discrimination power bound also\nleads to an upper bound on the generalization error of binary-classification\nwith FLDA.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 05:35:36 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 04:12:22 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Bian", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1208.3279", "submitter": "David Weiss", "authors": "David Weiss, Benjamin Sapp, Ben Taskar", "title": "Structured Prediction Cascades", "comments": "32 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction tasks pose a fundamental trade-off between the need for\nmodel complexity to increase predictive power and the limited computational\nresources for inference in the exponentially-sized output spaces such models\nrequire. We formulate and develop the Structured Prediction Cascade\narchitecture: a sequence of increasingly complex models that progressively\nfilter the space of possible outputs. The key principle of our approach is that\neach model in the cascade is optimized to accurately filter and refine the\nstructured output state space of the next model, speeding up both learning and\ninference in the next layer of the cascade. We learn cascades by optimizing a\nnovel convex loss function that controls the trade-off between the filtering\nefficiency and the accuracy of the cascade, and provide generalization bounds\nfor both accuracy and efficiency. We also extend our approach to intractable\nmodels using tree-decomposition ensembles, and provide algorithms and theory\nfor this setting. We evaluate our approach on several large-scale problems,\nachieving state-of-the-art performance in handwriting recognition and human\npose recognition. We find that structured prediction cascades allow tremendous\nspeedups and the use of previously intractable features and models in both\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 16:20:23 GMT"}], "update_date": "2012-08-17", "authors_parsed": [["Weiss", "David", ""], ["Sapp", "Benjamin", ""], ["Taskar", "Ben", ""]]}, {"id": "1208.3380", "submitter": "Wei Sun", "authors": "Wei Sun, Junhui Wang and Yixin Fang", "title": "Consistent selection of tuning parameters via variable selection\n  stability", "comments": "Published in JMLR (http://jmlr.org/papers/v14/)", "journal-ref": "Journal of Machine Learning Research 2013, Vol. 14, 3419-3440", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression models are popularly used in high-dimensional data\nanalysis to conduct variable selection and model fitting simultaneously.\nWhereas success has been widely reported in literature, their performances\nlargely depend on the tuning parameters that balance the trade-off between\nmodel fitting and model sparsity. Existing tuning criteria mainly follow the\nroute of minimizing the estimated prediction error or maximizing the posterior\nmodel probability, such as cross-validation, AIC and BIC. This article\nintroduces a general tuning parameter selection criterion based on a novel\nconcept of variable selection stability. The key idea is to select the tuning\nparameters so that the resultant penalized regression model is stable in\nvariable selection. The asymptotic selection consistency is established for\nboth fixed and diverging dimensions. The effectiveness of the proposed\ncriterion is also demonstrated in a variety of simulated examples as well as an\napplication to the prostate cancer data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 14:23:40 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2013 05:11:59 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Sun", "Wei", ""], ["Wang", "Junhui", ""], ["Fang", "Yixin", ""]]}, {"id": "1208.3422", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle", "title": "Distance Metric Learning for Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in metric learning has significantly improved the\nstate-of-the-art in k-nearest neighbor classification. Support vector machines\n(SVM), particularly with RBF kernels, are amongst the most popular\nclassification algorithms that uses distance metrics to compare examples. This\npaper provides an empirical analysis of the efficacy of three of the most\npopular Mahalanobis metric learning algorithms as pre-processing for SVM\ntraining. We show that none of these algorithms generate metrics that lead to\nparticularly satisfying improvements for SVM-RBF classification. As a remedy we\nintroduce support vector metric learning (SVML), a novel algorithm that\nseamlessly combines the learning of a Mahalanobis metric with the training of\nthe RBF-SVM parameters. We demonstrate the capabilities of SVML on nine\nbenchmark data sets of varying sizes and difficulties. In our study, SVML\noutperforms all alternative state-of-the-art metric learning algorithms in\nterms of accuracy and establishes itself as a serious alternative to the\nstandard Euclidean metric with model selection by cross validation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 17:16:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2013 20:26:55 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Xu", "Zhixiang", ""], ["Weinberger", "Kilian Q.", ""], ["Chapelle", "Olivier", ""]]}, {"id": "1208.3687", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Vishal M. Patel, Rama Chellappa", "title": "Information-theoretic Dictionary Learning for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a two-stage approach for learning dictionaries for object\nclassification tasks based on the principle of information maximization. The\nproposed method seeks a dictionary that is compact, discriminative, and\ngenerative. In the first stage, dictionary atoms are selected from an initial\ndictionary by maximizing the mutual information measure on dictionary\ncompactness, discrimination and reconstruction. In the second stage, the\nselected dictionary atoms are updated for improved reconstructive and\ndiscriminative power using a simple gradient ascent algorithm on mutual\ninformation. Experiments using real datasets demonstrate the effectiveness of\nour approach for image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 20:38:56 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1208.3728", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Online Learning with Predictable Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for online linear optimization that take advantage of\nbenign (as opposed to worst-case) sequences. Specifically if the sequence\nencountered by the learner is described well by a known \"predictable process\",\nthe algorithms presented enjoy tighter bounds as compared to the typical worst\ncase bounds. Additionally, the methods achieve the usual worst-case regret\nbounds if the sequence is not benign. Our approach can be seen as a way of\nadding prior knowledge about the sequence within the paradigm of online\nlearning. The setting is shown to encompass partial and side information.\nVariance and path-length bounds can be seen as particular examples of online\nlearning with simple predictable sequences.\n  We further extend our methods and results to include competing with a set of\npossible predictable processes (models), that is \"learning\" the predictable\nprocess itself concurrently with using it to obtain better regret guarantees.\nWe show that such model selection is possible under various assumptions on the\navailable feedback. Our results suggest a promising direction of further\nresearch with potential applications to stock market and time series\nprediction.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 24 May 2014 10:56:17 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1208.3822", "submitter": "Jim Jing-Yan Wang", "authors": "Jingyan Wang", "title": "Joint-ViVo: Selecting and Weighting Visual Words Jointly for\n  Bag-of-Features based Tissue Classification in Medical Images", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically classifying the tissues types of Region of Interest (ROI) in\nmedical imaging has been an important application in Computer-Aided Diagnosis\n(CAD), such as classification of breast parenchymal tissue in the mammogram,\nclassify lung disease patterns in High-Resolution Computed Tomography (HRCT)\netc. Recently, bag-of-features method has shown its power in this field,\ntreating each ROI as a set of local features. In this paper, we investigate\nusing the bag-of-features strategy to classify the tissue types in medical\nimaging applications. Two important issues are considered here: the visual\nvocabulary learning and weighting. Although there are already plenty of\nalgorithms to deal with them, all of them treat them independently, namely, the\nvocabulary learned first and then the histogram weighted. Inspired by\nAuto-Context who learns the features and classifier jointly, we try to develop\na novel algorithm that learns the vocabulary and weights jointly. The new\nalgorithm, called Joint-ViVo, works in an iterative way. In each iteration, we\nfirst learn the weights for each visual word by maximizing the margin of ROI\ntriplets, and then select the most discriminate visual words based on the\nlearned weights for the next iteration. We test our algorithm on three tissue\nclassification tasks: identifying brain tissue type in magnetic resonance\nimaging (MRI), classifying lung tissue in HRCT images, and classifying breast\ntissue density in mammograms. The results show that Joint-ViVo can perform\neffectively for classifying tissues.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 11:12:44 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 07:30:44 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Wang", "Jingyan", ""]]}, {"id": "1208.3839", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang", "title": "Discriminative Sparse Coding on Multi-Manifold for Data Representation\n  and Classification", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding has been popularly used as an effective data representation\nmethod in various applications, such as computer vision, medical imaging and\nbioinformatics, etc. However, the conventional sparse coding algorithms and its\nmanifold regularized variants (graph sparse coding and Laplacian sparse\ncoding), learn the codebook and codes in a unsupervised manner and neglect the\nclass information available in the training set. To address this problem, in\nthis paper we propose a novel discriminative sparse coding method based on\nmulti-manifold, by learning discriminative class-conditional codebooks and\nsparse codes from both data feature space and class labels. First, the entire\ntraining set is partitioned into multiple manifolds according to the class\nlabels. Then, we formulate the sparse coding as a manifold-manifold matching\nproblem and learn class-conditional codebooks and codes to maximize the\nmanifold margins of different classes. Lastly, we present a data point-manifold\nmatching error based strategy to classify the unlabeled data point.\nExperimental results on somatic mutations identification and breast tumors\nclassification in ultrasonic images tasks demonstrate the efficacy of the\nproposed data representation-classification approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 14:49:27 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 14:21:40 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""]]}, {"id": "1208.3845", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang and Mustafa AbdulJabbar", "title": "Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been continuously evolving in\nseveral areas like pattern recognition and information retrieval methods. It\nfactorizes a matrix into a product of 2 low-rank non-negative matrices that\nwill define parts-based, and linear representation of nonnegative data.\nRecently, Graph regularized NMF (GrNMF) is proposed to find a compact\nrepresentation,which uncovers the hidden semantics and simultaneously respects\nthe intrinsic geometric structure. In GNMF, an affinity graph is constructed\nfrom the original data space to encode the geometrical information. In this\npaper, we propose a novel idea which engages a Multiple Kernel Learning\napproach into refining the graph structure that reflects the factorization of\nthe matrix and the new data space. The GrNMF is improved by utilizing the graph\nrefined by the kernel learning, and then a novel kernel learning method is\nintroduced under the GrNMF framework. Our approach shows encouraging results of\nthe proposed algorithm in comparison to the state-of-the-art clustering\nalgorithms like NMF, GrNMF, SVD etc.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 15:21:09 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2012 07:24:22 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 14:21:50 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""], ["AbdulJabbar", "Mustafa", ""]]}, {"id": "1208.3943", "submitter": "Jay Gholap B.Tech.(Computer Engineering)", "authors": "Jay Gholap", "title": "Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility", "comments": "5 Pages", "journal-ref": "Published in Asian Journal of Computer Science and Information\n  Technology,Vol 2,No. 8 (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining involves the systematic analysis of large data sets, and data\nmining in agricultural soil datasets is exciting and modern research area. The\nproductive capacity of a soil depends on soil fertility. Achieving and\nmaintaining appropriate levels of soil fertility, is of utmost importance if\nagricultural land is to remain capable of nourishing crop production. In this\nresearch, Steps for building a predictive model of soil fertility have been\nexplained.\n  This paper aims at predicting soil fertility class using decision tree\nalgorithms in data mining . Further, it focuses on performance tuning of J48\ndecision tree algorithm with the help of meta-techniques such as attribute\nselection and boosting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 08:48:40 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Gholap", "Jay", ""]]}, {"id": "1208.4138", "submitter": "Zahoor Khan", "authors": "Ashraf Mohammed Iqbal, Abidalrahman Moh'd, Zahoor Khan", "title": "Semi-supervised Clustering Ensemble by Voting", "comments": "The International Conference on Information and Communication Systems\n  (ICICS 2009), Amman, Jordan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering ensemble is one of the most recent advances in unsupervised\nlearning. It aims to combine the clustering results obtained using different\nalgorithms or from different runs of the same clustering algorithm for the same\ndata set, this is accomplished using on a consensus function, the efficiency\nand accuracy of this method has been proven in many works in literature. In the\nfirst part of this paper we make a comparison among current approaches to\nclustering ensemble in literature. All of these approaches consist of two main\nsteps: the ensemble generation and consensus function. In the second part of\nthe paper, we suggest engaging supervision in the clustering ensemble procedure\nto get more enhancements on the clustering results. Supervision can be applied\nin two places: either by using semi-supervised algorithms in the clustering\nensemble generation step or in the form of a feedback used by the consensus\nfunction stage. Also, we introduce a flexible two parameter weighting\nmechanism, the first parameter describes the compatibility between the datasets\nunder study and the semi-supervised clustering algorithms used to generate the\nbase partitions, the second parameter is used to provide the user feedback on\nthe these partitions. The two parameters are engaged in a \"relabeling and\nvoting\" based consensus function to produce the final clustering.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 23:21:10 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Iqbal", "Ashraf Mohammed", ""], ["Moh'd", "Abidalrahman", ""], ["Khan", "Zahoor", ""]]}, {"id": "1208.4183", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu", "title": "Learning LiNGAM based on data with more variables than observations", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very important topic in systems biology is developing statistical methods\nthat automatically find causal relations in gene regulatory networks with no\nprior knowledge of causal connectivity. Many methods have been developed for\ntime series data. However, discovery methods based on steady-state data are\noften necessary and preferable since obtaining time series data can be more\nexpensive and/or infeasible for many biological systems. A conventional\napproach is causal Bayesian networks. However, estimation of Bayesian networks\nis ill-posed. In many cases it cannot uniquely identify the underlying causal\nnetwork and only gives a large class of equivalent causal networks that cannot\nbe distinguished between based on the data distribution. We propose a new\ndiscovery algorithm for uniquely identifying the underlying causal network of\ngenes. To the best of our knowledge, the proposed method is the first algorithm\nfor learning gene networks based on a fully identifiable causal model called\nLiNGAM. We here compare our algorithm with competing algorithms using\nartificially-generated data, although it is definitely better to test it based\non real microarray gene expression data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 03:46:08 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Shimizu", "Shohei", ""]]}, {"id": "1208.4271", "submitter": "Giuseppe Jurman", "authors": "Davide Albanese, Michele Filosi, Roberto Visintainer, Samantha\n  Riccadonna, Giuseppe Jurman, Cesare Furlanello", "title": "Minerva and minepy: a C engine for the MINE suite and its R, Python and\n  MATLAB wrappers", "comments": "Bioinformatics 2012, in press", "journal-ref": null, "doi": "10.1093/bioinformatics/bts707", "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel implementation in ANSI C of the MINE family of\nalgorithms for computing maximal information-based measures of dependence\nbetween two variables in large datasets, with the aim of a low memory footprint\nand ease of integration within bioinformatics pipelines. We provide the\nlibraries minerva (with the R interface) and minepy for Python, MATLAB, Octave\nand C++. The C solution reduces the large memory requirement of the original\nJava implementation, has good upscaling properties, and offers a native\nparallelization for the R interface. Low memory requirements are demonstrated\non the MINE benchmarks as well as on large (n=1340) microarray and Illumina\nGAII RNA-seq transcriptomics datasets.\n  Availability and Implementation: Source code and binaries are freely\navailable for download under GPL3 licence at http://minepy.sourceforge.net for\nminepy and through the CRAN repository http://cran.r-project.org for the R\npackage minerva. All software is multiplatform (MS Windows, Linux and OSX).\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 14:03:36 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2012 09:32:58 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Albanese", "Davide", ""], ["Filosi", "Michele", ""], ["Visintainer", "Roberto", ""], ["Riccadonna", "Samantha", ""], ["Jurman", "Giuseppe", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1208.4398", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Rama Chellappa", "title": "A Unified Approach for Modeling and Recognition of Individual Actions\n  and Group Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing group activities is challenging due to the difficulties in\nisolating individual entities, finding the respective roles played by the\nindividuals and representing the complex interactions among the participants.\nIndividual actions and group activities in videos can be represented in a\ncommon framework as they share the following common feature: both are composed\nof a set of low-level features describing motions, e.g., optical flow for each\npixel or a trajectory for each feature point, according to a set of composition\nconstraints in both temporal and spatial dimensions. In this paper, we present\na unified model to assess the similarity between two given individual or group\nactivities. Our approach avoids explicit extraction of individual actors,\nidentifying and representing the inter-person interactions. With the proposed\napproach, retrieval from a video database can be performed through\nQuery-by-Example; and activities can be recognized by querying videos\ncontaining known activities. The suggested video matching process can be\nperformed in an unsupervised manner. We demonstrate the performance of our\napproach by recognizing a set of human actions and football plays.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 22:40:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Chellappa", "Rama", ""]]}, {"id": "1208.4411", "submitter": "Ahmed Hefny", "authors": "Avinava Dubey, Ahmed Hefny, Sinead Williamson, Eric P. Xing", "title": "A non-parametric mixture model for topic modeling over time", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single, stationary topic model such as latent Dirichlet allocation is\ninappropriate for modeling corpora that span long time periods, as the\npopularity of topics is likely to change over time. A number of models that\nincorporate time have been proposed, but in general they either exhibit limited\nforms of temporal variation, or require computationally expensive inference\nmethods. In this paper we propose non-parametric Topics over Time (npTOT), a\nmodel for time-varying topics that allows an unbounded number of topics and\nexible distribution over the temporal variations in those topics' popularity.\nWe develop a collapsed Gibbs sampler for the proposed model and compare against\nexisting models on synthetic and real document sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 02:02:40 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dubey", "Avinava", ""], ["Hefny", "Ahmed", ""], ["Williamson", "Sinead", ""], ["Xing", "Eric P.", ""]]}, {"id": "1208.5062", "submitter": "Yao Xie", "authors": "Yao Xie, Jiaji Huang, Rebecca Willett", "title": "Changepoint detection for high-dimensional time series with missing data", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2012.2234082", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to change-point detection when the\nobserved high-dimensional data may have missing elements. The performance of\nclassical methods for change-point detection typically scales poorly with the\ndimensionality of the data, so that a large number of observations are\ncollected after the true change-point before it can be reliably detected.\nFurthermore, missing components in the observed data handicap conventional\napproaches. The proposed method addresses these challenges by modeling the\ndynamic distribution underlying the data as lying close to a time-varying\nlow-dimensional submanifold embedded within the ambient observation space.\nSpecifically, streaming data is used to track a submanifold approximation,\nmeasure deviations from this approximation, and calculate a series of\nstatistics of the deviations for detecting when the underlying manifold has\nchanged in a sharp or unexpected manner. The approach described in this paper\nleverages several recent results in the field of high-dimensional data\nanalysis, including subspace tracking with missing data, multiscale analysis\ntechniques for point clouds, online optimization, and change-point detection\nperformance analysis. Simulations and experiments highlight the robustness and\nefficacy of the proposed approach in detecting an abrupt change in an otherwise\nslowly varying low-dimensional manifold.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2012 20:36:36 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2012 20:27:36 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2012 20:30:49 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Xie", "Yao", ""], ["Huang", "Jiaji", ""], ["Willett", "Rebecca", ""]]}, {"id": "1208.5092", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaogang Wang, Deli Zhao and Xiaoou Tang", "title": "Graph Degree Linkage: Agglomerative Clustering on a Directed Graph", "comments": "Proceedings of European Conference on Computer Vision (ECCV), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple but effective graph-based agglomerative\nalgorithm, for clustering high-dimensional data. We explore the different roles\nof two fundamental concepts in graph theory, indegree and outdegree, in the\ncontext of clustering. The average indegree reflects the density near a sample,\nand the average outdegree characterizes the local geometry around a sample.\nBased on such insights, we define the affinity measure of clusters via the\nproduct of average indegree and average outdegree. The product-based affinity\nmakes our algorithm robust to noise. The algorithm has three main advantages:\ngood performance, easy implementation, and high computational efficiency. We\ntest the algorithm on two fundamental computer vision problems: image\nclustering and object matching. Extensive experiments demonstrate that it\noutperforms the state-of-the-arts in both applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2012 02:51:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zhang", "Wei", ""], ["Wang", "Xiaogang", ""], ["Zhao", "Deli", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1208.6338", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "A Widely Applicable Bayesian Information Criterion", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical model or a learning machine is called regular if the map taking\na parameter to a probability distribution is one-to-one and if its Fisher\ninformation matrix is always positive definite. If otherwise, it is called\nsingular. In regular statistical models, the Bayes free energy, which is\ndefined by the minus logarithm of Bayes marginal likelihood, can be\nasymptotically approximated by the Schwarz Bayes information criterion (BIC),\nwhereas in singular models such approximation does not hold.\n  Recently, it was proved that the Bayes free energy of a singular model is\nasymptotically given by a generalized formula using a birational invariant, the\nreal log canonical threshold (RLCT), instead of half the number of parameters\nin BIC. Theoretical values of RLCTs in several statistical models are now being\ndiscovered based on algebraic geometrical methodology. However, it has been\ndifficult to estimate the Bayes free energy using only training samples,\nbecause an RLCT depends on an unknown true distribution.\n  In the present paper, we define a widely applicable Bayesian information\ncriterion (WBIC) by the average log likelihood function over the posterior\ndistribution with the inverse temperature $1/\\log n$, where $n$ is the number\nof training samples. We mathematically prove that WBIC has the same asymptotic\nexpansion as the Bayes free energy, even if a statistical model is singular for\nand unrealizable by a statistical model. Since WBIC can be numerically\ncalculated without any information about a true distribution, it is a\ngeneralized version of BIC onto singular statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 00:31:34 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Watanabe", "Sumio", ""]]}]