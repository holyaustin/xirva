[{"id": "1404.0099", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Daniel Selsam and Yura Perov", "title": "Venture: a higher-order probabilistic programming platform with\n  programmable inference", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Venture, an interactive virtual machine for probabilistic\nprogramming that aims to be sufficiently expressive, extensible, and efficient\nfor general-purpose use. Like Church, probabilistic models and inference\nproblems in Venture are specified via a Turing-complete, higher-order\nprobabilistic language descended from Lisp. Unlike Church, Venture also\nprovides a compositional language for custom inference strategies built out of\nscalable exact and approximate techniques. We also describe four key aspects of\nVenture's implementation that build on ideas from probabilistic graphical\nmodels. First, we describe the stochastic procedure interface (SPI) that\nspecifies and encapsulates primitive random variables. The SPI supports custom\ncontrol flow, higher-order probabilistic procedures, partially exchangeable\nsequences and ``likelihood-free'' stochastic simulators. It also supports\nexternal models that do inference over latent variables hidden from Venture.\nSecond, we describe probabilistic execution traces (PETs), which represent\nexecution histories of Venture programs. PETs capture conditional dependencies,\nexistential dependencies and exchangeable coupling. Third, we describe\npartitions of execution histories called scaffolds that factor global inference\nproblems into coherent sub-problems. Finally, we describe a family of\nstochastic regeneration algorithms for efficiently modifying PET fragments\ncontained within scaffolds. Stochastic regeneration linear runtime scaling in\ncases where many previous approaches scaled quadratically. We show how to use\nstochastic regeneration and the SPI to implement general-purpose inference\nstrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals\nbased on particle Markov chain Monte Carlo and mean-field variational inference\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 01:44:05 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Selsam", "Daniel", ""], ["Perov", "Yura", ""]]}, {"id": "1404.0298", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Yingbin Liang, H. Vincent Poor", "title": "A Kernel-Based Nonparametric Test for Anomaly Detection over Line\n  Networks", "comments": "This paper has been withdrawn because we have submitted a complete\n  version. The complete version is arXiv:1604.01351", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonparametric problem of detecting existence of an anomalous interval\nover a one dimensional line network is studied. Nodes corresponding to an\nanomalous interval (if exists) receive samples generated by a distribution q,\nwhich is different from the distribution p that generates samples for other\nnodes. If anomalous interval does not exist, then all nodes receive samples\ngenerated by p. It is assumed that the distributions p and q are arbitrary, and\nare unknown. In order to detect whether an anomalous interval exists, a test is\nbuilt based on mean embeddings of distributions into a reproducing kernel\nHilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It is\nshown that as the network size n goes to infinity, if the minimum length of\ncandidate anomalous intervals is larger than a threshold which has the order\nO(log n), the proposed test is asymptotically successful, i.e., the probability\nof detection error approaches zero asymptotically. An efficient algorithm to\nperform the test with substantial computational complexity reduction is\nproposed, and is shown to be asymptotically successful if the condition on the\nminimum length of candidate anomalous interval is satisfied. Numerical results\nare provided, which are consistent with the theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 16:12:51 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 20:28:29 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Zou", "Shaofeng", ""], ["Liang", "Yingbin", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1404.0329", "submitter": "Samuel Kaski", "authors": "Ali Faisal, Jaakko Peltonen, Elisabeth Georgii, Johan Rung and Samuel\n  Kaski", "title": "Toward computational cumulative biology by combining models of\n  biological datasets", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0113053", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main challenge of data-driven sciences is how to make maximal use of the\nprogressively expanding databases of experimental datasets in order to keep\nresearch cumulative. We introduce the idea of a modeling-based dataset\nretrieval engine designed for relating a researcher's experimental dataset to\nearlier work in the field. The search is (i) data-driven to enable new\nfindings, going beyond the state of the art of keyword searches in annotations,\n(ii) modeling-driven, to both include biological knowledge and insights learned\nfrom data, and (iii) scalable, as it is accomplished without building one\nunified grand model of all data. Assuming each dataset has been modeled\nbeforehand, by the researchers or by database managers, we apply a rapidly\ncomputable and optimizable combination model to decompose a new dataset into\ncontributions from earlier relevant models. By using the data-driven\ndecomposition we identify a network of interrelated datasets from a large\nannotated human gene expression atlas. While tissue type and disease were major\ndriving forces for determining relevant datasets, the found relationships were\nricher and the model-based search was more accurate than keyword search; it\nmoreover recovered biologically meaningful relationships that are not\nstraightforwardly visible from annotations, for instance, between cells in\ndifferent developmental stages such as thymocytes and T-cells. Data-driven\nlinks and citations matched to a large extent; the data-driven links even\nuncovered corrections to the publication data, as two of the most linked\ndatasets were not highly cited and turned out to have wrong publication entries\nin the database.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 17:55:57 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Faisal", "Ali", ""], ["Peltonen", "Jaakko", ""], ["Georgii", "Elisabeth", ""], ["Rung", "Johan", ""], ["Kaski", "Samuel", ""]]}, {"id": "1404.0400", "submitter": "Georgios Evangelopoulos", "authors": "Chiyuan Zhang, Georgios Evangelopoulos, Stephen Voinea, Lorenzo\n  Rosasco, Tomaso Poggio", "title": "A Deep Representation for Invariance And Music Classification", "comments": "5 pages, CBMM Memo No. 002, (to appear) IEEE 2014 International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014)", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854954", "report-no": "CBMM Memo No. 002", "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations in the auditory cortex might be based on mechanisms similar\nto the visual ventral stream; modules for building invariance to\ntransformations and multiple layers for compositionality and selectivity. In\nthis paper we propose the use of such computational modules for extracting\ninvariant and discriminative audio representations. Building on a theory of\ninvariance in hierarchical architectures, we propose a novel, mid-level\nrepresentation for acoustical signals, using the empirical distributions of\nprojections on a set of templates and their transformations. Under the\nassumption that, by construction, this dictionary of templates is composed from\nsimilar classes, and samples the orbit of variance-inducing signal\ntransformations (such as shift and scale), the resulting signature is\ntheoretically guaranteed to be unique, invariant to transformations and stable\nto deformations. Modules of projection and pooling can then constitute layers\nof deep networks, for learning composite representations. We present the main\ntheoretical and computational aspects of a framework for unsupervised learning\nof invariant audio representations, empirically evaluated on music genre\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 21:15:32 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Chiyuan", ""], ["Evangelopoulos", "Georgios", ""], ["Voinea", "Stephen", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1404.0431", "submitter": "Christopher Aicher", "authors": "Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset", "title": "Learning Latent Block Structure in Weighted Networks", "comments": "28 Pages", "journal-ref": "Journal of Complex Networks (2015) 3 (2): 221-248", "doi": "10.1093/comnet/cnu026", "report-no": null, "categories": "stat.ML cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is an important task in network analysis, in which we aim\nto learn a network partition that groups together vertices with similar\ncommunity-level connectivity patterns. By finding such groups of vertices with\nsimilar structural roles, we extract a compact representation of the network's\nlarge-scale structure, which can facilitate its scientific interpretation and\nthe prediction of unknown or future interactions. Popular approaches, including\nthe stochastic block model, assume edges are unweighted, which limits their\nutility by throwing away potentially useful information. We introduce the\n`weighted stochastic block model' (WSBM), which generalizes the stochastic\nblock model to networks with edge weights drawn from any exponential family\ndistribution. This model learns from both the presence and weight of edges,\nallowing it to discover structure that would otherwise be hidden when weights\nare discarded or thresholded. We describe a Bayesian variational algorithm for\nefficiently approximating this model's posterior distribution over latent block\nstructures. We then evaluate the WSBM's performance on both edge-existence and\nedge-weight prediction tasks for a set of real-world weighted networks. In all\ncases, the WSBM performs as well or better than the best alternatives on these\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 02:09:42 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 19:20:27 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Aicher", "Christopher", ""], ["Jacobs", "Abigail Z.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1404.0541", "submitter": "Johannes Lederer", "authors": "Johannes Lederer and Christian M\\\"uller", "title": "Don't Fall for Tuning Parameters: Tuning-Free Variable Selection in High\n  Dimensions With the TREX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lasso is a seminal contribution to high-dimensional statistics, but it hinges\non a tuning parameter that is difficult to calibrate in practice. A partial\nremedy for this problem is Square-Root Lasso, because it inherently calibrates\nto the noise variance. However, Square-Root Lasso still requires the\ncalibration of a tuning parameter to all other aspects of the model. In this\nstudy, we introduce TREX, an alternative to Lasso with an inherent calibration\nto all aspects of the model. This adaptation to the entire model renders TREX\nan estimator that does not require any calibration of tuning parameters. We\nshow that TREX can outperform cross-validated Lasso in terms of variable\nselection and computational efficiency. We also introduce a bootstrapped\nversion of TREX that can further improve variable selection. We illustrate the\npromising performance of TREX both on synthetic data and on a recent\nhigh-dimensional biological data set that considers riboflavin production in B.\nsubtilis.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 13:11:36 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 23:32:15 GMT"}, {"version": "v3", "created": "Sun, 24 May 2015 09:04:21 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1404.0751", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Martin Azizyan, Aarti Singh", "title": "Subspace Learning from Extremely Compressed Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning the principal subspace of a large set of vectors from an\nextremely small number of compressive measurements of each vector. Our\ntheoretical results show that even a constant number of measurements per column\nsuffices to approximate the principal subspace to arbitrary precision, provided\nthat the number of vectors is large. This result is achieved by a simple\nalgorithm that computes the eigenvectors of an estimate of the covariance\nmatrix. The main insight is to exploit an averaging effect that arises from\napplying a different random projection to each vector. We provide a number of\nsimulations confirming our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 02:58:37 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 15:36:21 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Azizyan", "Martin", ""], ["Singh", "Aarti", ""]]}, {"id": "1404.0752", "submitter": "Jem Corcoran", "authors": "Jem Corcoran and Daniel Tran and Nicholas Levine", "title": "An Efficient Search Strategy for Aggregation and Discretization of\n  Attributes of Bayesian Networks Using Minimum Description Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are convenient graphical expressions for high dimensional\nprobability distributions representing complex relationships between a large\nnumber of random variables. They have been employed extensively in areas such\nas bioinformatics, artificial intelligence, diagnosis, and risk management. The\nrecovery of the structure of a network from data is of prime importance for the\npurposes of modeling, analysis, and prediction. Most recovery algorithms in the\nliterature assume either discrete of continuous but Gaussian data. For general\ncontinuous data, discretization is usually employed but often destroys the very\nstructure one is out to recover. Friedman and Goldszmidt suggest an approach\nbased on the minimum description length principle that chooses a discretization\nwhich preserves the information in the original data set, however it is one\nwhich is difficult, if not impossible, to implement for even moderately sized\nnetworks. In this paper we provide an extremely efficient search strategy which\nallows one to use the Friedman and Goldszmidt discretization in practice.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 03:15:26 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Corcoran", "Jem", ""], ["Tran", "Daniel", ""], ["Levine", "Nicholas", ""]]}, {"id": "1404.0979", "submitter": "Martin Kasparick", "authors": "Martin Kasparick, Renato L. G. Cavalcante, Stefan Valentin, Slawomir\n  Stanczak, Masahiro Yukawa", "title": "Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side\n  Information", "comments": "IEEE Transactions on Vehicular Technology; revised and extended\n  version with new simulation scenario", "journal-ref": null, "doi": "10.1109/TVT.2015.2453391", "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of reconstructing coverage maps from\npath-loss measurements in cellular networks. We propose and evaluate two\nkernel-based adaptive online algorithms as an alternative to typical offline\nmethods. The proposed algorithms are application-tailored extensions of\npowerful iterative methods such as the adaptive projected subgradient method\nand a state-of-the-art adaptive multikernel method. Assuming that the moving\ntrajectories of users are available, it is shown how side information can be\nincorporated in the algorithms to improve their convergence performance and the\nquality of the estimation. The complexity is significantly reduced by imposing\nsparsity-awareness in the sense that the algorithms exploit the compressibility\nof the measurement data to reduce the amount of data which is saved and\nprocessed. Finally, we present extensive simulations based on realistic data to\nshow that our algorithms provide fast, robust estimates of coverage maps in\nreal-world scenarios. Envisioned applications include path-loss prediction\nalong trajectories of mobile users as a building block for anticipatory\nbuffering or traffic offloading.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 15:46:54 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 16:13:59 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 07:50:01 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 17:56:35 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Kasparick", "Martin", ""], ["Cavalcante", "Renato L. G.", ""], ["Valentin", "Stefan", ""], ["Stanczak", "Slawomir", ""], ["Yukawa", "Masahiro", ""]]}, {"id": "1404.1100", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "A Tutorial on Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a mainstay of modern data analysis - a\nblack box that is widely used but (sometimes) poorly understood. The goal of\nthis paper is to dispel the magic behind this black box. This manuscript\nfocuses on building a solid intuition for how and why principal component\nanalysis works. This manuscript crystallizes this knowledge by deriving from\nsimple intuitions, the mathematics behind PCA. This tutorial does not shy away\nfrom explaining the ideas informally, nor does it shy away from the\nmathematics. The hope is that by addressing both aspects, readers of all levels\nwill be able to gain a better understanding of PCA as well as the when, the how\nand the why of applying this technique.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 21:16:49 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.1238", "submitter": "Chris Oates", "authors": "Chris J. Oates, Jim Q. Smith, Sach Mukherjee, James Cussens", "title": "Exact Estimation of Multiple Directed Acyclic Graphs", "comments": "Revised version - 12/11/14", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating the structure of multiple\nrelated directed acyclic graph (DAG) models. Building on recent developments in\nexact estimation of DAGs using integer linear programming (ILP), we present an\nILP approach for joint estimation over multiple DAGs, that does not require\nthat the vertices in each DAG share a common ordering. Furthermore, we allow\nalso for (potentially unknown) dependency structure between the DAGs. Results\nare presented on both simulated data and fMRI data obtained from multiple\nsubjects.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 12:50:48 GMT"}, {"version": "v2", "created": "Fri, 18 Apr 2014 15:28:25 GMT"}, {"version": "v3", "created": "Wed, 12 Nov 2014 09:51:00 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Oates", "Chris J.", ""], ["Smith", "Jim Q.", ""], ["Mukherjee", "Sach", ""], ["Cussens", "James", ""]]}, {"id": "1404.1333", "submitter": "Li Li", "authors": "Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang,\n  Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert M\\\"uller,\n  Kieron Burke", "title": "Understanding Machine-learned Density Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression is used to approximate the kinetic energy of\nnon-interacting fermions in a one-dimensional box as a functional of their\ndensity. The properties of different kernels and methods of cross-validation\nare explored, and highly accurate energies are achieved. Accurate {\\em\nconstrained optimal densities} are found via a modified Euler-Lagrange\nconstrained minimization of the total energy. A projected gradient descent\nalgorithm is derived using local principal component analysis. Additionally, a\nsparse grid representation of the density can be used without degrading the\nperformance of the methods. The implications for machine-learned density\nfunctional approximations are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 18:20:23 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 01:23:13 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Li", "Li", ""], ["Snyder", "John C.", ""], ["Pelaschier", "Isabelle M.", ""], ["Huang", "Jessica", ""], ["Niranjan", "Uma-Naresh", ""], ["Duncan", "Paul", ""], ["Rupp", "Matthias", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1404.1356", "submitter": "Olivier Wintenberger", "authors": "Olivier Wintenberger (LSTA)", "title": "Optimal learning with Bernstein Online Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new recursive aggregation procedure called Bernstein Online\nAggregation (BOA). The exponential weights include an accuracy term and a\nsecond order term that is a proxy of the quadratic variation as in Hazan and\nKale (2010). This second term stabilizes the procedure that is optimal in\ndifferent senses. We first obtain optimal regret bounds in the deterministic\ncontext. Then, an adaptive version is the first exponential weights algorithm\nthat exhibits a second order bound with excess losses that appears first in\nGaillard et al. (2014). The second order bounds in the deterministic context\nare extended to a general stochastic context using the cumulative predictive\nrisk. Such conversion provides the main result of the paper, an inequality of a\nnovel type comparing the procedure with any deterministic aggregation procedure\nfor an integrated criteria. Then we obtain an observable estimate of the excess\nof risk of the BOA procedure. To assert the optimality, we consider finally the\niid case for strongly convex and Lipschitz continuous losses and we prove that\nthe optimal rate of aggregation of Tsybakov (2003) is achieved. The batch\nversion of the BOA procedure is then the first adaptive explicit algorithm that\nsatisfies an optimal oracle inequality with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:33:55 GMT"}, {"version": "v2", "created": "Sun, 20 Apr 2014 19:26:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 17:58:04 GMT"}, {"version": "v4", "created": "Tue, 30 Aug 2016 06:44:14 GMT"}, {"version": "v5", "created": "Tue, 13 Sep 2016 14:23:48 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Wintenberger", "Olivier", "", "LSTA"]]}, {"id": "1404.1361", "submitter": "Alexander Jung", "authors": "Alexander Jung", "title": "Learning the Conditional Independence Structure of Stationary Time\n  Series: A Multitask Learning Approach", "comments": "to be submitted to IEEE Trans. Sig. Proc", "journal-ref": null, "doi": "10.1109/TSP.2015.2460219", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for inferring the conditional independence graph (CIG) of\na high-dimensional Gaussian vector time series (discrete-time process) from a\nfinite-length observation. By contrast to existing approaches, we do not rely\non a parametric process model (such as, e.g., an autoregressive model) for the\nobserved random process. Instead, we only require certain smoothness properties\n(in the Fourier domain) of the process. The proposed inference scheme works\neven for sample sizes much smaller than the number of scalar process components\nif the true underlying CIG is sufficiently sparse. A theoretical performance\nanalysis provides conditions which guarantee that the probability of the\nproposed inference method to deliver a wrong CIG is below a prescribed value.\nThese conditions imply lower bounds on the sample size such that the new method\nis consistent asymptotically. Some numerical experiments validate our\ntheoretical performance analysis and demonstrate superior performance of our\nscheme compared to an existing (parametric) approach in case of model mismatch.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:41:34 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 11:37:27 GMT"}, {"version": "v3", "created": "Sun, 11 Jan 2015 21:10:36 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Jung", "Alexander", ""]]}, {"id": "1404.1371", "submitter": "Hai Shu", "authors": "Hai Shu, Bin Nan, Robert Koeppe", "title": "Multiple Testing for Neuroimaging via Hidden Markov Random Field", "comments": "A MATLAB package implementing the proposed FDR procedure is available\n  with this paper at the Biometrics website on Wiley Online Library", "journal-ref": "Biometrics, 71(3), pp.741-750 (2015)", "doi": "10.1111/biom.12329", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional voxel-level multiple testing procedures in neuroimaging, mostly\n$p$-value based, often ignore the spatial correlations among neighboring voxels\nand thus suffer from substantial loss of power. We extend the\nlocal-significance-index based procedure originally developed for the hidden\nMarkov chain models, which aims to minimize the false nondiscovery rate subject\nto a constraint on the false discovery rate, to three-dimensional neuroimaging\ndata using a hidden Markov random field model. A generalized\nexpectation-maximization algorithm for maximizing the penalized likelihood is\nproposed for estimating the model parameters. Extensive simulations show that\nthe proposed approach is more powerful than conventional false discovery rate\nprocedures. We apply the method to the comparison between mild cognitive\nimpairment, a disease status with increased risk of developing Alzheimer's or\nanother dementia, and normal controls in the FDG-PET imaging study of the\nAlzheimer's Disease Neuroimaging Initiative.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 20:00:05 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 05:23:39 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Shu", "Hai", ""], ["Nan", "Bin", ""], ["Koeppe", "Robert", ""]]}, {"id": "1404.1377", "submitter": "Zheng Wang", "authors": "Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu and\n  Jieping Ye", "title": "Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and scalable low rank matrix\ncompletion algorithm. The key idea is to extend orthogonal matching pursuit\nmethod from the vector case to the matrix case. We further propose an economic\nversion of our algorithm by introducing a novel weight updating rule to reduce\nthe time and storage complexity. Both versions are computationally inexpensive\nfor each matrix pursuit iteration, and find satisfactory results in a few\niterations. Another advantage of our proposed algorithm is that it has only one\ntunable parameter, which is the rank. It is easy to understand and to use by\nthe user. This becomes especially important in large-scale learning problems.\nIn addition, we rigorously show that both versions achieve a linear convergence\nrate, which is significantly better than the previous known results. We also\nempirically compare the proposed algorithms with several state-of-the-art\nmatrix completion algorithms on many real-world datasets, including the\nlarge-scale recommendation dataset Netflix as well as the MovieLens datasets.\nNumerical results show that our proposed algorithm is more efficient than\ncompeting algorithms while achieving similar or better prediction performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 20:00:30 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 19:09:09 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Wang", "Zheng", ""], ["Lai", "Ming-Jun", ""], ["Lu", "Zhaosong", ""], ["Fan", "Wei", ""], ["Davulcu", "Hasan", ""], ["Ye", "Jieping", ""]]}, {"id": "1404.1425", "submitter": "Kun  Yang", "authors": "Dangna Li, Kun Yang and Wing Hung Wong", "title": "Density Estimation via Discrepancy Based Adaptive Sequential Partition", "comments": "Binary Partition, Star Discrepancy, Density Estimation, Mode Seeking,\n  Level Set Tree", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $iid$ observations from an unknown absolute continuous distribution\ndefined on some domain $\\Omega$, we propose a nonparametric method to learn a\npiecewise constant function to approximate the underlying probability density\nfunction. Our density estimate is a piecewise constant function defined on a\nbinary partition of $\\Omega$. The key ingredient of the algorithm is to use\ndiscrepancy, a concept originates from Quasi Monte Carlo analysis, to control\nthe partition process. The resulting algorithm is simple, efficient, and has a\nprovable convergence rate. We empirically demonstrate its efficiency as a\ndensity estimation method. We present its applications on a wide range of\ntasks, including finding good initializations for k-means.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 03:43:28 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 01:40:56 GMT"}, {"version": "v3", "created": "Wed, 23 Apr 2014 05:20:54 GMT"}, {"version": "v4", "created": "Sun, 11 Mar 2018 05:16:22 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "Dangna", ""], ["Yang", "Kun", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1404.1492", "submitter": "James Brofos", "authors": "James Brofos", "title": "Ensemble Committees for Stock Return Classification and Prediction", "comments": "15 pages, 4 figures, Neukom Institute Computational Undergraduate\n  Research prize - second place", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a portfolio trading strategy formulated by algorithms in\nthe field of machine learning. The profitability of the strategy is measured by\nthe algorithm's capability to consistently and accurately identify stock\nindices with positive or negative returns, and to generate a preferred\nportfolio allocation on the basis of a learned model. Stocks are characterized\nby time series data sets consisting of technical variables that reflect market\nconditions in a previous time interval, which are utilized produce binary\nclassification decisions in subsequent intervals. The learned model is\nconstructed as a committee of random forest classifiers, a non-linear support\nvector machine classifier, a relevance vector machine classifier, and a\nconstituent ensemble of k-nearest neighbors classifiers. The Global Industry\nClassification Standard (GICS) is used to explore the ensemble model's efficacy\nwithin the context of various fields of investment including Energy, Materials,\nFinancials, and Information Technology. Data from 2006 to 2012, inclusive, are\nconsidered, which are chosen for providing a range of market circumstances for\nevaluating the model. The model is observed to achieve an accuracy of\napproximately 70% when predicting stock price returns three months in advance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 17:09:05 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Brofos", "James", ""]]}, {"id": "1404.1504", "submitter": "Steve Hanneke", "authors": "Yair Wiener, Steve Hanneke, Ran El-Yaniv", "title": "A Compression Technique for Analyzing Disagreement-Based Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new and improved characterization of the label complexity of\ndisagreement-based active learning, in which the leading quantity is the\nversion space compression set size. This quantity is defined as the size of the\nsmallest subset of the training data that induces the same version space. We\nshow various applications of the new characterization, including a tight\nanalysis of CAL and refined label complexity bounds for linear separators under\nmixtures of Gaussians and axis-aligned rectangles under product densities. The\nversion space compression set size, as well as the new characterization of the\nlabel complexity, can be naturally extended to agnostic learning problems, for\nwhich we show new speedup results for two well known active learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 18:58:12 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Wiener", "Yair", ""], ["Hanneke", "Steve", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1404.1530", "submitter": "Christos Boutsidis", "authors": "Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis", "title": "Provable Deterministic Leverage Score Sampling", "comments": "20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain theoretically a curious empirical phenomenon: \"Approximating a\nmatrix by deterministically selecting a subset of its columns with the\ncorresponding largest leverage scores results in a good low-rank matrix\nsurrogate\". To obtain provable guarantees, previous work requires randomized\nsampling of the columns with probabilities proportional to their leverage\nscores.\n  In this work, we provide a novel theoretical analysis of deterministic\nleverage score sampling. We show that such deterministic sampling can be\nprovably as accurate as its randomized counterparts, if the leverage scores\nfollow a moderately steep power-law decay. We support this power-law assumption\nby providing empirical evidence that such decay laws are abundant in real-world\ndata sets. We then demonstrate empirically the performance of deterministic\nleverage score sampling, which many times matches or outperforms the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 00:08:54 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 10:19:07 GMT"}, {"version": "v3", "created": "Tue, 3 Jun 2014 01:23:16 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1404.1935", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik, Ami Wiesel", "title": "Tyler's Covariance Matrix Estimator in Elliptical Models with Convex\n  Structure", "comments": "arXiv admin note: text overlap with arXiv:1311.0594", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address structured covariance estimation in elliptical distributions by\nassuming that the covariance is a priori known to belong to a given convex set,\ne.g., the set of Toeplitz or banded matrices. We consider the General Method of\nMoments (GMM) optimization applied to robust Tyler's scatter M-estimator\nsubject to these convex constraints. Unfortunately, GMM turns out to be\nnon-convex due to the objective. Instead, we propose a new COCA estimator - a\nconvex relaxation which can be efficiently solved. We prove that the relaxation\nis tight in the unconstrained case for a finite number of samples, and in the\nconstrained case asymptotically. We then illustrate the advantages of COCA in\nsynthetic simulations with structured compound Gaussian distributions. In these\nexamples, COCA outperforms competing methods such as Tyler's estimator and its\nprojection onto the structure set.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 20:09:39 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 07:12:58 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1404.2007", "submitter": "Jeremy Sabourin", "authors": "Jeremy Sabourin, William Valdar, and Andrew Nobel", "title": "A Permutation Approach for Selecting the Penalty Parameter in Penalized\n  Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple, efficient, permutation based procedure for selecting\nthe penalty parameter in the LASSO. The procedure, which is intended for\napplications where variable selection is the primary focus, can be applied in a\nvariety of structural settings, including generalized linear models. We briefly\ndiscuss connections between permutation selection and existing theory for the\nLASSO. In addition, we present a simulation study and an analysis of three real\ndata sets in which permutation selection is compared with cross-validation\n(CV), the Bayesian information criterion (BIC), and a selection method based on\nrecently developed testing procedures for the LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 04:44:59 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Sabourin", "Jeremy", ""], ["Valdar", "William", ""], ["Nobel", "Andrew", ""]]}, {"id": "1404.2083", "submitter": "Vladimir Vovk", "authors": "Evgeny Burnaev and Vladimir Vovk", "title": "Efficiency of conformalized ridge regression", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a method of producing prediction sets that can be\napplied on top of a wide range of prediction algorithms. The method has a\nguaranteed coverage probability under the standard IID assumption regardless of\nwhether the assumptions (often considerably more restrictive) of the underlying\nalgorithm are satisfied. However, for the method to be really useful it is\ndesirable that in the case where the assumptions of the underlying algorithm\nare satisfied, the conformal predictor loses little in efficiency as compared\nwith the underlying algorithm (whereas being a conformal predictor, it has the\nstronger guarantee of validity). In this paper we explore the degree to which\nthis additional requirement of efficiency is satisfied in the case of Bayesian\nridge regression; we find that asymptotically conformal prediction sets differ\nlittle from ridge regression prediction intervals when the standard Bayesian\nassumptions are satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 10:49:08 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1404.2124", "submitter": "Julian Wolfson", "authors": "Julian Wolfson, Sunayan Bandyopadhyay, Mohamed Elidrisi, Gabriela\n  Vazquez-Benitez, Donald Musgrove, Gediminas Adomavicius, Paul Johnson,\n  Patrick O'Connor", "title": "A Naive Bayes machine learning approach to risk prediction using\n  censored, time-to-event data", "comments": "21 pages (including references and appendix), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting an individual's risk of experiencing a future clinical outcome is\na statistical task with important consequences for both practicing clinicians\nand public health experts. Modern observational databases such as electronic\nhealth records (EHRs) provide an alternative to the longitudinal cohort studies\ntraditionally used to construct risk models, bringing with them both\nopportunities and challenges. Large sample sizes and detailed covariate\nhistories enable the use of sophisticated machine learning techniques to\nuncover complex associations and interactions, but observational databases are\noften ``messy,'' with high levels of missing data and incomplete patient\nfollow-up. In this paper, we propose an adaptation of the well-known Naive\nBayes (NB) machine learning approach for classification to time-to-event\noutcomes subject to censoring. We compare the predictive performance of our\nmethod to the Cox proportional hazards model which is commonly used for risk\nprediction in healthcare populations, and illustrate its application to\nprediction of cardiovascular risk using an EHR dataset from a large Midwest\nintegrated healthcare system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 13:34:29 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Wolfson", "Julian", ""], ["Bandyopadhyay", "Sunayan", ""], ["Elidrisi", "Mohamed", ""], ["Vazquez-Benitez", "Gabriela", ""], ["Musgrove", "Donald", ""], ["Adomavicius", "Gediminas", ""], ["Johnson", "Paul", ""], ["O'Connor", "Patrick", ""]]}, {"id": "1404.2189", "submitter": "Julian Wolfson", "authors": "Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela\n  Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson,\n  and Patrick J. O'Connor", "title": "Data mining for censored time-to-event data: A Bayesian network model\n  for predicting cardiovascular risk from electronic health record data", "comments": "31 pages (including references), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for predicting the risk of cardiovascular events based on individual\npatient characteristics are important tools for managing patient care. Most\ncurrent and commonly used risk prediction models have been built from carefully\nselected epidemiological cohorts. However, the homogeneity and limited size of\nsuch cohorts restricts the predictive power and generalizability of these risk\nmodels to other populations. Electronic health data (EHD) from large health\ncare systems provide access to data on large, heterogeneous, and\ncontemporaneous patient populations. The unique features and challenges of EHD,\nincluding missing risk factor information, non-linear relationships between\nrisk factors and cardiovascular event outcomes, and differing effects from\ndifferent patient subgroups, demand novel machine learning approaches to risk\nmodel development. In this paper, we present a machine learning approach based\non Bayesian networks trained on EHD to predict the probability of having a\ncardiovascular event within five years. In such data, event status may be\nunknown for some individuals as the event time is right-censored due to\ndisenrollment and incomplete follow-up. Since many traditional data mining\nmethods are not well-suited for such data, we describe how to modify both\nmodelling and assessment techniques to account for censored observation times.\nWe show that our approach can lead to better predictive performance than the\nCox proportional hazards model (i.e., a regression-based approach commonly used\nfor censored, time-to-event data) or a Bayesian network with {\\em{ad hoc}}\napproaches to right-censoring. Our techniques are motivated by and illustrated\non data from a large U.S. Midwestern health care system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 15:51:10 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Bandyopadhyay", "Sunayan", ""], ["Wolfson", "Julian", ""], ["Vock", "David M.", ""], ["Vazquez-Benitez", "Gabriela", ""], ["Adomavicius", "Gediminas", ""], ["Elidrisi", "Mohamed", ""], ["Johnson", "Paul E.", ""], ["O'Connor", "Patrick J.", ""]]}, {"id": "1404.2353", "submitter": "Denis Sidorov", "authors": "Victor Kurbatsky, Nikita Tomin, Vadim Spiryaev, Paul Leahy, Denis\n  Sidorov and Alexei Zhukov", "title": "Power System Parameters Forecasting Using Hilbert-Huang Transform and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel hybrid data-driven approach is developed for forecasting power system\nparameters with the goal of increasing the efficiency of short-term forecasting\nstudies for non-stationary time-series. The proposed approach is based on mode\ndecomposition and a feature analysis of initial retrospective data using the\nHilbert-Huang transform and machine learning algorithms. The random forests and\ngradient boosting trees learning techniques were examined. The decision tree\ntechniques were used to rank the importance of variables employed in the\nforecasting models. The Mean Decrease Gini index is employed as an impurity\nfunction. The resulting hybrid forecasting models employ the radial basis\nfunction neural network and support vector regression. Apart from introduction\nand references the paper is organized as follows. The section 2 presents the\nbackground and the review of several approaches for short-term forecasting of\npower system parameters. In the third section a hybrid machine learning-based\nalgorithm using Hilbert-Huang transform is developed for short-term forecasting\nof power system parameters. Fourth section describes the decision tree learning\nalgorithms used for the issue of variables importance. Finally in section six\nthe experimental results in the following electric power problems are\npresented: active power flow forecasting, electricity price forecasting and for\nthe wind speed and direction forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 02:11:17 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Kurbatsky", "Victor", ""], ["Tomin", "Nikita", ""], ["Spiryaev", "Vadim", ""], ["Leahy", "Paul", ""], ["Sidorov", "Denis", ""], ["Zhukov", "Alexei", ""]]}, {"id": "1404.2553", "submitter": "Marie-Liesse Cauwet", "authors": "Marie-Liesse Cauwet (INRIA Saclay - Ile de France)", "title": "Noisy Optimization: Convergence with a Fixed Number of Resamplings", "comments": "EvoStar (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that evolution strategies in continuous domains might not\nconverge in the presence of noise. It is also known that, under mild\nassumptions, and using an increasing number of resamplings, one can mitigate\nthe effect of additive noise and recover convergence. We show new sufficient\nconditions for the convergence of an evolutionary algorithm with constant\nnumber of resamplings; in particular, we get fast rates (log-linear\nconvergence) provided that the variance decreases around the optimum slightly\nfaster than in the so-called multiplicative noise model. Keywords: Noisy\noptimization, evolutionary algorithm, theory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 17:21:38 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Cauwet", "Marie-Liesse", "", "INRIA Saclay - Ile de France"]]}, {"id": "1404.2644", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Yingyu Liang, Alireza Bagheri Garakani,\n  Maria-Florina Balcan, Fei Sha", "title": "A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning", "comments": "Extended version of the SIAM Data Mining 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 22:16:39 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 04:08:51 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 15:14:19 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Liang", "Yingyu", ""], ["Garakani", "Alireza Bagheri", ""], ["Balcan", "Maria-Florina", ""], ["Sha", "Fei", ""]]}, {"id": "1404.2655", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Yuehaw Khoo and Amit Singer", "title": "Open problem: Tightness of maximum likelihood semidefinite relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have observed an interesting, yet unexplained, phenomenon: Semidefinite\nprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tend\nto be tight in recovery problems with noisy data, even when MLE cannot exactly\nrecover the ground truth. Several results establish tightness of SDP based\nrelaxations in the regime where exact recovery from MLE is possible. However,\nto the best of our knowledge, their tightness is not understood beyond this\nregime. As an illustrative example, we focus on the generalized Procrustes\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 00:19:17 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Khoo", "Yuehaw", ""], ["Singer", "Amit", ""]]}, {"id": "1404.2986", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "A Tutorial on Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) has become a standard data analysis\ntechnique applied to an array of problems in signal processing and machine\nlearning. This tutorial provides an introduction to ICA based on linear algebra\nformulating an intuition for ICA from first principles. The goal of this\ntutorial is to provide a solid foundation on this advanced topic so that one\nmight learn the motivation behind ICA, learn why and when to apply this\ntechnique and in the process gain an introduction to this exciting field of\nactive research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 02:37:11 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.3012", "submitter": "Shun Kataoka", "authors": "Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi and\n  Chiou-Ting Hsu", "title": "Bayesian image segmentations by Potts prior and loopy belief propagation", "comments": "24 pages, 9 figures", "journal-ref": "Journal of the Physical Society of Japan 83 (2014) 124002", "doi": "10.7566/JPSJ.83.124002", "report-no": null, "categories": "cs.CV cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian image segmentation model based on Potts prior\nand loopy belief propagation. The proposed Bayesian model involves several\nterms, including the pairwise interactions of Potts models, and the average\nvectors and covariant matrices of Gauss distributions in color image modeling.\nThese terms are often referred to as hyperparameters in statistical machine\nlearning theory. In order to determine these hyperparameters, we propose a new\nscheme for hyperparameter estimation based on conditional maximization of\nentropy in the Potts prior. The algorithm is given based on loopy belief\npropagation. In addition, we compare our conditional maximum entropy framework\nwith the conventional maximum likelihood framework, and also clarify how the\nfirst order phase transitions in LBP's for Potts models influence our\nhyperparameter estimation procedures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 06:31:03 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 10:26:46 GMT"}, {"version": "v3", "created": "Wed, 4 Jun 2014 11:18:44 GMT"}, {"version": "v4", "created": "Fri, 15 Aug 2014 00:58:40 GMT"}, {"version": "v5", "created": "Mon, 18 Aug 2014 04:45:26 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Tanaka", "Kazuyuki", ""], ["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Waizumi", "Yuji", ""], ["Hsu", "Chiou-Ting", ""]]}, {"id": "1404.3174", "submitter": "Yang Tang", "authors": "Yang Tang, Ryan P. Browne and Paul D. McNicholas", "title": "Model Based Clustering of High-Dimensional Binary Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2014.12.009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixture of latent trait models with common slope parameters\n(MCLT) for model-based clustering of high-dimensional binary data, a data type\nfor which few established methods exist. Recent work on clustering of binary\ndata, based on a $d$-dimensional Gaussian latent variable, is extended by\nincorporating common factor analyzers. Accordingly, our approach facilitates a\nlow-dimensional visual representation of the clusters. We extend the model\nfurther by the incorporation of random block effects. The dependencies in each\nblock are taken into account through block-specific parameters that are\nconsidered to be random variables. A variational approximation to the\nlikelihood is exploited to derive a fast algorithm for determining the model\nparameters. Our approach is demonstrated on real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:15:30 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 02:56:17 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Tang", "Yang", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1404.3219", "submitter": "Carsten Peterson", "authors": "Hong Pi and Carsten Peterson", "title": "Estimating nonlinear regression errors without doing regression", "comments": null, "journal-ref": null, "doi": null, "report-no": "LU TP 94-19", "categories": "stat.ML nlin.CD q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for estimating nonlinear regression errors and their distributions\nwithout performing regression is presented. Assuming continuity of the modeling\nfunction the variance is given in terms of conditional probabilities extracted\nfrom the data. For N data points the computational demand is N2. Comparing the\npredicted residual errors with those derived from a linear model assumption\nprovides a signal for nonlinearity. The method is successfully illustrated with\ndata generated by the Ikeda and Lorenz maps augmented with noise. As a\nby-product the embedding dimensions of these maps are also extracted.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 20:22:40 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Pi", "Hong", ""], ["Peterson", "Carsten", ""]]}, {"id": "1404.3331", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Oscar Hernan Madrid Padilla, James G. Scott", "title": "Priors for Random Count Matrices Derived from a Family of Negative\n  Binomial Processes", "comments": "To appear in Journal of the American Statistical Association (Theory\n  and Methods). 31 pages + 11 page supplement, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a family of probability distributions for random count matrices\nwith a potentially unbounded number of rows and columns. The three\ndistributions we consider are derived from the gamma-Poisson, gamma-negative\nbinomial, and beta-negative binomial processes. Because the models lead to\nclosed-form Gibbs sampling update equations, they are natural candidates for\nnonparametric Bayesian priors over count matrices. A key aspect of our analysis\nis the recognition that, although the random count matrices within the family\nare defined by a row-wise construction, their columns can be shown to be i.i.d.\nThis fact is used to derive explicit formulas for drawing all the columns at\nonce. Moreover, by analyzing these matrices' combinatorial structure, we\ndescribe how to sequentially construct a column-i.i.d. random count matrix one\nrow at a time, and derive the predictive distribution of a new row count vector\nwith previously unseen features. We describe the similarities and differences\nbetween the three priors, and argue that the greater flexibility of the gamma-\nand beta- negative binomial processes, especially their ability to model\nover-dispersed, heavy-tailed count data, makes these well suited to a wide\nvariety of real-world applications. As an example of our framework, we\nconstruct a naive-Bayes text classifier to categorize a count vector to one of\nseveral existing random count matrices of different categories. The classifier\nsupports an unbounded number of features, and unlike most existing methods, it\ndoes not require a predefined finite vocabulary to be shared by all the\ncategories, and needs neither feature selection nor parameter tuning. Both the\ngamma- and beta- negative binomial processes are shown to significantly\noutperform the gamma-Poisson process for document categorization, with\ncomparable performance to other state-of-the-art supervised text classification\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 23:59:09 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 16:58:58 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 15:54:40 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1404.3415", "submitter": "Evgeniy Abramov G.", "authors": "E. G. Abramov, A. B. Komissarov, D. A. Kornyakov", "title": "Generalized version of the support vector machine for binary\n  classification problems: supporting hyperplane machine", "comments": "22 pages with 3 figures, 1 Octave script", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper there is proposed a generalized version of the SVM for binary\nclassification problems in the case of using an arbitrary transformation x ->\ny. An approach similar to the classic SVM method is used. The problem is widely\nexplained. Various formulations of primal and dual problems are proposed. For\none of the most important cases the formulae are derived in detail. A simple\ncomputational example is demonstrated. The algorithm and its implementation is\npresented in Octave language.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 18:57:30 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 17:37:11 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Abramov", "E. G.", ""], ["Komissarov", "A. B.", ""], ["Kornyakov", "D. A.", ""]]}, {"id": "1404.3418", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats, Robert D. Nowak, Richard G. Baraniuk", "title": "Active Learning for Undirected Graphical Model Selection", "comments": "AISTATS 2014", "journal-ref": "Proceedings of the 17th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&CP\n  volume 33", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies graphical model selection, i.e., the problem of estimating\na graph of statistical relationships among a collection of random variables.\nConventional graphical model selection algorithms are passive, i.e., they\nrequire all the measurements to have been collected before processing begins.\nWe propose an active learning algorithm that uses junction tree representations\nto adapt future measurements based on the information gathered from prior\nmeasurements. We prove that, under certain conditions, our active learning\nalgorithm requires fewer scalar measurements than any passive algorithm to\nreliably estimate a graph. A range of numerical results validate our theory and\ndemonstrates the benefits of active learning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 19:09:43 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Vats", "Divyanshu", ""], ["Nowak", "Robert D.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1404.3439", "submitter": "Omur  Arslan", "authors": "Omur Arslan and Daniel E. Koditschek", "title": "Anytime Hierarchical Clustering", "comments": "13 pages, 6 figures, 5 tables, in preparation for submission to a\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new anytime hierarchical clustering method that iteratively\ntransforms an arbitrary initial hierarchy on the configuration of measurements\nalong a sequence of trees we prove for a fixed data set must terminate in a\nchain of nested partitions that satisfies a natural homogeneity requirement.\nEach recursive step re-edits the tree so as to improve a local measure of\ncluster homogeneity that is compatible with a number of commonly used (e.g.,\nsingle, average, complete) linkage functions. As an alternative to the standard\nbatch algorithms, we present numerical evidence to suggest that appropriate\nadaptations of this method can yield decentralized, scalable algorithms\nsuitable for distributed/parallel computation of clustering hierarchies and\nonline tracking of clustering trees applicable to large, dynamically changing\ndatabases and anomaly detection.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 23:07:20 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Arslan", "Omur", ""], ["Koditschek", "Daniel E.", ""]]}, {"id": "1404.3581", "submitter": "Arnaud Joly", "authors": "Arnaud Joly, Pierre Geurts, Louis Wehenkel", "title": "Random forests with random projections of the output space for high\n  dimensional multi-label classification", "comments": null, "journal-ref": "Machine Learning and Knowledge Discovery in Databases, 2014, Part\n  I, pp 607-622", "doi": "10.1007/978-3-662-44848-9_39", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the idea of random projections applied to the output space, so as to\nenhance tree-based ensemble methods in the context of multi-label\nclassification. We show how learning time complexity can be reduced without\naffecting computational complexity and accuracy of predictions. We also show\nthat random output space projections may be used in order to reach different\nbias-variance tradeoffs, over a broad panel of benchmark problems, and that\nthis may lead to improved accuracy while reducing significantly the\ncomputational burden of the learning stage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 13:52:29 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 10:27:00 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 15:29:39 GMT"}, {"version": "v4", "created": "Mon, 29 Sep 2014 16:01:50 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Joly", "Arnaud", ""], ["Geurts", "Pierre", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1404.3591", "submitter": "Andreas Argyriou", "authors": "Andreas Argyriou and Marco Signoretto and Johan Suykens", "title": "Hybrid Conditional Gradient - Smoothing Algorithms with Applications to\n  Sparse and Low Rank Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a hybrid conditional gradient - smoothing algorithm (HCGS) for\nsolving composite convex optimization problems which contain several terms over\na bounded set. Examples of these include regularization problems with several\nnorms as penalties and a norm constraint. HCGS extends conditional gradient\nmethods to cases with multiple nonsmooth terms, in which standard conditional\ngradient methods may be difficult to apply. The HCGS algorithm borrows\ntechniques from smoothing proximal methods and requires first-order\ncomputations (subgradients and proximity operations). Unlike proximal methods,\nHCGS benefits from the advantages of conditional gradient methods, which render\nit more efficient on certain large scale optimization problems. We demonstrate\nthese advantages with simulations on two matrix optimization problems:\nregularization of matrices with combined $\\ell_1$ and trace norm penalties; and\na convex relaxation of sparse PCA.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 14:09:43 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 19:29:31 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Argyriou", "Andreas", ""], ["Signoretto", "Marco", ""], ["Suykens", "Johan", ""]]}, {"id": "1404.3840", "submitter": "Chaochao Lu", "authors": "Chaochao Lu, Xiaoou Tang", "title": "Surpassing Human-Level Face Verification Performance on LFW with\n  GaussianFace", "comments": "Appearing in Proceedings of the 29th AAAI Conference on Artificial\n  Intelligence (AAAI-15), Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification remains a challenging problem in very complex conditions\nwith large variations such as pose, illumination, expression, and occlusions.\nThis problem is exacerbated when we rely unrealistically on a single training\ndata source, which is often insufficient to cover the intrinsically complex\nface variations. This paper proposes a principled multi-task learning approach\nbased on Discriminative Gaussian Process Latent Variable Model, named\nGaussianFace, to enrich the diversity of training data. In comparison to\nexisting methods, our model exploits additional data from multiple\nsource-domains to improve the generalization performance of face verification\nin an unknown target-domain. Importantly, our model can adapt automatically to\ncomplex data distributions, and therefore can well capture complex face\nvariations inherent in multiple sources. Extensive experiments demonstrate the\neffectiveness of the proposed model in learning from diverse data sources and\ngeneralize to unseen domain. Specifically, the accuracy of our algorithm\nachieves an impressive accuracy rate of 98.52% on the well-known and\nchallenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the\nhuman-level performance in face verification (97.53%) on LFW is surpassed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 07:51:23 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 14:37:38 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 03:37:36 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Lu", "Chaochao", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1404.3862", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yonatan Glassner, Shie Mannor", "title": "Optimizing the CVaR via Sampling", "comments": "To appear in AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Value at Risk (CVaR) is a prominent risk measure that is being\nused extensively in various domains. We develop a new formula for the gradient\nof the CVaR in the form of a conditional expectation. Based on this formula, we\npropose a novel sampling-based estimator for the CVaR gradient, in the spirit\nof the likelihood-ratio method. We analyze the bias of the estimator, and prove\nthe convergence of a corresponding stochastic gradient descent algorithm to a\nlocal CVaR optimum. Our method allows to consider CVaR optimization in new\ndomains. As an example, we consider a reinforcement learning application, and\nlearn a risk-sensitive controller for the game of Tetris.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 10:32:05 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 15:35:36 GMT"}, {"version": "v3", "created": "Tue, 16 Sep 2014 15:32:48 GMT"}, {"version": "v4", "created": "Sat, 22 Nov 2014 14:44:54 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Tamar", "Aviv", ""], ["Glassner", "Yonatan", ""], ["Mannor", "Shie", ""]]}, {"id": "1404.3989", "submitter": "Andrew Beam", "authors": "Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle", "title": "Bayesian Neural Networks for Genetic Association Studies of Complex\n  Disease", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-014-0368-0", "report-no": null, "categories": "q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal genetic variants from large genetic association studies\nposes many difficult challenges. Assessing which genetic markers are involved\nin determining trait status is a computationally demanding task, especially in\nthe presence of gene-gene interactions. A non-parametric Bayesian approach in\nthe form of a Bayesian neural network is proposed for use in analyzing genetic\nassociation studies. Demonstrations on synthetic and real data reveal they are\nable to efficiently and accurately determine which variants are involved in\ndetermining case-control status. Using graphics processing units (GPUs) the\ntime needed to build these models is decreased by several orders of magnitude.\nIn comparison with commonly used approaches for detecting genetic interactions,\nBayesian neural networks perform very well across a broad spectrum of possible\ngenetic relationships while having the computational efficiency needed to\nhandle large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 17:11:53 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 00:44:21 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Beam", "Andrew L.", ""], ["Motsinger-Reif", "Alison", ""], ["Doyle", "Jon", ""]]}, {"id": "1404.4095", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Multi-borders classification", "comments": "Corrected error in equations: second and third equations were not\n  linearly independent. Corrected figure to match. \"Hierarchical\" scheme is a\n  decision tree", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of possible methods of generalizing binary classification to\nmulti-class classification increases exponentially with the number of class\nlabels. Often, the best method of doing so will be highly problem dependent.\nHere we present classification software in which the partitioning of\nmulti-class classification problems into binary classification problems is\nspecified using a recursive control language.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:06:35 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 05:19:49 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 03:43:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1404.4105", "submitter": "Aur\\'elien Bellet", "authors": "Yuan Shi and Aur\\'elien Bellet and Fei Sha", "title": "Sparse Compositional Metric Learning", "comments": "18 pages. To be published in Proceedings of the 27th AAAI Conference\n  on Artificial Intelligence (AAAI 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for metric learning by framing it as learning a\nsparse combination of locally discriminative metrics that are inexpensive to\ngenerate from the training data. This flexible framework allows us to naturally\nderive formulations for global, multi-task and local metric learning. The\nresulting algorithms have several advantages over existing methods in the\nliterature: a much smaller number of parameters to be estimated and a\nprincipled way to generalize learned metrics to new testing data points. To\nanalyze the approach theoretically, we derive a generalization bound that\njustifies the sparse combination. Empirically, we evaluate our algorithms on\nseveral datasets against state-of-the-art metric learning methods. The results\nare consistent with our theoretical findings and demonstrate the superiority of\nour approach in terms of classification performance and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:55:53 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Shi", "Yuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1404.4175", "submitter": "Emanuele Olivetti", "authors": "Emanuele Olivetti, Seyed Mostafa Kia, Paolo Avesani", "title": "MEG Decoding Across Subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding is a data analysis paradigm for neuroimaging experiments that\nis based on predicting the stimulus presented to the subject from the\nconcurrent brain activity. In order to make inference at the group level, a\nstraightforward but sometimes unsuccessful approach is to train a classifier on\nthe trials of a group of subjects and then to test it on unseen trials from new\nsubjects. The extreme difficulty is related to the structural and functional\nvariability across the subjects. We call this approach \"decoding across\nsubjects\". In this work, we address the problem of decoding across subjects for\nmagnetoencephalographic (MEG) experiments and we provide the following\ncontributions: first, we formally describe the problem and show that it belongs\nto a machine learning sub-field called transductive transfer learning (TTL).\nSecond, we propose to use a simple TTL technique that accounts for the\ndifferences between train data and test data. Third, we propose the use of\nensemble learning, and specifically of stacked generalization, to address the\nvariability across subjects within train data, with the aim of producing more\nstable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, we\ncompare the standard approach of not modelling the differences across subjects,\nto the proposed one of combining TTL and ensemble learning. We show that the\nproposed approach is consistently more accurate than the standard one.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 09:21:26 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Kia", "Seyed Mostafa", ""], ["Avesani", "Paolo", ""]]}, {"id": "1404.4178", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Robert Kohn, Mattias Villani, Minh-Ngoc Tran", "title": "Speeding Up MCMC by Efficient Data Subsampling", "comments": "Main changes: The theory has been significantly revised", "journal-ref": null, "doi": "10.1080/01621459.2018.1448827", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework\nwhere the likelihood function for $n$ observations is estimated from a random\nsubset of $m$ observations. We introduce a highly efficient unbiased estimator\nof the log-likelihood based on control variates, such that the computing cost\nis much smaller than that of the full log-likelihood in standard MCMC. The\nlikelihood estimate is bias-corrected and used in two dependent pseudo-marginal\nalgorithms to sample from a perturbed posterior, for which we derive the\nasymptotic error with respect to $n$ and $m$, respectively. We propose a\npractical estimator of the error and show that the error is negligible even for\na very small $m$ in our applications. We demonstrate that Subsampling MCMC is\nsubstantially more efficient than standard MCMC in terms of sampling efficiency\nfor a given computational budget, and that it outperforms other subsampling\nmethods for MCMC proposed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 09:33:36 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 19:45:08 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 07:05:04 GMT"}, {"version": "v4", "created": "Mon, 12 Dec 2016 15:39:30 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 00:29:59 GMT"}, {"version": "v6", "created": "Mon, 1 Jan 2018 05:19:34 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Villani", "Mattias", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1404.4351", "submitter": "Navodit Misra", "authors": "Navodit Misra and Ercan E. Kuruoglu", "title": "Stable Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable random variables are motivated by the central limit theorem for\ndensities with (potentially) unbounded variance and can be thought of as\nnatural generalizations of the Gaussian distribution to skewed and heavy-tailed\nphenomenon. In this paper, we introduce stable graphical (SG) models, a class\nof multivariate stable densities that can also be represented as Bayesian\nnetworks whose edges encode linear dependencies between random variables. One\nmajor hurdle to the extensive use of stable distributions is the lack of a\nclosed-form analytical expression for their densities. This makes penalized\nmaximum-likelihood based learning computationally demanding. We establish\ntheoretically that the Bayesian information criterion (BIC) can asymptotically\nbe reduced to the computationally more tractable minimum dispersion criterion\n(MDC) and develop StabLe, a structure learning algorithm based on MDC. We use\nsimulated datasets for five benchmark network topologies to empirically\ndemonstrate how StabLe improves upon ordinary least squares (OLS) regression.\nWe also apply StabLe to microarray gene expression data for lymphoblastoid\ncells from 727 individuals belonging to eight global population groups. We\nestablish that StabLe improves test set performance relative to OLS via\nten-fold cross-validation. Finally, we develop SGEX, a method for quantifying\ndifferential expression of genes between different population groups.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 19:12:47 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Misra", "Navodit", ""], ["Kuruoglu", "Ercan E.", ""]]}, {"id": "1404.4408", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang and Alexander Rakhlin", "title": "Geometric Inference for General High-Dimensional Linear Inverse Problems", "comments": "39 pages, 6 figures", "journal-ref": "The Annals of Statistics 44 (2016) 1536-1563", "doi": "10.1214/15-AOS1426", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified geometric framework for the statistical\nanalysis of a general ill-posed linear inverse model which includes as special\ncases noisy compressed sensing, sign vector recovery, trace regression,\northogonal matrix estimation, and noisy matrix completion. We propose\ncomputationally feasible convex programs for statistical inference including\nestimation, confidence intervals and hypothesis testing. A theoretical\nframework is developed to characterize the local estimation rate of convergence\nand to provide statistical inference guarantees. Our results are built based on\nthe local conic geometry and duality. The difficulty of statistical inference\nis captured by the geometric characterization of the local tangent cone through\nthe Gaussian width and Sudakov minoration estimate.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 01:10:47 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 17:27:23 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 22:51:29 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1404.4412", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou and Andrzej Cichocki and Qibin Zhao and Shengli Xie", "title": "Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness", "comments": "appears in IEEE Transactions on Image Processing, 2015", "journal-ref": null, "doi": "10.1109/TIP.2015.2478396", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction\nof nonnegative parts-based and physically meaningful latent components from\nhigh-dimensional tensor data while preserving the natural multilinear structure\nof data. However, as the data tensor often has multiple modes and is\nlarge-scale, existing NTD algorithms suffer from a very high computational\ncomplexity in terms of both storage and computation time, which has been one\nmajor obstacle for practical applications of NTD. To overcome these\ndisadvantages, we show how low (multilinear) rank approximation (LRA) of\ntensors is able to significantly simplify the computation of the gradients of\nthe cost function, upon which a family of efficient first-order NTD algorithms\nare developed. Besides dramatically reducing the storage complexity and running\ntime, the new algorithms are quite flexible and robust to noise because any\nwell-established LRA approaches can be applied. We also show how nonnegativity\nincorporating sparsity substantially improves the uniqueness property and\npartially alleviates the curse of dimensionality of the Tucker decompositions.\nSimulation results on synthetic and real-world data justify the validity and\nhigh efficiency of the proposed NTD algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 01:52:09 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 08:58:14 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Zhao", "Qibin", ""], ["Xie", "Shengli", ""]]}, {"id": "1404.4644", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "A New Space for Comparing Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a new mathematical representations for graph, which allows direct\ncomparison between different graph structures, is an open-ended research\ndirection. Having such a representation is the first prerequisite for a variety\nof machine learning algorithms like classification, clustering, etc., over\ngraph datasets. In this paper, we propose a symmetric positive semidefinite\nmatrix with the $(i,j)$-{th} entry equal to the covariance between normalized\nvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representation\nfor graph with adjacency matrix $A$. We show that the proposed matrix\nrepresentation encodes the spectrum of the underlying adjacency matrix and it\nalso contains information about the counts of small sub-structures present in\nthe graph such as triangles and small paths. In addition, we show that this\nmatrix is a \\emph{\"graph invariant\"}. All these properties make the proposed\nmatrix a suitable object for representing graphs.\n  The representation, being a covariance matrix in a fixed dimensional metric\nspace, gives a mathematical embedding for graphs. This naturally leads to a\nmeasure of similarity on graph objects. We define similarity between two given\ngraphs as a Bhattacharya similarity measure between their corresponding\ncovariance matrix representations. As shown in our experimental study on the\ntask of social network classification, such a similarity measure outperforms\nother widely used state-of-the-art methodologies. Our proposed method is also\ncomputationally efficient. The computation of both the matrix representation\nand the similarity value can be performed in operations linear in the number of\nedges. This makes our method scalable in practice.\n  We believe our theoretical and empirical results provide evidence for\nstudying truncated power iterations, of the adjacency matrix, to characterize\nsocial networks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:39:24 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.4655", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Hierarchical Quasi-Clustering Methods for Asymmetric Networks", "comments": "Accepted to the 31st International Conference on Machine Learning\n  (ICML), Beijing, China, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces hierarchical quasi-clustering methods, a generalization\nof hierarchical clustering for asymmetric networks where the output structure\npreserves the asymmetry of the input data. We show that this output structure\nis equivalent to a finite quasi-ultrametric space and study admissibility with\nrespect to two desirable properties. We prove that a modified version of single\nlinkage is the only admissible quasi-clustering method. Moreover, we show\nstability of the proposed method and we establish invariance properties\nfulfilled by it. Algorithms are further developed and the value of\nquasi-clustering analysis is illustrated with a study of internal migration\nwithin United States.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 21:16:13 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1404.4667", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Gonzalo Mateos, and Georgios B. Giannakis", "title": "Subspace Learning and Imputation for Streaming Big Data Matrices and\n  Tensors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2417491", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting latent low-dimensional structure from high-dimensional data is of\nparamount importance in timely inference tasks encountered with `Big Data'\nanalytics. However, increasingly noisy, heterogeneous, and incomplete datasets\nas well as the need for {\\em real-time} processing of streaming data pose major\nchallenges to this end. In this context, the present paper permeates benefits\nfrom rank minimization to scalable imputation of missing data, via tracking\nlow-dimensional subspaces and unraveling latent (possibly multi-way) structure\nfrom \\emph{incomplete streaming} data. For low-rank matrix data, a subspace\nestimator is proposed based on an exponentially-weighted least-squares\ncriterion regularized with the nuclear norm. After recasting the non-separable\nnuclear norm into a form amenable to online optimization, real-time algorithms\nwith complementary strengths are developed and their convergence is established\nunder simplifying technical assumptions. In a stationary setting, the\nasymptotic estimates obtained offer the well-documented performance guarantees\nof the {\\em batch} nuclear-norm regularized estimator. Under the same unifying\nframework, a novel online (adaptive) algorithm is developed to obtain multi-way\ndecompositions of \\emph{low-rank tensors} with missing entries, and perform\nimputation as a byproduct. Simulated tests with both synthetic as well as real\nInternet and cardiac magnetic resonance imagery (MRI) data confirm the efficacy\nof the proposed algorithms, and their superior performance relative to\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 22:55:08 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Mardani", "Morteza", ""], ["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1404.4812", "submitter": "Tobias Fritz", "authors": "Tobias Fritz", "title": "Beyond Bell's Theorem II: Scenarios with arbitrary causal structure", "comments": "46 pages, 10 figures. v2: updated references. To appear in Comm Math\n  Phys", "journal-ref": "Comm. Math. Phys. 341(2), 391-434 (2016)", "doi": "10.1007/s00220-015-2495-5", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been found that Bell scenarios are only a small subclass of\ninteresting setups for studying the non-classical features of quantum theory\nwithin spacetime. We find that it is possible to talk about classical\ncorrelations, quantum correlations and other kinds of correlations on any\ndirected acyclic graph, and this captures various extensions of Bell scenarios\nwhich have been considered in the literature. From a conceptual point of view,\nthe main feature of our approach is its high level of unification: while the\nnotions of source, choice of setting and measurement play all seemingly\ndifferent roles in a Bell scenario, our formalism shows that they are all\ninstances of the same concept of \"event\".\n  Our work can also be understood as a contribution to the subject of causal\ninference with latent variables. Among other things, we introduce hidden\nBayesian networks as a generalization of hidden Markov models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 15:11:34 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 00:03:32 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Fritz", "Tobias", ""]]}, {"id": "1404.4997", "submitter": "Eric Price", "authors": "Moritz Hardt and Eric Price", "title": "Tight bounds for learning a mixture of two gaussians", "comments": "STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider the problem of identifying the parameters of an unknown mixture\nof two arbitrary $d$-dimensional gaussians from a sequence of independent\nrandom samples. Our main results are upper and lower bounds giving a\ncomputationally efficient moment-based estimator with an optimal convergence\nrate, thus resolving a problem introduced by Pearson (1894). Denoting by\n$\\sigma^2$ the variance of the unknown mixture, we prove that\n$\\Theta(\\sigma^{12})$ samples are necessary and sufficient to estimate each\nparameter up to constant additive error when $d=1.$ Our upper bound extends to\narbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$\nusing a novel---yet simple---dimensionality reduction technique. We further\nidentify several interesting special cases where the sample complexity is\nnotably smaller than our optimal worst-case bound. For instance, if the means\nof the two components are separated by $\\Omega(\\sigma)$ the sample complexity\nreduces to $O(\\sigma^2)$ and this is again optimal.\n  Our results also apply to learning each component of the mixture up to small\nerror in total variation distance, where our algorithm gives strong\nimprovements in sample complexity over previous work. We also extend our lower\nbound to mixtures of $k$ Gaussians, showing that $\\Omega(\\sigma^{6k-2})$\nsamples are necessary to estimate each parameter up to constant additive error.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 23:59:35 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:15:35 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 04:47:58 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Hardt", "Moritz", ""], ["Price", "Eric", ""]]}, {"id": "1404.5028", "submitter": "Hiroaki  Sasaki", "authors": "Hiroaki Sasaki, Aapo Hyv\\\"arinen, Masashi Sugiyama", "title": "Clustering via Mode Seeking by Direct Estimation of the Gradient of a\n  Log-Density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean shift clustering finds the modes of the data probability density by\nidentifying the zero points of the density gradient. Since it does not require\nto fix the number of clusters in advance, the mean shift has been a popular\nclustering algorithm in various application fields. A typical implementation of\nthe mean shift is to first estimate the density by kernel density estimation\nand then compute its gradient. However, since good density estimation does not\nnecessarily imply accurate estimation of the density gradient, such an indirect\ntwo-step approach is not reliable. In this paper, we propose a method to\ndirectly estimate the gradient of the log-density without going through density\nestimation. The proposed method gives the global solution analytically and thus\nis computationally efficient. We then develop a mean-shift-like fixed-point\nalgorithm to find the modes of the density for clustering. As in the mean\nshift, one does not need to set the number of clusters in advance. We\nempirically show that the proposed clustering method works much better than the\nmean shift especially for high-dimensional data. Experimental results further\nindicate that the proposed method outperforms existing clustering methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 08:43:44 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Hyv\u00e4rinen", "Aapo", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1404.5122", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao", "title": "Spatiotemporal Sparse Bayesian Learning with Applications to Compressed\n  Sensing of Multichannel Physiological Signals", "comments": "Codes are available at:\n  https://sites.google.com/site/researchbyzhang/stsbl", "journal-ref": "IEEE Transactions On Neural Systems And Rehabilitation\n  Engineering, Vol. 22, No. 6, pp. 1186-1197, November 2014", "doi": "10.1109/TNSRE.2014.2319334", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is an important issue in continuous wireless\ntelemonitoring of physiological signals. Compressed sensing (CS) is a promising\nframework to address it, due to its energy-efficient data compression\nprocedure. However, most CS algorithms have difficulty in data recovery due to\nnon-sparsity characteristic of many physiological signals. Block sparse\nBayesian learning (BSBL) is an effective approach to recover such signals with\nsatisfactory recovery quality. However, it is time-consuming in recovering\nmultichannel signals, since its computational load almost linearly increases\nwith the number of channels.\n  This work proposes a spatiotemporal sparse Bayesian learning algorithm to\nrecover multichannel signals simultaneously. It not only exploits temporal\ncorrelation within each channel signal, but also exploits inter-channel\ncorrelation among different channel signals. Furthermore, its computational\nload is not significantly affected by the number of channels. The proposed\nalgorithm was applied to brain computer interface (BCI) and EEG-based driver's\ndrowsiness estimation. Results showed that the algorithm had both better\nrecovery performance and much higher speed than BSBL. Particularly, the\nproposed algorithm ensured that the BCI classification and the drowsiness\nestimation had little degradation even when data were compressed by 80%, making\nit very suitable for continuous wireless telemonitoring of multichannel\nsignals.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 06:35:57 GMT"}, {"version": "v2", "created": "Sat, 15 Nov 2014 01:53:29 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhang", "Zhilin", ""], ["Jung", "Tzyy-Ping", ""], ["Makeig", "Scott", ""], ["Pi", "Zhouyue", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1404.5165", "submitter": "Kian Hsiang Low", "authors": "Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul", "title": "GP-Localize: Persistent Mobile Robot Localization using Online Sparse\n  Gaussian Process Observation Model", "comments": "28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended\n  version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to robot exploration and mapping is the task of persistent\nlocalization in environmental fields characterized by spatially correlated\nmeasurements. This paper presents a Gaussian process localization (GP-Localize)\nalgorithm that, in contrast to existing works, can exploit the spatially\ncorrelated field measurements taken during a robot's exploration (instead of\nrelying on prior training data) for efficiently and scalably learning the GP\nobservation model online through our proposed novel online sparse GP. As a\nresult, GP-Localize is capable of achieving constant time and memory (i.e.,\nindependent of the size of the data) per filtering step, which demonstrates the\npractical feasibility of using GPs for persistent robot localization and\nautonomy. Empirical evaluation via simulated experiments with real-world\ndatasets and a real robot experiment shows that GP-Localize outperforms\nexisting GP localization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 10:28:00 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 08:03:33 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Xu", "Nuo", ""], ["Low", "Kian Hsiang", ""], ["Chen", "Jie", ""], ["Lim", "Keng Kiat", ""], ["Ozgul", "Etkin Baris", ""]]}, {"id": "1404.5214", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Graph Kernels via Functional Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a representation of graph as a functional object derived from the\npower iteration of the underlying adjacency matrix. The proposed functional\nrepresentation is a graph invariant, i.e., the functional remains unchanged\nunder any reordering of the vertices. This property eliminates the difficulty\nof handling exponentially many isomorphic forms. Bhattacharyya kernel\nconstructed between these functionals significantly outperforms the\nstate-of-the-art graph kernels on 3 out of the 4 standard benchmark graph\nclassification datasets, demonstrating the superiority of our approach. The\nproposed methodology is simple and runs in time linear in the number of edges,\nwhich makes our kernel more efficient and scalable compared to many widely\nadopted graph kernels with running time cubic in the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 14:56:17 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.5443", "submitter": "Ville Tolvanen", "authors": "Ville Tolvanen, Pasi Jyl\\\"anki, Aki Vehtari", "title": "Approximate Inference for Nonstationary Heteroscedastic Gaussian process\n  Regression", "comments": null, "journal-ref": "2014 IEEE International Workshop on Machine Learning for Signal\n  Processing (MLSP)", "doi": "10.1109/MLSP.2014.6958906", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for approximate integration over the\nuncertainty of noise and signal variances in Gaussian process (GP) regression.\nOur efficient and straightforward approach can also be applied to integration\nover input dependent noise variance (heteroscedasticity) and input dependent\nsignal variance (nonstationarity) by setting independent GP priors for the\nnoise and signal variances. We use expectation propagation (EP) for inference\nand compare results to Markov chain Monte Carlo in two simulated data sets and\nthree empirical examples. The results show that EP produces comparable results\nwith less computational burden.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 10:04:10 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Tolvanen", "Ville", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Vehtari", "Aki", ""]]}, {"id": "1404.5692", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Parikshit Shah, Stephen Wright", "title": "Forward - Backward Greedy Algorithms for Atomic Norm Regularization", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2461515", "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many signal processing applications, the aim is to reconstruct a signal\nthat has a simple representation with respect to a certain basis or frame.\nFundamental elements of the basis known as \"atoms\" allow us to define \"atomic\nnorms\" that can be used to formulate convex regularizations for the\nreconstruction problem. Efficient algorithms are available to solve these\nformulations in certain special cases, but an approach that works well for\ngeneral atomic norms, both in terms of speed and reconstruction accuracy,\nremains to be found. This paper describes an optimization algorithm called\nCoGEnT that produces solutions with succinct atomic representations for\nreconstruction problems, generally formulated with atomic-norm constraints.\nCoGEnT combines a greedy selection scheme based on the conditional gradient\napproach with a backward (or \"truncation\") step that exploits the quadratic\nnature of the objective to reduce the basis size. We establish convergence\nproperties and validate the algorithm via extensive numerical experiments on a\nsuite of signal processing applications. Our algorithm and analysis also allow\nfor inexact forward steps and for occasional enhancements of the current\nrepresentation to be performed. CoGEnT can outperform the basic conditional\ngradient method, and indeed many methods that are tailored to specific\napplications, when the enhancement and truncation steps are defined\nappropriately. We also introduce several novel applications that are enabled by\nthe atomic-norm framework, including tensor completion, moment problems in\nsignal processing, and graph deconvolution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 03:31:45 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 01:33:16 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Rao", "Nikhil", ""], ["Shah", "Parikshit", ""], ["Wright", "Stephen", ""]]}, {"id": "1404.5793", "submitter": "Muneki Yasuda", "authors": "Shun Kataoka, Muneki Yasuda, Kazuyuki Tanaka", "title": "Bayesian Reconstruction of Missing Observations", "comments": null, "journal-ref": "Interdisciplinary Information Sciences, Vol.21, No.1, pp.11-23,\n  2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on an interpolation method referred to Bayesian reconstruction in\nthis paper. Whereas in standard interpolation methods missing data are\ninterpolated deterministically, in Bayesian reconstruction, missing data are\ninterpolated probabilistically using a Bayesian treatment. In this paper, we\naddress the framework of Bayesian reconstruction and its application to the\ntraffic data reconstruction problem in the field of traffic engineering. In the\nlatter part of this paper, we describe the evaluation of the statistical\nperformance of our Bayesian traffic reconstruction model using a statistical\nmechanical approach and clarify its statistical behavior.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 12:02:59 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 04:36:38 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1404.5903", "submitter": "Che-Yu Liu Mr.", "authors": "Che-Yu Liu, S\\'ebastien Bubeck", "title": "Most Correlated Arms Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the most mutually correlated arms among many\narms. We show that adaptive arms sampling strategies can have significant\nadvantages over the non-adaptive uniform sampling strategy. Our proposed\nalgorithms rely on a novel correlation estimator. The use of this accurate\nestimator allows us to get improved results for a wide range of problem\ninstances.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 17:25:02 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Liu", "Che-Yu", ""], ["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1404.6000", "submitter": "T. Tony Cai", "authors": "T. Tony Cai, Xiaodong Li", "title": "Robust and computationally feasible community detection in the presence\n  of arbitrary outlier nodes", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1290 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1027-1059", "doi": "10.1214/14-AOS1290", "report-no": "IMS-AOS-AOS1290", "categories": "math.ST cs.IT math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection, which aims to cluster $N$ nodes in a given graph into\n$r$ distinct groups based on the observed undirected edges, is an important\nproblem in network data analysis. In this paper, the popular stochastic block\nmodel (SBM) is extended to the generalized stochastic block model (GSBM) that\nallows for adversarial outlier nodes, which are connected with the other nodes\nin the graph in an arbitrary way. Under this model, we introduce a procedure\nusing convex optimization followed by $k$-means algorithm with $k=r$. Both\ntheoretical and numerical properties of the method are analyzed. A theoretical\nguarantee is given for the procedure to accurately detect the communities with\nsmall misclassification rate under the setting where the number of clusters can\ngrow with $N$. This theoretical result admits to the best-known result in the\nliterature of computationally feasible community detection in SBM without\noutliers. Numerical results show that our method is both computationally fast\nand robust to different kinds of outliers, while some popular computationally\nfast community detection algorithms, such as spectral clustering applied to\nadjacency matrices or graph Laplacians, may fail to retrieve the major clusters\ndue to a small portion of outliers. We apply a slight modification of our\nmethod to a political blogs data set, showing that our method is competent in\npractice and comparable to existing computationally feasible methods in the\nliterature. To the best of the authors' knowledge, our result is the first in\nthe literature in terms of clustering communities with fast growing numbers\nunder the GSBM where a portion of arbitrary outlier nodes exist.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 23:36:19 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 14:50:49 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 16:11:01 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 06:00:00 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Cai", "T. Tony", ""], ["Li", "Xiaodong", ""]]}, {"id": "1404.6074", "submitter": "Marie Schrynemackers", "authors": "Marie Schrynemackers, Louis Wehenkel, M. Madan Babu and Pierre Geurts", "title": "Classifying pairs with trees for supervised biological network inference", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in biology and computational approaches have been\nlargely investigated for their inference. In particular, supervised machine\nlearning methods can be used to complete a partially known network by\nintegrating various measurements. Two main supervised frameworks have been\nproposed: the local approach, which trains a separate model for each network\nnode, and the global approach, which trains a single model over pairs of nodes.\nHere, we systematically investigate, theoretically and empirically, the\nexploitation of tree-based ensemble methods in the context of these two\napproaches for biological network inference. We first formalize the problem of\nnetwork inference as classification of pairs, unifying in the process\nhomogeneous and bipartite graphs and discussing two main sampling schemes. We\nthen present the global and the local approaches, extending the later for the\nprediction of interactions between two unseen network nodes, and discuss their\nspecializations to tree-based ensemble methods, highlighting their\ninterpretability and drawing links with clustering techniques. Extensive\ncomputational experiments are carried out with these methods on various\nbiological networks that clearly highlight that these methods are competitive\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 10:22:33 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Schrynemackers", "Marie", ""], ["Wehenkel", "Louis", ""], ["Babu", "M. Madan", ""], ["Geurts", "Pierre", ""]]}, {"id": "1404.6216", "submitter": "Ping Li", "authors": "Ping Li", "title": "CoRE Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"CoRE kernel\" stands for correlation-resemblance kernel. In many\napplications (e.g., vision), the data are often high-dimensional, sparse, and\nnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binary\nsparse data and demonstrate the effectiveness of the new kernels through a\nclassification experiment. CoRE kernels are simple with no tuning parameters.\nHowever, training nonlinear kernel SVM can be (very) costly in time and memory\nand may not be suitable for truly large-scale industrial applications (e.g.\nsearch). In order to make the proposed CoRE kernels more practical, we develop\nbasic probabilistic hashing algorithms which transform nonlinear kernels into\nlinear kernels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:35:37 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1404.6283", "submitter": "Bryan Daniels", "authors": "Bryan C. Daniels and Ilya Nemenman", "title": "Automated adaptive inference of coarse-grained dynamical models in\n  systems biology", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular regulatory dynamics is driven by large and intricate networks of\ninteractions at the molecular scale, whose sheer size obfuscates understanding.\nIn light of limited experimental data, many parameters of such dynamics are\nunknown, and thus models built on the detailed, mechanistic viewpoint overfit\nand are not predictive. At the other extreme, simple ad hoc models of complex\nprocesses often miss defining features of the underlying systems. Here we\npropose an approach that instead constructs phenomenological, coarse-grained\nmodels of network dynamics that automatically adapt their complexity to the\namount of available data. Such adaptive models lead to accurate predictions\neven when microscopic details of the studied systems are unknown due to\ninsufficient data. The approach is computationally tractable, even for a\nrelatively large number of dynamical variables, allowing its software\nrealization, named Sir Isaac, to make successful predictions even when\nimportant dynamic variables are unobserved. For example, it matches the known\nphase space structure for simulated planetary motion data, avoids overfitting\nin a complex biological signaling system, and produces accurate predictions for\na yeast glycolysis model with only tens of data points and over half of the\ninteracting species unobserved.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 22:35:56 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Daniels", "Bryan C.", ""], ["Nemenman", "Ilya", ""]]}, {"id": "1404.6289", "submitter": "Qing Zhou", "authors": "Yuliya Marchetti and Qing Zhou", "title": "Solution Path Clustering with Adaptive Concave Penalty", "comments": "36 pages", "journal-ref": "Electronic Journal of Statistics, 8 (2014): 1569-1603", "doi": "10.1214/14-EJS934", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast accumulation of large amounts of complex data has created a need for\nmore sophisticated statistical methodologies to discover interesting patterns\nand better extract information from these data. The large scale of the data\noften results in challenging high-dimensional estimation problems where only a\nminority of the data shows specific grouping patterns. To address these\nemerging challenges, we develop a new clustering methodology that introduces\nthe idea of a regularization path into unsupervised learning. A regularization\npath for a clustering problem is created by varying the degree of sparsity\nconstraint that is imposed on the differences between objects via the minimax\nconcave penalty with adaptive tuning parameters. Instead of providing a single\nsolution represented by a cluster assignment for each object, the method\nproduces a short sequence of solutions that determines not only the cluster\nassignment but also a corresponding number of clusters for each solution. The\noptimization of the penalized loss function is carried out through an MM\nalgorithm with block coordinate descent. The advantages of this clustering\nalgorithm compared to other existing methods are as follows: it does not\nrequire the input of the number of clusters; it is capable of simultaneously\nseparating irrelevant or noisy observations that show no grouping pattern,\nwhich can greatly improve data interpretation; it is a general methodology that\ncan be applied to many clustering problems. We test this method on various\nsimulated datasets and on gene expression data, where it shows better or\ncompetitive performance compared against several clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 23:12:14 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Marchetti", "Yuliya", ""], ["Zhou", "Qing", ""]]}, {"id": "1404.6473", "submitter": "Lucas Mentch", "authors": "Lucas Mentch, Giles Hooker", "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests", "comments": "To appear in The Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 16:15:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 18:52:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1404.6640", "submitter": "Martin Slawski", "authors": "Martin Slawski, Matthias Hein", "title": "Estimation of positive definite M-matrices and structure learning for\n  attractive Gaussian Markov Random fields", "comments": "long version of a manuscript accepted for publication in Linear\n  Algebra and its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector with finite second moments. If its precision matrix\nis an M-matrix, then all partial correlations are non-negative. If that random\nvector is additionally Gaussian, the corresponding Markov random field (GMRF)\nis called attractive. We study estimation of M-matrices taking the role of\ninverse second moment or precision matrices using sign-constrained\nlog-determinant divergence minimization. We also treat the high-dimensional\ncase with the number of variables exceeding the sample size. The additional\nsign-constraints turn out to greatly simplify the estimation problem: we\nprovide evidence that explicit regularization is no longer required. To solve\nthe resulting convex optimization problem, we propose an algorithm based on\nblock coordinate descent, in which each sub-problem can be recast as\nnon-negative least squares problem. Illustrations on both simulated and real\nworld data are provided.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 13:22:37 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Slawski", "Martin", ""], ["Hein", "Matthias", ""]]}, {"id": "1404.6702", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh", "title": "A Constrained Matrix-Variate Gaussian Process for Transposable Data", "comments": "23 pages, Preliminary version, Accepted for publication in Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transposable data represents interactions among two sets of entities, and are\ntypically represented as a matrix containing the known interaction values.\nAdditional side information may consist of feature vectors specific to entities\ncorresponding to the rows and/or columns of such a matrix. Further information\nmay also be available in the form of interactions or hierarchies among entities\nalong the same mode (axis). We propose a novel approach for modeling\ntransposable data with missing interactions given additional side information.\nThe interactions are modeled as noisy observations from a latent noise free\nmatrix generated from a matrix-variate Gaussian process. The construction of\nrow and column covariances using side information provides a flexible mechanism\nfor specifying a-priori knowledge of the row and column correlations in the\ndata. Further, the use of such a prior combined with the side information\nenables predictions for new rows and columns not observed in the training data.\nIn this work, we combine the matrix-variate Gaussian process model with low\nrank constraints. The constrained Gaussian process approach is applied to the\nprediction of hidden associations between genes and diseases using a small set\nof observed associations as well as prior covariances induced by gene-gene\ninteraction networks and disease ontologies. The proposed approach is also\napplied to recommender systems data which involves predicting the item ratings\nof users using known associations as well as prior covariances induced by\nsocial networks. We present experimental results that highlight the performance\nof constrained matrix-variate Gaussian process as compared to state of the art\napproaches in each domain.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 01:46:49 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Lee", "Cheng", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1404.6769", "submitter": "Christophe Giraud", "authors": "Christophe Giraud, Fran\\c{c}ois Roueff, Andres Sanchez-Perez", "title": "Aggregation of predictors for nonstationary sub-linear processes and\n  online adaptive forecasting of time varying autoregressive processes", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1345 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2412-2450", "doi": "10.1214/15-AOS1345", "report-no": "IMS-AOS-AOS1345", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of aggregating a finite number of\npredictors for nonstationary sub-linear processes. We provide oracle\ninequalities relying essentially on three ingredients: (1) a uniform bound of\nthe $\\ell^1$ norm of the time varying sub-linear coefficients, (2) a Lipschitz\nassumption on the predictors and (3) moment conditions on the noise appearing\nin the linear representation. Two kinds of aggregations are considered giving\nrise to different moment conditions on the noise and more or less sharp oracle\ninequalities. We apply this approach for deriving an adaptive predictor for\nlocally stationary time varying autoregressive (TVAR) processes. It is obtained\nby aggregating a finite number of well chosen predictors, each of them enjoying\nan optimal minimax convergence rate under specific smoothness conditions on the\nTVAR coefficients. We show that the obtained aggregated predictor achieves a\nminimax rate while adapting to the unknown smoothness. To prove this result, a\nlower bound is established for the minimax rate of the prediction risk for the\nTVAR process. Numerical experiments complete this study. An important feature\nof this approach is that the aggregated predictor can be computed recursively\nand is thus applicable in an online prediction context.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:26:52 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 18:47:09 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 12:43:57 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 17:26:27 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2015 10:11:03 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Giraud", "Christophe", ""], ["Roueff", "Fran\u00e7ois", ""], ["Sanchez-Perez", "Andres", ""]]}, {"id": "1404.6818", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Michael Tschannen and Helmut B\\\"olcskei", "title": "Subspace clustering of dimensionality-reduced data", "comments": "ISIT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering unlabeled\nhigh-dimensional data points into a union of low-dimensional linear subspaces,\nassumed unknown. In practice one may have access to dimensionality-reduced\nobservations of the data only, resulting, e.g., from \"undersampling\" due to\ncomplexity and speed constraints on the acquisition device. More pertinently,\neven if one has access to the high-dimensional data set it is often desirable\nto first project the data points into a lower-dimensional space and to perform\nthe clustering task there; this reduces storage requirements and computational\ncost. The purpose of this paper is to quantify the impact of\ndimensionality-reduction through random projection on the performance of the\nsparse subspace clustering (SSC) and the thresholding based subspace clustering\n(TSC) algorithms. We find that for both algorithms dimensionality reduction\ndown to the order of the subspace dimensions is possible without incurring\nsignificant performance degradation. The mathematical engine behind our\ntheorems is a result quantifying how the affinities between subspaces change\nunder random dimensionality reducing projections.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 20:02:11 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Heckel", "Reinhard", ""], ["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1404.6853", "submitter": "Rachel Ward", "authors": "Karin Knudson, Rayan Saab, and Rachel Ward", "title": "One-bit compressive sensing with norm estimation", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the recovery of an unknown signal ${x}$ from quantized linear\nmeasurements. In the one-bit compressive sensing setting, one typically assumes\nthat ${x}$ is sparse, and that the measurements are of the form\n$\\operatorname{sign}(\\langle {a}_i, {x} \\rangle) \\in \\{\\pm1\\}$. Since such\nmeasurements give no information on the norm of ${x}$, recovery methods from\nsuch measurements typically assume that $\\| {x} \\|_2=1$. We show that if one\nallows more generally for quantized affine measurements of the form\n$\\operatorname{sign}(\\langle {a}_i, {x} \\rangle + b_i)$, and if the vectors\n${a}_i$ are random, an appropriate choice of the affine shifts $b_i$ allows\nnorm recovery to be easily incorporated into existing methods for one-bit\ncompressive sensing. Additionally, we show that for arbitrary fixed ${x}$ in\nthe annulus $r \\leq \\| {x} \\|_2 \\leq R$, one may estimate the norm $\\| {x}\n\\|_2$ up to additive error $\\delta$ from $m \\gtrsim R^4 r^{-2} \\delta^{-2}$\nsuch binary measurements through a single evaluation of the inverse Gaussian\nerror function. Finally, all of our recovery guarantees can be made universal\nover sparse vectors, in the sense that with high probability, one set of\nmeasurements and thresholds can successfully estimate all sparse vectors ${x}$\nwithin a Euclidean ball of known radius.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 02:01:33 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 23:58:38 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 04:17:58 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Knudson", "Karin", ""], ["Saab", "Rayan", ""], ["Ward", "Rachel", ""]]}, {"id": "1404.6876", "submitter": "Voot Tangkaratt", "authors": "Voot Tangkaratt, Ning Xie, and Masashi Sugiyama", "title": "Conditional Density Estimation with Dimensionality Reduction via\n  Squared-Loss Conditional Entropy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression aims at estimating the conditional mean of output given input.\nHowever, regression is not informative enough if the conditional density is\nmultimodal, heteroscedastic, and asymmetric. In such a case, estimating the\nconditional density itself is preferable, but conditional density estimation\n(CDE) is challenging in high-dimensional space. A naive approach to coping with\nhigh-dimensionality is to first perform dimensionality reduction (DR) and then\nexecute CDE. However, such a two-step process does not perform well in practice\nbecause the error incurred in the first DR step can be magnified in the second\nCDE step. In this paper, we propose a novel single-shot procedure that performs\nCDE and DR simultaneously in an integrated way. Our key idea is to formulate DR\nas the problem of minimizing a squared-loss variant of conditional entropy, and\nthis is solved via CDE. Thus, an additional CDE step is not needed after DR. We\ndemonstrate the usefulness of the proposed method through extensive experiments\non various datasets including humanoid robot transition and computer art.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 06:30:39 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Tangkaratt", "Voot", ""], ["Xie", "Ning", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1404.7048", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Dimitrios Mavroeidis, Francesco Calabrese, Pascal\n  Frossard", "title": "Multiscale Event Detection in Social Media", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, vol. 29, no. 5, pp.\n  1374-1405, September 2015", "doi": "10.1007/s10618-015-0421-2", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection has been one of the most important research topics in social\nmedia analysis. Most of the traditional approaches detect events based on fixed\ntemporal and spatial resolutions, while in reality events of different scales\nusually occur simultaneously, namely, they span different intervals in time and\nspace. In this paper, we propose a novel approach towards multiscale event\ndetection using social media data, which takes into account different temporal\nand spatial scales of events in the data. Specifically, we explore the\nproperties of the wavelet transform, which is a well-developed multiscale\ntransform in signal processing, to enable automatic handling of the interaction\nbetween temporal and spatial scales. We then propose a novel algorithm to\ncompute a data similarity graph at appropriate scales and detect events of\ndifferent scales simultaneously by a single graph-based clustering process.\nFurthermore, we present spatiotemporal statistical analysis of the noisy\ninformation present in the data stream, which allows us to define a novel\nterm-filtering procedure for the proposed event detection algorithm and helps\nus study its behavior using simulated noisy data. Experimental results on both\nsynthetically generated data and real world data collected from Twitter\ndemonstrate the meaningfulness and effectiveness of the proposed approach. Our\nframework further extends to numerous application domains that involve\nmultiscale and multiresolution data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 13:28:37 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 00:15:42 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Dong", "Xiaowen", ""], ["Mavroeidis", "Dimitrios", ""], ["Calabrese", "Francesco", ""], ["Frossard", "Pascal", ""]]}, {"id": "1404.7055", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, and Sebastien Roch", "title": "Data Requirement for Phylogenetic Inference from Multiple Loci: A New\n  Distance Method", "comments": "19 pages, 2 figures. Preliminary version to appear in IEEE ISIT 2014.\n  Added acknowledgements and made the proof of the \"equality\" part of Theorem 3\n  explicit in Appendix C", "journal-ref": null, "doi": "10.1109/TCBB.2014.2361685", "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the evolutionary history of a set of\nspecies (phylogeny or species tree) from several genes. It is known that the\nevolutionary history of individual genes (gene trees) might be topologically\ndistinct from each other and from the underlying species tree, possibly\nconfounding phylogenetic analysis. A further complication in practice is that\none has to estimate gene trees from molecular sequences of finite length. We\nprovide the first full data-requirement analysis of a species tree\nreconstruction method that takes into account estimation errors at the gene\nlevel. Under that criterion, we also devise a novel reconstruction algorithm\nthat provably improves over all previous methods in a regime of interest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 16:54:20 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 09:53:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1404.7203", "submitter": "Mert Pilanci", "authors": "Mert Pilanci, Martin J. Wainwright", "title": "Randomized Sketches of Convex Programs with Sharp Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projection (RP) is a classical technique for reducing storage and\ncomputational costs. We analyze RP-based approximations of convex programs, in\nwhich the original optimization problem is approximated by the solution of a\nlower-dimensional problem. Such dimensionality reduction is essential in\ncomputation-limited settings, since the complexity of general convex\nprogramming can be quite high (e.g., cubic for quadratic programs, and\nsubstantially higher for semidefinite programs). In addition to computational\nsavings, random projection is also useful for reducing memory usage, and has\nuseful properties for privacy-sensitive optimization. We prove that the\napproximation ratio of this procedure can be bounded in terms of the geometry\nof constraint set. For a broad class of random projections, including those\nbased on various sub-Gaussian distributions as well as randomized Hadamard and\nFourier transforms, the data matrix defining the cost function can be projected\ndown to the statistical dimension of the tangent cone of the constraints at the\noriginal solution, which is often substantially smaller than the original\ndimension. We illustrate consequences of our theory for various cases,\nincluding unconstrained and $\\ell_1$-constrained least squares, support vector\nmachines, low-rank matrix estimation, and discuss implications on\nprivacy-sensitive optimization and some connections with de-noising and\ncompressed sensing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 00:57:59 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1404.7236", "submitter": "Han Liu", "authors": "Jianqing Fan, Han Liu, Yang Ning, Hui Zou", "title": "High Dimensional Semiparametric Latent Graphical Model for Mixed Data", "comments": "34 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are commonly used tools for modeling multivariate random\nvariables. While there exist many convenient multivariate distributions such as\nGaussian distribution for continuous data, mixed data with the presence of\ndiscrete variables or a combination of both continuous and discrete variables\nposes new challenges in statistical modeling. In this paper, we propose a\nsemiparametric model named latent Gaussian copula model for binary and mixed\ndata. The observed binary data are assumed to be obtained by dichotomizing a\nlatent variable satisfying the Gaussian copula distribution or the\nnonparanormal distribution. The latent Gaussian model with the assumption that\nthe latent variables are multivariate Gaussian is a special case of the\nproposed model. A novel rank-based approach is proposed for both latent graph\nestimation and latent principal component analysis. Theoretically, the proposed\nmethods achieve the same rates of convergence for both precision matrix\nestimation and eigenvector estimation, as if the latent variables were\nobserved. Under similar conditions, the consistency of graph structure recovery\nand feature selection for leading eigenvectors is established. The performance\nof the proposed methods is numerically assessed through simulation studies, and\nthe usage of our methods is illustrated by a genetic dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 05:12:50 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Ning", "Yang", ""], ["Zou", "Hui", ""]]}, {"id": "1404.7306", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin", "title": "Generalized Nonconvex Nonsmooth Low-Rank Minimization", "comments": "IEEE International Conference on Computer Vision and Pattern\n  Recognition, 2014", "journal-ref": null, "doi": "10.1109/CVPR.2014.526", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As surrogate functions of $L_0$-norm, many nonconvex penalty functions have\nbeen proposed to enhance the sparse vector recovery. It is easy to extend these\nnonconvex penalty functions on singular values of a matrix to enhance low-rank\nmatrix recovery. However, different from convex optimization, solving the\nnonconvex low-rank minimization problem is much more challenging than the\nnonconvex sparse minimization problem. We observe that all the existing\nnonconvex penalty functions are concave and monotonically increasing on\n$[0,\\infty)$. Thus their gradients are decreasing functions. Based on this\nproperty, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to\nsolve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively\nsolves a Weighted Singular Value Thresholding (WSVT) problem. By setting the\nweight vector as the gradient of the concave penalty function, the WSVT problem\nhas a closed form solution. In theory, we prove that IRNN decreases the\nobjective function value monotonically, and any limit point is a stationary\npoint. Extensive experiments on both synthetic data and real images demonstrate\nthat IRNN enhances the low-rank matrix recovery compared with state-of-the-art\nconvex algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:45:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1404.7456", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter", "title": "Automatic Differentiation of Algorithms for Machine Learning", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation---the mechanical transformation of numeric computer\nprograms to calculate derivatives efficiently and accurately---dates to the\norigin of the computer age. Reverse mode automatic differentiation both\nantedates and generalizes the method of backwards propagation of errors used in\nmachine learning. Despite this, practitioners in a variety of fields, including\nmachine learning, have been little influenced by automatic differentiation, and\nmake scant use of available tools. Here we review the technique of automatic\ndifferentiation, describe its two main modes, and explain how it can benefit\nmachine learning practitioners. To reach the widest possible audience our\ntreatment assumes only elementary differential calculus, and does not assume\nany knowledge of linear algebra.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 17:19:25 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1404.7552", "submitter": "Geoffrey Schiebinger", "authors": "Geoffrey Schiebinger, Martin J. Wainwright, Bin Yu", "title": "The geometry of kernelized spectral clustering", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1283 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 2, 819-846", "doi": "10.1214/14-AOS1283", "report-no": "IMS-AOS-AOS1283", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data sets is a standard problem in many areas of science and\nengineering. The method of spectral clustering is based on embedding the data\nset using a kernel function, and using the top eigenvectors of the normalized\nLaplacian to recover the connected components. We study the performance of\nspectral clustering in recovering the latent labels of i.i.d. samples from a\nfinite mixture of nonparametric distributions. The difficulty of this label\nrecovery problem depends on the overlap between mixture components and how\neasily a mixture component is divided into two nonoverlapping components. When\nthe overlap is small compared to the indivisibility of the mixture components,\nthe principal eigenspace of the population-level normalized Laplacian operator\nis approximately spanned by the square-root kernelized component densities. In\nthe finite sample setting, and under the same assumption, embedded samples from\ndifferent components are approximately orthogonal with high probability when\nthe sample size is large. As a corollary we control the fraction of samples\nmislabeled by spectral clustering under finite mixtures with nonparametric\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 23:23:57 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 00:03:06 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 05:16:34 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Schiebinger", "Geoffrey", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1404.7789", "submitter": "Pan Zhang", "authors": "Pan Zhang, Cristopher Moore, and Lenka Zdeborov\\'a", "title": "Phase transitions in semisupervised clustering of sparse networks", "comments": null, "journal-ref": "Phys. Rev. E 90, 052802 (2014)", "doi": "10.1103/PhysRevE.90.052802", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting labels of nodes in a network, such as community memberships or\ndemographic variables, is an important problem with applications in social and\nbiological networks. A recently-discovered phase transition puts fundamental\nlimits on the accuracy of these predictions if we have access only to the\nnetwork topology. However, if we know the correct labels of some fraction\n$\\alpha$ of the nodes, we can do better. We study the phase diagram of this\n\"semisupervised\" learning problem for networks generated by the stochastic\nblock model. We use the cavity method and the associated belief propagation\nalgorithm to study what accuracy can be achieved as a function of $\\alpha$. For\n$k = 2$ groups, we find that the detectability transition disappears for any\n$\\alpha > 0$, in agreement with previous work. For larger $k$ where a hard but\ndetectable regime exists, we find that the easy/hard transition (the point at\nwhich efficient algorithms can do better than chance) becomes a line of\ntransitions where the accuracy jumps discontinuously at a critical value of\n$\\alpha$. This line ends in a critical point with a second-order transition,\nbeyond which the accuracy is a continuous function of $\\alpha$. We demonstrate\nqualitatively similar transitions in two real-world networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 16:40:22 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Zhang", "Pan", ""], ["Moore", "Cristopher", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1404.7796", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (IST Austria), Amaury Habrard (LHC), St\\'ephane Ayache\n  (LIF)", "title": "Majority Vote of Diverse Classifiers for Late Fusion", "comments": "IAPR Joint International Workshops on Statistical Techniques in\n  Pattern Recognition and Structural and Syntactic Pattern Recignition, Joensuu\n  : Finland (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, a lot of attention has been devoted to multimedia\nindexing by fusing multimodal informations. Two kinds of fusion schemes are\ngenerally considered: The early fusion and the late fusion. We focus on late\nclassifier fusion, where one combines the scores of each modality at the\ndecision level. To tackle this problem, we investigate a recent and elegant\nwell-founded quadratic program named MinCq coming from the machine learning\nPAC-Bayesian theory. MinCq looks for the weighted combination, over a set of\nreal-valued functions seen as voters, leading to the lowest misclassification\nrate, while maximizing the voters' diversity. We propose an extension of MinCq\ntailored to multimedia indexing. Our method is based on an order-preserving\npairwise loss adapted to ranking that allows us to improve Mean Averaged\nPrecision measure while taking into account the diversity of the voters that we\nwant to fuse. We provide evidence that this method is naturally adapted to late\nfusion procedures and confirm the good behavior of our approach on the\nchallenging PASCAL VOC'07 benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 16:55:00 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 08:06:24 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Morvant", "Emilie", "", "IST Austria"], ["Habrard", "Amaury", "", "LHC"], ["Ayache", "St\u00e9phane", "", "LIF"]]}]