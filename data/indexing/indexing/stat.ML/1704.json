[{"id": "1704.00003", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung and Chao-Yuan Wu and Manzil Zaheer and Alexander J.\n  Smola", "title": "Spectral Methods for Nonparametric Models", "comments": "Keywords: Spectral Methods, Indian Buffet Process, Hierarchical\n  Dirichlet Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric models are versatile, albeit computationally expensive, tool\nfor modeling mixture models. In this paper, we introduce spectral methods for\nthe two most popular nonparametric models: the Indian Buffet Process (IBP) and\nthe Hierarchical Dirichlet Process (HDP). We show that using spectral methods\nfor the inference of nonparametric models are computationally and statistically\nefficient. In particular, we derive the lower-order moments of the IBP and the\nHDP, propose spectral algorithms for both models, and provide reconstruction\nguarantees for the algorithms. For the HDP, we further show that applying\nhierarchical models on dataset with hierarchical structure, which can be solved\nwith the generalized spectral HDP, produces better solutions to that of flat\nmodels regarding likelihood performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 03:50:03 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Wu", "Chao-Yuan", ""], ["Zaheer", "Manzil", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1704.00023", "submitter": "Tegjyot Singh Sethi", "authors": "Tegjyot Singh Sethi, Mehmed Kantardzic", "title": "On the Reliable Detection of Concept Drift from Streaming Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers deployed in the real world operate in a dynamic environment,\nwhere the data distribution can change over time. These changes, referred to as\nconcept drift, can cause the predictive performance of the classifier to drop\nover time, thereby making it obsolete. To be of any real use, these classifiers\nneed to detect drifts and be able to adapt to them, over time. Detecting drifts\nhas traditionally been approached as a supervised task, with labeled data\nconstantly being used for validating the learned model. Although effective in\ndetecting drifts, these techniques are impractical, as labeling is a difficult,\ncostly and time consuming activity. On the other hand, unsupervised change\ndetection techniques are unreliable, as they produce a large number of false\nalarms. The inefficacy of the unsupervised techniques stems from the exclusion\nof the characteristics of the learned classifier, from the detection process.\nIn this paper, we propose the Margin Density Drift Detection (MD3) algorithm,\nwhich tracks the number of samples in the uncertainty region of a classifier,\nas a metric to detect drift. The MD3 algorithm is a distribution independent,\napplication independent, model independent, unsupervised and incremental\nalgorithm for reliably detecting drifts from data streams. Experimental\nevaluation on 6 drift induced datasets and 4 additional datasets from the\ncybersecurity domain demonstrates that the MD3 approach can reliably detect\ndrifts, with significantly fewer false alarms compared to unsupervised feature\nbased drift detectors. The reduced false alarms enables the signaling of drifts\nonly when they are most likely to affect classification performance. As such,\nthe MD3 approach leads to a detection scheme which is credible, label efficient\nand general in its applicability.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 18:55:48 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sethi", "Tegjyot Singh", ""], ["Kantardzic", "Mehmed", ""]]}, {"id": "1704.00028", "submitter": "Ishaan Gulrajani", "authors": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin,\n  Aaron Courville", "title": "Improved Training of Wasserstein GANs", "comments": "NIPS camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:25:00 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 17:52:41 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 23:03:49 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Gulrajani", "Ishaan", ""], ["Ahmed", "Faruk", ""], ["Arjovsky", "Martin", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1704.00060", "submitter": "Anqi Wu", "authors": "Anqi Wu, Mikio C. Aoi, Jonathan W. Pillow", "title": "Exploiting gradients and Hessians in Bayesian optimization and Bayesian\n  quadrature", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exciting branch of machine learning research focuses on methods for\nlearning, optimizing, and integrating unknown functions that are difficult or\ncostly to evaluate. A popular Bayesian approach to this problem uses a Gaussian\nprocess (GP) to construct a posterior distribution over the function of\ninterest given a set of observed measurements, and selects new points to\nevaluate using the statistics of this posterior. Here we extend these methods\nto exploit derivative information from the unknown function. We describe\nmethods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings\nwhere first and second derivatives may be evaluated along with the function\nitself. We perform sampling-based inference in order to incorporate uncertainty\nover hyperparameters, and show that both hyperparameter and function\nuncertainty decrease much more rapidly when using derivative information.\nMoreover, we introduce techniques for overcoming ill-conditioning issues that\nhave plagued earlier methods for gradient-enhanced Gaussian processes and\nkriging. We illustrate the efficacy of these methods using applications to real\nand simulated Bayesian optimization and quadrature problems, and show that\nexploting derivatives can provide substantial gains over standard methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 21:13:08 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 20:21:30 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Wu", "Anqi", ""], ["Aoi", "Mikio C.", ""], ["Pillow", "Jonathan W.", ""]]}, {"id": "1704.00090", "submitter": "Marc-Andr\\'e Gardner", "authors": "Marc-Andr\\'e Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen,\n  Emiliano Gambaretto, Christian Gagn\\'e, Jean-Fran\\c{c}ois Lalonde", "title": "Learning to Predict Indoor Illumination from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method to infer high dynamic range illumination from\na single, limited field-of-view, low dynamic range photograph of an indoor\nscene. In contrast to previous work that relies on specialized image capture,\nuser input, and/or simple scene models, we train an end-to-end deep neural\nnetwork that directly regresses a limited field-of-view photo to HDR\nillumination, without strong assumptions on scene geometry, material\nproperties, or lighting. We show that this can be accomplished in a three step\nprocess: 1) we train a robust lighting classifier to automatically annotate the\nlocation of light sources in a large dataset of LDR environment maps, 2) we use\nthese annotations to train a deep neural network that predicts the location of\nlights in a scene from a single limited field-of-view photo, and 3) we\nfine-tune this network using a small dataset of HDR environment maps to predict\nlight intensities. This allows us to automatically recover high-quality HDR\nillumination estimates that significantly outperform previous state-of-the-art\nmethods. Consequently, using our illumination estimates for applications like\n3D object insertion, we can achieve results that are photo-realistic, which is\nvalidated via a perceptual user study.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 00:50:12 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 19:20:01 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 08:32:24 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gardner", "Marc-Andr\u00e9", ""], ["Sunkavalli", "Kalyan", ""], ["Yumer", "Ersin", ""], ["Shen", "Xiaohui", ""], ["Gambaretto", "Emiliano", ""], ["Gagn\u00e9", "Christian", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1704.00112", "submitter": "Yixin Zhu", "authors": "Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin,\n  Lap-Fai Yu, Demetri Terzopoulos, Song-Chun Zhu", "title": "Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel\n  Ground Truth using Stochastic Grammars", "comments": "Accepted in IJCV 2018", "journal-ref": null, "doi": "10.1007/s11263-018-1103-5", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a systematic learning-based approach to the generation of massive\nquantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D\nimages thereof, with associated ground truth information, for the purposes of\ntraining, benchmarking, and diagnosing learning-based computer vision and\nrobotics algorithms. In particular, we devise a learning-based pipeline of\nalgorithms capable of automatically generating and rendering a potentially\ninfinite variety of indoor scenes by using a stochastic grammar, represented as\nan attributed Spatial And-Or Graph, in conjunction with state-of-the-art\nphysics-based rendering. Our pipeline is capable of synthesizing scene layouts\nwith high diversity, and it is configurable inasmuch as it enables the precise\ncustomization and control of important attributes of the generated scenes. It\nrenders photorealistic RGB images of the generated scenes while automatically\nsynthesizing detailed, per-pixel ground truth data, including visible surface\ndepth and normal, object identity, and material information (detailed to object\nparts), as well as environments (e.g., illuminations and camera viewpoints). We\ndemonstrate the value of our synthesized dataset, by improving performance in\ncertain machine-learning-based scene understanding tasks--depth and surface\nnormal prediction, semantic segmentation, reconstruction, etc.--and by\nproviding benchmarks for and diagnostics of trained models by modifying object\nattributes and scene properties in a controllable manner.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 03:05:29 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 00:50:58 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 15:24:55 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Jiang", "Chenfanfu", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Huang", "Siyuan", ""], ["Lin", "Jenny", ""], ["Yu", "Lap-Fai", ""], ["Terzopoulos", "Demetri", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.00116", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, William B. Haskell, Vincent Y. F. Tan", "title": "Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration\n  Strategies", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2784360", "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the stochastic limited-memory BFGS (L-BFGS) algorithm. By\nproposing a new framework for the convergence analysis, we prove improved\nconvergence rates and computational complexities of the stochastic L-BFGS\nalgorithms compared to previous works. In addition, we propose several\npractical acceleration strategies to speed up the empirical performance of such\nalgorithms. We also provide theoretical analyses for most of the strategies.\nExperiments on large-scale logistic and ridge regression problems demonstrate\nthat our proposed strategies yield significant improvements vis-\\`a-vis\ncompeting state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 03:53:08 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 18:04:18 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 05:54:35 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhao", "Renbo", ""], ["Haskell", "William B.", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1704.00217", "submitter": "Zhiting Hu", "authors": "Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu, Eric P. Xing", "title": "Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 19:29:21 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qin", "Lianhui", ""], ["Zhang", "Zhisong", ""], ["Zhao", "Hai", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1704.00260", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Kevin Shih, Saurabh Singh, and Derek Hoiem", "title": "Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks", "comments": "Accepted in ICCV 2017. The arxiv version has an extra analysis on\n  correlation with human attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 08:01:30 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 05:34:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gupta", "Tanmay", ""], ["Shih", "Kevin", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1704.00367", "submitter": "U. N. Niranjan", "authors": "U.N. Niranjan, Arun Rajkumar, Theja Tulabandhula", "title": "Provable Inductive Robust PCA via Iterative Hard Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust PCA problem, wherein, given an input data matrix that is the\nsuperposition of a low-rank matrix and a sparse matrix, we aim to separate out\nthe low-rank and sparse components, is a well-studied problem in machine\nlearning. One natural question that arises is that, as in the inductive\nsetting, if features are provided as input as well, can we hope to do better?\nAnswering this in the affirmative, the main goal of this paper is to study the\nrobust PCA problem while incorporating feature information. In contrast to\nprevious works in which recovery guarantees are based on the convex relaxation\nof the problem, we propose a simple iterative algorithm based on\nhard-thresholding of appropriate residuals. Under weaker assumptions than\nprevious works, we prove the global convergence of our iterative procedure;\nmoreover, it admits a much faster convergence rate and lesser computational\ncomplexity per iteration. In practice, through systematic synthetic and real\ndata simulations, we confirm our theoretical findings regarding improvements\nobtained by using feature information.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 21:32:34 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 19:47:57 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Niranjan", "U. N.", ""], ["Rajkumar", "Arun", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1704.00387", "submitter": "Anatol Wegner", "authors": "Anatol E. Wegner, Luis Ospina-Forero, Robert E. Gaunt, Charlotte M.\n  Deane, Gesine Reinert", "title": "Identifying networks with common organizational principles", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex systems can be represented as networks, and the problem of\nnetwork comparison is becoming increasingly relevant. There are many techniques\nfor network comparison, from simply comparing network summary statistics to\nsophisticated but computationally costly alignment-based approaches. Yet it\nremains challenging to accurately cluster networks that are of a different size\nand density, but hypothesized to be structurally similar. In this paper, we\naddress this problem by introducing a new network comparison methodology that\nis aimed at identifying common organizational principles in networks. The\nmethodology is simple, intuitive and applicable in a wide variety of settings\nranging from the functional classification of proteins to tracking the\nevolution of a world trade network.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:32:14 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Wegner", "Anatol E.", ""], ["Ospina-Forero", "Luis", ""], ["Gaunt", "Robert E.", ""], ["Deane", "Charlotte M.", ""], ["Reinert", "Gesine", ""]]}, {"id": "1704.00514", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Anders S{\\o}gaard", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase boundary classification (KBC) is the task of detecting keyphrases\nin scientific articles and labelling them with respect to predefined types.\nAlthough important in practice, this task is so far underexplored, partly due\nto the lack of labelled data. To overcome this, we explore several auxiliary\ntasks, including semantic super-sense tagging and identification of multi-word\nexpressions, and cast the task as a multi-task learning problem with deep\nrecurrent neural networks. Our multi-task models perform significantly better\nthan previous state of the art approaches on two scientific KBC datasets,\nparticularly for long keyphrases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:25:22 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 16:48:49 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1704.00520", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael U. Gutmann, Arijus Pleska, Aki Vehtari,\n  Pekka Marttinen", "title": "Efficient acquisition rules for model-based approximate Bayesian\n  computation", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": "10.1214/18-BA1121", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, Bayesian optimisation (BO) and\nsurrogate models such as Gaussian processes have been proposed. Bayesian\noptimisation enables one to intelligently decide where to evaluate the model\nnext but common BO strategies are not designed for the goal of estimating the\nposterior distribution. Our paper addresses this gap in the literature. We\npropose to compute the uncertainty in the ABC posterior density, which is due\nto a lack of simulations to estimate this quantity accurately, and define a\nloss function that measures this uncertainty. We then propose to select the\nnext evaluation location to minimise the expected loss. Experiments show that\nthe proposed method often produces the most accurate approximations as compared\nto common BO strategies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:40:15 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 18:43:02 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 13:57:47 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael U.", ""], ["Pleska", "Arijus", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1704.00541", "submitter": "J\\'er\\'emy Emile Cohen", "authors": "J\\'er\\'emy E.Cohen and Nicolas Gillis", "title": "Dictionary-based Tensor Canonical Polyadic Decomposition", "comments": null, "journal-ref": "IEEE Trans. on Signal Processing 66 (7), pp. 1876-1889, 2018", "doi": "10.1109/TSP.2017.2777393", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure interpretability of extracted sources in tensor decomposition, we\nintroduce in this paper a dictionary-based tensor canonical polyadic\ndecomposition which enforces one factor to belong exactly to a known\ndictionary. A new formulation of sparse coding is proposed which enables high\ndimensional tensors dictionary-based canonical polyadic decomposition. The\nbenefits of using a dictionary in tensor decomposition models are explored both\nin terms of parameter identifiability and estimation accuracy. Performances of\nthe proposed algorithms are evaluated on the decomposition of simulated data\nand the unmixing of hyperspectral images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 12:03:39 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 10:10:47 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Cohen", "J\u00e9r\u00e9my E.", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1704.00607", "submitter": "Jalal Etesami", "authors": "Jalal Etesami, Kun Zhang, Negar Kiyavash", "title": "A New Measure of Conditional Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional dependencies among the variables of a network is of\ngreat interest to many disciplines. This paper studies some shortcomings of the\nexisting dependency measures in detecting direct causal influences or their\nlack of ability for group selection to capture strong dependencies and\naccordingly introduces a new statistical dependency measure to overcome them.\nThis measure is inspired by Dobrushin's coefficients and based on the fact that\nthere is no dependency between $X$ and $Y$ given another variable $Z$, if and\nonly if the conditional distribution of $Y$ given $X=x$ and $Z=z$ does not\nchange when $X$ takes another realization $x'$ while $Z$ takes the same\nrealization $z$. We show the advantages of this measure over the related\nmeasures in the literature. Moreover, we establish the connection between our\nmeasure and the integral probability metric (IPM) that helps to develop\nestimators of the measure with lower complexity compared to other relevant\ninformation theoretic based measures. Finally, we show the performance of this\nmeasure through numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 06:20:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 13:14:33 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Etesami", "Jalal", ""], ["Zhang", "Kun", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1704.00637", "submitter": "Lars Maal{\\o}e", "authors": "Lars Maal{\\o}e and Marco Fraccaro and Ole Winther", "title": "Semi-Supervised Generation with Cluster-aware Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models trained with large amounts of unlabelled data have\nproven to be powerful within the domain of unsupervised learning. Many real\nlife data sets contain a small amount of labelled data points, that are\ntypically disregarded when training generative models. We propose the\nCluster-aware Generative Model, that uses unlabelled information to infer a\nlatent representation that models the natural clustering of the data, and\nadditional labelled data points to refine this clustering. The generative\nperformances of the model significantly improve when labelled information is\nexploited, obtaining a log-likelihood of -79.38 nats on permutation invariant\nMNIST, while also achieving competitive semi-supervised classification\naccuracies. The model can also be trained fully unsupervised, and still improve\nthe log-likelihood performance with respect to related methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:25:47 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Maal\u00f8e", "Lars", ""], ["Fraccaro", "Marco", ""], ["Winther", "Ole", ""]]}, {"id": "1704.00708", "submitter": "Chi Jin", "authors": "Rong Ge, Chi Jin, Yi Zheng", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified\n  Geometric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new framework that captures the common landscape\nunderlying the common non-convex low-rank matrix problems including matrix\nsensing, matrix completion and robust PCA. In particular, we show for all above\nproblems (including asymmetric cases): 1) all local minima are also globally\noptimal; 2) no high-order saddle points exists. These results explain why\nsimple algorithms such as stochastic gradient descent have global converge, and\nefficiently optimize these non-convex objective functions in practice. Our\nframework connects and simplifies the existing analyses on optimization\nlandscapes for matrix sensing and symmetric matrix completion. The framework\nnaturally leads to new results for asymmetric matrix completion and robust PCA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:49:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Ge", "Rong", ""], ["Jin", "Chi", ""], ["Zheng", "Yi", ""]]}, {"id": "1704.00756", "submitter": "Romain Laroche", "authors": "Romain Laroche and Mehdi Fatemi and Joshua Romoff and Harm van Seijen", "title": "Multi-Advisor Reinforcement Learning", "comments": "Submitted at ICLR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tackling a single-agent RL problem by distributing it to $n$\nlearners. These learners, called advisors, endeavour to solve the problem from\na different focus. Their advice, taking the form of action values, is then\ncommunicated to an aggregator, which is in control of the system. We show that\nthe local planning method for the advisors is critical and that none of the\nones found in the literature is flawless: the egocentric planning overestimates\nvalues of states where the other advisors disagree, and the agnostic planning\nis inefficient around danger zones. We introduce a novel approach called\nempathic and discuss its theoretical aspects. We empirically examine and\nvalidate our theoretical findings on a fruit collection task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 18:37:12 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 20:35:27 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Laroche", "Romain", ""], ["Fatemi", "Mehdi", ""], ["Romoff", "Joshua", ""], ["van Seijen", "Harm", ""]]}, {"id": "1704.00767", "submitter": "Iain Carmichael", "authors": "Iain Carmichael and J.S. Marron", "title": "Geometric Insights into Support Vector Machine Behavior using the KKT\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is a powerful and widely used classification\nalgorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide\nrigorous mathematical proof for new insights into the behavior of SVM. These\ninsights provide perhaps unexpected relationships between SVM and two other\nlinear classifiers: the mean difference and the maximal data piling direction.\nFor example, we show that in many cases SVM can be viewed as a cropped version\nof these classifiers. By carefully exploring these connections we show how SVM\ntuning behavior is affected by characteristics including: balanced vs.\nunbalanced classes, low vs. high dimension, separable vs. non-separable data.\nThese results provide further insights into tuning SVM via cross-validation by\nexplaining observed pathological behavior and motivating improved\ncross-validation methodology. Finally, we also provide new results on the\ngeometry of complete data piling directions in high dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:08:54 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:43:27 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Carmichael", "Iain", ""], ["Marron", "J. S.", ""]]}, {"id": "1704.00773", "submitter": "Thomas Nedelec", "authors": "Thomas Nedelec, Nicolas Le Roux and Vianney Perchet", "title": "A comparative study of counterfactual estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comparative study of several widely used off-policy estimators\n(Empirical Average, Basic Importance Sampling and Normalized Importance\nSampling), detailing the different regimes where they are individually\nsuboptimal. We then exhibit properties optimal estimators should possess. In\nthe case where examples have been gathered using multiple policies, we show\nthat fused estimators dominate basic ones but can still be improved.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:16:06 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 08:00:07 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 13:58:22 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Nedelec", "Thomas", ""], ["Roux", "Nicolas Le", ""], ["Perchet", "Vianney", ""]]}, {"id": "1704.00794", "submitter": "Karl {\\O}yvind Mikalsen", "authors": "Karl {\\O}yvind Mikalsen, Filippo Maria Bianchi, Cristina Soguero-Ruiz\n  and Robert Jenssen", "title": "Time Series Cluster Kernel for Learning Similarities between\n  Multivariate Time Series with Missing Data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-based approaches represent a promising direction for time series\nanalysis. However, many such methods rely on parameter tuning, and some have\nshortcomings if the time series are multivariate (MTS), due to dependencies\nbetween attributes, or the time series contain missing data. In this paper, we\naddress these challenges within the powerful context of kernel methods by\nproposing the robust \\emph{time series cluster kernel} (TCK). The approach\ntaken leverages the missing data handling properties of Gaussian mixture models\n(GMM) augmented with informative prior distributions. An ensemble learning\napproach is exploited to ensure robustness to parameters by combining the\nclustering results of many GMM to form the final kernel.\n  We evaluate the TCK on synthetic and real data and compare to other\nstate-of-the-art techniques. The experimental results demonstrate that the TCK\nis robust to parameter choices, provides competitive results for MTS without\nmissing data and outstanding results for missing data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 20:16:58 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 12:23:24 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Mikalsen", "Karl \u00d8yvind", ""], ["Bianchi", "Filippo Maria", ""], ["Soguero-Ruiz", "Cristina", ""], ["Jenssen", "Robert", ""]]}, {"id": "1704.00828", "submitter": "Vin\\'icius Veloso de Melo", "authors": "L\\'eo Fran\\c{c}oso Dal Piccol Sotto and Vin\\'icius Veloso de Melo", "title": "A Probabilistic Linear Genetic Programming with Stochastic Context-Free\n  Grammar for solving Symbolic Regression problems", "comments": "Genetic and Evolutionary Computation Conference (GECCO) 2017, Berlin,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Linear Genetic Programming (LGP) algorithms are based only on the\nselection mechanism to guide the search. Genetic operators combine or mutate\nrandom portions of the individuals, without knowing if the result will lead to\na fitter individual. Probabilistic Model Building Genetic Programming (PMB-GP)\nmethods were proposed to overcome this issue through a probability model that\ncaptures the structure of the fit individuals and use it to sample new\nindividuals. This work proposes the use of LGP with a Stochastic Context-Free\nGrammar (SCFG), that has a probability distribution that is updated according\nto selected individuals. We proposed a method for adapting the grammar into the\nlinear representation of LGP. Tests performed with the proposed probabilistic\nmethod, and with two hybrid approaches, on several symbolic regression\nbenchmark problems show that the results are statistically better than the\nobtained by the traditional LGP.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 22:28:25 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Sotto", "L\u00e9o Fran\u00e7oso Dal Piccol", ""], ["de Melo", "Vin\u00edcius Veloso", ""]]}, {"id": "1704.00963", "submitter": "Eero Siivola", "authors": "Eero Siivola, Aki Vehtari, Jarno Vanhatalo, Javier Gonz\\'alez, Michael\n  Riis Andersen", "title": "Correcting boundary over-exploration deficiencies in Bayesian\n  optimization with virtual derivative sign observations", "comments": "6 pages, 7 figures", "journal-ref": "2018 IEEE 28th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a global optimization strategy designed to find\nthe minimum of an expensive black-box function, typically defined on a compact\nsubset of $\\mathcal{R}^d$, by using a Gaussian process (GP) as a surrogate\nmodel for the objective. Although currently available acquisition functions\naddress this goal with different degree of success, an over-exploration effect\nof the contour of the search space is typically observed. However, in problems\nlike the configuration of machine learning algorithms, the function domain is\nconservatively large and with a high probability the global minimum does not\nsit on the boundary of the domain. We propose a method to incorporate this\nknowledge into the search process by adding virtual derivative observations in\nthe \\gp at the boundary of the search space. We use the properties of GPs to\nimpose conditions on the partial derivatives of the objective. The method is\napplicable with any acquisition function, it is easy to use and consistently\nreduces the number of evaluations required to optimize the objective\nirrespective of the acquisition used. We illustrate the benefits of our\napproach in an extensive experimental comparison.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 11:40:20 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 14:50:39 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 11:49:01 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Siivola", "Eero", ""], ["Vehtari", "Aki", ""], ["Vanhatalo", "Jarno", ""], ["Gonz\u00e1lez", "Javier", ""], ["Andersen", "Michael Riis", ""]]}, {"id": "1704.00979", "submitter": "Artem Sevastopolsky", "authors": "Artem Sevastopolsky", "title": "Optic Disc and Cup Segmentation Methods for Glaucoma Detection with\n  Modification of U-Net Convolutional Neural Network", "comments": "accepted for publication in \"Pattern Recognition and Image Analysis:\n  Advances in Mathematical Theory and Applications\" journal, ISSN 1054-6618", "journal-ref": null, "doi": "10.1134/S1054661817030269", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is the second leading cause of blindness all over the world, with\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\ntime, glaucoma causes irreversible damage to the optic nerve leading to\nblindness. The optic nerve head examination, which involves measurement of\ncup-to-disc ratio, is considered one of the most valuable methods of structural\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\nof optic disc and optic cup on eye fundus images and can be performed by modern\ncomputer vision algorithms. This work presents universal approach for automatic\noptic disc and cup segmentation, which is based on deep learning, namely,\nmodification of U-Net convolutional neural network. Our experiments include\ncomparison with the best known methods on publicly available databases\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\nour method achieves quality comparable to current state-of-the-art methods,\noutperforming them in terms of the prediction time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 12:28:12 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sevastopolsky", "Artem", ""]]}, {"id": "1704.01041", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan, Roman Vershynin", "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component\n  Analysis: Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Non-Gaussian Component Analysis (NGCA) is about finding a\nmaximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points\nprojected onto $E$ follow a non-gaussian distribution. Although this is an\nappropriate model for some real world data analysis problems, there has been\nlittle progress on this problem over the last decade.\n  In this paper, we attempt to address this state of affairs in two ways.\nFirst, we give a new characterization of standard gaussian distributions in\nhigh-dimensions, which lead to effective tests for non-gaussianness. Second, we\npropose a simple algorithm, \\emph{Reweighted PCA}, as a method for solving the\nNGCA problem. We prove that for a general unknown non-gaussian distribution,\nthis algorithm recovers at least one direction in $E$, with sample and time\ncomplexity depending polynomially on the dimension of the ambient space. We\nconjecture that the algorithm actually recovers the entire $E$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:46:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Tan", "Yan Shuo", ""], ["Vershynin", "Roman", ""]]}, {"id": "1704.01079", "submitter": "Tuo Zhao", "authors": "Haotian Pang, Robert Vanderbei, Han Liu, Tuo Zhao", "title": "Homotopy Parametric Simplex Method for Sparse Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional sparse learning has imposed a great computational challenge\nto large scale data analysis. In this paper, we are interested in a broad class\nof sparse learning approaches formulated as linear programs parametrized by a\n{\\em regularization factor}, and solve them by the parametric simplex method\n(PSM). Our parametric simplex method offers significant advantages over other\ncompeting methods: (1) PSM naturally obtains the complete solution path for all\nvalues of the regularization parameter; (2) PSM provides a high precision dual\ncertificate stopping criterion; (3) PSM yields sparse solutions through very\nfew iterations, and the solution sparsity significantly reduces the\ncomputational cost per iteration. Particularly, we demonstrate the superiority\nof PSM over various sparse learning approaches, including Dantzig selector for\nsparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME\nfor sparse precision matrix estimation, sparse differential network estimation,\nand sparse Linear Programming Discriminant (LPD) analysis. We then provide\nsufficient conditions under which PSM always outputs sparse solutions such that\nits computational performance can be significantly boosted. Thorough numerical\nexperiments are provided to demonstrate the outstanding performance of the PSM\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:56:55 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 18:11:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Pang", "Haotian", ""], ["Vanderbei", "Robert", ""], ["Liu", "Han", ""], ["Zhao", "Tuo", ""]]}, {"id": "1704.01087", "submitter": "Feras Saad", "authors": "Feras Saad, Leonardo Casarsa, Vikash Mansinghka", "title": "Probabilistic Search for Structured Data via Probabilistic Programming\n  and Nonparametric Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases are widespread, yet extracting relevant data can be difficult.\nWithout substantial domain knowledge, multivariate search queries often return\nsparse or uninformative results. This paper introduces an approach for\nsearching structured data based on probabilistic programming and nonparametric\nBayes. Users specify queries in a probabilistic language that combines standard\nSQL database search operators with an information theoretic ranking function\ncalled predictive relevance. Predictive relevance can be calculated by a fast\nsparse matrix algorithm based on posterior samples from CrossCat, a\nnonparametric Bayesian model for high-dimensional, heterogeneously-typed data\ntables. The result is a flexible search technique that applies to a broad class\nof information retrieval problems, which we integrate into BayesDB, a\nprobabilistic programming platform for probabilistic data analysis. This paper\ndemonstrates applications to databases of US colleges, global macroeconomic\nindicators of public health, and classic cars. We found that human evaluators\noften prefer the results from probabilistic search to results from a standard\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 16:18:07 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Saad", "Feras", ""], ["Casarsa", "Leonardo", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1704.01168", "submitter": "Eric Nalisnick", "authors": "Eric Nalisnick, Padhraic Smyth", "title": "Learning Approximately Objective Priors", "comments": "UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informative Bayesian priors are often difficult to elicit, and when this is\nthe case, modelers usually turn to noninformative or objective priors. However,\nobjective priors such as the Jeffreys and reference priors are not tractable to\nderive for many models of interest. We address this issue by proposing\ntechniques for learning reference prior approximations: we select a parametric\nfamily and optimize a black-box lower bound on the reference prior objective to\nfind the member of the family that serves as a good approximation. We\nexperimentally demonstrate the method's effectiveness by recovering Jeffreys\npriors and learning the Variational Autoencoder's reference prior.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 20:07:26 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 04:23:54 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 18:53:05 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Nalisnick", "Eric", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1704.01184", "submitter": "Chao Lan", "authors": "Chao Lan, Sai Nivedita Chandrasekaran, Jun Huan", "title": "On the Unreported-Profile-is-Negative Assumption for Predictive\n  Cheminformatics", "comments": "the quality of the current version is unsatisfactory. we decide to\n  withdraw the manuscript. thank you", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cheminformatics, compound-target binding profiles has been a main source\nof data for research. For data repositories that only provide positive\nprofiles, a popular assumption is that unreported profiles are all negative. In\nthis paper, we caution audience not to take this assumption for granted, and\npresent empirical evidence of its ineffectiveness from a machine learning\nperspective. Our examination is based on a setting where binding profiles are\nused as features to train predictive models; we show (1) prediction performance\ndegrades when the assumption fails and (2) explicit recovery of unreported\nprofiles improves prediction performance. In particular, we propose a framework\nthat jointly recovers profiles and learns predictive model, and show it\nachieves further performance improvement. The presented study not only suggests\napplying matrix recovery methods to recover unreported profiles, but also\ninitiates a new missing feature problem which we called Learning with Positive\nand Unknown Features.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:33:10 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 14:48:57 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 19:31:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lan", "Chao", ""], ["Chandrasekaran", "Sai Nivedita", ""], ["Huan", "Jun", ""]]}, {"id": "1704.01223", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Alejandro Ribeiro", "title": "Greedy Sampling of Graph Signals", "comments": "14 pages, 14 figures. Accepted for publication on IEEE Transactions\n  on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2755586", "report-no": null, "categories": "cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is a fundamental topic in graph signal processing, having found\napplications in estimation, clustering, and video compression. In contrast to\ntraditional signal processing, the irregularity of the signal domain makes\nselecting a sampling set non-trivial and hard to analyze. Indeed, though\nconditions for graph signal interpolation from noiseless samples exist, they do\nnot lead to a unique sampling set. The presence of noise makes choosing among\nthese sampling sets a hard combinatorial problem. Although greedy sampling\nschemes are commonly used in practice, they have no performance guarantee. This\nwork takes a twofold approach to address this issue. First, universal\nperformance bounds are derived for the Bayesian estimation of graph signals\nfrom noisy samples. In contrast to currently available bounds, they are not\nrestricted to specific sampling schemes and hold for any sampling sets. Second,\nthis paper provides near-optimal guarantees for greedy sampling by introducing\nthe concept of approximate submodularity and updating the classical greedy\nbound. It then provides explicit bounds on the approximate supermodularity of\nthe interpolation mean-square error showing that it can be optimized with\nworst-case guarantees using greedy search even though it is not supermodular.\nSimulations illustrate the derived bound for different graph models and show an\napplication of graph signal sampling to reduce the complexity of kernel\nprincipal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 00:09:37 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 19:48:21 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1704.01255", "submitter": "Maithra Raghu", "authors": "Ravi Kumar, Maithra Raghu, Tamas Sarlos, Andrew Tomkins", "title": "Linear Additive Markov Processes", "comments": "Accepted to WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP\nmay be influenced by states visited in the distant history of the process, but\nunlike higher-order Markov processes, LAMP retains an efficient\nparametrization. LAMP also allows the specific dependence on history to be\nlearned efficiently from data. We characterize some theoretical properties of\nLAMP, including its steady-state and mixing time. We then give an algorithm\nbased on alternating minimization to learn LAMP models from data. Finally, we\nperform a series of real-world experiments to show that LAMP is more powerful\nthan first-order Markov processes, and even holds its own against deep\nsequential models (LSTMs) with a negligible increase in parameter complexity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:26:41 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Kumar", "Ravi", ""], ["Raghu", "Maithra", ""], ["Sarlos", "Tamas", ""], ["Tomkins", "Andrew", ""]]}, {"id": "1704.01280", "submitter": "Li-Chia Yang", "authors": "Li-Chia Yang, Szu-Yu Chou, Jen-Yu Liu, Yi-Hsuan Yang, Yi-An Chen", "title": "Revisiting the problem of audio-based hit song prediction using\n  convolutional neural networks", "comments": "To appear in the proceedings of 2017 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict whether a song can be a hit has impor- tant\napplications in the music industry. Although it is true that the popularity of\na song can be greatly affected by exter- nal factors such as social and\ncommercial influences, to which degree audio features computed from musical\nsignals (whom we regard as internal factors) can predict song popularity is an\ninteresting research question on its own. Motivated by the recent success of\ndeep learning techniques, we attempt to ex- tend previous work on hit song\nprediction by jointly learning the audio features and prediction models using\ndeep learning. Specifically, we experiment with a convolutional neural net-\nwork model that takes the primitive mel-spectrogram as the input for feature\nlearning, a more advanced JYnet model that uses an external song dataset for\nsupervised pre-training and auto-tagging, and the combination of these two\nmodels. We also consider the inception model to characterize audio infor-\nmation in different scales. Our experiments suggest that deep structures are\nindeed more accurate than shallow structures in predicting the popularity of\neither Chinese or Western Pop songs in Taiwan. We also use the tags predicted\nby JYnet to gain insights into the result of different models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 06:39:51 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Yang", "Li-Chia", ""], ["Chou", "Szu-Yu", ""], ["Liu", "Jen-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Chen", "Yi-An", ""]]}, {"id": "1704.01312", "submitter": "Pirmin Lemberger", "authors": "Pirmin Lemberger", "title": "On Generalization and Regularization in Deep Learning", "comments": "11 pages, 3 figures pedagogical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do large neural network generalize so well on complex tasks such as image\nclassification or speech recognition? What exactly is the role regularization\nfor them? These are arguably among the most important open questions in machine\nlearning today. In a recent and thought provoking paper [C. Zhang et al.]\nseveral authors performed a number of numerical experiments that hint at the\nneed for novel theoretical concepts to account for this phenomenon. The paper\nstirred quit a lot of excitement among the machine learning community but at\nthe same time it created some confusion as discussions on OpenReview.net\ntestifies. The aim of this pedagogical paper is to make this debate accessible\nto a wider audience of data scientists without advanced theoretical knowledge\nin statistical learning. The focus here is on explicit mathematical definitions\nand on a discussion of relevant concepts, not on proofs for which we provide\nreferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:48:01 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 19:58:27 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Lemberger", "Pirmin", ""]]}, {"id": "1704.01382", "submitter": "Adrian Wills", "authors": "Adrian G. Wills and Thomas B. Sch\\\"on", "title": "On the construction of probabilistic Newton-type algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that many of the existing quasi-Newton algorithms\ncan be formulated as learning algorithms, capable of learning local models of\nthe cost functions. Importantly, this understanding allows us to safely start\nassembling probabilistic Newton-type algorithms, applicable in situations where\nwe only have access to noisy observations of the cost function and its\nderivatives. This is where our interest lies.\n  We make contributions to the use of the non-parametric and probabilistic\nGaussian process models in solving these stochastic optimisation problems.\nSpecifically, we present a new algorithm that unites these approximations\ntogether with recent probabilistic line search routines to deliver a\nprobabilistic quasi-Newton approach.\n  We also show that the probabilistic optimisation algorithms deliver promising\nresults on challenging nonlinear system identification problems where the very\nnature of the problem is such that we can only access the cost function and its\nderivative via noisy observations, since there are no closed-form expressions\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 12:34:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Wills", "Adrian G.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1704.01427", "submitter": "Andres Masegosa R", "authors": "Andr\\'es R. Masegosa, Ana M. Mart\\'inez, Dar\\'io Ramos-L\\'opez, Rafael\n  Caba\\~nas, Antonio Salmer\\'on, Thomas D. Nielsen, Helge Langseth, Anders L.\n  Madsen", "title": "AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AMIDST Toolbox is a software for scalable probabilistic machine learning\nwith a spe- cial focus on (massive) streaming data. The toolbox supports a\nflexible modeling language based on probabilistic graphical models with latent\nvariables and temporal dependencies. The specified models can be learnt from\nlarge data sets using parallel or distributed implementa- tions of Bayesian\nlearning algorithms for either streaming or batch data. These algorithms are\nbased on a flexible variational message passing scheme, which supports discrete\nand continu- ous variables from a wide range of probability distributions.\nAMIDST also leverages existing functionality and algorithms by interfacing to\nsoftware tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open\nsource toolbox written in Java and available at http://www.amidsttoolbox.com\nunder the Apache Software License version 2.0.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 11:58:21 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Masegosa", "Andr\u00e9s R.", ""], ["Mart\u00ednez", "Ana M.", ""], ["Ramos-L\u00f3pez", "Dar\u00edo", ""], ["Caba\u00f1as", "Rafael", ""], ["Salmer\u00f3n", "Antonio", ""], ["Nielsen", "Thomas D.", ""], ["Langseth", "Helge", ""], ["Madsen", "Anders L.", ""]]}, {"id": "1704.01430", "submitter": "Dominik Janzing", "authors": "Dominik Janzing and Bernhard Schoelkopf", "title": "Detecting confounding in multivariate linear models via spectral\n  analysis", "comments": "27 pages, 16 figures", "journal-ref": "Journal of Causal Inference, 2017", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a model where one target variable Y is correlated with a vector\nX:=(X_1,...,X_d) of predictor variables being potential causes of Y. We\ndescribe a method that infers to what extent the statistical dependences\nbetween X and Y are due to the influence of X on Y and to what extent due to a\nhidden common cause (confounder) of X and Y. The method relies on concentration\nof measure results for large dimensions d and an independence assumption\nstating that, in the absence of confounding, the vector of regression\ncoefficients describing the influence of each X on Y typically has `generic\norientation' relative to the eigenspaces of the covariance matrix of X. For the\nspecial case of a scalar confounder we show that confounding typically spoils\nthis generic orientation in a characteristic way that can be used to\nquantitatively estimate the amount of confounding.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:54:29 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Janzing", "Dominik", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1704.01445", "submitter": "Jack Fitzsimons", "authors": "Jack Fitzsimons, Kurt Cutajar, Michael Osborne, Stephen Roberts,\n  Maurizio Filippone", "title": "Bayesian Inference of Log Determinants", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-determinant of a kernel matrix appears in a variety of machine\nlearning problems, ranging from determinantal point processes and generalized\nMarkov random fields, through to the training of Gaussian processes. Exact\ncalculation of this term is often intractable when the size of the kernel\nmatrix exceeds a few thousand. In the spirit of probabilistic numerics, we\nreinterpret the problem of computing the log-determinant as a Bayesian\ninference problem. In particular, we combine prior knowledge in the form of\nbounds from matrix theory and evidence derived from stochastic trace estimation\nto obtain probabilistic estimates for the log-determinant and its associated\nuncertainty within a given computational budget. Beyond its novelty and\ntheoretic appeal, the performance of our proposal is competitive with\nstate-of-the-art approaches to approximating the log-determinant, while also\nquantifying the uncertainty due to budget-constrained evidence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:23:53 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Fitzsimons", "Jack", ""], ["Cutajar", "Kurt", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1704.01460", "submitter": "Siavash Haghiri", "authors": "Siavash Haghiri, Debarghya Ghoshdastidar and Ulrike von Luxburg", "title": "Comparison Based Nearest Neighbor Search", "comments": "16 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine learning in a comparison-based setting where we are given\na set of points in a metric space, but we have no access to the actual\ndistances between the points. Instead, we can only ask an oracle whether the\ndistance between two points $i$ and $j$ is smaller than the distance between\nthe points $i$ and $k$. We are concerned with data structures and algorithms to\nfind nearest neighbors based on such comparisons. We focus on a simple yet\neffective algorithm that recursively splits the space by first selecting two\nrandom pivot points and then assigning all other points to the closer of the\ntwo (comparison tree). We prove that if the metric space satisfies certain\nexpansion conditions, then with high probability the height of the comparison\ntree is logarithmic in the number of points, leading to efficient search\nperformance. We also provide an upper bound for the failure probability to\nreturn the true nearest neighbor. Experiments show that the comparison tree is\ncompetitive with algorithms that have access to the actual distance values, and\nneeds less triplet comparisons than other competitors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:54:28 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Haghiri", "Siavash", ""], ["Ghoshdastidar", "Debarghya", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1704.01474", "submitter": "Kai Chen", "authors": "Kai Chen and Mathias Seuret", "title": "Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Convolutional Neural Network (CNN) based page\nsegmentation method for handwritten historical document images. We consider\npage segmentation as a pixel labeling problem, i.e., each pixel is classified\nas one of the predefined classes. Traditional methods in this area rely on\ncarefully hand-crafted features or large amounts of prior knowledge. In\ncontrast, we propose to learn features from raw image pixels using a CNN. While\nmany researchers focus on developing deep CNN architectures to solve different\nproblems, we train a simple CNN with only one convolution layer. We show that\nthe simple architecture achieves competitive results against other deep\narchitectures on different public datasets. Experiments also demonstrate the\neffectiveness and superiority of the proposed method compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 15:12:25 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 10:16:49 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Chen", "Kai", ""], ["Seuret", "Mathias", ""]]}, {"id": "1704.01523", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\n  Neural Networks", "comments": "Accepted at SemEval 2017. The first two authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 50 million scholarly articles have been published: they constitute a\nunique repository of knowledge. In particular, one may infer from them\nrelations between scientific concepts, such as synonyms and hyponyms.\nArtificial neural networks have been recently explored for relation extraction.\nIn this work, we continue this line of work and present a system based on a\nconvolutional neural network to extract relations. Our model ranked first in\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\narticles (subtask C).\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:54:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1704.01547", "submitter": "Wieland Brendel", "authors": "Wieland Brendel, Matthias Bethge", "title": "Comment on \"Biologically inspired protection of deep networks from\n  adversarial attacks\"", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper suggests that Deep Neural Networks can be protected from\ngradient-based adversarial perturbations by driving the network activations\ninto a highly saturated regime. Here we analyse such saturated networks and\nshow that the attacks fail due to numerical limitations in the gradient\ncomputations. A simple stabilisation of the gradient estimates enables\nsuccessful and efficient attacks. Thus, it has yet to be shown that the\nrobustness observed in highly saturated networks is not simply due to numerical\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:47:25 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1704.01574", "submitter": "Kevin Amaral", "authors": "Kevin M. Amaral, Ping Chen, Scott Crouter, Wei Ding", "title": "Bag-of-Words Method Applied to Accelerometer Measurements for the\n  Purpose of Classification and Energy Estimation", "comments": "10 pages, 6 tables, 2 figures. This paper has been withdrawn due to a\n  few crucial errors in the early sections of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerometer measurements are the prime type of sensor information most\nthink of when seeking to measure physical activity. On the market, there are\nmany fitness measuring devices which aim to track calories burned and steps\ncounted through the use of accelerometers. These measurements, though good\nenough for the average consumer, are noisy and unreliable in terms of the\nprecision of measurement needed in a scientific setting. The contribution of\nthis paper is an innovative and highly accurate regression method which uses an\nintermediary two-stage classification step to better direct the regression of\nenergy expenditure values from accelerometer counts.\n  We show that through an additional unsupervised layer of intermediate feature\nconstruction, we can leverage latent patterns within accelerometer counts to\nprovide better grounds for activity classification than expert-constructed\ntimeseries features. For this, our approach utilizes a mathematical model\noriginating in natural language processing, the bag-of-words model, that has in\nthe past years been appearing in diverse disciplines outside of the natural\nlanguage processing field such as image processing. Further emphasizing the\nnatural language connection to stochastics, we use a gaussian mixture model to\nlearn the dictionary upon which the bag-of-words model is built. Moreover, we\nshow that with the addition of these features, we're able to improve regression\nroot mean-squared error of energy expenditure by approximately 1.4 units over\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:21:26 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 23:01:36 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Amaral", "Kevin M.", ""], ["Chen", "Ping", ""], ["Crouter", "Scott", ""], ["Ding", "Wei", ""]]}, {"id": "1704.01605", "submitter": "Daniel O'Malley", "authors": "Daniel O'Malley, Velimir V. Vesselinov, Boian S. Alexandrov, Ludmil B.\n  Alexandrov", "title": "Nonnegative/binary matrix factorization with a D-Wave quantum annealer", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0206653", "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  D-Wave quantum annealers represent a novel computational architecture and\nhave attracted significant interest, but have been used for few real-world\ncomputations. Machine learning has been identified as an area where quantum\nannealing may be useful. Here, we show that the D-Wave 2X can be effectively\nused as part of an unsupervised machine learning method. This method can be\nused to analyze large datasets. The D-Wave only limits the number of features\nthat can be extracted from the dataset. We apply this method to learn the\nfeatures from a set of facial images.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:49:56 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["O'Malley", "Daniel", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""], ["Alexandrov", "Ludmil B.", ""]]}, {"id": "1704.01664", "submitter": "Cheng Ju", "authors": "Cheng Ju and Aur\\'elien Bibaut and Mark J. van der Laan", "title": "The Relative Performance of Ensemble Methods with Deep Convolutional\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have been successfully applied to a variety of\nmachine learning tasks, including image recognition, semantic segmentation, and\nmachine translation. However, few studies fully investigated ensembles of\nartificial neural networks. In this work, we investigated multiple widely used\nensemble methods, including unweighted averaging, majority voting, the Bayes\nOptimal Classifier, and the (discrete) Super Learner, for image recognition\ntasks, with deep neural networks as candidate algorithms. We designed several\nexperiments, with the candidate algorithms being the same network structure\nwith different model checkpoints within a single training process, networks\nwith same structure but trained multiple times stochastically, and networks\nwith different structure. In addition, we further studied the over-confidence\nphenomenon of the neural networks, as well as its impact on the ensemble\nmethods. Across all of our experiments, the Super Learner achieved best\nperformance among all the ensemble methods in this study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:04:43 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Ju", "Cheng", ""], ["Bibaut", "Aur\u00e9lien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1704.01665", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song", "title": "Learning Combinatorial Optimization Algorithms over Graphs", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of good heuristics or approximation algorithms for NP-hard\ncombinatorial optimization problems often requires significant specialized\nknowledge and trial-and-error. Can we automate this challenging, tedious\nprocess, and learn the algorithms instead? In many real-world applications, it\nis typically the case that the same optimization problem is solved again and\nagain on a regular basis, maintaining the same problem structure but differing\nin the data. This provides an opportunity for learning heuristic algorithms\nthat exploit the structure of such recurring problems. In this paper, we\npropose a unique combination of reinforcement learning and graph embedding to\naddress this challenge. The learned greedy policy behaves like a meta-algorithm\nthat incrementally constructs a solution, and the action is determined by the\noutput of a graph embedding network capturing the current state of the\nsolution. We show that our framework can be applied to a diverse range of\noptimization problems over graphs, and learns effective algorithms for the\nMinimum Vertex Cover, Maximum Cut and Traveling Salesman problems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:08:07 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 22:08:37 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 23:12:54 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 19:47:20 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Dai", "Hanjun", ""], ["Khalil", "Elias B.", ""], ["Zhang", "Yuyu", ""], ["Dilkina", "Bistra", ""], ["Song", "Le", ""]]}, {"id": "1704.01700", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury", "title": "Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an L-BFGS optimization algorithm on Riemannian manifolds using\nminibatched stochastic variance reduction techniques for fast convergence with\nconstant step sizes, without resorting to linesearch methods designed to\nsatisfy Wolfe conditions. We provide a new convergence proof for strongly\nconvex functions without using curvature conditions on the manifold, as well as\na convergence discussion for nonconvex functions. We discuss a couple of ways\nto obtain the correction pairs used to calculate the product of the gradient\nwith the inverse Hessian, and empirically demonstrate their use in synthetic\nexperiments on computation of Karcher means for symmetric positive definite\nmatrices and leading eigenvalues of large scale data matrices. We compare our\nmethod to VR-PCA for the latter experiment, along with Riemannian SVRG for both\ncases, and show strong convergence results for a range of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 03:34:29 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 22:02:30 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 15:02:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Roychowdhury", "Anirban", ""]]}, {"id": "1704.01701", "submitter": "Elaine Angelino", "authors": "Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer,\n  Cynthia Rudin", "title": "Learning Certifiably Optimal Rule Lists for Categorical Data", "comments": "A short version of this work appeared in KDD '17 as \"Learning\n  Certifiably Optimal Rule Lists\"", "journal-ref": "Journal of Machine Learning Research 18(234):1-78, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and implementation of a custom discrete optimization\ntechnique for building rule lists over a categorical feature space. Our\nalgorithm produces rule lists with optimal training performance, according to\nthe regularized empirical risk, with a certificate of optimality. By leveraging\nalgorithmic bounds, efficient data structures, and computational reuse, we\nachieve several orders of magnitude speedup in time and a massive reduction of\nmemory consumption. We demonstrate that our approach produces optimal rule\nlists on practical problems in seconds. Our results indicate that it is\npossible to construct optimal sparse rule lists that are approximately as\naccurate as the COMPAS proprietary risk prediction tool on data from Broward\nCounty, Florida, but that are completely interpretable. This framework is a\nnovel alternative to CART and other decision tree methods for interpretable\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:02:35 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 19:00:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 17:49:44 GMT"}, {"version": "v4", "created": "Fri, 3 Aug 2018 22:51:24 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Angelino", "Elaine", ""], ["Larus-Stone", "Nicholas", ""], ["Alabi", "Daniel", ""], ["Seltzer", "Margo", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1704.01704", "submitter": "Yi Han", "authors": "Yi Han, Benjamin I. P. Rubinstein", "title": "Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks", "comments": "10 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the wide use of machine learning in adversarial settings including\ncomputer security, recent studies have demonstrated vulnerabilities to evasion\nattacks---carefully crafted adversarial samples that closely resemble\nlegitimate instances, but cause misclassification. In this paper, we examine\nthe adequacy of the leading approach to generating adversarial samples---the\ngradient descent approach. In particular (1) we perform extensive experiments\non three datasets, MNIST, USPS and Spambase, in order to analyse the\neffectiveness of the gradient-descent method against non-linear support vector\nmachines, and conclude that carefully reduced kernel smoothness can\nsignificantly increase robustness to the attack; (2) we demonstrate that\nseparated inter-class support vectors lead to more secure models, and propose a\nquantity similar to margin that can efficiently predict potential\nsusceptibility to gradient-descent attacks, before the attack is launched; and\n(3) we design a new adversarial sample construction algorithm based on\noptimising the multiplicative ratio of class decision functions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 04:35:40 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 04:32:43 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Han", "Yi", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1704.01858", "submitter": "Nicholas Monath", "authors": "Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, Andrew McCallum", "title": "An Online Hierarchical Algorithm for Extreme Clustering", "comments": "20 pages. Code available here: https://github.com/iesl/xcluster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern clustering methods scale well to a large number of data items, N,\nbut not to a large number of clusters, K. This paper introduces PERCH, a new\nnon-greedy algorithm for online hierarchical clustering that scales to both\nmassive N and K--a problem setting we term extreme clustering. Our algorithm\nefficiently routes new data points to the leaves of an incrementally-built\ntree. Motivated by the desire for both accuracy and speed, our approach\nperforms tree rotations for the sake of enhancing subtree purity and\nencouraging balancedness. We prove that, under a natural separability\nassumption, our non-greedy algorithm will produce trees with perfect dendrogram\npurity regardless of online data arrival order. Our experiments demonstrate\nthat PERCH constructs more accurate trees than other tree-building clustering\nalgorithms and scales well with both N and K, achieving a higher quality\nclustering than the strongest flat clustering competitor in nearly half the\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:29:10 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Kobren", "Ari", ""], ["Monath", "Nicholas", ""], ["Krishnamurthy", "Akshay", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.01864", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen, Tom Heskes", "title": "Robust Causal Estimation in the Large-Sample Limit without Strict\n  Faithfulness", "comments": "10 pages, 12 figures, Proceedings of the 20th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2017", "journal-ref": "PMLR 54:1523-1531, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal effect estimation from observational data is an important and much\nstudied research topic. The instrumental variable (IV) and local causal\ndiscovery (LCD) patterns are canonical examples of settings where a closed-form\nexpression exists for the causal effect of one variable on another, given the\npresence of a third variable. Both rely on faithfulness to infer that the\nlatter only influences the target effect via the cause variable. In reality, it\nis likely that this assumption only holds approximately and that there will be\nat least some form of weak interaction. This brings about the paradoxical\nsituation that, in the large-sample limit, no predictions are made, as\ndetecting the weak edge invalidates the setting. We introduce an alternative\napproach by replacing strict faithfulness with a prior that reflects the\nexistence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a\nposterior distribution over the target causal effect estimator which shows\nthat, in many cases, we can still make good estimates. We demonstrate the\napproach in an application on a simple linear-Gaussian setting, using the\nMultiNest sampling algorithm, and compare it with established techniques to\nshow our method is robust even when strict faithfulness is violated.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:39:54 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "1704.01871", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Massive Data Clustering in Moderate Dimensions from the Dual Spaces of\n  Observation and Attribute Data Clouds", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis of very high dimensional data can benefit from the\nproperties of such high dimensionality. Informally expressed, in this work, our\nfocus is on the analogous situation when the dimensionality is moderate to\nsmall, relative to a massively sized set of observations. Mathematically\nexpressed, these are the dual spaces of observations and attributes. The point\ncloud of observations is in attribute space, and the point cloud of attributes\nis in observation space. In this paper, we begin by summarizing various\nperspectives related to methodologies that are used in multivariate analytics.\nWe draw on these to establish an efficient clustering processing pipeline, both\npartitioning and hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:48:44 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1704.01896", "submitter": "Yixi Xu", "authors": "Yixi Xu, Jean Honorio, Xiao Wang", "title": "On the Statistical Efficiency of Compositional Nonparametric Prediction", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS), 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a compositional nonparametric method in which a\nmodel is expressed as a labeled binary tree of $2k+1$ nodes, where each node is\neither a summation, a multiplication, or the application of one of the $q$\nbasis functions to one of the $p$ covariates. We show that in order to recover\na labeled binary tree from a given dataset, the sufficient number of samples is\n$O(k\\log(pq)+\\log(k!))$, and the necessary number of samples is $\\Omega(k\\log\n(pq)-\\log(k!))$. We further propose a greedy algorithm for regression in order\nto validate our theoretical findings through synthetic experiments.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:43:08 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 01:46:35 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 15:28:26 GMT"}, {"version": "v4", "created": "Fri, 20 Oct 2017 00:49:39 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Yixi", ""], ["Honorio", "Jean", ""], ["Wang", "Xiao", ""]]}, {"id": "1704.01918", "submitter": "Hassan Naseri", "authors": "Hassan Naseri, Visa Koivunen", "title": "A Bayesian algorithm for distributed network localization using distance\n  and direction data", "comments": "Notice: This work has been submitted to the IEEE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable, accurate, and affordable positioning service is highly required\nin wireless networks. In this paper, the novel Message Passing Hybrid\nLocalization (MPHL) algorithm is proposed to solve the problem of cooperative\ndistributed localization using distance and direction estimates. This hybrid\napproach combines two sensing modalities to reduce the uncertainty in\nlocalizing the network nodes. A statistical model is formulated for the\nproblem, and approximate minimum mean square error (MMSE) estimates of the node\nlocations are computed. The proposed MPHL is a distributed algorithm based on\nbelief propagation (BP) and Markov chain Monte Carlo (MCMC) sampling. It\nimproves the identifiability of the localization problem and reduces its\nsensitivity to the anchor node geometry, compared to distance-only or\ndirection-only localization techniques. For example, the unknown location of a\nnode can be found if it has only a single neighbor; and a whole network can be\nlocalized using only a single anchor node. Numerical results are presented\nshowing that the average localization error is significantly reduced in almost\nevery simulation scenario, about 50% in most cases, compared to the competing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:32:50 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 16:19:20 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Naseri", "Hassan", ""], ["Koivunen", "Visa", ""]]}, {"id": "1704.01920", "submitter": "Rahaf Aljundi", "authors": "Amal Rannen Triki, Rahaf Aljundi, Mathew B. Blaschko and Tinne\n  Tuytelaars", "title": "Encoder Based Lifelong Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2017.148", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new lifelong learning solution where a single model\nis trained for a sequence of tasks. The main challenge that vision systems face\nin this context is catastrophic forgetting: as they tend to adapt to the most\nrecently seen task, they lose performance on the tasks that were learned\npreviously. Our method aims at preserving the knowledge of the previous tasks\nwhile learning a new one by using autoencoders. For each task, an\nunder-complete autoencoder is learned, capturing the features that are crucial\nfor its achievement. When a new task is presented to the system, we prevent the\nreconstructions of the features with these autoencoders from changing, which\nhas the effect of preserving the information on which the previous tasks are\nmainly relying. At the same time, the features are given space to adjust to the\nmost recent environment as only their projection into a low dimension\nsubmanifold is controlled. The proposed system is evaluated on image\nclassification tasks and shows a reduction of forgetting over the\nstate-of-the-art\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:37:15 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Triki", "Amal Rannen", ""], ["Aljundi", "Rahaf", ""], ["Blaschko", "Mathew B.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1704.01942", "submitter": "Minsuk Kahng", "authors": "Minsuk Kahng, Pierre Y. Andrews, Aditya Kalro, Duen Horng Chau", "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models", "comments": "Will be presented at IEEE VAST 2017 and published in IEEE\n  Transactions on Visualization and Computer Graphics, 24(1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models have achieved state-of-the-art accuracies for many\nprediction tasks, understanding these models remains a challenge. Despite the\nrecent interest in developing visual tools to help users interpret deep\nlearning models, the complexity and wide variety of models deployed in\nindustry, and the large-scale datasets that they used, pose unique design\nchallenges that are inadequately addressed by existing work. Through\nparticipatory design sessions with over 15 researchers and engineers at\nFacebook, we have developed, deployed, and iteratively improved ActiVis, an\ninteractive visualization system for interpreting large-scale deep learning\nmodels and results. By tightly integrating multiple coordinated views, such as\na computation graph overview of the model architecture, and a neuron activation\nview for pattern discovery and comparison, users can explore complex deep\nneural network models at both the instance- and subset-level. ActiVis has been\ndeployed on Facebook's machine learning platform. We present case studies with\nFacebook researchers and engineers, and usage scenarios of how ActiVis may work\nwith different models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 17:18:02 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 00:22:56 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kahng", "Minsuk", ""], ["Andrews", "Pierre Y.", ""], ["Kalro", "Aditya", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1704.01975", "submitter": "Daniela Moctezuma", "authors": "Eric S. Tellez, Daniela Moctezuma, Sabino Miranda-J\\'imenez, Mario\n  Graff", "title": "An Automated Text Categorization Framework based on Hyperparameter\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great variety of text tasks such as topic or spam identification, user\nprofiling, and sentiment analysis can be posed as a supervised learning problem\nand tackle using a text classifier. A text classifier consists of several\nsubprocesses, some of them are general enough to be applied to any supervised\nlearning problem, whereas others are specifically designed to tackle a\nparticular task, using complex and computational expensive processes such as\nlemmatization, syntactic analysis, etc. Contrary to traditional approaches, we\npropose a minimalistic and wide system able to tackle text classification tasks\nindependent of domain and language, namely microTC. It is composed by some easy\nto implement text transformations, text representations, and a supervised\nlearning algorithm. These pieces produce a competitive classifier even in the\ndomain of informally written text. We provide a detailed description of microTC\nalong with an extensive experimental comparison with relevant state-of-the-art\nmethods. mircoTC was compared on 30 different datasets. Regarding accuracy,\nmicroTC obtained the best performance in 20 datasets while achieves competitive\nresults in the remaining 10. The compared datasets include several problems\nlike topic and polarity classification, spam detection, user profiling and\nauthorship attribution. Furthermore, it is important to state that our approach\nallows the usage of the technology even without knowledge of machine learning\nand natural language processing.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 18:01:22 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 22:30:13 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Tellez", "Eric S.", ""], ["Moctezuma", "Daniela", ""], ["Miranda-J\u00edmenez", "Sabino", ""], ["Graff", "Mario", ""]]}, {"id": "1704.02007", "submitter": "We Chen", "authors": "Zhe Sun, Ting Wang, Ke Deng, Xiao-Feng Wang, Robert Lafyatis, Ying\n  Ding, Ming Hu, Wei Chen", "title": "DIMM-SC: A Dirichlet mixture model for clustering droplet-based single\n  cell transcriptomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Single cell transcriptome sequencing (scRNA-Seq) has become a\nrevolutionary tool to study cellular and molecular processes at single cell\nresolution. Among existing technologies, the recently developed droplet-based\nplatform enables efficient parallel processing of thousands of single cells\nwith direct counting of transcript copies using Unique Molecular Identifier\n(UMI). Despite the technology advances, statistical methods and computational\ntools are still lacking for analyzing droplet-based scRNA-Seq data.\nParticularly, model-based approaches for clustering large-scale single cell\ntranscriptomic data are still under-explored. Methods: We developed DIMM-SC, a\nDirichlet Mixture Model for clustering droplet-based Single Cell transcriptomic\ndata. This approach explicitly models UMI count data from scRNA-Seq experiments\nand characterizes variations across different cell clusters via a Dirichlet\nmixture prior. An expectation-maximization algorithm is used for parameter\ninference. Results: We performed comprehensive simulations to evaluate DIMM-SC\nand compared it with existing clustering methods such as K-means, CellTree and\nSeurat. In addition, we analyzed public scRNA-Seq datasets with known cluster\nlabels and in-house scRNA-Seq datasets from a study of systemic sclerosis with\nprior biological knowledge to benchmark and validate DIMM-SC. Both simulation\nstudies and real data applications demonstrated that overall, DIMM-SC achieves\nsubstantially improved clustering accuracy and much lower clustering\nvariability compared to other existing clustering methods. More importantly, as\na model-based approach, DIMM-SC is able to quantify the clustering uncertainty\nfor each single cell, facilitating rigorous statistical inference and\nbiological interpretations, which are typically unavailable from existing\nclustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 20:01:29 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Sun", "Zhe", ""], ["Wang", "Ting", ""], ["Deng", "Ke", ""], ["Wang", "Xiao-Feng", ""], ["Lafyatis", "Robert", ""], ["Ding", "Ying", ""], ["Hu", "Ming", ""], ["Chen", "Wei", ""]]}, {"id": "1704.02038", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, Adarsh Subbaswamy, Suchi Saria", "title": "Treatment-Response Models for Counterfactual Reasoning with\n  Continuous-time, Continuous-valued Interventions", "comments": "In Proceedings of the Thirty-Third Conference on Uncertainty in\n  Artificial Intelligence (UAI-2017), Sydney, Australia, August 2017. The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effects can be estimated from observational data as the difference\nin potential outcomes. In this paper, we address the challenge of estimating\nthe potential outcome when treatment-dose levels can vary continuously over\ntime. Further, the outcome variable may not be measured at a regular frequency.\nOur proposed solution represents the treatment response curves using linear\ntime-invariant dynamical systems---this provides a flexible means for modeling\nresponse over time to highly variable dose curves. Moreover, for multivariate\ndata, the proposed method: uncovers shared structure in treatment response and\nthe baseline across multiple markers; and, flexibly models challenging\ncorrelation structure both across and within signals over time. For this, we\nbuild upon the framework of multiple-output Gaussian Processes. On simulated\nand a challenging clinical dataset, we show significant gains in accuracy over\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 22:42:13 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 02:16:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Soleimani", "Hossein", ""], ["Subbaswamy", "Adarsh", ""], ["Saria", "Suchi", ""]]}, {"id": "1704.02060", "submitter": "Meilei Jiang", "authors": "Qing Feng, Meilei Jiang, Jan Hannig, J. S. Marron", "title": "Angle-Based Joint and Individual Variation Explained", "comments": "arXiv admin note: text overlap with arXiv:1512.04060", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrative analysis of disparate data blocks measured on a common set of\nexperimental subjects is a major challenge in modern data analysis. This data\nstructure naturally motivates the simultaneous exploration of the joint and\nindividual variation within each data block resulting in new insights. For\ninstance, there is a strong desire to integrate the multiple genomic data sets\nin The Cancer Genome Atlas to characterize the common and also the unique\naspects of cancer genetics and cell biology for each source. In this paper we\nintroduce Angle-Based Joint and Individual Variation Explained capturing both\njoint and individual variation within each data block. This is a major\nimprovement over earlier approaches to this challenge in terms of a new\nconceptual understanding, much better adaption to data heterogeneity and a fast\nlinear algebra computation. Important mathematical contributions are the use of\nscore subspaces as the principal descriptors of variation structure and the use\nof perturbation theory as the guide for variation segmentation. This leads to\nan exploratory data analysis method which is insensitive to the heterogeneity\namong data blocks and does not require separate normalization. An application\nto cancer data reveals different behaviors of each type of signal in\ncharacterizing tumor subtypes. An application to a mortality data set reveals\ninteresting historical lessons. Software and data are available at GitHub\n<https://github.com/MeileiJiang/AJIVE_Project>.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 00:33:29 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 03:49:25 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 04:19:26 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Feng", "Qing", ""], ["Jiang", "Meilei", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "1704.02081", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Elnaz Barshan, and Alexander Wong", "title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks", "comments": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:1609.01360", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:28:02 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Barshan", "Elnaz", ""], ["Wong", "Alexander", ""]]}, {"id": "1704.02107", "submitter": "Alexander Jung", "authors": "Alexander Jung and Nguyen Tran Quang and Alexandru Mara", "title": "When is Network Lasso Accurate?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"least absolute shrinkage and selection operator\" (Lasso) method has been\nadapted recently for networkstructured datasets. In particular, this network\nLasso method allows to learn graph signals from a small number of noisy signal\nsamples by using the total variation of a graph signal for regularization.\nWhile efficient and scalable implementations of the network Lasso are\navailable, only little is known about the conditions on the underlying network\nstructure which ensure network Lasso to be accurate. By leveraging concepts of\ncompressed sensing, we address this gap and derive precise conditions on the\nunderlying network topology and sampling set which guarantee the network Lasso\nfor a particular loss function to deliver an accurate estimate of the entire\nunderlying graph signal. We also quantify the error incurred by network Lasso\nin terms of two constants which reflect the connectivity of the sampled nodes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 06:56:23 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:07:18 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 12:30:38 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Jung", "Alexander", ""], ["Quang", "Nguyen Tran", ""], ["Mara", "Alexandru", ""]]}, {"id": "1704.02124", "submitter": "Wojciech Fedorko", "authors": "Jannicke Pearkes, Wojciech Fedorko, Alison Lister, Colin Gay", "title": "Jet Constituents for Deep Neural Network Based Top Quark Tagging", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature on deep neural networks for tagging of highly energetic\njets resulting from top quark decays has focused on image based techniques or\nmultivariate approaches using high-level jet substructure variables. Here, a\nsequential approach to this task is taken by using an ordered sequence of jet\nconstituents as training inputs. Unlike the majority of previous approaches,\nthis strategy does not result in a loss of information during pixelisation or\nthe calculation of high level features. The jet classification method achieves\na background rejection of 45 at a 50% efficiency operating point for\nreconstruction level jets with transverse momentum range of 600 to 2500 GeV and\nis insensitive to multiple proton-proton interactions at the levels expected\nthroughout Run 2 of the LHC.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 08:16:29 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 11:49:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Pearkes", "Jannicke", ""], ["Fedorko", "Wojciech", ""], ["Lister", "Alison", ""], ["Gay", "Colin", ""]]}, {"id": "1704.02162", "submitter": "Manuel L\\'opez-Radcenco", "authors": "Manuel L\\'opez-Radcenco, Ronan Fablet, Abdeldjalil A\\\"issa-El-Bey,\n  Pierre Ailliot", "title": "Locally-adapted convolution-based super-resolution of\n  irregularly-sampled ocean remote sensing data", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution is a classical problem in image processing, with numerous\napplications to remote sensing image enhancement. Here, we address the\nsuper-resolution of irregularly-sampled remote sensing images. Using an optimal\ninterpolation as the low-resolution reconstruction, we explore locally-adapted\nmultimodal convolutional models and investigate different dictionary-based\ndecompositions, namely based on principal component analysis (PCA), sparse\npriors and non-negativity constraints. We consider an application to the\nreconstruction of sea surface height (SSH) fields from two information sources,\nalong-track altimeter data and sea surface temperature (SST) data. The reported\nexperiments demonstrate the relevance of the proposed model, especially\nlocally-adapted parametrizations with non-negativity constraints, to outperform\noptimally-interpolated reconstructions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:51:23 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 15:24:51 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["L\u00f3pez-Radcenco", "Manuel", ""], ["Fablet", "Ronan", ""], ["A\u00efssa-El-Bey", "Abdeldjalil", ""], ["Ailliot", "Pierre", ""]]}, {"id": "1704.02227", "submitter": "Maciej Zieba", "authors": "Maciej Zieba, Lei Wang", "title": "Training Triplet Networks with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triplet networks are widely used models that are characterized by good\nperformance in classification and retrieval tasks. In this work we propose to\ntrain a triplet network by putting it as the discriminator in Generative\nAdversarial Nets (GANs). We make use of the good capability of representation\nlearning of the discriminator to increase the predictive quality of the model.\nWe evaluated our approach on Cifar10 and MNIST datasets and observed\nsignificant improvement on the classification performance using the simple k-nn\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 17:09:20 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Zieba", "Maciej", ""], ["Wang", "Lei", ""]]}, {"id": "1704.02232", "submitter": "Sejun Park", "authors": "Sejun Park, Yunhun Jang, Andreas Galanis, Jinwoo Shin, Daniel\n  Stefankovic, Eric Vigoda", "title": "Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gibbs sampler is a particularly popular Markov chain used for learning\nand inference problems in Graphical Models (GMs). These tasks are\ncomputationally intractable in general, and the Gibbs sampler often suffers\nfrom slow mixing. In this paper, we study the Swendsen-Wang dynamics which is a\nmore sophisticated Markov chain designed to overcome bottlenecks that impede\nthe Gibbs sampler. We prove O(\\log n) mixing time for attractive binary\npairwise GMs (i.e., ferromagnetic Ising models) on stochastic partitioned\ngraphs having n vertices, under some mild conditions, including low temperature\nregions where the Gibbs sampler provably mixes exponentially slow. Our\nexperiments also confirm that the Swendsen-Wang sampler significantly\noutperforms the Gibbs sampler when they are used for learning parameters of\nattractive GMs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 05:12:36 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Park", "Sejun", ""], ["Jang", "Yunhun", ""], ["Galanis", "Andreas", ""], ["Shin", "Jinwoo", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "1704.02254", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and S\\'ebastien Racaniere and Daan Wierstra and Shakir\n  Mohamed", "title": "Recurrent Environment Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that can simulate how environments change in response to actions can\nbe used by agents to plan and act efficiently. We improve on previous\nenvironment simulators from high-dimensional pixel observations by introducing\nrecurrent neural networks that are able to make temporally and spatially\ncoherent predictions for hundreds of time-steps into the future. We present an\nin-depth analysis of the factors affecting performance, providing the most\nextensive attempt to advance the understanding of the properties of these\nmodels. We address the issue of computationally inefficiency with a model that\ndoes not need to generate a high-dimensional image at each time-step. We show\nthat our approach can be used to improve exploration and is adaptable to many\ndiverse environments, namely 10 Atari games, a 3D car racing environment, and\ncomplex 3D mazes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:53:54 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:43:32 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Chiappa", "Silvia", ""], ["Racaniere", "S\u00e9bastien", ""], ["Wierstra", "Daan", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1704.02304", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "It Takes (Only) Two: Adversarial Generator-Encoder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:38:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:09:41 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 15:05:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1704.02345", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Ali Ghodsi", "title": "Fast Spectral Clustering Using Autoencoders and Landmarks", "comments": "8 Pages- Accepted in 14th International Conference on Image Analysis\n  and Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an algorithm for performing spectral clustering\nefficiently. Spectral clustering is a powerful clustering algorithm that\nsuffers from high computational complexity, due to eigen decomposition. In this\nwork, we first build the adjacency matrix of the corresponding graph of the\ndataset. To build this matrix, we only consider a limited number of points,\ncalled landmarks, and compute the similarity of all data points with the\nlandmarks. Then, we present a definition of the Laplacian matrix of the graph\nthat enable us to perform eigen decomposition efficiently, using a deep\nautoencoder. The overall complexity of the algorithm for eigen decomposition is\n$O(np)$, where $n$ is the number of data points and $p$ is the number of\nlandmarks. At last, we evaluate the performance of the algorithm in different\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:40:52 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Banijamali", "Ershad", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1704.02346", "submitter": "Luciana Ferrer", "authors": "Luciana Ferrer", "title": "Joint Probabilistic Linear Discriminant Analysis", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard probabilistic linear discriminant analysis (PLDA) for speaker\nrecognition assumes that the sample's features (usually, i-vectors) are given\nby a sum of three terms: a term that depends on the speaker identity, a term\nthat models the within-speaker variability and is assumed independent across\nsamples, and a final term that models any remaining variability and is also\nindependent across samples. In this work, we propose a generalization of this\nmodel where the within-speaker variability is not necessarily assumed\nindependent across samples but dependent on another discrete variable. This\nvariable, which we call the channel variable as in the standard PLDA approach,\ncould be, for example, a discrete category for the channel characteristics, the\nlanguage spoken by the speaker, the type of speech in the sample\n(conversational, monologue, read), etc. The value of this variable is assumed\nto be known during training but not during testing. Scoring is performed, as in\nstandard PLDA, by computing a likelihood ratio between the null hypothesis that\nthe two sides of a trial belong to the same speaker versus the alternative\nhypothesis that the two sides belong to different speakers. The two likelihoods\nare computed by marginalizing over two hypothesis about the channels in both\nsides of a trial: that they are the same and that they are different. This way,\nwe expect that the new model will be better at coping with same-channel versus\ndifferent-channel trials than standard PLDA, since knowledge about the channel\n(or language, or speech style) is used during training and implicitly\nconsidered during scoring.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 18:42:05 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 15:16:56 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ferrer", "Luciana", ""]]}, {"id": "1704.02370", "submitter": "Diego Saldana", "authors": "Diego Saldana Miranda", "title": "A Brief Introduction to the Temporal Group LASSO and its Potential\n  Applications in Healthcare", "comments": "Corrected typographical error in the conclusion. Added a reference to\n  Proximal Algorithms (Parikh and Boyd, 2014). Modified \"An Introduction\" to \"A\n  Brief Introduction\" in the title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Temporal Group LASSO is an example of a multi-task, regularized\nregression approach for the prediction of response variables that vary over\ntime. The aim of this work is to introduce the reader to the concepts behind\nthe Temporal Group LASSO and its related methods, as well as to the type of\npotential applications in a healthcare setting that the method has. We argue\nthat the method is attractive because of its ability to reduce overfitting,\nselect predictors, learn smooth effect patterns over time, and finally, its\nsimplicity\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 20:45:31 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 17:17:56 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Miranda", "Diego Saldana", ""]]}, {"id": "1704.02502", "submitter": "Natalia Da Silva", "authors": "Natalia da Silva, Dianne Cook and Eun-Kyung Lee", "title": "Interactive Graphics for Visually Diagnosing Forest Classifiers in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes structuring data and constructing plots to explore\nforest classification models interactively. A forest classifier is an example\nof an ensemble, produced by bagging multiple trees. The process of bagging and\ncombining results from multiple trees, produces numerous diagnostics which,\nwith interactive graphics, can provide a lot of insight into class structure in\nhigh dimensions. Various aspects are explored in this paper, to assess model\ncomplexity, individual model contributions, variable importance and dimension\nreduction, and uncertainty in prediction associated with individual\nobservations. The ideas are applied to the random forest algorithm, and to the\nprojection pursuit forest, but could be more broadly applied to other bagged\nensembles. Interactive graphics are built in R, using the ggplot2, plotly, and\nshiny packages.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 14:41:38 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["da Silva", "Natalia", ""], ["Cook", "Dianne", ""], ["Lee", "Eun-Kyung", ""]]}, {"id": "1704.02532", "submitter": "Senthil Yogamani", "authors": "Ahmad El Sallab, Mohammed Abdou, Etienne Perot and Senthil Yogamani", "title": "Deep Reinforcement Learning framework for Autonomous Driving", "comments": "Reprinted with permission of IS&T: The Society for Imaging Science\n  and Technology, sole copyright owners of Electronic Imaging, Autonomous\n  Vehicles and Machines 2017", "journal-ref": "IS&T Electronic Imaging, Autonomous Vehicles and Machines 2017,\n  AVM-023, pg. 70-76 (2017)", "doi": "10.2352/ISSN.2470-1173.2017.19.AVM-023", "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes. Despite its perceived utility, it has not yet been\nsuccessfully applied in automotive applications. Motivated by the successful\ndemonstrations of learning of Atari games and Go by Google DeepMind, we propose\na framework for autonomous driving using deep reinforcement learning. This is\nof particular relevance as it is difficult to pose autonomous driving as a\nsupervised learning problem due to strong interactions with the environment\nincluding other vehicles, pedestrians and roadworks. As it is a relatively new\narea of research for autonomous driving, we provide a short overview of deep\nreinforcement learning and then describe our proposed framework. It\nincorporates Recurrent Neural Networks for information integration, enabling\nthe car to handle partially observable scenarios. It also integrates the recent\nwork on attention models to focus on relevant information, thereby reducing the\ncomputational complexity for deployment on embedded hardware. The framework was\ntested in an open source 3D car racing simulator called TORCS. Our simulation\nresults demonstrate learning of autonomous maneuvering in a scenario of complex\nroad curvatures and simple interaction of other vehicles.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 20:04:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Abdou", "Mohammed", ""], ["Perot", "Etienne", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1704.02534", "submitter": "Swayambhoo Jain", "authors": "Swayambhoo Jain, Alexander Gutierrez, and Jarvis Haupt", "title": "Noisy Tensor Completion for Tensors with a Sparse Canonical Polyadic\n  Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of noisy tensor completion for tensors\nthat admit a canonical polyadic or CANDECOMP/PARAFAC (CP) decomposition with\none of the factors being sparse. We present general theoretical error bounds\nfor an estimate obtained by using a complexity-regularized maximum likelihood\nprinciple and then instantiate these bounds for the case of additive white\nGaussian noise. We also provide an ADMM-type algorithm for solving the\ncomplexity-regularized maximum likelihood problem and validate the theoretical\nfinding via experiments on synthetic data set.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 20:17:26 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Gutierrez", "Alexander", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1704.02568", "submitter": "Wenlin Dai", "authors": "Wenlin Dai, Marc G. Genton", "title": "An Outlyingness Matrix for Multivariate Functional Data Classification", "comments": "26 pages, 7 figures, Statistica Sinica, 2017", "journal-ref": null, "doi": "10.5705/ss.202016.0537", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of multivariate functional data is an important task in\nscientific research. Unlike point-wise data, functional data are usually\nclassified by their shapes rather than by their scales. We define an\noutlyingness matrix by extending directional outlyingness, an effective measure\nof the shape variation of curves that combines the direction of outlyingness\nwith conventional depth. We propose two classifiers based on directional\noutlyingness and the outlyingness matrix, respectively. Our classifiers provide\nbetter performance compared with existing depth-based classifiers when applied\non both univariate and multivariate functional data from simulation studies. We\nalso test our methods on two data problems: speech recognition and gesture\nclassification, and obtain results that are consistent with the findings from\nthe simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 07:17:08 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 06:34:13 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1704.02578", "submitter": "Hamed Masnadi-Shirazi", "authors": "Hamed Masnadi-Shirazi", "title": "Strictly Proper Kernel Scoring Rules and Divergences with an Application\n  to Kernel Two-Sample Hypothesis Testing", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study strictly proper scoring rules in the Reproducing Kernel Hilbert\nSpace. We propose a general Kernel Scoring rule and associated Kernel\nDivergence. We consider conditions under which the Kernel Score is strictly\nproper. We then demonstrate that the Kernel Score includes the Maximum Mean\nDiscrepancy as a special case. We also consider the connections between the\nKernel Score and the minimum risk of a proper loss function. We show that the\nKernel Score incorporates more information pertaining to the projected embedded\ndistributions compared to the Maximum Mean Discrepancy. Finally, we show how to\nintegrate the information provided from different Kernel Divergences, such as\nthe proposed Bhattacharyya Kernel Divergence, using a one-class classifier for\nimproved two-sample hypothesis testing results.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 09:21:50 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 09:27:45 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Masnadi-Shirazi", "Hamed", ""]]}, {"id": "1704.02621", "submitter": "Panayiotis Benos", "authors": "Andrew J Sedgewick, Joseph D. Ramsey, Peter Spirtes, Clark Glymour,\n  Panayiotis V. Benos", "title": "Mixed Graphical Models for Causal Analysis of Multi-modal Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical causal models are an important tool for knowledge discovery because\nthey can represent both the causal relations between variables and the\nmultivariate probability distributions over the data. Once learned, causal\ngraphs can be used for classification, feature selection and hypothesis\ngeneration, while revealing the underlying causal network structure and thus\nallowing for arbitrary likelihood queries over the data. However, current\nalgorithms for learning sparse directed graphs are generally designed to handle\nonly one type of data (continuous-only or discrete-only), which limits their\napplicability to a large class of multi-modal biological datasets that include\nmixed type variables. To address this issue, we developed new methods that\nmodify and combine existing methods for finding undirected graphs with methods\nfor finding directed graphs. These hybrid methods are not only faster, but also\nperform better than the directed graph estimation methods alone for a variety\nof parameter settings and data set sizes. Here, we describe a new conditional\nindependence test for learning directed graphs over mixed data types and we\ncompare performances of different graph learning strategies on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 15:58:35 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Sedgewick", "Andrew J", ""], ["Ramsey", "Joseph D.", ""], ["Spirtes", "Peter", ""], ["Glymour", "Clark", ""], ["Benos", "Panayiotis V.", ""]]}, {"id": "1704.02658", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker and Nate Strawn", "title": "Distributed Statistical Estimation and Rates of Convergence in Normal\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a class of new algorithms for distributed statistical\nestimation that exploit divide-and-conquer approach. We show that one of the\nkey benefits of the divide-and-conquer strategy is robustness, an important\ncharacteristic for large distributed systems. We establish connections between\nperformance of these distributed algorithms and the rates of convergence in\nnormal approximation, and prove non-asymptotic deviations guarantees, as well\nas limit theorems, for the resulting estimators. Our techniques are illustrated\nthrough several examples: in particular, we obtain new results for the\nmedian-of-means estimator, as well as provide performance guarantees for\ndistributed maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 20:43:55 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 21:50:04 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 22:25:54 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Minsker", "Stanislav", ""], ["Strawn", "Nate", ""]]}, {"id": "1704.02686", "submitter": "Shuchin Aeron", "authors": "Eric Bailey and Shuchin Aeron", "title": "Word Embeddings via Tensor Factorization", "comments": "More simulation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular word embedding techniques involve implicit or explicit\nfactorization of a word co-occurrence based matrix into low rank factors. In\nthis paper, we aim to generalize this trend by using numerical methods to\nfactor higher-order word co-occurrence based arrays, or \\textit{tensors}. We\npresent four word embeddings using tensor factorization and analyze their\nadvantages and disadvantages. One of our main contributions is a novel joint\nsymmetric tensor factorization technique related to the idea of coupled tensor\nfactorization. We show that embeddings based on tensor factorization can be\nused to discern the various meanings of polysemous words without being\nexplicitly trained to do so, and motivate the intuition behind why this works\nin a way that doesn't with existing methods. We also modify an existing word\nembedding evaluation metric known as Outlier Detection [Camacho-Collados and\nNavigli, 2016] to evaluate the quality of the order-$N$ relations that a word\nembedding captures, and show that tensor-based methods outperform existing\nmatrix-based methods at this task. Experimentally, we show that all of our word\nembeddings either outperform or are competitive with state-of-the-art baselines\ncommonly used today on a variety of recent datasets. Suggested applications of\ntensor factorization-based word embeddings are given, and all source code and\npre-trained vectors are publicly available online.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:24:37 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 18:56:30 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bailey", "Eric", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1704.02718", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "Distributed Learning for Cooperative Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of cooperative inference where a group of agents\ninteract over a network and seek to estimate a joint parameter that best\nexplains a set of observations. Agents do not know the network topology or the\nobservations of other agents. We explore a variational interpretation of the\nBayesian posterior density, and its relation to the stochastic mirror descent\nalgorithm, to propose a new distributed learning algorithm. We show that, under\nappropriate assumptions, the beliefs generated by the proposed algorithm\nconcentrate around the true parameter exponentially fast. We provide explicit\nnon-asymptotic bounds for the convergence rate. Moreover, we develop explicit\nand computationally efficient algorithms for observation models belonging to\nexponential families.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 06:04:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1704.02739", "submitter": "Yunqi Bu", "authors": "Yunqi Bu and Johannes Lederer", "title": "Integrating Additional Knowledge Into Estimation of Graphical Models", "comments": "16 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of graphical models, we typically have more information than\njust the samples themselves. A prime example is the estimation of brain\nconnectivity networks based on fMRI data, where in addition to the samples\nthemselves, the spatial positions of the measurements are readily available.\nWith particular regard for this application, we are thus interested in ways to\nincorporate additional knowledge most effectively into graph estimation. Our\napproach to this is to make neighborhood selection receptive to additional\nknowledge by strengthening the role of the tuning parameters. We demonstrate\nthat this concept (i) can improve reproducibility, (ii) is computationally\nconvenient and efficient, and (iii) carries a lucid Bayesian interpretation. We\nspecifically show that the approach provides effective estimations of brain\nconnectivity graphs from fMRI data. However, providing a general scheme for the\ninclusion of additional knowledge, our concept is expected to have applications\nin a wide range of domains.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 07:33:54 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 04:26:11 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bu", "Yunqi", ""], ["Lederer", "Johannes", ""]]}, {"id": "1704.02771", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, G. Camps-Valls", "title": "Group Importance Sampling for Particle Filtering and MCMC", "comments": "To appear in Digital Signal Processing. Related Matlab demos are\n  provided at https://github.com/lukafree/GIS.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods and their implementations by means of sophisticated Monte\nCarlo techniques have become very popular in signal processing over the last\nyears. Importance Sampling (IS) is a well-known Monte Carlo technique that\napproximates integrals involving a posterior distribution by means of weighted\nsamples. In this work, we study the assignation of a single weighted sample\nwhich compresses the information contained in a population of weighted samples.\nPart of the theory that we present as Group Importance Sampling (GIS) has been\nemployed implicitly in different works in the literature. The provided analysis\nyields several theoretical and practical consequences. For instance, we discuss\nthe application of GIS into the Sequential Importance Resampling framework and\nshow that Independent Multiple Try Metropolis schemes can be interpreted as a\nstandard Metropolis-Hastings algorithm, following the GIS approach. We also\nintroduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.\nThe first one, named Group Metropolis Sampling method, produces a Markov chain\nof sets of weighted samples. All these sets are then employed for obtaining a\nunique global estimator. The second one is the Distributed Particle\nMetropolis-Hastings technique, where different parallel particle filters are\njointly used to drive an MCMC algorithm. Different resampled trajectories are\ncompared and then tested with a proper acceptance probability. The novel\nschemes are tested in different numerical experiments such as learning the\nhyperparameters of Gaussian Processes, two localization problems in a wireless\nsensor network (with synthetic and real data) and the tracking of vegetation\nparameters given satellite observations, where they are compared with several\nbenchmark Monte Carlo techniques. Three illustrative Matlab demos are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:20:47 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 14:22:22 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 20:51:47 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 09:19:51 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Camps-Valls", "G.", ""]]}, {"id": "1704.02798", "submitter": "Meire Fortunato", "authors": "Meire Fortunato, Charles Blundell, Oriol Vinyals", "title": "Bayesian Recurrent Neural Networks", "comments": "12th Women in Machine Learning Workshop (WiML 2017), co-located with\n  the 31st Conference on Neural Information Processing Systems (NeurIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training, also reducing the amount of parameters by 80\\%. Secondly, we\ndemonstrate how a novel kind of posterior approximation yields further\nimprovements to the performance of Bayesian RNNs. We incorporate local gradient\ninformation into the approximate posterior to sharpen it around the current\nbatch statistics. We show how this technique is not exclusive to recurrent\nneural networks and can be applied more widely to train Bayesian neural\nnetworks. We also empirically demonstrate how Bayesian RNNs are superior to\ntraditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:59:05 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:25:08 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 12:14:16 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 22:04:45 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Fortunato", "Meire", ""], ["Blundell", "Charles", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1704.02799", "submitter": "Mohammad Azzeh", "authors": "Israa Ahmed Zriqat, Ahmad Mousa Altamimi, Mohammad Azzeh", "title": "A Comparative Study for Predicting Heart Diseases Using Data Mining\n  Classification Methods", "comments": null, "journal-ref": "ISSN 1947-5500", "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the precision of heart diseases detection has been investigated by\nmany researchers in the literature. Such improvement induced by the\noverwhelming health care expenditures and erroneous diagnosis. As a result,\nvarious methodologies have been proposed to analyze the disease factors aiming\nto decrease the physicians practice variation and reduce medical costs and\nerrors. In this paper, our main motivation is to develop an effective\nintelligent medical decision support system based on data mining techniques. In\nthis context, five data mining classifying algorithms, with large datasets,\nhave been utilized to assess and analyze the risk factors statistically related\nto heart diseases in order to compare the performance of the implemented\nclassifiers (e.g., Na\\\"ive Bayes, Decision Tree, Discriminant, Random Forest,\nand Support Vector Machine). To underscore the practical viability of our\napproach, the selected classifiers have been implemented using MATLAB tool with\ntwo datasets. Results of the conducted experiments showed that all\nclassification algorithms are predictive and can give relatively correct\nanswer. However, the decision tree outperforms other classifiers with an\naccuracy rate of 99.0% followed by Random forest. That is the case because both\nof them have relatively same mechanism but the Random forest can build ensemble\nof decision tree. Although ensemble learning has been proved to produce\nsuperior results, but in our case the decision tree has outperformed its\nensemble version.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 11:03:14 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zriqat", "Israa Ahmed", ""], ["Altamimi", "Ahmad Mousa", ""], ["Azzeh", "Mohammad", ""]]}, {"id": "1704.02828", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni and Eric Maris", "title": "Integral Transforms from Finite Data: An Application of Gaussian Process\n  Regression to Fourier Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing accurate estimates of the Fourier transform of analog signals from\ndiscrete data points is important in many fields of science and engineering.\nThe conventional approach of performing the discrete Fourier transform of the\ndata implicitly assumes periodicity and bandlimitedness of the signal. In this\npaper, we use Gaussian process regression to estimate the Fourier transform (or\nany other integral transform) without making these assumptions. This is\npossible because the posterior expectation of Gaussian process regression maps\na finite set of samples to a function defined on the whole real line, expressed\nas a linear combination of covariance functions. We estimate the covariance\nfunction from the data using an appropriately designed gradient ascent method\nthat constrains the solution to a linear combination of tractable kernel\nfunctions. This procedure results in a posterior expectation of the analog\nsignal whose Fourier transform can be obtained analytically by exploiting\nlinearity. Our simulations show that the new method leads to sharper and more\nprecise estimation of the spectral density both in noise-free and\nnoise-corrupted signals. We further validate the method in two real-world\napplications: the analysis of the yearly fluctuation in atmospheric CO2 level\nand the analysis of the spectral content of brain signals.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 12:23:42 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 23:59:48 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Ambrogioni", "Luca", ""], ["Maris", "Eric", ""]]}, {"id": "1704.02846", "submitter": "Sael Lee", "authors": "Jaya Thomas and Lee Sael", "title": "Multi-Kernel LS-SVM Based Bio-Clinical Data Integration: Applications to\n  Ovarian Cancer", "comments": "27 pages, 7 figures, extends the work presented in 6th International\n  Conference on Emerging Databases, accepted for publication in the IJDBM", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical research facilitates to acquire a diverse type of data from the\nsame individual for particular cancer. Recent studies show that utilizing such\ndiverse data results in more accurate predictions. The major challenge faced is\nhow to utilize such diverse data sets in an effective way. In this paper, we\nintroduce a multiple kernel based pipeline for integrative analysis of\nhigh-throughput molecular data (somatic mutation, copy number alteration, DNA\nmethylation and mRNA) and clinical data. We apply the pipeline on Ovarian\ncancer data from TCGA. After multiple kernels have been generated from the\nweighted sum of individual kernels, it is used to stratify patients and predict\nclinical outcomes. We examine the survival time, vital status, and neoplasm\ncancer status of each subtype to verify how well they cluster. We have also\nexamined the power of molecular and clinical data in predicting dichotomized\noverall survival data and to classify the tumor grade for the cancer samples.\nIt was observed that the integration of various data types yields higher\nlog-rank statistics value. We were also able to predict clinical status with\nhigher accuracy as compared to using individual data types.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:15:36 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 05:17:06 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Thomas", "Jaya", ""], ["Sael", "Lee", ""]]}, {"id": "1704.02853", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman,\n  Andrew McCallum", "title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations\n  from Scientific Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the SemEval task of extracting keyphrases and relations between\nthem from scientific documents, which is crucial for understanding which\npublications describe which processes, tasks and materials. Although this was a\nnew task, we had a total of 26 submissions across 3 evaluation scenarios. We\nexpect the task and the findings reported in this paper to be relevant for\nresearchers working on understanding scientific content, as well as the broader\nknowledge base population and information extraction communities.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:43:40 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 10:41:31 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 15:32:41 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Das", "Mrinal", ""], ["Riedel", "Sebastian", ""], ["Vikraman", "Lakshmi", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.02882", "submitter": "Hadrien Hendrikx", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Hadrien Hendrikx, Alexandre\n  Maurer", "title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 14:38:37 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:01:28 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Hendrikx", "Hadrien", ""], ["Maurer", "Alexandre", ""]]}, {"id": "1704.02906", "submitter": "Viveka Kulharia", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri and Philip H. S.\n  Torr and Puneet K. Dokania", "title": "Multi-Agent Diverse Generative Adversarial Networks", "comments": "This is an updated version of our CVPR'18 paper with the same title.\n  In this version, we also introduce MAD-GAN-Sim in Appendix B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\nNetworks (GANs) and its conditional variants to address the well known problem\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\nincorporating multiple generators and one discriminator. Second, to enforce\nthat different generators capture diverse high probability modes, the\ndiscriminator of MAD-GAN is designed such that along with finding the real and\nfake samples, it is also required to identify the generator that generated the\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\nlearn to push different generators towards different identifiable modes. We\nperform extensive experiments on synthetic and real datasets and compare\nMAD-GAN with different variants of GAN. We show high quality diverse sample\ngenerations for challenging tasks such as image-to-image translation and face\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\ndifferent modalities when trained using highly challenging diverse-class\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\nend, we show its efficacy on the unsupervised feature representation task. In\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\nwhich encourages different generators to generate diverse samples based on a\nuser defined similarity metric. We show its performance on the image-to-image\ntranslation, and also show its effectiveness on the unsupervised feature\nrepresentation task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 23:29:16 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 16:21:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1704.02916", "submitter": "Chris Cremer", "authors": "Chris Cremer, Quaid Morris, David Duvenaud", "title": "Reinterpreting Importance-Weighted Autoencoders", "comments": "ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard interpretation of importance-weighted autoencoders is that they\nmaximize a tighter lower bound on the marginal likelihood than the standard\nevidence lower bound. We give an alternate interpretation of this procedure:\nthat it optimizes the standard variational lower bound, but using a more\ncomplex distribution. We formally derive this result, present a tighter lower\nbound, and visualize the implicit importance-weighted distribution.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:45:41 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 01:56:16 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Cremer", "Chris", ""], ["Morris", "Quaid", ""], ["Duvenaud", "David", ""]]}, {"id": "1704.02958", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk, Ludwig Schmidt", "title": "On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:26:41 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1704.02966", "submitter": "Samuel Rota Bul\\`o", "authors": "Samuel Rota Bul\\`o, Gerhard Neuhold, Peter Kontschieder", "title": "Loss Max-Pooling for Semantic Image Segmentation", "comments": "accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel loss max-pooling concept for handling imbalanced\ntraining data distributions, applicable as alternative loss layer in the\ncontext of deep neural networks for semantic image segmentation. Most\nreal-world semantic segmentation datasets exhibit long tail distributions with\nfew object categories comprising the majority of data and consequently biasing\nthe classifiers towards them. Our method adaptively re-weights the\ncontributions of each pixel based on their observed losses, targeting\nunder-performing classification results as often encountered for\nunder-represented object classes. Our approach goes beyond conventional\ncost-sensitive learning attempts through adaptive considerations that allow us\nto indirectly address both, inter- and intra-class imbalances. We provide a\ntheoretical justification of our approach, complementary to experimental\nanalyses on benchmark datasets. In our experiments on the Cityscapes and Pascal\nVOC 2012 segmentation datasets we find consistently improved results,\ndemonstrating the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:44:33 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Bul\u00f2", "Samuel Rota", ""], ["Neuhold", "Gerhard", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1704.02971", "submitter": "Yao Qin", "authors": "Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and\n  Garrison Cottrell", "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series\n  Prediction", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonlinear autoregressive exogenous (NARX) model, which predicts the\ncurrent value of a time series based upon its previous values as well as the\ncurrent and past values of multiple driving (exogenous) series, has been\nstudied for decades. Despite the fact that various NARX models have been\ndeveloped, few of them can capture the long-term temporal dependencies\nappropriately and select the relevant driving series to make predictions. In\nthis paper, we propose a dual-stage attention-based recurrent neural network\n(DA-RNN) to address these two issues. In the first stage, we introduce an input\nattention mechanism to adaptively extract relevant driving series (a.k.a.,\ninput features) at each time step by referring to the previous encoder hidden\nstate. In the second stage, we use a temporal attention mechanism to select\nrelevant encoder hidden states across all time steps. With this dual-stage\nattention scheme, our model can not only make predictions effectively, but can\nalso be easily interpreted. Thorough empirical studies based upon the SML 2010\ndataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can\noutperform state-of-the-art methods for time series prediction.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 23:50:09 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 19:35:08 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 21:26:23 GMT"}, {"version": "v4", "created": "Mon, 14 Aug 2017 10:15:06 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Qin", "Yao", ""], ["Song", "Dongjin", ""], ["Chen", "Haifeng", ""], ["Cheng", "Wei", ""], ["Jiang", "Guofei", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1704.02978", "submitter": "Zafar Takhirov", "authors": "Zafar Takhirov and Joseph Wang and Marcia S. Louis and Venkatesh\n  Saligrama and Ajay Joshi", "title": "Field of Groves: An Energy-Efficient Random Forest", "comments": "Submitted as Work in Progress to DAC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms, like Convolutional Neural Networks (CNN),\nSupport Vector Machines (SVM), etc. have become widespread and can achieve high\nstatistical performance. However their accuracy decreases significantly in\nenergy-constrained mobile and embedded systems space, where all computations\nneed to be completed under a tight energy budget. In this work, we present a\nfield of groves (FoG) implementation of random forests (RF) that achieves an\naccuracy comparable to CNNs and SVMs under tight energy budgets. Evaluation of\nthe FoG shows that at comparable accuracy it consumes ~1.48x, ~24x, ~2.5x, and\n~34.7x lower energy per classification compared to conventional RF, SVM_RBF ,\nMLP, and CNN, respectively. FoG is ~6.5x less energy efficient than SVM_LR, but\nachieves 18% higher accuracy on average across all considered datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:02:07 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Takhirov", "Zafar", ""], ["Wang", "Joseph", ""], ["Louis", "Marcia S.", ""], ["Saligrama", "Venkatesh", ""], ["Joshi", "Ajay", ""]]}, {"id": "1704.03033", "submitter": "Maria Bauza Villalonga", "authors": "Maria Bauza, Alberto Rodriguez", "title": "A probabilistic data-driven model for planar pushing", "comments": "8 pages, 11 figures, ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven approach to model planar pushing\ninteraction to predict both the most likely outcome of a push and its expected\nvariability. The learned models rely on a variation of Gaussian processes with\ninput-dependent noise called Variational Heteroscedastic Gaussian processes\n(VHGP) that capture the mean and variance of a stochastic function. We show\nthat we can learn accurate models that outperform analytical models after less\nthan 100 samples and saturate in performance with less than 1000 samples. We\nvalidate the results against a collected dataset of repeated trajectories, and\nuse the learned models to study questions such as the nature of the variability\nin pushing, and the validity of the quasi-static assumption.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:41:41 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 22:21:13 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Bauza", "Maria", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1704.03058", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu", "title": "CERN: Confidence-Energy Recurrent Network for Group Activity Recognition", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 21:08:39 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Shu", "Tianmin", ""], ["Todorovic", "Sinisa", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1704.03141", "submitter": "Yejin Kim", "authors": "Yejin Kim, Jimeng Sun, Hwanjo Yu, Xiaoqian Jiang", "title": "Federated Tensor Factorization for Computational Phenotyping", "comments": null, "journal-ref": null, "doi": "10.1145/3097983.3098118", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization models offer an effective approach to convert massive\nelectronic health records into meaningful clinical concepts (phenotypes) for\ndata analysis. These models need a large amount of diverse samples to avoid\npopulation bias. An open challenge is how to derive phenotypes jointly across\nmultiple hospitals, in which direct patient-level data sharing is not possible\n(e.g., due to institutional policies). In this paper, we developed a novel\nsolution to enable federated tensor factorization for computational phenotyping\nwithout sharing patient-level data. We developed secure data harmonization and\nfederated computation procedures based on alternating direction method of\nmultipliers (ADMM). Using this method, the multiple hospitals iteratively\nupdate tensors and transfer secure summarized information to a central server,\nand the server aggregates the information to generate phenotypes. We\ndemonstrated with real medical datasets that our method resembles the\ncentralized training model (based on combined datasets) in terms of accuracy\nand phenotypes discovery while respecting privacy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:28:03 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kim", "Yejin", ""], ["Sun", "Jimeng", ""], ["Yu", "Hwanjo", ""], ["Jiang", "Xiaoqian", ""]]}, {"id": "1704.03144", "submitter": "Maziar Raissi", "authors": "Maziar Raissi", "title": "Parametric Gaussian Process Regression for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the concept of parametric Gaussian processes (PGPs),\nwhich is built upon the seemingly self-contradictory idea of making Gaussian\nprocesses parametric. Parametric Gaussian processes, by construction, are\ndesigned to operate in \"big data\" regimes where one is interested in\nquantifying the uncertainty associated with noisy data. The proposed\nmethodology circumvents the well-established need for stochastic variational\ninference, a scalable algorithm for approximating posterior distributions. The\neffectiveness of the proposed approach is demonstrated using an illustrative\nexample with simulated data and a benchmark dataset in the airline industry\nwith approximately 6 million records.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:57:24 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 20:12:45 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Raissi", "Maziar", ""]]}, {"id": "1704.03165", "submitter": "Leonardo F. R. Ribeiro", "authors": "Leonardo F. R. Ribeiro, Pedro H. P. Savarese, Daniel R. Figueiredo", "title": "struc2vec: Learning Node Representations from Structural Identity", "comments": "10 pages, KDD2017, Research Track", "journal-ref": null, "doi": "10.1145/3097983.3098061", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural identity is a concept of symmetry in which network nodes are\nidentified according to the network structure and their relationship to other\nnodes. Structural identity has been studied in theory and practice over the\npast decades, but only recently has it been addressed with representational\nlearning techniques. This work presents struc2vec, a novel and flexible\nframework for learning latent representations for the structural identity of\nnodes. struc2vec uses a hierarchy to measure node similarity at different\nscales, and constructs a multilayer graph to encode structural similarities and\ngenerate structural context for nodes. Numerical experiments indicate that\nstate-of-the-art techniques for learning node representations fail in capturing\nstronger notions of structural identity, while struc2vec exhibits much superior\nperformance in this task, as it overcomes limitations of prior approaches. As a\nconsequence, numerical experiments indicate that struc2vec improves performance\non classification tasks that depend more on structural identity.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:32:36 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 01:35:12 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 20:47:08 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ribeiro", "Leonardo F. R.", ""], ["Savarese", "Pedro H. P.", ""], ["Figueiredo", "Daniel R.", ""]]}, {"id": "1704.03223", "submitter": "Zahra Mousavi", "authors": "Zahra Mousavi, Heshaam Faili", "title": "Persian Wordnet Construction using Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated supervised method for Persian wordnet\nconstruction. Using a Persian corpus and a bi-lingual dictionary, the initial\nlinks between Persian words and Princeton WordNet synsets have been generated.\nThese links will be discriminated later as correct or incorrect by employing\nseven features in a trained classification system. The whole method is just a\nclassification system, which has been trained on a train set containing FarsNet\nas a set of correct instances. State of the art results on the automatically\nderived Persian wordnet is achieved. The resulted wordnet with a precision of\n91.18% includes more than 16,000 words and 22,000 synsets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:47:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mousavi", "Zahra", ""], ["Faili", "Heshaam", ""]]}, {"id": "1704.03296", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "comments": "Final camera-ready paper published at ICCV 2017 (Supplementary\n  materials:\n  http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Fong_Interpretable_Explanations_of_ICCV_2017_supplemental.pdf)", "journal-ref": "Proceedings of the 2017 IEEE International Conference on Computer\n  Vision (ICCV)", "doi": "10.1109/ICCV.2017.371", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:15:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 13:53:21 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 16:03:33 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1704.03354", "submitter": "Flavio Calmon", "authors": "Flavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, and Kush\n  R. Varshney", "title": "Optimized Data Pre-Processing for Discrimination Prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-discrimination is a recognized objective in algorithmic decision making.\nIn this paper, we introduce a novel probabilistic formulation of data\npre-processing for reducing discrimination. We propose a convex optimization\nfor learning a data transformation with three goals: controlling\ndiscrimination, limiting distortion in individual data samples, and preserving\nutility. We characterize the impact of limited sample size in accomplishing\nthis objective, and apply two instances of the proposed optimization to\ndatasets, including one on real-world criminal recidivism. The results\ndemonstrate that all three criteria can be simultaneously achieved and also\nreveal interesting patterns of bias in American society.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:26:00 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Calmon", "Flavio P.", ""], ["Wei", "Dennis", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1704.03453", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick\n  McDaniel", "title": "The Space of Transferable Adversarial Examples", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are maliciously perturbed inputs designed to mislead\nmachine learning (ML) models at test-time. They often transfer: the same\nadversarial example fools more than one model.\n  In this work, we propose novel methods for estimating the previously unknown\ndimensionality of the space of adversarial inputs. We find that adversarial\nexamples span a contiguous subspace of large (~25) dimensionality. Adversarial\nsubspaces with higher dimensionality are more likely to intersect. We find that\nfor two different models, a significant fraction of their subspaces is shared,\nthus enabling transferability.\n  In the first quantitative analysis of the similarity of different models'\ndecision boundaries, we show that these boundaries are actually close in\narbitrary directions, whether adversarial or benign. We conclude by formally\nstudying the limits of transferability. We derive (1) sufficient conditions on\nthe data distribution that imply transferability for simple model classes and\n(2) examples of scenarios in which transfer does not occur. These findings\nindicate that it may be possible to design defenses against transfer-based\nattacks, even for models that are vulnerable to direct attacks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:59:12 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 18:14:30 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Papernot", "Nicolas", ""], ["Goodfellow", "Ian", ""], ["Boneh", "Dan", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1704.03477", "submitter": "David Ha", "authors": "David Ha and Douglas Eck", "title": "A Neural Representation of Sketch Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sketch-rnn, a recurrent neural network (RNN) able to construct\nstroke-based drawings of common objects. The model is trained on thousands of\ncrude human-drawn images representing hundreds of classes. We outline a\nframework for conditional and unconditional sketch generation, and describe new\nrobust training methods for generating coherent sketch drawings in a vector\nformat.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 01:26:05 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 16:28:23 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 16:40:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ha", "David", ""], ["Eck", "Douglas", ""]]}, {"id": "1704.03568", "submitter": "Christopher Funk", "authors": "Christopher Funk and Yanxi Liu", "title": "Beyond Planar Symmetry: Modeling human perception of reflection and\n  rotation symmetries in the wild", "comments": "To appear in the International Conference on Computer Vision (ICCV)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans take advantage of real world symmetries for various tasks, yet\ncapturing their superb symmetry perception mechanism with a computational model\nremains elusive. Motivated by a new study demonstrating the extremely high\ninter-person accuracy of human perceived symmetries in the wild, we have\nconstructed the first deep-learning neural network for reflection and rotation\nsymmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common\nObject in COntext) dataset with nearly 11K consistent symmetry-labels from more\nthan 400 human observers. We employ novel methods to convert discrete human\nlabels into symmetry heatmaps, capture symmetry densely in an image and\nquantitatively evaluate Sym-NET against multiple existing computer vision\nalgorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO\nphotos, Sym-NET significantly outperforms all other competitors. Beyond\nmathematically well-defined symmetries on a plane, Sym-NET demonstrates\nabilities to identify viewpoint-varied 3D symmetries, partially occluded\nsymmetrical objects, and symmetries at a semantic level.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 23:25:25 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:11:05 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Funk", "Christopher", ""], ["Liu", "Yanxi", ""]]}, {"id": "1704.03581", "submitter": "Alexander Terenin", "authors": "Alexander Terenin, M{\\aa}ns Magnusson, Leif Jonsson, and David Draper", "title": "P\\'olya Urn Latent Dirichlet Allocation: a doubly sparse massively\n  parallel sampler", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  41(7):1709-1719, 2019", "doi": "10.1109/TPAMI.2018.2832641", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a topic model widely used in natural\nlanguage processing and machine learning. Most approaches to training the model\nrely on iterative algorithms, which makes it difficult to run LDA on big\ncorpora that are best analyzed in parallel and distributed computational\nenvironments. Indeed, current approaches to parallel inference either don't\nconverge to the correct posterior or require storage of large dense matrices in\nmemory. We present a novel sampler that overcomes both problems, and we show\nthat this sampler is faster, both empirically and theoretically, than previous\nGibbs samplers for LDA. We do so by employing a novel P\\'olya-urn-based\napproximation in the sparse partially collapsed sampler for LDA. We prove that\nthe approximation error vanishes with data size, making our algorithm\nasymptotically exact, a property of importance for large-scale topic models. In\naddition, we show, via an explicit example, that - contrary to popular belief\nin the topic modeling literature - partially collapsed samplers can be more\nefficient than fully collapsed samplers. We conclude by comparing the\nperformance of our algorithm with that of other approaches on well-known\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 01:02:27 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 20:00:08 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 12:48:23 GMT"}, {"version": "v4", "created": "Sun, 10 Jun 2018 22:10:28 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 12:39:06 GMT"}, {"version": "v6", "created": "Fri, 3 Aug 2018 13:07:04 GMT"}, {"version": "v7", "created": "Thu, 22 Oct 2020 16:14:22 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Terenin", "Alexander", ""], ["Magnusson", "M\u00e5ns", ""], ["Jonsson", "Leif", ""], ["Draper", "David", ""]]}, {"id": "1704.03626", "submitter": "Shinnosuke Takamichi", "authors": "Shinnosuke Takamichi, Tomoki Koriyama, Hiroshi Saruwatari", "title": "Sampling-based speech parameter generation using moment-matching\n  networks", "comments": "Submitted to INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents sampling-based speech parameter generation using\nmoment-matching networks for Deep Neural Network (DNN)-based speech synthesis.\nAlthough people never produce exactly the same speech even if we try to express\nthe same linguistic and para-linguistic information, typical statistical speech\nsynthesis produces completely the same speech, i.e., there is no\ninter-utterance variation in synthetic speech. To give synthetic speech natural\ninter-utterance variation, this paper builds DNN acoustic models that make it\npossible to randomly sample speech parameters. The DNNs are trained so that\nthey make the moments of generated speech parameters close to those of natural\nspeech parameters. Since the variation of speech parameters is compressed into\na low-dimensional simple prior noise vector, our algorithm has lower\ncomputation cost than direct sampling of speech parameters. As the first step\ntowards generating synthetic speech that has natural inter-utterance variation,\nthis paper investigates whether or not the proposed sampling-based generation\ndeteriorates synthetic speech quality. In evaluation, we compare speech quality\nof conventional maximum likelihood-based generation and proposed sampling-based\ngeneration. The result demonstrates the proposed generation causes no\ndegradation in speech quality.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 05:46:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Takamichi", "Shinnosuke", ""], ["Koriyama", "Tomoki", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1704.03636", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Philipp Grohs and Helmut B\\\"olcskei", "title": "Energy Propagation in Deep Convolutional Neural Networks", "comments": "Corrected errors in arguments on the spectral decay of Sobolev\n  functions and on the volume of tubes, IEEE Transactions on Information\n  Theory, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical machine learning tasks employ very deep convolutional neural\nnetworks. Such large depths pose formidable computational challenges in\ntraining and operating the network. It is therefore important to understand how\nfast the energy contained in the propagated signals (a.k.a. feature maps)\ndecays across layers. In addition, it is desirable that the feature extractor\ngenerated by the network be informative in the sense of the only signal mapping\nto the all-zeros feature vector being the zero input signal. This \"trivial\nnull-set\" property can be accomplished by asking for \"energy conservation\" in\nthe sense of the energy in the feature vector being proportional to that of the\ncorresponding input signal. This paper establishes conditions for energy\nconservation (and thus for a trivial null-set) for a wide class of deep\nconvolutional neural network-based feature extractors and characterizes\ncorresponding feature map energy decay rates. Specifically, we consider general\nscattering networks employing the modulus non-linearity and we find that under\nmild analyticity and high-pass conditions on the filters (which encompass,\ninter alia, various constructions of Weyl-Heisenberg filters, wavelets,\nridgelets, ($\\alpha$)-curvelets, and shearlets) the feature map energy decays\nat least polynomially fast. For broad families of wavelets and Weyl-Heisenberg\nfilters, the guaranteed decay rate is shown to be exponential. Moreover, we\nprovide handy estimates of the number of layers needed to have at least\n$((1-\\varepsilon)\\cdot 100)\\%$ of the input signal energy be contained in the\nfeature vector.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 06:27:20 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:04:58 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 08:31:13 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1704.03639", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Lin Ma, and Xuezhi Tan", "title": "Joint Semi-supervised RSS Dimensionality Reduction and Fingerprint Based\n  Algorithm for Indoor Localization", "comments": "14 figures. Institute of Navigation (ION GNSS+ 2014), 27th\n  International Technical Meeting of The Satellite Division Conference on", "journal-ref": "Zhou, C.F. et al., 2016. Joint Semi-supervised RSS Dimensionality\n  Reduction and Fingerprint Based Algorithm for Indoor Localization. ,\n  (SEPTEMBER 2014)", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent development in mobile computing devices and as the ubiquitous\ndeployment of access points(APs) of Wireless Local Area Networks(WLANs), WLAN\nbased indoor localization systems(WILSs) are of mounting concentration and are\nbecoming more and more prevalent for they do not require additional\ninfrastructure. As to the localization methods in WILSs, for the approaches\nused to localization in satellite based global position systems are difficult\nto achieve in indoor environments, fingerprint based localization\nalgorithms(FLAs) are predominant in the RSS based schemes. However, the\nperformance of FLAs has close relationship with the number of APs and the\nnumber of reference points(RPs) in WILSs, especially as the redundant\ndeployment of APs and RPs in the system. There are two fatal problems, curse of\ndimensionality (CoD) and asymmetric matching(AM), caused by increasing number\nof APs and breaking down APs during online stage. In this paper, a\nsemi-supervised RSS dimensionality reduction algorithm is proposed to solve\nthese two dilemmas at the same time and there are numerous analyses about the\ntheoretical realization of the proposed method. Another significant innovation\nof this paper is jointing the fingerprint based algorithm with CM-SDE algorithm\nto improve the localization accuracy of indoor localization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 06:35:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhou", "Caifa", ""], ["Ma", "Lin", ""], ["Tan", "Xuezhi", ""]]}, {"id": "1704.03651", "submitter": "Javier Gonz\\'alez", "authors": "Javier Gonzalez and Zhenwen Dai and Andreas Damianou and Neil D.\n  Lawrence", "title": "Preferential Bayesian Optimization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) has emerged during the last few years as an\neffective approach to optimizing black-box functions where direct queries of\nthe objective are expensive. In this paper we consider the case where direct\naccess to the function is not possible, but information about user preferences\nis. Such scenarios arise in problems where human preferences are modeled, such\nas A/B tests or recommender systems. We present a new framework for this\nscenario that we call Preferential Bayesian Optimization (PBO) which allows us\nto find the optimum of a latent function that can only be queried through\npairwise comparisons, the so-called duels. PBO extends the applicability of\nstandard BO ideas and generalizes previous discrete dueling approaches by\nmodeling the probability of the winner of each duel by means of a Gaussian\nprocess model with a Bernoulli likelihood. The latent preference function is\nused to define a family of acquisition functions that extend usual policies\nused in BO. We illustrate the benefits of PBO in a variety of experiments,\nshowing that PBO needs drastically fewer comparisons for finding the optimum.\nAccording to our experiments, the way of modeling correlations in PBO is key in\nobtaining this advantage.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 07:49:54 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Gonzalez", "Javier", ""], ["Dai", "Zhenwen", ""], ["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1704.03711", "submitter": "Dorian Cazau", "authors": "D. Cazau, G. Nuel", "title": "Investigation on the use of Hidden-Markov Models in automatic\n  transcription of music", "comments": "arXiv admin note: text overlap with arXiv:1703.09772", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) are a ubiquitous tool to model time series data,\nand have been widely used in two main tasks of Automatic Music Transcription\n(AMT): note segmentation, i.e. identifying the played notes after a multi-pitch\nestimation, and sequential post-processing, i.e. correcting note segmentation\nusing training data. In this paper, we employ the multi-pitch estimation method\ncalled Probabilistic Latent Component Analysis (PLCA), and develop AMT systems\nby integrating different HMM-based modules in this framework. For note\nsegmentation, we use two different twostate on/o? HMMs, including a\nhigher-order one for duration modeling. For sequential post-processing, we\nfocused on a musicological modeling of polyphonic harmonic transitions, using a\nfirst- and second-order HMMs whose states are defined through candidate note\nmixtures. These different PLCA plus HMM systems have been evaluated\ncomparatively on two different instrument repertoires, namely the piano (using\nthe MAPS database) and the marovany zither. Our results show that the use of\nHMMs could bring noticeable improvements to transcription results, depending on\nthe instrument repertoire.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 11:20:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cazau", "D.", ""], ["Nuel", "G.", ""]]}, {"id": "1704.03743", "submitter": "Giles Tetteh", "authors": "Giles Tetteh, Markus Rempfler, Bjoern H. Menze, Claus Zimmer", "title": "Deep-FExt: Deep Feature Extraction for Vessel Segmentation and\n  Centerline Prediction", "comments": "9 pages", "journal-ref": "Wang Q., Shi Y., Suk HI., Suzuki K. (eds) Machine Learning in\n  Medical Imaging. MLMI 2017. Lecture Notes in Computer Science, vol 10541.\n  Springer, Cham", "doi": "10.1007/978-3-319-67389-9_40", "report-no": "pp 344-352", "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a very crucial task in image and pixel (voxel)\nclassification and regression in biomedical image modeling. In this work we\npresent a machine learning based feature extraction scheme based on inception\nmodels for pixel classification tasks. We extract features under multi-scale\nand multi-layer schemes through convolutional operators. Layers of Fully\nConvolutional Network are later stacked on this feature extraction layers and\ntrained end-to-end for the purpose of classification. We test our model on the\nDRIVE and STARE public data sets for the purpose of segmentation and centerline\ndetection and it out performs most existing hand crafted or deterministic\nfeature schemes found in literature. We achieve an average maximum Dice of 0.85\non the DRIVE data set which out performs the scores from the second human\nannotator of this data set. We also achieve an average maximum Dice of 0.85 and\nkappa of 0.84 on the STARE data set. Though these datasets are mainly 2-D we\nalso propose ways of extending this feature extraction scheme to handle 3-D\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:10:20 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Tetteh", "Giles", ""], ["Rempfler", "Markus", ""], ["Menze", "Bjoern H.", ""], ["Zimmer", "Claus", ""]]}, {"id": "1704.03754", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis", "title": "A Proof of Orthogonal Double Machine Learning with $Z$-Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two stage estimation with a non-parametric first stage and a\ngeneralized method of moments second stage, in a simpler setting than\n(Chernozhukov et al. 2016). We give an alternative proof of the theorem given\nin (Chernozhukov et al. 2016) that orthogonal second stage moments, sample\nsplitting and $n^{1/4}$-consistency of the first stage, imply\n$\\sqrt{n}$-consistency and asymptotic normality of second stage estimates. Our\nproof is for a variant of their estimator, which is based on the empirical\nversion of the moment condition (Z-estimator), rather than a minimization of a\nnorm of the empirical vector of moments (M-estimator). This note is meant\nprimarily for expository purposes, rather than as a new technical contribution.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:34:56 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:37:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Syrgkanis", "Vasilis", ""]]}, {"id": "1704.03817", "submitter": "Ruohan Wang", "authors": "Ruohan Wang, Antoine Cully, Hyung Jin Chang, Yiannis Demiris", "title": "MAGAN: Margin Adaptation for Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs)\nalgorithm, a novel training procedure for GANs to improve stability and\nperformance by using an adaptive hinge loss function. We estimate the\nappropriate hinge loss margin with the expected energy of the target\ndistribution, and derive principled criteria for when to update the margin. We\nprove that our method converges to its global optimum under certain\nassumptions. Evaluated on the task of unsupervised image generation, the\nproposed training procedure is simple yet robust on a diverse set of data, and\nachieves qualitative and quantitative improvements compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 16:15:38 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 20:58:28 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 12:37:36 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Ruohan", ""], ["Cully", "Antoine", ""], ["Chang", "Hyung Jin", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1704.03844", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Renato L. F. Cunha, Evandro Caldeira, Luciana Fujii", "title": "Determining Song Similarity via Machine Learning Techniques and Tagging\n  Information", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of determining item similarity is a crucial one in a recommender\nsystem. This constitutes the base upon which the recommender system will work\nto determine which items are more likely to be enjoyed by a user, resulting in\nmore user engagement. In this paper we tackle the problem of determining song\nsimilarity based solely on song metadata (such as the performer, and song\ntitle) and on tags contributed by users. We evaluate our approach under a\nseries of different machine learning algorithms. We conclude that tf-idf\nachieves better results than Word2Vec to model the dataset to feature vectors.\nWe also conclude that k-NN models have better performance than SVMs and Linear\nRegression for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:07:51 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Cunha", "Renato L. F.", ""], ["Caldeira", "Evandro", ""], ["Fujii", "Luciana", ""]]}, {"id": "1704.03866", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robustly Learning a Gaussian: Getting Optimal Error, Efficiently", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of learning the parameters of a\nhigh-dimensional Gaussian in the presence of noise -- where an\n$\\varepsilon$-fraction of our samples were chosen by an adversary. We give\nrobust estimators that achieve estimation error $O(\\varepsilon)$ in the total\nvariation distance, which is optimal up to a universal constant that is\nindependent of the dimension.\n  In the case where just the mean is unknown, our robustness guarantee is\noptimal up to a factor of $\\sqrt{2}$ and the running time is polynomial in $d$\nand $1/\\epsilon$. When both the mean and covariance are unknown, the running\ntime is polynomial in $d$ and quasipolynomial in $1/\\varepsilon$. Moreover all\nof our algorithms require only a polynomial number of samples. Our work shows\nthat the same sorts of error guarantees that were established over fifty years\nago in the one-dimensional setting can also be achieved by efficient algorithms\nin high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:55:05 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:52:55 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1704.03913", "submitter": "Austin Benson", "authors": "Hao Yin, Austin R. Benson, Jure Leskovec", "title": "Higher-order clustering in networks", "comments": null, "journal-ref": "Phys. Rev. E 97, 052306 (2018)", "doi": "10.1103/PhysRevE.97.052306", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental property of complex networks is the tendency for edges to\ncluster. The extent of the clustering is typically quantified by the clustering\ncoefficient, which is the probability that a length-2 path is closed, i.e.,\ninduces a triangle in the network. However, higher-order cliques beyond\ntriangles are crucial to understanding complex networks, and the clustering\nbehavior with respect to such higher-order network structures is not well\nunderstood. Here we introduce higher-order clustering coefficients that measure\nthe closure probability of higher-order network cliques and provide a more\ncomprehensive view of how the edges of complex networks cluster. Our\nhigher-order clustering coefficients are a natural generalization of the\ntraditional clustering coefficient. We derive several properties about\nhigher-order clustering coefficients and analyze them under common random graph\nmodels. Finally, we use higher-order clustering coefficients to gain new\ninsights into the structure of real-world networks from several domains.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 19:48:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 03:35:00 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yin", "Hao", ""], ["Benson", "Austin R.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1704.03925", "submitter": "Chong You", "authors": "Chong You and Daniel P. Robinson and Ren\\'e Vidal", "title": "Provable Self-Representation Based Outlier Detection in a Union of\n  Subspaces", "comments": "16 pages. CVPR 2017 spotlight oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision tasks involve processing large amounts of data\ncontaminated by outliers, which need to be detected and rejected. While outlier\ndetection methods based on robust statistics have existed for decades, only\nrecently have methods based on sparse and low-rank representation been\ndeveloped along with guarantees of correct outlier detection when the inliers\nlie in one or more low-dimensional subspaces. This paper proposes a new outlier\ndetection method that combines tools from sparse representation with random\nwalks on a graph. By exploiting the property that data points can be expressed\nas sparse linear combinations of each other, we obtain an asymmetric affinity\nmatrix among data points, which we use to construct a weighted directed graph.\nBy defining a suitable Markov Chain from this graph, we establish a connection\nbetween inliers/outliers and essential/inessential states of the Markov chain,\nwhich allows us to detect outliers by using random walks. We provide a\ntheoretical analysis that justifies the correctness of our method under\ngeometric and connectivity assumptions. Experimental results on image databases\ndemonstrate its superiority with respect to state-of-the-art sparse and\nlow-rank outlier detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:45:48 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["You", "Chong", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1704.03926", "submitter": "Bence Cserna", "authors": "Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml", "title": "Value Directed Exploration in Multi-Armed Bandits with Structured Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits are a quintessential machine learning problem requiring\nthe balancing of exploration and exploitation. While there has been progress in\ndeveloping algorithms with strong theoretical guarantees, there has been less\nfocus on practical near-optimal finite-time performance. In this paper, we\npropose an algorithm for Bayesian multi-armed bandits that utilizes\nvalue-function-driven online planning techniques. Building on previous work on\nUCB and Gittins index, we introduce linearly-separable value functions that\ntake both the expected return and the benefit of exploration into consideration\nto perform n-step lookahead. The algorithm enjoys a sub-linear performance\nguarantee and we present simulation results that confirm its strength in\nproblems with structured priors. The simplicity and generality of our approach\nmakes it a strong candidate for analyzing more complex multi-armed bandit\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:46:50 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 14:02:27 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Cserna", "Bence", ""], ["Petrik", "Marek", ""], ["Russel", "Reazul Hasan", ""], ["Ruml", "Wheeler", ""]]}, {"id": "1704.03942", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Beyond Uniform Priors in Bayesian Network Structure Learning", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is often performed in a Bayesian setting,\nevaluating candidate structures using their posterior probabilities for a given\ndata set. Score-based algorithms then use those posterior probabilities as an\nobjective function and return the maximum a posteriori network as the learned\nmodel. For discrete Bayesian networks, the canonical choice for a posterior\nscore is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood\nwith a uniform (U) graph prior, which assumes a uniform prior both on the\nnetwork structures and on the parameters of the networks. In this paper, we\ninvestigate the problems arising from these assumptions, focusing on those\ncaused by small sample sizes and sparse data. We then propose an alternative\nposterior score: the Bayesian Dirichlet sparse (BDs) marginal likelihood with a\nmarginal uniform (MU) graph prior. Like U+BDeu, MU+BDs does not require any\nprior information on the probabilistic structure of the data and can be used as\na replacement noninformative score. We study its theoretical properties and we\nevaluate its performance in an extensive simulation study, showing that MU+BDs\nis both more accurate than U+BDeu in learning the structure of the network and\ncompetitive in predicting power, while not being computationally more complex\nto estimate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 22:01:09 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1704.03944", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, Honglak\n  Lee", "title": "Discriminative Bimodal Networks for Visual Localization and Detection\n  with Natural Language Queries", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating image regions with text queries has been recently explored as a\nnew way to bridge visual and linguistic representations. A few pioneering\napproaches have been proposed based on recurrent neural language models trained\ngeneratively (e.g., generating captions), but achieving somewhat limited\nlocalization accuracy. To better address natural-language-based visual entity\nlocalization, we propose a discriminative approach. We formulate a\ndiscriminative bimodal neural network (DBNet), which can be trained by a\nclassifier with extensive use of negative samples. Our training objective\nencourages better localization on single images, incorporates text phrases in a\nbroad range, and properly pairs image regions with text phrases into positive\nand negative examples. Experiments on the Visual Genome dataset demonstrate the\nproposed DBNet significantly outperforms previous state-of-the-art methods both\nfor localization on single images and for detection on multiple images. We we\nalso establish an evaluation protocol for natural-language visual detection.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 22:09:36 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 07:22:14 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Zhang", "Yuting", ""], ["Yuan", "Luyao", ""], ["Guo", "Yijie", ""], ["He", "Zhiyuan", ""], ["Huang", "I-An", ""], ["Lee", "Honglak", ""]]}, {"id": "1704.03971", "submitter": "Sitao Xiang", "authors": "Sitao Xiang, Hao Li", "title": "On the Effects of Batch and Weight Normalization in Generative\n  Adversarial Networks", "comments": "v3 rejected by NIPS 2017, updated and re-submitted to CVPR 2018. v4:\n  add experiments with ResNet and like to new code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are highly effective unsupervised\nlearning frameworks that can generate very sharp data, even for data such as\nimages with complex, highly multimodal distributions. However GANs are known to\nbe very hard to train, suffering from problems such as mode collapse and\ndisturbing visual artifacts. Batch normalization (BN) techniques have been\nintroduced to address the training. Though BN accelerates the training in the\nbeginning, our experiments show that the use of BN can be unstable and\nnegatively impact the quality of the trained model. The evaluation of BN and\nnumerous other recent schemes for improving GAN training is hindered by the\nlack of an effective objective quality measure for GAN models. To address these\nissues, we first introduce a weight normalization (WN) approach for GAN\ntraining that significantly improves the stability, efficiency and the quality\nof the generated samples. To allow a methodical evaluation, we introduce\nsquared Euclidean reconstruction error on a test set as a new objective\nmeasure, to assess training performance in terms of speed, stability, and\nquality of generated samples. Our experiments with a standard DCGAN\narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)\nindicate that training using WN is generally superior to BN for GANs, achieving\n10% lower mean squared loss for reconstruction and significantly better\nqualitative results than BN. We further demonstrate the stability of WN on a\n21-layer ResNet trained with the CelebA data set. The code for this paper is\navailable at https://github.com/stormraiser/gan-weightnorm-resnet\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:15:28 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 02:28:03 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 07:16:59 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 01:56:42 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Xiang", "Sitao", ""], ["Li", "Hao", ""]]}, {"id": "1704.03976", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii", "title": "Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning", "comments": "To be appeared in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:45:27 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 04:52:47 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Miyato", "Takeru", ""], ["Maeda", "Shin-ichi", ""], ["Koyama", "Masanori", ""], ["Ishii", "Shin", ""]]}, {"id": "1704.04010", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan", "title": "ZigZag: A new approach to adaptive online learning", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel family of algorithms for the online learning setting with\nregret against any data sequence bounded by the empirical Rademacher complexity\nof that sequence. To develop a general theory of when this type of adaptive\nregret bound is achievable we establish a connection to the theory of\ndecoupling inequalities for martingales in Banach spaces. When the hypothesis\nclass is a set of linear functions bounded in some norm, such a regret bound is\nachievable if and only if the norm satisfies certain decoupling inequalities\nfor martingales. Donald Burkholder's celebrated geometric characterization of\ndecoupling inequalities (1984) states that such an inequality holds if and only\nif there exists a special function called a Burkholder function satisfying\ncertain restricted concavity properties. Our online learning algorithms are\nefficient in terms of queries to this function.\n  We realize our general theory by giving novel efficient algorithms for\nclasses including lp norms, Schatten p-norms, group norms, and reproducing\nkernel Hilbert spaces. The empirical Rademacher complexity regret bound implies\n--- when used in the i.i.d. setting --- a data-dependent complexity bound for\nexcess risk after online-to-batch conversion. To showcase the power of the\nempirical Rademacher complexity regret bound, we derive improved rates for a\nsupervised learning generalization of the online learning with low rank experts\ntask and for the online matrix prediction task.\n  In addition to obtaining tight data-dependent regret bounds, our algorithms\nenjoy improved efficiency over previous techniques based on Rademacher\ncomplexity, automatically work in the infinite horizon setting, and are\nscale-free. To obtain such adaptive methods, we introduce novel machinery, and\nthe resulting algorithms are not based on the standard tools of online convex\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 06:50:34 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1704.04031", "submitter": "Matthew Pearce", "authors": "Matthew C. Pearce and Simon R. White", "title": "Infinite Sparse Structured Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorisation methods decompose multivariate observations as linear\ncombinations of latent feature vectors. The Indian Buffet Process (IBP)\nprovides a way to model the number of latent features required for a good\napproximation in terms of regularised reconstruction error. Previous work has\nfocussed on latent feature vectors with independent entries. We extend the\nmodel to include nondiagonal latent covariance structures representing\ncharacteristics such as smoothness. This is done by . Using simulations we\ndemonstrate that under appropriate conditions a smoothness prior helps to\nrecover the true latent features, while denoising more accurately. We\ndemonstrate our method on a real neuroimaging dataset, where computational\ntractability is a sufficient challenge that the efficient strategy presented\nhere is essential.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 08:37:56 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Pearce", "Matthew C.", ""], ["White", "Simon R.", ""]]}, {"id": "1704.04039", "submitter": "Vladimir Golkov", "authors": "Vladimir Golkov, Marcin J. Skwark, Atanas Mirchev, Georgi Dikov,\n  Alexander R. Geanes, Jeffrey Mendenhall, Jens Meiler and Daniel Cremers", "title": "3D Deep Learning for Biological Function Prediction from Physical Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the biological function of molecules, be it proteins or drug-like\ncompounds, from their atomic structure is an important and long-standing\nproblem. Function is dictated by structure, since it is by spatial interactions\nthat molecules interact with each other, both in terms of steric\ncomplementarity, as well as intermolecular forces. Thus, the electron density\nfield and electrostatic potential field of a molecule contain the \"raw\nfingerprint\" of how this molecule can fit to binding partners. In this paper,\nwe show that deep learning can predict biological function of molecules\ndirectly from their raw 3D approximated electron density and electrostatic\npotential fields. Protein function based on EC numbers is predicted from the\napproximated electron density field. In another experiment, the activity of\nsmall molecules is predicted with quality comparable to state-of-the-art\ndescriptor-based methods. We propose several alternative computational models\nfor the GPU with different memory and runtime requirements for different sizes\nof molecules and of databases. We also propose application-specific\nmulti-channel data representations. With future improvements of training\ndatasets and neural network settings in combination with complementary\ninformation sources (sequence, genomic context, expression level), deep\nlearning can be expected to show its generalization power and revolutionize the\nfield of molecular function prediction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:11:23 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Golkov", "Vladimir", ""], ["Skwark", "Marcin J.", ""], ["Mirchev", "Atanas", ""], ["Dikov", "Georgi", ""], ["Geanes", "Alexander R.", ""], ["Mendenhall", "Jeffrey", ""], ["Meiler", "Jens", ""], ["Cremers", "Daniel", ""]]}, {"id": "1704.04050", "submitter": "Caifa Zhou", "authors": "Lin Ma, Caifa Zhou, Xi Liu, Yubin Xu", "title": "Adaptive Neighboring Selection Algorithm Based on Curvature Prediction\n  in Manifold Learning", "comments": "3 figures, from Journal of Harbin Institute of Technology", "journal-ref": "Journal of Harbin Institute of Technology, 20(3), pp.119--123\n  (2013)", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently manifold learning algorithm for dimensionality reduction attracts\nmore and more interests, and various linear and nonlinear, global and local\nalgorithms are proposed. The key step of manifold learning algorithm is the\nneighboring region selection. However, so far for the references we know, few\nof which propose a generally accepted algorithm to well select the neighboring\nregion. So in this paper, we propose an adaptive neighboring selection\nalgorithm, which successfully applies the LLE and ISOMAP algorithms in the\ntest. It is an algorithm that can find the optimal K nearest neighbors of the\ndata points on the manifold. And the theoretical basis of the algorithm is the\napproximated curvature of the data point on the manifold. Based on Riemann\nGeometry, Jacob matrix is a proper mathematical concept to predict the\napproximated curvature. By verifying the proposed algorithm on embedding Swiss\nroll from R3 to R2 based on LLE and ISOMAP algorithm, the simulation results\nshow that the proposed adaptive neighboring selection algorithm is feasible and\nable to find the optimal value of K, making the residual variance relatively\nsmall and better visualization of the results. By quantitative analysis, the\nembedding quality measured by residual variance is increased 45.45% after using\nthe proposed algorithm in LLE.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:33:56 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ma", "Lin", ""], ["Zhou", "Caifa", ""], ["Liu", "Xi", ""], ["Xu", "Yubin", ""]]}, {"id": "1704.04110", "submitter": "David Salinas", "authors": "David Salinas, Valentin Flunkert, Jan Gasthaus", "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting, i.e. estimating the probability distribution of a\ntime series' future given its past, is a key enabler for optimizing business\nprocesses. In retail businesses, for example, forecasting demand is crucial for\nhaving the right inventory available at the right time at the right place. In\nthis paper we propose DeepAR, a methodology for producing accurate\nprobabilistic forecasts, based on training an auto regressive recurrent network\nmodel on a large number of related time series. We demonstrate how by applying\ndeep learning techniques to forecasting, one can overcome many of the\nchallenges faced by widely-used classical approaches to the problem. We show\nthrough extensive empirical evaluation on several real-world forecasting data\nsets accuracy improvements of around 15% compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:11:53 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:39:14 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 13:43:50 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Salinas", "David", ""], ["Flunkert", "Valentin", ""], ["Gasthaus", "Jan", ""]]}, {"id": "1704.04137", "submitter": "Yu-I Ha", "authors": "Yu-I Ha, Sejeong Kwon, Meeyoung Cha and Jungseock Joo", "title": "Fashion Conversation Data on Instagram", "comments": "10 pages, 6 figures, This paper will be presented at ICWSM'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fashion industry is establishing its presence on a number of\nvisual-centric social media like Instagram. This creates an interesting clash\nas fashion brands that have traditionally practiced highly creative and\neditorialized image marketing now have to engage with people on the platform\nthat epitomizes impromptu, realtime conversation. What kinds of fashion images\ndo brands and individuals share and what are the types of visual features that\nattract likes and comments? In this research, we take both quantitative and\nqualitative approaches to answer these questions. We analyze visual features of\nfashion posts first via manual tagging and then via training on convolutional\nneural networks. The classified images were examined across four types of\nfashion brands: mega couture, small couture, designers, and high street. We\nfind that while product-only images make up the majority of fashion\nconversation in terms of volume, body snaps and face images that portray\nfashion items more naturally tend to receive a larger number of likes and\ncomments by the audience. Our findings bring insights into building an\nautomated tool for classifying or generating influential fashion information.\nWe make our novel dataset of {24,752} labeled images on fashion conversations,\ncontaining visual and textual cues, available for the research community.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 13:49:50 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ha", "Yu-I", ""], ["Kwon", "Sejeong", ""], ["Cha", "Meeyoung", ""], ["Joo", "Jungseock", ""]]}, {"id": "1704.04222", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, James Glass", "title": "Learning Latent Representations for Speech Generation and Transformation", "comments": "Accepted to Interspeech 2017", "journal-ref": "Interspeech 2017, pp 1273-1277", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to model a generative process and learn a latent representation\nfor speech in an unsupervised fashion will be crucial to process vast\nquantities of unlabelled speech data. Recently, deep probabilistic generative\nmodels such as Variational Autoencoders (VAEs) have achieved tremendous success\nin modeling natural images. In this paper, we apply a convolutional VAE to\nmodel the generative process of natural speech. We derive latent space\narithmetic operations to disentangle learned latent representations. We\ndemonstrate the capability of our model to modify the phonetic content or the\nspeaker identity for speech segments using the derived operations, without the\nneed for parallel supervisory data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 17:41:11 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:41:54 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1704.04235", "submitter": "Xiaofang Wang", "authors": "Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Chao Wang, Yuxing Tang,\n  Liming Chen", "title": "Close Yet Distinctive Domain Adaptation", "comments": "11pages, 3 figures, ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is transfer learning which aims to generalize a learning\nmodel across training and testing data with different distributions. Most\nprevious research tackle this problem in seeking a shared feature\nrepresentation between source and target domains while reducing the mismatch of\ntheir data distributions. In this paper, we propose a close yet discriminative\ndomain adaptation method, namely CDDA, which generates a latent feature\nrepresentation with two interesting properties. First, the discrepancy between\nthe source and target domain, measured in terms of both marginal and\nconditional probability distribution via Maximum Mean Discrepancy is minimized\nso as to attract two domains close to each other. More importantly, we also\ndesign a repulsive force term, which maximizes the distances between each label\ndependent sub-domain to all others so as to drag different class dependent\nsub-domains far away from each other and thereby increase the discriminative\npower of the adapted domain. Moreover, given the fact that the underlying data\nmanifold could have complex geometric structure, we further propose the\nconstraints of label smoothness and geometric structure consistency for label\npropagation. Extensive experiments are conducted on 36 cross-domain image\nclassification tasks over four public datasets. The comprehensive results show\nthat the proposed method consistently outperforms the state-of-the-art methods\nwith significant margins.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 08:30:21 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Luo", "Lingkun", ""], ["Wang", "Xiaofang", ""], ["Hu", "Shiqiang", ""], ["Wang", "Chao", ""], ["Tang", "Yuxing", ""], ["Chen", "Liming", ""]]}, {"id": "1704.04285", "submitter": "Edward Cheung", "authors": "Edward Cheung and Yuying Li", "title": "Projection Free Rank-Drop Steps", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2017/213", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe (FW) algorithm has been widely used in solving nuclear norm\nconstrained problems, since it does not require projections. However, FW often\nyields high rank intermediate iterates, which can be very expensive in time and\nspace costs for large problems. To address this issue, we propose a rank-drop\nmethod for nuclear norm constrained problems. The goal is to generate descent\nsteps that lead to rank decreases, maintaining low-rank solutions throughout\nthe algorithm. Moreover, the optimization problems are constrained to ensure\nthat the rank-drop step is also feasible and can be readily incorporated into a\nprojection-free minimization method, e.g., Frank-Wolfe. We demonstrate that by\nincorporating rank-drop steps into the Frank-Wolfe algorithm, the rank of the\nsolution is greatly reduced compared to the original Frank-Wolfe or its common\nvariants.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 22:10:23 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 17:05:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cheung", "Edward", ""], ["Li", "Yuying", ""]]}, {"id": "1704.04289", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei", "title": "Stochastic Gradient Descent as Approximate Bayesian Inference", "comments": "35 pages, published version (JMLR 2017)", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-35", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent with a constant learning rate (constant SGD)\nsimulates a Markov chain with a stationary distribution. With this perspective,\nwe derive several new results. (1) We show that constant SGD can be used as an\napproximate Bayesian posterior inference algorithm. Specifically, we show how\nto adjust the tuning parameters of constant SGD to best match the stationary\ndistribution to a posterior, minimizing the Kullback-Leibler divergence between\nthese two distributions. (2) We demonstrate that constant SGD gives rise to a\nnew variational EM algorithm that optimizes hyperparameters in complex\nprobabilistic models. (3) We also propose SGD with momentum for sampling and\nshow how to adjust the damping coefficient accordingly. (4) We analyze MCMC\nalgorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we\nquantify the approximation errors due to finite learning rates. Finally (5), we\nuse the stochastic process perspective to give a short proof of why Polyak\naveraging is optimal. Based on this idea, we propose a scalable approximate\nMCMC algorithm, the Averaged Stochastic Gradient Sampler.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 22:17:30 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 21:07:09 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Mandt", "Stephan", ""], ["Hoffman", "Matthew D.", ""], ["Blei", "David M.", ""]]}, {"id": "1704.04333", "submitter": "Yuxin Peng", "authors": "Jinwei Qi, Xin Huang, and Yuxin Peng", "title": "Cross-media Similarity Metric Learning with Unified Deep Networks", "comments": "19 pages, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highlighting research topic in the multimedia area, cross-media\nretrieval aims to capture the complex correlations among multiple media types.\nLearning better shared representation and distance metric for multimedia data\nis important to boost the cross-media retrieval. Motivated by the strong\nability of deep neural network in feature representation and comparison\nfunctions learning, we propose the Unified Network for Cross-media Similarity\nMetric (UNCSM) to associate cross-media shared representation learning with\ndistance metric in a unified framework. First, we design a two-pathway deep\nnetwork pretrained with contrastive loss, and employ double triplet similarity\nloss for fine-tuning to learn the shared representation for each media type by\nmodeling the relative semantic similarity. Second, the metric network is\ndesigned for effectively calculating the cross-media similarity of the shared\nrepresentation, by modeling the pairwise similar and dissimilar constraints.\nCompared to the existing methods which mostly ignore the dissimilar constraints\nand only use sample distance metric as Euclidean distance separately, our UNCSM\napproach unifies the representation learning and distance metric to preserve\nthe relative similarity as well as embrace more complex similarity functions\nfor further improving the cross-media retrieval accuracy. The experimental\nresults show that our UNCSM approach outperforms 8 state-of-the-art methods on\n4 widely-used cross-media datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:25:50 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Qi", "Jinwei", ""], ["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1704.04375", "submitter": "Constantino Antonio Garc\\'ia", "authors": "Constantino A. Garc\\'ia, Abraham Otero, Paulo F\\'elix, Jes\\'us\n  Presedo, David G. M\\'arquez", "title": "Non-parametric Estimation of Stochastic Differential Equations with\n  Sparse Gaussian Processes", "comments": null, "journal-ref": "Phys. Rev. E 96, 022104 (2017)", "doi": "10.1103/PhysRevE.96.022104", "report-no": null, "categories": "stat.ML physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Stochastic Differential Equations (SDEs) to the analysis\nof temporal data has attracted increasing attention, due to their ability to\ndescribe complex dynamics with physically interpretable equations. In this\npaper, we introduce a non-parametric method for estimating the drift and\ndiffusion terms of SDEs from a densely observed discrete time series. The use\nof Gaussian processes as priors permits working directly in a function-space\nview and thus the inference takes place directly in this space. To cope with\nthe computational complexity that requires the use of Gaussian processes, a\nsparse Gaussian process approximation is provided. This approximation permits\nthe efficient computation of predictions for the drift and diffusion terms by\nusing a distribution over a small subset of pseudo-samples. The proposed method\nhas been validated using both simulated data and real data from economy and\npaleoclimatology. The application of the method to real data demonstrates its\nability to capture the behaviour of complex systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 09:46:18 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 11:00:27 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Garc\u00eda", "Constantino A.", ""], ["Otero", "Abraham", ""], ["F\u00e9lix", "Paulo", ""], ["Presedo", "Jes\u00fas", ""], ["M\u00e1rquez", "David G.", ""]]}, {"id": "1704.04478", "submitter": "Neil Hallonquist", "authors": "Neil Hallonquist", "title": "Graphical Models: An Extension to Random Graphs, Trees, and Other\n  Objects", "comments": "111 pages, with extended discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider an extension of graphical models to random graphs,\ntrees, and other objects. To do this, many fundamental concepts for\nmultivariate random variables (e.g., marginal variables, Gibbs distribution,\nMarkov properties) must be extended to other mathematical objects; it turns out\nthat this extension is possible, as we will discuss, if we have a consistent,\ncomplete system of projections on a given object. Each projection defines a\nmarginal random variable, allowing one to specify independence assumptions\nbetween them. Furthermore, these independencies can be specified in terms of a\nsmall subset of these marginal variables (which we call the atomic variables),\nallowing the compact representation of independencies by a directed graph.\nProjections also define factors, functions on the projected object space, and\nhence a projection family defines a set of possible factorizations for a\ndistribution; these can be compactly represented by an undirected graph.\n  The invariances used in graphical models are essential for learning\ndistributions, not just on multivariate random variables, but also on other\nobjects. When they are applied to random graphs and random trees, the result is\na general class of models that is applicable to a broad range of problems,\nincluding those in which the graphs and trees have complicated edge structures.\nThese models need not be conditioned on a fixed number of vertices, as is often\nthe case in the literature for random graphs, and can be used for problems in\nwhich attributes are associated with vertices and edges. For graphs,\napplications include the modeling of molecules, neural networks, and relational\nreal-world scenes; for trees, applications include the modeling of infectious\ndiseases, cell fusion, the structure of language, and the structure of objects\nin visual scenes. Many classic models are particular instances of this\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:38:45 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 14:13:18 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Hallonquist", "Neil", ""]]}, {"id": "1704.04548", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Ahmed El Alaoui, Benjamin Recht", "title": "On the Gap Between Strict-Saddles and True Convexity: An Omega(log d)\n  Lower Bound for Eigenvector Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.CO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a \\emph{query complexity} lower bound on rank-one principal\ncomponent analysis (PCA). We consider an oracle model where, given a symmetric\nmatrix $M \\in \\mathbb{R}^{d \\times d}$, an algorithm is allowed to make $T$\n\\emph{exact} queries of the form $w^{(i)} = Mv^{(i)}$ for $i \\in\n\\{1,\\dots,T\\}$, where $v^{(i)}$ is drawn from a distribution which depends\narbitrarily on the past queries and measurements $\\{v^{(j)},w^{(j)}\\}_{1 \\le j\n\\le i-1}$. We show that for a small constant $\\epsilon$, any adaptive,\nrandomized algorithm which can find a unit vector $\\widehat{v}$ for which\n$\\widehat{v}^{\\top}M\\widehat{v} \\ge (1-\\epsilon)\\|M\\|$, with even small\nprobability, must make $T = \\Omega(\\log d)$ queries. In addition to settling a\nwidely-held folk conjecture, this bound demonstrates a fundamental gap between\nconvex optimization and \"strict-saddle\" non-convex optimization of which PCA is\na canonical example: in the former, first-order methods can have dimension-free\niteration complexity, whereas in PCA, the iteration complexity of\ngradient-based methods must necessarily grow with the dimension. Our argument\nproceeds via a reduction to estimating the rank-one spike in a deformed Wigner\nmodel. We establish lower bounds for this model by developing a \"truncated\"\nanalogue of the $\\chi^2$ Bayes-risk lower bound of Chen et al.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 21:56:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Simchowitz", "Max", ""], ["Alaoui", "Ahmed El", ""], ["Recht", "Benjamin", ""]]}, {"id": "1704.04567", "submitter": "Jie Zhong", "authors": "Jie Zhong, Yijun Huang, Ji Liu", "title": "Asynchronous Parallel Empirical Variance Guided Algorithms for the\n  Thresholding Bandit Problem", "comments": "added lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the multi-armed thresholding bandit problem --\nidentifying all arms whose expected rewards are above a predefined threshold\nvia as few pulls (or rounds) as possible -- proposed by Locatelli et al. [2016]\nrecently. Although the proposed algorithm in Locatelli et al. [2016] achieves\nthe optimal round complexity in a certain sense, there still remain unsolved\nissues. This paper proposes an asynchronous parallel thresholding algorithm and\nits parameter-free version to improve the efficiency and the applicability. On\none hand, the proposed two algorithms use the empirical variance to guide the\npull decision at each round, and significantly improve the round complexity of\nthe \"optimal\" algorithm when all arms have bounded high order moments. The\nproposed algorithms can be proven to be optimal. On the other hand, most bandit\nalgorithms assume that the reward can be observed immediately after the pull or\nthe next decision would not be made before all rewards are observed. Our\nproposed asynchronous parallel algorithms allow making the choice of the next\npull with unobserved rewards from earlier pulls, which avoids such an\nunrealistic assumption and significantly improves the identification process.\nOur theoretical analysis justifies the effectiveness and the efficiency of\nproposed asynchronous parallel algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 02:42:30 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 00:14:13 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhong", "Jie", ""], ["Huang", "Yijun", ""], ["Liu", "Ji", ""]]}, {"id": "1704.04629", "submitter": "Luca Martino", "authors": "Luca Martino, Victor Elvira", "title": "Metropolis Sampling", "comments": "Wiley StatsRef-Statistics Reference Online, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) sampling methods are widely applied in Bayesian inference,\nsystem simulation and optimization problems. The Markov Chain Monte Carlo\n(MCMC) algorithms are a well-known class of MC methods which generate a Markov\nchain with the desired invariant distribution. In this document, we focus on\nthe Metropolis-Hastings (MH) sampler, which can be considered as the atom of\nthe MCMC techniques, introducing the basic notions and different properties. We\ndescribe in details all the elements involved in the MH algorithm and the most\nrelevant variants. Several improvements and recent extensions proposed in the\nliterature are also briefly discussed, providing a quick but exhaustive\noverview of the current Metropolis-based sampling's world.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 12:15:30 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "Victor", ""]]}, {"id": "1704.04650", "submitter": "Christian Igel", "authors": "Jan Kremer, Kristoffer Stensbo-Smidt, Fabian Gieseke, Kim Steenstrup\n  Pedersen, Christian Igel", "title": "Big Universe, Big Data: Machine Learning and Image Analysis for\n  Astronomy", "comments": null, "journal-ref": "IEEE Intelligent Systems, vol. 32, no. , pp. 16-22, Mar.-Apr. 2017", "doi": "10.1109/MIS.2017.40", "report-no": null, "categories": "astro-ph.IM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysics and cosmology are rich with data. The advent of wide-area\ndigital cameras on large aperture telescopes has led to ever more ambitious\nsurveys of the sky. Data volumes of entire surveys a decade ago can now be\nacquired in a single night and real-time analysis is often desired. Thus,\nmodern astronomy requires big data know-how, in particular it demands highly\nefficient machine learning and image analysis algorithms. But scalability is\nnot the only challenge: Astronomy applications touch several current machine\nlearning research questions, such as learning from biased data and dealing with\nlabel and measurement noise. We argue that this makes astronomy a great domain\nfor computer science research, as it pushes the boundaries of data analysis. In\nthe following, we will present this exciting application area for data\nscientists. We will focus on exemplary results, discuss main challenges, and\nhighlight some recent methodological advancements in machine learning and image\nanalysis triggered by astronomical applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 15:32:13 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kremer", "Jan", ""], ["Stensbo-Smidt", "Kristoffer", ""], ["Gieseke", "Fabian", ""], ["Pedersen", "Kim Steenstrup", ""], ["Igel", "Christian", ""]]}, {"id": "1704.04688", "submitter": "Giles Hooker", "authors": "Giles Hooker and Cliff Hooker", "title": "Machine Learning and the Future of Realism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preceding three decades have seen the emergence, rise, and proliferation\nof machine learning (ML). From half-recognised beginnings in perceptrons,\nneural nets, and decision trees, algorithms that extract correlations (that is,\npatterns) from a set of data points have broken free from their origin in\ncomputational cognition to embrace all forms of problem solving, from voice\nrecognition to medical diagnosis to automated scientific research and\ndriverless cars, and it is now widely opined that the real industrial\nrevolution lies less in mobile phone and similar than in the maturation and\nuniversal application of ML. Among the consequences just might be the triumph\nof anti-realism over realism.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 20:49:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Hooker", "Giles", ""], ["Hooker", "Cliff", ""]]}, {"id": "1704.04718", "submitter": "Xu Youjun Xu Youjun", "authors": "Youjun Xu, Jianfeng Pei, Luhua Lai", "title": "Deep Learning Based Regression and Multi-class Models for Acute Oral\n  Toxicity Prediction with Automatic Chemical Feature Extraction", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For quantitative structure-property relationship (QSPR) studies in\nchemoinformatics, it is important to get interpretable relationship between\nchemical properties and chemical features. However, the predictive power and\ninterpretability of QSPR models are usually two different objectives that are\ndifficult to achieve simultaneously. A deep learning architecture using\nmolecular graph encoding convolutional neural networks (MGE-CNN) provided a\nuniversal strategy to construct interpretable QSPR models with high predictive\npower. Instead of using application-specific preset molecular descriptors or\nfingerprints, the models can be resolved using raw and pertinent features\nwithout manual intervention or selection. In this study, we developed acute\noral toxicity (AOT) models of compounds using the MGE-CNN architecture as a\ncase study. Three types of high-level predictive models: regression model\n(deepAOT-R), multi-classification model (deepAOT-C) and multi-task model\n(deepAOT-CR) for AOT evaluation were constructed. These models highly\noutperformed previously reported models. For the two external datasets\ncontaining 1673 (test set I) and 375 (test set II) compounds, the R2 and mean\nabsolute error (MAE) of deepAOT-R on the test set I were 0.864 and 0.195, and\nthe prediction accuracy of deepAOT-C was 95.5% and 96.3% on the test set I and\nII, respectively. The two external prediction accuracy of deepAOT-CR is 95.0%\nand 94.1%, while the R2 and MAE are 0.861 and 0.204 for test set I,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 04:17:32 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 02:10:10 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 09:52:38 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Xu", "Youjun", ""], ["Pei", "Jianfeng", ""], ["Lai", "Luhua", ""]]}, {"id": "1704.04799", "submitter": "Alexander Jung", "authors": "Saeed Basirian and Alexander Jung", "title": "Random Walk Sampling for Big Data over Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown recently that graph signals with small total variation can\nbe accurately recovered from only few samples if the sampling set satisfies a\ncertain condition, referred to as the network nullspace property. Based on this\nrecovery condition, we propose a sampling strategy for smooth graph signals\nbased on random walks. Numerical experiments demonstrate the effectiveness of\nthis approach for graph signals obtained from a synthetic random graph model as\nwell as a real-world dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 17:43:38 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Basirian", "Saeed", ""], ["Jung", "Alexander", ""]]}, {"id": "1704.04812", "submitter": "Dennis Forster", "authors": "J\\\"org L\\\"ucke, Dennis Forster", "title": "$k$-means as a variational EM approximation of Gaussian mixture models", "comments": null, "journal-ref": "Pattern Recognition Letters, 125: 349-356, 2019", "doi": "10.1016/j.patrec.2019.04.001", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that $k$-means (Lloyd's algorithm) is obtained as a special case when\ntruncated variational EM approximations are applied to Gaussian Mixture Models\n(GMM) with isotropic Gaussians. In contrast to the standard way to relate\n$k$-means and GMMs, the provided derivation shows that it is not required to\nconsider Gaussians with small variances or the limit case of zero variances.\nThere are a number of consequences that directly follow from our approach: (A)\n$k$-means can be shown to increase a free energy associated with truncated\ndistributions and this free energy can directly be reformulated in terms of the\n$k$-means objective; (B) $k$-means generalizations can directly be derived by\nconsidering the 2nd closest, 3rd closest etc. cluster in addition to just the\nclosest one; and (C) the embedding of $k$-means into a free energy framework\nallows for theoretical interpretations of other $k$-means generalizations in\nthe literature. In general, truncated variational EM provides a natural and\nrigorous quantitative link between $k$-means-like clustering and GMM clustering\nalgorithms which may be very relevant for future theoretical and empirical\nstudies.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 20:06:30 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 14:30:43 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 11:15:47 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 18:18:27 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 09:37:48 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["L\u00fccke", "J\u00f6rg", ""], ["Forster", "Dennis", ""]]}, {"id": "1704.04833", "submitter": "Chendi Huang", "authors": "Chendi Huang, Xinwei Sun, Jiechao Xiong, Yuan Yao", "title": "Boosting with Structural Sparsity: A Differential Inclusion Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting as gradient descent algorithms is one popular method in machine\nlearning. In this paper a novel Boosting-type algorithm is proposed based on\nrestricted gradient descent with structural sparsity control whose underlying\ndynamics are governed by differential inclusions. In particular, we present an\niterative regularization path with structural sparsity where the parameter is\nsparse under some linear transforms, based on variable splitting and the\nLinearized Bregman Iteration. Hence it is called \\emph{Split LBI}. Despite its\nsimplicity, Split LBI outperforms the popular generalized Lasso in both theory\nand experiments. A theory of path consistency is presented that equipped with a\nproper early stopping, Split LBI may achieve model selection consistency under\na family of Irrepresentable Conditions which can be weaker than the necessary\nand sufficient condition for generalized Lasso. Furthermore, some $\\ell_2$\nerror bounds are also given at the minimax optimal rates. The utility and\nbenefit of the algorithm are illustrated by several applications including\nimage denoising, partial order ranking of sport teams, and world university\ngrouping with crowdsourced ranking data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 23:58:18 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Huang", "Chendi", ""], ["Sun", "Xinwei", ""], ["Xiong", "Jiechao", ""], ["Yao", "Yuan", ""]]}, {"id": "1704.04839", "submitter": "Li Ma", "authors": "Jacopo Soriano and Li Ma", "title": "Mixture modeling on related samples by $\\psi$-stick breaking and kernel\n  perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great interest recently in applying nonparametric kernel\nmixtures in a hierarchical manner to model multiple related data samples\njointly. In such settings several data features are commonly present: (i) the\nrelated samples often share some, if not all, of the mixture components but\nwith differing weights, (ii) only some, not all, of the mixture components vary\nacross the samples, and (iii) often the shared mixture components across\nsamples are not aligned perfectly in terms of their location and spread, but\nrather display small misalignments either due to systematic cross-sample\ndifference or more often due to uncontrolled, extraneous causes. Properly\nincorporating these features in mixture modeling will enhance the efficiency of\ninference, whereas ignoring them not only reduces efficiency but can jeopardize\nthe validity of the inference due to issues such as confounding. We introduce\ntwo techniques for incorporating these features in modeling related data\nsamples using kernel mixtures. The first technique, called $\\psi$-stick\nbreaking, is a joint generative process for the mixing weights through the\nbreaking of both a stick shared by all the samples for the components that do\nnot vary in size across samples and an idiosyncratic stick for each sample for\nthose components that do vary in size. The second technique is to imbue random\nperturbation into the kernels, thereby accounting for cross-sample\nmisalignment. These techniques can be used either separately or together in\nboth parametric and nonparametric kernel mixtures. We derive efficient Bayesian\ninference recipes based on MCMC sampling for models featuring these techniques,\nand illustrate their work through both simulated data and a real flow cytometry\ndata set in prediction/estimation, cross-sample calibration, and testing\nmulti-sample differences.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 00:58:37 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Soriano", "Jacopo", ""], ["Ma", "Li", ""]]}, {"id": "1704.04962", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Pietro Li\\'o", "title": "Bayesian Hybrid Matrix Factorisation for Data Integration", "comments": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017)", "journal-ref": "PMLR 54:557-566, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Bayesian hybrid matrix factorisation model (HMF) for\ndata integration, based on combining multiple matrix factorisation methods,\nthat can be used for in- and out-of-matrix prediction of missing values. The\nmodel is very general and can be used to integrate many datasets across\ndifferent entity types, including repeated experiments, similarity matrices,\nand very sparse datasets. We apply our method on two biological applications,\nand extensively compare it to state-of-the-art machine learning and matrix\nfactorisation models. For in-matrix predictions on drug sensitivity datasets we\nobtain consistently better performances than existing methods. This is\nespecially the case when we increase the sparsity of the datasets. Furthermore,\nwe perform out-of-matrix predictions on methylation and gene expression\ndatasets, and obtain the best results on two of the three datasets, especially\nwhen the predictivity of datasets is high.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:39:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Brouwer", "Thomas", ""], ["Li\u00f3", "Pietro", ""]]}, {"id": "1704.04966", "submitter": "Fanhua Shang", "authors": "Fanhua Shang", "title": "Larger is Better: The Effect of Learning Rates Enjoyed by Stochastic\n  Optimization with Progressive Variance Reduction", "comments": "36 pages, 10 figures. The simple variant of SVRG is much better than\n  the best-known stochastic method, Katyusha", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple variant of the original stochastic\nvariance reduction gradient (SVRG), where hereafter we refer to as the variance\nreduced stochastic gradient descent (VR-SGD). Different from the choices of the\nsnapshot point and starting point in SVRG and its proximal variant, Prox-SVRG,\nthe two vectors of each epoch in VR-SGD are set to the average and last iterate\nof the previous epoch, respectively. This setting allows us to use much larger\nlearning rates or step sizes than SVRG, e.g., 3/(7L) for VR-SGD vs 1/(10L) for\nSVRG, and also makes our convergence analysis more challenging. In fact, a\nlarger learning rate enjoyed by VR-SGD means that the variance of its\nstochastic gradient estimator asymptotically approaches zero more rapidly.\nUnlike common stochastic methods such as SVRG and proximal stochastic methods\nsuch as Prox-SVRG, we design two different update rules for smooth and\nnon-smooth objective functions, respectively. In other words, VR-SGD can tackle\nnon-smooth and/or non-strongly convex problems directly without using any\nreduction techniques such as quadratic regularizers. Moreover, we analyze the\nconvergence properties of VR-SGD for strongly convex problems, which show that\nVR-SGD attains a linear convergence rate. We also provide the convergence\nguarantees of VR-SGD for non-strongly convex problems. Experimental results\nshow that the performance of VR-SGD is significantly better than its\ncounterparts, SVRG and Prox-SVRG, and it is also much better than the best\nknown stochastic method, Katyusha.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 13:50:43 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Shang", "Fanhua", ""]]}, {"id": "1704.04997", "submitter": "Ardavan Saeedi", "authors": "Ardavan Saeedi, Matthew D. Hoffman, Stephen J. DiVerdi, Asma\n  Ghandeharioun, Matthew J. Johnson, Ryan P. Adams", "title": "Multimodal Prediction and Personalization of Photo Edits with Deep\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional-grade software applications are powerful but\ncomplicated$-$expert users can achieve impressive results, but novices often\nstruggle to complete even basic tasks. Photo editing is a prime example: after\nloading a photo, the user is confronted with an array of cryptic sliders like\n\"clarity\", \"temp\", and \"highlights\". An automatically generated suggestion\ncould help, but there is no single \"correct\" edit for a given image$-$different\nexperts may make very different aesthetic decisions when faced with the same\nimage, and a single expert may make different choices depending on the intended\nuse of the image (or on a whim). We therefore want a system that can propose\nmultiple diverse, high-quality edits while also learning from and adapting to a\nuser's aesthetic preferences. In this work, we develop a statistical model that\nmeets these objectives. Our model builds on recent advances in neural network\ngenerative modeling and scalable inference, and uses hierarchical structure to\nlearn editing patterns across many diverse users. Empirically, we find that our\nmodel outperforms other approaches on this challenging multimodal prediction\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 15:15:12 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Hoffman", "Matthew D.", ""], ["DiVerdi", "Stephen J.", ""], ["Ghandeharioun", "Asma", ""], ["Johnson", "Matthew J.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1704.05017", "submitter": "Mathieu Galtier", "authors": "Mathieu Galtier and Camille Marini", "title": "Morpheo: Traceable Machine Learning on Hidden data", "comments": "whitepaper, 9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morpheo is a transparent and secure machine learning platform collecting and\nanalysing large datasets. It aims at building state-of-the art prediction\nmodels in various fields where data are sensitive. Indeed, it offers strong\nprivacy of data and algorithm, by preventing anyone to read the data, apart\nfrom the owner and the chosen algorithms. Computations in Morpheo are\norchestrated by a blockchain infrastructure, thus offering total traceability\nof operations. Morpheo aims at building an attractive economic ecosystem around\ndata prediction by channelling crypto-money from prediction requests to useful\ndata and algorithms providers. Morpheo is designed to handle multiple data\nsources in a transfer learning approach in order to mutualize knowledge\nacquired from large datasets for applications with smaller but similar\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:24:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Galtier", "Mathieu", ""], ["Marini", "Camille", ""]]}, {"id": "1704.05041", "submitter": "Youngmin Ha", "authors": "Youngmin Ha", "title": "Fast multi-output relevance vector regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to decrease the time complexity of multi-output relevance\nvector regression from O(VM^3) to O(V^3+M^3), where V is the number of output\ndimensions, M is the number of basis functions, and V<M. The experimental\nresults demonstrate that the proposed method is more competitive than the\nexisting method, with regard to computation time. MATLAB codes are available at\nhttp://www.mathworks.com/matlabcentral/fileexchange/49131.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:32:05 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Ha", "Youngmin", ""]]}, {"id": "1704.05098", "submitter": "Yun Yang", "authors": "Yun Yang", "title": "Statistical inference for high dimensional regression via Constrained\n  Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method for estimation and constructing\nconfidence intervals for low-dimensional components in a high-dimensional\nmodel. The proposed estimator, called Constrained Lasso (CLasso) estimator, is\nobtained by simultaneously solving two estimating equations---one imposing a\nzero-bias constraint for the low-dimensional parameter and the other forming an\n$\\ell_1$-penalized procedure for the high-dimensional nuisance parameter. By\ncarefully choosing the zero-bias constraint, the resulting estimator of the low\ndimensional parameter is shown to admit an asymptotically normal limit\nattaining the Cram\\'{e}r-Rao lower bound in a semiparametric sense. We propose\na tuning-free iterative algorithm for implementing the CLasso. We show that\nwhen the algorithm is initialized at the Lasso estimator, the de-sparsified\nestimator proposed in van de Geer et al. [\\emph{Ann. Statist.} {\\bf 42} (2014)\n1166--1202] is asymptotically equivalent to the first iterate of the algorithm.\nWe analyse the asymptotic properties of the CLasso estimator and show the\nglobally linear convergence of the algorithm. We also demonstrate encouraging\nempirical performance of the CLasso through numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 19:21:33 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Yang", "Yun", ""]]}, {"id": "1704.05135", "submitter": "Stanislas Lauly", "authors": "Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho", "title": "Does Neural Machine Translation Benefit from Larger Context?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural machine translation architecture that models the\nsurrounding text in addition to the source sentence. These models lead to\nbetter performance, both in terms of general translation quality and pronoun\nprediction, when trained on small corpora, although this improvement largely\ndisappears when trained with a larger corpus. We also discover that\nattention-based neural machine translation is well suited for pronoun\nprediction and compares favorably with other approaches that were specifically\ndesigned for this task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:42:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Jean", "Sebastien", ""], ["Lauly", "Stanislas", ""], ["Firat", "Orhan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1704.05147", "submitter": "Daoming Lyu", "authors": "Bo Liu, Daoming Lyu, Wen Dong, Saad Biaz", "title": "O$^2$TD: (Near)-Optimal Off-Policy TD Learning", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal difference learning and Residual Gradient methods are the most\nwidely used temporal difference based learning algorithms; however, it has been\nshown that none of their objective functions is optimal w.r.t approximating the\ntrue value function $V$. Two novel algorithms are proposed to approximate the\ntrue value function $V$. This paper makes the following contributions: (1) A\nbatch algorithm that can help find the approximate optimal off-policy\nprediction of the true value function $V$. (2) A linear computational cost (per\nstep) near-optimal algorithm that can learn from a collection of off-policy\nsamples. (3) A new perspective of the emphatic temporal difference learning\nwhich bridges the gap between off-policy optimality and off-policy stability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 23:18:48 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 22:22:52 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Liu", "Bo", ""], ["Lyu", "Daoming", ""], ["Dong", "Wen", ""], ["Biaz", "Saad", ""]]}, {"id": "1704.05193", "submitter": "Sijia Liu", "authors": "Sijia Liu, Pin-Yu Chen, and Alfred O. Hero", "title": "Accelerated Distributed Dual Averaging over Evolving Networks of Growing\n  Connectivity", "comments": null, "journal-ref": "IEEE Tran. on Signal Processing, 2018", "doi": "10.1109/TSP.2018.2793878", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accelerating distributed optimization in\nmulti-agent networks by sequentially adding edges. Specifically, we extend the\ndistributed dual averaging (DDA) subgradient algorithm to evolving networks of\ngrowing connectivity and analyze the corresponding improvement in convergence\nrate. It is known that the convergence rate of DDA is influenced by the\nalgebraic connectivity of the underlying network, where better connectivity\nleads to faster convergence. However, the impact of network topology design on\nthe convergence rate of DDA has not been fully understood. In this paper, we\nbegin by designing network topologies via edge selection and scheduling. For\nedge selection, we determine the best set of candidate edges that achieves the\noptimal tradeoff between the growth of network connectivity and the usage of\nnetwork resources. The dynamics of network evolution is then incurred by edge\nscheduling. Further, we provide a tractable approach to analyze the improvement\nin the convergence rate of DDA induced by the growth of network connectivity.\nOur analysis reveals the connection between network topology design and the\nconvergence rate of DDA, and provides quantitative evaluation of DDA\nacceleration for distributed optimization that is absent in the existing\nanalysis. Lastly, numerical experiments show that DDA can be significantly\naccelerated using a sequence of well-designed networks, and our theoretical\npredictions are well matched to its empirical convergence behavior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 04:02:36 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 01:47:19 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1704.05194", "submitter": "Xiaoqiang Zhu", "authors": "Kun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, Zhe Wang", "title": "Learning Piece-wise Linear Models from Large Scale Data for Ad Click\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CTR prediction in real-world business is a difficult machine learning problem\nwith large scale nonlinear sparse data. In this paper, we introduce an\nindustrial strength solution with model named Large Scale Piece-wise Linear\nModel (LS-PLM). We formulate the learning problem with $L_1$ and $L_{2,1}$\nregularizers, leading to a non-convex and non-smooth optimization problem.\nThen, we propose a novel algorithm to solve it efficiently, based on\ndirectional derivatives and quasi-Newton method. In addition, we design a\ndistributed system which can run on hundreds of machines parallel and provides\nus with the industrial scalability. LS-PLM model can capture nonlinear patterns\nfrom massive sparse data, saving us from heavy feature engineering jobs. Since\n2012, LS-PLM has become the main CTR prediction model in Alibaba's online\ndisplay advertising system, serving hundreds of millions users every day.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 04:03:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Gai", "Kun", ""], ["Zhu", "Xiaoqiang", ""], ["Li", "Han", ""], ["Liu", "Kai", ""], ["Wang", "Zhe", ""]]}, {"id": "1704.05201", "submitter": "Jun Han Mr", "authors": "Jun Han and Qiang Liu", "title": "Stein Variational Adaptive Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive importance sampling algorithm which incorporates\nStein variational gradient decent algorithm (SVGD) with importance sampling\n(IS). Our algorithm leverages the nonparametric transforms in SVGD to\niteratively decrease the KL divergence between our importance proposal and the\ntarget distribution. The advantages of this algorithm are twofold: first, our\nalgorithm turns SVGD into a standard IS algorithm, allowing us to use standard\ndiagnostic and analytic tools of IS to evaluate and interpret the results;\nsecond, we do not restrict the choice of our importance proposal to predefined\ndistribution families like traditional (adaptive) IS methods. Empirical\nexperiments demonstrate that our algorithm performs well on evaluating\npartition functions of restricted Boltzmann machines and testing likelihood of\nvariational auto-encoders.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 04:46:17 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 02:23:37 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 02:29:00 GMT"}, {"version": "v4", "created": "Mon, 10 Jul 2017 17:32:05 GMT"}, {"version": "v5", "created": "Tue, 11 Jul 2017 06:32:59 GMT"}, {"version": "v6", "created": "Tue, 25 Jul 2017 03:46:25 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Han", "Jun", ""], ["Liu", "Qiang", ""]]}, {"id": "1704.05271", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou, Grigorios Tsoumakas, Manos Laliotis, Nikos\n  Markantonatos and Ioannis Vlahavas", "title": "Large-Scale Online Semantic Indexing of Biomedical Articles via an\n  Ensemble of Multi-Label Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In this paper we present the approaches and methods employed in\norder to deal with a large scale multi-label semantic indexing task of\nbiomedical papers. This work was mainly implemented within the context of the\nBioASQ challenge of 2014. Methods: The main contribution of this work is a\nmulti-label ensemble method that incorporates a McNemar statistical\nsignificance test in order to validate the combination of the constituent\nmachine learning algorithms. Some secondary contributions include a study on\nthe temporal aspects of the BioASQ corpus (observations apply also to the\nBioASQ's super-set, the PubMed articles collection) and the proper adaptation\nof the algorithms used to deal with this challenging classification task.\nResults: The ensemble method we developed is compared to other approaches in\nexperimental scenarios with subsets of the BioASQ corpus giving positive\nresults. During the BioASQ 2014 challenge we obtained the first place during\nthe first batch and the third in the two following batches. Our success in the\nBioASQ challenge proved that a fully automated machine-learning approach, which\ndoes not implement any heuristics and rule-based approaches, can be highly\ncompetitive and outperform other approaches in similar challenging contexts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 11:17:00 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Tsoumakas", "Grigorios", ""], ["Laliotis", "Manos", ""], ["Markantonatos", "Nikos", ""], ["Vlahavas", "Ioannis", ""]]}, {"id": "1704.05310", "submitter": "Armand Joulin", "authors": "Piotr Bojanowski, Armand Joulin", "title": "Unsupervised Learning by Predicting Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 12:51:47 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""]]}, {"id": "1704.05318", "submitter": "Mickael Binois", "authors": "Micka\\\"el Binois, David Ginsbourger (3MI-ENSMSE), Olivier Roustant\n  (GdR MASCOT-NUM)", "title": "On the choice of the low-dimensional domain for global optimization via\n  random embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of taking many variables into account in optimization problems\nmay be overcome under the hypothesis of low effective dimensionality. Then, the\nsearch of solutions can be reduced to the random embedding of a low dimensional\nspace into the original one, resulting in a more manageable optimization\nproblem. Specifically, in the case of time consuming black-box functions and\nwhen the budget of evaluations is severely limited, global optimization with\nrandom embeddings appears as a sound alternative to random search. Yet, in the\ncase of box constraints on the native variables, defining suitable bounds on a\nlow dimensional domain appears to be complex. Indeed, a small search domain\ndoes not guarantee to find a solution even under restrictive hypotheses about\nthe function, while a larger one may slow down convergence dramatically. Here\nwe tackle the issue of low-dimensional domain selection based on a detailed\nstudy of the properties of the random embedding, giving insight on the\naforementioned difficulties. In particular, we describe a minimal\nlow-dimensional set in correspondence with the embedded search space. We\nadditionally show that an alternative equivalent embedding procedure yields\nsimultaneously a simpler definition of the low-dimensional minimal set and\nbetter properties in practice. Finally, the performance and robustness gains of\nthe proposed enhancements for Bayesian optimization are illustrated on\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:10:41 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 13:55:21 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 14:16:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Binois", "Micka\u00ebl", "", "3MI-ENSMSE"], ["Ginsbourger", "David", "", "3MI-ENSMSE"], ["Roustant", "Olivier", "", "GdR MASCOT-NUM"]]}, {"id": "1704.05356", "submitter": "Nicolas Pr\\'ollochs", "authors": "Nicolas Pr\\\"ollochs, Stefan Feuerriegel, Dirk Neumann", "title": "Understanding Negations in Information Processing: Learning from\n  Replicating Human Behavior", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information systems experience an ever-growing volume of unstructured data,\nparticularly in the form of textual materials. This represents a rich source of\ninformation from which one can create value for people, organizations and\nbusinesses. For instance, recommender systems can benefit from automatically\nunderstanding preferences based on user reviews or social media. However, it is\ndifficult for computer programs to correctly infer meaning from narrative\ncontent. One major challenge is negations that invert the interpretation of\nwords and sentences. As a remedy, this paper proposes a novel learning strategy\nto detect negations: we apply reinforcement learning to find a policy that\nreplicates the human perception of negations based on an exogenous response,\nsuch as a user rating for reviews. Our method yields several benefits, as it\neliminates the former need for expensive and subjective manual labeling in an\nintermediate stage. Moreover, the inferred policy can be used to derive\nstatistical inferences and implications regarding how humans process and act on\nnegations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:27:46 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Pr\u00f6llochs", "Nicolas", ""], ["Feuerriegel", "Stefan", ""], ["Neumann", "Dirk", ""]]}, {"id": "1704.05409", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo and Simone Melzi", "title": "Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality", "comments": "Preprint version - Lecture Notes in Computer Science - Springer 2017", "journal-ref": "New Frontiers in Mining Complex Patterns, Fifth International\n  workshop, nfMCP2016. Lecture Notes in Computer Science - Springer", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:21:05 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""]]}, {"id": "1704.05420", "submitter": "Cem Subakan", "authors": "Y. Cem Subakan, Paris Smaragdis", "title": "Diagonal RNNs in Symbolic Music Modeling", "comments": "Submitted to Waspaa 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new Recurrent Neural Network (RNN) architecture.\nThe novelty is simple: We use diagonal recurrent matrices instead of full. This\nresults in better test likelihood and faster convergence compared to regular\nfull RNNs in most of our experiments. We show the benefits of using diagonal\nrecurrent matrices with popularly used LSTM and GRU architectures as well as\nwith the vanilla RNN architecture, on four standard symbolic music datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 23:36:18 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Subakan", "Y. Cem", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1704.05566", "submitter": "Jeremy Morton", "authors": "Jeremy Morton and Mykel J. Kochenderfer", "title": "Simultaneous Policy Learning and Latent State Inference for Imitating\n  Driver Behavior", "comments": "7 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for learning driver models that account for\nvariables that cannot be observed directly. When trained on a synthetic\ndataset, our models are able to learn encodings for vehicle trajectories that\ndistinguish between four distinct classes of driver behavior. Such encodings\nare learned without any knowledge of the number of driver classes or any\nobjective that directly requires the models to learn encodings for each class.\nWe show that driving policies trained with knowledge of latent variables are\nmore effective than baseline methods at imitating the driver behavior that they\nare trained to replicate. Furthermore, we demonstrate that the actions chosen\nby our policy are heavily influenced by the latent variable settings that are\nprovided to them.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 00:23:59 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Morton", "Jeremy", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1704.05712", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker\n  Fischer", "title": "Universal Adversarial Perturbations Against Semantic Image Segmentation", "comments": "Final version for ICCV including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:48:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 08:35:25 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 18:55:54 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Brox", "Thomas", ""], ["Fischer", "Volker", ""]]}, {"id": "1704.05786", "submitter": "Joseph Sakaya", "authors": "Joseph Sakaya and Arto Klami", "title": "Importance Sampled Stochastic Optimization for Variational Inference", "comments": "10 pages, 10 figures; published in Uncertainty in Artificial\n  Intelligence, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference approximates the posterior distribution of a\nprobabilistic model with a parameterized density by maximizing a lower bound\nfor the model evidence. Modern solutions fit a flexible approximation with\nstochastic gradient descent, using Monte Carlo approximation for the gradients.\nThis enables variational inference for arbitrary differentiable probabilistic\nmodels, and consequently makes variational inference feasible for probabilistic\nprogramming languages. In this work we develop more efficient inference\nalgorithms for the task by considering importance sampling estimates for the\ngradients. We show how the gradient with respect to the approximation\nparameters can often be evaluated efficiently without needing to re-compute\ngradients of the model itself, and then proceed to derive practical algorithms\nthat use importance sampled estimates to speed up computation.We present\nimportance sampled stochastic gradient descent that outperforms standard\nstochastic gradient descent by a clear margin for a range of models, and\nprovide a justifiable variant of stochastic average gradients for variational\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:51:46 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 11:54:22 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Sakaya", "Joseph", ""], ["Klami", "Arto", ""]]}, {"id": "1704.05820", "submitter": "Yichong Xu", "authors": "Yichong Xu, Hongyang Zhang, Aarti Singh, Kyle Miller, Artur Dubrawski", "title": "Noise-Tolerant Interactive Learning from Pairwise Comparisons", "comments": "28 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of interactively learning a binary classifier using\nnoisy labeling and pairwise comparison oracles, where the comparison oracle\nanswers which one in the given two instances is more likely to be positive.\nLearning from such oracles has multiple applications where obtaining direct\nlabels is harder but pairwise comparisons are easier, and the algorithm can\nleverage both types of oracles. In this paper, we attempt to characterize how\nthe access to an easier comparison oracle helps in improving the label and\ntotal query complexity. We show that the comparison oracle reduces the learning\nproblem to that of learning a threshold function. We then present an algorithm\nthat interactively queries the label and comparison oracles and we characterize\nits query complexity under Tsybakov and adversarial noise conditions for the\ncomparison and labeling oracles. Our lower bounds show that our label and total\nquery complexity is almost optimal.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:00:55 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 21:06:43 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Xu", "Yichong", ""], ["Zhang", "Hongyang", ""], ["Singh", "Aarti", ""], ["Miller", "Kyle", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1704.05822", "submitter": "Hideyuki Miyahara", "authors": "Hideyuki Miyahara, Koji Tsumura, and Yuki Sughiyama", "title": "Deterministic Quantum Annealing Expectation-Maximization Algorithm", "comments": null, "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment 2017,\n  113404 (2017)", "doi": "10.1088/1742-5468/aa967e", "report-no": null, "categories": "stat.ML cond-mat.stat-mech physics.comp-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation (MLE) is one of the most important methods in\nmachine learning, and the expectation-maximization (EM) algorithm is often used\nto obtain maximum likelihood estimates. However, EM heavily depends on initial\nconfigurations and fails to find the global optimum. On the other hand, in the\nfield of physics, quantum annealing (QA) was proposed as a novel optimization\napproach. Motivated by QA, we propose a quantum annealing extension of EM,\nwhich we call the deterministic quantum annealing expectation-maximization\n(DQAEM) algorithm. We also discuss its advantage in terms of the path integral\nformulation. Furthermore, by employing numerical simulations, we illustrate how\nit works in MLE and show that DQAEM outperforms EM.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:02:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Miyahara", "Hideyuki", ""], ["Tsumura", "Koji", ""], ["Sughiyama", "Yuki", ""]]}, {"id": "1704.05948", "submitter": "Li Chen", "authors": "Li Chen, Mingwei Zhang, Chih-Yuan Yang, Ravi Sahita", "title": "Semi-supervised classification for dynamic Android malware detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of threats to Android phones creates challenges for malware\ndetection. Manually labeling the samples into benign or different malicious\nfamilies requires tremendous human efforts, while it is comparably easy and\ncheap to obtain a large amount of unlabeled APKs from various sources.\nMoreover, the fast-paced evolution of Android malware continuously generates\nderivative malware families. These families often contain new signatures, which\ncan escape detection when using static analysis. These practical challenges can\nalso cause traditional supervised machine learning algorithms to degrade in\nperformance.\n  In this paper, we propose a framework that uses model-based semi-supervised\n(MBSS) classification scheme on the dynamic Android API call logs. The\nsemi-supervised approach efficiently uses the labeled and unlabeled APKs to\nestimate a finite mixture model of Gaussian distributions via conditional\nexpectation-maximization and efficiently detects malwares during out-of-sample\ntesting. We compare MBSS with the popular malware detection classifiers such as\nsupport vector machine (SVM), $k$-nearest neighbor (kNN) and linear\ndiscriminant analysis (LDA). Under the ideal classification setting, MBSS has\ncompetitive performance with 98\\% accuracy and very low false positive rate for\nin-sample classification. For out-of-sample testing, the out-of-sample test\ndata exhibit similar behavior of retrieving phone information and sending to\nthe network, compared with in-sample training set. When this similarity is\nstrong, MBSS and SVM with linear kernel maintain 90\\% detection rate while\n$k$NN and LDA suffer great performance degradation. When this similarity is\nslightly weaker, all classifiers degrade in performance, but MBSS still\nperforms significantly better than other classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 22:29:04 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Chen", "Li", ""], ["Zhang", "Mingwei", ""], ["Yang", "Chih-Yuan", ""], ["Sahita", "Ravi", ""]]}, {"id": "1704.05960", "submitter": "Milad Zafar Nezhad", "authors": "Milad Zafar Nezhad, Dongxiao Zhu, Xiangrui Li, Kai Yang, Phillip Levy", "title": "SAFS: A Deep Feature Selection Approach for Precision Medicine", "comments": null, "journal-ref": null, "doi": "10.1109/BIBM.2016.7822569", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep feature selection method based on deep\narchitecture. Our method uses stacked auto-encoders for feature representation\nin higher-level abstraction. We developed and applied a novel feature learning\napproach to a specific precision medicine problem, which focuses on assessing\nand prioritizing risk factors for hypertension (HTN) in a vulnerable\ndemographic subgroup (African-American). Our approach is to use deep learning\nto identify significant risk factors affecting left ventricular mass indexed to\nbody surface area (LVMI) as an indicator of heart damage risk. The results show\nthat our feature learning and representation approach leads to better results\nin comparison with others.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 00:01:28 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Nezhad", "Milad Zafar", ""], ["Zhu", "Dongxiao", ""], ["Li", "Xiangrui", ""], ["Yang", "Kai", ""], ["Levy", "Phillip", ""]]}, {"id": "1704.05982", "submitter": "Tao Wu", "authors": "Tao Wu and David Gleich", "title": "Retrospective Higher-Order Markov Processes for User Trails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users form information trails as they browse the web, checkin with a\ngeolocation, rate items, or consume media. A common problem is to predict what\na user might do next for the purposes of guidance, recommendation, or\nprefetching. First-order and higher-order Markov chains have been widely used\nmethods to study such sequences of data. First-order Markov chains are easy to\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\nin contrast, have too many parameters and suffer from overfitting the training\ndata. Fitting these parameters with regularization and smoothing only offers\nmild improvements. In this paper we propose the retrospective higher-order\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\nis a special case of a higher-order Markov chain where the transitions depend\nretrospectively on a single history state instead of an arbitrary combination\nof history states. There are two immediate computational advantages: the number\nof parameters is linear in the order of the Markov chain and the model can be\nfit to large state spaces. Furthermore, by providing a specific structure to\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\nutilizing history states without risks of overfitting the data. We demonstrate\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\nmethod on various real application datasets spanning geolocation data, review\nsequences, and business locations. The RHOMP model uniformly outperforms\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\nfactorizations in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 02:14:17 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Wu", "Tao", ""], ["Gleich", "David", ""]]}, {"id": "1704.06001", "submitter": "Pooya Khorrami", "authors": "Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad\n  Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A. Hasegawa-Johnson, Roy H.\n  Campbell, Thomas S. Huang", "title": "Fast Generation for Convolutional Autoregressive Models", "comments": "Accepted at ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional autoregressive models have recently demonstrated\nstate-of-the-art performance on a number of generation tasks. While fast,\nparallel training methods have been crucial for their success, generation is\ntypically implemented in a na\\\"{i}ve fashion where redundant computations are\nunnecessarily repeated. This results in slow generation, making such models\ninfeasible for production environments. In this work, we describe a method to\nspeed up generation in convolutional autoregressive models. The key idea is to\ncache hidden states to avoid redundant computation. We apply our fast\ngeneration method to the Wavenet and PixelCNN++ models and achieve up to\n$21\\times$ and $183\\times$ speedups respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:13:21 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Babaeizadeh", "Mohammad", ""], ["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Hasegawa-Johnson", "Mark A.", ""], ["Campbell", "Roy H.", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1704.06025", "submitter": "Bicheng Ying", "authors": "Bicheng Ying, Ali H. Sayed", "title": "Performance Limits of Stochastic Sub-Gradient Learning, Part II:\n  Multi-Agent Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis in Part I revealed interesting properties for subgradient\nlearning algorithms in the context of stochastic optimization when gradient\nnoise is present. These algorithms are used when the risk functions are\nnon-smooth and involve non-differentiable components. They have been long\nrecognized as being slow converging methods. However, it was revealed in Part I\nthat the rate of convergence becomes linear for stochastic optimization\nproblems, with the error iterate converging at an exponential rate $\\alpha^i$\nto within an $O(\\mu)-$neighborhood of the optimizer, for some $\\alpha \\in\n(0,1)$ and small step-size $\\mu$. The conclusion was established under weaker\nassumptions than the prior literature and, moreover, several important problems\n(such as LASSO, SVM, and Total Variation) were shown to satisfy these weaker\nassumptions automatically (but not the previously used conditions from the\nliterature). These results revealed that sub-gradient learning methods have\nmore favorable behavior than originally thought when used to enable continuous\nadaptation and learning. The results of Part I were exclusive to single-agent\nadaptation. The purpose of the current Part II is to examine the implications\nof these discoveries when a collection of networked agents employs subgradient\nlearning as their cooperative mechanism. The analysis will show that, despite\nthe coupled dynamics that arises in a networked scenario, the agents are still\nable to attain linear convergence in the stochastic case; they are also able to\nreach agreement within $O(\\mu)$ of the optimizer.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 06:35:26 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1704.06033", "submitter": "Hongyoon Choi Dr", "authors": "Hongyoon Choi, Kyong Hwan Jin", "title": "Predicting Cognitive Decline with Deep Learning of Brain Metabolism and\n  Amyloid Imaging", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For effective treatment of Alzheimer disease (AD), it is important to\nidentify subjects who are most likely to exhibit rapid cognitive decline.\nHerein, we developed a novel framework based on a deep convolutional neural\nnetwork which can predict future cognitive decline in mild cognitive impairment\n(MCI) patients using flurodeoxyglucose and florbetapir positron emission\ntomography (PET). The architecture of the network only relies on baseline PET\nstudies of AD and normal subjects as the training dataset. Feature extraction\nand complicated image preprocessing including nonlinear warping are unnecessary\nfor our approach. Accuracy of prediction (84.2%) for conversion to AD in MCI\npatients outperformed conventional feature-based quantification approaches. ROC\nanalyses revealed that performance of CNN-based approach was significantly\nhigher than that of the conventional quantification methods (p < 0.05). Output\nscores of the network were strongly correlated with the longitudinal change in\ncognitive measurements. These results show the feasibility of deep learning as\na tool for predicting disease outcome using brain images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 07:33:18 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Choi", "Hongyoon", ""], ["Jin", "Kyong Hwan", ""]]}, {"id": "1704.06062", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Amit Mandelbaum, Daphna Weinshall", "title": "Every Untrue Label is Untrue in its Own Way: Controlling Error Type with\n  the Log Bilinear Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the method of choice in many application domains of\nmachine learning in recent years, especially for multi-class classification\ntasks. The most common loss function used in this context is the cross-entropy\nloss, which reduces to the log loss in the typical case when there is a single\ncorrect response label. While this loss is insensitive to the identity of the\nassigned class in the case of misclassification, in practice it is often the\ncase that some errors may be more detrimental than others. Here we present the\nbilinear-loss (and related log-bilinear-loss) which differentially penalizes\nthe different wrong assignments of the model. We thoroughly test this method\nusing standard models and benchmark image datasets. As one application, we show\nthe ability of this method to better contain error within the correct\nsuper-class, in the hierarchically labeled CIFAR100 dataset, without affecting\nthe overall performance of the classifier.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 09:29:09 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Mandelbaum", "Amit", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1704.06084", "submitter": "Steffen Thoma", "authors": "Steffen Thoma, Achim Rettinger, Fabian Both", "title": "Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a baseline approach for cross-modal knowledge fusion. Different\nbasic fusion methods are evaluated on existing embedding approaches to show the\npotential of joining knowledge about certain concepts across modalities in a\nfused concept representation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 10:49:51 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Thoma", "Steffen", ""], ["Rettinger", "Achim", ""], ["Both", "Fabian", ""]]}, {"id": "1704.06125", "submitter": "Mathieu Cliche", "authors": "Mathieu Cliche", "title": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and\n  LSTMs", "comments": "Published in Proceedings of SemEval-2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our attempt at producing a state-of-the-art Twitter\nsentiment classifier using Convolutional Neural Networks (CNNs) and Long Short\nTerm Memory (LSTMs) networks. Our system leverages a large amount of unlabeled\ndata to pre-train word embeddings. We then use a subset of the unlabeled data\nto fine tune the embeddings using distant supervision. The final CNNs and LSTMs\nare trained on the SemEval-2017 Twitter dataset where the embeddings are fined\ntuned again. To boost performances we ensemble several CNNs and LSTMs together.\nOur approach achieved first rank on all of the five English subtasks amongst 40\nteams.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 13:10:25 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Cliche", "Mathieu", ""]]}, {"id": "1704.06131", "submitter": "Yewen Pu", "authors": "Yewen Pu, Leslie P Kaelbling, Armando Solar-Lezama", "title": "Learning to Acquire Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of diagnosis where a set of simple observations are\nused to infer a potentially complex hidden hypothesis. Finding the optimal\nsubset of observations is intractable in general, thus we focus on the problem\nof active diagnosis, where the agent selects the next most-informative\nobservation based on the results of previous observations. We show that under\nthe assumption of uniform observation entropy, one can build an implication\nmodel which directly predicts the outcome of the potential next observation\nconditioned on the results of past observations, and selects the observation\nwith the maximum entropy. This approach enjoys reduced computation complexity\nby bypassing the complicated hypothesis space, and can be trained on\nobservation data alone, learning how to query without knowledge of the hidden\nhypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 13:28:02 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 12:58:45 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Pu", "Yewen", ""], ["Kaelbling", "Leslie P", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1704.06176", "submitter": "Cem Deniz", "authors": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, James\n  S. Babb, Stephen Honig, Kyunghyun Cho, and Gregory Chang", "title": "Segmentation of the Proximal Femur from MR Images using Deep\n  Convolutional Neural Networks", "comments": "This is a pre-print of an article published in Scientific Reports.\n  The final authenticated version is available online at:\n  https://doi.org/10.1038/s41598-018-34817-6", "journal-ref": "Scientific Reports, volume 8, Article number: 16485 (2018)", "doi": "10.1038/s41598-018-34817-6", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) has been proposed as a complimentary method\nto measure bone quality and assess fracture risk. However, manual segmentation\nof MR images of bone is time-consuming, limiting the use of MRI measurements in\nthe clinical practice. The purpose of this paper is to present an automatic\nproximal femur segmentation method that is based on deep convolutional neural\nnetworks (CNNs). This study had institutional review board approval and written\ninformed consent was obtained from all subjects. A dataset of volumetric\nstructural MR images of the proximal femur from 86 subject were\nmanually-segmented by an expert. We performed experiments by training two\ndifferent CNN architectures with multiple number of initial feature maps and\nlayers, and tested their segmentation performance against the gold standard of\nmanual segmentations using four-fold cross-validation. Automatic segmentation\nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05\nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN\narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The\nhigh segmentation accuracy provided by CNNs has the potential to help bring the\nuse of structural MRI measurements of bone quality into clinical practice for\nmanagement of osteoporosis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 14:54:29 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 21:15:40 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 20:36:28 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 18:32:16 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 14:46:00 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Deniz", "Cem M.", ""], ["Xiang", "Siyuan", ""], ["Hallyburton", "Spencer", ""], ["Welbeck", "Arakua", ""], ["Babb", "James S.", ""], ["Honig", "Stephen", ""], ["Cho", "Kyunghyun", ""], ["Chang", "Gregory", ""]]}, {"id": "1704.06199", "submitter": "Alessandro Rozza", "authors": "Franco Manessi and Alessandro Rozza and Mario Manzo", "title": "Dynamic Graph Convolutional Networks", "comments": null, "journal-ref": "The final version has been published in Elsevier Pattern\n  Recognition, August 2019", "doi": "10.1016/j.patcog.2019.107000", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different classification tasks need to manage structured data, which are\nusually modeled as graphs. Moreover, these graphs can be dynamic, meaning that\nthe vertices/edges of each graph may change during time. Our goal is to jointly\nexploit structured data and temporal information through the use of a neural\nnetwork model. To the best of our knowledge, this task has not been addressed\nusing these kind of architectures. For this reason, we propose two novel\napproaches, which combine Long Short-Term Memory networks and Graph\nConvolutional Networks to learn long short-term dependencies together with\ngraph structure. The quality of our methods is confirmed by the promising\nresults achieved.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:54:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Manessi", "Franco", ""], ["Rozza", "Alessandro", ""], ["Manzo", "Mario", ""]]}, {"id": "1704.06256", "submitter": "Quanquan Gu", "authors": "Jinghui Chen and Lingxiao Wang and Xiao Zhang and Quanquan Gu", "title": "Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption", "comments": "29 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust phase retrieval problem of recovering the unknown\nsignal from the magnitude-only measurements, where the measurements can be\ncontaminated by both sparse arbitrary corruption and bounded random noise. We\npropose a new nonconvex algorithm for robust phase retrieval, namely Robust\nWirtinger Flow to jointly estimate the unknown signal and the sparse\ncorruption. We show that our proposed algorithm is guaranteed to converge\nlinearly to the unknown true signal up to a minimax optimal statistical\nprecision in such a challenging setting. Compared with existing robust phase\nretrieval methods, we achieve an optimal sample complexity of $O(n)$ in both\nnoisy and noise-free settings. Thorough experiments on both synthetic and real\ndatasets corroborate our theory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 17:59:05 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 22:35:25 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chen", "Jinghui", ""], ["Wang", "Lingxiao", ""], ["Zhang", "Xiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1704.06279", "submitter": "Maciej Koch-Janusz", "authors": "Maciej Koch-Janusz and Zohar Ringel", "title": "Mutual Information, Neural Networks and the Renormalization Group", "comments": "The accepted (substantially extended) version", "journal-ref": "Nature Physics 14, 578--582 (2018)", "doi": "10.1038/s41567-018-0081-4", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical systems differring in their microscopic details often display\nstrikingly similar behaviour when probed at macroscopic scales. Those universal\nproperties, largely determining their physical characteristics, are revealed by\nthe powerful renormalization group (RG) procedure, which systematically retains\n\"slow\" degrees of freedom and integrates out the rest. However, the important\ndegrees of freedom may be difficult to identify. Here we demonstrate a machine\nlearning algorithm capable of identifying the relevant degrees of freedom and\nexecuting RG steps iteratively without any prior knowledge about the system. We\nintroduce an artificial neural network based on a model-independent,\ninformation-theoretic characterization of a real-space RG procedure, performing\nthis task. We apply the algorithm to classical statistical physics problems in\none and two dimensions. We demonstrate RG flow and extract the Ising critical\nexponent. Our results demonstrate that machine learning techniques can extract\nabstract physical concepts and consequently become an integral part of theory-\nand model-building.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 18:02:50 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 15:06:30 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Koch-Janusz", "Maciej", ""], ["Ringel", "Zohar", ""]]}, {"id": "1704.06363", "submitter": "Arthur Szlam", "authors": "Sam Gross and Marc'Aurelio Ranzato and Arthur Szlam", "title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision", "comments": "Appearing in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training convolutional networks (CNN's) that fit on a single GPU with\nminibatch stochastic gradient descent has become effective in practice.\nHowever, there is still no effective method for training large CNN's that do\nnot fit in the memory of a few GPU cards, or for parallelizing CNN training. In\nthis work we show that a simple hard mixture of experts model can be\nefficiently trained to good effect on large scale hashtag (multilabel)\nprediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991,\nCollobert et. al. 2003), but in the past, researchers have had to devise\nsophisticated methods to deal with data fragmentation. We show empirically that\nmodern weakly supervised data sets are large enough to support naive\npartitioning schemes where each data point is assigned to a single expert.\nBecause the experts are independent, training them in parallel is easy, and\nevaluation is cheap for the size of the model. Furthermore, we show that we can\nuse a single decoding layer for all the experts, allowing a unified feature\nembedding space. We demonstrate that it is feasible (and in fact relatively\npainless) to train far larger models than could be practically trained with\nstandard CNN architectures, and that the extra capacity can be well used on\ncurrent datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 23:45:27 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Gross", "Sam", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""]]}, {"id": "1704.06497", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Artem Sokolov, Stefan Riezler", "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit structured prediction describes a stochastic optimization framework\nwhere learning is performed from partial feedback. This feedback is received in\nthe form of a task loss evaluation to a predicted output structure, without\nhaving access to gold standard structures. We advance this framework by lifting\nlinear bandit learning to neural sequence-to-sequence learning problems using\nattention-based recurrent neural networks. Furthermore, we show how to\nincorporate control variates into our learning algorithms for variance\nreduction and improved generalization. We present an evaluation on a neural\nmachine translation task that shows improvements of up to 5.89 BLEU points for\ndomain adaptation from simulated bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:56:00 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 17:00:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kreutzer", "Julia", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1704.06625", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, Ali Mousavi, Richard G. Baraniuk", "title": "Learned D-AMP: Principled Neural Network based Compressive Image\n  Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive image recovery is a challenging problem that requires fast and\naccurate algorithms. Recently, neural networks have been applied to this\nproblem with promising results. By exploiting massively parallel GPU processing\narchitectures and oodles of training data, they can run orders of magnitude\nfaster than existing techniques. However, these methods are largely\nunprincipled black boxes that are difficult to train and often-times specific\nto a single measurement matrix.\n  It was recently demonstrated that iterative sparse-signal-recovery algorithms\ncan be \"unrolled\" to form interpretable deep networks. Taking inspiration from\nthis work, we develop a novel neural network architecture that mimics the\nbehavior of the denoising-based approximate message passing (D-AMP) algorithm.\nWe call this new network Learned D-AMP (LDAMP).\n  The LDAMP network is easy to train, can be applied to a variety of different\nmeasurement matrices, and comes with a state-evolution heuristic that\naccurately predicts its performance. Most importantly, it outperforms the\nstate-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and\nrun time. At high resolutions, and when used with sensing matrices that have\nfast implementations, LDAMP runs over $50\\times$ faster than BM3D-AMP and\nhundreds of times faster than NLR-CS.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:40:29 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 02:46:02 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 21:49:15 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 21:36:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Mousavi", "Ali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1704.06656", "submitter": "Mahdi Zarei", "authors": "Mahdi Zarei", "title": "Feature selection algorithm based on Catastrophe model to improve the\n  performance of regression analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a new feature selection algorithm to remove the\nirrelevant or redundant features in the data sets. In this algorithm the\nimportance of a feature is based on its fitting to the Catastrophe model.\nAkaike information crite- rion value is used for ranking the features in the\ndata set. The proposed algorithm is compared with well-known RELIEF feature\nselection algorithm. Breast Cancer, Parkinson Telemonitoring data and Slice\nlocality data sets are used to evaluate the model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 17:32:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zarei", "Mahdi", ""]]}, {"id": "1704.06687", "submitter": "Mathieu Cliche", "authors": "Mathieu Cliche, David Rosenberg, Dhruv Madeka and Connie Yee", "title": "Scatteract: Automated extraction of data from scatter plots", "comments": "Submitted to ECML PKDD 2017 proceedings, 16 pages", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2017. Lecture Notes in Computer Science, vol 10534. Springer, Cham", "doi": "10.1007/978-3-319-71249-9_9", "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:25:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Cliche", "Mathieu", ""], ["Rosenberg", "David", ""], ["Madeka", "Dhruv", ""], ["Yee", "Connie", ""]]}, {"id": "1704.06735", "submitter": "Hao Peng", "authors": "Hao Peng, Shandian Zhe and Yuan Qi", "title": "Asynchronous Distributed Variational Gaussian Processes for Regression", "comments": "International Conference on Machine Learning 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are powerful non-parametric function estimators.\nHowever, their applications are largely limited by the expensive computational\ncost of the inference procedures. Existing stochastic or distributed\nsynchronous variational inferences, although have alleviated this issue by\nscaling up GPs to millions of samples, are still far from satisfactory for\nreal-world large applications, where the data sizes are often orders of\nmagnitudes larger, say, billions. To solve this problem, we propose ADVGP, the\nfirst Asynchronous Distributed Variational Gaussian Process inference for\nregression, on the recent large-scale machine learning platform,\nPARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a\nweight space augmentation, and implements the highly efficient, asynchronous\nproximal gradient optimization. While maintaining comparable or better\npredictive performance, ADVGP greatly improves upon the efficiency of the\nexisting variational methods. With ADVGP, we effortlessly scale up GP\nregression to a real-world application with billions of samples and demonstrate\nan excellent, superior prediction accuracy to the popular linear models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 02:22:19 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 22:27:40 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 19:47:02 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Peng", "Hao", ""], ["Zhe", "Shandian", ""], ["Qi", "Yuan", ""]]}, {"id": "1704.06743", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (University of Sydney and Capital Markets\n  Cooperative Research Centre (CMCRC)), Aditya Krishna Menon (Data61/CSIRO and\n  the Australian National University), and Sanjay Chawla (Qatar Computing\n  Research Institute (QCRI), HBKU)", "title": "Robust, Deep and Inductive Anomaly Detection", "comments": "Accepted ECML PKDD 2017 Skopje, Macedonia 18-22 September the\n  European Conference On Machine Learning & Principles and Practice of\n  Knowledge Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PCA is a classical statistical technique whose simplicity and maturity has\nseen it find widespread use as an anomaly detection technique. However, it is\nlimited in this regard by being sensitive to gross perturbations of the input,\nand by seeking a linear subspace that captures normal behaviour. The first\nissue has been dealt with by robust PCA, a variant of PCA that explicitly\nallows for some data points to be arbitrarily corrupted, however, this does not\nresolve the second issue, and indeed introduces the new issue that one can no\nlonger inductively find anomalies on a test set. This paper addresses both\nissues in a single model, the robust autoencoder. This method learns a\nnonlinear subspace that captures the majority of data points, while allowing\nfor some data to have arbitrary corruption. The model is simple to train and\nleverages recent advances in the optimisation of deep neural networks.\nExperiments on a range of real-world datasets highlight the model's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 04:12:24 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 18:46:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 08:47:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chalapathy", "Raghavendra", "", "University of Sydney and Capital Markets\n  Cooperative Research Centre"], ["Menon", "Aditya Krishna", "", "Data61/CSIRO and\n  the Australian National University"], ["Chawla", "Sanjay", "", "Qatar Computing\n  Research Institute"]]}, {"id": "1704.06803", "submitter": "Federico Monti", "authors": "Federico Monti, Michael M. Bronstein, Xavier Bresson", "title": "Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion models are among the most common formulations of\nrecommender systems. Recent works have showed a boost of performance of these\ntechniques when introducing the pairwise relationships between users/items in\nthe form of graphs, and imposing smoothness priors on these graphs. However,\nsuch techniques do not fully exploit the local stationarity structures of\nuser/item graphs, and the number of parameters to learn is linear w.r.t. the\nnumber of users and items. We propose a novel approach to overcome these\nlimitations by using geometric deep learning on graphs. Our matrix completion\narchitecture combines graph convolutional neural networks and recurrent neural\nnetworks to learn meaningful statistical graph-structured patterns and the\nnon-linear diffusion process that generates the known ratings. This neural\nnetwork system requires a constant number of parameters independent of the\nmatrix size. We apply our method on both synthetic and real datasets, showing\nthat it outperforms state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 14:02:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""], ["Bresson", "Xavier", ""]]}, {"id": "1704.06885", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "A General Theory for Training Learning Machine", "comments": "55 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1602.03950", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the deep learning is pushing the machine learning to a new stage,\nbasic theories of machine learning are still limited. The principle of\nlearning, the role of the a prior knowledge, the role of neuron bias, and the\nbasis for choosing neural transfer function and cost function, etc., are still\nfar from clear. In this paper, we present a general theoretical framework for\nmachine learning. We classify the prior knowledge into common and\nproblem-dependent parts, and consider that the aim of learning is to maximally\nincorporate them. The principle we suggested for maximizing the former is the\ndesign risk minimization principle, while the neural transfer function, the\ncost function, as well as pretreatment of samples, are endowed with the role\nfor maximizing the latter. The role of the neuron bias is explained from a\ndifferent angle. We develop a Monte Carlo algorithm to establish the\ninput-output responses, and we control the input-output sensitivity of a\nlearning machine by controlling that of individual neurons. Applications of\nfunction approaching and smoothing, pattern recognition and classification, are\nprovided to illustrate how to train general learning machines based on our\ntheory and algorithm. Our method may in addition induce new applications, such\nas the transductive inference.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 05:48:18 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1704.06933", "submitter": "Lijun Wu", "authors": "Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai,\n  Tie-Yan Liu", "title": "Adversarial Neural Machine Translation", "comments": "ACML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:08:47 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 13:35:31 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 03:29:54 GMT"}, {"version": "v4", "created": "Sun, 30 Sep 2018 14:04:21 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wu", "Lijun", ""], ["Xia", "Yingce", ""], ["Zhao", "Li", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Lai", "Jianhuang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1704.06977", "submitter": "Xin Bing", "authors": "Xin Bing, Florentina Bunea, Yang Ning, Marten Wegkamp", "title": "Adaptive Estimation in Structured Factor Models with Applications to\n  Overlapping Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel estimation method, called LOVE, of the entries\nand structure of a loading matrix A in a sparse latent factor model X = AZ + E,\nfor an observable random vector X in Rp, with correlated unobservable factors Z\n\\in RK, with K unknown, and independent noise E. Each row of A is scaled and\nsparse. In order to identify the loading matrix A, we require the existence of\npure variables, which are components of X that are associated, via A, with one\nand only one latent factor. Despite the fact that the number of factors K, the\nnumber of the pure variables, and their location are all unknown, we only\nrequire a mild condition on the covariance matrix of Z, and a minimum of only\ntwo pure variables per latent factor to show that A is uniquely defined, up to\nsigned permutations. Our proofs for model identifiability are constructive, and\nlead to our novel estimation method of the number of factors and of the set of\npure variables, from a sample of size n of observations on X. This is the first\nstep of our LOVE algorithm, which is optimization-free, and has low\ncomputational complexity of order p2. The second step of LOVE is an easily\nimplementable linear program that estimates A. We prove that the resulting\nestimator is minimax rate optimal up to logarithmic factors in p. The model\nstructure is motivated by the problem of overlapping variable clustering,\nubiquitous in data science. We define the population level clusters as groups\nof those components of X that are associated, via the sparse matrix A, with the\nsame unobservable latent factor, and multi-factor association is allowed.\nClusters are respectively anchored by the pure variables, and form overlapping\nsub-groups of the p-dimensional random vector X. The Latent model approach to\nOVErlapping clustering is reflected in the name of our algorithm, LOVE.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:43:44 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 04:06:07 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 03:01:20 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2019 23:02:35 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Ning", "Yang", ""], ["Wegkamp", "Marten", ""]]}, {"id": "1704.07008", "submitter": "Weixin Cai", "authors": "Weixin Cai, Nima S. Hejazi, Alan E. Hubbard", "title": "Data-adaptive statistics for multiple hypothesis testing in\n  high-dimensional settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical inference problems in areas like astronomy, genomics, and\nmarketing routinely involve the simultaneous testing of thousands -- even\nmillions -- of null hypotheses. For high-dimensional multivariate\ndistributions, these hypotheses may concern a wide range of parameters, with\ncomplex and unknown dependence structures among variables. In analyzing such\nhypothesis testing procedures, gains in efficiency and power can be achieved by\nperforming variable reduction on the set of hypotheses prior to testing. We\npresent in this paper an approach using data-adaptive multiple testing that\nserves exactly this purpose. This approach applies data mining techniques to\nscreen the full set of covariates on equally sized partitions of the whole\nsample via cross-validation. This generalized screening procedure is used to\ncreate average ranks for covariates, which are then used to generate a reduced\n(sub)set of hypotheses, from which we compute test statistics that are\nsubsequently subjected to standard multiple testing corrections. The principal\nadvantage of this methodology lies in its providing valid statistical inference\nwithout the \\textit{a priori} specifying which hypotheses will be tested. Here,\nwe present the theoretical details of this approach, confirm its validity via a\nsimulation study, and exemplify its use by applying it to the analysis of data\non microRNA differential expression.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 00:50:29 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Cai", "Weixin", ""], ["Hejazi", "Nima S.", ""], ["Hubbard", "Alan E.", ""]]}, {"id": "1704.07050", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Using Global Constraints and Reranking to Improve Cognates Detection", "comments": "10 pages, 6 figures, 6 tables; published in the Proceedings of the\n  55th Annual Meeting of the Association for Computational Linguistics, pages\n  1983-1992, Vancouver, Canada, July 2017", "journal-ref": "In Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics, pages 1983-1992, Vancouver, Canada, July 2017.\n  Association for Computational Linguistics", "doi": "10.18653/v1/P17-1181", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global constraints and reranking have not been used in cognates detection\nresearch to date. We propose methods for using global constraints by performing\nrescoring of the score matrices produced by state of the art cognates detection\nsystems. Using global constraints to perform rescoring is complementary to\nstate of the art methods for performing cognates detection and results in\nsignificant performance improvements beyond current state of the art\nperformance on publicly available datasets with different language pairs and\nvarious conditions such as different levels of baseline state of the art\nperformance and different data size conditions, including with more realistic\nlarge data size conditions than have been evaluated with in the past.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:04:50 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 20:19:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1704.07147", "submitter": "Yuki Fujimoto", "authors": "Yuki Fujimoto and Toru Ohira", "title": "A Neural Network model with Bidirectional Whitening", "comments": "16pages", "journal-ref": "In: Rutkowski L., Scherer R., Korytkowski M., Pedrycz W.,\n  Tadeusiewicz R., Zurada J. (eds) Artificial Intelligence and Soft Computing.\n  ICAISC 2018. Lecture Notes in Computer Science, vol 10841. Springer, Cham", "doi": "10.1007/978-3-319-91253-0_5", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a new model and algorithm which performs an efficient Natural\ngradient descent for Multilayer Perceptrons. Natural gradient descent was\noriginally proposed from a point of view of information geometry, and it\nperforms the steepest descent updates on manifolds in a Riemannian space. In\nparticular, we extend an approach taken by the \"Whitened neural networks\"\nmodel. We make the whitening process not only in feed-forward direction as in\nthe original model, but also in the back-propagation phase. Its efficacy is\nshown by an application of this \"Bidirectional whitened neural networks\" model\nto a handwritten character recognition data (MNIST data).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:18:58 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Fujimoto", "Yuki", ""], ["Ohira", "Toru", ""]]}, {"id": "1704.07223", "submitter": "Jack Fitzsimons", "authors": "Jack Fitzsimons, Diego Granziol, Kurt Cutajar, Michael Osborne,\n  Maurizio Filippone, Stephen Roberts", "title": "Entropic Trace Estimates for Log Determinants", "comments": "16 pages, 4 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalable calculation of matrix determinants has been a bottleneck to the\nwidespread application of many machine learning methods such as determinantal\npoint processes, Gaussian processes, generalised Markov random fields, graph\nmodels and many others. In this work, we estimate log determinants under the\nframework of maximum entropy, given information in the form of moment\nconstraints from stochastic trace estimation. The estimates demonstrate a\nsignificant improvement on state-of-the-art alternative methods, as shown on a\nwide variety of UFL sparse matrices. By taking the example of a general Markov\nrandom field, we also demonstrate how this approach can significantly\naccelerate inference in large-scale learning methods involving the log\ndeterminant.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:45:21 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Fitzsimons", "Jack", ""], ["Granziol", "Diego", ""], ["Cutajar", "Kurt", ""], ["Osborne", "Michael", ""], ["Filippone", "Maurizio", ""], ["Roberts", "Stephen", ""]]}, {"id": "1704.07228", "submitter": "Kiran Koshy Thekumparampil", "authors": "Sahand Negahban and Sewoong Oh and Kiran K. Thekumparampil and Jiaming\n  Xu", "title": "Learning from Comparisons and Choices", "comments": "77 pages, 12 figures; added new experiments and references. arXiv\n  admin note: substantial text overlap with arXiv:1506.07947", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When tracking user-specific online activities, each user's preference is\nrevealed in the form of choices and comparisons. For example, a user's purchase\nhistory is a record of her choices, i.e. which item was chosen among a subset\nof offerings. A user's preferences can be observed either explicitly as in\nmovie ratings or implicitly as in viewing times of news articles. Given such\nindividualized ordinal data in the form of comparisons and choices, we address\nthe problem of collaboratively learning representations of the users and the\nitems. The learned features can be used to predict a user's preference of an\nunseen item to be used in recommendation systems. This also allows one to\ncompute similarities among users and items to be used for categorization and\nsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)\nmodel in marketing and transportation, and also more recent successes in word\nembedding and crowdsourced image embedding, we pose this problem as learning\nthe MNL model parameters that best explain the data. We propose a convex\nrelaxation for learning the MNL model, and show that it is minimax optimal up\nto a logarithmic factor by comparing its performance to a fundamental lower\nbound. This characterizes the minimax sample complexity of the problem, and\nproves that the proposed estimator cannot be improved upon other than by a\nlogarithmic factor. Further, the analysis identifies how the accuracy depends\non the topology of sampling via the spectrum of the sampling graph. This\nprovides a guideline for designing surveys when one can choose which items are\nto be compared. This is accompanied by numerical simulations on synthetic and\nreal data sets, confirming our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:49:14 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 10:26:16 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Negahban", "Sahand", ""], ["Oh", "Sewoong", ""], ["Thekumparampil", "Kiran K.", ""], ["Xu", "Jiaming", ""]]}, {"id": "1704.07352", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria, Bamdev Mishra", "title": "Structured low-rank matrix learning: algorithms and applications", "comments": "Accepted in ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a low-rank matrix, constrained to lie in\na linear subspace, and introduce a novel factorization for modeling such\nmatrices. A salient feature of the proposed factorization scheme is it\ndecouples the low-rank and the structural constraints onto separate factors. We\nformulate the optimization problem on the Riemannian spectrahedron manifold,\nwhere the Riemannian framework allows to develop computationally efficient\nconjugate gradient and trust-region algorithms. Experiments on problems such as\nstandard/robust/non-negative matrix completion, Hankel matrix learning and\nmulti-task learning demonstrate the efficacy of our approach. A shorter version\nof this work has been published in ICML'18.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:47:29 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 04:10:24 GMT"}, {"version": "v3", "created": "Sat, 23 Dec 2017 07:50:44 GMT"}, {"version": "v4", "created": "Mon, 12 Feb 2018 18:07:36 GMT"}, {"version": "v5", "created": "Fri, 15 Jun 2018 10:27:07 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Jawanpuria", "Pratik", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1704.07353", "submitter": "Subhadeep Paul", "authors": "Subhadeep Paul, Yuguo Chen", "title": "Spectral and matrix factorization methods for consistent community\n  detection in multi-layer networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a consensus community structure by\ncombining information from multiple layers of a multi-layer network using\nmethods based on the spectral clustering or a low-rank matrix factorization. As\na general theme, these \"intermediate fusion\" methods involve obtaining a low\ncolumn rank matrix by optimizing an objective function and then using the\ncolumns of the matrix for clustering. However, the theoretical properties of\nthese methods remain largely unexplored. In the absence of statistical\nguarantees on the objective functions, it is difficult to determine if the\nalgorithms optimizing the objectives will return good community structures. We\ninvestigate the consistency properties of the global optimizer of some of these\nobjective functions under the multi-layer stochastic blockmodel. For this\npurpose, we derive several new asymptotic results showing consistency of the\nintermediate fusion techniques along with the spectral clustering of mean\nadjacency matrix under a high dimensional setup, where the number of nodes, the\nnumber of layers and the number of communities of the multi-layer graph grow.\nOur numerical study shows that the intermediate fusion techniques outperform\nlate fusion methods, namely spectral clustering on aggregate spectral kernel\nand module allegiance matrix in sparse networks, while they outperform the\nspectral clustering of mean adjacency matrix in multi-layer networks that\ncontain layers with both homophilic and heterophilic communities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:47:40 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 21:17:33 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 06:13:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Paul", "Subhadeep", ""], ["Chen", "Yuguo", ""]]}, {"id": "1704.07423", "submitter": "Julia Ling", "authors": "Julia Ling, Max Hutchinson, Erin Antono, Sean Paradiso, and Bryce\n  Meredig", "title": "High-Dimensional Materials and Process Optimization using Data-driven\n  Experimental Design with Well-Calibrated Uncertainty Estimates", "comments": null, "journal-ref": "Integrating Materials and Manufacturing Innovation, (2017)", "doi": "10.1007/s40192-017-0098-z", "report-no": null, "categories": "stat.ML cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of composition and processing to obtain materials that\nexhibit desirable characteristics has historically relied on a combination of\nscientist intuition, trial and error, and luck. We propose a methodology that\ncan accelerate this process by fitting data-driven models to experimental data\nas it is collected to suggest which experiment should be performed next. This\nmethodology can guide the scientist to test the most promising candidates\nearlier, and can supplement scientific intuition and knowledge with data-driven\ninsights. A key strength of the proposed framework is that it scales to\nhigh-dimensional parameter spaces, as are typical in materials discovery\napplications. Importantly, the data-driven models incorporate uncertainty\nanalysis, so that new experiments are proposed based on a combination of\nexploring high-uncertainty candidates and exploiting high-performing regions of\nparameter space. Over four materials science test cases, our methodology led to\nthe optimal candidate being found with three times fewer required measurements\nthan random guessing on average.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 05:07:35 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 16:26:17 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Ling", "Julia", ""], ["Hutchinson", "Max", ""], ["Antono", "Erin", ""], ["Paradiso", "Sean", ""], ["Meredig", "Bryce", ""]]}, {"id": "1704.07433", "submitter": "Haw-Shiuan Chang", "authors": "Haw-Shiuan Chang and Erik Learned-Miller and Andrew McCallum", "title": "Active Bias: Training More Accurate Neural Networks by Emphasizing High\n  Variance Samples", "comments": "camera-ready version for NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-paced learning and hard example mining re-weight training instances to\nimprove learning accuracy. This paper presents two improved alternatives based\non lightweight estimates of sample uncertainty in stochastic gradient descent\n(SGD): the variance in predicted probability of the correct class across\niterations of mini-batch SGD, and the proximity of the correct class\nprobability to the decision threshold. Extensive experimental results on six\ndatasets show that our methods reliably improve accuracy in various network\narchitectures, including additional gains on top of other popular training\ntechniques, such as residual learning, momentum, ADAM, batch normalization,\ndropout, and distillation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 19:48:49 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 18:04:36 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 03:18:57 GMT"}, {"version": "v4", "created": "Sat, 6 Jan 2018 20:33:39 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Chang", "Haw-Shiuan", ""], ["Learned-Miller", "Erik", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.07453", "submitter": "Daniel Lizotte", "authors": "Daniel J. Lizotte and Arezoo Tahmasebi", "title": "On Prediction and Tolerance Intervals for Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate tolerance interval methods for dynamic treatment\nregimes (DTRs) that can provide more detailed prognostic information to\npatients who will follow an estimated optimal regime. Although the problem of\nconstructing confidence intervals for DTRs has been extensively studied,\nprediction and tolerance intervals have received little attention. We begin by\nreviewing in detail different interval estimation and prediction methods and\nthen adapting them to the DTR setting. We illustrate some of the challenges\nassociated with tolerance interval estimation stemming from the fact that we do\nnot typically have data that were generated from the estimated optimal regime.\nWe give an extensive empirical evaluation of the methods and discussed several\npractical aspects of method choice, and we present an example application using\ndata from a clinical trial. Finally, we discuss future directions within this\nimportant emerging area of DTR research.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:28:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lizotte", "Daniel J.", ""], ["Tahmasebi", "Arezoo", ""]]}, {"id": "1704.07461", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Martin J. Wainwright, Thomas A. Courtade", "title": "Denoising Linear Models with Permuted Data", "comments": "To appear in part at ISIT 2017, Aachen", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate linear regression model with shuffled data and additive\nGaussian noise arises in various correspondence estimation and matching\nproblems. Focusing on the denoising aspect of this problem, we provide a\ncharacterization the minimax error rate that is sharp up to logarithmic\nfactors. We also analyze the performance of two versions of a computationally\nefficient estimator, and establish their consistency for a large range of input\nparameters. Finally, we provide an exact algorithm for the noiseless problem\nand demonstrate its performance on an image point-cloud matching task. Our\nanalysis also extends to datasets with outliers.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:46:48 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "1704.07487", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan", "title": "Bootstrapping Graph Convolutional Neural Networks for Autism Spectrum\n  Disorder Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using predictive models to identify patterns that can act as biomarkers for\ndifferent neuropathoglogical conditions is becoming highly prevalent. In this\npaper, we consider the problem of Autism Spectrum Disorder (ASD) classification\nwhere previous work has shown that it can be beneficial to incorporate a wide\nvariety of meta features, such as socio-cultural traits, into predictive\nmodeling. A graph-based approach naturally suits these scenarios, where a\ncontextual graph captures traits that characterize a population, while the\nspecific brain activity patterns are utilized as a multivariate signal at the\nnodes. Graph neural networks have shown improvements in inferencing with\ngraph-structured data. Though the underlying graph strongly dictates the\noverall performance, there exists no systematic way of choosing an appropriate\ngraph in practice, thus making predictive models non-robust. To address this,\nwe propose a bootstrapped version of graph convolutional neural networks\n(G-CNNs) that utilizes an ensemble of weakly trained G-CNNs, and reduce the\nsensitivity of models on the choice of graph construction. We demonstrate its\neffectiveness on the challenging Autism Brain Imaging Data Exchange (ABIDE)\ndataset and show that our approach improves upon recently proposed graph-based\nneural networks. We also show that our method remains more robust to noisy\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 22:52:32 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 02:42:31 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1704.07505", "submitter": "Feng Nan", "authors": "Feng Nan and Venkatesh Saligrama", "title": "Dynamic Model Selection for Prediction Under a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic model selection approach for resource-constrained\nprediction. Given an input instance at test-time, a gating function identifies\na prediction model for the input among a collection of models. Our objective is\nto minimize overall average cost without sacrificing accuracy. We learn gating\nand prediction models on fully labeled training data by means of a bottom-up\nstrategy. Our novel bottom-up method is a recursive scheme whereby a\nhigh-accuracy complex model is first trained. Then a low-complexity gating and\nprediction model are subsequently learnt to adaptively approximate the\nhigh-accuracy model in regions where low-cost models are capable of making\nhighly accurate predictions. We pose an empirical loss minimization problem\nwith cost constraints to jointly train gating and prediction models. On a\nnumber of benchmark datasets our method outperforms state-of-the-art achieving\nhigher accuracy for the same cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:17:22 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1704.07515", "submitter": "Shin Ando Ph. D.", "authors": "Shin Ando and Chun-Yuan Huang", "title": "Deep Over-sampling Framework for Classifying Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a challenging issue in practical classification problems\nfor deep learning models as well as traditional models. Traditionally\nsuccessful countermeasures such as synthetic over-sampling have had limited\nsuccess with complex, structured data handled by deep learning models. In this\npaper, we propose Deep Over-sampling (DOS), a framework for extending the\nsynthetic over-sampling method to exploit the deep feature space acquired by a\nconvolutional neural network (CNN). Its key feature is an explicit, supervised\nrepresentation learning, for which the training data presents each raw input\nsample with a synthetic embedding target in the deep feature space, which is\nsampled from the linear subspace of in-class neighbors. We implement an\niterative process of training the CNN and updating the targets, which induces\nsmaller in-class variance among the embeddings, to increase the discriminative\npower of the deep representation. We present an empirical study using public\nbenchmarks, which shows that the DOS framework not only counteracts class\nimbalance better than the existing method, but also improves the performance of\nthe CNN in the standard, balanced settings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 02:12:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 06:40:40 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 21:02:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ando", "Shin", ""], ["Huang", "Chun-Yuan", ""]]}, {"id": "1704.07520", "submitter": "Qiang Liu", "authors": "Qiang Liu", "title": "Stein Variational Gradient Descent as Gradient Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient descent (SVGD) is a deterministic sampling\nalgorithm that iteratively transports a set of particles to approximate given\ndistributions, based on an efficient gradient-based update that guarantees to\noptimally decrease the KL divergence within a function space. This paper\ndevelops the first theoretical analysis on SVGD, discussing its weak\nconvergence properties and showing that its asymptotic behavior is captured by\na gradient flow of the KL divergence functional under a new metric structure\ninduced by Stein operator. We also provide a number of results on Stein\noperator and Stein's identity using the notion of weak derivative, including a\nnew proof of the distinguishability of Stein discrepancy under weak conditions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 03:01:41 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:27:31 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Liu", "Qiang", ""]]}, {"id": "1704.07531", "submitter": "Longshaokan Wang", "authors": "Longshaokan Wang, Eric B. Laber, Katie Witkiewitz", "title": "Sufficient Markov Decision Processes with Alternating Deep Neural\n  Networks", "comments": "31 pages, 3 figures, extended abstract in the proceedings of\n  RLDM2017. (v2 revisions: Fixed a minor bug in the code w.r.t. setting seed,\n  as a result numbers in the simulation experiments had some slight changes,\n  but conclusions stayed the same. Corrected typos. Improved notations.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in mobile computing technologies have made it possible to monitor\nand apply data-driven interventions across complex systems in real time. Markov\ndecision processes (MDPs) are the primary model for sequential decision\nproblems with a large or indefinite time horizon. Choosing a representation of\nthe underlying decision process that is both Markov and low-dimensional is\nnon-trivial. We propose a method for constructing a low-dimensional\nrepresentation of the original decision process for which: 1. the MDP model\nholds; 2. a decision strategy that maximizes mean utility when applied to the\nlow-dimensional representation also maximizes mean utility when applied to the\noriginal process. We use a deep neural network to define a class of potential\nprocess representations and estimate the process of lowest dimension within\nthis class. The method is illustrated using data from a mobile study on heavy\ndrinking and smoking among college students.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:10:37 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 05:59:04 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wang", "Longshaokan", ""], ["Laber", "Eric B.", ""], ["Witkiewitz", "Katie", ""]]}, {"id": "1704.07535", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Mitchell Stern, Dan Klein", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "comments": "ACL 2017. MR and MS contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks like code generation and semantic parsing require mapping unstructured\n(or partially structured) inputs to well-formed, executable outputs. We\nintroduce abstract syntax networks, a modeling framework for these problems.\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\na decoder with a dynamically-determined modular structure paralleling the\nstructure of the output tree. On the benchmark Hearthstone dataset for code\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\nno task-specific engineering.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:37:35 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07548", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Jinpeng Li, Wei-long Zheng, Bao-liang Lu,\n  Huiguang He", "title": "Semi-supervised Bayesian Deep Multi-modal Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In emotion recognition, it is difficult to recognize human's emotional states\nusing just a single modality. Besides, the annotation of physiological\nemotional data is particularly expensive. These two aspects make the building\nof effective emotion recognition model challenging. In this paper, we first\nbuild a multi-view deep generative model to simulate the generative process of\nmulti-modality emotional data. By imposing a mixture of Gaussians assumption on\nthe posterior approximation of the latent variables, our model can learn the\nshared deep representation from multiple modalities. To solve the\nlabeled-data-scarcity problem, we further extend our multi-view model to\nsemi-supervised learning scenario by casting the semi-supervised classification\nproblem as a specialized missing data imputation task. Our semi-supervised\nmulti-view deep generative framework can leverage both labeled and unlabeled\ndata from multiple modalities, where the weight factor for each modality can be\nlearned automatically. Compared with previous emotion recognition methods, our\nmethod is more robust and flexible. The experiments conducted on two real\nmulti-modal emotion datasets have demonstrated the superiority of our framework\nover a number of competitors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 06:29:59 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["Li", "Jinpeng", ""], ["Zheng", "Wei-long", ""], ["Lu", "Bao-liang", ""], ["He", "Huiguang", ""]]}, {"id": "1704.07554", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Nadia Fawaz", "title": "A relevance-scalability-interpretability tradeoff with temporally\n  evolving user personas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current work characterizes the users of a VoD streaming space through\nuser-personas based on a tenure timeline and temporal behavioral features in\nthe absence of explicit user profiles. A combination of tenure timeline and\ntemporal characteristics caters to business needs of understanding the\nevolution and phases of user behavior as their accounts age. The personas\nconstructed in this work successfully represent both dominant and niche\ncharacterizations while providing insightful maturation of user behavior in the\nsystem. The two major highlights of our personas are demonstration of stability\nalong tenure timelines on a population level, while exhibiting interesting\nmigrations between labels on an individual granularity and clear\ninterpretability of user labels. Finally, we show a trade-off between an\nindispensable trio of guarantees, relevance-scalability-interpretability by\nusing summary information from personas in a CTR (Click through rate)\npredictive model. The proposed method of uncovering latent personas, consequent\ninsights from these and application of information from personas to predictive\nmodels are broadly applicable to other streaming based products.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 06:41:21 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 07:59:08 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Fawaz", "Nadia", ""]]}, {"id": "1704.07751", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Dan Klein", "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:52:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07807", "submitter": "Ming Yan", "authors": "Zhi Li and Wei Shi and Ming Yan", "title": "A decentralized proximal-gradient method with network independent\n  step-sizes and separated convergence rates", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, 67 (2019), 4494-4506", "doi": "10.1109/TSP.2019.2926022", "report-no": null, "categories": "math.OC cs.DC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel proximal-gradient algorithm for a decentralized\noptimization problem with a composite objective containing smooth and\nnon-smooth terms. Specifically, the smooth and nonsmooth terms are dealt with\nby gradient and proximal updates, respectively. The proposed algorithm is\nclosely related to a previous algorithm, PG-EXTRA \\cite{shi2015proximal}, but\nhas a few advantages. First of all, agents use uncoordinated step-sizes, and\nthe stable upper bounds on step-sizes are independent of network topologies.\nThe step-sizes depend on local objective functions, and they can be as large as\nthose of the gradient descent. Secondly, for the special case without\nnon-smooth terms, linear convergence can be achieved under the strong convexity\nassumption. The dependence of the convergence rate on the objective functions\nand the network are separated, and the convergence rate of the new algorithm is\nas good as one of the two convergence rates that match the typical rates for\nthe general gradient descent and the consensus averaging. We provide numerical\nexperiments to demonstrate the efficacy of the introduced algorithm and\nvalidate our theoretical discoveries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:36:15 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 02:26:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Li", "Zhi", ""], ["Shi", "Wei", ""], ["Yan", "Ming", ""]]}, {"id": "1704.07888", "submitter": "Waheed Bajwa", "authors": "Matthew Nokleby and Waheed U. Bajwa", "title": "Stochastic Optimization from Distributed, Streaming Data in Rate-limited\n  Networks", "comments": "16 pages, 6 figures; Accepted for publication in IEEE Transactions on\n  Signal and Information Processing over Networks", "journal-ref": "Published in IEEE Trans. Signal Inform. Proc. over Netw., vol. 5,\n  no. 1, pp. 152-167, Mar. 2019", "doi": "10.1109/TSIPN.2018.2866320", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by machine learning applications in networks of sensors,\ninternet-of-things (IoT) devices, and autonomous agents, we propose techniques\nfor distributed stochastic convex learning from high-rate data streams. The\nsetup involves a network of nodes---each one of which has a stream of data\narriving at a constant rate---that solve a stochastic convex optimization\nproblem by collaborating with each other over rate-limited communication links.\nTo this end, we present and analyze two algorithms---termed distributed\nstochastic approximation mirror descent (D-SAMD) and accelerated distributed\nstochastic approximation mirror descent (AD-SAMD)---that are based on two\nstochastic variants of mirror descent and in which nodes collaborate via\napproximate averaging of the local, noisy subgradients using distributed\nconsensus. Our main contributions are (i) bounds on the convergence rates of\nD-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio\nof the data streaming and communication rates, and (ii) sufficient conditions\nfor order-optimum convergence of these algorithms. In particular, we show that\nfor sufficiently well-connected networks, distributed learning schemes can\nobtain order-optimum convergence even if the communications rate is small.\nFurther we find that the use of accelerated methods significantly enlarges the\nregime in which order-optimum convergence is achieved; this is in contrast to\nthe centralized setting, where accelerated methods usually offer only a modest\nimprovement. Finally, we demonstrate the effectiveness of the proposed\nalgorithms using numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 19:52:52 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 18:41:03 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 10:31:27 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 08:42:35 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Nokleby", "Matthew", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1704.07926", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang", "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum\n  Marginal Likelihood", "comments": "Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to learn a semantic parser that maps natural language utterances\ninto executable programs when only indirect supervision is available: examples\nare labeled with the correct execution result, but not the program itself.\nConsequently, we must search the space of programs for those that output the\ncorrect result, while not being misled by spurious programs: incorrect programs\nthat coincidentally output the correct result. We connect two common learning\nparadigms, reinforcement learning (RL) and maximum marginal likelihood (MML),\nand then present a new learning algorithm that combines the strengths of both.\nThe new algorithm guards against spurious programs by combining the systematic\nsearch traditionally employed in MML with the randomized exploration of RL, and\nby updating parameters such that probability is spread more evenly across\nconsistent programs. We apply our learning algorithm to a new neural semantic\nparser and show significant gains over existing state-of-the-art results on a\nrecent context-dependent semantic parsing task.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 22:51:12 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Guu", "Kelvin", ""], ["Pasupat", "Panupong", ""], ["Liu", "Evan Zheran", ""], ["Liang", "Percy", ""]]}, {"id": "1704.07943", "submitter": "Fang Liu", "authors": "Swapna Buccapatnam, Fang Liu, Atilla Eryilmaz, Ness B. Shroff", "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on\n  Networks", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit (MAB) problem in the presence of\nside-observations across actions that occur as a result of an underlying\nnetwork structure. In our model, a bipartite graph captures the relationship\nbetween actions and a common set of unknowns such that choosing an action\nreveals observations for the unknowns that it is connected to. This models a\ncommon scenario in online social networks where users respond to their friends'\nactivity, thus providing side information about each other's preferences. Our\ncontributions are as follows: 1) We derive an asymptotic lower bound (with\nrespect to time) as a function of the bi-partite network structure on the\nregret of any uniformly good policy that achieves the maximum long-term average\nreward. 2) We propose two policies - a randomized policy; and a policy based on\nthe well-known upper confidence bound (UCB) policies - both of which explore\neach action at a rate that is a function of its network position. We show,\nunder mild assumptions, that these policies achieve the asymptotic lower bound\non the regret up to a multiplicative factor, independent of the network\nstructure. Finally, we use numerical examples on a real-world social network\nand a routing example network to demonstrate the benefits obtained by our\npolicies over other existing policies.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 01:53:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 20:39:01 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Buccapatnam", "Swapna", ""], ["Liu", "Fang", ""], ["Eryilmaz", "Atilla", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1704.07953", "submitter": "Feihu Huang", "authors": "Feihu Huang and Songcan Chen", "title": "Linear Convergence of Accelerated Stochastic Gradient Descent for\n  Nonconvex Nonsmooth Optimization", "comments": "This paper has been withdrawn by the author due to some errors in the\n  proof of the convergence analysis. They will modify these errors as soon as\n  possible", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic gradient descent (SGD) method for the\nnonconvex nonsmooth optimization, and propose an accelerated SGD method by\ncombining the variance reduction technique with Nesterov's extrapolation\ntechnique. Moreover, based on the local error bound condition, we establish the\nlinear convergence of our method to obtain a stationary point of the nonconvex\noptimization. In particular, we prove that not only the sequence generated\nlinearly converges to a stationary point of the problem, but also the\ncorresponding sequence of objective values is linearly convergent. Finally,\nsome numerical experiments demonstrate the effectiveness of our method. To the\nbest of our knowledge, it is first proved that the accelerated SGD method\nconverges linearly to the local minimum of the nonconvex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 02:43:24 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 23:05:21 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Huang", "Feihu", ""], ["Chen", "Songcan", ""]]}, {"id": "1704.07971", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Jason D. Lee", "title": "A Flexible Framework for Hypothesis Testing in High-dimensions", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing in the linear regression model is a fundamental\nstatistical problem. We consider linear regression in the high-dimensional\nregime where the number of parameters exceeds the number of samples ($p> n$).\nIn order to make informative inference, we assume that the model is\napproximately sparse, that is the effect of covariates on the response can be\nwell approximated by conditioning on a relatively small number of covariates\nwhose identities are unknown. We develop a framework for testing very general\nhypotheses regarding the model parameters. Our framework encompasses testing\nwhether the parameter lies in a convex cone, testing the signal strength, and\ntesting arbitrary functionals of the parameter. We show that the proposed\nprocedure controls the type I error, and also analyze the power of the\nprocedure. Our numerical experiments confirm our theoretical findings and\ndemonstrate that we control false positive rate (type I error) near the nominal\nlevel, and have high power. By duality between hypotheses testing and\nconfidence intervals, the proposed framework can be used to obtain valid\nconfidence intervals for various functionals of the model parameters. For\nlinear functionals, the length of confidence intervals is shown to be minimax\nrate optimal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 05:01:16 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:35:33 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 08:10:47 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 06:11:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1704.07987", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni", "title": "Training L1-Regularized Models with Orthant-Wise Passive Descent\n  Algorithms", "comments": "Accepted to The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18). Feb 2018, New Orleans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $L_1$-regularized models are widely used for sparse regression or\nclassification tasks. In this paper, we propose the orthant-wise passive\ndescent algorithm (OPDA) for optimizing $L_1$-regularized models, as an\nimproved substitute of proximal algorithms, which are the standard tools for\noptimizing the models nowadays. OPDA uses a stochastic variance-reduced\ngradient (SVRG) to initialize the descent direction, then apply a novel\nalignment operator to encourage each element keeping the same sign after one\niteration of update, so the parameter remains in the same orthant as before. It\nalso explicitly suppresses the magnitude of each element to impose sparsity.\nThe quasi-Newton update can be utilized to incorporate curvature information\nand accelerate the speed. We prove a linear convergence rate for OPDA on\ngeneral smooth and strongly-convex loss functions. By conducting experiments on\n$L_1$-regularized logistic regression and convolutional neural networks, we\nshow that OPDA outperforms state-of-the-art stochastic proximal algorithms,\nimplying a wide range of applications in training sparse models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 07:07:13 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 15:20:53 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 08:57:23 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wangni", "Jianqiao", ""]]}, {"id": "1704.08045", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "The loss surface of deep and wide neural networks", "comments": "ICML 2017. Main results now hold for larger classes of loss functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the optimization problem behind deep neural networks is highly\nnon-convex, it is frequently observed in practice that training deep networks\nseems possible without getting stuck in suboptimal points. It has been argued\nthat this is the case as all local minima are close to being globally optimal.\nWe show that this is (almost) true, in fact almost all local minima are\nglobally optimal, for a fully connected network with squared loss and analytic\nactivation function given that the number of hidden units of one layer of the\nnetwork is larger than the number of training points and the network structure\nfrom this layer on is pyramidal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:24:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 19:43:39 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1704.08067", "submitter": "Arnaud Joly", "authors": "Arnaud Joly", "title": "Exploiting random projections and sparsity with random forests and\n  gradient boosting methods -- Application to multi-label and multi-output\n  learning, random forest model compression and leveraging input sparsity", "comments": "PhD Thesis, Liege, Dec 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within machine learning, the supervised learning field aims at modeling the\ninput-output relationship of a system, from past observations of its behavior.\nDecision trees characterize the input-output relationship through a series of\nnested $if-then-else$ questions, the testing nodes, leading to a set of\npredictions, the leaf nodes. Several of such trees are often combined together\nfor state-of-the-art performance: random forest ensembles average the\npredictions of randomized decision trees trained independently in parallel,\nwhile tree boosting ensembles train decision trees sequentially to refine the\npredictions made by the previous ones. The emergence of new applications\nrequires scalable supervised learning algorithms in terms of computational\npower and memory space with respect to the number of inputs, outputs, and\nobservations without sacrificing accuracy. In this thesis, we identify three\nmain areas where decision tree methods could be improved for which we provide\nand evaluate original algorithmic solutions: (i) learning over high dimensional\noutput spaces, (ii) learning with large sample datasets and stringent memory\nconstraints at prediction time and (iii) learning over high dimensional sparse\ninput spaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:45:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Joly", "Arnaud", ""]]}, {"id": "1704.08095", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki and Ann B. Lee", "title": "Converting High-Dimensional Regression to High-Dimensional Conditional\n  Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing demand for nonparametric conditional density estimators\n(CDEs) in fields such as astronomy and economics. In astronomy, for example,\none can dramatically improve estimates of the parameters that dictate the\nevolution of the Universe by working with full conditional densities instead of\nregression (i.e., conditional mean) estimates. More generally, standard\nregression falls short in any prediction problem where the distribution of the\nresponse is more complex with multi-modality, asymmetry or heteroscedastic\nnoise. Nevertheless, much of the work on high-dimensional inference concerns\nregression and classification only, whereas research on density estimation has\nlagged behind. Here we propose FlexCode, a fully nonparametric approach to\nconditional density estimation that reformulates CDE as a non-parametric\northogonal series problem where the expansion coefficients are estimated by\nregression. By taking such an approach, one can efficiently estimate\nconditional densities and not just expectations in high dimensions by drawing\nupon the success in high-dimensional regression. Depending on the choice of\nregression procedure, our method can adapt to a variety of challenging\nhigh-dimensional settings with different structures in the data (e.g., a large\nnumber of irrelevant components and nonlinear manifold structure) as well as\ndifferent data types (e.g., functional data, mixed data types and sample sets).\nWe study the theoretical and empirical performance of our proposed method, and\nwe compare our approach with traditional conditional density estimators on\nsimulated as well as real-world data, such as photometric galaxy data, Twitter\ndata, and line-of-sight velocities in a galaxy cluster.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:21:43 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""]]}, {"id": "1704.08101", "submitter": "Sebastiaan J. van Zelst", "authors": "Sebastiaan J. van Zelst, Boudewijn F. van Dongen, Wil M.P. van der\n  Aalst", "title": "Event Stream-Based Process Discovery using Abstract Representations", "comments": "Accepted for publication in \"Knowledge and Information Systems; \"\n  (Springer: http://link.springer.com/journal/10115)", "journal-ref": null, "doi": "10.1007/s10115-017-1060-2", "report-no": null, "categories": "cs.DB cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of process discovery, originating from the area of process mining, is\nto discover a process model based on business process execution data. A\nmajority of process discovery techniques relies on an event log as an input. An\nevent log is a static source of historical data capturing the execution of a\nbusiness process. In this paper we focus on process discovery relying on online\nstreams of business process execution events. Learning process models from\nevent streams poses both challenges and opportunities, i.e. we need to handle\nunlimited amounts of data using finite memory and, preferably, constant time.\nWe propose a generic architecture that allows for adopting several classes of\nexisting process discovery techniques in context of event streams. Moreover, we\nprovide several instantiations of the architecture, accompanied by\nimplementations in the process mining tool-kit ProM (http://promtools.org).\nUsing these instantiations, we evaluate several dimensions of stream-based\nprocess discovery. The evaluation shows that the proposed architecture allows\nus to lift process discovery to the streaming domain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:10:35 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["van Zelst", "Sebastiaan J.", ""], ["van Dongen", "Boudewijn F.", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1704.08165", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger, Purvasha Chakravarti and Jining Qin", "title": "A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 15:37:50 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Hechtlinger", "Yotam", ""], ["Chakravarti", "Purvasha", ""], ["Qin", "Jining", ""]]}, {"id": "1704.08227", "submitter": "Rahul Kidambi", "authors": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli and\n  Aaron Sidford", "title": "Accelerating Stochastic Gradient Descent For Least Squares Regression", "comments": "54 pages, 3 figures, 1 table; updated acknowledgements, minor title\n  change. Paper appeared in the proceedings of the Conference on Learning\n  Theory (COLT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is widespread sentiment that it is not possible to effectively utilize\nfast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy\nball) for the purposes of stochastic optimization due to their instability and\nerror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,\nGlineur, and Nesterov 2014. This work considers these issues for the special\ncase of stochastic approximation for the least squares regression problem, and\nour main result refutes the conventional wisdom by showing that acceleration\ncan be made robust to statistical errors. In particular, this work introduces\nan accelerated stochastic gradient method that provably achieves the minimax\noptimal statistical risk faster than stochastic gradient descent. Critical to\nthe analysis is a sharp characterization of accelerated stochastic gradient\ndescent as a stochastic process. We hope this characterization gives insights\ntowards the broader question of designing simple and effective accelerated\nstochastic methods for more general convex and non-convex optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:30:27 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 18:11:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Jain", "Prateek", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1704.08231", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Dana Yang and W. D. Brinda", "title": "Estimating the Coefficients of a Mixture of Two Linear Regressions by\n  Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give convergence guarantees for estimating the coefficients of a symmetric\nmixture of two linear regressions by expectation maximization (EM). In\nparticular, we show that the empirical EM iterates converge to the target\nparameter vector at the parametric rate, provided the algorithm is initialized\nin an unbounded cone. In particular, if the initial guess has a sufficiently\nlarge cosine angle with the target parameter vector, a sample-splitting version\nof the EM algorithm converges to the true coefficient vector with high\nprobability. Interestingly, our analysis borrows from tools used in the problem\nof estimating the centers of a symmetric mixture of two Gaussians by EM. We\nalso show that the population EM operator for mixtures of two regressions is\nanti-contractive from the target parameter vector if the cosine angle between\nthe input vector and the target parameter vector is too small, thereby\nestablishing the necessity of our conic condition. Finally, we give empirical\nevidence supporting this theoretical observation, which suggests that the\nsample based EM algorithm performs poorly when initial guesses are drawn\naccordingly. Our simulation study also suggests that the EM algorithm performs\nwell even under model misspecification (i.e., when the covariate and error\ndistributions violate the model assumptions).\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:37:40 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:23:37 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 03:11:30 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Yang", "Dana", ""], ["Brinda", "W. D.", ""]]}, {"id": "1704.08265", "submitter": "Chunxia Zhang", "authors": "Chunxia Zhang, Yilei Wu and Mu Zhu", "title": "Pruning variable selection ensembles", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of variable selection, ensemble learning has gained increasing\ninterest due to its great potential to improve selection accuracy and to reduce\nfalse discovery rate. A novel ordering-based selective ensemble learning\nstrategy is designed in this paper to obtain smaller but more accurate\nensembles. In particular, a greedy sorting strategy is proposed to rearrange\nthe order by which the members are included into the integration process.\nThrough stopping the fusion process early, a smaller subensemble with higher\nselection accuracy can be obtained. More importantly, the sequential inclusion\ncriterion reveals the fundamental strength-diversity trade-off among ensemble\nmembers. By taking stability selection (abbreviated as StabSel) as an example,\nsome experiments are conducted with both simulated and real-world data to\nexamine the performance of the novel algorithm. Experimental results\ndemonstrate that pruned StabSel generally achieves higher selection accuracy\nand lower false discovery rates than StabSel and several other benchmark\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 18:01:10 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Zhang", "Chunxia", ""], ["Wu", "Yilei", ""], ["Zhu", "Mu", ""]]}, {"id": "1704.08303", "submitter": "Mehmet A. S\\\"uzen PhD", "authors": "Mehmet S\\\"uzen, Cornelius Weber and Joan J. Cerd\\`a", "title": "Spectral Ergodicity in Deep Learning Architectures via Surrogate Random\n  Matrices", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": "10.5281/zenodo.822411 and 10.5281/zenodo.579642", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel method to quantify spectral ergodicity for random\nmatrices is presented. The new methodology combines approaches rooted in the\nmetrics of Thirumalai-Mountain (TM) and Kullbach-Leibler (KL) divergence. The\nmethod is applied to a general study of deep and recurrent neural networks via\nthe analysis of random matrix ensembles mimicking typical weight matrices of\nthose systems. In particular, we examine circular random matrix ensembles:\ncircular unitary ensemble (CUE), circular orthogonal ensemble (COE), and\ncircular symplectic ensemble (CSE). Eigenvalue spectra and spectral ergodicity\nare computed for those ensembles as a function of network size. It is observed\nthat as the matrix size increases the level of spectral ergodicity of the\nensemble rises, i.e., the eigenvalue spectra obtained for a single realisation\nat random from the ensemble is closer to the spectra obtained averaging over\nthe whole ensemble. Based on previous results we conjecture that success of\ndeep learning architectures is strongly bound to the concept of spectral\nergodicity. The method to compute spectral ergodicity proposed in this work\ncould be used to optimise the size and architecture of deep as well as\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:26:08 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 21:47:38 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 09:57:03 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["S\u00fczen", "Mehmet", ""], ["Weber", "Cornelius", ""], ["Cerd\u00e0", "Joan J.", ""]]}, {"id": "1704.08305", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers", "title": "Limits of End-to-End Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning refers to training a possibly complex learning system by\napplying gradient-based learning to the system as a whole. End-to-end learning\nsystem is specifically designed so that all modules are differentiable. In\neffect, not only a central learning machine, but also all \"peripheral\" modules\nlike representation learning and memory formation are covered by a holistic\nlearning process. The power of end-to-end learning has been demonstrated on\nmany tasks, like playing a whole array of Atari video games with a single\narchitecture. While pushing for solutions to more challenging tasks, network\narchitectures keep growing more and more complex.\n  In this paper we ask the question whether and to what extent end-to-end\nlearning is a future-proof technique in the sense of scaling to complex and\ndiverse data processing architectures. We point out potential inefficiencies,\nand we argue in particular that end-to-end learning does not make optimal use\nof the modular design of present neural networks. Our surprisingly simple\nexperiments demonstrate these inefficiencies, up to the complete breakdown of\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 19:12:37 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Glasmachers", "Tobias", ""]]}, {"id": "1704.08306", "submitter": "Michael Smith", "authors": "Michael R. Smith, Aaron J. Hill, Kristofor D. Carlson, Craig M.\n  Vineyard, Jonathon Donaldson, David R. Follett, Pamela L. Follett, John H.\n  Naegle, Conrad D. James, James B. Aimone", "title": "A Digital Neuromorphic Architecture Efficiently Facilitating Complex\n  Synaptic Response Functions Applied to Liquid State Machines", "comments": "8 pages, 4 Figures, Preprint of 2017 IJCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information in neural networks is represented as weighted connections, or\nsynapses, between neurons. This poses a problem as the primary computational\nbottleneck for neural networks is the vector-matrix multiply when inputs are\nmultiplied by the neural network weights. Conventional processing architectures\nare not well suited for simulating neural networks, often requiring large\namounts of energy and time. Additionally, synapses in biological neural\nnetworks are not binary connections, but exhibit a nonlinear response function\nas neurotransmitters are emitted and diffuse between neurons. Inspired by\nneuroscience principles, we present a digital neuromorphic architecture, the\nSpiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex\nsynaptic response functions without requiring additional hardware components.\nWe consider the paradigm of spiking neurons with temporally coded information\nas opposed to non-spiking rate coded neurons used in most neural networks. In\nthis paradigm we examine liquid state machines applied to speech recognition\nand show how a liquid state machine with temporal dynamics maps onto the\nSTPU-demonstrating the flexibility and efficiency of the STPU for instantiating\nneural algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:12:31 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Smith", "Michael R.", ""], ["Hill", "Aaron J.", ""], ["Carlson", "Kristofor D.", ""], ["Vineyard", "Craig M.", ""], ["Donaldson", "Jonathon", ""], ["Follett", "David R.", ""], ["Follett", "Pamela L.", ""], ["Naegle", "John H.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1704.08349", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu, Yingying Fan, Kun Chen, Jinchi Lv and Wei Lin", "title": "SOFAR: large-scale association network learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern big data applications feature large scale in both numbers of\nresponses and predictors. Better statistical efficiency and scientific insights\ncan be enabled by understanding the large-scale response-predictor association\nnetwork structures via layers of sparse latent factors ranked by importance.\nYet sparsity and orthogonality have been two largely incompatible goals. To\naccommodate both features, in this paper we suggest the method of sparse\northogonal factor regression (SOFAR) via the sparse singular value\ndecomposition with orthogonality constrained optimization to learn the\nunderlying association networks, with broad applications to both unsupervised\nand supervised learning tasks such as biclustering with sparse singular value\ndecomposition, sparse principal component analysis, sparse factor analysis, and\nspare vector autoregression analysis. Exploiting the framework of\nconvexity-assisted nonconvex optimization, we derive nonasymptotic error bounds\nfor the suggested procedure characterizing the theoretical advantages. The\nstatistical guarantees are powered by an efficient SOFAR algorithm with\nconvergence property. Both computational and theoretical advantages of our\nprocedure are demonstrated with several simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:05:18 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Uematsu", "Yoshimasa", ""], ["Fan", "Yingying", ""], ["Chen", "Kun", ""], ["Lv", "Jinchi", ""], ["Lin", "Wei", ""]]}, {"id": "1704.08361", "submitter": "David Von Dollen", "authors": "David Von Dollen", "title": "Identifying Similarities in Epileptic Patients for Drug Resistance\n  Prediction", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, approximately 30% of epileptic patients treated with antiepileptic\ndrugs (AEDs) remain resistant to treatment (known as refractory patients). This\nproject seeks to understand the underlying similarities in refractory patients\nvs. other epileptic patients, identify features contributing to drug resistance\nacross underlying phenotypes for refractory patients, and develop predictive\nmodels for drug resistance in epileptic patients. In this study, epileptic\npatient data was examined to attempt to observe discernable similarities or\ndifferences in refractory patients (case) and other non-refractory patients\n(control) to map underlying mechanisms in causality. For the first part of the\nstudy, unsupervised algorithms such as Kmeans, Spectral Clustering, and\nGaussian Mixture Models were used to examine patient features projected into a\nlower dimensional space. Results from this study showed a high degree of\nnon-linearity in the underlying feature space. For the second part of this\nstudy, classification algorithms such as Logistic Regression, Gradient Boosted\nDecision Trees, and SVMs, were tested on the reduced-dimensionality features,\nwith accuracy results of 0.83(+/-0.3) testing using 7 fold cross validation.\nObservations of test results indicate using a radial basis function kernel PCA\nto reduce features ingested by a Gradient Boosted Decision Tree Ensemble lead\nto gains in improved accuracy in mapping a binary decision to highly non-linear\nfeatures collected from epileptic patients.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:53:16 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Von Dollen", "David", ""]]}, {"id": "1704.08383", "submitter": "Qingyang Li", "authors": "Qingyang Li, Dajiang Zhu, Jie Zhang, Derrek Paul Hibar, Neda\n  Jahanshad, Yalin Wang, Jieping Ye, Paul M. Thompson, Jie Wang", "title": "Large-scale Feature Selection of Risk Genetic Factors for Alzheimer's\n  Disease via Distributed Group Lasso Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) have achieved great success in the\ngenetic study of Alzheimer's disease (AD). Collaborative imaging genetics\nstudies across different research institutions show the effectiveness of\ndetecting genetic risk factors. However, the high dimensionality of GWAS data\nposes significant challenges in detecting risk SNPs for AD. Selecting relevant\nfeatures is crucial in predicting the response variable. In this study, we\npropose a novel Distributed Feature Selection Framework (DFSF) to conduct the\nlarge-scale imaging genetics studies across multiple institutions. To speed up\nthe learning process, we propose a family of distributed group Lasso screening\nrules to identify irrelevant features and remove them from the optimization.\nThen we select the relevant group features by performing the group Lasso\nfeature selection process in a sequence of parameters. Finally, we employ the\nstability selection to rank the top risk SNPs that might help detect the early\nstage of AD. To the best of our knowledge, this is the first distributed\nfeature selection model integrated with group Lasso feature selection as well\nas detecting the risk genetic factors across multiple research institutions\nsystem. Empirical studies are conducted on 809 subjects with 5.9 million SNPs\nwhich are distributed across several individual institutions, demonstrating the\nefficiency and effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:02:34 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Li", "Qingyang", ""], ["Zhu", "Dajiang", ""], ["Zhang", "Jie", ""], ["Hibar", "Derrek Paul", ""], ["Jahanshad", "Neda", ""], ["Wang", "Yalin", ""], ["Ye", "Jieping", ""], ["Thompson", "Paul M.", ""], ["Wang", "Jie", ""]]}, {"id": "1704.08424", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Andrew Gordon Wilson", "title": "Multimodal Word Distributions", "comments": "This paper also appears at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings provide point representations of words containing useful\nsemantic information. We introduce multimodal word distributions formed from\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\ninformation. To learn these distributions, we propose an energy-based\nmax-margin objective. We show that the resulting approach captures uniquely\nexpressive semantic information, and outperforms alternatives, such as word2vec\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\nsimilarity and entailment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 03:59:54 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 17:56:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1704.08488", "submitter": "Dieter Hendricks", "authors": "Dieter Hendricks and Stephen J. Roberts", "title": "Optimal client recommendation for market makers in illiquid financial\n  products", "comments": "12 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of liquidity provision in financial markets can result in\nprolonged exposure to illiquid instruments for market makers. In this case,\nwhere a proprietary position is not desired, pro-actively targeting the right\nclient who is likely to be interested can be an effective means to offset this\nposition, rather than relying on commensurate interest arising through natural\ndemand. In this paper, we consider the inference of a client profile for the\npurpose of corporate bond recommendation, based on typical recorded information\navailable to the market maker. Given a historical record of corporate bond\ntransactions and bond meta-data, we use a topic-modelling analogy to develop a\nprobabilistic technique for compiling a curated list of client recommendations\nfor a particular bond that needs to be traded, ranked by probability of\ninterest. We show that a model based on Latent Dirichlet Allocation offers\npromising performance to deliver relevant recommendations for sales traders.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 09:28:50 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Hendricks", "Dieter", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1704.08504", "submitter": "Szu-Wei Fu", "authors": "Szu-Wei Fu, Ting-yao Hu, Yu Tsao, and Xugang Lu", "title": "Complex spectrogram enhancement by convolutional neural network with\n  multi-metrics learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to address two issues existing in the current speech\nenhancement methods: 1) the difficulty of phase estimations; 2) a single\nobjective function cannot consider multiple metrics simultaneously. To solve\nthe first problem, we propose a novel convolutional neural network (CNN) model\nfor complex spectrogram enhancement, namely estimating clean real and imaginary\n(RI) spectrograms from noisy ones. The reconstructed RI spectrograms are\ndirectly used to synthesize enhanced speech waveforms. In addition, since\nlog-power spectrogram (LPS) can be represented as a function of RI\nspectrograms, its reconstruction is also considered as another target. Thus a\nunified objective function, which combines these two targets (reconstruction of\nRI spectrograms and LPS), is equivalent to simultaneously optimizing two\ncommonly used objective metrics: segmental signal-to-noise ratio (SSNR) and\nlogspectral distortion (LSD). Therefore, the learning process is called\nmulti-metrics learning (MML). Experimental results confirm the effectiveness of\nthe proposed CNN with RI spectrograms and MML in terms of improved standardized\nevaluation metrics on a speech enhancement task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 11:01:33 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 14:49:27 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Fu", "Szu-Wei", ""], ["Hu", "Ting-yao", ""], ["Tsao", "Yu", ""], ["Lu", "Xugang", ""]]}, {"id": "1704.08676", "submitter": "Daniele Ramazzotti", "authors": "Stefano Beretta and Mauro Castelli and Ivo Goncalves and Roberto\n  Henriques and Daniele Ramazzotti", "title": "Learning the structure of Bayesian Networks: A quantitative assessment\n  of the effect of different algorithmic schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging tasks when adopting Bayesian Networks (BNs) is\nthe one of learning their structure from data. This task is complicated by the\nhuge search space of possible solutions, and by the fact that the problem is\nNP-hard. Hence, full enumeration of all the possible solutions is not always\nfeasible and approximations are often required. However, to the best of our\nknowledge, a quantitative analysis of the performance and characteristics of\nthe different heuristics to solve this problem has never been done before.\n  For this reason, in this work, we provide a detailed comparison of many\ndifferent state-of-the-arts methods for structural learning on simulated data\nconsidering both BNs with discrete and continuous variables, and with different\nrates of noise in the data. In particular, we investigate the performance of\ndifferent widespread scores and algorithmic approaches proposed for the\ninference and the statistical pitfalls within them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:40:22 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 20:16:37 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Beretta", "Stefano", ""], ["Castelli", "Mauro", ""], ["Goncalves", "Ivo", ""], ["Henriques", "Roberto", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1704.08683", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Yingyu Liang and David P. Woodruff and\n  Hongyang Zhang", "title": "Matrix Completion and Related Problems via Strong Duality", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the strong duality of non-convex matrix factorization\nproblems: we show that under certain dual conditions, these problems and its\ndual have the same optimum. This has been well understood for convex\noptimization, but little was known for non-convex problems. We propose a novel\nanalytical framework and show that under certain dual conditions, the optimal\nsolution of the matrix factorization program is the same as its bi-dual and\nthus the global optimality of the non-convex program can be achieved by solving\nits bi-dual which is convex. These dual conditions are satisfied by a wide\nclass of matrix factorization problems, although matrix factorization problems\nare hard to solve in full generality. This analytical framework may be of\nindependent interest to non-convex optimization more broadly.\n  We apply our framework to two prototypical matrix factorization problems:\nmatrix completion and robust Principal Component Analysis (PCA). These are\nexamples of efficiently recovering a hidden matrix given limited reliable\nobservations of it. Our framework shows that exact recoverability and strong\nduality hold with nearly-optimal sample complexity guarantees for matrix\ncompletion and robust PCA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:54:46 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 17:51:47 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 11:39:03 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 14:48:14 GMT"}, {"version": "v5", "created": "Wed, 25 Apr 2018 14:14:39 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Liang", "Yingyu", ""], ["Woodruff", "David P.", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1704.08715", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Mikhail A. Ryabinin", "title": "A Siamese Deep Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Siamese Deep Forest (SDF) is proposed in the paper. It is based on the Deep\nForest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest\nmodification. It can be also regarded as an alternative to the well-known\nSiamese neural networks. The SDF uses a modified training set consisting of\nconcatenated pairs of vectors. Moreover, it defines the class distributions in\nthe deep forest as the weighted sum of the tree class probabilities such that\nthe weights are determined in order to reduce distances between similar pairs\nand to increase them between dissimilar points. We show that the weights can be\nobtained by solving a quadratic optimization problem. The SDF aims to prevent\noverfitting which takes place in neural networks when only limited training\ndata are available. The numerical experiments illustrate the proposed distance\nmetric method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 18:51:41 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Utkin", "Lev V.", ""], ["Ryabinin", "Mikhail A.", ""]]}, {"id": "1704.08727", "submitter": "Danil Kuzin", "authors": "Danil Kuzin, Olga Isupova, Lyudmila Mihaylova", "title": "Structured Sparse Modelling with Hierarchical GP", "comments": "SPARS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new Bayesian model for sparse linear regression with a\nspatio-temporal structure is proposed. It incorporates the structural\nassumptions based on a hierarchical Gaussian process prior for spike and slab\ncoefficients. We design an inference algorithm based on Expectation Propagation\nand evaluate the model over the real data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 19:36:41 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Kuzin", "Danil", ""], ["Isupova", "Olga", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1704.08742", "submitter": "Patrick Breheny", "authors": "Yaohui Zeng, Tianbao Yang, Patrick Breheny", "title": "Hybrid safe-strong rules for efficient optimization in lasso-type\n  problems", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso model has been widely used for model selection in data mining,\nmachine learning, and high-dimensional statistical analysis. However, with the\nultrahigh-dimensional, large-scale data sets now collected in many real-world\napplications, it is important to develop algorithms to solve the lasso that\nefficiently scale up to problems of this size. Discarding features from certain\nsteps of the algorithm is a powerful technique for increasing efficiency and\naddressing the Big Data challenge. In this paper, we propose a family of hybrid\nsafe-strong rules (HSSR) which incorporate safe screening rules into the\nsequential strong rule (SSR) to remove unnecessary computational burden. In\nparticular, we present two instances of HSSR, namely SSR-Dome and SSR-BEDPP,\nfor the standard lasso problem. We further extend SSR-BEDPP to the elastic net\nand group lasso problems to demonstrate the generalizability of the hybrid\nscreening idea. Extensive numerical experiments with synthetic and real data\nsets are conducted for both the standard lasso and the group lasso problems.\nResults show that our proposed hybrid rules can substantially outperform\nexisting state-of-the-art rules.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 20:53:16 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:41:25 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 16:27:57 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zeng", "Yaohui", ""], ["Yang", "Tianbao", ""], ["Breheny", "Patrick", ""]]}, {"id": "1704.08756", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz", "title": "A Network Perspective on Stratification of Multi-Label Data", "comments": "submitted for ECML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, we have witnessed the development of multi-label\nclassification methods which utilize the structure of the label space in a\ndivide and conquer approach to improve classification performance and allow\nlarge data sets to be classified efficiently. Yet most of the available data\nsets have been provided in train/test splits that did not account for\nmaintaining a distribution of higher-order relationships between labels among\nsplits or folds. We present a new approach to stratifying multi-label data for\nclassification purposes based on the iterative stratification approach proposed\nby Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the\niterative approach to take into account second-order relationships between\nlabels. Obtained results are evaluated using statistical properties of obtained\nstrata as presented by Sechidis. We also propose new statistical measures\nrelevant to second-order quality: label pairs distribution, the percentage of\nlabel pairs without positive evidence in folds and label pair - fold pairs that\nhave no positive evidence for the label pair. We verify the impact of new\nmethods on classification performance of Binary Relevance, Label Powerset and a\nfast greedy community detection based label space partitioning classifier.\nRandom Forests serve as base classifiers. We check the variation of the number\nof communities obtained per fold, and the stability of their modularity score.\nSecond-Order Iterative Stratification is compared to standard k-fold, label\nset, and iterative stratification. The proposed approach lowers the variance of\nclassification quality, improves label pair oriented measures and example\ndistribution while maintaining a competitive quality in label-oriented\nmeasures. We also witness an increase in stability of network characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 21:43:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1704.08769", "submitter": "Sung-Min Park", "authors": "Miyeon Jung, You-Bin Lee, Sang-Man Jin, Sung-Min Park", "title": "Prediction of Daytime Hypoglycemic Events Using Continuous Glucose\n  Monitoring Data and Classification Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daytime hypoglycemia should be accurately predicted to achieve normoglycemia\nand to avoid disastrous situations. Hypoglycemia, an abnormally low blood\nglucose level, is divided into daytime hypoglycemia and nocturnal hypoglycemia.\nMany studies of hypoglycemia prevention deal with nocturnal hypoglycemia. In\nthis paper, we propose new predictor variables to predict daytime hypoglycemia\nusing continuous glucose monitoring (CGM) data. We apply classification and\nregression tree (CART) as a prediction method. The independent variables of our\nprediction model are the rate of decrease from a peak and absolute level of the\nBG at the decision point. The evaluation results showed that our model was able\nto detect almost 80% of hypoglycemic events 15 min in advance, which was higher\nthan the existing methods with similar conditions. The proposed method might\nachieve a real-time prediction as well as can be embedded into BG monitoring\ndevice.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 22:55:05 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Jung", "Miyeon", ""], ["Lee", "You-Bin", ""], ["Jin", "Sang-Man", ""], ["Park", "Sung-Min", ""]]}, {"id": "1704.08783", "submitter": "Gunwoong Park Gunwoong Park", "authors": "Gunwoong Park, Garvesh Raskutti", "title": "Learning Quadratic Variance Function (QVF) DAG models via OverDispersion\n  Scoring (ODS)", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning DAG or Bayesian network models is an important problem in\nmulti-variate causal inference. However, a number of challenges arises in\nlearning large-scale DAG models including model identifiability and\ncomputational complexity since the space of directed graphs is huge. In this\npaper, we address these issues in a number of steps for a broad class of DAG\nmodels where the noise or variance is signal-dependent. Firstly we introduce a\nnew class of identifiable DAG models, where each node has a distribution where\nthe variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG\nmodels include many interesting classes of distributions such as Poisson,\nBinomial, Geometric, Exponential, Gamma and many other distributions in which\nthe noise variance depends on the mean. We prove that this class of QVF DAG\nmodels is identifiable, and introduce a new algorithm, the OverDispersion\nScoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm\nis based on firstly learning the moralized or undirected graphical model\nrepresentation of the DAG to reduce the DAG search-space, and then exploiting\nthe quadratic variance property to learn the causal ordering. We show through\ntheoretical results and simulations that our algorithm is statistically\nconsistent in the high-dimensional p>n setting provided that the degree of the\nmoralized graph is bounded and performs well compared to state-of-the-art\nDAG-learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 01:36:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Park", "Gunwoong", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1704.08792", "submitter": "Renato Negrinho", "authors": "Renato Negrinho, Geoff Gordon", "title": "DeepArchitect: Automatically Designing and Training Deep Architectures", "comments": "12 pages, 10 figures. Code available at\n  https://github.com/negrinho/deep_architect. See\n  http://www.cs.cmu.edu/~negrinho/ for more info. In submission to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, performance is strongly affected by the choice of\narchitecture and hyperparameters. While there has been extensive work on\nautomatic hyperparameter optimization for simple spaces, complex spaces such as\nthe space of deep architectures remain largely unexplored. As a result, the\nchoice of architecture is done manually by the human expert through a slow\ntrial and error process guided mainly by intuition. In this paper we describe a\nframework for automatically designing and training deep models. We propose an\nextensible and modular language that allows the human expert to compactly\nrepresent complex search spaces over architectures and their hyperparameters.\nThe resulting search spaces are tree-structured and therefore easy to traverse.\nModels can be automatically compiled to computational graphs once values for\nall hyperparameters have been chosen. We can leverage the structure of the\nsearch space to introduce different model search algorithms, such as random\nsearch, Monte Carlo tree search (MCTS), and sequential model-based optimization\n(SMBO). We present experiments comparing the different algorithms on CIFAR-10\nand show that MCTS and SMBO outperform random search. In addition, these\nexperiments show that our framework can be used effectively for model\ndiscovery, as it is possible to describe expressive search spaces and discover\ncompetitive models without much effort from the human expert. Code for our\nframework and experiments has been made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 02:48:38 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Negrinho", "Renato", ""], ["Gordon", "Geoff", ""]]}, {"id": "1704.08829", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou, and Nesreen K. Ahmed", "title": "Deep Feature Learning for Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general graph representation learning framework called\nDeepGL for learning deep node and edge representations from large (attributed)\ngraphs. In particular, DeepGL begins by deriving a set of base features (e.g.,\ngraphlet features) and automatically learns a multi-layered hierarchical graph\nrepresentation where each successive layer leverages the output from the\nprevious layer to learn features of a higher-order. Contrary to previous work,\nDeepGL learns relational functions (each representing a feature) that\ngeneralize across-networks and therefore useful for graph-based transfer\nlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learns\ninterpretable features, and is space-efficient (by learning sparse feature\nvectors). In addition, DeepGL is expressive, flexible with many interchangeable\ncomponents, efficient with a time complexity of $\\mathcal{O}(|E|)$, and\nscalable for large networks via an efficient parallel implementation. Compared\nwith the state-of-the-art method, DeepGL is (1) effective for across-network\ntransfer learning tasks and attributed graph representation learning, (2)\nspace-efficient requiring up to 6x less memory, (3) fast with up to 182x\nspeedup in runtime performance, and (4) accurate with an average improvement of\n20% or more on many learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 07:31:11 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 10:31:05 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1704.08847", "submitter": "Moustapha Cisse", "authors": "Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin,\n  Nicolas Usunier", "title": "Parseval Networks: Improving Robustness to Adversarial Examples", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Parseval networks, a form of deep neural networks in which the\nLipschitz constant of linear, convolutional and aggregation layers is\nconstrained to be smaller than 1. Parseval networks are empirically and\ntheoretically motivated by an analysis of the robustness of the predictions\nmade by deep neural networks when their input is subject to an adversarial\nperturbation. The most important feature of Parseval networks is to maintain\nweight matrices of linear and convolutional layers to be (approximately)\nParseval tight frames, which are extensions of orthogonal matrices to\nnon-square matrices. We describe how these constraints can be maintained\nefficiently during SGD. We show that Parseval networks match the\nstate-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House\nNumbers (SVHN) while being more robust than their vanilla counterpart against\nadversarial examples. Incidentally, Parseval networks also tend to train faster\nand make a better usage of the full capacity of the networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:43:55 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 01:11:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Cisse", "Moustapha", ""], ["Bojanowski", "Piotr", ""], ["Grave", "Edouard", ""], ["Dauphin", "Yann", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1704.08913", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Jie Chen, C\\'edric Richard", "title": "Adaptation and learning over networks for nonlinear system modeling", "comments": "To be published as a chapter in `Adaptive Learning Methods for\n  Nonlinear System Modeling', Elsevier Publishing, Eds. D. Comminiello and J.C.\n  Principe (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we analyze nonlinear filtering problems in distributed\nenvironments, e.g., sensor networks or peer-to-peer protocols. In these\nscenarios, the agents in the environment receive measurements in a streaming\nfashion, and they are required to estimate a common (nonlinear) model by\nalternating local computations and communications with their neighbors. We\nfocus on the important distinction between single-task problems, where the\nunderlying model is common to all agents, and multitask problems, where each\nagent might converge to a different model due to, e.g., spatial dependencies or\nother factors. Currently, most of the literature on distributed learning in the\nnonlinear case has focused on the single-task case, which may be a strong\nlimitation in real-world scenarios. After introducing the problem and reviewing\nthe existing approaches, we describe a simple kernel-based algorithm tailored\nfor the multitask case. We evaluate the proposal on a simulated benchmark task,\nand we conclude by detailing currently open problems and lines of research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 13:08:51 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Scardapane", "Simone", ""], ["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1704.08999", "submitter": "Pan Li", "authors": "Pan Li, Baihong Jin, Dai Wang, Baosen Zhang", "title": "Distribution System Voltage Control under Uncertainties using Tractable\n  Chance Constraints", "comments": "20 pages, 4 figures. Accepted by IEEE Transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voltage control plays an important role in the operation of electricity\ndistribution networks, especially with high penetration of distributed energy\nresources. These resources introduce significant and fast varying\nuncertainties. In this paper, we focus on reactive power compensation to\ncontrol voltage in the presence of uncertainties. We adopt a chance constraint\napproach that accounts for arbitrary correlations between renewable resources\nat each of the buses. We show how the problem can be solved efficiently using\nhistorical samples via a stochastic quasi gradient method. We also show that\nthis optimization problem is convex for a wide variety of probabilistic\ndistributions. Compared to conventional per-bus chance constraints, our\nformulation is more robust to uncertainty and more computationally tractable.\nWe illustrate the results using standard IEEE distribution test feeders.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 16:38:26 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 00:58:17 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 21:42:28 GMT"}, {"version": "v4", "created": "Tue, 6 Nov 2018 06:36:20 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Pan", ""], ["Jin", "Baihong", ""], ["Wang", "Dai", ""], ["Zhang", "Baosen", ""]]}, {"id": "1704.09011", "submitter": "Khashayar Khosravi", "authors": "Hamsa Bastani and Mohsen Bayati and Khashayar Khosravi", "title": "Mostly Exploration-Free Algorithms for Contextual Bandits", "comments": "62 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual bandit literature has traditionally focused on algorithms that\naddress the exploration-exploitation tradeoff. In particular, greedy algorithms\nthat exploit current estimates without any exploration may be sub-optimal in\ngeneral. However, exploration-free greedy algorithms are desirable in practical\nsettings where exploration may be costly or unethical (e.g., clinical trials).\nSurprisingly, we find that a simple greedy algorithm can be rate optimal\n(achieves asymptotically optimal regret) if there is sufficient randomness in\nthe observed contexts (covariates). We prove that this is always the case for a\ntwo-armed bandit under a general class of context distributions that satisfy a\ncondition we term covariate diversity. Furthermore, even absent this condition,\nwe show that a greedy algorithm can be rate optimal with positive probability.\nThus, standard bandit algorithms may unnecessarily explore. Motivated by these\nresults, we introduce Greedy-First, a new algorithm that uses only observed\ncontexts and rewards to determine whether to follow a greedy algorithm or to\nexplore. We prove that this algorithm is rate optimal without any additional\nassumptions on the context distribution or the number of arms. Extensive\nsimulations demonstrate that Greedy-First successfully reduces exploration and\noutperforms existing (exploration-based) contextual bandit algorithms such as\nThompson sampling or upper confidence bound (UCB).\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:15:56 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:10:41 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 22:54:32 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 16:08:16 GMT"}, {"version": "v5", "created": "Tue, 2 Oct 2018 23:27:05 GMT"}, {"version": "v6", "created": "Fri, 30 Aug 2019 21:41:36 GMT"}, {"version": "v7", "created": "Mon, 25 Nov 2019 19:50:43 GMT"}, {"version": "v8", "created": "Sun, 19 Apr 2020 02:21:25 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bastani", "Hamsa", ""], ["Bayati", "Mohsen", ""], ["Khosravi", "Khashayar", ""]]}, {"id": "1704.09021", "submitter": "Xin Liu", "authors": "Xin Liu and Mahesan Niranjan", "title": "Parameter Estimation in Computational Biology by Approximate Bayesian\n  Computation coupled with Sensitivity Analysis", "comments": "17 pages, 5 figures. Under submission to Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of parameter estimation in models of systems biology\nfrom noisy observations. The models we consider are characterized by\nsimultaneous deterministic nonlinear differential equations whose parameters\nare either taken from in vitro experiments, or are hand-tuned during the model\ndevelopment process to reproduces observations from the system. We consider the\nfamily of algorithms coming under the Bayesian formulation of Approximate\nBayesian Computation (ABC), and show that sensitivity analysis could be\ndeployed to quantify the relative roles of different parameters in the system.\nParameters to which a system is relatively less sensitive (known as sloppy\nparameters) need not be estimated to high precision, while the values of\nparameters that are more critical (stiff parameters) need to be determined with\ncare. A tradeoff between computational complexity and the accuracy with which\nthe posterior distribution may be probed is an important characteristic of this\nclass of algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:33:10 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Liu", "Xin", ""], ["Niranjan", "Mahesan", ""]]}]