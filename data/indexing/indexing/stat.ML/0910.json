[{"id": "0910.0115", "submitter": "Philipp Hennig", "authors": "Philipp Hennig", "title": "Expectation Propagation on the Maximum of Correlated Normal Variables", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems involving questions of optimality ask for the maximum\nor the minimum of a finite set of unknown quantities. This technical report\nderives the first two posterior moments of the maximum of two correlated\nGaussian variables and the first two posterior moments of the two generating\nvariables (corresponding to Gaussian approximations minimizing relative\nentropy). It is shown how this can be used to build a heuristic approximation\nto the maximum relationship over a finite set of Gaussian variables, allowing\napproximate inference by Expectation Propagation on such quantities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 09:12:38 GMT"}], "update_date": "2009-10-02", "authors_parsed": [["Hennig", "Philipp", ""]]}, {"id": "0910.0483", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis, Aikaterini Mitrokotsa", "title": "Statistical Decision Making for Authentication and Intrusion Detection", "comments": "13 pages, 2 figures, to be presented at ICMLA 2009", "journal-ref": null, "doi": null, "report-no": "IAS-UVA-09-02", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User authentication and intrusion detection differ from standard\nclassification problems in that while we have data generated from legitimate\nusers, impostor or intrusion data is scarce or non-existent. We review existing\ntechniques for dealing with this problem and propose a novel alternative based\non a principled statistical decision-making view point. We examine the\ntechnique on a toy problem and validate it on complex real-world data from an\nRFID based access control system. The results indicate that it can\nsignificantly outperform the classical world model approach. The method could\nbe more generally useful in other decision-making scenarios where there is a\nlack of adversary data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 19:43:40 GMT"}], "update_date": "2009-12-26", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Mitrokotsa", "Aikaterini", ""]]}, {"id": "0910.0526", "submitter": "Holger Hoefling", "authors": "Holger Hoefling", "title": "A path algorithm for the Fused Lasso Signal Approximator", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is a very well known penalized regression model, which adds an\n$L_{1}$ penalty with parameter $\\lambda_{1}$ on the coefficients to the squared\nerror loss function. The Fused Lasso extends this model by also putting an\n$L_{1}$ penalty with parameter $\\lambda_{2}$ on the difference of neighboring\ncoefficients, assuming there is a natural ordering. In this paper, we develop a\nfast path algorithm for solving the Fused Lasso Signal Approximator that\ncomputes the solutions for all values of $\\lambda_1$ and $\\lambda_2$. In the\nsupplement, we also give an algorithm for the general Fused Lasso for the case\nwith predictor matrix $\\bX \\in \\mathds{R}^{n \\times p}$ with\n$\\text{rank}(\\bX)=p$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2009 09:03:31 GMT"}], "update_date": "2009-10-06", "authors_parsed": [["Hoefling", "Holger", ""]]}, {"id": "0910.0610", "submitter": "Ambuj  Tewari", "authors": "Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari", "title": "Regularization Techniques for Learning with Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing body of learning problems for which it is natural to\norganize the parameters into matrix, so as to appropriately regularize the\nparameters under some matrix norm (in order to impose some more sophisticated\nprior knowledge). This work describes and analyzes a systematic method for\nconstructing such matrix-based, regularization methods. In particular, we focus\non how the underlying statistical properties of a given problem can help us\ndecide which regularization function is appropriate.\n  Our methodology is based on the known duality fact: that a function is\nstrongly convex with respect to some norm if and only if its conjugate function\nis strongly smooth with respect to the dual norm. This result has already been\nfound to be a key component in deriving and analyzing several learning\nalgorithms. We demonstrate the potential of this framework by deriving novel\ngeneralization and regret bounds for multi-task learning, multi-class learning,\nand kernel learning.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2009 14:48:46 GMT"}, {"version": "v2", "created": "Sun, 17 Oct 2010 21:34:13 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Kakade", "Sham M.", ""], ["Shalev-Shwartz", "Shai", ""], ["Tewari", "Ambuj", ""]]}, {"id": "0910.0722", "submitter": "Peter B\\\"uhlmann", "authors": "Sara A. van de Geer and Peter B\\\"uhlmann", "title": "On the conditions used to prove oracle results for the Lasso", "comments": "33 pages, 1 figure", "journal-ref": "Electronic Journal of Statistics, 3, (2009), 1360-1392", "doi": "10.1214/09-EJS506", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oracle inequalities and variable selection properties for the Lasso in linear\nmodels have been established under a variety of different assumptions on the\ndesign matrix. We show in this paper how the different conditions and concepts\nrelate to each other. The restricted eigenvalue condition (Bickel et al., 2009)\nor the slightly weaker compatibility condition (van de Geer, 2007) are\nsufficient for oracle results. We argue that both these conditions allow for a\nfairly general class of design matrices. Hence, optimality of the Lasso for\nprediction and estimation holds for more general situations than what it\nappears from coherence (Bunea et al, 2007b,c) or restricted isometry (Candes\nand Tao, 2005) assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 11:00:51 GMT"}], "update_date": "2010-01-13", "authors_parsed": [["van de Geer", "Sara A.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0910.0949", "submitter": "Dariusz Plewczynski", "authors": "Dariusz Plewczynski (ICM, Interdisciplinary Centre for Mathematical\n  and Computational Modelling, University of Warsaw, Pawinskiego 5a Street,\n  02-106 Warsaw, Poland)", "title": "BRAINSTORMING: Consensus Learning in Practice", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here an introduction to Brainstorming approach, that was recently\nproposed as a consensus meta-learning technique, and used in several practical\napplications in bioinformatics and chemoinformatics. The consensus learning\ndenotes heterogeneous theoretical classification method, where one trains an\nensemble of machine learning algorithms using different types of input training\ndata representations. In the second step all solutions are gathered and the\nconsensus is build between them. Therefore no early solution, given even by a\ngenerally low performing algorithm, is not discarder until the late phase of\nprediction, when the final conclusion is drawn by comparing different machine\nlearning models. This final phase, i.e. consensus learning, is trying to\nbalance the generality of solution and the overall performance of trained\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 08:47:29 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Plewczynski", "Dariusz", "", "ICM, Interdisciplinary Centre for Mathematical\n  and Computational Modelling, University of Warsaw, Pawinskiego 5a Street,\n  02-106 Warsaw, Poland"]]}, {"id": "0910.1013", "submitter": "Stephane Canu", "authors": "Stephane Canu (LITIS), Xavier Mary, Alain Rakotomamonjy (LITIS)", "title": "Functional learning through kernels", "comments": null, "journal-ref": "Advances in Learning Theory: Methods, Models and Application, J.\n  Suykens, G. Horvath, S. Basu, C. Micchelli, J. Vandewalle (Ed.) (2003) 89-110", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the functional aspects of statistical learning theory. The\nmain point under consideration is the nature of the hypothesis set when no\nprior information is available but data. Within this framework we first discuss\nabout the hypothesis set: it is a vectorial space, it is a set of pointwise\ndefined functions, and the evaluation functional on this set is a continuous\nmapping. Based on these principles an original theory is developed generalizing\nthe notion of reproduction kernel Hilbert space to non hilbertian sets. Then it\nis shown that the hypothesis set of any learning machine has to be a\ngeneralized reproducing set. Therefore, thanks to a general \"representer\ntheorem\", the solution of the learning problem is still a linear combination of\na kernel. Furthermore, a way to design these kernels is given. To illustrate\nthis framework some examples of such reproducing sets and kernels are given.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 14:19:08 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Canu", "Stephane", "", "LITIS"], ["Mary", "Xavier", "", "LITIS"], ["Rakotomamonjy", "Alain", "", "LITIS"]]}, {"id": "0910.1022", "submitter": "David Blei", "authors": "David M. Blei and Peter I. Frazier", "title": "Distance Dependent Chinese Restaurant Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the distance dependent Chinese restaurant process (CRP), a\nflexible class of distributions over partitions that allows for\nnon-exchangeability. This class can be used to model many kinds of dependencies\nbetween data in infinite clustering models, including dependencies across time\nor space. We examine the properties of the distance dependent CRP, discuss its\nconnections to Bayesian nonparametric mixture models, and derive a Gibbs\nsampler for both observed and mixture settings. We study its performance with\nthree text corpora. We show that relaxing the assumption of exchangeability\nwith distance dependent CRPs can provide a better fit to sequential data. We\nalso show its alternative formulation of the traditional CRP leads to a\nfaster-mixing Gibbs sampling algorithm than the one based on the original\nformulation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 14:46:20 GMT"}, {"version": "v2", "created": "Fri, 17 Dec 2010 17:21:52 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2011 21:30:47 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Blei", "David M.", ""], ["Frazier", "Peter I.", ""]]}, {"id": "0910.2145", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen", "title": "Node harvest", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS367 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 2049-2072", "doi": "10.1214/10-AOAS367", "report-no": "IMS-AOAS-AOAS367", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When choosing a suitable technique for regression and classification with\nmultivariate predictor variables, one is often faced with a tradeoff between\ninterpretability and high predictive accuracy. To give a classical example,\nclassification and regression trees are easy to understand and interpret. Tree\nensembles like Random Forests provide usually more accurate predictions. Yet\ntree ensembles are also more difficult to analyze than single trees and are\noften criticized, perhaps unfairly, as `black box' predictors. Node harvest is\ntrying to reconcile the two aims of interpretability and predictive accuracy by\ncombining positive aspects of trees and tree ensembles. Results are very sparse\nand interpretable and predictive accuracy is extremely competitive, especially\nfor low signal-to-noise data. The procedure is simple: an initial set of a few\nthousand nodes is generated randomly. If a new observation falls into just a\nsingle node, its prediction is the mean response of all training observation\nwithin this node, identical to a tree-like prediction. A new observation falls\ntypically into several nodes and its prediction is then the weighted average of\nthe mean responses across all these nodes. The only role of node harvest is to\n`pick' the right nodes from the initial large ensemble of nodes by choosing\nnode weights, which amounts in the proposed algorithm to a quadratic\nprogramming problem with linear inequality constraints. The solution is sparse\nin the sense that only very few nodes are selected with a nonzero weight. This\nsparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to\nselect a tuning parameter for optimal predictive accuracy. Node harvest can\nhandle mixed data and missing values and is shown to be simple to interpret and\ncompetitive in predictive accuracy on a variety of data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2009 12:12:46 GMT"}, {"version": "v2", "created": "Fri, 7 Jan 2011 09:02:24 GMT"}], "update_date": "2011-01-10", "authors_parsed": [["Meinshausen", "Nicolai", ""]]}, {"id": "0910.2340", "submitter": "Laurent Rouviere", "authors": "G\\'erard Biau (LSTA), Benoit Cadre (IRMAR), Laurent Rouvi\\`ere (IRMAR)", "title": "A Stochastic Model for Collaborative Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative recommendation is an information-filtering technique that\nattempts to present information items (movies, music, books, news, images, Web\npages, etc.) that are likely of interest to the Internet user. Traditionally,\ncollaborative systems deal with situations with two types of variables, users\nand items. In its most common form, the problem is framed as trying to estimate\nratings for items that have not yet been consumed by a user. Despite\nwide-ranging literature, little is known about the statistical properties of\nrecommendation systems. In fact, no clear probabilistic model even exists\nallowing us to precisely describe the mathematical forces driving collaborative\nfiltering. To provide an initial contribution to this, we propose to set out a\ngeneral sequential stochastic model for collaborative recommendation and\nanalyze its asymptotic performance as the number of users grows. We offer an\nin-depth analysis of the so-called cosine-type nearest neighbor collaborative\nmethod, which is one of the most widely used algorithms in collaborative\nfiltering. We establish consistency of the procedure under mild assumptions on\nthe model. Rates of convergence and examples are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 18:57:05 GMT"}], "update_date": "2009-10-14", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Cadre", "Benoit", "", "IRMAR"], ["Rouvi\u00e8re", "Laurent", "", "IRMAR"]]}, {"id": "0910.2517", "submitter": "Zhiyi Chi", "authors": "Zhiyi Chi", "title": "$L_0$ regularized estimation for nonlinear models that have sparse\n  underlying linear structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of $\\beta$ for the nonlinear model $y =\nf(X\\sp{\\top}\\beta) + \\epsilon$ when $f$ is a nonlinear transformation that is\nknown, $\\beta$ has sparse nonzero coordinates, and the number of observations\ncan be much smaller than that of parameters ($n\\ll p$). We show that in order\nto bound the $L_2$ error of the $L_0$ regularized estimator $\\hat\\beta$, i.e.,\n$\\|\\hat\\beta - \\beta\\|_2$, it is sufficient to establish two conditions. Based\non this, we obtain bounds of the $L_2$ error for (1) $L_0$ regularized maximum\nlikelihood estimation (MLE) for exponential linear models and (2) $L_0$\nregularized least square (LS) regression for the more general case where $f$ is\nanalytic. For the analytic case, we rely on power series expansion of $f$,\nwhich requires taking into account the singularities of $f$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2009 02:35:54 GMT"}], "update_date": "2009-10-15", "authors_parsed": [["Chi", "Zhiyi", ""]]}, {"id": "0910.3099", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Zhen Liu", "title": "Efficient Bayesian analysis of multiple changepoint models with\n  dependence across segments", "comments": "26 pages, 4 fgures", "journal-ref": null, "doi": null, "report-no": "Lancaster eprint no 26279", "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian analysis of a class of multiple changepoint models.\nWhile there are a variety of efficient ways to analyse these models if the\nparameters associated with each segment are independent, there are few general\napproaches for models where the parameters are dependent. Under the assumption\nthat the dependence is Markov, we propose an efficient online algorithm for\nsampling from an approximation to the posterior distribution of the number and\nposition of the changepoints. In a simulation study, we show that the\napproximation introduced is negligible. We illustrate the power of our approach\nthrough fitting piecewise polynomial models to data, under a model which allows\nfor either continuity or discontinuity of the underlying curve at each\nchangepoint. This method is competitive with, or out-performs, other methods\nfor inferring curves from noisy data; and uniquely it allows for inference of\nthe locations of discontinuities in the underlying curve.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2009 12:22:40 GMT"}], "update_date": "2009-10-19", "authors_parsed": [["Fearnhead", "Paul", ""], ["Liu", "Zhen", ""]]}, {"id": "0910.4135", "submitter": "Florin Popescu", "authors": "Florin Popescu, Daniel Renz", "title": "Sparsification and feature selection by compressive linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Description Length (MDL) principle states that the optimal model\nfor a given data set is that which compresses it best. Due to practial\nlimitations the model can be restricted to a class such as linear regression\nmodels, which we address in this study. As in other formulations such as the\nLASSO and forward step-wise regression we are interested in sparsifying the\nfeature set while preserving generalization ability. We derive a\nwell-principled set of codes for both parameters and error residuals along with\nsmooth approximations to lengths of these codes as to allow gradient descent\noptimization of description length, and go on to show that sparsification and\nfeature selection using our approach is faster than the LASSO on several\ndatasets from the UCI and StatLib repositories, with favorable generalization\naccuracy, while being fully automatic, requiring neither cross-validation nor\ntuning of regularization hyper-parameters, allowing even for a nonlinear\nexpansion of the feature set followed by sparsification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2009 16:27:18 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Popescu", "Florin", ""], ["Renz", "Daniel", ""]]}, {"id": "0910.4397", "submitter": "Robert Nowak", "authors": "Robert D. Nowak", "title": "The Geometry of Generalized Binary Search", "comments": "corrected typo in Thm 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of determining a binary-valued function\nthrough a sequence of strategically selected queries. The focus is an algorithm\ncalled Generalized Binary Search (GBS). GBS is a well-known greedy algorithm\nfor determining a binary-valued function through a sequence of strategically\nselected queries. At each step, a query is selected that most evenly splits the\nhypotheses under consideration into two disjoint subsets, a natural\ngeneralization of the idea underlying classic binary search. This paper\ndevelops novel incoherence and geometric conditions under which GBS achieves\nthe information-theoretically optimal query complexity; i.e., given a\ncollection of N hypotheses, GBS terminates with the correct function after no\nmore than a constant times log N queries. Furthermore, a noise-tolerant version\nof GBS is developed that also achieves the optimal query complexity. These\nresults are applied to learning halfspaces, a problem arising routinely in\nimage processing and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2009 20:21:49 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2009 14:45:30 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2011 19:46:15 GMT"}, {"version": "v4", "created": "Mon, 8 Aug 2011 18:39:47 GMT"}, {"version": "v5", "created": "Tue, 25 Jun 2013 15:59:32 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Nowak", "Robert D.", ""]]}, {"id": "0910.4636", "submitter": "J\\\"uri Lember", "authors": "J. Lember", "title": "On approximation of smoothing probabilities for hidden Markov models", "comments": "submitted to Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the smoothing probabilities of hidden Markov model (HMM). We show\nthat under fairly general conditions for HMM, the exponential forgetting still\nholds, and the smoothing probabilities can be well approximated with the ones\nof double sided HMM. This makes it possible to use ergodic theorems. As an\napplications we consider the pointwise maximum a posteriori segmentation, and\nshow that the corresponding risks converge.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2009 10:11:01 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2011 16:28:33 GMT"}, {"version": "v3", "created": "Tue, 10 May 2011 09:30:25 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Lember", "J.", ""]]}, {"id": "0910.5454", "submitter": "Patrick C. McGuire", "authors": "P.C. McGuire, C. Gross, L. Wendt, A. Bonnici, V. Souza-Egipsy, J.\n  Ormo, E. Diaz-Martinez, B.H. Foing, R. Bose, S. Walter, M. Oesker, J. Ontrup,\n  R. Haschke, H. Ritter", "title": "The Cyborg Astrobiologist: Testing a Novelty-Detection Algorithm on Two\n  Mobile Exploration Systems at Rivas Vaciamadrid in Spain and at the Mars\n  Desert Research Station in Utah", "comments": "28 pages, 12 figures, accepted for publication in the International\n  Journal of Astrobiology", "journal-ref": "International Journal of Astrobiology, Vol. 9, pp. 11-27 (2010).", "doi": "10.1017/S1473550409990358", "report-no": null, "categories": "cs.CV astro-ph.EP astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (ABRIDGED) In previous work, two platforms have been developed for testing\ncomputer-vision algorithms for robotic planetary exploration (McGuire et al.\n2004b,2005; Bartolo et al. 2007). The wearable-computer platform has been\ntested at geological and astrobiological field sites in Spain (Rivas\nVaciamadrid and Riba de Santiuste), and the phone-camera has been tested at a\ngeological field site in Malta. In this work, we (i) apply a Hopfield\nneural-network algorithm for novelty detection based upon color, (ii) integrate\na field-capable digital microscope on the wearable computer platform, (iii)\ntest this novelty detection with the digital microscope at Rivas Vaciamadrid,\n(iv) develop a Bluetooth communication mode for the phone-camera platform, in\norder to allow access to a mobile processing computer at the field sites, and\n(v) test the novelty detection on the Bluetooth-enabled phone-camera connected\nto a netbook computer at the Mars Desert Research Station in Utah. This systems\nengineering and field testing have together allowed us to develop a real-time\ncomputer-vision system that is capable, for example, of identifying lichens as\nnovel within a series of images acquired in semi-arid desert environments. We\nacquired sequences of images of geologic outcrops in Utah and Spain consisting\nof various rock types and colors to test this algorithm. The algorithm robustly\nrecognized previously-observed units by their color, while requiring only a\nsingle image or a few images to learn colors as familiar, demonstrating its\nfast learning capability.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2009 18:26:39 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["McGuire", "P. C.", ""], ["Gross", "C.", ""], ["Wendt", "L.", ""], ["Bonnici", "A.", ""], ["Souza-Egipsy", "V.", ""], ["Ormo", "J.", ""], ["Diaz-Martinez", "E.", ""], ["Foing", "B. H.", ""], ["Bose", "R.", ""], ["Walter", "S.", ""], ["Oesker", "M.", ""], ["Ontrup", "J.", ""], ["Haschke", "R.", ""], ["Ritter", "H.", ""]]}, {"id": "0910.5561", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Xiaohai Sun, Bernhard Schoelkopf", "title": "Distinguishing Cause and Effect via Second Order Exponential Models", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to infer causal structures containing both discrete and\ncontinuous variables. The idea is to select causal hypotheses for which the\nconditional density of every variable, given its causes, becomes smooth. We\ndefine a family of smooth densities and conditional densities by second order\nexponential models, i.e., by maximizing conditional entropy subject to first\nand second statistical moments. If some of the variables take only values in\nproper subsets of R^n, these conditionals can induce different families of\njoint distributions even for Markov-equivalent graphs.\n  We consider the case of one binary and one real-valued variable where the\nmethod can distinguish between cause and effect. Using this example, we\ndescribe that sometimes a causal hypothesis must be rejected because\nP(effect|cause) and P(cause) share algorithmic information (which is untypical\nif they are chosen independently). This way, our method is in the same spirit\nas faithfulness-based causal inference because it also rejects non-generic\nmutual adjustments among DAG-parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2009 08:45:03 GMT"}], "update_date": "2009-10-30", "authors_parsed": [["Janzing", "Dominik", ""], ["Sun", "Xiaohai", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "0910.5761", "submitter": "Jose Bento", "authors": "Jose Bento, Andrea Montanari", "title": "Which graphical models are difficult to learn?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms systematically fail when the Markov random field\ndevelops long-range correlations. More precisely, this phenomenon appears to be\nrelated to the Ising model phase transition (although it does not coincide with\nit).\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 01:10:44 GMT"}], "update_date": "2009-11-07", "authors_parsed": [["Bento", "Jose", ""], ["Montanari", "Andrea", ""]]}]