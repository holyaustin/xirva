[{"id": "1703.00039", "submitter": "Hiromitsu Mizutani", "authors": "Hiromitsu Mizutani (1) and Ryota Kanai (1) ((1) Araya Inc.)", "title": "A description length approach to determining the number of k-means\n  clusters", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotic criterion to determine the optimal number of\nclusters in k-means. We consider k-means as data compression, and propose to\nadopt the number of clusters that minimizes the estimated description length\nafter compression. Here we report two types of compression ratio based on two\nways to quantify the description length of data after compression. This\napproach further offers a way to evaluate whether clusters obtained with\nk-means have a hierarchical structure by examining whether multi-stage\ncompression can further reduce the description length. We applied our criteria\nto determine the number of clusters to synthetic data and empirical\nneuroimaging data to observe the behavior of the criteria across different\ntypes of data set and suitability of the two types of criteria for different\ndatasets. We found that our method can offer reasonable clustering results that\nare useful for dimension reduction. While our numerical results revealed\ndependency of our criteria on the various aspects of dataset such as the\ndimensionality, the description length approach proposed here provides a useful\nguidance to determine the number of clusters in a principled manner when\nunderlying properties of the data are unknown and only inferred from\nobservation of data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 20:05:08 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Mizutani", "Hiromitsu", "", "Araya Inc"], ["Kanai", "Ryota", "", "Araya Inc"]]}, {"id": "1703.00048", "submitter": "Lihong Li", "authors": "Lihong Li and Yu Lu and Dengyong Zhou", "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits", "comments": "Published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits are widely used in Internet services from news\nrecommendation to advertising, and to Web search. Generalized linear models\n(logistical regression in particular) have demonstrated stronger performance\nthan linear models in many applications where rewards are binary. However, most\ntheoretical analyses on contextual bandits so far are on linear bandits. In\nthis work, we propose an upper confidence bound based algorithm for generalized\nlinear contextual bandits, which achieves an $\\tilde{O}(\\sqrt{dT})$ regret over\n$T$ rounds with $d$ dimensional feature vectors. This regret matches the\nminimax lower bound, up to logarithmic terms, and improves on the best previous\nresult by a $\\sqrt{d}$ factor, assuming the number of arms is fixed. A key\ncomponent in our analysis is to establish a new, sharp finite-sample confidence\nbound for maximum-likelihood estimates in generalized linear models, which may\nbe of independent interest. We also analyze a simpler upper confidence bound\nalgorithm, which is useful in practice, and prove it to have optimal regret for\ncertain cases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 20:39:44 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 04:07:45 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Li", "Lihong", ""], ["Lu", "Yu", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1703.00056", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova", "title": "Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments", "comments": "The short conference version of the paper was previously uploaded as\n  arXiv:1610.07524", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction instruments (RPI's) provide decision makers with an\nassessment of the likelihood that a criminal defendant will reoffend at a\nfuture point in time. While such instruments are gaining increasing popularity\nacross the country, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses several fairness criteria that have recently\nbeen applied to assess the fairness of recidivism prediction instruments. We\ndemonstrate that the criteria cannot all be simultaneously satisfied when\nrecidivism prevalence differs across groups. We then show how disparate impact\ncan arise when a recidivism prediction instrument fails to satisfy the\ncriterion of error rate balance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:12:37 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chouldechova", "Alexandra", ""]]}, {"id": "1703.00060", "submitter": "Lu Zhang", "authors": "Lu Zhang (1), Yongkai Wu (1), Xintao Wu (1) ((1) University of\n  Arkansas)", "title": "Achieving non-discrimination in prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrimination-aware classification is receiving an increasing attention in\ndata science fields. The pre-process methods for constructing a\ndiscrimination-free classifier first remove discrimination from the training\ndata, and then learn the classifier from the cleaned data. However, they lack a\ntheoretical guarantee for the potential discrimination when the classifier is\ndeployed for prediction. In this paper, we fill this gap by mathematically\nbounding the probability of the discrimination in prediction being within a\ngiven interval in terms of the training data and classifier. We adopt the\ncausal model for modeling the data generation mechanism, and formally defining\ndiscrimination in population, in a dataset, and in prediction. We obtain two\nimportant theoretical results: (1) the discrimination in prediction can still\nexist even if the discrimination in the training data is completely removed;\nand (2) not all pre-process methods can ensure non-discrimination in prediction\neven though they can achieve non-discrimination in the modified training data.\nBased on the results, we develop a two-phase framework for constructing a\ndiscrimination-free classifier with a theoretical guarantee. The experiments\ndemonstrate the theoretical results and show the effectiveness of our two-phase\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:20:19 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 19:59:00 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Lu", ""], ["Wu", "Yongkai", ""], ["Wu", "Xintao", ""]]}, {"id": "1703.00084", "submitter": "Yongcan Cao", "authors": "Kasthurirengan Suresh, Samuel Silva, Johnathan Votion, and Yongcan Cao", "title": "Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A\n  Machine Learning Approach", "comments": "submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-target pairing is an important step towards multi-target localization\nfor the intelligent operation of unmanned systems. Target localization plays a\ncrucial role in numerous applications, such as search, and rescue missions,\ntraffic management and surveillance. The objective of this paper is to present\nan innovative target location learning approach, where numerous machine\nlearning approaches, including K-means clustering and supported vector machines\n(SVM), are used to learn the data pattern across a list of spatially\ndistributed sensors. To enable the accurate data association from different\nsensors for accurate target localization, appropriate data pre-processing is\nessential, which is then followed by the application of different machine\nlearning algorithms to appropriately group data from different sensors for the\naccurate localization of multiple targets. Through simulation examples, the\nperformance of these machine learning algorithms is quantified and compared.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 23:16:19 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Suresh", "Kasthurirengan", ""], ["Silva", "Samuel", ""], ["Votion", "Johnathan", ""], ["Cao", "Yongcan", ""]]}, {"id": "1703.00091", "submitter": "Jean Daunizeau", "authors": "Jean Daunizeau", "title": "Semi-analytical approximations to statistical moments of sigmoid and\n  softmax mappings of normal variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is concerned with accurate and computationally efficient\napproximations of moments of Gaussian random variables passed through sigmoid\nor softmax mappings. These approximations are semi-analytical (i.e. they\ninvolve the numerical adjustment of parametric forms) and highly accurate (they\nyield 5% error at most). We also highlight a few niche applications of these\napproximations, which arise in the context of, e.g., drift-diffusion models of\ndecision making or non-parametric data clustering approaches. We provide these\nas examples of efficient alternatives to more tedious derivations that would be\nneeded if one was to approach the underlying mathematical issues in a more\nformal way. We hope that this technical note will be helpful to modellers\nfacing similar mathematical issues, although maybe stemming from different\nacademic prospects.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 00:01:04 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 08:55:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Daunizeau", "Jean", ""]]}, {"id": "1703.00102", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Jie Liu, Katya Scheinberg, Martin Tak\\'a\\v{c}", "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic\n  Recursive Gradient", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:2613-2621, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH),\nas well as its practical variant SARAH+, as a novel approach to the finite-sum\nminimization problems. Different from the vanilla SGD and other modern\nstochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple\nrecursive framework for updating stochastic gradient estimates; when comparing\nto SAG/SAGA, SARAH does not require a storage of past gradients. The linear\nconvergence rate of SARAH is proven under strong convexity assumption. We also\nprove a linear convergence rate (in the strongly convex case) for an inner loop\nof SARAH, the property that SVRG does not possess. Numerical experiments\ndemonstrate the efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 02:08:32 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 07:30:20 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Liu", "Jie", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1703.00119", "submitter": "Bo Liu", "authors": "Bo Liu, Xiao-Tong Yuan, Lezi Wang, Qingshan Liu, Dimitris N. Metaxas", "title": "Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to\n  Non-smooth Concave Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative Hard Thresholding (IHT) is a class of projected gradient descent\nmethods for optimizing sparsity-constrained minimization models, with the best\nknown efficiency and scalability in practice. As far as we know, the existing\nIHT-style methods are designed for sparse minimization in primal form. It\nremains open to explore duality theory and algorithms in such a non-convex and\nNP-hard problem setting. In this paper, we bridge this gap by establishing a\nduality theory for sparsity-constrained minimization with $\\ell_2$-regularized\nloss function and proposing an IHT-style algorithm for dual maximization. Our\nsparse duality theory provides a set of sufficient and necessary conditions\nunder which the original NP-hard/non-convex problem can be equivalently solved\nin a dual formulation. The proposed dual IHT algorithm is a super-gradient\nmethod for maximizing the non-smooth dual objective. An interesting finding is\nthat the sparse recovery performance of dual IHT is invariant to the Restricted\nIsometry Property (RIP), which is required by virtually all the existing primal\nIHT algorithms without sparsity relaxation. Moreover, a stochastic variant of\ndual IHT is proposed for large-scale stochastic optimization. Numerical results\ndemonstrate the superiority of dual IHT algorithms to the state-of-the-art\nprimal IHT-style algorithms in model estimation accuracy and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 03:30:47 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 02:14:17 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Liu", "Bo", ""], ["Yuan", "Xiao-Tong", ""], ["Wang", "Lezi", ""], ["Liu", "Qingshan", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1703.00144", "submitter": "Liang Zhao", "authors": "Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan and\n  Bo Yuan", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low\n  Displacement Rank", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently low displacement rank (LDR) matrices, or so-called structured\nmatrices, have been proposed to compress large-scale neural networks. Empirical\nresults have shown that neural networks with weight matrices of LDR matrices,\nreferred as LDR neural networks, can achieve significant reduction in space and\ncomputational complexity while retaining high accuracy. We formally study LDR\nmatrices in deep learning. First, we prove the universal approximation property\nof LDR neural networks with a mild condition on the displacement operators. We\nthen show that the error bounds of LDR neural networks are as efficient as\ngeneral neural networks with both single-layer and multiple-layer structure.\nFinally, we propose back-propagation based training algorithm for general LDR\nneural networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 05:38:16 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 16:15:40 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 15:57:19 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 01:53:39 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Zhao", "Liang", ""], ["Liao", "Siyu", ""], ["Wang", "Yanzhi", ""], ["Li", "Zhe", ""], ["Tang", "Jian", ""], ["Pan", "Victor", ""], ["Yuan", "Bo", ""]]}, {"id": "1703.00168", "submitter": "Chihiro Watanabe", "authors": "Chihiro Watanabe, Kaoru Hiramatsu, Kunio Kashino", "title": "Modular Representation of Layered Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layered neural networks have greatly improved the performance of various\napplications including image processing, speech recognition, natural language\nprocessing, and bioinformatics. However, it is still difficult to discover or\ninterpret knowledge from the inference provided by a layered neural network,\nsince its internal representation has many nonlinear and complex parameters\nembedded in hierarchical layers. Therefore, it becomes important to establish a\nnew methodology by which layered neural networks can be understood.\n  In this paper, we propose a new method for extracting a global and simplified\nstructure from a layered neural network. Based on network analysis, the\nproposed method detects communities or clusters of units with similar\nconnection patterns. We show its effectiveness by applying it to three use\ncases. (1) Network decomposition: it can decompose a trained neural network\ninto multiple small independent networks thus dividing the problem and reducing\nthe computation time. (2) Training assessment: the appropriateness of a trained\nresult with a given hyperparameter or randomly chosen initial parameters can be\nevaluated by using a modularity index. And (3) data analysis: in practical data\nit reveals the community structure in the input, hidden, and output layers,\nwhich serves as a clue for discovering knowledge from a trained neural network.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 07:58:29 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 06:55:48 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Watanabe", "Chihiro", ""], ["Hiramatsu", "Kaoru", ""], ["Kashino", "Kunio", ""]]}, {"id": "1703.00209", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "Online Natural Gradient as a Kalman Filter", "comments": "3rd version: expanded intro", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast Amari's natural gradient in statistical learning as a specific case\nof Kalman filtering. Namely, applying an extended Kalman filter to estimate a\nfixed unknown parameter of a probabilistic model from a series of observations,\nis rigorously equivalent to estimating this parameter via an online stochastic\nnatural gradient descent on the log-likelihood of the observations.\n  In the i.i.d. case, this relation is a consequence of the \"information\nfilter\" phrasing of the extended Kalman filter. In the recurrent (state space,\nnon-i.i.d.) case, we prove that the joint Kalman filter over states and\nparameters is a natural gradient on top of real-time recurrent learning (RTRL),\na classical algorithm to train recurrent models.\n  This exact algebraic correspondence provides relevant interpretations for\nnatural gradient hyperparameters such as learning rates or initialization and\nregularization of the Fisher information matrix.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 10:13:52 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 16:45:48 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 18:45:10 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1703.00284", "submitter": "Valentina Zantedeschi", "authors": "Valentina Zantedeschi, R\\'emi Emonet, Marc Sebban", "title": "L$^3$-SVMs: Landmarks-based Linear Local Support Vectors Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For their ability to capture non-linearities in the data and to scale to\nlarge training sets, local Support Vector Machines (SVMs) have received a\nspecial attention during the past decade. In this paper, we introduce a new\nlocal SVM method, called L$^3$-SVMs, which clusters the input space, carries\nout dimensionality reduction by projecting the data on landmarks, and jointly\nlearns a linear combination of local models. Simple and effective, our\nalgorithm is also theoretically well-founded. Using the framework of Uniform\nStability, we show that our SVM formulation comes with generalization\nguarantees on the true risk. The experiments based on the simplest\nconfiguration of our model (i.e. landmarks randomly selected, linear\nprojection, linear kernel) show that L$^3$-SVMs is very competitive w.r.t. the\nstate of the art and opens the door to new exciting lines of research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 13:22:43 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 11:58:57 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Emonet", "R\u00e9mi", ""], ["Sebban", "Marc", ""]]}, {"id": "1703.00286", "submitter": "Didier Fraix-Burnet", "authors": "Didier Fraix-Burnet (IPAG)", "title": "Phylogenetic Tools in Astrophysics", "comments": null, "journal-ref": "Wiley StatsRef: Statistics Reference Online, John Wiley \\\\& Sons,\n  Ltd, pp.1 - 6, 2017, 9781118445112", "doi": "10.1002/9781118445112.stat07935", "report-no": null, "categories": "astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate clustering in astrophysics is a recent development justified by\nthe bigger and bigger surveys of the sky. The phylogenetic approach is probably\nthe most unexpected technique that has appeared for the unsupervised\nclassification of galaxies, stellar populations or globular clusters. On one\nside, this is a somewhat natural way of classifying astrophysical entities\nwhich are all evolving objects. On the other side, several conceptual and\npractical difficulties arize, such as the hierarchical representation of the\nastrophysical diversity, the continuous nature of the parameters, and the\nadequation of the result to the usual practice for the physical interpretation.\nMost of these have now been solved through the studies of limited samples of\nstellar clusters and galaxies. Up to now, only the Maximum Parsimony\n(cladistics) has been used since it is the simplest and most general\nphylogenetic technique. Probabilistic and network approaches are obvious\nextensions that should be explored in the future.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 13:24:32 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Fraix-Burnet", "Didier", "", "IPAG"]]}, {"id": "1703.00329", "submitter": "Clement Bouttier", "authors": "Cl\\'ement Bouttier (ENAC, IMT), Ioana Gavra (IMT)", "title": "Convergence rate of a simulated annealing algorithm with noisy\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a modified version of the simulated annealing\nalgorithm for solving a stochastic global optimization problem. More precisely,\nwe address the problem of finding a global minimizer of a function with noisy\nevaluations. We provide a rate of convergence and its optimized parametrization\nto ensure a minimal number of evaluations for a given accuracy and a confidence\nlevel close to 1. This work is completed with a set of numerical\nexperimentations and assesses the practical performance both on benchmark test\ncases and on real world examples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 15:03:04 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Bouttier", "Cl\u00e9ment", "", "ENAC, IMT"], ["Gavra", "Ioana", "", "IMT"]]}, {"id": "1703.00371", "submitter": "Jamie Hayes", "authors": "Jamie Hayes and George Danezis", "title": "Generating Steganographic Images via Adversarial Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training was recently shown to be competitive against supervised\nlearning methods on computer vision tasks, however, studies have mainly been\nconfined to generative tasks such as image synthesis. In this paper, we apply\nadversarial training techniques to the discriminative task of learning a\nsteganographic algorithm. Steganography is a collection of techniques for\nconcealing information by embedding it within a non-secret medium, such as\ncover texts or images. We show that adversarial training can produce robust\nsteganographic techniques: our unsupervised training scheme produces a\nsteganographic algorithm that competes with state-of-the-art steganographic\ntechniques, and produces a robust steganalyzer, which performs the\ndiscriminative task of deciding if an image contains secret information. We\ndefine a game between three parties, Alice, Bob and Eve, in order to\nsimultaneously train both a steganographic algorithm and a steganalyzer. Alice\nand Bob attempt to communicate a secret message contained within an image,\nwhile Eve eavesdrops on their conversation and attempts to determine if secret\ninformation is embedded within the image. We represent Alice, Bob and Eve by\nneural networks, and validate our scheme on two independent image datasets,\nshowing our novel method of studying steganographic problems is surprisingly\ncompetitive against established steganographic techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 16:34:59 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 10:58:27 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 13:15:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hayes", "Jamie", ""], ["Danezis", "George", ""]]}, {"id": "1703.00381", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Barnabas Poczos, Jeff Schneider", "title": "The Statistical Recurrent Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated gated recurrent neural network architectures like LSTMs and\nGRUs have been shown to be highly effective in a myriad of applications. We\ndevelop an un-gated unit, the statistical recurrent unit (SRU), that is able to\nlearn long term dependencies in data by only keeping moving averages of\nstatistics. The SRU's architecture is simple, un-gated, and contains a\ncomparable number of parameters to LSTMs; yet, SRUs perform favorably to more\nsophisticated LSTM and GRU alternatives, often outperforming one or both in\nvarious tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an\nunbiased manner by optimizing respective architectures' hyperparameters in a\nBayesian optimization scheme for both synthetic and real-world tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 16:50:54 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Oliva", "Junier B.", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""]]}, {"id": "1703.00395", "submitter": "Lucas Theis", "authors": "Lucas Theis, Wenzhe Shi, Andrew Cunningham, Ferenc Husz\\'ar", "title": "Lossy Image Compression with Compressive Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of optimizing autoencoders for lossy\nimage compression. New media formats, changing hardware technology, as well as\ndiverse requirements and content types create a need for compression algorithms\nwhich are more flexible than existing codecs. Autoencoders have the potential\nto address this need, but are difficult to optimize directly due to the\ninherent non-differentiabilty of the compression loss. We here show that\nminimal changes to the loss are sufficient to train deep autoencoders\ncompetitive with JPEG 2000 and outperforming recently proposed approaches based\non RNNs. Our network is furthermore computationally efficient thanks to a\nsub-pixel architecture, which makes it suitable for high-resolution images.\nThis is in contrast to previous work on autoencoders for compression using\ncoarser approximations, shallower architectures, computationally expensive\nmethods, or focusing on small images.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:13:47 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Theis", "Lucas", ""], ["Shi", "Wenzhe", ""], ["Cunningham", "Andrew", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1703.00403", "submitter": "Brian McWilliams", "authors": "Christina Heinze-Deml, Brian McWilliams, Nicolai Meinshausen", "title": "Preserving Differential Privacy Between Features in Distributed\n  Estimation", "comments": null, "journal-ref": "Stat 7 (1), 2018", "doi": "10.1002/sta4.189", "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is crucial in many applications of machine learning. Legal, ethical\nand societal issues restrict the sharing of sensitive data making it difficult\nto learn from datasets that are partitioned between many parties. One important\ninstance of such a distributed setting arises when information about each\nrecord in the dataset is held by different data owners (the design matrix is\n\"vertically-partitioned\").\n  In this setting few approaches exist for private data sharing for the\npurposes of statistical estimation and the classical setup of differential\nprivacy with a \"trusted curator\" preparing the data does not apply. We work\nwith the notion of $(\\epsilon,\\delta)$-distributed differential privacy which\nextends single-party differential privacy to the distributed,\nvertically-partitioned case. We propose PriDE, a scalable framework for\ndistributed estimation where each party communicates perturbed random\nprojections of their locally held features ensuring\n$(\\epsilon,\\delta)$-distributed differential privacy is preserved. For\n$\\ell_2$-penalized supervised learning problems PriDE has bounded estimation\nerror compared with the optimal estimates obtained without privacy constraints\nin the non-distributed setting. We confirm this empirically on real world and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:30:14 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 08:59:48 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Heinze-Deml", "Christina", ""], ["McWilliams", "Brian", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1703.00410", "submitter": "Reuben Feinman", "authors": "Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, Andrew B. Gardner", "title": "Detecting Adversarial Samples from Artifacts", "comments": "Submitted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:43:10 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 12:51:12 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 23:31:59 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Feinman", "Reuben", ""], ["Curtin", "Ryan R.", ""], ["Shintre", "Saurabh", ""], ["Gardner", "Andrew B.", ""]]}, {"id": "1703.00439", "submitter": "Tomoya Murata", "authors": "Tomoya Murata and Taiji Suzuki", "title": "Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for\n  Regularized Empirical Risk Minimization", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new accelerated stochastic gradient method for\nefficiently solving the convex regularized empirical risk minimization problem\nin mini-batch settings. The use of mini-batches is becoming a golden standard\nin the machine learning community, because mini-batch settings stabilize the\ngradient estimate and can easily make good use of parallel computing. The core\nof our proposed method is the incorporation of our new \"double acceleration\"\ntechnique and variance reduction technique. We theoretically analyze our\nproposed method and show that our method much improves the mini-batch\nefficiencies of previous accelerated stochastic methods, and essentially only\nneeds size $\\sqrt{n}$ mini-batches for achieving the optimal iteration\ncomplexities for both non-strongly and strongly convex objectives, where $n$ is\nthe training set size. Further, we show that even in non-mini-batch settings,\nour method achieves the best known convergence rate for both non-strongly and\nstrongly convex objectives.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:49:35 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 02:11:18 GMT"}, {"version": "v3", "created": "Sun, 16 Apr 2017 11:11:48 GMT"}, {"version": "v4", "created": "Tue, 19 Sep 2017 12:48:00 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Murata", "Tomoya", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1703.00440", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "comments": "14 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:51:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 17:46:04 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1703.00441", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Learning to Optimize Neural Nets", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to Optimize is a recently proposed framework for learning\noptimization algorithms using reinforcement learning. In this paper, we explore\nlearning an optimization algorithm for training shallow neural nets. Such\nhigh-dimensional stochastic optimization problems present interesting\nchallenges for existing reinforcement learning algorithms. We develop an\nextension that is suited to learning optimization algorithms in this setting\nand demonstrate that the learned optimization algorithm consistently\noutperforms other known optimization algorithms even on unseen tasks and is\nrobust to changes in stochasticity of gradients and the neural net\narchitecture. More specifically, we show that an optimization algorithm trained\nwith the proposed method on the problem of training a neural net on MNIST\ngeneralizes to the problems of training neural nets on the Toronto Faces\nDataset, CIFAR-10 and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:52:23 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 18:59:01 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1703.00443", "submitter": "Brandon Amos", "authors": "Brandon Amos, J. Zico Kolter", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents OptNet, a network architecture that integrates\noptimization problems (here, specifically in the form of quadratic programs) as\nindividual layers in larger end-to-end trainable deep networks. These layers\nencode constraints and complex dependencies between the hidden states that\ntraditional convolutional and fully-connected layers often cannot capture. In\nthis paper, we explore the foundations for such an architecture: we show how\ntechniques from sensitivity analysis, bilevel optimization, and implicit\ndifferentiation can be used to exactly differentiate through these layers and\nwith respect to layer parameters; we develop a highly efficient solver for\nthese layers that exploits fast GPU-based batch solves within a primal-dual\ninterior point method, and which provides backpropagation gradients with\nvirtually no additional cost on top of the solve; and we highlight the\napplication of these approaches in several problems. In one notable example, we\nshow that the method is capable of learning to play mini-Sudoku (4x4) given\njust input and output games, with no a priori information about the rules of\nthe game; this highlights the ability of our architecture to learn hard\nconstraints better than other neural architectures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:58:48 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 17:59:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 19:44:25 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 18:03:26 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Amos", "Brandon", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1703.00535", "submitter": "Sven Schmit", "authors": "Sven Schmit and Carlos Riquelme", "title": "Human Interaction with Recommendation Systems", "comments": "Accepted to AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommendation algorithms rely on user data to generate recommendations.\nHowever, these recommendations also affect the data obtained from future users.\nThis work aims to understand the effects of this dynamic interaction. We\npropose a simple model where users with heterogeneous preferences arrive over\ntime. Based on this model, we prove that naive estimators, i.e. those which\nignore this feedback loop, are not consistent. We show that consistent\nestimators are efficient in the presence of myopic agents. Our results are\nvalidated using extensive simulations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 22:28:42 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 01:25:51 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 17:40:02 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Schmit", "Sven", ""], ["Riquelme", "Carlos", ""]]}, {"id": "1703.00564", "submitter": "Zhenqin Wu", "authors": "Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb\n  Geniesse, Aneesh S. Pappu, Karl Leswing and Vijay Pande", "title": "MoleculeNet: A Benchmark for Molecular Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular machine learning has been maturing rapidly over the last few years.\nImproved methods and the presence of larger datasets have enabled machine\nlearning algorithms to make increasingly accurate predictions about molecular\nproperties. However, algorithmic progress has been limited due to the lack of a\nstandard benchmark to compare the efficacy of proposed methods; most new\nalgorithms are benchmarked on different datasets making it challenging to gauge\nthe quality of proposed methods. This work introduces MoleculeNet, a large\nscale benchmark for molecular machine learning. MoleculeNet curates multiple\npublic datasets, establishes metrics for evaluation, and offers high quality\nopen-source implementations of multiple previously proposed molecular\nfeaturization and learning algorithms (released as part of the DeepChem open\nsource library). MoleculeNet benchmarks demonstrate that learnable\nrepresentations are powerful tools for molecular machine learning and broadly\noffer the best performance. However, this result comes with caveats. Learnable\nrepresentations still struggle to deal with complex tasks under data scarcity\nand highly imbalanced classification. For quantum mechanical and biophysical\ndatasets, the use of physics-aware featurizations can be more important than\nchoice of particular learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 00:39:53 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 08:05:38 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 00:52:38 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Wu", "Zhenqin", ""], ["Ramsundar", "Bharath", ""], ["Feinberg", "Evan N.", ""], ["Gomes", "Joseph", ""], ["Geniesse", "Caleb", ""], ["Pappu", "Aneesh S.", ""], ["Leswing", "Karl", ""], ["Pande", "Vijay", ""]]}, {"id": "1703.00573", "submitter": "Rong Ge", "authors": "Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "comments": "This is an updated version of an ICML'17 paper with the same title.\n  The main difference is that in the ICML'17 version the pure equilibrium\n  result was only proved for Wasserstein GAN. In the current version the result\n  applies to most reasonable training objectives. In particular, Theorem 4.3\n  now applies to both original GAN and Wasserstein GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that training of generative adversarial network (GAN) may not have\ngood generalization properties; e.g., training may appear successful but the\ntrained distribution may be far from target distribution in standard metrics.\nHowever, generalization does occur for a weaker metric called neural net\ndistance. It is also shown that an approximate pure equilibrium exists in the\ndiscriminator/generator game for a special class of generators with natural\ntraining objectives when generator capacity and training set sizes are\nmoderate.\n  This existence of equilibrium inspires MIX+GAN protocol, which can be\ncombined with any existing GAN training, and empirically shown to improve some\nof them.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 01:14:03 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 16:19:00 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 00:41:13 GMT"}, {"version": "v4", "created": "Sat, 17 Jun 2017 22:04:07 GMT"}, {"version": "v5", "created": "Tue, 1 Aug 2017 19:51:56 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Zhang", "Yi", ""]]}, {"id": "1703.00579", "submitter": "Carlos Riquelme Ruiz", "authors": "Carlos Riquelme, Mohammad Ghavamzadeh, Alessandro Lazaric", "title": "Active Learning for Accurate Estimation of Linear Models", "comments": "37 pages, 8 figures, International Conference on Machine Learning,\n  ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the sequential decision making problem where the goal is to\nestimate uniformly well a number of linear models, given a shared budget of\nrandom contexts independently sampled from a known distribution. The decision\nmaker must query one of the linear models for each incoming context, and\nreceives an observation corrupted by noise levels that are unknown, and depend\non the model instance. We present Trace-UCB, an adaptive allocation algorithm\nthat learns the noise levels while balancing contexts accordingly across the\ndifferent linear functions, and derive guarantees for simple regret in both\nexpectation and high-probability. Finally, we extend the algorithm and its\nguarantees to high dimensional settings, where the number of linear models\ntimes the dimension of the contextual space is higher than the total budget of\nsamples. Simulations with real data suggest that Trace-UCB is remarkably\nrobust, outperforming a number of baselines even when its assumptions are\nviolated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 01:29:57 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 19:41:29 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Riquelme", "Carlos", ""], ["Ghavamzadeh", "Mohammad", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "1703.00593", "submitter": "Gang Niu", "authors": "Ryuichi Kiryo, Gang Niu, Marthinus C. du Plessis, and Masashi Sugiyama", "title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator", "comments": "NIPS 2017 camera-ready version (this paper was selected for oral\n  presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From only positive (P) and unlabeled (U) data, a binary classifier could be\ntrained with PU learning, in which the state of the art is unbiased PU\nlearning. However, if its model is very flexible, empirical risks on training\ndata will go negative, and we will suffer from serious overfitting. In this\npaper, we propose a non-negative risk estimator for PU learning: when getting\nminimized, it is more robust against overfitting, and thus we are able to use\nvery flexible models (such as deep neural networks) given limited P data.\nMoreover, we analyze the bias, consistency, and mean-squared-error reduction of\nthe proposed risk estimator, and bound the estimation error of the resulting\nempirical risk minimizer. Experiments demonstrate that our risk estimator fixes\nthe overfitting problem of its unbiased counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 02:49:33 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 11:18:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kiryo", "Ryuichi", ""], ["Niu", "Gang", ""], ["Plessis", "Marthinus C. du", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1703.00598", "submitter": "Ming Lin", "authors": "Ming Lin, Shuang Qiu, Bin Hong, Jieping Ye", "title": "The Second Order Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a fundamental class of regression models called the second order\nlinear model (SLM). The SLM extends the linear model to high order functional\nspace and has attracted considerable research interest recently. Yet how to\nefficiently learn the SLM under full generality using nonconvex solver still\nremains an open question due to several fundamental limitations of the\nconventional gradient descent learning framework. In this study, we try to\nattack this problem from a gradient-free approach which we call the\nmoment-estimation-sequence (MES) method. We show that the conventional gradient\ndescent heuristic is biased by the skewness of the distribution therefore is no\nlonger the best practice of learning the SLM. Based on the MES framework, we\ndesign a nonconvex alternating iteration process to train a $d$-dimension\nrank-$k$ SLM within $O(kd)$ memory and one-pass of the dataset. The proposed\nmethod converges globally and linearly, achieves $\\epsilon$ recovery error\nafter retrieving $O[k^{2}d\\cdot\\mathrm{polylog}(kd/\\epsilon)]$ samples.\nFurthermore, our theoretical analysis reveals that not all SLMs can be learned\non every sub-gaussian distribution. When the instances are sampled from a\nso-called $\\tau$-MIP distribution, the SLM can be learned by $O(p/\\tau^{2})$\nsamples where $p$ and $\\tau$ are positive constants depending on the skewness\nand kurtosis of the distribution. For non-MIP distribution, an addition\ndiagonal-free oracle is necessary and sufficient to guarantee the learnability\nof the SLM. Numerical simulations verify the sharpness of our bounds on the\nsampling complexity and the linear convergence rate of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 03:23:01 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 22:21:09 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 14:20:17 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Lin", "Ming", ""], ["Qiu", "Shuang", ""], ["Hong", "Bin", ""], ["Ye", "Jieping", ""]]}, {"id": "1703.00607", "submitter": "Zijun Yao", "authors": "Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, Hui Xiong", "title": "Dynamic Word Embeddings for Evolving Semantic Discovery", "comments": "9 pages, published in the International Conference on Web Search and\n  Data Mining (WSDM 2018)", "journal-ref": null, "doi": "10.1145/3159652.3159703", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word evolution refers to the changing meanings and associations of words\nthroughout time, as a byproduct of human language evolution. By studying word\nevolution, we can infer social trends and language constructs over different\nperiods of human history. However, traditional techniques such as word\nrepresentation learning do not adequately capture the evolving language\nstructure and vocabulary. In this paper, we develop a dynamic statistical model\nto learn time-aware word vector representation. We propose a model that\nsimultaneously learns time-aware embeddings and solves the resulting \"alignment\nproblem\". This model is trained on a crawled NYTimes dataset. Additionally, we\ndevelop multiple intuitive evaluation strategies of temporal word embeddings.\nOur qualitative and quantitative tests indicate that our method not only\nreliably captures this evolution over time, but also consistently outperforms\nstate-of-the-art temporal embedding approaches on both semantic accuracy and\nalignment quality.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 03:59:18 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 17:10:42 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Yao", "Zijun", ""], ["Sun", "Yifan", ""], ["Ding", "Weicong", ""], ["Rao", "Nikhil", ""], ["Xiong", "Hui", ""]]}, {"id": "1703.00617", "submitter": "Benjamin Rubinstein", "authors": "Neil G. Marchant and Benjamin I. P. Rubinstein", "title": "In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential\n  Importance Sampling", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) presents unique challenges for evaluation methodology.\nWhile crowdsourcing platforms acquire ground truth, sound approaches to\nsampling must drive labelling efforts. In ER, extreme class imbalance between\nmatching and non-matching records can lead to enormous labelling requirements\nwhen seeking statistically consistent estimates for rigorous evaluation. This\npaper addresses this important challenge with the OASIS algorithm: a sampler\nand F-measure estimator for ER evaluation. OASIS draws samples from a (biased)\ninstrumental distribution, chosen to ensure estimators with optimal asymptotic\nvariance. As new labels are collected OASIS updates this instrumental\ndistribution via a Bayesian latent variable model of the annotator oracle, to\nquickly focus on unlabelled items providing more information. We prove that\nresulting estimates of F-measure, precision, recall converge to the true\npopulation values. Thorough comparisons of sampling methods on a variety of ER\ndatasets demonstrate significant labelling reductions of up to 83% without loss\nto estimate accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 04:49:22 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 07:34:10 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 01:28:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Marchant", "Neil G.", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1703.00645", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Robert Gillies, Kunlin Cao, Qi Song, Ulas Bagci", "title": "TumorNet: Lung Nodule Characterization Using Multi-View Convolutional\n  Neural Network with Gaussian Process", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of lung nodules as benign or malignant is one of the most\nimportant tasks in lung cancer diagnosis, staging and treatment planning. While\nthe variation in the appearance of the nodules remains large, there is a need\nfor a fast and robust computer aided system. In this work, we propose an\nend-to-end trainable multi-view deep Convolutional Neural Network (CNN) for\nnodule characterization. First, we use median intensity projection to obtain a\n2D patch corresponding to each dimension. The three images are then\nconcatenated to form a tensor, where the images serve as different channels of\nthe input image. In order to increase the number of training samples, we\nperform data augmentation by scaling, rotating and adding noise to the input\nimage. The trained network is used to extract features from the input image\nfollowed by a Gaussian Process (GP) regression to obtain the malignancy score.\nWe also empirically establish the significance of different high level nodule\nattributes such as calcification, sphericity and others for malignancy\ndetermination. These attributes are found to be complementary to the deep\nmulti-view CNN features and a significant improvement over other methods is\nobtained.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:26:37 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Gillies", "Robert", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Bagci", "Ulas", ""]]}, {"id": "1703.00663", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Introduction to Nonnegative Matrix Factorization", "comments": "18 pages, 4 figures", "journal-ref": "SIAG/OPT Views and News 25 (1), pp. 7-16 (2017)", "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and provide a short overview of nonnegative\nmatrix factorization (NMF). Several aspects of NMF are discussed, namely, the\napplication in hyperspectral imaging, geometry and uniqueness of NMF solutions,\ncomplexity, algorithms, and its link with extended formulations of polyhedra.\nIn order to put NMF into perspective, the more general problem class of\nconstrained low-rank matrix approximation problems is first briefly introduced.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 08:23:04 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1703.00674", "submitter": "Virag Shah", "authors": "Virag Shah, Lennart Gulikers, Laurent Massoulie, Milan Vojnovic", "title": "Adaptive Matching for Expert Systems with Uncertain Task Types", "comments": "A part of it presented at Allerton Conference 2017, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching in a two-sided market often incurs an externality: a matched\nresource may become unavailable to the other side of the market, at least for a\nwhile. This is especially an issue in online platforms involving human experts\nas the expert resources are often scarce. The efficient utilization of experts\nin these platforms is made challenging by the fact that the information\navailable about the parties involved is usually limited.\n  To address this challenge, we develop a model of a task-expert matching\nsystem where a task is matched to an expert using not only the prior\ninformation about the task but also the feedback obtained from the past\nmatches. In our model the tasks arrive online while the experts are fixed and\nconstrained by a finite service capacity. For this model, we characterize the\nmaximum task resolution throughput a platform can achieve. We show that the\nnatural greedy approaches where each expert is assigned a task most suitable to\nher skill is suboptimal, as it does not internalize the above externality. We\ndevelop a throughput optimal backpressure algorithm which does so by accounting\nfor the `congestion' among different task types. Finally, we validate our model\nand confirm our theoretical findings with data-driven simulations via logs of\nMath.StackExchange, a StackOverflow forum dedicated to mathematics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 09:11:32 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 13:04:57 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 22:59:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Shah", "Virag", ""], ["Gulikers", "Lennart", ""], ["Massoulie", "Laurent", ""], ["Vojnovic", "Milan", ""]]}, {"id": "1703.00676", "submitter": "Nils Kriege", "authors": "Nils M. Kriege, Marion Neumann, Christopher Morris, Kristian Kersting,\n  Petra Mutzel", "title": "A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery 33 (2019) 1505-1547", "doi": "10.1007/s10618-019-00652-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear kernel methods can be approximated by fast linear ones using\nsuitable explicit feature maps allowing their application to large scale\nproblems. We investigate how convolution kernels for structured data are\ncomposed from base kernels and construct corresponding feature maps. On this\nbasis we propose exact and approximative feature maps for widely used graph\nkernels based on the kernel trick. We analyze for which kernels and graph\nproperties computation by explicit feature maps is feasible and actually more\nefficient. In particular, we derive approximative, explicit feature maps for\nstate-of-the-art kernels supporting real-valued attributes including the\nGraphHopper and graph invariant kernels. In extensive experiments we show that\nour approaches often achieve a classification accuracy close to the exact\nmethods based on the kernel trick, but require only a fraction of their running\ntime. Moreover, we propose and analyze algorithms for computing random walk,\nshortest-path and subgraph matching kernels by explicit and implicit feature\nmaps. Our theoretical results are confirmed experimentally by observing a phase\ntransition when comparing running time with respect to label diversity, walk\nlengths and subgraph size, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 09:20:31 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 10:22:48 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 20:50:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kriege", "Nils M.", ""], ["Neumann", "Marion", ""], ["Morris", "Christopher", ""], ["Kersting", "Kristian", ""], ["Mutzel", "Petra", ""]]}, {"id": "1703.00734", "submitter": "Xiangju Qin", "authors": "Xiangju Qin, Paul Blomstedt, Eemeli Lepp\\\"aaho, Pekka Parviainen,\n  Samuel Kaski", "title": "Distributed Bayesian Matrix Factorization with Limited Communication", "comments": "28 pages, 8 figures. The paper is published in Machine Learning\n  journal. An implementation of the method is is available in SMURFF software\n  on github (bmfpp branch): https://github.com/ExaScience/smurff", "journal-ref": "Machine Learning, 2019", "doi": "10.1007/s10994-019-05778-2", "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank\nrepresentations of matrices and for predicting missing values and providing\nconfidence intervals. Scaling up the posterior inference for massive-scale\nmatrices is challenging and requires distributing both data and computation\nover many workers, making communication the main computational bottleneck.\nEmbarrassingly parallel inference would remove the communication needed, by\nusing completely independent computations on different data subsets, but it\nsuffers from the inherent unidentifiability of BMF solutions. We introduce a\nhierarchical decomposition of the joint posterior distribution, which couples\nthe subset inferences, allowing for embarrassingly parallel computations in a\nsequence of at most three stages. Using an efficient approximate\nimplementation, we show improvements empirically on both real and simulated\ndata. Our distributed approach is able to achieve a speed-up of almost an order\nof magnitude over the full posterior, with a negligible effect on predictive\naccuracy. Our method outperforms state-of-the-art embarrassingly parallel MCMC\nmethods in accuracy, and achieves results competitive to other available\ndistributed and parallel implementations of BMF.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:48:24 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:47:09 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 18:58:59 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 17:07:21 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Qin", "Xiangju", ""], ["Blomstedt", "Paul", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Parviainen", "Pekka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1703.00787", "submitter": "Carl Jidling", "authors": "Carl Jidling, Niklas Wahlstr\\\"om, Adrian Wills, Thomas B. Sch\\\"on", "title": "Linearly constrained Gaussian processes", "comments": "A few fixes and added citation inforomation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a modification of the covariance function in Gaussian processes\nto correctly account for known linear constraints. By modelling the target\nfunction as a transformation of an underlying function, the constraints are\nexplicitly incorporated in the model such that they are guaranteed to be\nfulfilled by any sample drawn or prediction made. We also propose a\nconstructive procedure for designing the transformation operator and illustrate\nthe result on both simulated and real-data examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 14:02:03 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 08:43:53 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Jidling", "Carl", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Wills", "Adrian", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1703.00837", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Meta Networks", "comments": "Accepted at ICML 2017 - rewrote: the main section; added: MetaNet\n  algorithmic procedure; performed: Mini-ImageNet evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been successfully applied in applications with a large\namount of labeled data. However, the task of rapid generalization on new\nconcepts with small training data while preserving performances on previously\nlearned ones still presents a significant challenge to neural network models.\nIn this work, we introduce a novel meta learning method, Meta Networks\n(MetaNet), that learns a meta-level knowledge across tasks and shifts its\ninductive biases via fast parameterization for rapid generalization. When\nevaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve\na near human-level performance and outperform the baseline approaches by up to\n6% accuracy. We demonstrate several appealing properties of MetaNet relating to\ngeneralization and continual learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:52:55 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 16:12:40 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1703.00839", "submitter": "Louis Aslett", "authors": "Pedro M. Esperan\\c{c}a, Louis J. M. Aslett, Chris C. Holmes", "title": "Encrypted accelerated least squares regression", "comments": "Accepted for AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information that is stored in an encrypted format is, by definition, usually\nnot amenable to statistical analysis or machine learning methods. In this paper\nwe present detailed analysis of coordinate and accelerated gradient descent\nalgorithms which are capable of fitting least squares and penalised ridge\nregression models, using data encrypted under a fully homomorphic encryption\nscheme. Gradient descent is shown to dominate in terms of encrypted\ncomputational speed, and theoretical results are proven to give parameter\nbounds which ensure correctness of decryption. The characteristics of encrypted\ncomputation are empirically shown to favour a non-standard acceleration\ntechnique. This demonstrates the possibility of approximating conventional\nstatistical regression methods using encrypted data without compromising\nprivacy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:53:52 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Esperan\u00e7a", "Pedro M.", ""], ["Aslett", "Louis J. M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1703.00854", "submitter": "Stephen Bach", "authors": "Stephen H. Bach, Bryan He, Alexander Ratner, Christopher R\\'e", "title": "Learning the Structure of Generative Models without Labeled Data", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, Sydney, Australia, PMLR 70, 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curating labeled training data has become the primary bottleneck in machine\nlearning. Recent frameworks address this bottleneck with generative models to\nsynthesize labels at scale from weak supervision sources. The generative\nmodel's dependency structure directly affects the quality of the estimated\nlabels, but selecting a structure automatically without any labeled data is a\ndistinct challenge. We propose a structure estimation method that maximizes the\n$\\ell_1$-regularized marginal pseudolikelihood of the observed data. Our\nanalysis shows that the amount of unlabeled data required to identify the true\nstructure scales sublinearly in the number of possible dependencies for a broad\nclass of models. Simulations show that our method is 100$\\times$ faster than a\nmaximum likelihood approach and selects $1/4$ as many extraneous dependencies.\nWe also show that our method provides an average of 1.5 F1 points of\nimprovement over existing, user-developed information extraction applications\non real-world data such as PubMed journal abstracts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 16:52:09 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 21:22:57 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bach", "Stephen H.", ""], ["He", "Bryan", ""], ["Ratner", "Alexander", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1703.00862", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Binarized Convolutional Landmark Localizers for Human Pose Estimation\n  and Face Alignment with Limited Resources", "comments": "ICCV 2017 Oral", "journal-ref": null, "doi": "10.1109/ICCV.2017.400", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to design architectures that retain the groundbreaking\nperformance of CNNs for landmark localization and at the same time are\nlightweight, compact and suitable for applications with limited computational\nresources. To this end, we make the following contributions: (a) we are the\nfirst to study the effect of neural network binarization on localization tasks,\nnamely human pose estimation and face alignment. We exhaustively evaluate\nvarious design choices, identify performance bottlenecks, and more importantly\npropose multiple orthogonal ways to boost performance. (b) Based on our\nanalysis, we propose a novel hierarchical, parallel and multi-scale residual\narchitecture that yields large performance improvement over the standard\nbottleneck block while having the same number of parameters, thus bridging the\ngap between the original network and its binarized counterpart. (c) We perform\na large number of ablation studies that shed light on the properties and the\nperformance of the proposed block. (d) We present results for experiments on\nthe most challenging datasets for human pose estimation and face alignment,\nreporting in many cases state-of-the-art performance. Code can be downloaded\nfrom https://www.adrianbulat.com/binary-cnn-landmarks\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:26:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 15:35:04 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1703.00864", "submitter": "Mark Rowland", "authors": "Krzysztof Choromanski, Mark Rowland, Adrian Weller", "title": "The Unreasonable Effectiveness of Structured Random Orthogonal\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a class of embeddings based on structured random matrices with\northogonal rows which can be applied in many machine learning applications\nincluding dimensionality reduction and kernel approximation. For both the\nJohnson-Lindenstrauss transform and the angular kernel, we show that we can\nselect matrices yielding guaranteed improved performance in accuracy and/or\nspeed compared to earlier methods. We introduce matrices with complex entries\nwhich give significant further accuracy improvement. We provide geometric and\nMarkov chain-based perspectives to help understand the benefits, and empirical\nresults which suggest that the approach is helpful in a wider range of\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:33:58 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 21:25:36 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 18:28:37 GMT"}, {"version": "v4", "created": "Tue, 2 Jan 2018 19:57:05 GMT"}, {"version": "v5", "created": "Mon, 3 Sep 2018 08:55:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Rowland", "Mark", ""], ["Weller", "Adrian", ""]]}, {"id": "1703.00868", "submitter": "Atilim Gunes Baydin", "authors": "Tuan Anh Le, Atilim Gunes Baydin, Robert Zinkov, Frank Wood", "title": "Using Synthetic Data to Train Neural Networks is Model-Based Reasoning", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We draw a formal connection between using synthetic training data to optimize\nneural network parameters and approximate, Bayesian, model-based reasoning. In\nparticular, training a neural network using synthetic data can be viewed as\nlearning a proposal distribution generator for approximate inference in the\nsynthetic-data generative model. We demonstrate this connection in a\nrecognition task where we develop a novel Captcha-breaking architecture and\ntrain it using synthetic data, demonstrating both state-of-the-art performance\nand a way of computing task-specific posterior uncertainty. Using a neural\nnetwork trained this way, we also demonstrate successful breaking of real-world\nCaptchas currently used by Facebook and Wikipedia. Reasoning from these\nempirical results and drawing connections with Bayesian modeling, we discuss\nthe robustness of synthetic data results and suggest important considerations\nfor ensuring good neural network generalization when training with synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:43:19 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Le", "Tuan Anh", ""], ["Baydin", "Atilim Gunes", ""], ["Zinkov", "Robert", ""], ["Wood", "Frank", ""]]}, {"id": "1703.00887", "submitter": "Chi Jin", "authors": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, Michael I.\n  Jordan", "title": "How to Escape Saddle Points Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that a perturbed form of gradient descent converges to a\nsecond-order stationary point in a number iterations which depends only\npoly-logarithmically on dimension (i.e., it is almost \"dimension-free\"). The\nconvergence rate of this procedure matches the well-known convergence rate of\ngradient descent to first-order stationary points, up to log factors. When all\nsaddle points are non-degenerate, all second-order stationary points are local\nminima, and our result thus shows that perturbed gradient descent can escape\nsaddle points almost for free. Our results can be directly applied to many\nmachine learning applications, including deep learning. As a particular\nconcrete example of such an application, we show that our results can be used\ndirectly to establish sharp global convergence rates for matrix factorization.\nOur results rely on a novel characterization of the geometry around saddle\npoints, which may be of independent interest to the non-convex optimization\ncommunity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:35:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Jin", "Chi", ""], ["Ge", "Rong", ""], ["Netrapalli", "Praneeth", ""], ["Kakade", "Sham M.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1703.00893", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Being Robust (in High Dimensions) Can Be Practical", "comments": "Appeared in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is much more challenging in high dimensions than it is in\none dimension: Most techniques either lead to intractable optimization problems\nor estimators that can tolerate only a tiny fraction of errors. Recent work in\ntheoretical computer science has shown that, in appropriate distributional\nmodels, it is possible to robustly estimate the mean and covariance with\npolynomial time algorithms that can tolerate a constant fraction of\ncorruptions, independent of the dimension. However, the sample and time\ncomplexity of these algorithms is prohibitively large for high-dimensional\napplications. In this work, we address both of these issues by establishing\nsample complexity bounds that are optimal, up to logarithmic factors, as well\nas giving various refinements that allow the algorithms to tolerate a much\nlarger fraction of corruptions. Finally, we show on both synthetic and real\ndata that our algorithms have state-of-the-art performance and suddenly make\nhigh-dimensional robust estimation a realistic possibility.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:50:33 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:55:28 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 17:59:21 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 17:51:44 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1703.00955", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P.\n  Xing", "title": "Toward Controlled Generation of Text", "comments": "Code adapted for text style transfer is released at:\n  https://github.com/asyml/texar/tree/master/examples/text_style_transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic generation and manipulation of text is challenging and has limited\nsuccess compared to recent deep generative modeling in visual domain. This\npaper aims at generating plausible natural language sentences, whose attributes\nare dynamically controlled by learning disentangled latent representations with\ndesignated semantics. We propose a new neural generative model which combines\nvariational auto-encoders and holistic attribute discriminators for effective\nimposition of semantic structures. With differentiable approximation to\ndiscrete text samples, explicit constraints on independent attribute controls,\nand efficient collaborative learning of generator and discriminators, our model\nlearns highly interpretable representations from even only word annotations,\nand produces realistic sentences with desired attributes. Quantitative\nevaluation validates the accuracy of sentence and attribute generation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 21:23:47 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 21:15:43 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 08:01:18 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 02:16:40 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Liang", "Xiaodan", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.00977", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Jaime Carbonell", "title": "Self-Paced Multitask Learning with Shared Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces self-paced task selection to multitask learning, where\ninstances from more closely related tasks are selected in a progression of\neasier-to-harder tasks, to emulate an effective human education strategy, but\napplied to multitask machine learning. We develop the mathematical foundation\nfor the approach based on iterative selection of the most appropriate task,\nlearning the task parameters, and updating the shared knowledge, optimizing a\nnew bi-convex loss function. This proposed method applies quite generally,\nincluding to multitask feature learning, multitask learning with alternating\nstructure optimization, etc. Results show that in each of the above\nformulations self-paced (easier-to-harder) task selection outperforms the\nbaseline version of these methods in all the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 22:49:14 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:40:13 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1703.00986", "submitter": "Wei Ping", "authors": "Wei Ping, Alexander Ihler", "title": "Belief Propagation in Conditional RBMs for Structured Prediction", "comments": "Artificial Intelligence and Statistics (AISTATS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular\nmodels for a wide range of applications. In previous work, learning on such\nmodels has been dominated by contrastive divergence~(CD) and its variants.\nBelief propagation~(BP) algorithms are believed to be slow for structured\nprediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as\nCD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we\npresent a matrix-based implementation of belief propagation algorithms on\nCRBMs, which is easily scalable to tens of thousands of visible and hidden\nunits. We demonstrate that, in both maximum likelihood and max-margin learning,\ntraining conditional RBMs with BP as the inference routine can provide\nsignificantly better results than current state-of-the-art CD methods on\nstructured prediction problems. We also include practical guidelines on\ntraining CRBMs with BP, and some insights on the interaction of learning and\ninference algorithms for CRBMs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:28:53 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Ping", "Wei", ""], ["Ihler", "Alexander", ""]]}, {"id": "1703.00989", "submitter": "Reza Bonyadi Reza Bonyadi", "authors": "Mohammad Reza Bonyadi, Quang M. Tieng, David C. Reutens", "title": "Optimization of distributions differences for classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new classification algorithm called Optimization\nof Distributions Differences (ODD). The algorithm aims to find a transformation\nfrom the feature space to a new space where the instances in the same class are\nas close as possible to one another while the gravity centers of these classes\nare as far as possible from one another. This aim is formulated as a\nmultiobjective optimization problem that is solved by a hybrid of an\nevolutionary strategy and the Quasi-Newton method. The choice of the\ntransformation function is flexible and could be any continuous space function.\nWe experiment with a linear and a non-linear transformation in this paper. We\nshow that the algorithm can outperform 6 other state-of-the-art classification\nmethods, namely naive Bayes, support vector machines, linear discriminant\nanalysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in\n12 standard classification datasets. Our results show that the method is less\nsensitive to the imbalanced number of instances comparing to these methods. We\nalso show that ODD maintains its performance better than other classification\nmethods in these datasets, hence, offers a better generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:42:33 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Bonyadi", "Mohammad Reza", ""], ["Tieng", "Quang M.", ""], ["Reutens", "David C.", ""]]}, {"id": "1703.00994", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Jaime Carbonell, Yiming Yang", "title": "Co-Clustering for Multitask Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new multitask learning framework that learns a shared\nrepresentation among the tasks, incorporating both task and feature clusters.\nThe jointly-induced clusters yield a shared latent subspace where task\nrelationships are learned more effectively and more generally than in\nstate-of-the-art multitask learning methods. The proposed general framework\nenables the derivation of more specific or restricted state-of-the-art\nmultitask methods. The paper also proposes a highly-scalable multitask learning\nalgorithm, based on the new framework, using conjugate gradient descent and\ngeneralized \\textit{Sylvester equations}. Experimental results on synthetic and\nbenchmark datasets show that the proposed method systematically outperforms\nseveral state-of-the-art multitask learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 00:03:14 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Carbonell", "Jaime", ""], ["Yang", "Yiming", ""]]}, {"id": "1703.01014", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III,\n  John Langford", "title": "Active Learning for Cost-Sensitive Classification", "comments": null, "journal-ref": "Journal of Machine Learning Research, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an active learning algorithm for cost-sensitive multiclass\nclassification: problems where different errors have different costs. Our\nalgorithm, COAL, makes predictions by regressing to each label's cost and\npredicting the smallest. On a new example, it uses a set of regressors that\nperform well on past data to estimate possible costs for each label. It queries\nonly the labels that could be the best, ignoring the sure losers. We prove COAL\ncan be efficiently implemented for any regression family that admits squared\nloss optimization; it also enjoys strong guarantees with respect to predictive\nperformance and labeling effort. We empirically compare COAL to passive\nlearning and several active learning baselines, showing significant\nimprovements in labeling effort and test cost on real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 02:17:13 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 21:52:19 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 14:11:53 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Huang", "Tzu-Kuo", ""], ["Daume", "Hal", "III"], ["Langford", "John", ""]]}, {"id": "1703.01026", "submitter": "Edward Barker", "authors": "Edward W. Barker and Charl J. Ras", "title": "Unsupervised Basis Function Adaptation for Reinforcement Learning", "comments": "Extended abstract submitted (3 March 2017) for 3rd Multidisciplinary\n  Conference on Reinforcement Learning and Decision Making (RLDM) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using reinforcement learning (RL) algorithms to evaluate a policy it is\ncommon, given a large state space, to introduce some form of approximation\narchitecture for the value function (VF). The exact form of this architecture\ncan have a significant effect on the accuracy of the VF estimate, however, and\ndetermining a suitable approximation architecture can often be a highly complex\ntask. Consequently there is a large amount of interest in the potential for\nallowing RL algorithms to adaptively generate approximation architectures.\n  We investigate a method of adapting approximation architectures which uses\nfeedback regarding the frequency with which an agent has visited certain states\nto guide which areas of the state space to approximate with greater detail.\nThis method is \"unsupervised\" in the sense that it makes no direct reference to\nreward or the VF estimate. We introduce an algorithm based upon this idea which\nadapts a state aggregation approximation architecture on-line.\n  A common method of scoring a VF estimate is to weight the squared Bellman\nerror of each state-action by the probability of that state-action occurring.\nAdopting this scoring method, and assuming $S$ states, we demonstrate\ntheoretically that - provided (1) the number of cells $X$ in the state\naggregation architecture is of order $\\sqrt{S}\\log_2{S}\\ln{S}$ or greater, (2)\nthe policy and transition function are close to deterministic, and (3) the\nprior for the transition function is uniformly distributed - our algorithm,\nused in conjunction with a suitable RL algorithm, can guarantee a score which\nis arbitrarily close to zero as $S$ becomes large. It is able to do this\ndespite having only $O(X \\log_2S)$ space complexity and negligible time\ncomplexity. The results take advantage of certain properties of the stationary\ndistributions of Markov chains.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 03:24:03 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Barker", "Edward W.", ""], ["Ras", "Charl J.", ""]]}, {"id": "1703.01056", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn, Michael Chertkov and Jinwoo Shin", "title": "Gauging Variational Inference", "comments": "Accepted in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing partition function is the most important statistical inference task\narising in applications of Graphical Models (GM). Since it is computationally\nintractable, approximate methods have been used to resolve the issue in\npractice, where mean-field (MF) and belief propagation (BP) are arguably the\nmost popular and successful approaches of a variational type. In this paper, we\npropose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP\n(G-BP), improving MF and BP, respectively. Both provide lower bounds for the\npartition function by utilizing the so-called gauge transformation which\nmodifies factors of GM while keeping the partition function invariant.\nMoreover, we prove that both G-MF and G-BP are exact for GMs with a single loop\nof a special structure, even though the bare MF and BP perform badly in this\ncase. Our extensive experiments, on complete GMs of relatively small size and\non large GM (up-to 300 variables) confirm that the newly proposed algorithms\noutperform and generalize MF and BP.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 06:54:50 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 09:54:05 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 00:15:11 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 04:48:34 GMT"}, {"version": "v5", "created": "Tue, 12 Sep 2017 03:02:06 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1703.01101", "submitter": "Volker Fischer", "authors": "Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas\n  Brox", "title": "Adversarial Examples for Semantic Image Segmentation", "comments": "ICLR 2017 workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods in general and Deep Neural Networks in particular\nhave shown to be vulnerable to adversarial perturbations. So far this\nphenomenon has mainly been studied in the context of whole-image\nclassification. In this contribution, we analyse how adversarial perturbations\ncan affect the task of semantic segmentation. We show how existing adversarial\nattackers can be transferred to this task and that it is possible to create\nimperceptible adversarial perturbations that lead a deep network to misclassify\nalmost all pixels of a chosen class while leaving network prediction nearly\nunchanged outside this class.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 10:27:16 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Fischer", "Volker", ""], ["Kumar", "Mummadi Chaithanya", ""], ["Metzen", "Jan Hendrik", ""], ["Brox", "Thomas", ""]]}, {"id": "1703.01106", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko Heikkil\\\"a and Eemil Lagerspetz and Samuel Kaski and Kana\n  Shimizu and Sasu Tarkoma and Antti Honkela", "title": "Differentially Private Bayesian Learning on Distributed Data", "comments": "13 pages, 7 figures. Modified text, changed algorithm used, included\n  tests on additional dataset, fixed several errors, added proof of asymptotic\n  efficiency to supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of machine learning, for example in health care, would\nbenefit from methods that can guarantee privacy of data subjects. Differential\nprivacy (DP) has become established as a standard for protecting learning\nresults. The standard DP algorithms require a single trusted party to have\naccess to the entire data, which is a clear weakness. We consider DP Bayesian\nlearning in a distributed setting, where each party only holds a single sample\nor a few samples of the data. We propose a learning strategy based on a secure\nmulti-party sum function for aggregating summaries from data holders and the\nGaussian mechanism for DP. Our method builds on an asymptotically optimal and\npractically efficient DP Bayesian inference with rapidly diminishing extra\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 10:44:47 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:11:26 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Heikkil\u00e4", "Mikko", ""], ["Lagerspetz", "Eemil", ""], ["Kaski", "Samuel", ""], ["Shimizu", "Kana", ""], ["Tarkoma", "Sasu", ""], ["Honkela", "Antti", ""]]}, {"id": "1703.01127", "submitter": "Dario Garcia-Gasulla PhD", "authors": "Dario Garcia-Gasulla, Ferran Par\\'es, Armand Vilalta, Jonatan Moreno,\n  Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "On the Behavior of Convolutional Nets for Feature Extraction", "comments": "Published in the Journal of Artificial Intelligence Research (JAIR),\n  Special Track on Deep Learning, Knowledge Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are representation learning techniques. During training,\na deep net is capable of generating a descriptive language of unprecedented\nsize and detail in machine learning. Extracting the descriptive language coded\nwithin a trained CNN model (in the case of image data), and reusing it for\nother purposes is a field of interest, as it provides access to the visual\ndescriptors previously learnt by the CNN after processing millions of images,\nwithout requiring an expensive training phase. Contributions to this field\n(commonly known as feature representation transfer or transfer learning) have\nbeen purely empirical so far, extracting all CNN features from a single layer\nclose to the output and testing their performance by feeding them to a\nclassifier. This approach has provided consistent results, although its\nrelevance is limited to classification tasks. In a completely different\napproach, in this paper we statistically measure the discriminative power of\nevery single feature found within a deep CNN, when used for characterizing\nevery class of 11 datasets. We seek to provide new insights into the behavior\nof CNN features, particularly the ones from convolutional layers, as this can\nbe relevant for their application to knowledge representation and reasoning.\nOur results confirm that low and middle level features may behave differently\nto high level features, but only under certain conditions. We find that all CNN\nfeatures can be used for knowledge representation purposes both by their\npresence or by their absence, doubling the information a single CNN feature may\nprovide. We also study how much noise these features may include, and propose a\nthresholding approach to discard most of it. All these insights have a direct\napplication to the generation of CNN embedding spaces.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 12:23:13 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 14:22:30 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 16:44:14 GMT"}, {"version": "v4", "created": "Mon, 29 Jan 2018 11:56:36 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Par\u00e9s", "Ferran", ""], ["Vilalta", "Armand", ""], ["Moreno", "Jonatan", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1703.01196", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and\n  Sample Complexity", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS) 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the directed acyclic graph (DAG) structure of a Bayesian network\nfrom observational data is a notoriously difficult problem for which many\nhardness results are known. In this paper we propose a provably polynomial-time\nalgorithm for learning sparse Gaussian Bayesian networks with equal noise\nvariance --- a class of Bayesian networks for which the DAG structure can be\nuniquely identified from observational data --- under high-dimensional\nsettings. We show that $O(k^4 \\log p)$ number of samples suffices for our\nmethod to recover the true DAG structure with high probability, where $p$ is\nthe number of variables and $k$ is the maximum Markov blanket size. We obtain\nour theoretical guarantees under a condition called Restricted Strong Adjacency\nFaithfulness, which is strictly weaker than strong faithfulness --- a condition\nthat other methods based on conditional independence testing need for their\nsuccess. The sample complexity of our method matches the information-theoretic\nlimits in terms of the dependence on $p$. We show that our method out-performs\nexisting state-of-the-art methods for learning Gaussian Bayesian networks in\nterms of recovering the true DAG structure while being comparable in speed to\nheuristic methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:05:44 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1703.01220", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Anil Anthony Bharath", "title": "Denoising Adversarial Autoencoders", "comments": "submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning is of growing interest because it unlocks the potential\nheld in vast amounts of unlabelled data to learn useful representations for\ninference. Autoencoders, a form of generative model, may be trained by learning\nto reconstruct unlabelled input data from a latent representation space. More\nrobust representations may be produced by an autoencoder if it learns to\nrecover clean input samples from corrupted ones. Representations may be further\nimproved by introducing regularisation during training to shape the\ndistribution of the encoded data in latent space. We suggest denoising\nadversarial autoencoders, which combine denoising and regularisation, shaping\nthe distribution of latent space using adversarial training. We introduce a\nnovel analysis that shows how denoising may be incorporated into the training\nand sampling of adversarial autoencoders. Experiments are performed to assess\nthe contributions that denoising makes to the learning of representations for\nclassification and sample synthesis. Our results suggest that autoencoders\ntrained using a denoising criterion achieve higher classification performance,\nand can synthesise samples that are more consistent with the input data than\nthose trained without a corruption process.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:59:16 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 20:21:44 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 16:07:58 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 17:18:16 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1703.01234", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and John Paul Gosling", "title": "A Bayesian computer model analysis of Robust Bayesian analyses", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:29:21 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Vernon", "Ian", ""], ["Gosling", "John Paul", ""]]}, {"id": "1703.01253", "submitter": "Jared Ostmeyer", "authors": "Jared Ostmeyer and Lindsay Cowell", "title": "Machine Learning on Sequential Data Using a Recurrent Weighted Average", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.11.066", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNN) are a type of statistical model designed to\nhandle sequential data. The model reads a sequence one symbol at a time. Each\nsymbol is processed based on information collected from the previous symbols.\nWith existing RNN architectures, each symbol is processed using only\ninformation from the previous processing step. To overcome this limitation, we\npropose a new kind of RNN model that computes a recurrent weighted average\n(RWA) over every past processing step. Because the RWA can be computed as a\nrunning average, the computational overhead scales like that of any other RNN\narchitecture. The approach essentially reformulates the attention mechanism\ninto a stand-alone model. The performance of the RWA model is assessed on the\nvariable copy problem, the adding problem, classification of artificial\ngrammar, classification of sequences by length, and classification of the MNIST\nimages (where the pixels are read sequentially one at a time). On almost every\ntask, the RWA model is found to outperform a standard LSTM model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 17:24:49 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:47:54 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 18:02:23 GMT"}, {"version": "v4", "created": "Sun, 26 Mar 2017 23:23:59 GMT"}, {"version": "v5", "created": "Thu, 4 May 2017 17:57:09 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Ostmeyer", "Jared", ""], ["Cowell", "Lindsay", ""]]}, {"id": "1703.01340", "submitter": "Chaofei Yang", "authors": "Chaofei Yang, Qing Wu, Hai Li, Yiran Chen", "title": "Generative Poisoning Attack Method Against Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisoning attack is identified as a severe security threat to machine\nlearning algorithms. In many applications, for example, deep neural network\n(DNN) models collect public data as the inputs to perform re-training, where\nthe input data can be poisoned. Although poisoning attack against support\nvector machines (SVM) has been extensively studied before, there is still very\nlimited knowledge about how such attack can be implemented on neural networks\n(NN), especially DNNs. In this work, we first examine the possibility of\napplying traditional gradient-based method (named as the direct gradient\nmethod) to generate poisoned data against NNs by leveraging the gradient of the\ntarget model w.r.t. the normal data. We then propose a generative method to\naccelerate the generation rate of the poisoned data: an auto-encoder\n(generator) used to generate poisoned data is updated by a reward function of\nthe loss, and the target NN model (discriminator) receives the poisoned data to\ncalculate the loss w.r.t. the normal data. Our experiment results show that the\ngenerative method can speed up the poisoned data generation rate by up to\n239.38x compared with the direct gradient method, with slightly lower model\naccuracy degradation. A countermeasure is also designed to detect such\npoisoning attack methods by checking the loss of the target model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 21:13:48 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Yang", "Chaofei", ""], ["Wu", "Qing", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1703.01347", "submitter": "Seyoung Yun", "authors": "Se-Young Yun and Jun Hyun Nam and Sangwoo Mo and Jinwoo Shin", "title": "Contextual Multi-armed Bandits under Feature Uncertainty", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study contextual multi-armed bandit problems under linear realizability on\nrewards and uncertainty (or noise) on features. For the case of identical noise\non features across actions, we propose an algorithm, coined {\\em NLinRel},\nhaving $O\\left(T^{\\frac{7}{8}} \\left(\\log{(dT)}+K\\sqrt{d}\\right)\\right)$ regret\nbound for $T$ rounds, $K$ actions, and $d$-dimensional feature vectors. Next,\nfor the case of non-identical noise, we observe that popular linear hypotheses\nincluding {\\em NLinRel} are impossible to achieve such sub-linear regret.\nInstead, under assumption of Gaussian feature vectors, we prove that a greedy\nalgorithm has $O\\left(T^{\\frac23}\\sqrt{\\log d}\\right)$ regret bound with\nrespect to the optimal linear hypothesis. Utilizing our theoretical\nunderstanding on the Gaussian case, we also design a practical variant of {\\em\nNLinRel}, coined {\\em Universal-NLinRel}, for arbitrary feature distributions.\nIt first runs {\\em NLinRel} for finding the `true' coefficient vector using\nfeature uncertainties and then adjust it to minimize its regret using the\nstatistical feature information. We justify the performance of {\\em\nUniversal-NLinRel} on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 21:39:56 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Yun", "Se-Young", ""], ["Nam", "Jun Hyun", ""], ["Mo", "Sangwoo", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1703.01363", "submitter": "Yuan Gao", "authors": "James V. Burke, Yuan Gao, Tim Hoheisel", "title": "Convex Geometry of the Generalized Matrix-Fractional Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized matrix-fractional (GMF) functions are a class of matrix support\nfunctions introduced by Burke and Hoheisel as a tool for unifying a range of\nseemingly divergent matrix optimization problems associated with inverse\nproblems, regularization and learning. In this paper we dramatically simplify\nthe support function representation for GMF functions as well as the\nrepresentation of their subdifferentials. These new representations allow the\nready computation of a range of important related geometric objects whose\nformulations were previously unavailable.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 00:00:16 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Burke", "James V.", ""], ["Gao", "Yuan", ""], ["Hoheisel", "Tim", ""]]}, {"id": "1703.01442", "submitter": "Abbas Hosseini", "authors": "Seyed Abbas Hosseini, Keivan Alizadeh, Ali Khodadadi, Ali Arabzadeh,\n  Mehrdad Farajtabar, Hongyuan Zha, Hamid R. Rabiee", "title": "Recurrent Poisson Factorization for Temporal Recommendation", "comments": "Submitted to KDD 2017 | Halifax, Nova Scotia - Canada - sigkdd, Codes\n  are available at https://github.com/AHosseini/RPF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson factorization is a probabilistic model of users and items for\nrecommendation systems, where the so-called implicit consumer data is modeled\nby a factorized Poisson distribution. There are many variants of Poisson\nfactorization methods who show state-of-the-art performance on real-world\nrecommendation tasks. However, most of them do not explicitly take into account\nthe temporal behavior and the recurrent activities of users which is essential\nto recommend the right item to the right user at the right time. In this paper,\nwe introduce Recurrent Poisson Factorization (RPF) framework that generalizes\nthe classical PF methods by utilizing a Poisson process for modeling the\nimplicit feedback. RPF treats time as a natural constituent of the model and\nbrings to the table a rich family of time-sensitive factorization models. To\nelaborate, we instantiate several variants of RPF who are capable of handling\ndynamic user preferences and item specification (DRPF), modeling the\nsocial-aspect of product adoption (SRPF), and capturing the consumption\nheterogeneity among users and items (HRPF). We also develop a variational\nalgorithm for approximate posterior inference that scales up to massive data\nsets. Furthermore, we demonstrate RPF's superior performance over many\nstate-of-the-art methods on synthetic dataset, and large scale real-world\ndatasets on music streaming logs, and user-item interactions in M-Commerce\nplatforms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 11:20:51 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Hosseini", "Seyed Abbas", ""], ["Alizadeh", "Keivan", ""], ["Khodadadi", "Ali", ""], ["Arabzadeh", "Ali", ""], ["Farajtabar", "Mehrdad", ""], ["Zha", "Hongyuan", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1703.01444", "submitter": "Abdelghafour Halimi", "authors": "Abdelghafour Halimi, Hadj Batatia, Jimmy Le Digabel, Gwendal Josse and\n  Jean-Yves Tourneret", "title": "An unsupervised bayesian approach for the joint reconstruction and\n  classification of cutaneous reflectance confocal microscopy images", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "BayesClassifV1_02_2017", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a new Bayesian algorithm for the joint reconstruction and\nclassification of reflectance confocal microscopy (RCM) images, with\napplication to the identification of human skin lentigo. The proposed Bayesian\napproach takes advantage of the distribution of the multiplicative speckle\nnoise affecting the true reflectivity of these images and of appropriate priors\nfor the unknown model parameters. A Markov chain Monte Carlo (MCMC) algorithm\nis proposed to jointly estimate the model parameters and the image of true\nreflectivity while classifying images according to the distribution of their\nreflectivity. Precisely, a Metropolis-whitin-Gibbs sampler is investigated to\nsample the posterior distribution of the Bayesian model associated with RCM\nimages and to build estimators of its parameters, including labels indicating\nthe class of each RCM image. The resulting algorithm is applied to synthetic\ndata and to real images from a clinical study containing healthy and lentigo\npatients.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 11:37:00 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Halimi", "Abdelghafour", ""], ["Batatia", "Hadj", ""], ["Digabel", "Jimmy Le", ""], ["Josse", "Gwendal", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1703.01460", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Sudanthi Wijewickrema, Shuo Zhou, Yun Zhou, Zakaria\n  Mhammedi, Stephen O'Leary, James Bailey", "title": "Adversarial Generation of Real-time Feedback with Neural Networks for\n  Simulation-based Training", "comments": "Appeared in the Proceedings of the 26th International Joint\n  Conference on Artificial Intelligence (IJCAI), Melbourne, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based training (SBT) is gaining popularity as a low-cost and\nconvenient training technique in a vast range of applications. However, for a\nSBT platform to be fully utilized as an effective training tool, it is\nessential that feedback on performance is provided automatically in real-time\nduring training. It is the aim of this paper to develop an efficient and\neffective feedback generation method for the provision of real-time feedback in\nSBT. Existing methods either have low effectiveness in improving novice skills\nor suffer from low efficiency, resulting in their inability to be used in\nreal-time. In this paper, we propose a neural network based method to generate\nfeedback using the adversarial technique. The proposed method utilizes a\nbounded adversarial update to minimize a L1 regularized loss via\nback-propagation. We empirically show that the proposed method can be used to\ngenerate simple, yet effective feedback. Also, it was observed to have high\neffectiveness and efficiency when compared to existing methods, thus making it\na promising option for real-time feedback generation in SBT.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 14:24:27 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:54:58 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 14:16:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ma", "Xingjun", ""], ["Wijewickrema", "Sudanthi", ""], ["Zhou", "Shuo", ""], ["Zhou", "Yun", ""], ["Mhammedi", "Zakaria", ""], ["O'Leary", "Stephen", ""], ["Bailey", "James", ""]]}, {"id": "1703.01488", "submitter": "Akash Srivastava", "authors": "Akash Srivastava and Charles Sutton", "title": "Autoencoding Variational Inference For Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are one of the most popular methods for learning representations\nof text, but a major challenge is that any change to the topic model requires\nmathematically deriving a new inference algorithm. A promising approach to\naddress this problem is autoencoding variational Bayes (AEVB), but it has\nproven diffi- cult to apply to topic models in practice. We present what is to\nour knowledge the first effective AEVB based inference method for latent\nDirichlet allocation (LDA), which we call Autoencoded Variational Inference For\nTopic Model (AVITM). This model tackles the problems caused for AEVB by the\nDirichlet prior and by component collapsing. We find that AVITM matches\ntraditional methods in accuracy with much better inference time. Indeed,\nbecause of the inference network, we find that it is unnecessary to pay the\ncomputational cost of running variational optimization on test data. Because\nAVITM is black box, it is readily applied to new topic models. As a dramatic\nillustration of this, we present a new topic model called ProdLDA, that\nreplaces the mixture model in LDA with a product of experts. By changing only\none line of code from LDA, we find that ProdLDA yields much more interpretable\ntopics, even if LDA is trained via collapsed Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 16:28:15 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Srivastava", "Akash", ""], ["Sutton", "Charles", ""]]}, {"id": "1703.01499", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sambit Ghadai, Gavin Young, Soumik Sarkar, Adarsh\n  Krishnamurthy", "title": "A Machine-Learning Framework for Design for Manufacturability", "comments": "this is a duplicate submission. Hence want to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  this is a duplicate submission(original is arXiv:1612.02141). Hence want to\nwithdraw it\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 17:37:32 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 14:55:52 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Balu", "Aditya", ""], ["Ghadai", "Sambit", ""], ["Young", "Gavin", ""], ["Sarkar", "Soumik", ""], ["Krishnamurthy", "Adarsh", ""]]}, {"id": "1703.01506", "submitter": "Felipe Gutierrez-Barragan", "authors": "Felipe Gutierrez-Barragan, Vamsi K. Ithapu, Chris Hinrichs, Camille\n  Maumet, Sterling C. Johnson, Thomas E. Nichols, Vikas Singh, and the ADNI", "title": "Accelerating Permutation Testing in Voxel-wise Analysis through Subspace\n  Tracking: A new plugin for SnPM", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.07.025", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation testing is a non-parametric method for obtaining the max null\ndistribution used to compute corrected $p$-values that provide strong control\nof false positives. In neuroimaging, however, the computational burden of\nrunning such an algorithm can be significant. We find that by viewing the\npermutation testing procedure as the construction of a very large permutation\ntesting matrix, $T$, one can exploit structural properties derived from the\ndata and the test statistics to reduce the runtime under certain conditions. In\nparticular, we see that $T$ is low-rank plus a low-variance residual. This\nmakes $T$ a good candidate for low-rank matrix completion, where only a very\nsmall number of entries of $T$ ($\\sim0.35\\%$ of all entries in our experiments)\nhave to be computed to obtain a good estimate. Based on this observation, we\npresent RapidPT, an algorithm that efficiently recovers the max null\ndistribution commonly obtained through regular permutation testing in\nvoxel-wise analysis. We present an extensive validation on a synthetic dataset\nand four varying sized datasets against two baselines: Statistical\nNonParametric Mapping (SnPM13) and a standard permutation testing\nimplementation (referred as NaivePT). We find that RapidPT achieves its best\nruntime performance on medium sized datasets ($50 \\leq n \\leq 200$), with\nspeedups of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger\ndatasets ($n \\geq 200$) RapidPT outperforms NaivePT (6x - 200x) on all\ndatasets, and provides large speedups over SnPM13 when more than 10000\npermutations (2x - 15x) are needed. The implementation is a standalone toolbox\nand also integrated within SnPM13, able to leverage multi-core architectures\nwhen available.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 19:07:42 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 18:03:54 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Gutierrez-Barragan", "Felipe", ""], ["Ithapu", "Vamsi K.", ""], ["Hinrichs", "Chris", ""], ["Maumet", "Camille", ""], ["Johnson", "Sterling C.", ""], ["Nichols", "Thomas E.", ""], ["Singh", "Vikas", ""], ["ADNI", "the", ""]]}, {"id": "1703.01526", "submitter": "Prashanth R", "authors": "R. Prashanth, Sumantra Dutta Roy, Pravat K. Mandal, Shantanu Ghosh", "title": "High Accuracy Classification of Parkinson's Disease through Shape\n  Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging", "comments": "9 pages, 5 figures, Accepted in the IEEE Journal of Biomedical and\n  Health Informatics, Additional supplementary documents available at\n  http://ieeexplore.ieee.org/document/7442754/", "journal-ref": null, "doi": "10.1109/JBHI.2016.2547901", "report-no": null, "categories": "stat.AP cs.CV physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate identification of parkinsonian syndromes (PS) involving\npresynaptic degeneration from non-degenerative variants such as Scans Without\nEvidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for\neffective patient management as the course, therapy and prognosis differ\nsubstantially between the two groups. In this study, we use Single Photon\nEmission Computed Tomography (SPECT) images from healthy normal, early PD and\nSWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative\n(PPMI) database, and process them to compute shape- and surface fitting-based\nfeatures for the three groups. We use these features to develop and compare\nvarious classification models that can discriminate between scans showing\ndopaminergic deficit, as in PD, from scans without the deficit, as in healthy\nnormal or SWEDD. Along with it, we also compare these features with Striatal\nBinding Ratio (SBR)-based features, which are well-established and clinically\nused, by computing a feature importance score using Random forests technique.\nWe observe that the Support Vector Machine (SVM) classifier gave the best\nperformance with an accuracy of 97.29%. These features also showed higher\nimportance than the SBR-based features. We infer from the study that shape\nanalysis and surface fitting are useful and promising methods for extracting\ndiscriminatory features that can be used to develop diagnostic models that\nmight have the potential to help clinicians in the diagnostic process.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 21:50:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Prashanth", "R.", ""], ["Roy", "Sumantra Dutta", ""], ["Mandal", "Pravat K.", ""], ["Ghosh", "Shantanu", ""]]}, {"id": "1703.01536", "submitter": "Rajiv Sambasivan", "authors": "Rajiv Sambasivan and Sourish Das", "title": "A Statistical Machine Learning Approach to Yield Curve Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yield curve forecasting is an important problem in finance. In this work we\nexplore the use of Gaussian Processes in conjunction with a dynamic modeling\nstrategy, much like the Kalman Filter, to model the yield curve. Gaussian\nProcesses have been successfully applied to model functional data in a variety\nof applications. A Gaussian Process is used to model the yield curve. The\nhyper-parameters of the Gaussian Process model are updated as the algorithm\nreceives yield curve data. Yield curve data is typically available as a time\nseries with a frequency of one day. We compare existing methods to forecast the\nyield curve with the proposed method. The results of this study showed that\nwhile a competing method (a multivariate time series method) performed well in\nforecasting the yields at the short term structure region of the yield curve,\nGaussian Processes perform well in the medium and long term structure regions\nof the yield curve. Accuracy in the long term structure region of the yield\ncurve has important practical implications. The Gaussian Process framework\nyields uncertainty and probability estimates directly in contrast to other\ncompeting methods. Analysts are frequently interested in this information. In\nthis study the proposed method has been applied to yield curve forecasting,\nhowever it can be applied to model high frequency time series data or data\nstreams in other domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 23:43:36 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""]]}, {"id": "1703.01541", "submitter": "Mathieu Blondel", "authors": "Marco Cuturi and Mathieu Blondel", "title": "Soft-DTW: a Differentiable Loss Function for Time-Series", "comments": "Published in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a differentiable learning loss between time series,\nbuilding upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the\nEuclidean distance, DTW can compare time series of variable size and is robust\nto shifts or dilatations across the time dimension. To compute DTW, one\ntypically solves a minimal-cost alignment problem between two time series using\ndynamic programming. Our work takes advantage of a smoothed formulation of DTW,\ncalled soft-DTW, that computes the soft-minimum of all alignment costs. We show\nin this paper that soft-DTW is a differentiable loss function, and that both\nits value and gradient can be computed with quadratic time/space complexity\n(DTW has quadratic time but linear space complexity). We show that this\nregularization is particularly well suited to average and cluster time series\nunder the DTW geometry, a task for which our proposal significantly outperforms\nexisting baselines. Next, we propose to tune the parameters of a machine that\noutputs time series by minimizing its fit with ground-truth labels in a\nsoft-DTW sense.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 01:30:28 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 11:10:29 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Cuturi", "Marco", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1703.01557", "submitter": "Lidong Bing", "authors": "Lidong Bing and William W. Cohen and Bhuwan Dhingra", "title": "Using Graphs of Classifiers to Impose Declarative Constraints on\n  Semi-supervised Learning", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach to modeling semi-supervised learning (SSL)\nalgorithms. Specifically, we present a declarative language for modeling both\ntraditional supervised classification tasks and many SSL heuristics, including\nboth well-known heuristics such as co-training and novel domain-specific\nheuristics. In addition to representing individual SSL heuristics, we show that\nmultiple heuristics can be automatically combined using Bayesian optimization\nmethods. We experiment with two classes of tasks, link-based text\nclassification and relation extraction. We show modest improvements on\nwell-studied link-based classification benchmarks, and state-of-the-art results\non relation-extraction tasks for two realistic domains.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 04:43:41 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 07:46:21 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Bing", "Lidong", ""], ["Cohen", "William W.", ""], ["Dhingra", "Bhuwan", ""]]}, {"id": "1703.01594", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Pierre-Olivier Amblard, Simon Barthelm\\'e", "title": "Graph sampling with determinantal processes", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new random sampling strategy for k-bandlimited signals defined\non graphs, based on determinantal point processes (DPP). For small graphs, ie,\nin cases where the spectrum of the graph is accessible, we exhibit a DPP\nsampling scheme that enables perfect recovery of bandlimited signals. For large\ngraphs, ie, in cases where the graph's spectrum is not accessible, we\ninvestigate, both theoretically and empirically, a sub-optimal but much faster\nDPP based on loop-erased random walks on the graph. Preliminary experiments\nshow promising results especially in cases where the number of measurements\nshould stay as small as possible and for graphs that have a strong community\nstructure. Our sampling scheme is efficient and can be applied to graphs with\nup to $10^6$ nodes.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 13:18:19 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Amblard", "Pierre-Olivier", ""], ["Barthelm\u00e9", "Simon", ""]]}, {"id": "1703.01606", "submitter": "Lior Wolf", "authors": "Tomer Galanti, Lior Wolf", "title": "A Theory of Output-Side Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning a mapping from an input space to an output space, the\nassumption that the sample distribution of the training data is the same as\nthat of the test data is often violated. Unsupervised domain shift methods\nadapt the learned function in order to correct for this shift. Previous work\nhas focused on utilizing unlabeled samples from the target distribution. We\nconsider the complementary problem in which the unlabeled samples are given\npost mapping, i.e., we are given the outputs of the mapping of unknown samples\nfrom the shifted domain. Two other variants are also studied: the two sided\nversion, in which unlabeled samples are give from both the input and the output\nspaces, and the Domain Transfer problem, which was recently formalized. In all\ncases, we derive generalization bounds that employ discrepancy terms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 15:12:09 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Galanti", "Tomer", ""], ["Wolf", "Lior", ""]]}, {"id": "1703.01610", "submitter": "Wei Chen", "authors": "Qinshi Wang and Wei Chen", "title": "Improving Regret Bounds for Combinatorial Semi-Bandits with\n  Probabilistically Triggered Arms and Its Applications", "comments": "This is the full version of the paper accepted at NIPS'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study combinatorial multi-armed bandit with probabilistically triggered\narms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior\nCMAB-T studies where the regret bounds contain a possibly exponentially large\nfactor of $1/p^*$, where $p^*$ is the minimum positive probability that an arm\nis triggered by any action. We address this issue by introducing a triggering\nprobability modulated (TPM) bounded smoothness condition into the general\nCMAB-T framework, and show that many applications such as influence\nmaximization bandit and combinatorial cascading bandit satisfy this TPM\ncondition. As a result, we completely remove the factor of $1/p^*$ from the\nregret bounds, achieving significantly better regret bounds for influence\nmaximization and cascading bandits than before. Finally, we provide lower bound\nresults showing that the factor $1/p^*$ is unavoidable for general CMAB-T\nproblems, suggesting that the TPM condition is crucial in removing this factor.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 15:31:35 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 08:25:41 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 05:50:04 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 19:21:09 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 07:55:43 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Qinshi", ""], ["Chen", "Wei", ""]]}, {"id": "1703.01619", "submitter": "Graham Neubig", "authors": "Graham Neubig", "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial", "comments": "65 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 16:10:11 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Neubig", "Graham", ""]]}, {"id": "1703.01717", "submitter": "Jack Gorham", "authors": "Jackson Gorham and Lester Mackey", "title": "Measuring Sample Quality with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid\nsampling at the cost of more biased inference. Since standard MCMC diagnostics\nfail to detect these biases, researchers have developed computable Stein\ndiscrepancy measures that provably determine the convergence of a sample to its\ntarget distribution. This approach was recently combined with the theory of\nreproducing kernels to define a closed-form kernel Stein discrepancy (KSD)\ncomputable by summing kernel evaluations across pairs of sample points. We\ndevelop a theory of weak convergence for KSDs based on Stein's method,\ndemonstrate that commonly used KSDs fail to detect non-convergence even for\nGaussian targets, and show that kernels with slowly decaying tails provably\ndetermine convergence for a large class of target distributions. The resulting\nconvergence-determining KSDs are suitable for comparing biased, exact, and\ndeterministic sample sequences and simpler to compute and parallelize than\nalternative Stein discrepancies. We use our tools to compare biased samplers,\nselect sampler hyperparameters, and improve upon existing KSD approaches to\none-sample hypothesis testing and sample quality improvement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 03:22:39 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 06:04:43 GMT"}, {"version": "v3", "created": "Fri, 7 Jul 2017 20:41:24 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 23:30:56 GMT"}, {"version": "v5", "created": "Fri, 21 Jul 2017 04:38:46 GMT"}, {"version": "v6", "created": "Thu, 3 Aug 2017 21:23:32 GMT"}, {"version": "v7", "created": "Sat, 19 Aug 2017 01:35:40 GMT"}, {"version": "v8", "created": "Wed, 13 Sep 2017 20:51:38 GMT"}, {"version": "v9", "created": "Thu, 15 Oct 2020 02:08:48 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gorham", "Jackson", ""], ["Mackey", "Lester", ""]]}, {"id": "1703.01780", "submitter": "Antti Tarvainen", "authors": "Antti Tarvainen and Harri Valpola", "title": "Mean teachers are better role models: Weight-averaged consistency\n  targets improve semi-supervised deep learning results", "comments": "In this version: Corrected hyperparameters of the 4000-label CIFAR-10\n  ResNet experiment. Changed Antti's contact info, Advances in Neural\n  Information Processing Systems 30 (NIPS 2017) pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Temporal Ensembling has achieved state-of-the-art\nresults in several semi-supervised learning benchmarks. It maintains an\nexponential moving average of label predictions on each training example, and\npenalizes predictions that are inconsistent with this target. However, because\nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy\nwhen learning large datasets. To overcome this problem, we propose Mean\nTeacher, a method that averages model weights instead of label predictions. As\nan additional benefit, Mean Teacher improves test accuracy and enables training\nwith fewer labels than Temporal Ensembling. Without changing the network\narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250\nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also\nshow that a good network architecture is crucial to performance. Combining Mean\nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with\n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels\nfrom 35.24% to 9.11%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:34:56 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 07:41:30 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 11:14:43 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 09:13:01 GMT"}, {"version": "v5", "created": "Mon, 8 Jan 2018 08:10:09 GMT"}, {"version": "v6", "created": "Mon, 16 Apr 2018 10:39:11 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Tarvainen", "Antti", ""], ["Valpola", "Harri", ""]]}, {"id": "1703.01785", "submitter": "Luca Franceschi", "authors": "Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil", "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization", "comments": "- Posted the ICML Camera Ready version. - Added a link to a newer\n  package implementation of the algorithms", "journal-ref": "Franceschi, L., Donini, M., Frasconi, P. & Pontil, M.. (2017).\n  Forward and Reverse Gradient-Based Hyperparameter Optimization. Proceedings\n  of the 34th International Conference on Machine Learning, in PMLR\n  70:1165-1173", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two procedures (reverse-mode and forward-mode) for computing the\ngradient of the validation error with respect to the hyperparameters of any\niterative learning algorithm such as stochastic gradient descent. These\nprocedures mirror two methods of computing gradients for recurrent neural\nnetworks and have different trade-offs in terms of running time and space\nrequirements. Our formulation of the reverse-mode procedure is linked to\nprevious work by Maclaurin et al. [2015] but does not require reversible\ndynamics. The forward-mode procedure is suitable for real-time hyperparameter\nupdates, which may significantly speed up hyperparameter optimization on large\ndatasets. We present experiments on data cleaning and on learning task\ninteractions. We also present one large-scale experiment where the use of\nprevious gradient-based methods would be prohibitive.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:44:32 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 19:00:39 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 17:17:59 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Franceschi", "Luca", ""], ["Donini", "Michele", ""], ["Frasconi", "Paolo", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1703.01804", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Gregory Valiant", "title": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition\n  Algorithm for Practical Use", "comments": "Minor updates to presentation. Appears in ICML'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular Alternating Least Squares (ALS) algorithm for tensor\ndecomposition is efficient and easy to implement, but often converges to poor\nlocal optima---particularly when the weights of the factors are non-uniform. We\npropose a modification of the ALS approach that is as efficient as standard\nALS, but provably recovers the true factors with random initialization under\nstandard incoherence assumptions on the factors of the tensor. We demonstrate\nthe significant practical superiority of our approach over traditional ALS for\na variety of tasks on synthetic data---including tensor factorization on exact,\nnoisy and over-complete tensors, as well as tensor completion---and for\ncomputing word embeddings from a third-order word tri-occurrence tensor.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 10:31:00 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 21:15:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Sharan", "Vatsal", ""], ["Valiant", "Gregory", ""]]}, {"id": "1703.01842", "submitter": "Mathilde M\\'enoret", "authors": "Mathilde M\\'enoret, Nicolas Farrugia, Bastien Pasdeloup and Vincent\n  Gripon", "title": "Evaluating Graph Signal Processing for Neuroimaging Through\n  Classification and Dimensionality Reduction", "comments": "5 pages, GlobalSIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Signal Processing (GSP) is a promising framework to analyze\nmulti-dimensional neuroimaging datasets, while taking into account both the\nspatial and functional dependencies between brain signals. In the present work,\nwe apply dimensionality reduction techniques based on graph representations of\nthe brain to decode brain activity from real and simulated fMRI datasets. We\nintroduce seven graphs obtained from a) geometric structure and/or b)\nfunctional connectivity between brain areas at rest, and compare them when\nperforming dimension reduction for classification. We show that mixed graphs\nusing both a) and b) offer the best performance. We also show that graph\nsampling methods perform better than classical dimension reduction including\nPrincipal Component Analysis (PCA) and Independent Component Analysis (ICA).\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 12:45:39 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 08:40:56 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 12:41:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["M\u00e9noret", "Mathilde", ""], ["Farrugia", "Nicolas", ""], ["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""]]}, {"id": "1703.01872", "submitter": "Antonio Loquercio", "authors": "Antonio Loquercio, Francesca Della Torre, Massimo Buscema", "title": "Computational Eco-Systems for Handwritten Digits Recognition", "comments": "Withdrawn from Journal for flaws in methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the importance of diversity in biological system, we built an\nheterogeneous system that could achieve this goal. Our architecture could be\nsummarized in two basic steps. First, we generate a diverse set of\nclassification hypothesis using both Convolutional Neural Networks, currently\nthe state-of-the-art technique for this task, among with other traditional and\ninnovative machine learning techniques. Then, we optimally combine them through\nMeta-Nets, a family of recently developed and performing ensemble methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 13:59:39 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 14:58:39 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Loquercio", "Antonio", ""], ["Della Torre", "Francesca", ""], ["Buscema", "Massimo", ""]]}, {"id": "1703.01898", "submitter": "Dani Yogatama", "authors": "Dani Yogatama, Chris Dyer, Wang Ling, Phil Blunsom", "title": "Generative and Discriminative Text Classification with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically characterize the performance of discriminative and generative\nLSTM models for text classification. We find that although RNN-based generative\nmodels are more powerful than their bag-of-words ancestors (e.g., they account\nfor conditional dependencies across words in a document), they have higher\nasymptotic error rates than discriminatively trained RNN models. However we\nalso find that generative models approach their asymptotic error rate more\nrapidly than their discriminative counterparts---the same pattern that Ng &\nJordan (2001) proved holds for linear classification models that make more\nnaive conditional independence assumptions. Building on this finding, we\nhypothesize that RNN-based generative classification models will be more robust\nto shifts in the data distribution. This hypothesis is confirmed in a series of\nexperiments in zero-shot and continual learning settings that show that\ngenerative models substantially outperform discriminative models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 14:40:09 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 01:27:23 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Yogatama", "Dani", ""], ["Dyer", "Chris", ""], ["Ling", "Wang", ""], ["Blunsom", "Phil", ""]]}, {"id": "1703.01923", "submitter": "Oliver Lauwers", "authors": "Oliver Lauwers, Bart De Moor", "title": "A time series distance measure for efficient clustering of input output\n  signals by their underlying dynamics", "comments": "6 pages, 4 figures, sent in for review to IEEE L-CSS (CDC 2017\n  option)", "journal-ref": null, "doi": "10.1109/LCSYS.2017.2715399", "report-no": null, "categories": "cs.SY math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from a dataset with input/output time series generated by multiple\ndeterministic linear dynamical systems, this paper tackles the problem of\nautomatically clustering these time series. We propose an extension to the\nso-called Martin cepstral distance, that allows to efficiently cluster these\ntime series, and apply it to simulated electrical circuits data. Traditionally,\ntwo ways of handling the problem are used. The first class of methods employs a\ndistance measure on time series (e.g. Euclidean, Dynamic Time Warping) and a\nclustering technique (e.g. k-means, k-medoids, hierarchical clustering) to find\nnatural groups in the dataset. It is, however, often not clear whether these\ndistance measures effectively take into account the specific temporal\ncorrelations in these time series. The second class of methods uses the\ninput/output data to identify a dynamic system using an identification scheme,\nand then applies a model norm-based distance (e.g. H2, H-infinity) to find out\nwhich systems are similar. This, however, can be very time consuming for large\namounts of long time series data. We show that the new distance measure\npresented in this paper performs as good as when every input/output pair is\nmodelled explicitly, but remains computationally much less complex. The\ncomplexity of calculating this distance between two time series of length N is\nO(N logN).\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:33:18 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Lauwers", "Oliver", ""], ["De Moor", "Bart", ""]]}, {"id": "1703.01925", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Brooks Paige, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Grammar Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been wildly successful at learning coherent\nlatent representations for continuous data such as video and audio. However,\ngenerative modeling of discrete data such as arithmetic expressions and\nmolecular structures still poses significant challenges. Crucially,\nstate-of-the-art methods often produce outputs that are not valid. We make the\nkey observation that frequently, discrete data can be represented as a parse\ntree from a context-free grammar. We propose a variational autoencoder which\nencodes and decodes directly to and from these parse trees, ensuring the\ngenerated outputs are always valid. Surprisingly, we show that not only does\nour model more often generate valid outputs, it also learns a more coherent\nlatent space in which nearby points decode to similar discrete outputs. We\ndemonstrate the effectiveness of our learned models by showing their improved\nperformance in Bayesian optimization for symbolic regression and molecular\nsynthesis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:36:37 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kusner", "Matt J.", ""], ["Paige", "Brooks", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1703.01961", "submitter": "Christos Louizos", "authors": "Christos Louizos and Max Welling", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural\n  Networks", "comments": "Appearing at the International Conference on Machine Learning (ICML)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reinterpret multiplicative noise in neural networks as auxiliary random\nvariables that augment the approximate posterior in a variational setting for\nBayesian neural networks. We show that through this interpretation it is both\nefficient and straightforward to improve the approximation by employing\nnormalizing flows while still allowing for local reparametrizations and a\ntractable lower bound. In experiments we show that with this new approximation\nwe can significantly improve upon classical mean field for Bayesian neural\nnetworks on both predictive accuracy as well as predictive uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:39:16 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 21:05:58 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Louizos", "Christos", ""], ["Welling", "Max", ""]]}, {"id": "1703.01962", "submitter": "Constantin Grigo", "authors": "Constantin Grigo and Phaedon-Stelios Koutsourelakis", "title": "Probabilistic Reduced-Order Modeling for Stochastic Partial Differential\n  Equations", "comments": null, "journal-ref": "Eccomas ProceediaUNCECOMP (2017) 111-129", "doi": "10.7712/120217.5356.16731", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a Bayesian formulation to coarse-graining (CG) of PDEs where the\ncoefficients (e.g. material parameters) exhibit random, fine scale variability.\nThe direct solution to such problems requires grids that are small enough to\nresolve this fine scale variability which unavoidably requires the repeated\nsolution of very large systems of algebraic equations. We establish a\nphysically inspired, data-driven coarse-grained model which learns a low-\ndimensional set of microstructural features that are predictive of the\nfine-grained model (FG) response. Once learned, those features provide a sharp\ndistribution over the coarse scale effec- tive coefficients of the PDE that are\nmost suitable for prediction of the fine scale model output. This ultimately\nallows to replace the computationally expensive FG by a generative proba-\nbilistic model based on evaluating the much cheaper CG several times. Sparsity\nenforcing pri- ors further increase predictive efficiency and reveal\nmicrostructural features that are important in predicting the FG response.\nMoreover, the model yields probabilistic rather than single-point predictions,\nwhich enables the quantification of the unavoidable epistemic uncertainty that\nis present due to the information loss that occurs during the coarse-graining\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:42:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Grigo", "Constantin", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1703.01968", "submitter": "Zi Wang", "authors": "Zi Wang and Stefanie Jegelka", "title": "Max-value Entropy Search for Efficient Bayesian Optimization", "comments": "Proceedings of the 34th International Conference on Machine Learning,\n  Sydney, Australia, PMLR 70, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy Search (ES) and Predictive Entropy Search (PES) are popular and\nempirically successful Bayesian Optimization techniques. Both rely on a\ncompelling information-theoretic motivation, and maximize the information\ngained about the $\\arg\\max$ of the unknown function; yet, both are plagued by\nthe expensive computation for estimating entropies. We propose a new criterion,\nMax-value Entropy Search (MES), that instead uses the information about the\nmaximum function value. We show relations of MES to other Bayesian optimization\nmethods, and establish a regret bound. We observe that MES maintains or\nimproves the good empirical performance of ES/PES, while tremendously\nlightening the computational burden. In particular, MES is much more robust to\nthe number of samples used for computing the entropy, and hence more efficient\nfor higher dimensional problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:52:54 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 17:32:01 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 18:05:14 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1703.01973", "submitter": "Zi Wang", "authors": "Zi Wang and Chengtao Li and Stefanie Jegelka and Pushmeet Kohli", "title": "Batched High-dimensional Bayesian Optimization via Structural Kernel\n  Learning", "comments": "Proceedings of the 34th International Conference on Machine Learning,\n  Sydney, Australia, PMLR 70, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of high-dimensional black-box functions is an extremely\nchallenging problem. While Bayesian optimization has emerged as a popular\napproach for optimizing black-box functions, its applicability has been limited\nto low-dimensional problems due to its computational and statistical challenges\narising from high-dimensional settings. In this paper, we propose to tackle\nthese challenges by (1) assuming a latent additive structure in the function\nand inferring it properly for more efficient and effective BO, and (2)\nperforming multiple evaluations in parallel to reduce the number of iterations\nrequired by the method. Our novel approach learns the latent structure with\nGibbs sampling and constructs batched queries using determinantal point\nprocesses. Experimental validations on both synthetic and real-world functions\ndemonstrate that the proposed method outperforms the existing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:01:19 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 16:04:06 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1703.01988", "submitter": "Alexander Pritzel", "authors": "Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adri\\`a\n  Puigdom\\`enech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles\n  Blundell", "title": "Neural Episodic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning methods attain super-human performance in a wide\nrange of environments. Such methods are grossly inefficient, often taking\norders of magnitudes more data than humans to achieve reasonable performance.\nWe propose Neural Episodic Control: a deep reinforcement learning agent that is\nable to rapidly assimilate new experiences and act upon them. Our agent uses a\nsemi-tabular representation of the value function: a buffer of past experience\ncontaining slowly changing state representations and rapidly updated estimates\nof the value function. We show across a wide range of environments that our\nagent learns significantly faster than other state-of-the-art, general purpose\ndeep reinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:23:27 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Pritzel", "Alexander", ""], ["Uria", "Benigno", ""], ["Srinivasan", "Sriram", ""], ["Puigdom\u00e8nech", "Adri\u00e0", ""], ["Vinyals", "Oriol", ""], ["Hassabis", "Demis", ""], ["Wierstra", "Daan", ""], ["Blundell", "Charles", ""]]}, {"id": "1703.02000", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang,\n  Yong Yu, Jun Wang", "title": "Activation Maximization Generative Adversarial Nets", "comments": "Accepted as a conference paper on ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels have been empirically shown useful in improving the sample\nquality of generative adversarial nets (GANs). In this paper, we mathematically\nstudy the properties of the current variants of GANs that make use of class\nlabel information. With class aware gradient and cross-entropy decomposition,\nwe reveal how class labels and associated losses influence GAN's training.\nBased on that, we propose Activation Maximization Generative Adversarial\nNetworks (AM-GAN) as an advanced solution. Comprehensive experiments have been\nconducted to validate our analysis and evaluate the effectiveness of our\nsolution, where AM-GAN outperforms other strong baselines and achieves\nstate-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we\ndemonstrate that, with the Inception ImageNet classifier, Inception Score\nmainly tracks the diversity of the generator, and there is, however, no\nreliable evidence that it can reflect the true sample quality. We thus propose\na new metric, called AM Score, to provide a more accurate estimation of the\nsample quality. Our proposed model also outperforms the baseline methods in the\nnew metric.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 17:42:55 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 16:33:55 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 15:32:29 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 16:56:07 GMT"}, {"version": "v5", "created": "Sat, 5 Aug 2017 08:17:04 GMT"}, {"version": "v6", "created": "Wed, 8 Nov 2017 13:49:19 GMT"}, {"version": "v7", "created": "Tue, 30 Jan 2018 18:28:35 GMT"}, {"version": "v8", "created": "Wed, 11 Jul 2018 05:43:27 GMT"}, {"version": "v9", "created": "Fri, 16 Nov 2018 07:18:19 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zhou", "Zhiming", ""], ["Cai", "Han", ""], ["Rong", "Shu", ""], ["Song", "Yuxuan", ""], ["Ren", "Kan", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1703.02059", "submitter": "Manuel Gomez Rodriguez", "authors": "Ali Zarezade and Abir De and Hamid Rabiee and Manuel Gomez Rodriguez", "title": "Cheshire: An Online Algorithm for Activity Maximization in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement in social networks depends critically on the number of online\nactions their users take in the network. Can we design an algorithm that finds\nwhen to incentivize users to take actions to maximize the overall activity in a\nsocial network? In this paper, we model the number of online actions over time\nusing multidimensional Hawkes processes, derive an alternate representation of\nthese processes based on stochastic differential equations (SDEs) with jumps\nand, exploiting this alternate representation, address the above question from\nthe perspective of stochastic optimal control of SDEs with jumps. We find that\nthe optimal level of incentivized actions depends linearly on the current level\nof overall actions. Moreover, the coefficients of this linear relationship can\nbe found by solving a matrix Riccati differential equation, which can be solved\nefficiently, and a first order differential equation, which has a closed form\nsolution. As a result, we are able to design an efficient online algorithm,\nCheshire, to sample the optimal times of the users' incentivized actions.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently maximize the number of online actions more\neffectively than the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:01:03 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zarezade", "Ali", ""], ["De", "Abir", ""], ["Rabiee", "Hamid", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1703.02065", "submitter": "Or Sharir", "authors": "Or Sharir and Amnon Shashua", "title": "On the Expressive Power of Overlapping Architectures of Deep Learning", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressive efficiency refers to the relation between two architectures A and\nB, whereby any function realized by B could be replicated by A, but there\nexists functions realized by A, which cannot be replicated by B unless its size\ngrows significantly larger. For example, it is known that deep networks are\nexponentially efficient with respect to shallow networks, in the sense that a\nshallow network must grow exponentially large in order to approximate the\nfunctions represented by a deep network of polynomial size. In this work, we\nextend the study of expressive efficiency to the attribute of network\nconnectivity and in particular to the effect of \"overlaps\" in the convolutional\nprocess, i.e., when the stride of the convolution is smaller than its filter\nsize (receptive field). To theoretically analyze this aspect of network's\ndesign, we focus on a well-established surrogate for ConvNets called\nConvolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically\nthat our results hold for standard ConvNets as well. Specifically, our analysis\nshows that having overlapping local receptive fields, and more broadly denser\nconnectivity, results in an exponential increase in the expressive capacity of\nneural networks. Moreover, while denser connectivity can increase the\nexpressive capacity, we show that the most common types of modern architectures\nalready exhibit exponential increase in expressivity, without relying on\nfully-connected layers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:07:12 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 09:05:54 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 14:02:11 GMT"}, {"version": "v4", "created": "Sat, 24 Feb 2018 14:47:00 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1703.02089", "submitter": "Jean Daunizeau", "authors": "Jean Daunizeau", "title": "The variational Laplace approach to approximate Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approaches to approximate Bayesian inference provide very\nefficient means of performing parameter estimation and model selection. Among\nthese, so-called variational-Laplace or VL schemes rely on Gaussian\napproximations to posterior densities on model parameters. In this note, we\nreview the main variants of VL approaches, that follow from considering\nnonlinear models of continuous and/or categorical data. En passant, we also\nderive a few novel theoretical results that complete the portfolio of existing\nanalyses of variational Bayesian approaches, including investigations of their\nasymptotic convergence. We also suggest practical ways of extending existing VL\napproaches to hierarchical generative models that include (e.g., precision)\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:02:22 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 17:45:14 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Daunizeau", "Jean", ""]]}, {"id": "1703.02091", "submitter": "Han Zhu", "authors": "Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, Kun Gai", "title": "Optimized Cost per Click in Taobao Display Advertising", "comments": "Accepted by KDD 2017", "journal-ref": null, "doi": "10.1145/3097983.3098134", "report-no": null, "categories": "cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taobao, as the largest online retail platform in the world, provides billions\nof online display advertising impressions for millions of advertisers every\nday. For commercial purposes, the advertisers bid for specific spots and target\ncrowds to compete for business traffic. The platform chooses the most suitable\nads to display in tens of milliseconds. Common pricing methods include cost per\nmille (CPM) and cost per click (CPC). Traditional advertising systems target\ncertain traits of users and ad placements with fixed bids, essentially regarded\nas coarse-grained matching of bid and traffic quality. However, the fixed bids\nset by the advertisers competing for different quality requests cannot fully\noptimize the advertisers' key requirements. Moreover, the platform has to be\nresponsible for the business revenue and user experience. Thus, we proposed a\nbid optimizing strategy called optimized cost per click (OCPC) which\nautomatically adjusts the bid to achieve finer matching of bid and traffic\nquality of page view (PV) request granularity. Our approach optimizes\nadvertisers' demands, platform business revenue and user experience and as a\nwhole improves traffic allocation efficiency. We have validated our approach in\nTaobao display advertising system in production. The online A/B test shows our\nalgorithm yields substantially better results than previous fixed bid manner.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 09:51:37 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 03:37:42 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 04:46:51 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 05:06:12 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Zhu", "Han", ""], ["Jin", "Junqi", ""], ["Tan", "Chang", ""], ["Pan", "Fei", ""], ["Zeng", "Yifan", ""], ["Li", "Han", ""], ["Gai", "Kun", ""]]}, {"id": "1703.02102", "submitter": "Yemi Okesanjo", "authors": "Yemi Okesanjo, Victor Kofia", "title": "Revisiting stochastic off-policy action-value gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy stochastic actor-critic methods rely on approximating the\nstochastic policy gradient in order to derive an optimal policy. One may also\nderive the optimal policy by approximating the action-value gradient. The use\nof action-value gradients is desirable as policy improvement occurs along the\ndirection of steepest ascent. This has been studied extensively within the\ncontext of natural gradient actor-critic algorithms and more recently within\nthe context of deterministic policy gradients. In this paper we briefly discuss\nthe off-policy stochastic counterpart to deterministic action-value gradients,\nas well as an incremental approach for following the policy gradient in lieu of\nthe natural gradient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 20:33:24 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 02:25:10 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Okesanjo", "Yemi", ""], ["Kofia", "Victor", ""]]}, {"id": "1703.02111", "submitter": "Duncan Barrack S", "authors": "Duncan Barrack and Simon Preston", "title": "Classification and clustering for observations of event time data using\n  non-homogeneous Poisson process models", "comments": "cleaned up figures and text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of the form of event times arise in various applications. A simple model\nfor such data is a non-homogeneous Poisson process (NHPP) which is specified by\na rate function that depends on time. We consider the problem of having access\nto multiple independent observations of event time data, observed on a common\ninterval, from which we wish to classify or cluster the observations according\nto their rate functions. Each rate function is unknown but assumed to belong to\na finite number of rate functions each defining a distinct class. We model the\nrate functions using a spline basis expansion, the coefficients of which need\nto be estimated from data. The classification approach consists of using\ntraining data for which the class membership is known, to calculate maximum\nlikelihood estimates of the coefficients for each group, then assigning test\nobservations to a group by a maximum likelihood criterion. For clustering, by\nanalogy to the Gaussian mixture model approach for Euclidean data, we consider\nmixtures of NHPP and use the expectation-maximisation algorithm to estimate the\ncoefficients of the rate functions for the component models and group\nmembership probabilities for each observation. The classification and\nclustering approaches perform well on both synthetic and real-world data sets.\nCode associated with this paper is available at\nhttps://github.com/duncan-barrack/NHPP .\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 21:15:01 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 19:11:50 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 20:59:44 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 19:06:03 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Barrack", "Duncan", ""], ["Preston", "Simon", ""]]}, {"id": "1703.02155", "submitter": "Quang N. Tran", "authors": "Ba-Ngu Vo, Dinh Phung, Quang N. Tran, and Ba-Tuong Vo", "title": "Model-Based Multiple Instance Learning", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Multiple Instance (MI) data are point patterns -- sets or multi-sets of\nunordered points -- appropriate statistical point pattern models have not been\nused in MI learning. This article proposes a framework for model-based MI\nlearning using point process theory. Likelihood functions for point pattern\ndata derived from point process theory enable principled yet conceptually\ntransparent extensions of learning tasks, such as classification, novelty\ndetection and clustering, to point pattern data. Furthermore, tractable point\npattern models as well as solutions for learning and decision making from point\npattern data are developed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 00:03:32 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 16:11:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Vo", "Ba-Ngu", ""], ["Phung", "Dinh", ""], ["Tran", "Quang N.", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1703.02156", "submitter": "Jiaming Song", "authors": "Jiaming Song, Russell Stewart, Shengjia Zhao and Stefano Ermon", "title": "On the Limits of Learning Representations with Label-Based Supervision", "comments": "Submitted to ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in neural network based classifiers have transformed automatic\nfeature learning from a pipe dream of stronger AI to a routine and expected\nproperty of practical systems. Since the emergence of AlexNet every winning\nsubmission of the ImageNet challenge has employed end-to-end representation\nlearning, and due to the utility of good representations for transfer learning,\nrepresentation learning has become as an important and distinct task from\nsupervised learning. At present, this distinction is inconsequential, as\nsupervised methods are state-of-the-art in learning transferable\nrepresentations. But recent work has shown that generative models can also be\npowerful agents of representation learning. Will the representations learned\nfrom these generative methods ever rival the quality of those from their\nsupervised competitors? In this work, we argue in the affirmative, that from an\ninformation theoretic perspective, generative models have greater potential for\nrepresentation learning. Based on several experimentally validated assumptions,\nwe show that supervised learning is upper bounded in its capacity for\nrepresentation learning in ways that certain generative models, such as\nGenerative Adversarial Networks (GANs) are not. We hope that our analysis will\nprovide a rigorous motivation for further exploration of generative\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 00:09:31 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Song", "Jiaming", ""], ["Stewart", "Russell", ""], ["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "1703.02184", "submitter": "Xiansheng Guo", "authors": "Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah", "title": "Indoor Localization Using Visible Light Via Fusion Of Multiple\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multiple classifiers fusion localization technique using received signal\nstrengths (RSSs) of visible light is proposed, in which the proposed system\ntransmits different intensity modulated sinusoidal signals by LEDs and the\nsignals received by a Photo Diode (PD) placed at various grid points. First, we\nobtain some {\\emph{approximate}} received signal strengths (RSSs) fingerprints\nby capturing the peaks of power spectral density (PSD) of the received signals\nat each given grid point. Unlike the existing RSSs based algorithms, several\nrepresentative machine learning approaches are adopted to train multiple\nclassifiers based on these RSSs fingerprints. The multiple classifiers\nlocalization estimators outperform the classical RSS-based LED localization\napproaches in accuracy and robustness. To further improve the localization\nperformance, two robust fusion localization algorithms, namely, grid\nindependent least square (GI-LS) and grid dependent least square (GD-LS), are\nproposed to combine the outputs of these classifiers. We also use a singular\nvalue decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical\nstability problem when the prediction matrix is singular. Experiments conducted\non intensity modulated direct detection (IM/DD) systems have demonstrated the\neffectiveness of the proposed algorithms. The experimental results show that\nthe probability of having mean square positioning error (MSPE) of less than 5cm\nachieved by GD-LS is improved by 93.03\\% and 93.15\\%, respectively, as compared\nto those by the RSS ratio (RSSR) and RSS matching methods with the FFT length\nof 2000.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:41:07 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 06:44:32 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Guo", "Xiansheng", ""], ["Shao", "Sihua", ""], ["Ansari", "Nirwan", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1703.02185", "submitter": "Xiansheng Guo", "authors": "Xiansheng Guo, Nirwan Ansari, Huiyong Li", "title": "Indoor Localization by Fusing a Group of Fingerprints Based on Random\n  Forests", "comments": "arXiv admin note: text overlap with arXiv:1609.00661", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indoor localization based on SIngle Of Fingerprint (SIOF) is rather\nsusceptible to the changing environment, multipath, and non-line-of-sight\n(NLOS) propagation. Building SIOF is also a very time-consuming process.\nRecently, we first proposed a GrOup Of Fingerprints (GOOF) to improve the\nlocalization accuracy and reduce the burden of building fingerprints. However,\nthe main drawback is the timeliness. In this paper, we propose a novel\nlocalization framework by Fusing A Group Of fingerprinTs (FAGOT) based on\nrandom forests. In the offline phase, we first build a GOOF from different\ntransformations of the received signals of multiple antennas. Then, we design\nmultiple GOOF strong classifiers based on Random Forests (GOOF-RF) by training\neach fingerprint in the GOOF. In the online phase, we input the corresponding\ntransformations of the real measurements into these strong classifiers to\nobtain multiple independent decisions. Finally, we propose a Sliding Window\naIded Mode-based (SWIM) fusion algorithm to balance the localization accuracy\nand time. Our proposed approaches can work better in an unknown indoor\nscenario. The burden of building fingerprints can also be reduced drastically.\nWe demonstrate the performance of our algorithms through simulations and real\nexperimental data using two Universal Software Radio Peripheral (USRP)\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:41:40 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Guo", "Xiansheng", ""], ["Ansari", "Nirwan", ""], ["Li", "Huiyong", ""]]}, {"id": "1703.02205", "submitter": "Szu-Wei Fu", "authors": "Szu-Wei Fu, Yu Tsao, Xugang Lu, Hisashi Kawai", "title": "Raw Waveform-based Speech Enhancement by Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a fully convolutional network (FCN) model for raw\nwaveform-based speech enhancement. The proposed system performs speech\nenhancement in an end-to-end (i.e., waveform-in and waveform-out) manner, which\ndif-fers from most existing denoising methods that process the magnitude\nspectrum (e.g., log power spectrum (LPS)) only. Because the fully connected\nlayers, which are involved in deep neural networks (DNN) and convolutional\nneural networks (CNN), may not accurately characterize the local information of\nspeech signals, particularly with high frequency components, we employed fully\nconvolutional layers to model the waveform. More specifically, FCN consists of\nonly convolutional layers and thus the local temporal structures of speech\nsignals can be efficiently and effectively preserved with relatively few\nweights. Experimental results show that DNN- and CNN-based models have limited\ncapability to restore high frequency components of waveforms, thus leading to\ndecreased intelligibility of enhanced speech. By contrast, the proposed FCN\nmodel can not only effectively recover the waveforms but also outperform the\nLPS-based DNN baseline in terms of short-time objective intelligibility (STOI)\nand perceptual evaluation of speech quality (PESQ). In addition, the number of\nmodel parameters in FCN is approximately only 0.2% compared with that in both\nDNN and CNN.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 04:03:27 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 09:51:36 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 11:10:07 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Fu", "Szu-Wei", ""], ["Tsao", "Yu", ""], ["Lu", "Xugang", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1703.02236", "submitter": "Cheng Ju", "authors": "Cheng Ju, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard\n  Wyss, Sebastian Schneeweiss, Mark J. van der Laan", "title": "Propensity score prediction for electronic healthcare databases using\n  Super Learner and High-dimensional Propensity Score Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal learner for prediction modeling varies depending on the\nunderlying data-generating distribution. Super Learner (SL) is a generic\nensemble learning algorithm that uses cross-validation to select among a\n\"library\" of candidate prediction models. The SL is not restricted to a single\nprediction model, but uses the strengths of a variety of learning algorithms to\nadapt to different databases. While the SL has been shown to perform well in a\nnumber of settings, it has not been thoroughly evaluated in large electronic\nhealthcare databases that are common in pharmacoepidemiology and comparative\neffectiveness research. In this study, we applied and evaluated the performance\nof the SL in its ability to predict treatment assignment using three electronic\nhealthcare databases. We considered a library of algorithms that consisted of\nboth nonparametric and parametric models. We also considered a novel strategy\nfor prediction modeling that combines the SL with the high-dimensional\npropensity score (hdPS) variable selection algorithm. Predictive performance\nwas assessed using three metrics: the negative log-likelihood, area under the\ncurve (AUC), and time complexity. Results showed that the best individual\nalgorithm, in terms of predictive performance, varied across datasets. The SL\nwas able to adapt to the given dataset and optimize predictive performance\nrelative to any individual learner. Combining the SL with the hdPS was the most\nconsistent prediction method and may be promising for PS estimation and\nprediction modeling in electronic healthcare databases.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:38:02 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 18:52:48 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ju", "Cheng", ""], ["Combs", "Mary", ""], ["Lendle", "Samuel D", ""], ["Franklin", "Jessica M", ""], ["Wyss", "Richard", ""], ["Schneeweiss", "Sebastian", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1703.02310", "submitter": "Shirli Di-Castro Shashua", "authors": "Shirli Di-Castro Shashua, Shie Mannor", "title": "Deep Robust Kalman Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Robust Markov Decision Process (RMDP) is a sequential decision making model\nthat accounts for uncertainty in the parameters of dynamic systems. This\nuncertainty introduces difficulties in learning an optimal policy, especially\nfor environments with large state spaces. We propose two algorithms, RTD-DQN\nand Deep-RoK, for solving large-scale RMDPs using nonlinear approximation\nschemes such as deep neural networks. The RTD-DQN algorithm incorporates the\nrobust Bellman temporal difference error into a robust loss function, yielding\nrobust policies for the agent. The Deep-RoK algorithm is a robust Bayesian\nmethod, based on the Extended Kalman Filter (EKF), that accounts for both the\nuncertainty in the weights of the approximated value function and the\nuncertainty in the transition probabilities, improving the robustness of the\nagent. We provide theoretical results for our approach and test the proposed\nalgorithms on a continuous state domain.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:16:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Shashua", "Shirli Di-Castro", ""], ["Mannor", "Shie", ""]]}, {"id": "1703.02317", "submitter": "Emre Cakir", "authors": "Emre\\c{C}ak{\\i}r, Sharath Adavanne, Giambattista Parascandolo,\n  Konstantinos Drossos, Tuomas Virtanen", "title": "Convolutional Recurrent Neural Networks for Bird Audio Detection", "comments": "Submitted to EUSIPCO 2017 Special Session on Bird Audio Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird sounds possess distinctive spectral structure which may exhibit small\nshifts in spectrum depending on the bird species and environmental conditions.\nIn this paper, we propose using convolutional recurrent neural networks on the\ntask of automated bird audio detection in real-life environments. In the\nproposed method, convolutional layers extract high dimensional, local frequency\nshift invariant features, while recurrent layers capture longer term\ndependencies between the features extracted from short time frames. This method\nachieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data\nand obtains the second place in the Bird Audio Detection challenge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:36:30 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Emre\u00c7ak\u0131r", "", ""], ["Adavanne", "Sharath", ""], ["Parascandolo", "Giambattista", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1703.02379", "submitter": "Christopher Morris", "authors": "Christopher Morris, Kristian Kersting, Petra Mutzel", "title": "Global Weisfeiler-Lehman Graph Kernels", "comments": "10 pages, accepted at IEEE ICDM 2017 (\"Glocalized Weisfeiler-Lehman\n  Graph Kernels: Global-Local Feature Maps of Graphs\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art graph kernels only take local graph properties into\naccount, i.e., the kernel is computed with regard to properties of the\nneighborhood of vertices or other small substructures. On the other hand,\nkernels that do take global graph propertiesinto account may not scale well to\nlarge graph databases. Here we propose to start exploring the space between\nlocal and global graph kernels, striking the balance between both worlds.\nSpecifically, we introduce a novel graph kernel based on the $k$-dimensional\nWeisfeiler-Lehman algorithm. Unfortunately, the $k$-dimensional\nWeisfeiler-Lehman algorithm scales exponentially in $k$. Consequently, we\ndevise a stochastic version of the kernel with provable approximation\nguarantees using conditional Rademacher averages. On bounded-degree graphs, it\ncan even be computed in constant time. We support our theoretical results with\nexperiments on several graph classification benchmarks, showing that our\nkernels often outperform the state-of-the-art in terms of classification\naccuracies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 13:57:55 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 16:46:20 GMT"}, {"version": "v3", "created": "Fri, 22 Sep 2017 13:12:36 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Morris", "Christopher", ""], ["Kersting", "Kristian", ""], ["Mutzel", "Petra", ""]]}, {"id": "1703.02391", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo,\n  Li-Jia Li", "title": "Learning from Noisy Labels with Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of learning from noisy labels is very useful in many visual\nrecognition tasks, as a vast amount of data with noisy labels are relatively\neasy to obtain. Traditionally, the label noises have been treated as\nstatistical outliers, and approaches such as importance re-weighting and\nbootstrap have been proposed to alleviate the problem. According to our\nobservation, the real-world noisy labels exhibit multi-mode characteristics as\nthe true labels, rather than behaving like independent random outliers. In this\nwork, we propose a unified distillation framework to use side information,\nincluding a small clean dataset and label relations in knowledge graph, to\n\"hedge the risk\" of learning from noisy labels. Furthermore, unlike the\ntraditional approaches evaluated based on simulated label noises, we propose a\nsuite of new benchmark datasets, in Sports, Species and Artifacts domains, to\nevaluate the task of learning from noisy labels in the practical setting. The\nempirical study demonstrates the effectiveness of our proposed method in all\nthe domains.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 14:15:14 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 07:21:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Li", "Yuncheng", ""], ["Yang", "Jianchao", ""], ["Song", "Yale", ""], ["Cao", "Liangliang", ""], ["Luo", "Jiebo", ""], ["Li", "Li-Jia", ""]]}, {"id": "1703.02403", "submitter": "Anton Osokin", "authors": "Anton Osokin, Francis Bach, Simon Lacoste-Julien", "title": "On Structured Prediction Theory with Calibrated Convex Surrogate Losses", "comments": "Appears in: Advances in Neural Information Processing Systems 30\n  (NIPS 2017). 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel theoretical insights on structured prediction in the context\nof efficient convex surrogate loss minimization with consistency guarantees.\nFor any task loss, we construct a convex surrogate that can be optimized via\nstochastic gradient descent and we prove tight bounds on the so-called\n\"calibration function\" relating the excess surrogate risk to the actual risk.\nIn contrast to prior related work, we carefully monitor the effect of the\nexponential number of classes in the learning guarantees as well as on the\noptimization complexity. As an interesting consequence, we formalize the\nintuition that some task losses make learning harder than others, and that the\nclassical 0-1 loss is ill-suited for general structured prediction.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 14:39:15 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 11:16:05 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 13:38:49 GMT"}, {"version": "v4", "created": "Mon, 29 Jan 2018 08:25:28 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Osokin", "Anton", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1703.02433", "submitter": "Bilal Farooq", "authors": "Isma\\\"il Saadi, Melvin Wong, Bilal Farooq, Jacques Teller, Mario Cools", "title": "An investigation into machine learning approaches for forecasting\n  spatio-temporal demand in ride-hailing service", "comments": "Currently under review for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present machine learning approaches for characterizing and\nforecasting the short-term demand for on-demand ride-hailing services. We\npropose the spatio-temporal estimation of the demand that is a function of\nvariable effects related to traffic, pricing and weather conditions. With\nrespect to the methodology, a single decision tree, bootstrap-aggregated\n(bagged) decision trees, random forest, boosted decision trees, and artificial\nneural network for regression have been adapted and systematically compared\nusing various statistics, e.g. R-square, Root Mean Square Error (RMSE), and\nslope. To better assess the quality of the models, they have been tested on a\nreal case study using the data of DiDi Chuxing, the main on-demand ride hailing\nservice provider in China. In the current study, 199,584 time-slots describing\nthe spatio-temporal ride-hailing demand has been extracted with an\naggregated-time interval of 10 mins. All the methods are trained and validated\non the basis of two independent samples from this dataset. The results revealed\nthat boosted decision trees provide the best prediction accuracy (RMSE=16.41),\nwhile avoiding the risk of over-fitting, followed by artificial neural network\n(20.09), random forest (23.50), bagged decision trees (24.29) and single\ndecision tree (33.55).\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:26:38 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Saadi", "Isma\u00efl", ""], ["Wong", "Melvin", ""], ["Farooq", "Bilal", ""], ["Teller", "Jacques", ""], ["Cools", "Mario", ""]]}, {"id": "1703.02435", "submitter": "Sebastian Johann Wetzel", "authors": "Sebastian Johann Wetzel", "title": "Unsupervised learning of phase transitions: from principal component\n  analysis to variational autoencoders", "comments": "corrected typos", "journal-ref": "Phys. Rev. E 96, 022140 (2017)", "doi": "10.1103/PhysRevE.96.022140", "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ unsupervised machine learning techniques to learn latent parameters\nwhich best describe states of the two-dimensional Ising model and the\nthree-dimensional XY model. These methods range from principal component\nanalysis to artificial neural network based variational autoencoders. The\nstates are sampled using a Monte-Carlo simulation above and below the critical\ntemperature. We find that the predicted latent parameters correspond to the\nknown order parameters. The latent representation of the states of the models\nin question are clustered, which makes it possible to identify phases without\nprior knowledge of their existence or the underlying Hamiltonian. Furthermore,\nwe find that the reconstruction loss function can be used as a universal\nidentifier for phase transitions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:34:12 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 23:44:34 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Wetzel", "Sebastian Johann", ""]]}, {"id": "1703.02492", "submitter": "Thiernithi Variddhisa\\\"i", "authors": "Thiernithi Variddhisai and Danilo Mandic", "title": "Online Multilinear Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for online tensor dictionary learning is proposed. With the\nassumption of separable dictionaries, tensor contraction is used to diminish a\n$N$-way model of $\\mathcal{O}\\left(L^N\\right)$ into a simple matrix equation of\n$\\mathcal{O}\\left(NL^2\\right)$ with a real-time capability. To avoid numerical\ninstability due to inversion of sparse matrix, a class of stochastic gradient\nwith memory is formulated via a least-square solution to guarantee convergence\nand robustness. Both gradient descent with exact line search and Newton's\nmethod are discussed and realized. Extensions onto how to deal with bad\ninitialization and outliers are also explained in detail. Experiments on two\nsynthetic signals confirms an impressive performance of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 17:52:13 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 22:48:17 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 14:27:40 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2020 14:38:11 GMT"}, {"version": "v5", "created": "Tue, 10 Mar 2020 12:45:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Variddhisai", "Thiernithi", ""], ["Mandic", "Danilo", ""]]}, {"id": "1703.02518", "submitter": "Martin Jaggi", "authors": "Dmytro Perekrestenko, Volkan Cevher, Martin Jaggi", "title": "Faster Coordinate Descent via Adaptive Importance Sampling", "comments": "appearing at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coordinate descent methods employ random partial updates of decision\nvariables in order to solve huge-scale convex optimization problems. In this\nwork, we introduce new adaptive rules for the random selection of their\nupdates. By adaptive, we mean that our selection rules are based on the dual\nresidual or the primal-dual gap estimates and can change at each iteration. We\ntheoretically characterize the performance of our selection rules and\ndemonstrate improvements over the state-of-the-art, and extend our theory and\nalgorithms to general convex objectives. Numerical evidence with hinge-loss\nsupport vector machines and Lasso confirm that the practice follows the theory.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:36:55 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Perekrestenko", "Dmytro", ""], ["Cevher", "Volkan", ""], ["Jaggi", "Martin", ""]]}, {"id": "1703.02527", "submitter": "Branislav Kveton", "authors": "Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton,\n  Csaba Szepesvari, and Zheng Wen", "title": "Online Learning to Rank in Stochastic Click Models", "comments": "Proceedings of the 34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning to rank is a core problem in information retrieval and\nmachine learning. Many provably efficient algorithms have been recently\nproposed for this problem in specific click models. The click model is a model\nof how the user interacts with a list of documents. Though these results are\nsignificant, their impact on practice is limited, because all proposed\nalgorithms are designed for specific click models and lack convergence\nguarantees in other models. In this work, we propose BatchRank, the first\nonline learning to rank algorithm for a broad class of click models. The class\nencompasses two most fundamental click models, the cascade and position-based\nmodels. We derive a gap-dependent upper bound on the $T$-step regret of\nBatchRank and evaluate it on a range of web search queries. We observe that\nBatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an\nexisting algorithm for the cascade model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:53:58 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 07:13:15 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Zoghi", "Masrour", ""], ["Tunys", "Tomas", ""], ["Ghavamzadeh", "Mohammad", ""], ["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Wen", "Zheng", ""]]}, {"id": "1703.02528", "submitter": "Samuel Albanie", "authors": "Samuel Albanie, S\\'ebastien Ehrhardt, Jo\\~ao F. Henriques", "title": "Stopping GAN Violence: Generative Unadversarial Networks", "comments": "Under review as a conference paper at SIGBOVIK 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the costs of human violence have attracted a great deal of attention\nfrom the research community, the effects of the network-on-network (NoN)\nviolence popularised by Generative Adversarial Networks have yet to be\naddressed. In this work, we quantify the financial, social, spiritual,\ncultural, grammatical and dermatological impact of this aggression and address\nthe issue by proposing a more peaceful approach which we term Generative\nUnadversarial Networks (GUNs). Under this framework, we simultaneously train\ntwo models: a generator G that does its best to capture whichever data\ndistribution it feels it can manage, and a motivator M that helps G to achieve\nits dream. Fighting is strictly verboten and both models evolve by learning to\nrespect their differences. The framework is both theoretically and electrically\ngrounded in game theory, and can be viewed as a winner-shares-all two-player\ngame in which both players work as a team to achieve the best score.\nExperiments show that by working in harmony, the proposed model is able to\nclaim both the moral and log-likelihood high ground. Our work builds on a rich\nhistory of carefully argued position-papers, published as anonymous YouTube\ncomments, which prove that the optimal solution to NoN violence is more GUNs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:54:04 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Albanie", "Samuel", ""], ["Ehrhardt", "S\u00e9bastien", ""], ["Henriques", "Jo\u00e3o F.", ""]]}, {"id": "1703.02570", "submitter": "Amina Mollaysa", "authors": "Amina Mollaysa, Pablo Strasser, Alexandros Kalousis", "title": "Regularising Non-linear Models Using Feature Side-information", "comments": "11 page with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Very often features come with their own vectorial descriptions which provide\ndetailed information about their properties. We refer to these vectorial\ndescriptions as feature side-information. In the standard learning scenario,\ninput is represented as a vector of features and the feature side-information\nis most often ignored or used only for feature selection prior to model\nfitting. We believe that feature side-information which carries information\nabout features intrinsic property will help improve model prediction if used in\na proper way during learning process. In this paper, we propose a framework\nthat allows for the incorporation of the feature side-information during the\nlearning of very general model families to improve the prediction performance.\nWe control the structures of the learned models so that they reflect features\nsimilarities as these are defined on the basis of the side-information. We\nperform experiments on a number of benchmark datasets which show significant\npredictive performance gains, over a number of baselines, as a result of the\nexploitation of the side-information.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 19:47:22 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Mollaysa", "Amina", ""], ["Strasser", "Pablo", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1703.02596", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, Angelo Cardoso, C.H. Bryan Liu, Roberto\n  Pagliari, Marc Peter Deisenroth", "title": "Customer Lifetime Value Prediction Using Embeddings", "comments": "10 pages, 11 figures", "journal-ref": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining Pages 1753-1762, 2017", "doi": "10.1145/3097983.3098123", "report-no": null, "categories": "cs.LG cs.CY cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Customer LifeTime Value (CLTV) prediction system deployed at\nASOS.com, a global online fashion retailer. CLTV prediction is an important\nproblem in e-commerce where an accurate estimate of future value allows\nretailers to effectively allocate marketing spend, identify and nurture high\nvalue customers and mitigate exposure to losses. The system at ASOS provides\ndaily estimates of the future value of every customer and is one of the\ncornerstones of the personalised shopping experience. The state of the art in\nthis domain uses large numbers of handcrafted features and ensemble regressors\nto forecast value, predict churn and evaluate customer loyalty. Recently,\ndomains including language, vision and speech have shown dramatic advances by\nreplacing handcrafted features with features that are learned automatically\nfrom data. We detail the system deployed at ASOS and show that learning feature\nrepresentations is a promising extension to the state of the art in CLTV\nmodelling. We propose a novel way to generate embeddings of customers, which\naddresses the issue of the ever changing product catalogue and obtain a\nsignificant improvement over an exhaustive set of handcrafted features.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 21:18:11 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:20:06 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 16:40:44 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Cardoso", "Angelo", ""], ["Liu", "C. H. Bryan", ""], ["Pagliari", "Roberto", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1703.02622", "submitter": "Ashok Cutkosky", "authors": "Ashok Cutkosky and Kwabena Boahen", "title": "Online Convex Optimization with Unconstrained Domains and Losses", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 29 (2016)\n  748-756", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online convex optimization algorithm (RescaledExp) that\nachieves optimal regret in the unconstrained setting without prior knowledge of\nany bounds on the loss functions. We prove a lower bound showing an exponential\nseparation between the regret of existing algorithms that require a known bound\non the loss functions and any algorithm that does not require such knowledge.\nRescaledExp matches this lower bound asymptotically in the number of\niterations. RescaledExp is naturally hyperparameter-free and we demonstrate\nempirically that it matches prior optimization algorithms that require\nhyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:14:53 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Cutkosky", "Ashok", ""], ["Boahen", "Kwabena", ""]]}, {"id": "1703.02624", "submitter": "Lin Xiao", "authors": "Jialei Wang and Lin Xiao", "title": "Exploiting Strong Convexity from Data with Primal-Dual First-Order\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-13", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider empirical risk minimization of linear predictors with convex loss\nfunctions. Such problems can be reformulated as convex-concave saddle point\nproblems, and thus are well suitable for primal-dual first-order algorithms.\nHowever, primal-dual algorithms often require explicit strongly convex\nregularization in order to obtain fast linear convergence, and the required\ndual proximal mapping may not admit closed-form or efficient solution. In this\npaper, we develop both batch and randomized primal-dual algorithms that can\nexploit strong convexity from data adaptively and are capable of achieving\nlinear convergence even without regularization. We also present dual-free\nvariants of the adaptive primal-dual algorithms that do not require computing\nthe dual proximal mapping, which are especially suitable for logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:17:17 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Wang", "Jialei", ""], ["Xiao", "Lin", ""]]}, {"id": "1703.02628", "submitter": "C\\'edric Malherbe", "authors": "C\\'edric Malherbe and Nicolas Vayatis", "title": "Global optimization of Lipschitz functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the paper is to design sequential strategies which lead to\nefficient optimization of an unknown function under the only assumption that it\nhas a finite Lipschitz constant. We first identify sufficient conditions for\nthe consistency of generic sequential algorithms and formulate the expected\nminimax rate for their performance. We introduce and analyze a first algorithm\ncalled LIPO which assumes the Lipschitz constant to be known. Consistency,\nminimax rates for LIPO are proved, as well as fast rates under an additional\nH\\\"older like condition. An adaptive version of LIPO is also introduced for the\nmore realistic setup where the Lipschitz constant is unknown and has to be\nestimated along with the optimization. Similar theoretical guarantees are shown\nto hold for the adaptive LIPO algorithm and a numerical assessment is provided\nat the end of the paper to illustrate the potential of this strategy with\nrespect to state-of-the-art methods over typical benchmark problems for global\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:29:54 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 14:33:52 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 19:09:35 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Malherbe", "C\u00e9dric", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1703.02629", "submitter": "Ashok Cutkosky", "authors": "Ashok Cutkosky and Kwabena Boahen", "title": "Online Learning Without Prior Information", "comments": "12 pages main text; 35 pages total; COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of optimization and online learning algorithms today\nrequire some prior information about the data (often in the form of bounds on\ngradients or on the optimal parameter value). When this information is not\navailable, these algorithms require laborious manual tuning of various\nhyperparameters, motivating the search for algorithms that can adapt to the\ndata with no prior information. We describe a frontier of new lower bounds on\nthe performance of such algorithms, reflecting a tradeoff between a term that\ndepends on the optimal parameter value and a term that depends on the\ngradients' rate of growth. Further, we construct a family of algorithms whose\nperformance matches any desired point on this frontier, which no previous\nalgorithm reaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:32:06 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 01:29:10 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Cutkosky", "Ashok", ""], ["Boahen", "Kwabena", ""]]}, {"id": "1703.02641", "submitter": "Frederic Sala", "authors": "Frederic Sala, Shahroze Kabir, Guy Van den Broeck, and Lara Dolecek", "title": "Don't Fear the Bit Flips: Optimized Coding Strategies for Binary\n  Classification", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After being trained, classifiers must often operate on data that has been\ncorrupted by noise. In this paper, we consider the impact of such noise on the\nfeatures of binary classifiers. Inspired by tools for classifier robustness, we\nintroduce the same classification probability (SCP) to measure the resulting\ndistortion on the classifier outputs. We introduce a low-complexity estimate of\nthe SCP based on quantization and polynomial multiplication. We also study\nchannel coding techniques based on replication error-correcting codes. In\ncontrast to the traditional channel coding approach, where error-correction is\nmeant to preserve the data and is agnostic to the application, our schemes\nspecifically aim to maximize the SCP (equivalently minimizing the distortion of\nthe classifier output) for the same redundancy overhead.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 00:04:01 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Sala", "Frederic", ""], ["Kabir", "Shahroze", ""], ["Broeck", "Guy Van den", ""], ["Dolecek", "Lara", ""]]}, {"id": "1703.02645", "submitter": "Murat Kocaoglu", "authors": "Murat Kocaoglu, Alexandros G. Dimakis and Sriram Vishwanath", "title": "Cost-Optimal Learning of Causal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a causal graph over a set of variables\nwith interventions. We study the cost-optimal causal graph learning problem:\nFor a given skeleton (undirected version of the causal graph), design the set\nof interventions with minimum total cost, that can uniquely identify any causal\ngraph with the given skeleton. We show that this problem is solvable in\npolynomial time. Later, we consider the case when the number of interventions\nis limited. For this case, we provide polynomial time algorithms when the\nskeleton is a tree or a clique tree. For a general chordal skeleton, we develop\nan efficient greedy algorithm, which can be improved when the causal graph\nskeleton is an interval graph.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 00:15:54 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1703.02647", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi", "title": "Streaming Weak Submodularity: Interpreting Neural Networks on the Fly", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, it is important to explain the\npredictions of a black-box classifier. For example, why does a deep neural\nnetwork assign an image to a particular class? We cast interpretability of\nblack-box classifiers as a combinatorial maximization problem and propose an\nefficient streaming algorithm to solve it subject to cardinality constraints.\nBy extending ideas from Badanidiyuru et al. [2014], we provide a constant\nfactor approximation guarantee for our algorithm in the case of random stream\norder and a weakly submodular objective function. This is the first such\ntheoretical guarantee for this general class of functions, and we also show\nthat no such algorithm exists for a worst case stream order. Our algorithm\nobtains similar explanations of Inception V3 predictions $10$ times faster than\nthe state-of-the-art LIME framework of Ribeiro et al. [2016].\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 00:31:30 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 20:17:54 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 19:18:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Dimakis", "Alexandros G.", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "1703.02662", "submitter": "Tomo Miyazaki", "authors": "Tomo Miyazaki, Shinichiro Omachi", "title": "Structural Data Recognition with Graph Model Boosting", "comments": "8 pages", "journal-ref": "IEEE Access, 2018", "doi": "10.1109/ACCESS.2018.2876860", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for structural data recognition using a\nlarge number of graph models. In general, prevalent methods for structural data\nrecognition have two shortcomings: 1) Only a single model is used to capture\nstructural variation. 2) Naive recognition methods are used, such as the\nnearest neighbor method. In this paper, we propose strengthening the\nrecognition performance of these models as well as their ability to capture\nstructural variation. The proposed method constructs a large number of graph\nmodels and trains decision trees using the models. This paper makes two main\ncontributions. The first is a novel graph model that can quickly perform\ncalculations, which allows us to construct several models in a feasible amount\nof time. The second contribution is a novel approach to structural data\nrecognition: graph model boosting. Comprehensive structural variations can be\ncaptured with a large number of graph models constructed in a boosting\nframework, and a sophisticated classifier can be formed by aggregating the\ndecision trees. Consequently, we can carry out structural data recognition with\npowerful recognition capability in the face of comprehensive structural\nvariation. The experiments shows that the proposed method achieves impressive\nresults and outperforms existing methods on datasets of IAM graph database\nrepository.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 01:45:54 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Miyazaki", "Tomo", ""], ["Omachi", "Shinichiro", ""]]}, {"id": "1703.02674", "submitter": "Chengtao Li", "authors": "Chengtao Li, Stefanie Jegelka and Suvrit Sra", "title": "Polynomial Time Algorithms for Dual Volume Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dual volume sampling, a method for selecting k columns from an n x m\nshort and wide matrix (n <= k <= m) such that the probability of selection is\nproportional to the volume spanned by the rows of the induced submatrix. This\nmethod was proposed by Avron and Boutsidis (2013), who showed it to be a\npromising method for column subset selection and its multiple applications.\nHowever, its wider adoption has been hampered by the lack of polynomial time\nsampling algorithms. We remove this hindrance by developing an exact\n(randomized) polynomial time sampling algorithm as well as its derandomization.\nThereafter, we study dual volume sampling via the theory of real stable\npolynomials and prove that its distribution satisfies the \"Strong Rayleigh\"\nproperty. This result has numerous consequences, including a provably\nfast-mixing Markov chain sampler that makes dual volume sampling much more\nattractive to practitioners. This sampler is closely related to classical\nalgorithms for popular experimental design methods that are to date lacking\ntheoretical analysis but are known to empirically work well.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 02:22:30 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 04:07:18 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 00:58:36 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1703.02679", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Matt Barnes, Willie Neiswanger", "title": "Performance Bounds for Graphical Record Linkage", "comments": "11 pages with supplement; 4 figures and 2 tables; to appear in\n  AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage involves merging records in large, noisy databases to remove\nduplicate entities. It has become an important area because of its widespread\noccurrence in bibliometrics, public health, official statistics production,\npolitical science, and beyond. Traditional linkage methods directly linking\nrecords to one another are computationally infeasible as the number of records\ngrows. As a result, it is increasingly common for researchers to treat record\nlinkage as a clustering task, in which each latent entity is associated with\none or more noisy database records. We critically assess performance bounds\nusing the Kullback-Leibler (KL) divergence under a Bayesian record linkage\nframework, making connections to Kolchin partition models. We provide an upper\nbound using the KL divergence and a lower bound on the minimum probability of\nmisclassifying a latent entity. We give insights for when our bounds hold using\nsimulated data and provide practical user guidance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:07:37 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Barnes", "Matt", ""], ["Neiswanger", "Willie", ""]]}, {"id": "1703.02682", "submitter": "Murat Kocaoglu", "authors": "Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis and Sujay\n  Sanghavi", "title": "Sparse Quadratic Logistic Regression in Sub-quadratic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider support recovery in the quadratic logistic regression setting -\nwhere the target depends on both p linear terms $x_i$ and up to $p^2$ quadratic\nterms $x_i x_j$. Quadratic terms enable prediction/modeling of higher-order\neffects between features and the target, but when incorporated naively may\ninvolve solving a very large regression problem. We consider the sparse case,\nwhere at most $s$ terms (linear or quadratic) are non-zero, and provide a new\nfaster algorithm. It involves (a) identifying the weak support (i.e. all\nrelevant variables) and (b) standard logistic regression optimization only on\nthese chosen variables. The first step relies on a novel insight about\ncorrelation tests in the presence of non-linearity, and takes $O(pn)$ time for\n$n$ samples - giving potentially huge computational gains over the naive\napproach. Motivated by insights from the boolean case, we propose a non-linear\ncorrelation test for non-binary finite support case that involves hashing a\nvariable and then correlating with the output variable. We also provide\nexperimental results to demonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:21:08 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Shanmugam", "Karthikeyan", ""], ["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1703.02689", "submitter": "Erik Lindgren", "authors": "Erik M. Lindgren, Alexandros G. Dimakis, Adam Klivans", "title": "Exact MAP Inference by Avoiding Fractional Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graphical model, one essential problem is MAP inference, that is,\nfinding the most likely configuration of states according to the model.\nAlthough this problem is NP-hard, large instances can be solved in practice. A\nmajor open question is to explain why this is true. We give a natural condition\nunder which we can provably perform MAP inference in polynomial time. We\nrequire that the number of fractional vertices in the LP relaxation exceeding\nthe optimal solution is bounded by a polynomial in the problem size. This\nresolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for\ngeneral LP relaxations of integer programs, known techniques can only handle a\nconstant number of fractional vertices whose value exceeds the optimal\nsolution. We experimentally verify this condition and demonstrate how efficient\nvarious integer programming methods are at removing fractional solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:55:27 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lindgren", "Erik M.", ""], ["Dimakis", "Alexandros G.", ""], ["Klivans", "Adam", ""]]}, {"id": "1703.02690", "submitter": "Erik Lindgren", "authors": "Erik M. Lindgren, Shanshan Wu, Alexandros G. Dimakis", "title": "Leveraging Sparsity for Efficient Submodular Data Summarization", "comments": "In NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The facility location problem is widely used for summarizing large datasets\nand has additional applications in sensor placement, image retrieval, and\nclustering. One difficulty of this problem is that submodular optimization\nalgorithms require the calculation of pairwise benefits for all items in the\ndataset. This is infeasible for large problems, so recent work proposed to only\ncalculate nearest neighbor benefits. One limitation is that several strong\nassumptions were invoked to obtain provable approximation guarantees. In this\npaper we establish that these extra assumptions are not necessary---solving the\nsparsified problem will be almost optimal under the standard assumptions of the\nproblem. We then analyze a different method of sparsification that is a better\nmodel for methods such as Locality Sensitive Hashing to accelerate the nearest\nneighbor computations and extend the use of the problem to a broader family of\nsimilarities. We validate our approach by demonstrating that it rapidly\ngenerates interpretable summaries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:56:27 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Lindgren", "Erik M.", ""], ["Wu", "Shanshan", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1703.02721", "submitter": "Rajiv Khanna", "authors": "Rajiv Khanna, Ethan Elenberg, Alexandros G. Dimakis, Sahand Negahban", "title": "On Approximation Guarantees for Greedy Low Rank Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new approximation guarantees for greedy low rank matrix estimation\nunder standard assumptions of restricted strong convexity and smoothness. Our\nnovel analysis also uncovers previously unknown connections between the low\nrank estimation and combinatorial optimization, so much so that our bounds are\nreminiscent of corresponding approximation bounds in submodular maximization.\nAdditionally, we also provide statistical recovery guarantees. Finally, we\npresent empirical comparison of greedy estimation with established baselines on\ntwo important real-world problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:20:10 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Khanna", "Rajiv", ""], ["Elenberg", "Ethan", ""], ["Dimakis", "Alexandros G.", ""], ["Negahban", "Sahand", ""]]}, {"id": "1703.02723", "submitter": "Rajiv Khanna", "authors": "Rajiv Khanna, Ethan Elenberg, Alexandros G. Dimakis, Sahand Negahban,\n  Joydeep Ghosh", "title": "Scalable Greedy Feature Selection via Weak Submodularity", "comments": "To appear in AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy algorithms are widely used for problems in machine learning such as\nfeature selection and set function optimization. Unfortunately, for large\ndatasets, the running time of even greedy algorithms can be quite high. This is\nbecause for each greedy step we need to refit a model or calculate a function\nusing the previously selected choices and the new candidate.\n  Two algorithms that are faster approximations to the greedy forward selection\nwere introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve\nbetter performance by exploiting distributed computation and stochastic\nevaluation respectively. Both algorithms have provable performance guarantees\nfor submodular functions.\n  In this paper we show that divergent from previously held opinion,\nsubmodularity is not required to obtain approximation guarantees for these two\nalgorithms. Specifically, we show that a generalized concept of weak\nsubmodularity suffices to give multiplicative approximation guarantees. Our\nresult extends the applicability of these algorithms to a larger class of\nfunctions. Furthermore, we show that a bounded submodularity ratio can be used\nto provide data dependent bounds that can sometimes be tighter also for\nsubmodular functions. We empirically validate our work by showing superior\nperformance of fast greedy approximations versus several established baselines\non artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:21:46 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Khanna", "Rajiv", ""], ["Elenberg", "Ethan", ""], ["Dimakis", "Alexandros G.", ""], ["Negahban", "Sahand", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1703.02724", "submitter": "Anru Zhang", "authors": "Anru Zhang and Dong Xia", "title": "Tensor SVD: Statistical and Computational Limits", "comments": "Typos fixed", "journal-ref": null, "doi": null, "report-no": "IEEE Transactions on Information Theory 64 (11), 7311-7338", "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a general framework for tensor singular value\ndecomposition (tensor SVD), which focuses on the methodology and theory for\nextracting the hidden low-rank structure from high-dimensional tensor data.\nComprehensive results are developed on both the statistical and computational\nlimits for tensor SVD. This problem exhibits three different phases according\nto the signal-to-noise ratio (SNR). In particular, with strong SNR, we show\nthat the classical higher-order orthogonal iteration achieves the minimax\noptimal rate of convergence in estimation; with weak SNR, the\ninformation-theoretical lower bound implies that it is impossible to have\nconsistent estimation in general; with moderate SNR, we show that the\nnon-convex maximum likelihood estimation provides optimal solution, but with\nNP-hard computational cost; moreover, under the hardness hypothesis of\nhypergraphic planted clique detection, there are no polynomial-time algorithms\nperforming consistently in general.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:22:56 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 18:54:51 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 17:29:56 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 12:35:34 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Anru", ""], ["Xia", "Dong", ""]]}, {"id": "1703.02757", "submitter": "El Mahdi El Mhamdi", "authors": "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer", "title": "Byzantine-Tolerant Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of data, the need for scalability and the complexity of models\nused in modern machine learning calls for distributed implementations. Yet, as\nof today, distributed machine learning frameworks have largely ignored the\npossibility of arbitrary (i.e., Byzantine) failures. In this paper, we study\nthe robustness to Byzantine failures at the fundamental level of stochastic\ngradient descent (SGD), the heart of most machine learning algorithms. Assuming\na set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can\nSGD be, without limiting the dimension, nor the size of the parameter space.\n  We first show that no gradient descent update rule based on a linear\ncombination of the vectors proposed by the workers (i.e, current approaches)\ntolerates a single Byzantine failure. We then formulate a resilience property\nof the update rule capturing the basic requirements to guarantee convergence\ndespite $f$ Byzantine workers. We finally propose Krum, an update rule that\nsatisfies the resilience property aforementioned. For a $d$-dimensional\nlearning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 09:26:36 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Blanchard", "Peva", ""], ["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Stainer", "Julien", ""]]}, {"id": "1703.02819", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov", "title": "Introduction to Formal Concept Analysis and Its Applications in\n  Information Retrieval and Related Fields", "comments": null, "journal-ref": "RuSSIR 2014, Nizhniy Novgorod, Russia, CCIS vol. 505, Springer\n  42-141", "doi": "10.1007/978-3-319-25485-2_3", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial on Formal Concept Analysis (FCA) and its\napplications. FCA is an applied branch of Lattice Theory, a mathematical\ndiscipline which enables formalisation of concepts as basic units of human\nthinking and analysing data in the object-attribute form. Originated in early\n80s, during the last three decades, it became a popular human-centred tool for\nknowledge representation and data analysis with numerous applications. Since\nthe tutorial was specially prepared for RuSSIR 2014, the covered FCA topics\ninclude Information Retrieval with a focus on visualisation aspects, Machine\nLearning, Data Mining and Knowledge Discovery, Text Mining and several others.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 12:53:21 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ignatov", "Dmitry I.", ""]]}, {"id": "1703.02834", "submitter": "Pierre-Alexandre Mattei", "authors": "Charles Bouveyron (EPIONE, JAD), Pierre Latouche (MAP5 - UMR 8145),\n  Pierre-Alexandre Mattei", "title": "Exact Dimensionality Selection for Bayesian PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 13:47:17 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 13:00:55 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bouveyron", "Charles", "", "EPIONE, JAD"], ["Latouche", "Pierre", "", "MAP5 - UMR 8145"], ["Mattei", "Pierre-Alexandre", ""]]}, {"id": "1703.02899", "submitter": "Andreas Doerr", "authors": "Andreas Doerr, Duy Nguyen-Tuong, Alonso Marco, Stefan Schaal,\n  Sebastian Trimpe", "title": "Model-Based Policy Search for Automatic Tuning of Multivariate PID\n  Controllers", "comments": "Accepted final version to appear in 2017 IEEE International\n  Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PID control architectures are widely used in industrial applications. Despite\ntheir low number of open parameters, tuning multiple, coupled PID controllers\ncan become tedious in practice. In this paper, we extend PILCO, a model-based\npolicy search framework, to automatically tune multivariate PID controllers\npurely based on data observed on an otherwise unknown system. The system's\nstate is extended appropriately to frame the PID policy as a static state\nfeedback policy. This renders PID tuning possible as the solution of a finite\nhorizon optimal control problem without further a priori knowledge. The\nframework is applied to the task of balancing an inverted pendulum on a seven\ndegree-of-freedom robotic arm, thereby demonstrating its capabilities of fast\nand data-efficient policy learning, even on complex real world problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 16:28:17 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Doerr", "Andreas", ""], ["Nguyen-Tuong", "Duy", ""], ["Marco", "Alonso", ""], ["Schaal", "Stefan", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "1703.02910", "submitter": "Yarin Gal", "authors": "Yarin Gal and Riashat Islam and Zoubin Ghahramani", "title": "Deep Bayesian Active Learning with Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 16:53:57 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Gal", "Yarin", ""], ["Islam", "Riashat", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1703.02914", "submitter": "Yarin Gal", "authors": "Yingzhen Li and Yarin Gal", "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain uncertainty estimates with real-world Bayesian deep learning\nmodels, practical inference approximations are needed. Dropout variational\ninference (VI) for example has been used for machine vision and medical\napplications, but VI can severely underestimates model uncertainty.\nAlpha-divergences are alternative divergences to VI's KL objective, which are\nable to avoid VI's uncertainty underestimation. But these are hard to use in\npractice: existing techniques can only use Gaussian approximating\ndistributions, and require existing models to be changed radically, thus are of\nlimited use for practitioners. We propose a re-parametrisation of the\nalpha-divergence objectives, deriving a simple inference technique which,\ntogether with dropout, can be easily implemented with existing models by simply\nchanging the loss of the model. We demonstrate improved uncertainty estimates\nand accuracy compared to VI in dropout networks. We study our model's epistemic\nuncertainty far away from the data using adversarial images, showing that these\ncan be distinguished from non-adversarial images by examining our model's\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:00:21 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Li", "Yingzhen", ""], ["Gal", "Yarin", ""]]}, {"id": "1703.02965", "submitter": "Omer Dror", "authors": "Omer Dror, Boaz Nadler, Erhan Bilal and Yuval Kluger", "title": "Unsupervised Ensemble Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a regression problem where there is no labeled data and the only\nobservations are the predictions $f_i(x_j)$ of $m$ experts $f_{i}$ over many\nsamples $x_j$. With no knowledge on the accuracy of the experts, is it still\npossible to accurately estimate the unknown responses $y_{j}$? Can one still\ndetect the least or most accurate experts? In this work we propose a framework\nto study these questions, based on the assumption that the $m$ experts have\nuncorrelated deviations from the optimal predictor. Assuming the first two\nmoments of the response are known, we develop methods to detect the best and\nworst regressors, and derive U-PCR, a novel principal components approach for\nunsupervised ensemble regression. We provide theoretical support for U-PCR and\nillustrate its improved accuracy over the ensemble mean and median on a variety\nof regression problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 18:58:20 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Dror", "Omer", ""], ["Nadler", "Boaz", ""], ["Bilal", "Erhan", ""], ["Kluger", "Yuval", ""]]}, {"id": "1703.03020", "submitter": "Sarah Parisot", "authors": "Sarah Parisot, Sofia Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo\n  Guerrerro Moreno, Ben Glocker, Daniel Rueckert", "title": "Spectral Graph Convolutions for Population-based Disease Prediction", "comments": "International Conference on Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the wealth of imaging and non-imaging information for disease\nprediction tasks requires models capable of representing, at the same time,\nindividual features as well as data associations between subjects from\npotentially large populations. Graphs provide a natural framework for such\ntasks, yet previous graph-based approaches focus on pairwise similarities\nwithout modelling the subjects' individual characteristics and features. On the\nother hand, relying solely on subject-specific imaging feature vectors fails to\nmodel the interaction and similarity between subjects, which can reduce\nperformance. In this paper, we introduce the novel concept of Graph\nConvolutional Networks (GCN) for brain analysis in populations, combining\nimaging and non-imaging data. We represent populations as a sparse graph where\nits vertices are associated with image-based feature vectors and the edges\nencode phenotypic information. This structure was used to train a GCN model on\npartially labelled graphs, aiming to infer the classes of unlabelled nodes from\nthe node features and pairwise associations between subjects. We demonstrate\nthe potential of the method on the challenging ADNI and ABIDE databases, as a\nproof of concept of the benefit from integrating contextual information in\nclassification tasks. This has a clear impact on the quality of the\npredictions, leading to 69.5% accuracy for ABIDE (outperforming the current\nstate of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion,\nsignificantly outperforming standard linear classifiers where only individual\nfeatures are considered.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 20:07:28 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 11:09:10 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 17:24:40 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Parisot", "Sarah", ""], ["Ktena", "Sofia Ira", ""], ["Ferrante", "Enzo", ""], ["Lee", "Matthew", ""], ["Moreno", "Ricardo Guerrerro", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1703.03038", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti and Marco S. Nobile and Paolo Cazzaniga and\n  Giancarlo Mauri and Marco Antoniotti", "title": "Parallel Implementation of Efficient Search Schemes for the Inference of\n  Cancer Progression Models", "comments": null, "journal-ref": null, "doi": "10.1109/CIBCB.2016.7758109", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence and development of cancer is a consequence of the accumulation\nover time of genomic mutations involving a specific set of genes, which\nprovides the cancer clones with a functional selective advantage. In this work,\nwe model the order of accumulation of such mutations during the progression,\nwhich eventually leads to the disease, by means of probabilistic graphic\nmodels, i.e., Bayesian Networks (BNs). We investigate how to perform the task\nof learning the structure of such BNs, according to experimental evidence,\nadopting a global optimization meta-heuristics. In particular, in this work we\nrely on Genetic Algorithms, and to strongly reduce the execution time of the\ninference -- which can also involve multiple repetitions to collect\nstatistically significant assessments of the data -- we distribute the\ncalculations using both multi-threading and a multi-node architecture. The\nresults show that our approach is characterized by good accuracy and\nspecificity; we also demonstrate its feasibility, thanks to a 84x reduction of\nthe overall execution time with respect to a traditional sequential\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 21:29:52 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Ramazzotti", "Daniele", ""], ["Nobile", "Marco S.", ""], ["Cazzaniga", "Paolo", ""], ["Mauri", "Giancarlo", ""], ["Antoniotti", "Marco", ""]]}, {"id": "1703.03044", "submitter": "Maher Al-Shoukairi", "authors": "Maher Al-Shoukairi, Philip Schniter, Bhaskar D. Rao", "title": "A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2764855", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for the sparse signal recovery problem\nthat incorporates damped Gaussian generalized approximate message passing\n(GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning\n(SBL). In particular, GGAMP is used to implement the E-step in SBL in place of\nmatrix inversion, leveraging the fact that GGAMP is guaranteed to converge with\nappropriate damping. The resulting GGAMP-SBL algorithm is much more robust to\narbitrary measurement matrix $\\boldsymbol{A}$ than the standard damped GAMP\nalgorithm while being much lower complexity than the standard SBL algorithm. We\nthen extend the approach from the single measurement vector (SMV) case to the\ntemporally correlated multiple measurement vector (MMV) case, leading to the\nGGAMP-TSBL algorithm. We verify the robustness and computational advantages of\nthe proposed algorithms through numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 21:44:12 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 00:14:27 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Al-Shoukairi", "Maher", ""], ["Schniter", "Philip", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1703.03167", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (SELECT, LMO)", "title": "Cross-validation", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is a survey on cross-validation. We define all classical\ncross-validation procedures, and we study their properties for two different\ngoals: estimating the risk of a given estimator, and selecting the best\nestimator among a given family. For the risk estimation problem, we compute the\nbias (which can also be corrected) and the variance of cross-validation\nmethods. For estimator selection, we first provide a first-order analysis\n(based on expectations). Then, we explain how to take into account second-order\nterms (from variance computations, and by taking into account the usefulness of\noverpenalization). This allows, in the end, to provide some guidelines for\nchoosing the best cross-validation method for a given learning problem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 07:40:53 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Arlot", "Sylvain", "", "SELECT, LMO"]]}, {"id": "1703.03208", "submitter": "Ashish Bora", "authors": "Ashish Bora, Ajil Jalal, Eric Price, Alexandros G. Dimakis", "title": "Compressed Sensing using Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of compressed sensing is to estimate a vector from an\nunderdetermined system of noisy linear measurements, by making use of prior\nknowledge on the structure of vectors in the relevant domain. For almost all\nresults in this literature, the structure is represented by sparsity in a\nwell-chosen basis. We show how to achieve guarantees similar to standard\ncompressed sensing but without employing sparsity at all. Instead, we suppose\nthat vectors lie near the range of a generative model $G: \\mathbb{R}^k \\to\n\\mathbb{R}^n$. Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly\n$O(k \\log L)$ random Gaussian measurements suffice for an $\\ell_2/\\ell_2$\nrecovery guarantee. We demonstrate our results using generative models from\npublished variational autoencoder and generative adversarial networks. Our\nmethod can use $5$-$10$x fewer measurements than Lasso for the same accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 10:11:03 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Bora", "Ashish", ""], ["Jalal", "Ajil", ""], ["Price", "Eric", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1703.03216", "submitter": "Song Liu Dr.", "authors": "Song Liu, Akiko Takeda, Taiji Suzuki, Kenji Fukumizu", "title": "Trimmed Density Ratio Estimation", "comments": "Made minor revisions. Restructured the introductory sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density ratio estimation is a vital tool in both machine learning and\nstatistical community. However, due to the unbounded nature of density ratio,\nthe estimation procedure can be vulnerable to corrupted data points, which\noften pushes the estimated ratio toward infinity. In this paper, we present a\nrobust estimator which automatically identifies and trims outliers. The\nproposed estimator has a convex formulation, and the global optimum can be\nobtained via subgradient descent. We analyze the parameter estimation error of\nthis estimator under high-dimensional settings. Experiments are conducted to\nverify the effectiveness of the estimator.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 10:24:27 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 13:58:17 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 12:53:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Liu", "Song", ""], ["Takeda", "Akiko", ""], ["Suzuki", "Taiji", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1703.03352", "submitter": "Toby Hocking", "authors": "Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque", "title": "A log-linear time algorithm for constrained changepoint detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Changepoint detection is a central problem in time series and genomic data.\nFor some applications, it is natural to impose constraints on the directions of\nchanges. One example is ChIP-seq data, for which adding an up-down constraint\nimproves peak detection accuracy, but makes the optimization problem more\ncomplicated. We show how a recently proposed functional pruning technique can\nbe adapted to solve such constrained changepoint detection problems. This leads\nto a new algorithm which can solve problems with arbitrary affine constraints\non adjacent segment means, and which has empirical time complexity that is\nlog-linear in the amount of data. This algorithm achieves state-of-the-art\naccuracy in a benchmark of several genomic data sets, and is orders of\nmagnitude faster than existing algorithms that have similar accuracy. Our\nimplementation is available as the PeakSegPDPA function in the coseg R package,\nhttps://github.com/tdhock/coseg\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:17:39 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Hocking", "Toby Dylan", ""], ["Rigaill", "Guillem", ""], ["Fearnhead", "Paul", ""], ["Bourque", "Guillaume", ""]]}, {"id": "1703.03373", "submitter": "Jakob Richter", "authors": "Bernd Bischl and Jakob Richter and Jakob Bossek and Daniel Horn and\n  Janek Thomas and Michel Lang", "title": "mlrMBO: A Modular Framework for Model-Based Optimization of Expensive\n  Black-Box Functions", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present mlrMBO, a flexible and comprehensive R toolbox for model-based\noptimization (MBO), also known as Bayesian optimization, which addresses the\nproblem of expensive black-box optimization by approximating the given\nobjective function through a surrogate regression model. It is designed for\nboth single- and multi-objective optimization with mixed continuous,\ncategorical and conditional parameters. Additional features include multi-point\nbatch proposal, parallelization, visualization, logging and error-handling.\nmlrMBO is implemented in a modular fashion, such that single components can be\neasily replaced or adapted by the user for specific use cases, e.g., any\nregression learner from the mlr toolbox for machine learning can be used, and\ninfill criteria and infill optimizers are easily exchangeable. We empirically\ndemonstrate that mlrMBO provides state-of-the-art performance by comparing it\non different benchmark scenarios against a wide range of other optimizers,\nincluding DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and\nHyperopt.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:52:50 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 10:19:13 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 10:45:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Bischl", "Bernd", ""], ["Richter", "Jakob", ""], ["Bossek", "Jakob", ""], ["Horn", "Daniel", ""], ["Thomas", "Janek", ""], ["Lang", "Michel", ""]]}, {"id": "1703.03454", "submitter": "Zhaohan Guo", "authors": "Zhaohan Daniel Guo, Emma Brunskill", "title": "Sample Efficient Feature Selection for Factored MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the state of the real world is often represented\nby feature vectors. However, not all of the features may be pertinent for\nsolving the current task. We propose Feature Selection Explore and Exploit\n(FS-EE), an algorithm that automatically selects the necessary features while\nlearning a Factored Markov Decision Process, and prove that under mild\nassumptions, its sample complexity scales with the in-degree of the dynamics of\njust the necessary features, rather than the in-degree of all features. This\ncan result in a much better sample complexity when the in-degree of the\nnecessary features is smaller than the in-degree of all features.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 20:22:27 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Guo", "Zhaohan Daniel", ""], ["Brunskill", "Emma", ""]]}, {"id": "1703.03457", "submitter": "Michael Minyi Zhang", "authors": "Michael M. Zhang, Avinava Dubey, Sinead A. Williamson", "title": "Parallel Markov Chain Monte Carlo for the Indian Buffet Process", "comments": "Workshop paper in Bayesian Nonparametrics: The Next Generation, NIPS\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indian Buffet Process based models are an elegant way for discovering\nunderlying features within a data set, but inference in such models can be\nslow. Inferring underlying features using Markov chain Monte Carlo either\nrelies on an uncollapsed representation, which leads to poor mixing, or on a\ncollapsed representation, which leads to a quadratic increase in computational\ncomplexity. Existing attempts at distributing inference have introduced\nadditional approximation within the inference procedure. In this paper we\npresent a novel algorithm to perform asymptotically exact parallel Markov chain\nMonte Carlo inference for Indian Buffet Process models. We take advantage of\nthe fact that the features are conditionally independent under the\nbeta-Bernoulli process. Because of this conditional independence, we can\npartition the features into two parts: one part containing only the finitely\nmany instantiated features and the other part containing the infinite tail of\nuninstantiated features. For the finite partition, parallel inference is simple\ngiven the instantiation of features. But for the infinite tail, performing\nuncollapsed MCMC leads to poor mixing and hence we collapse out the features.\nThe resulting hybrid sampler, while being parallel, produces samples\nasymptotically from the true posterior.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 20:43:40 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Zhang", "Michael M.", ""], ["Dubey", "Avinava", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1703.03503", "submitter": "Heinrich Jiang", "authors": "Heinrich Jiang", "title": "Density Level Set Estimation on Manifolds with DBSCAN", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70: 1684-1693 (2017)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that DBSCAN can estimate the connected components of the\n$\\lambda$-density level set $\\{ x : f(x) \\ge \\lambda\\}$ given $n$ i.i.d.\nsamples from an unknown density $f$. We characterize the regularity of the\nlevel set boundaries using parameter $\\beta > 0$ and analyze the estimation\nerror under the Hausdorff metric. When the data lies in $\\mathbb{R}^D$ we\nobtain a rate of $\\widetilde{O}(n^{-1/(2\\beta + D)})$, which matches known\nlower bounds up to logarithmic factors. When the data lies on an embedded\nunknown $d$-dimensional manifold in $\\mathbb{R}^D$, then we obtain a rate of\n$\\widetilde{O}(n^{-1/(2\\beta + d\\cdot \\max\\{1, \\beta \\})})$. Finally, we\nprovide adaptive parameter tuning in order to attain these rates with no a\npriori knowledge of the intrinsic dimension, density, or $\\beta$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 01:28:15 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 21:54:45 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Jiang", "Heinrich", ""]]}, {"id": "1703.03507", "submitter": "Chase Shimmin", "authors": "Chase Shimmin, Peter Sadowski, Pierre Baldi, Edison Weik, Daniel\n  Whiteson, Edward Goul, Andreas S{\\o}gaard", "title": "Decorrelated Jet Substructure Tagging using Adversarial Neural Networks", "comments": null, "journal-ref": "Phys. Rev. D 96, 074034 (2017)", "doi": "10.1103/PhysRevD.96.074034", "report-no": null, "categories": "hep-ex physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a strategy for constructing a neural network jet substructure\ntagger which powerfully discriminates boosted decay signals while remaining\nlargely uncorrelated with the jet mass. This reduces the impact of systematic\nuncertainties in background modeling while enhancing signal purity, resulting\nin improved discovery significance relative to existing taggers. The network is\ntrained using an adversarial strategy, resulting in a tagger that learns to\nbalance classification accuracy with decorrelation. As a benchmark scenario, we\nconsider the case where large-radius jets originating from a boosted resonance\ndecay are discriminated from a background of nonresonant quark and gluon jets.\nWe show that in the presence of systematic uncertainties on the background\nrate, our adversarially-trained, decorrelated tagger considerably outperforms a\nconventionally trained neural network, despite having a slightly worse\nsignal-background separation power. We generalize the adversarial training\ntechnique to include a parametric dependence on the signal hypothesis, training\na single network that provides optimized, interpolatable decorrelated jet\ntagging across a continuous range of hypothetical resonance masses, after\ntraining on discrete choices of the signal mass.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 01:42:39 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Shimmin", "Chase", ""], ["Sadowski", "Peter", ""], ["Baldi", "Pierre", ""], ["Weik", "Edison", ""], ["Whiteson", "Daniel", ""], ["Goul", "Edward", ""], ["S\u00f8gaard", "Andreas", ""]]}, {"id": "1703.03596", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil and Sheetal Kalyani", "title": "High SNR Consistent Compressive Sensing", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High signal to noise ratio (SNR) consistency of model selection criteria in\nlinear regression models has attracted a lot of attention recently. However,\nmost of the existing literature on high SNR consistency deals with model order\nselection. Further, the limited literature available on the high SNR\nconsistency of subset selection procedures (SSPs) is applicable to linear\nregression with full rank measurement matrices only. Hence, the performance of\nSSPs used in underdetermined linear models (a.k.a compressive sensing (CS)\nalgorithms) at high SNR is largely unknown. This paper fills this gap by\nderiving necessary and sufficient conditions for the high SNR consistency of\npopular CS algorithms like $l_0$-minimization, basis pursuit de-noising or\nLASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions\nanalytically establish the high SNR inconsistency of CS algorithms when used\nwith the tuning parameters discussed in literature. Novel tuning parameters\nwith SNR adaptations are developed using the sufficient conditions and the\nchoice of SNR adaptations are discussed analytically using convergence rate\nanalysis. CS algorithms with the proposed tuning parameters are numerically\nshown to be high SNR consistent and outperform existing tuning parameters in\nthe moderate to high SNR regime.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 09:34:37 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1703.03717", "submitter": "Andrew Ross", "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez", "title": "Right for the Right Reasons: Training Differentiable Models by\n  Constraining their Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are among the most accurate supervised learning methods in\nuse today, but their opacity makes them difficult to trust in critical\napplications, especially when conditions in training differ from those in test.\nRecent work on explanations for black-box models has produced tools (e.g. LIME)\nto show the implicit rules behind predictions, which can help us identify when\nmodels are right for the wrong reasons. However, these methods do not scale to\nexplaining entire datasets and cannot correct the problems they reveal. We\nintroduce a method for efficiently explaining and regularizing differentiable\nmodels by examining and selectively penalizing their input gradients, which\nprovide a normal to the decision boundary. We apply these penalties both based\non expert annotation and in an unsupervised fashion that encourages diverse\nmodels with qualitatively different decision boundaries for the same\nclassification problem. On multiple datasets, we show our approach generates\nfaithful explanations and models that generalize much better when conditions\ndiffer between training and test.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 15:35:32 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 05:38:45 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ross", "Andrew Slavin", ""], ["Hughes", "Michael C.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1703.03722", "submitter": "Nematollah Zarmehi", "authors": "Nematollah Zarmehi and Farokh Marvasti", "title": "Recovery of Sparse and Low Rank Components of Matrices Using Iterative\n  Method with Adaptive Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:06:21 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:37:45 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zarmehi", "Nematollah", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1703.03859", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Jos\\'e Bento", "title": "Markov Chain Lifting and Distributed ADMM", "comments": "This work was also selected for a talk at NIPS 2016, Optimization for\n  Machine Learning Workshop (OPT 2016)", "journal-ref": "IEEE Signal Processing Letters (Volume: 24, Issue: 3, March 2017)", "doi": "10.1109/LSP.2017.2654860", "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time to converge to the steady state of a finite Markov chain can be\ngreatly reduced by a lifting operation, which creates a new Markov chain on an\nexpanded state space. For a class of quadratic objectives, we show an analogous\nbehavior where a distributed ADMM algorithm can be seen as a lifting of\nGradient Descent algorithm. This provides a deep insight for its faster\nconvergence rate under optimal parameter tuning. We conjecture that this gain\nis always present, as opposed to the lifting of a Markov chain which sometimes\nonly provides a marginal speedup.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:25:56 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1703.03862", "submitter": "Jesus Daniel Arroyo Reli\\'on", "authors": "Shangsi Wang, Jes\\'us Arroyo, Joshua T. Vogelstein, Carey E. Priebe", "title": "Joint Embedding of Graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2019.2948619", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and dimension reduction for networks is critical in a wide\nvariety of domains. Efficiently and accurately learning features for multiple\ngraphs has important applications in statistical inference on graphs. We\npropose a method to jointly embed multiple undirected graphs. Given a set of\ngraphs, the joint embedding method identifies a linear subspace spanned by rank\none symmetric matrices and projects adjacency matrices of graphs into this\nsubspace. The projection coefficients can be treated as features of the graphs,\nwhile the embedding components can represent vertex features. We also propose a\nrandom graph model for multiple graphs that generalizes other classical models\nfor graphs. We show through theory and numerical experiments that under the\nmodel, the joint embedding method produces estimates of parameters with small\nerrors. Via simulation experiments, we demonstrate that the joint embedding\nmethod produces features which lead to state of the art performance in\nclassifying graphs. Applying the joint embedding method to human brain graphs,\nwe find it extracts interpretable features with good prediction accuracy in\ndifferent tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:46:09 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 01:19:23 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 03:58:47 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 16:15:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Shangsi", ""], ["Arroyo", "Jes\u00fas", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1703.03863", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Jos\\'e Bento", "title": "Tuning Over-Relaxed ADMM", "comments": "NIPS 2016, Optimizing the Optimizer Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of Integral Quadratic Constraints (IQC) reduces the computation\nof upper bounds on the convergence rate of several optimization algorithms to a\nsemi-definite program (SDP). In the case of over-relaxed Alternating Direction\nMethod of Multipliers (ADMM), an explicit and closed form solution to this SDP\nwas derived in our recent work [1]. The purpose of this paper is twofold.\nFirst, we summarize these results. Second, we explore one of its consequences\nwhich allows us to obtain general and simple formulas for optimal parameter\nselection. These results are valid for arbitrary strongly convex objective\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:47:31 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 14:38:36 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1703.03864", "submitter": "Tim Salimans", "authors": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever", "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 23:02:19 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 23:28:48 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Salimans", "Tim", ""], ["Ho", "Jonathan", ""], ["Chen", "Xi", ""], ["Sidor", "Szymon", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1703.03869", "submitter": "Thomson Nguyen", "authors": "Philip Spanoudes, Thomson Nguyen", "title": "Deep Learning in Customer Churn Prediction: Unsupervised Feature\n  Learning on Abstract Company Independent Feature Vectors", "comments": "23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As companies increase their efforts in retaining customers, being able to\npredict accurately ahead of time, whether a customer will churn in the\nforeseeable future is an extremely powerful tool for any marketing team. The\npaper describes in depth the application of Deep Learning in the problem of\nchurn prediction. Using abstract feature vectors, that can generated on any\nsubscription based company's user event logs, the paper proves that through the\nuse of the intrinsic property of Deep Neural Networks (learning secondary\nfeatures in an unsupervised manner), the complete pipeline can be applied to\nany subscription based company with extremely good churn predictive\nperformance. Furthermore the research documented in the paper was performed for\nFramed Data (a company that sells churn prediction as a service for other\ncompanies) in conjunction with the Data Science Institute at Lancaster\nUniversity, UK. This paper is the intellectual property of Framed Data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 23:26:33 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Spanoudes", "Philip", ""], ["Nguyen", "Thomson", ""]]}, {"id": "1703.03888", "submitter": "Jose Luis Garcia-Arroyo", "authors": "Jose Luis Garcia-Arroyo and Begonya Garcia-Zapirain", "title": "Segmentation of skin lesions based on fuzzy classification of pixels and\n  histogram thresholding", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an innovative method for segmentation of skin lesions in\ndermoscopy images developed by the authors, based on fuzzy classification of\npixels and histogram thresholding.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 01:18:14 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Garcia-Arroyo", "Jose Luis", ""], ["Garcia-Zapirain", "Begonya", ""]]}, {"id": "1703.04025", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Jiaying Gu, Qing Zhou", "title": "Learning Large-Scale Bayesian Networks with the sparsebn Package", "comments": "To appear in the Journal of Statistical Software, 39 pages, 7 figures", "journal-ref": "Journal of Statistical Software, 91(11), 1-38, 2019", "doi": "10.18637/jss.v091.i11", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graphical models from data is an important problem with wide\napplications, ranging from genomics to the social sciences. Nowadays datasets\noften have upwards of thousands---sometimes tens or hundreds of thousands---of\nvariables and far fewer samples. To meet this challenge, we have developed a\nnew R package called sparsebn for learning the structure of large, sparse\ngraphical models with a focus on Bayesian networks. While there are many\nexisting software packages for this task, this package focuses on the unique\nsetting of learning large networks from high-dimensional data, possibly with\ninterventions. As such, the methods provided place a premium on scalability and\nconsistency in a high-dimensional setting. Furthermore, in the presence of\ninterventions, the methods implemented here achieve the goal of learning a\ncausal network from data. Additionally, the sparsebn package is fully\ncompatible with existing software packages for network analysis.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 20:07:06 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 23:22:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Aragam", "Bryon", ""], ["Gu", "Jiaying", ""], ["Zhou", "Qing", ""]]}, {"id": "1703.04046", "submitter": "Akara Supratak", "authors": "Akara Supratak, Hao Dong, Chao Wu, Yike Guo", "title": "DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw\n  Single-Channel EEG", "comments": "This article has been published in IEEE Transactions on Neural\n  Systems and Rehabilitation Engineering", "journal-ref": null, "doi": "10.1109/TNSRE.2017.2721116", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes a deep learning model, named DeepSleepNet, for\nautomatic sleep stage scoring based on raw single-channel EEG. Most of the\nexisting methods rely on hand-engineered features which require prior knowledge\nof sleep analysis. Only a few of them encode the temporal information such as\ntransition rules, which is important for identifying the next sleep stages,\ninto the extracted features. In the proposed model, we utilize Convolutional\nNeural Networks to extract time-invariant features, and bidirectional-Long\nShort-Term Memory to learn transition rules among sleep stages automatically\nfrom EEG epochs. We implement a two-step training algorithm to train our model\nefficiently. We evaluated our model using different single-channel EEGs\n(F4-EOG(Left), Fpz-Cz and Pz-Oz) from two public sleep datasets, that have\ndifferent properties (e.g., sampling rate) and scoring standards (AASM and\nR&K). The results showed that our model achieved similar overall accuracy and\nmacro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared to the\nstate-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both\ndatasets. This demonstrated that, without changing the model architecture and\nthe training algorithm, our model could automatically learn features for sleep\nstage scoring from different raw single-channel EEGs from different datasets\nwithout utilizing any hand-engineered features.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 00:15:32 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 21:11:11 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Supratak", "Akara", ""], ["Dong", "Hao", ""], ["Wu", "Chao", ""], ["Guo", "Yike", ""]]}, {"id": "1703.04070", "submitter": "Nikhil Mishra", "authors": "Nikhil Mishra, Pieter Abbeel, Igor Mordatch", "title": "Prediction and Control with Temporal Segment Models", "comments": "camera-ready version, ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for learning the dynamics of complex nonlinear systems\nbased on deep generative models over temporal segments of states and actions.\nUnlike dynamics models that operate over individual discrete timesteps, we\nlearn the distribution over future state trajectories conditioned on past\nstate, past action, and planned future action trajectories, as well as a latent\nprior over action trajectories. Our approach is based on convolutional\nautoregressive models and variational autoencoders. It makes stable and\naccurate predictions over long horizons for complex, stochastic systems,\neffectively expressing uncertainty and modeling the effects of collisions,\nsensory noise, and action delays. The learned dynamics model and action prior\ncan be used for end-to-end, fully differentiable trajectory optimization and\nmodel-based policy optimization, which we use to evaluate the performance and\nsample-efficiency of our method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 04:59:15 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 04:54:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Mishra", "Nikhil", ""], ["Abbeel", "Pieter", ""], ["Mordatch", "Igor", ""]]}, {"id": "1703.04078", "submitter": "Huaixiu Zheng", "authors": "Saifeng Liu, Huaixiu Zheng, Yesu Feng, Wei Li", "title": "Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric\n  MRI", "comments": "4 pages, 4 figures, Proc. SPIE 10134, Medical Imaging 2017", "journal-ref": null, "doi": "10.1117/12.2277121", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep learning architecture (XmasNet) based on convolutional neural\nnetworks was developed for the classification of prostate cancer lesions, using\nthe 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end\ntraining was performed for XmasNet, with data augmentation done through 3D\nrotation and slicing, in order to incorporate the 3D information of the lesion.\nXmasNet outperformed traditional machine learning models based on engineered\nfeatures, for both train and test data. For the test data, XmasNet outperformed\n69 methods from 33 participating groups and achieved the second highest AUC\n(0.84) in the PROSTATEx challenge. This study shows the great potential of deep\nlearning for cancer imaging.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 07:19:55 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Liu", "Saifeng", ""], ["Zheng", "Huaixiu", ""], ["Feng", "Yesu", ""], ["Li", "Wei", ""]]}, {"id": "1703.04081", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Lena A. J\\\"ager, Bruno Nicenboim", "title": "Feature overwriting as a finite mixture process: Evidence from\n  comprehension data", "comments": "6 pages, 2 figures, 1 table, submitted to MathPsych/ICCM 2017,\n  Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ungrammatical sentence \"The key to the cabinets are on the table\" is\nknown to lead to an illusion of grammaticality. As discussed in the\nmeta-analysis by Jaeger et al., 2017, faster reading times are observed at the\nverb are in the agreement-attraction sentence above compared to the equally\nungrammatical sentence \"The key to the cabinet are on the table\". One\nexplanation for this facilitation effect is the feature percolation account:\nthe plural feature on cabinets percolates up to the head noun key, leading to\nthe illusion. An alternative account is in terms of cue-based retrieval (Lewis\n& Vasishth, 2005), which assumes that the non-subject noun cabinets is\nmisretrieved due to a partial feature-match when a dependency completion\nprocess at the auxiliary initiates a memory access for a subject with plural\nmarking. We present evidence for yet another explanation for the observed\nfacilitation. Because the second sentence has two nouns with identical number,\nit is possible that these are, in some proportion of trials, more difficult to\nkeep distinct, leading to slower reading times at the verb in the first\nsentence above; this is the feature overwriting account of Nairne, 1990. We\nshow that the feature overwriting proposal can be implemented as a finite\nmixture process. We reanalysed ten published data-sets, fitting hierarchical\nBayesian mixture models to these data assuming a two-mixture distribution. We\nshow that in nine out of the ten studies, a mixture distribution corresponding\nto feature overwriting furnishes a superior fit over both the feature\npercolation and the cue-based retrieval accounts.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 08:11:29 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 08:15:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Vasishth", "Shravan", ""], ["J\u00e4ger", "Lena A.", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1703.04082", "submitter": "Sejun Park", "authors": "Sejun Park, Eunho Yang, Jinwoo Shin", "title": "Sequential Local Learning for Latent Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning parameters of latent graphical models (GM) is inherently much harder\nthan that of no-latent ones since the latent variables make the corresponding\nlog-likelihood non-concave. Nevertheless, expectation-maximization schemes are\npopularly used in practice, but they are typically stuck in local optima. In\nthe recent years, the method of moments have provided a refreshing angle for\nresolving the non-convex issue, but it is applicable to a quite limited class\nof latent GMs. In this paper, we aim for enhancing its power via enlarging such\na class of latent GMs. To this end, we introduce two novel concepts, coined\nmarginalization and conditioning, which can reduce the problem of learning a\nlarger GM to that of a smaller one. More importantly, they lead to a sequential\nlearning framework that repeatedly increases the learning portion of given\nlatent GM, and thus covers a significantly broader and more complicated class\nof loopy latent GMs which include convolutional and random regular models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 08:18:11 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 02:38:19 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Park", "Sejun", ""], ["Yang", "Eunho", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1703.04140", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "J\\\"orn-Henrik Jacobsen, Edouard Oyallon, St\\'ephane Mallat, Arnold\n  W.M. Smeulders", "title": "Multiscale Hierarchical Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network algorithms are difficult to analyze because they lack\nstructure allowing to understand the properties of underlying transforms and\ninvariants. Multiscale hierarchical convolutional networks are structured deep\nconvolutional networks where layers are indexed by progressively higher\ndimensional attributes, which are learned from training data. Each new layer is\ncomputed with multidimensional convolutions along spatial and attribute\nvariables. We introduce an efficient implementation of such networks where the\ndimensionality is progressively reduced by averaging intermediate layers along\nattribute indices. Hierarchical networks are tested on CIFAR image data bases\nwhere they obtain comparable precisions to state of the art networks, with much\nfewer parameters. We study some properties of the attributes learned from these\ndatabases.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 16:29:44 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["Oyallon", "Edouard", ""], ["Mallat", "St\u00e9phane", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1703.04145", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Anna Schroeder, Oliver Breitwieser, Andreas\n  Gr\\\"ubl, Johannes Schemmel, Karlheinz Meier", "title": "Robustness from structure: Inference with hierarchical spiking networks\n  on analog neuromorphic hardware", "comments": "accepted at IJCNN 2017", "journal-ref": "International Joint Conference on Neural Networks (IJCNN), 2017", "doi": "10.1109/IJCNN.2017.7966123", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How spiking networks are able to perform probabilistic inference is an\nintriguing question, not only for understanding information processing in the\nbrain, but also for transferring these computational principles to neuromorphic\nsilicon circuits. A number of computationally powerful spiking network models\nhave been proposed, but most of them have only been tested, under ideal\nconditions, in software simulations. Any implementation in an analog, physical\nsystem, be it in vivo or in silico, will generally lead to distorted dynamics\ndue to the physical properties of the underlying substrate. In this paper, we\ndiscuss several such distortive effects that are difficult or impossible to\nremove by classical calibration routines or parameter training. We then argue\nthat hierarchical networks of leaky integrate-and-fire neurons can offer the\nrequired robustness for physical implementation and demonstrate this with both\nsoftware simulations and emulation on an accelerated analog neuromorphic\ndevice.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 17:29:11 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Schroeder", "Anna", ""], ["Breitwieser", "Oliver", ""], ["Gr\u00fcbl", "Andreas", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1703.04200", "submitter": "Ben Poole", "authors": "Friedemann Zenke, Ben Poole, Surya Ganguli", "title": "Continual Learning Through Synaptic Intelligence", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has led to remarkable advances across diverse\napplications, it struggles in domains where the data distribution changes over\nthe course of learning. In stark contrast, biological neural networks\ncontinually adapt to changing domains, possibly by leveraging complex molecular\nmachinery to solve many tasks simultaneously. In this study, we introduce\nintelligent synapses that bring some of this biological complexity into\nartificial neural networks. Each synapse accumulates task relevant information\nover time, and exploits this information to rapidly store new memories without\nforgetting old ones. We evaluate our approach on continual learning of\nclassification tasks, and show that it dramatically reduces forgetting while\nmaintaining computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 00:02:48 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:54:57 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 19:57:42 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Zenke", "Friedemann", ""], ["Poole", "Ben", ""], ["Ganguli", "Surya", ""]]}, {"id": "1703.04334", "submitter": "Fani Tsapeli", "authors": "Fani Tsapeli, Peter Tino, Mirco Musolesi", "title": "Probabilistic Matching: Causal Inference under Measurement Errors", "comments": "In Proceedings of International Joint Conference Of Neural Networks\n  (IJCNN) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of data produced daily from large variety of sources has\nboosted the need of novel approaches on causal inference analysis from\nobservational data. Observational data often contain noisy or missing entries.\nMoreover, causal inference studies may require unobserved high-level\ninformation which needs to be inferred from other observed attributes. In such\ncases, inaccuracies of the applied inference methods will result in noisy\noutputs. In this study, we propose a novel approach for causal inference when\none or more key variables are noisy. Our method utilizes the knowledge about\nthe uncertainty of the real values of key variables in order to reduce the bias\ninduced by noisy measurements. We evaluate our approach in comparison with\nexisting methods both on simulated and real scenarios and we demonstrate that\nour method reduces the bias and avoids false causal inference conclusions in\nmost cases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:28 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Tsapeli", "Fani", ""], ["Tino", "Peter", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1703.04335", "submitter": "Mark McLeod", "authors": "Mark McLeod, Michael A. Osborne, Stephen J. Roberts", "title": "Practical Bayesian Optimization for Variable Cost Objectives", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian Optimization approach for black-box functions\nwith an environmental variable whose value determines the tradeoff between\nevaluation cost and the fidelity of the evaluations. Further, we use a novel\napproach to sampling support points, allowing faster construction of the\nacquisition function. This allows us to achieve optimization with lower\noverheads than previous approaches and is implemented for a more general class\nof problem. We show this approach to be effective on synthetic and real world\nbenchmark problems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:52 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 12:01:32 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["McLeod", "Mark", ""], ["Osborne", "Michael A.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1703.04379", "submitter": "Nanyang Ye", "authors": "Nanyang Ye, Zhanxing Zhu, Rafal K. Mantiuk", "title": "Langevin Dynamics with Continuous Tempering for Training Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing non-convex and high-dimensional objective functions is\nchallenging, especially when training modern deep neural networks. In this\npaper, a novel approach is proposed which divides the training process into two\nconsecutive phases to obtain better generalization performance: Bayesian\nsampling and stochastic optimization. The first phase is to explore the energy\nlandscape and to capture the \"fat\" modes; and the second one is to fine-tune\nthe parameter learned from the first phase. In the Bayesian learning phase, we\napply continuous tempering and stochastic approximation into the Langevin\ndynamics to create an efficient and effective sampler, in which the temperature\nis adjusted automatically according to the designed \"temperature dynamics\".\nThese strategies can overcome the challenge of early trapping into bad local\nminima and have achieved remarkable improvements in various types of neural\nnetworks as shown in our theoretical analysis and empirical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 13:27:56 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 16:52:32 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 17:15:28 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 12:27:09 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Ye", "Nanyang", ""], ["Zhu", "Zhanxing", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1703.04389", "submitter": "Jian Wu", "authors": "Jian Wu, Matthias Poloczek, Andrew Gordon Wilson, and Peter I. Frazier", "title": "Bayesian Optimization with Gradients", "comments": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has been successful at global optimization of\nexpensive-to-evaluate multimodal objective functions. However, unlike most\noptimization methods, Bayesian optimization typically does not use derivative\ninformation. In this paper we show how Bayesian optimization can exploit\nderivative information to decrease the number of objective function evaluations\nrequired for good performance. In particular, we develop a novel Bayesian\noptimization algorithm, the derivative-enabled knowledge-gradient (dKG), for\nwhich we show one-step Bayes-optimality, asymptotic consistency, and greater\none-step value of information than is possible in the derivative-free setting.\nOur procedure accommodates noisy and incomplete derivative information, comes\nin both sequential and batch forms, and can optionally reduce the computational\ncost of inference through automatically selected retention of a single\ndirectional derivative. We also compute the d-KG acquisition function and its\ngradient using a novel fast discretization-free technique. We show d-KG\nprovides state-of-the-art performance compared to a wide range of optimization\nprocedures with and without gradients, on benchmarks including logistic\nregression, deep learning, kernel learning, and k-nearest neighbors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 13:45:13 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 13:27:45 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 04:05:29 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Wu", "Jian", ""], ["Poloczek", "Matthias", ""], ["Wilson", "Andrew Gordon", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1703.04455", "submitter": "Zexun Chen", "authors": "Zexun Chen, Bo Wang and Alexander N. Gorban", "title": "Multivariate Gaussian and Student$-t$ Process Regression for\n  Multi-output Prediction", "comments": null, "journal-ref": "NEURAL COMPUT APPL 32 (2020): 3005-3028", "doi": "10.1007/s00521-019-04687-8", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process model for vector-valued function has been shown to be useful\nfor multi-output prediction. The existing method for this model is to\nre-formulate the matrix-variate Gaussian distribution as a multivariate normal\ndistribution. Although it is effective in many cases, re-formulation is not\nalways workable and is difficult to apply to other distributions because not\nall matrix-variate distributions can be transformed to respective multivariate\ndistributions, such as the case for matrix-variate Student$-t$ distribution. In\nthis paper, we propose a unified framework which is used not only to introduce\na novel multivariate Student$-t$ process regression model (MV-TPR) for\nmulti-output prediction, but also to reformulate the multivariate Gaussian\nprocess regression (MV-GPR) that overcomes some limitations of the existing\nmethods. Both MV-GPR and MV-TPR have closed-form expressions for the marginal\nlikelihoods and predictive distributions under this unified framework and thus\ncan adopt the same optimization approaches as used in the conventional GPR. The\nusefulness of the proposed methods is illustrated through several simulated and\nreal data examples. In particular, we verify empirically that MV-TPR has\nsuperiority for the datasets considered, including air quality prediction and\nbike rent prediction. At last, the proposed methods are shown to produce\nprofitable investment strategies in the stock markets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 15:43:00 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 17:37:03 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 16:02:48 GMT"}, {"version": "v4", "created": "Wed, 4 Oct 2017 15:21:43 GMT"}, {"version": "v5", "created": "Thu, 18 Jan 2018 16:59:36 GMT"}, {"version": "v6", "created": "Sun, 6 Jan 2019 20:20:05 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chen", "Zexun", ""], ["Wang", "Bo", ""], ["Gorban", "Alexander N.", ""]]}, {"id": "1703.04599", "submitter": "Quoc Tran-Dinh", "authors": "Tianxiao Sun, Quoc Tran-Dinh", "title": "Generalized Self-Concordant Functions: A Recipe for Newton-Type Methods", "comments": "47 pages, 2 figures", "journal-ref": "Mathematical Programming (2018)", "doi": null, "report-no": "UNC-STOR-March-2017", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the smooth structure of convex functions by generalizing a powerful\nconcept so-called self-concordance introduced by Nesterov and Nemirovskii in\nthe early 1990s to a broader class of convex functions, which we call\ngeneralized self-concordant functions. This notion allows us to develop a\nunified framework for designing Newton-type methods to solve convex optimiza-\ntion problems. The proposed theory provides a mathematical tool to analyze both\nlocal and global convergence of Newton-type methods without imposing\nunverifiable assumptions as long as the un- derlying functionals fall into our\ngeneralized self-concordant function class. First, we introduce the class of\ngeneralized self-concordant functions, which covers standard self-concordant\nfunctions as a special case. Next, we establish several properties and key\nestimates of this function class, which can be used to design numerical\nmethods. Then, we apply this theory to develop several Newton-type methods for\nsolving a class of smooth convex optimization problems involving the\ngeneralized self- concordant functions. We provide an explicit step-size for\nthe damped-step Newton-type scheme which can guarantee a global convergence\nwithout performing any globalization strategy. We also prove a local quadratic\nconvergence of this method and its full-step variant without requiring the\nLipschitz continuity of the objective Hessian. Then, we extend our result to\ndevelop proximal Newton-type methods for a class of composite convex\nminimization problems involving generalized self-concordant functions. We also\nachieve both global and local convergence without additional assumption.\nFinally, we verify our theoretical results via several numerical examples, and\ncompare them with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 17:23:02 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 18:46:13 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 12:14:27 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Sun", "Tianxiao", ""], ["Tran-Dinh", "Quoc", ""]]}, {"id": "1703.04691", "submitter": "Anastasia Borovykh", "authors": "Anastasia Borovykh, Sander Bohte, Cornelis W. Oosterlee", "title": "Conditional Time Series Forecasting with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for conditional time series forecasting based on an\nadaptation of the recent deep convolutional WaveNet architecture. The proposed\nnetwork contains stacks of dilated convolutions that allow it to access a broad\nrange of history when forecasting, a ReLU activation function and conditioning\nis performed by applying multiple convolutional filters in parallel to separate\ntime series which allows for the fast processing of data and the exploitation\nof the correlation structure between the multivariate time series. We test and\nanalyze the performance of the convolutional network both unconditionally as\nwell as conditionally for financial time series forecasting using the S&P500,\nthe volatility index, the CBOE interest rate and several exchange rates and\nextensively compare it to the performance of the well-known autoregressive\nmodel and a long-short term memory network. We show that a convolutional\nnetwork is well-suited for regression-type problems and is able to effectively\nlearn dependencies in and between the series without the need for long\nhistorical time series, is a time-efficient and easy to implement alternative\nto recurrent-type networks and tends to outperform linear and recurrent models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 20:07:12 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 11:18:56 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 18:56:56 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 10:26:15 GMT"}, {"version": "v5", "created": "Mon, 17 Sep 2018 15:08:00 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Borovykh", "Anastasia", ""], ["Bohte", "Sander", ""], ["Oosterlee", "Cornelis W.", ""]]}, {"id": "1703.04697", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Joseph Salmon", "title": "On the benefits of output sparsity for multi-label classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-label classification framework, where each observation can be\nassociated with a set of labels, has generated a tremendous amount of attention\nover recent years. The modern multi-label problems are typically large-scale in\nterms of number of observations, features and labels, and the amount of labels\ncan even be comparable with the amount of observations. In this context,\ndifferent remedies have been proposed to overcome the curse of dimensionality.\nIn this work, we aim at exploiting the output sparsity by introducing a new\nloss, called the sparse weighted Hamming loss. This proposed loss can be seen\nas a weighted version of classical ones, where active and inactive labels are\nweighted separately. Leveraging the influence of sparsity in the loss function,\nwe provide improved generalization bounds for the empirical risk minimizer, a\nsuitable property for large-scale problems. For this new loss, we derive rates\nof convergence linear in the underlying output-sparsity rather than linear in\nthe number of labels. In practice, minimizing the associated risk can be\nperformed efficiently by using convex surrogates and modern convex optimization\nalgorithms. We provide experiments on various real-world datasets demonstrating\nthe pertinence of our approach when compared to non-weighted techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 20:19:08 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Denis", "Christophe", ""], ["Hebiri", "Mohamed", ""], ["Salmon", "Joseph", ""]]}, {"id": "1703.04730", "submitter": "Pang Wei Koh", "authors": "Pang Wei Koh and Percy Liang", "title": "Understanding Black-box Predictions via Influence Functions", "comments": "International Conference on Machine Learning, 2017. (This version\n  adds more historical references and fixes typos.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we explain the predictions of a black-box model? In this paper, we\nuse influence functions -- a classic technique from robust statistics -- to\ntrace a model's prediction through the learning algorithm and back to its\ntraining data, thereby identifying training points most responsible for a given\nprediction. To scale up influence functions to modern machine learning\nsettings, we develop a simple, efficient implementation that requires only\noracle access to gradients and Hessian-vector products. We show that even on\nnon-convex and non-differentiable models where the theory breaks down,\napproximations to influence functions can still provide valuable information.\nOn linear models and convolutional neural networks, we demonstrate that\ninfluence functions are useful for multiple purposes: understanding model\nbehavior, debugging models, detecting dataset errors, and even creating\nvisually-indistinguishable training-set attacks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 21:07:01 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 02:31:54 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 22:40:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Koh", "Pang Wei", ""], ["Liang", "Percy", ""]]}, {"id": "1703.04757", "submitter": "Nima Dehmamy", "authors": "Nima Dehmamy, Neda Rohani and Aggelos Katsaggelos", "title": "Separation of time scales and direct computation of weights in deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence is revolutionizing our lives at an ever increasing\npace. At the heart of this revolution is the recent advancements in deep neural\nnetworks (DNN), learning to perform sophisticated, high-level tasks. However,\ntraining DNNs requires massive amounts of data and is very computationally\nintensive. Gaining analytical understanding of the solutions found by DNNs can\nhelp us devise more efficient training algorithms, replacing the commonly used\nmthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and\nshow that, indeed, direct computation of the solutions is possible in many\ncases. We show that a high performing setup used in DNNs introduces a\nseparation of time-scales in the training dynamics, allowing SGD to train\nlayers from the lowest (closest to input) to the highest. We then show that for\neach layer, the distribution of solutions found by SGD can be estimated using a\nclass-based principal component analysis (PCA) of the layer's input. This\nfinding allows us to forgo SGD entirely and directly derive the DNN parameters\nusing this class-based PCA, which can be well estimated using significantly\nless data than SGD. We implement these results on image datasets MNIST, CIFAR10\nand CIFAR100 and find that, in fact, layers derived using our class-based PCA\nperform comparable or superior to neural networks of the same size and\narchitecture trained using SGD. We also confirm that the class-based PCA often\nconverges using a fraction of the data required for SGD. Thus, using our method\ntraining time can be reduced both by requiring less training data than SGD, and\nby eliminating layers in the costly backpropagation step of the training.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:13:41 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 07:53:52 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 20:30:28 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Dehmamy", "Nima", ""], ["Rohani", "Neda", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1703.04775", "submitter": "Andrea Tacchetti", "authors": "Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos", "title": "Discriminate-and-Rectify Encoders: Learning from Image Transformation\n  Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of a learning task is increased by transformations in the\ninput space that preserve class identity. Visual object recognition for example\nis affected by changes in viewpoint, scale, illumination or planar\ntransformations. While drastically altering the visual appearance, these\nchanges are orthogonal to recognition and should not be reflected in the\nrepresentation or feature encoding used for learning. We introduce a framework\nfor weakly supervised learning of image embeddings that are robust to\ntransformations and selective to the class distribution, using sets of\ntransforming examples (orbit sets), deep parametrizations and a novel\norbit-based loss. The proposed loss combines a discriminative, contrastive part\nfor orbits with a reconstruction error that learns to rectify orbit\ntransformations. The learned embeddings are evaluated in distance metric-based\ntasks, such as one-shot classification under geometric transformations, as well\nas face verification and retrieval under more realistic visual variability. Our\nresults suggest that orbit sets, suitably computed or observed, can be used for\nefficient, weakly-supervised learning of semantically relevant image\nembeddings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:21:48 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Voinea", "Stephen", ""], ["Evangelopoulos", "Georgios", ""]]}, {"id": "1703.04778", "submitter": "John McCoy", "authors": "John McCoy and Drazen Prelec", "title": "A statistical model for aggregating judgments by incorporating peer\n  predictions", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model to aggregate the answers of respondents\nanswering multiple-choice questions. The model does not assume that everyone\nhas access to the same information, and so does not assume that the consensus\nanswer is correct. Instead, it infers the most probable world state, even if\nonly a minority vote for it. Each respondent is modeled as receiving a signal\ncontingent on the actual world state, and as using this signal to both\ndetermine their own answer and predict the answers given by others. By\nincorporating respondent's predictions of others' answers, the model infers\nlatent parameters corresponding to the prior over world states and the\nprobability of different signals being received in all possible world states,\nincluding counterfactual ones. Unlike other probabilistic models for\naggregation, our model applies to both single and multiple questions, in which\ncase it estimates each respondent's expertise. The model shows good\nperformance, compared to a number of other probabilistic models, on data from\nseven studies covering different types of expertise.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:23:17 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["McCoy", "John", ""], ["Prelec", "Drazen", ""]]}, {"id": "1703.04782", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark\n  Schmidt, Frank Wood", "title": "Online Learning Rate Adaptation with Hypergradient Descent", "comments": "11 pages, 4 figures", "journal-ref": "In Sixth International Conference on Learning Representations\n  (ICLR), Vancouver, Canada, April 30 -- May 3, 2018.\n  https://openreview.net/forum?id=BkrsAzWAb", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general method for improving the convergence rate of\ngradient-based optimizers that is easy to implement and works well in practice.\nWe demonstrate the effectiveness of the method in a range of optimization\nproblems by applying it to stochastic gradient descent, stochastic gradient\ndescent with Nesterov momentum, and Adam, showing that it significantly reduces\nthe need for the manual tuning of the initial learning rate for these commonly\nused algorithms. Our method works by dynamically updating the learning rate\nduring optimization using the gradient with respect to the learning rate of the\nupdate rule itself. Computing this \"hypergradient\" needs little additional\ncomputation, requires only one extra copy of the original gradient to be stored\nin memory, and relies upon nothing more than what is provided by reverse-mode\nautomatic differentiation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:28:27 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 23:38:42 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 01:36:49 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Cornish", "Robert", ""], ["Rubio", "David Martinez", ""], ["Schmidt", "Mark", ""], ["Wood", "Frank", ""]]}, {"id": "1703.04813", "submitter": "Olga Wichrowska", "authors": "Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio\n  Gomez Colmenarejo, Misha Denil, Nando de Freitas, Jascha Sohl-Dickstein", "title": "Learned Optimizers that Scale and Generalize", "comments": "Final ICML paper after reviewer suggestions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to learn has emerged as an important direction for achieving\nartificial intelligence. Two of the primary barriers to its adoption are an\ninability to scale to larger problems and a limited ability to generalize to\nnew tasks. We introduce a learned gradient descent optimizer that generalizes\nwell to new tasks, and which has significantly reduced memory and computation\noverhead. We achieve this by introducing a novel hierarchical RNN architecture,\nwith minimal per-parameter overhead, augmented with additional architectural\nfeatures that mirror the known structure of optimization tasks. We also develop\na meta-training ensemble of small, diverse optimization tasks capturing common\nproperties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM\non problems in this corpus. More importantly, it performs comparably or better\nwhen applied to small convolutional neural networks, despite seeing no neural\nnetworks in its meta-training set. Finally, it generalizes to train Inception\nV3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps,\noptimization problems that are of a vastly different scale than those it was\ntrained on. We release an open source implementation of the meta-training\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:05:54 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 21:55:33 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 22:22:38 GMT"}, {"version": "v4", "created": "Thu, 7 Sep 2017 23:38:09 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Wichrowska", "Olga", ""], ["Maheswaranathan", "Niru", ""], ["Hoffman", "Matthew W.", ""], ["Colmenarejo", "Sergio Gomez", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1703.04823", "submitter": "Jose Lugo-Martinez", "authors": "Jose Lugo-Martinez and Predrag Radivojac", "title": "Classification in biological networks with hypergraphlet kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and cellular systems are often modeled as graphs in which vertices\nrepresent objects of interest (genes, proteins, drugs) and edges represent\nrelational ties among these objects (binds-to, interacts-with, regulates). This\napproach has been highly successful owing to the theory, methodology and\nsoftware that support analysis and learning on graphs. Graphs, however, often\nsuffer from information loss when modeling physical systems due to their\ninability to accurately represent multiobject relationships. Hypergraphs, a\ngeneralization of graphs, provide a framework to mitigate information loss and\nunify disparate graph-based methodologies. In this paper, we present a\nhypergraph-based approach for modeling physical systems and formulate vertex\nclassification, edge classification and link prediction problems on\n(hyper)graphs as instances of vertex classification on (extended, dual)\nhypergraphs in a semi-supervised setting. We introduce a novel kernel method on\nvertex- and edge-labeled (colored) hypergraphs for analysis and learning. The\nmethod is based on exact and inexact (via hypergraph edit distances)\nenumeration of small simple hypergraphs, referred to as hypergraphlets, rooted\nat a vertex of interest. We extensively evaluate this method and show its\npotential use in a positive-unlabeled setting to estimate the number of missing\nand false positive links in protein-protein interaction networks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:20:17 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Lugo-Martinez", "Jose", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1703.04832", "submitter": "Dinh Phung", "authors": "Dinh Phung and Ba-Ngu Bo", "title": "A Random Finite Set Model for Data Clustering", "comments": "In Proceedings of International Conference on Fusion (FUSION),\n  Salamanca, Spain, July 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of data clustering is to partition data points into groups to\nminimize a given objective function. While most existing clustering algorithms\ntreat each data point as vector, in many applications each datum is not a\nvector but a point pattern or a set of points. Moreover, many existing\nclustering methods require the user to specify the number of clusters, which is\nnot available in advance. This paper proposes a new class of models for data\nclustering that addresses set-valued data as well as unknown number of\nclusters, using a Dirichlet Process mixture of Poisson random finite sets. We\nalso develop an efficient Markov Chain Monte Carlo posterior inference\ntechnique that can learn the number of clusters and mixture parameters\nautomatically from the data. Numerical studies are presented to demonstrate the\nsalient features of this new model, in particular its capacity to discover\nextremely unbalanced clusters in data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:35:57 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Phung", "Dinh", ""], ["Bo", "Ba-Ngu", ""]]}, {"id": "1703.04864", "submitter": "Young Woong Park", "authors": "Young Woong Park", "title": "Optimization for L1-Norm Error Fitting via Data Aggregation", "comments": null, "journal-ref": "INFORMS Journal on Computing 33-1(2021): 120-142", "doi": "10.1287/ijoc.2019.0908", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data aggregation-based algorithm with monotonic convergence to a\nglobal optimum for a generalized version of the L1-norm error fitting model\nwith an assumption of the fitting function. The proposed algorithm generalizes\nthe recent algorithm in the literature, aggregate and iterative disaggregate\n(AID), which selectively solves three specific L1-norm error fitting problems.\nWith the proposed algorithm, any L1-norm error fitting model can be solved\noptimally if it follows the form of the L1-norm error fitting problem and if\nthe fitting function satisfies the assumption. The proposed algorithm can also\nsolve multi-dimensional fitting problems with arbitrary constraints on the\nfitting coefficients matrix. The generalized problem includes popular models\nsuch as regression and the orthogonal Procrustes problem. The results of the\ncomputational experiment show that the proposed algorithms are faster than the\nstate-of-the-art benchmarks for L1-norm regression subset selection and L1-norm\nregression over a sphere. Further, the relative performance of the proposed\nalgorithm improves as data size increases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:16:09 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 05:05:36 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 20:22:18 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Park", "Young Woong", ""]]}, {"id": "1703.04890", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Hiroyuki Sato and Bamdev Mishra", "title": "Riemannian stochastic quasi-Newton algorithm with variance reduction and\n  its convergence analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variance reduction algorithms have recently become popular for\nminimizing the average of a large, but finite number of loss functions. The\npresent paper proposes a Riemannian stochastic quasi-Newton algorithm with\nvariance reduction (R-SQN-VR). The key challenges of averaging, adding, and\nsubtracting multiple gradients are addressed with notions of retraction and\nvector transport. We present convergence analyses of R-SQN-VR on both\nnon-convex and retraction-convex functions under retraction and vector\ntransport operators. The proposed algorithm is evaluated on the Karcher mean\ncomputation on the symmetric positive-definite manifold and the low-rank matrix\ncompletion on the Grassmann manifold. In all cases, the proposed algorithm\noutperforms the state-of-the-art Riemannian batch and stochastic gradient\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:34:39 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 15:52:38 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 09:47:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Sato", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1703.04940", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Moses Charikar and Gregory Valiant", "title": "Resilience: A Criterion for Learning in the Presence of Arbitrary\n  Outliers", "comments": "32 pages, full version of ITCS2018 paper (minor citation edit from\n  v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a criterion, resilience, which allows properties of a dataset\n(such as its mean or best low rank approximation) to be robustly computed, even\nin the presence of a large fraction of arbitrary additional data. Resilience is\na weaker condition than most other properties considered so far in the\nliterature, and yet enables robust estimation in a broader variety of settings.\nWe provide new information-theoretic results on robust distribution learning,\nrobust estimation of stochastic block models, and robust mean estimation under\nbounded $k$th moments. We also provide new algorithmic results on robust\ndistribution learning, as well as robust mean estimation in $\\ell_p$-norms.\nAmong our proof techniques is a method for pruning a high-dimensional\ndistribution with bounded $1$st moments to a stable \"core\" with bounded $2$nd\nmoments, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 05:43:48 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 07:22:21 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 03:16:54 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Charikar", "Moses", ""], ["Valiant", "Gregory", ""]]}, {"id": "1703.04943", "submitter": "Zahra Razaee", "authors": "Zahra S. Razaee, Arash A. Amini, Jingyi Jessica Li", "title": "Matched bipartite block model with covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection or clustering is a fundamental task in the analysis of\nnetwork data. Many real networks have a bipartite structure which makes\ncommunity detection challenging. In this paper, we consider a model which\nallows for matched communities in the bipartite setting, in addition to node\ncovariates with information about the matching. We derive a simple fast\nalgorithm for fitting the model based on variational inference ideas and show\nits effectiveness on both simulated and real data. A variation of the model to\nallow for degree-correction is also considered, in addition to a novel approach\nto fitting such degree-corrected models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 05:47:37 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Razaee", "Zahra S.", ""], ["Amini", "Arash A.", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "1703.04980", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Lauge S{\\o}rensen and David M. J. Tax and\n  Jesper Holst Pedersen and Marco Loog and Marleen de Bruijne", "title": "Classification of COPD with Multiple Instance Learning", "comments": "Published at International Conference on Pattern Recognition (ICPR)\n  2014", "journal-ref": null, "doi": "10.1109/ICPR.2014.268", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic obstructive pulmonary disease (COPD) is a lung disease where early\ndetection benefits the survival rate. COPD can be quantified by classifying\npatches of computed tomography images, and combining patch labels into an\noverall diagnosis for the image. As labeled patches are often not available,\nimage labels are propagated to the patches, incorrectly labeling healthy\npatches in COPD patients as being affected by the disease. We approach\nquantification of COPD from lung images as a multiple instance learning (MIL)\nproblem, which is more suitable for such weakly labeled data. We investigate\nvarious MIL assumptions in the context of COPD and show that although a concept\nregion with COPD-related disease patterns is present, considering the whole\ndistribution of lung tissue patches improves the performance. The best method\nis based on averaging instances and obtains an AUC of 0.742, which is higher\nthan the previously reported best of 0.713 on the same dataset. Using the full\ntraining set further increases performance to 0.776, which is significantly\nhigher (DeLong test) than previous results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:41:49 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["S\u00f8rensen", "Lauge", ""], ["Tax", "David M. J.", ""], ["Pedersen", "Jesper Holst", ""], ["Loog", "Marco", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1703.04981", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Annegreet van Opbroek and M. Arfan Ikram and\n  Meike W. Vernooij and Marleen de Bruijne", "title": "Transfer Learning by Asymmetric Image Weighting for Segmentation across\n  Scanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning has been very successful for automatic segmentation of\nimages from a single scanner. However, several papers report deteriorated\nperformances when using classifiers trained on images from one scanner to\nsegment images from other scanners. We propose a transfer learning classifier\nthat adapts to differences between training and test images. This method uses a\nweighted ensemble of classifiers trained on individual images. The weight of\neach classifier is determined by the similarity between its training image and\nthe test image.\n  We examine three unsupervised similarity measures, which can be used in\nscenarios where no labeled data from a newly introduced scanner or scanning\nprotocol is available. The measures are based on a divergence, a bag distance,\nand on estimating the labels with a clustering procedure. These measures are\nasymmetric. We study whether the asymmetry can improve classification. Out of\nthe three similarity measures, the bag similarity measure is the most robust\nacross different studies and achieves excellent results on four brain tissue\nsegmentation datasets and three white matter lesion segmentation datasets,\nacquired at different centers and with different scanners and scanning\nprotocols. We show that the asymmetry can indeed be informative, and that\ncomputing the similarity from the test image to the training images is more\nappropriate than the opposite direction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:43:10 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["van Opbroek", "Annegreet", ""], ["Ikram", "M. Arfan", ""], ["Vernooij", "Meike W.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1703.04986", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Lauge S{\\o}rensen and David M. J. Tax and\n  Marleen de Bruijne and Marco Loog", "title": "Label Stability in Multiple Instance Learning", "comments": "Published at MICCAI 2015", "journal-ref": null, "doi": "10.1007/978-3-319-24553-9_66", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of \\emph{instance label stability} in multiple\ninstance learning (MIL) classifiers. These classifiers are trained only on\nglobally annotated images (bags), but often can provide fine-grained\nannotations for image pixels or patches (instances). This is interesting for\ncomputer aided diagnosis (CAD) and other medical image analysis tasks for which\nonly a coarse labeling is provided. Unfortunately, the instance labels may be\nunstable. This means that a slight change in training data could potentially\nlead to abnormalities being detected in different parts of the image, which is\nundesirable from a CAD point of view. Despite MIL gaining popularity in the CAD\nliterature, this issue has not yet been addressed. We investigate the stability\nof instance labels provided by several MIL classifiers on 5 different datasets,\nof which 3 are medical image datasets (breast histopathology, diabetic\nretinopathy and computed tomography lung images). We propose an unsupervised\nmeasure to evaluate instance stability, and demonstrate that a\nperformance-stability trade-off can be made when comparing MIL classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:46:18 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cheplygina", "Veronika", ""], ["S\u00f8rensen", "Lauge", ""], ["Tax", "David M. J.", ""], ["de Bruijne", "Marleen", ""], ["Loog", "Marco", ""]]}, {"id": "1703.05060", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Petre Stoica and Thomas B. Sch\\\"on", "title": "Online Learning for Distribution-Free Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an online learning method for prediction, which is important in\nproblems with large and/or streaming data sets. We formulate the learning\napproach using a covariance-fitting methodology, and show that the resulting\npredictor has desirable computational and distribution-free properties: It is\nimplemented online with a runtime that scales linearly in the number of\nsamples; has a constant memory requirement; avoids local minima problems; and\nprunes away redundant feature dimensions without relying on restrictive\nassumptions on the data distribution. In conjunction with the split conformal\napproach, it also produces distribution-free prediction confidence intervals in\na computationally efficient manner. The method is demonstrated on both real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 10:20:32 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1703.05080", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil and Sheetal Kalyani", "title": "Tuning Free Orthogonal Matching Pursuit", "comments": "13 pages. 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal matching pursuit (OMP) is a widely used compressive sensing (CS)\nalgorithm for recovering sparse signals in noisy linear regression models. The\nperformance of OMP depends on its stopping criteria (SC). SC for OMP discussed\nin literature typically assumes knowledge of either the sparsity of the signal\nto be estimated $k_0$ or noise variance $\\sigma^2$, both of which are\nunavailable in many practical applications. In this article we develop a\nmodified version of OMP called tuning free OMP or TF-OMP which does not require\na SC. TF-OMP is proved to accomplish successful sparse recovery under the usual\nassumptions on restricted isometry constants (RIC) and mutual coherence of\ndesign matrix. TF-OMP is numerically shown to deliver a highly competitive\nperformance in comparison with OMP having \\textit{a priori} knowledge of $k_0$\nor $\\sigma^2$. Greedy algorithm for robust de-noising (GARD) is an OMP like\nalgorithm proposed for efficient estimation in classical overdetermined linear\nregression models corrupted by sparse outliers. However, GARD requires the\nknowledge of inlier noise variance which is difficult to estimate. We also\nproduce a tuning free algorithm (TF-GARD) for efficient estimation in the\npresence of sparse outliers by extending the operating principle of TF-OMP to\nGARD. TF-GARD is numerically shown to achieve a performance comparable to that\nof the existing implementation of GARD.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 11:07:13 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1703.05082", "submitter": "Fabricio Murai", "authors": "Fabricio Murai, Diogo Renn\\'o, Bruno Ribeiro, Gisele L. Pappa, Don\n  Towsley, Krista Gile", "title": "Selective Harvesting over Networks", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active search (AS) on graphs focuses on collecting certain labeled nodes\n(targets) given global knowledge of the network topology and its edge weights\nunder a query budget. However, in most networks, nodes, topology and edge\nweights are all initially unknown. We introduce selective harvesting, a variant\nof AS where the next node to be queried must be chosen among the neighbors of\nthe current queried node set; the available training data for deciding which\nnode to query is restricted to the subgraph induced by the queried set (and\ntheir node attributes) and their neighbors (without any node or edge\nattributes). Therefore, selective harvesting is a sequential decision problem,\nwhere we must decide which node to query at each step. A classifier trained in\nthis scenario suffers from a tunnel vision effect: without recourse to\nindependent sampling, the urge to query promising nodes forces classifiers to\ngather increasingly biased training data, which we show significantly hurts the\nperformance of AS methods and standard classifiers. We find that it is possible\nto collect a much larger set of targets by using multiple classifiers, not by\ncombining their predictions as an ensemble, but switching between classifiers\nused at each step, as a way to ease the tunnel vision effect. We discover that\nswitching classifiers collects more targets by (a) diversifying the training\ndata and (b) broadening the choices of nodes that can be queried next. This\nhighlights an exploration, exploitation, and diversification trade-off in our\nproblem that goes beyond the exploration and exploitation duality found in\nclassic sequential decision problems. From these observations we propose D3TS,\na method based on multi-armed bandits for non-stationary stochastic processes\nthat enforces classifier diversity, matching or exceeding the performance of\ncompeting methods on seven real network datasets in our evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 11:17:02 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Murai", "Fabricio", ""], ["Renn\u00f3", "Diogo", ""], ["Ribeiro", "Bruno", ""], ["Pappa", "Gisele L.", ""], ["Towsley", "Don", ""], ["Gile", "Krista", ""]]}, {"id": "1703.05160", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anshumali Shrivastava", "title": "A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators\n  for Partition Function Computation in Log-Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are arguably the most successful class of graphical models\nfor large-scale applications because of their simplicity and tractability.\nLearning and inference with these models require calculating the partition\nfunction, which is a major bottleneck and intractable for large state spaces.\nImportance Sampling (IS) and MCMC-based approaches are lucrative. However, the\ncondition of having a \"good\" proposal distribution is often not satisfied in\npractice.\n  In this paper, we add a new dimension to efficient estimation via sampling.\nWe propose a new sampling scheme and an unbiased estimator that estimates the\npartition function accurately in sub-linear time. Our samples are generated in\nnear-constant time using locality sensitive hashing (LSH), and so are\ncorrelated and unnormalized. We demonstrate the effectiveness of our proposed\napproach by comparing the accuracy and speed of estimating the partition\nfunction against other state-of-the-art estimation techniques including IS and\nthe efficient variant of Gumbel-Max sampling. With our efficient sampling\nscheme, we accurately train real-world language models using only 1-2% of\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:01:21 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Spring", "Ryan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1703.05175", "submitter": "Jake Snell", "authors": "Jake Snell, Kevin Swersky, Richard S. Zemel", "title": "Prototypical Networks for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose prototypical networks for the problem of few-shot classification,\nwhere a classifier must generalize to new classes not seen in the training set,\ngiven only a small number of examples of each new class. Prototypical networks\nlearn a metric space in which classification can be performed by computing\ndistances to prototype representations of each class. Compared to recent\napproaches for few-shot learning, they reflect a simpler inductive bias that is\nbeneficial in this limited-data regime, and achieve excellent results. We\nprovide an analysis showing that some simple design decisions can yield\nsubstantial improvements over recent approaches involving complicated\narchitectural choices and meta-learning. We further extend prototypical\nnetworks to zero-shot learning and achieve state-of-the-art results on the\nCU-Birds dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:31:55 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 22:48:54 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Snell", "Jake", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1703.05189", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Filip Tronarp, Toni Karvonen, Simo S\\\"arkk\\\"a and\n  Ond\\v{r}ej Straka", "title": "Student-t Process Quadratures for Filtering of Non-Linear Systems with\n  Heavy-Tailed Noise", "comments": "15 pages, 3 figures, submitted to 20th International Conference on\n  Information Fusion, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to design a moment transformation for Student- t\ndistributed random variables, which is able to account for the error in the\nnumerically computed mean. We employ Student-t process quadrature, an instance\nof Bayesian quadrature, which allows us to treat the integral itself as a\nrandom variable whose variance provides information about the incurred\nintegration error. Advantage of the Student- t process quadrature over the\ntraditional Gaussian process quadrature, is that the integral variance depends\nalso on the function values, allowing for a more robust modelling of the\nintegration error. The moment transform is applied in nonlinear sigma-point\nfiltering and evaluated on two numerical examples, where it is shown to\noutperform the state-of-the-art moment transforms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:47:02 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 09:36:40 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Tronarp", "Filip", ""], ["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Straka", "Ond\u0159ej", ""]]}, {"id": "1703.05298", "submitter": "Dario Zanca", "authors": "Francesco Giannini, Vincenzo Laveglia, Alessandro Rossi, Dario Zanca,\n  Andrea Zugarini", "title": "Neural Networks for Beginners. A fast implementation in Matlab, Torch,\n  TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an introduction to some Machine Learning tools within\nthe most common development environments. It mainly focuses on practical\nproblems, skipping any theoretical introduction. It is oriented to both\nstudents trying to approach Machine Learning and experts looking for new\nframeworks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 18:01:20 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 08:32:19 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Giannini", "Francesco", ""], ["Laveglia", "Vincenzo", ""], ["Rossi", "Alessandro", ""], ["Zanca", "Dario", ""], ["Zugarini", "Andrea", ""]]}, {"id": "1703.05411", "submitter": "Tien Thanh Nguyen", "authors": "Tien Thanh Nguyen, Xuan Cuong Pham, Alan Wee-Chung Liew, Witold\n  Pedrycz", "title": "Aggregation of Classifiers: A Justifiable Information Granularity\n  Approach", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a new approach to combine multi-classifiers in an\nensemble system. Instead of using numeric membership values encountered in\nfixed combining rules, we construct interval membership values associated with\neach class prediction at the level of meta-data of observation by using\nconcepts of information granules. In the proposed method, uncertainty\n(diversity) of findings produced by the base classifiers is quantified by\ninterval-based information granules. The discriminative decision model is\ngenerated by considering both the bounds and the length of the obtained\nintervals. We select ten and then fifteen learning algorithms to build a\nheterogeneous ensemble system and then conducted the experiment on a number of\nUCI datasets. The experimental results demonstrate that the proposed approach\nperforms better than the benchmark algorithms including six fixed combining\nmethods, one trainable combining method, AdaBoost, Bagging, and Random\nSubspace.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 22:48:05 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Nguyen", "Tien Thanh", ""], ["Pham", "Xuan Cuong", ""], ["Liew", "Alan Wee-Chung", ""], ["Pedrycz", "Witold", ""]]}, {"id": "1703.05430", "submitter": "Bangalore Ravi Kiran", "authors": "Kiran Bangalore Ravi, Jean Serra", "title": "Cost-complexity pruning of random forests", "comments": "Previous version in proceedings of ISMM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests perform bootstrap-aggregation by sampling the training samples\nwith replacement. This enables the evaluation of out-of-bag error which serves\nas a internal cross-validation mechanism. Our motivation lies in using the\nunsampled training samples to improve each decision tree in the ensemble. We\nstudy the effect of using the out-of-bag samples to improve the generalization\nerror first of the decision trees and second the random forest by post-pruning.\nA preliminary empirical study on four UCI repository datasets show consistent\ndecrease in the size of the forests without considerable loss in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 23:58:19 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 16:28:28 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Ravi", "Kiran Bangalore", ""], ["Serra", "Jean", ""]]}, {"id": "1703.05449", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Ian Osband and R\\'emi Munos", "title": "Minimax Regret Bounds for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of provably optimal exploration in reinforcement\nlearning for finite horizon MDPs. We show that an optimistic modification to\nvalue iteration achieves a regret bound of $\\tilde{O}( \\sqrt{HSAT} +\nH^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states,\n$A$ the number of actions and $T$ the number of time-steps. This result\nimproves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved\nby the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new\nresults is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of\n$\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bound of\n$\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key\ninsights. We use careful application of concentration inequalities to the\noptimal value function as a whole, rather than to the transitions probabilities\n(to improve scaling in $S$), and we define Bernstein-based \"exploration\nbonuses\" that use the empirical variance of the estimated values at the next\nstates (to improve scaling in $H$).\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 01:31:33 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 13:00:06 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Osband", "Ian", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1703.05452", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Jean-Michel Renders, Morteza Haghir Chehreghani, Andreas\n  Krause", "title": "Efficient Online Learning for Optimizing Value of Information: Theory\n  and Application to Interactive Troubleshooting", "comments": "18 pages, 6 figures, to appear in the Conference on Uncertainty in\n  Artificial Intelligence (UAI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal value of information (VoI) problem, where the goal is\nto sequentially select a set of tests with a minimal cost, so that one can\nefficiently make the best decision based on the observed outcomes. Existing\nalgorithms are either heuristics with no guarantees, or scale poorly (with\nexponential run time in terms of the number of available tests). Moreover,\nthese methods assume a known distribution over the test outcomes, which is\noften not the case in practice. We propose an efficient sampling-based online\nlearning framework to address the above issues. First, assuming the\ndistribution over hypotheses is known, we propose a dynamic hypothesis\nenumeration strategy, which allows efficient information gathering with strong\ntheoretical guarantees. We show that with sufficient amount of samples, one can\nidentify a near-optimal decision with high probability. Second, when the\nparameters of the hypotheses distribution are unknown, we propose an algorithm\nwhich learns the parameters progressively via posterior sampling in an online\nfashion. We further establish a rigorous bound on the expected regret. We\ndemonstrate the effectiveness of our approach on a real-world interactive\ntroubleshooting application and show that one can efficiently make high-quality\ndecisions with low cost.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 01:37:25 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 16:53:10 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Chen", "Yuxin", ""], ["Renders", "Jean-Michel", ""], ["Chehreghani", "Morteza Haghir", ""], ["Krause", "Andreas", ""]]}, {"id": "1703.05537", "submitter": "Francesco Orsini", "authors": "Francesco Orsini, Daniele Baracchi and Paolo Frasconi", "title": "Shift Aggregate Extract Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an architecture based on deep hierarchical decompositions to\nlearn effective representations of large graphs. Our framework extends classic\nR-decompositions used in kernel methods, enabling nested \"part-of-part\"\nrelations. Unlike recursive neural networks, which unroll a template on input\ngraphs directly, we unroll a neural network template over the decomposition\nhierarchy, allowing us to deal with the high degree variability that typically\ncharacterize social network graphs. Deep hierarchical decompositions are also\namenable to domain compression, a technique that reduces both space and time\ncomplexity by exploiting symmetries. We show empirically that our approach is\ncompetitive with current state-of-the-art graph classification methods,\nparticularly when dealing with social network datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 09:52:48 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Orsini", "Francesco", ""], ["Baracchi", "Daniele", ""], ["Frasconi", "Paolo", ""]]}, {"id": "1703.05667", "submitter": "David Belanger", "authors": "David Belanger, Bishan Yang, Andrew McCallum", "title": "End-to-End Learning for Structured Prediction Energy Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Prediction Energy Networks (SPENs) are a simple, yet expressive\nfamily of structured prediction models (Belanger and McCallum, 2016). An energy\nfunction over candidate structured outputs is given by a deep network, and\npredictions are formed by gradient-based optimization. This paper presents\nend-to-end learning for SPENs, where the energy function is discriminatively\ntrained by back-propagating through gradient-based prediction. In our\nexperience, the approach is substantially more accurate than the structured SVM\nmethod of Belanger and McCallum (2016), as it allows us to use more\nsophisticated non-convex energies. We provide a collection of techniques for\nimproving the speed, accuracy, and memory requirements of end-to-end SPENs, and\ndemonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005\nsemantic role labeling tasks. In both, inexact minimization of non-convex SPEN\nenergies is superior to baseline methods that use simplistic energy functions\nthat can be minimized exactly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 15:14:48 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 04:50:13 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Belanger", "David", ""], ["Yang", "Bishan", ""], ["McCallum", "Andrew", ""]]}, {"id": "1703.05687", "submitter": "Robert Richardson", "authors": "Robert R. Richardson, Michael A. Osborne and David A. Howey", "title": "Gaussian process regression for forecasting battery state of health", "comments": "13 pages, 7 figures, published in the Journal of Power Sources, 2017", "journal-ref": "Journal of Power Sources, Volume 357, 31 July 2017, Pages 209 to\n  219", "doi": "10.1016/j.jpowsour.2017.05.004", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting the future capacity and remaining useful life of\nbatteries is necessary to ensure reliable system operation and to minimise\nmaintenance costs. The complex nature of battery degradation has meant that\nmechanistic modelling of capacity fade has thus far remained intractable;\nhowever, with the advent of cloud-connected devices, data from cells in various\napplications is becoming increasingly available, and the feasibility of\ndata-driven methods for battery prognostics is increasing. Here we propose\nGaussian process (GP) regression for forecasting battery state of health, and\nhighlight various advantages of GPs over other data-driven and mechanistic\napproaches. GPs are a type of Bayesian non-parametric method, and hence can\nmodel complex systems whilst handling uncertainty in a principled manner. Prior\ninformation can be exploited by GPs in a variety of ways: explicit mean\nfunctions can be used if the functional form of the underlying degradation\nmodel is available, and multiple-output GPs can effectively exploit\ncorrelations between data from different cells. We demonstrate the predictive\ncapability of GPs for short-term and long-term (remaining useful life)\nforecasting on a selection of capacity vs. cycle datasets from lithium-ion\ncells.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:00:00 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:23:37 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Richardson", "Robert R.", ""], ["Osborne", "Michael A.", ""], ["Howey", "David A.", ""]]}, {"id": "1703.05785", "submitter": "Paris Giampouras", "authors": "Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos D.\n  Koutroumbas", "title": "Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and\n  Blind Unmixing of Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the number of endmembers existing in a scene constitutes a\ncritical task in the hyperspectral unmixing process. The accuracy of this\nestimate plays a crucial role in subsequent unsupervised unmixing steps i.e.,\nthe derivation of the spectral signatures of the endmembers (endmembers'\nextraction) and the estimation of the abundance fractions of the pixels. A\ncommon practice amply followed in literature is to treat endmembers' number\nestimation and unmixing, independently as two separate tasks, providing the\noutcome of the former as input to the latter. In this paper, we go beyond this\ncomputationally demanding strategy. More precisely, we set forth a multiple\nconstrained optimization framework, which encapsulates endmembers' number\nestimation and unsupervised unmixing in a single task. This is attained by\nsuitably formulating the problem via a low-rank and sparse nonnegative matrix\nfactorization rationale, where low-rankness is promoted with the use of a\nsophisticated $\\ell_2/\\ell_1$ norm penalty term. An alternating proximal\nalgorithm is then proposed for minimizing the emerging cost function. The\nresults obtained by simulated and real data experiments verify the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 18:25:21 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Giampouras", "Paris V.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Koutroumbas", "Konstantinos D.", ""]]}, {"id": "1703.05840", "submitter": "Yi Zhou", "authors": "Guanghui Lan, Sebastian Pokutta, Yi Zhou and Daniel Zink", "title": "Conditional Accelerated Lazy Stochastic Gradient Descent", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a conditional accelerated lazy stochastic gradient\ndescent algorithm with optimal number of calls to a stochastic first-order\noracle and convergence rate $O\\left(\\frac{1}{\\varepsilon^2}\\right)$ improving\nover the projection-free, Online Frank-Wolfe based stochastic gradient descent\nof Hazan and Kale [2012] with convergence rate\n$O\\left(\\frac{1}{\\varepsilon^4}\\right)$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 22:15:17 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 20:28:16 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 13:24:06 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 16:43:11 GMT"}, {"version": "v5", "created": "Thu, 15 Feb 2018 23:36:11 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Lan", "Guanghui", ""], ["Pokutta", "Sebastian", ""], ["Zhou", "Yi", ""], ["Zink", "Daniel", ""]]}, {"id": "1703.05841", "submitter": "Andrea Locatelli", "authors": "Andrea Locatelli, Alexandra Carpentier and Samory Kpotufe", "title": "Adaptivity to Noise Parameters in Nonparametric Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses various open questions in the theory of active learning\nfor nonparametric classification. Our contributions are both statistical and\nalgorithmic: -We establish new minimax-rates for active learning under common\n\\textit{noise conditions}. These rates display interesting transitions -- due\nto the interaction between noise \\textit{smoothness and margin} -- not present\nin the passive setting. Some such transitions were previously conjectured, but\nremained unconfirmed. -We present a generic algorithmic strategy for adaptivity\nto unknown noise smoothness and margin; our strategy achieves optimal rates in\nmany general situations; furthermore, unlike in previous work, we avoid the\nneed for \\textit{adaptive confidence sets}, resulting in strictly milder\ndistributional requirements.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 22:37:55 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Locatelli", "Andrea", ""], ["Carpentier", "Alexandra", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1703.05849", "submitter": "Marc Ratkovic", "authors": "Marc Ratkovic and Dustin Tingley", "title": "Estimation and Inference on Nonlinear and Heterogeneous Effects", "comments": "Unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple regression has been the go-to method for data analysis for\ngenerations of scholars due to its transparency, interpretability, and\ndesirable theoretical properties. However, the method's simplicity precludes\nthe discovery of complex heterogeneities in the data. We introduce the Method\nof Direct Estimation and Inference (MDEI) that embraces these potential\ncomplexities, is interpretable, has desirable theoretical guarantees, and,\nunlike some existing methods, returns appropriate uncertainty estimates. The\nproposed method uses a machine learning regression methodology to estimate the\nobservation-level effect of a treatment variable. Importantly, we introduce a\nrobust approach to uncertainty estimates. We provide simulation evidence and an\napplication illustrating the performance of the method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 23:33:24 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 20:29:07 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 20:30:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ratkovic", "Marc", ""], ["Tingley", "Dustin", ""]]}, {"id": "1703.06043", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Sebastian Schmitt, Johann Kl\\\"ahn, David\n  St\\\"ockel, Anna Schroeder, Guillaume Bellec, Johannes Bill, Oliver\n  Breitwieser, Ilja Bytschok, Andreas Gr\\\"ubl, Maurice G\\\"uttler, Andreas\n  Hartel, Stephan Hartmann, Dan Husmann, Kai Husmann, Sebastian Jeltsch, Vitali\n  Karasenko, Mitja Kleider, Christoph Koke, Alexander Kononov, Christian Mauch,\n  Eric M\\\"uller, Paul M\\\"uller, Johannes Partzsch, Thomas Pfeil, Stefan\n  Schiefer, Stefan Scholze, Anand Subramoney, Vasilis Thanasoulis, Bernhard\n  Vogginger, Robert Legenstein, Wolfgang Maass, Ren\\'e Sch\\\"uffny, Christian\n  Mayr, Johannes Schemmel, Karlheinz Meier", "title": "Pattern representation and recognition with accelerated analog\n  neuromorphic systems", "comments": "accepted at ISCAS 2017", "journal-ref": "Circuits and Systems (ISCAS), 2017 IEEE International Symposium on", "doi": "10.1109/ISCAS.2017.8050530", "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being originally inspired by the central nervous system, artificial\nneural networks have diverged from their biological archetypes as they have\nbeen remodeled to fit particular tasks. In this paper, we review several\npossibilites to reverse map these architectures to biologically more realistic\nspiking networks with the aim of emulating them on fast, low-power neuromorphic\nhardware. Since many of these devices employ analog components, which cannot be\nperfectly controlled, finding ways to compensate for the resulting effects\nrepresents a key challenge. Here, we discuss three different strategies to\naddress this problem: the addition of auxiliary network components for\nstabilizing activity, the utilization of inherently robust architectures and a\ntraining method for hardware-emulated networks that functions without perfect\nknowledge of the system's dynamics and parameters. For all three scenarios, we\ncorroborate our theoretical considerations with experimental results on\naccelerated analog neuromorphic platforms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 14:59:17 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 15:16:43 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Schmitt", "Sebastian", ""], ["Kl\u00e4hn", "Johann", ""], ["St\u00f6ckel", "David", ""], ["Schroeder", "Anna", ""], ["Bellec", "Guillaume", ""], ["Bill", "Johannes", ""], ["Breitwieser", "Oliver", ""], ["Bytschok", "Ilja", ""], ["Gr\u00fcbl", "Andreas", ""], ["G\u00fcttler", "Maurice", ""], ["Hartel", "Andreas", ""], ["Hartmann", "Stephan", ""], ["Husmann", "Dan", ""], ["Husmann", "Kai", ""], ["Jeltsch", "Sebastian", ""], ["Karasenko", "Vitali", ""], ["Kleider", "Mitja", ""], ["Koke", "Christoph", ""], ["Kononov", "Alexander", ""], ["Mauch", "Christian", ""], ["M\u00fcller", "Eric", ""], ["M\u00fcller", "Paul", ""], ["Partzsch", "Johannes", ""], ["Pfeil", "Thomas", ""], ["Schiefer", "Stefan", ""], ["Scholze", "Stefan", ""], ["Subramoney", "Anand", ""], ["Thanasoulis", "Vasilis", ""], ["Vogginger", "Bernhard", ""], ["Legenstein", "Robert", ""], ["Maass", "Wolfgang", ""], ["Sch\u00fcffny", "Ren\u00e9", ""], ["Mayr", "Christian", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1703.06065", "submitter": "Urvashi Oswal", "authors": "Urvashi Oswal, Swayambhoo Jain, Kevin S. Xu, and Brian Eriksson", "title": "Block CUR: Decomposing Matrices using Groups of Columns", "comments": "shorter version to appear in ECML-PKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in large-scale data analysis is to approximate a matrix\nusing a combination of specifically sampled rows and columns, known as CUR\ndecomposition. Unfortunately, in many real-world environments, the ability to\nsample specific individual rows or columns of the matrix is limited by either\nsystem constraints or cost. In this paper, we consider matrix approximation by\nsampling predefined \\emph{blocks} of columns (or rows) from the matrix. We\npresent an algorithm for sampling useful column blocks and provide novel\nguarantees for the quality of the approximation. This algorithm has application\nin problems as diverse as biometric data analysis to distributed computing. We\ndemonstrate the effectiveness of the proposed algorithms for computing the\nBlock CUR decomposition of large matrices in a distributed setting with\nmultiple nodes in a compute cluster, where such blocks correspond to columns\n(or rows) of the matrix stored on the same node, which can be retrieved with\nmuch less overhead than retrieving individual columns stored across different\nnodes. In the biometric setting, the rows correspond to different users and\ncolumns correspond to users' biometric reaction to external stimuli, {\\em\ne.g.,}~watching video content, at a particular time instant. There is\nsignificant cost in acquiring each user's reaction to lengthy content so we\nsample a few important scenes to approximate the biometric response. An\nindividual time sample in this use case cannot be queried in isolation due to\nthe lack of context that caused that biometric reaction. Instead, collections\nof time segments ({\\em i.e.,} blocks) must be presented to the user. The\npractical application of these algorithms is shown via experimental results\nusing real-world user biometric data from a content testing environment.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 16:08:23 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 14:27:52 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Oswal", "Urvashi", ""], ["Jain", "Swayambhoo", ""], ["Xu", "Kevin S.", ""], ["Eriksson", "Brian", ""]]}, {"id": "1703.06103", "submitter": "Thomas Kipf", "authors": "Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den\n  Berg, Ivan Titov, Max Welling", "title": "Modeling Relational Data with Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs enable a wide variety of applications, including question\nanswering and information retrieval. Despite the great effort invested in their\ncreation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)\nremain incomplete. We introduce Relational Graph Convolutional Networks\n(R-GCNs) and apply them to two standard knowledge base completion tasks: Link\nprediction (recovery of missing facts, i.e. subject-predicate-object triples)\nand entity classification (recovery of missing entity attributes). R-GCNs are\nrelated to a recent class of neural networks operating on graphs, and are\ndeveloped specifically to deal with the highly multi-relational data\ncharacteristic of realistic knowledge bases. We demonstrate the effectiveness\nof R-GCNs as a stand-alone model for entity classification. We further show\nthat factorization models for link prediction such as DistMult can be\nsignificantly improved by enriching them with an encoder model to accumulate\nevidence over multiple inference steps in the relational graph, demonstrating a\nlarge improvement of 29.8% on FB15k-237 over a decoder-only baseline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:09:14 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 13:43:41 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 15:49:12 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 19:53:49 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Schlichtkrull", "Michael", ""], ["Kipf", "Thomas N.", ""], ["Bloem", "Peter", ""], ["Berg", "Rianne van den", ""], ["Titov", "Ivan", ""], ["Welling", "Max", ""]]}, {"id": "1703.06104", "submitter": "Shuang Qiu", "authors": "Shuang Qiu and Tingjin Luo and Jieping Ye and Ming Lin", "title": "Nonconvex One-bit Single-label Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extreme scenario in multi-label learning where each training\ninstance is endowed with a single one-bit label out of multiple labels. We\nformulate this problem as a non-trivial special case of one-bit rank-one matrix\nsensing and develop an efficient non-convex algorithm based on alternating\npower iteration. The proposed algorithm is able to recover the underlying\nlow-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$\nfeatures and $d_2$ classes, the proposed algorithm achieves $O(\\epsilon)$\nrecovery error after retrieving $O(k^{1.5}d_1 d_2/\\epsilon)$ one-bit labels\nwithin $O(kd)$ memory. Our bound is nearly optimal in the order of\n$O(1/\\epsilon)$. This significantly improves the state-of-the-art sampling\ncomplexity of one-bit multi-label learning. We perform experiments to verify\nour theory and evaluate the performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:09:15 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Qiu", "Shuang", ""], ["Luo", "Tingjin", ""], ["Ye", "Jieping", ""], ["Lin", "Ming", ""]]}, {"id": "1703.06114", "submitter": "Manzil Zaheer", "authors": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos,\n  Ruslan Salakhutdinov, Alexander Smola", "title": "Deep Sets", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of designing models for machine learning tasks defined\non \\emph{sets}. In contrast to traditional approach of operating on fixed\ndimensional vectors, we consider objective functions defined on sets that are\ninvariant to permutations. Such problems are widespread, ranging from\nestimation of population statistics \\cite{poczos13aistats}, to anomaly\ndetection in piezometer data of embankment dams \\cite{Jung15Exploration}, to\ncosmology \\cite{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem\ncharacterizes the permutation invariant functions and provides a family of\nfunctions to which any permutation invariant objective function must belong.\nThis family of functions has a special structure which enables us to design a\ndeep network architecture that can operate on sets and which can be deployed on\na variety of scenarios including both unsupervised and supervised learning\ntasks. We also derive the necessary and sufficient conditions for permutation\nequivariance in deep models. We demonstrate the applicability of our method on\npopulation statistic estimation, point cloud classification, set expansion, and\noutlier detection.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 21:02:53 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 16:04:56 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 18:54:19 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Zaheer", "Manzil", ""], ["Kottur", "Satwik", ""], ["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""], ["Salakhutdinov", "Ruslan", ""], ["Smola", "Alexander", ""]]}, {"id": "1703.06131", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Daniele Bigoni, Youssef Marzouk", "title": "Inference via low-dimensional couplings", "comments": "78 pages, 25 figures", "journal-ref": "Journal of Machine Learning Research, volume 19 (66): 1-71, 2018", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the low-dimensional structure of deterministic transformations\nbetween random variables, i.e., transport maps between probability measures. In\nthe context of statistics and machine learning, these transformations can be\nused to couple a tractable \"reference\" measure (e.g., a standard Gaussian) with\na target measure of interest. Direct simulation from the desired measure can\nthen be achieved by pushing forward reference samples through the map. Yet\ncharacterizing such a map---e.g., representing and evaluating it---grows\nchallenging in high dimensions. The central contribution of this paper is to\nestablish a link between the Markov properties of the target measure and the\nexistence of low-dimensional couplings, induced by transport maps that are\nsparse and/or decomposable. Our analysis not only facilitates the construction\nof transformations in high-dimensional settings, but also suggests new\ninference methodologies for continuous non-Gaussian graphical models. For\ninstance, in the context of nonlinear state-space models, we describe new\nvariational algorithms for filtering, smoothing, and sequential parameter\ninference. These algorithms can be understood as the natural\ngeneralization---to the non-Gaussian case---of the square-root\nRauch-Tung-Striebel Gaussian smoother.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:50:44 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 13:07:43 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 15:41:07 GMT"}, {"version": "v4", "created": "Sun, 1 Jul 2018 23:28:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Spantini", "Alessio", ""], ["Bigoni", "Daniele", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1703.06177", "submitter": "Chengan Du", "authors": "Chengan Du, Yunpeng Zhao and Feng Wang", "title": "On Consistency of Graph-based Semi-supervised Learning", "comments": "This paper is accepted by 2019 IEEE 39th International Conference on\n  Distributed Computing Systems (ICDCS)", "journal-ref": "IEEE 39th International Conference on Distributed Computing\n  Systems (ICDCS) 2019", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based semi-supervised learning is one of the most popular methods in\nmachine learning. Some of its theoretical properties such as bounds for the\ngeneralization error and the convergence of the graph Laplacian regularizer\nhave been studied in computer science and statistics literatures. However, a\nfundamental statistical property, the consistency of the estimator from this\nmethod has not been proved. In this article, we study the consistency problem\nunder a non-parametric framework. We prove the consistency of graph-based\nlearning in the case that the estimated scores are enforced to be equal to the\nobserved responses for the labeled data. The sample sizes of both labeled and\nunlabeled data are allowed to grow in this result. When the estimated scores\nare not required to be equal to the observed responses, a tuning parameter is\nused to balance the loss function and the graph Laplacian regularizer. We give\na counterexample demonstrating that the estimator for this case can be\ninconsistent. The theoretical findings are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 19:24:09 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 20:10:48 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Du", "Chengan", ""], ["Zhao", "Yunpeng", ""], ["Wang", "Feng", ""]]}, {"id": "1703.06217", "submitter": "Mason McGill", "authors": "Mason McGill and Pietro Perona", "title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks", "comments": "ICML 2017. Code at https://github.com/MasonMcGill/multipath-nn Video\n  abstract at https://youtu.be/NHQsDaycwyQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and systematically evaluate three strategies for training\ndynamically-routed artificial neural networks: graphs of learned\ntransformations through which different input signals may take different paths.\nThough some approaches have advantages over others, the resulting networks are\noften qualitatively similar. We find that, in dynamically-routed networks\ntrained to classify images, layers and branches become specialized to process\ndistinct categories of images. Additionally, given a fixed computational\nbudget, dynamically-routed networks tend to perform better than comparable\nstatically-routed networks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 23:52:14 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 22:14:36 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["McGill", "Mason", ""], ["Perona", "Pietro", ""]]}, {"id": "1703.06222", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Rina Foygel Barber, Martin J. Wainwright, Michael I.\n  Jordan", "title": "A unified treatment of multiple testing with prior knowledge using the\n  p-filter", "comments": "36 pages, 1 figure, accepted for publication at the Annals of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:08:59 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 01:30:58 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 06:10:07 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 20:17:20 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 07:25:11 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Barber", "Rina Foygel", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1703.06229", "submitter": "Jacopo Cavazza", "authors": "Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal, Vittorio\n  Murino", "title": "Curriculum Dropout", "comments": "Accepted at ICCV (International Conference on Computer Vision) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a very effective way of regularizing neural networks.\nStochastically \"dropping out\" units with a certain probability discourages\nover-specific co-adaptations of feature detectors, preventing overfitting and\nimproving network generalization. Besides, Dropout can be interpreted as an\napproximate model aggregation technique, where an exponential number of smaller\nnetworks are averaged in order to get a more powerful ensemble. In this paper,\nwe show that using a fixed dropout probability during training is a suboptimal\nchoice. We thus propose a time scheduling for the probability of retaining\nneurons in the network. This induces an adaptive regularization scheme that\nsmoothly increases the difficulty of the optimization problem. This idea of\n\"starting easy\" and adaptively increasing the difficulty of the learning\nproblem has its roots in curriculum learning and allows one to train better\nmodels. Indeed, we prove that our optimization strategy implements a very\ngeneral curriculum scheme, by gradually adding noise to both the input and\nintermediate feature representations within the network architecture.\nExperiments on seven image classification datasets and different network\narchitectures show that our method, named Curriculum Dropout, frequently yields\nto better generalization and, at worst, performs just as well as the standard\nDropout method.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:59:40 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 06:27:09 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Morerio", "Pietro", ""], ["Cavazza", "Jacopo", ""], ["Volpi", "Riccardo", ""], ["Vidal", "Rene", ""], ["Murino", "Vittorio", ""]]}, {"id": "1703.06240", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, Barnabas\n  Poczos", "title": "Multi-fidelity Bayesian Optimisation with Continuous Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit methods for black-box optimisation, such as Bayesian optimisation, are\nused in a variety of applications including hyper-parameter tuning and\nexperiment design. Recently, \\emph{multi-fidelity} methods have garnered\nconsiderable attention since function evaluations have become increasingly\nexpensive in such applications. Multi-fidelity methods use cheap approximations\nto the function of interest to speed up the overall optimisation process.\nHowever, most multi-fidelity methods assume only a finite number of\napproximations. In many practical applications however, a continuous spectrum\nof approximations might be available. For instance, when tuning an expensive\nneural network, one might choose to approximate the cross validation\nperformance using less data $N$ and/or few training iterations $T$. Here, the\napproximations are best viewed as arising out of a continuous two dimensional\nspace $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA,\nfor this setting. We characterise its theoretical properties and show that it\nachieves better regret than than strategies which ignore the approximations.\nBOCA outperforms several other baselines in synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 03:28:40 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Dasarathy", "Gautam", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1703.06270", "submitter": "Ramin M. Hasani", "authors": "Ramin M. Hasani, Victoria Beneder, Magdalena Fuchs, David Lung and\n  Radu Grosu", "title": "SIM-CE: An Advanced Simulink Platform for Studying the Brain of\n  Caenorhabditis elegans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SIM-CE, an advanced, user-friendly modeling and simulation\nenvironment in Simulink for performing multi-scale behavioral analysis of the\nnervous system of Caenorhabditis elegans (C. elegans). SIM-CE contains an\nimplementation of the mathematical models of C. elegans's neurons and synapses,\nin Simulink, which can be easily extended and particularized by the user. The\nSimulink model is able to capture both complex dynamics of ion channels and\nadditional biophysical detail such as intracellular calcium concentration. We\ndemonstrate the performance of SIM-CE by carrying out neuronal, synaptic and\nneural-circuit-level behavioral simulations. Such environment enables the user\nto capture unknown properties of the neural circuits, test hypotheses and\ndetermine the origin of many behavioral plasticities exhibited by the worm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 08:27:42 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 15:49:11 GMT"}, {"version": "v3", "created": "Sat, 25 Mar 2017 13:19:14 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Hasani", "Ramin M.", ""], ["Beneder", "Victoria", ""], ["Fuchs", "Magdalena", ""], ["Lung", "David", ""], ["Grosu", "Radu", ""]]}, {"id": "1703.06272", "submitter": "Ramin M. Hasani", "authors": "Ramin M. Hasani, Guodong Wang and Radu Grosu", "title": "An Automated Auto-encoder Correlation-based Health-Monitoring and\n  Prognostic Method for Machine Bearings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an intelligent ultimate technique for health-monitoring\nand prognostic of common rotary machine components, particularly bearings.\nDuring a run-to-failure experiment, rich unsupervised features from vibration\nsensory data are extracted by a trained sparse auto-encoder. Then, the\ncorrelation of the extracted attributes of the initial samples (presumably\nhealthy at the beginning of the test) with the succeeding samples is calculated\nand passed through a moving-average filter. The normalized output is named\nauto-encoder correlation-based (AEC) rate which stands for an informative\nattribute of the system depicting its health status and precisely identifying\nthe degradation starting point. We show that AEC technique well-generalizes in\nseveral run-to-failure tests. AEC collects rich unsupervised features form the\nvibration data fully autonomous. We demonstrate the superiority of the AEC over\nmany other state-of-the-art approaches for the health monitoring and prognostic\nof machine bearings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 08:38:51 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Hasani", "Ramin M.", ""], ["Wang", "Guodong", ""], ["Grosu", "Radu", ""]]}, {"id": "1703.06324", "submitter": "Biswa Sengupta", "authors": "B Sengupta and E Vasquez and Y Qian", "title": "Deep Tensor Encoding", "comments": "KDD Workshop on ML meets Fashion 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an encoding of feature vectors in terms of an over-complete\ndictionary or a information geometric (Fisher vectors) construct is wide-spread\nin statistical signal processing and computer vision. In content based\ninformation retrieval using deep-learning classifiers, such encodings are\nlearnt on the flattened last layer, without adherence to the multi-linear\nstructure of the underlying feature tensor. We illustrate a variety of feature\nencodings incl. sparse dictionary coding and Fisher vectors along with\nproposing that a structured tensor factorization scheme enables us to perform\nretrieval that can be at par, in terms of average precision, with Fisher vector\nencoded image signatures. In short, we illustrate how structural constraints\nincrease retrieval fidelity.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 17:49:42 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:08:48 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sengupta", "B", ""], ["Vasquez", "E", ""], ["Qian", "Y", ""]]}, {"id": "1703.06327", "submitter": "Sewoong Oh", "authors": "Ashish Khetan, Sewoong Oh", "title": "Spectrum Estimation from a Few Entries", "comments": "52 pages; 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular values of a data in a matrix form provide insights on the structure\nof the data, the effective dimensionality, and the choice of hyper-parameters\non higher-level data analysis tools. However, in many practical applications\nsuch as collaborative filtering and network analysis, we only get a partial\nobservation. Under such scenarios, we consider the fundamental problem of\nrecovering spectral properties of the underlying matrix from a sampling of its\nentries. We are particularly interested in directly recovering the spectrum,\nwhich is the set of singular values, and also in sample-efficient approaches\nfor recovering a spectral sum function, which is an aggregate sum of the same\nfunction applied to each of the singular values. We propose first estimating\nthe Schatten $k$-norms of a matrix, and then applying Chebyshev approximation\nto the spectral sum function or applying moment matching in Wasserstein\ndistance to recover the singular values. The main technical challenge is in\naccurately estimating the Schatten norms from a sampling of a matrix. We\nintroduce a novel unbiased estimator based on counting small structures in a\ngraph and provide guarantees that match its empirical performance. Our\ntheoretical analysis shows that Schatten norms can be recovered accurately from\nstrictly smaller number of samples compared to what is needed to recover the\nunderlying low-rank matrix. Numerical experiments suggest that we significantly\nimprove upon a competing approach of using matrix completion methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 18:12:17 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1703.06476", "submitter": "Mario Lucic", "authors": "Olivier Bachem, Mario Lucic, Andreas Krause", "title": "Practical Coreset Constructions for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate coresets - succinct, small summaries of large data sets - so\nthat solutions found on the summary are provably competitive with solution\nfound on the full data set. We provide an overview over the state-of-the-art in\ncoreset construction for machine learning. In Section 2, we present both the\nintuition behind and a theoretically sound framework to construct coresets for\ngeneral problems and apply it to $k$-means clustering. In Section 3 we\nsummarize existing coreset construction algorithms for a variety of machine\nlearning problems such as maximum likelihood estimation of mixture models,\nBayesian non-parametric models, principal component analysis, regression and\ngeneral empirical risk minimization.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 17:45:29 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 22:40:16 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Krause", "Andreas", ""]]}, {"id": "1703.06513", "submitter": "Sumeet Katariya", "authors": "Sumeet Katariya, Branislav Kveton, Csaba Szepesv\\'ari, Claire Vernade,\n  Zheng Wen", "title": "Bernoulli Rank-$1$ Bandits for Click Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability that a user will click a search result depends both on its\nrelevance and its position on the results page. The position based model\nexplains this behavior by ascribing to every item an attraction probability,\nand to every position an examination probability. To be clicked, a result must\nbe both attractive and examined. The probabilities of an item-position pair\nbeing clicked thus form the entries of a rank-$1$ matrix. We propose the\nlearning problem of a Bernoulli rank-$1$ bandit where at each step, the\nlearning agent chooses a pair of row and column arms, and receives the product\nof their Bernoulli-distributed values as a reward. This is a special case of\nthe stochastic rank-$1$ bandit problem considered in recent work that proposed\nan elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret\nscales linearly with the number of rows and columns on \"benign\" instances.\nThese are the instances where the minimum of the average row and column rewards\n$\\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to\nbe competitive with straightforward bandit strategies as $\\mu \\rightarrow 0$.\nIn this paper we propose Rank1ElimKL which simply replaces the (crude)\nconfidence intervals of Rank1Elim with confidence intervals based on\nKullback-Leibler (KL) divergences, and with the help of a novel result\nconcerning the scaling of KL divergences we prove that with this change, our\nalgorithm will be competitive no matter the value of $\\mu$. Experiments with\nsynthetic data confirm that on benign instances the performance of Rank1ElimKL\nis significantly better than that of even Rank1Elim, while experiments with\nmodels derived from real data confirm that the improvements are significant\nacross the board, regardless of whether the data is benign or not.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 21:06:51 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Vernade", "Claire", ""], ["Wen", "Zheng", ""]]}, {"id": "1703.06528", "submitter": "Florian Dumpert", "authors": "Florian Dumpert", "title": "Universal Consistency and Robustness of Localized Support Vector\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive amount of available data potentially used to discover patters in\nmachine learning is a challenge for kernel based algorithms with respect to\nruntime and storage capacities. Local approaches might help to relieve these\nissues. From a statistical point of view local approaches allow additionally to\ndeal with different structures in the data in different ways. This paper\nanalyses properties of localized kernel based, non-parametric statistical\nmachine learning methods, in particular of support vector machines (SVMs) and\nmethods close to them. We will show there that locally learnt kernel methods\nare universal consistent. Furthermore, we give an upper bound for the maxbias\nin order to show statistical robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 22:23:01 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Dumpert", "Florian", ""]]}, {"id": "1703.06537", "submitter": "Varvara Kollia", "authors": "Varvara Kollia, Noureddine Tayebi", "title": "A Controlled Set-Up Experiment to Establish Personalized Baselines for\n  Real-Life Emotion Recognition", "comments": "15 pages, 2 figures, 9 tables, Statistics-Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design, conduct and present the results of a highly personalized baseline\nemotion recognition experiment, which aims to set reliable ground-truth\nestimates for the subject's emotional state for real-life prediction under\nsimilar conditions using a small number of physiological sensors. We also\npropose an adaptive stimuli-selection mechanism that would use the user's\nfeedback as guide for future stimuli selection in the controlled-setup\nexperiment and generate optimal ground-truth personalized sessions\nsystematically. Initial results are very promising (85% accuracy) and variable\nimportance analysis shows that only a few features, which are easy-to-implement\nin portable devices, would suffice to predict the subject's emotional state.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 23:28:39 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Kollia", "Varvara", ""], ["Tayebi", "Noureddine", ""]]}, {"id": "1703.06686", "submitter": "Kiran Karra", "authors": "Kiran Karra, Lamine Mili", "title": "Copula Index for Detecting Dependence and Monotonicity between\n  Stochastic Signals", "comments": "Keywords: copula, statistical dependency, monotonic, equitability,\n  discrete 40 pages", "journal-ref": null, "doi": "10.1016/j.ins.2019.02.007", "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a nonparametric copula-based index for detecting the\nstrength and monotonicity structure of linear and nonlinear statistical\ndependence between pairs of random variables or stochastic signals. Our index,\ntermed Copula Index for Detecting Dependence and Monotonicity (CIM), satisfies\nseveral desirable properties of measures of association, including Renyi's\nproperties, the data processing inequality (DPI), and consequently\nself-equitability. Synthetic data simulations reveal that the statistical power\nof CIM compares favorably to other state-of-the-art measures of association\nthat are proven to satisfy the DPI. Simulation results with real-world data\nreveal the CIM's unique ability to detect the monotonicity structure among\nstochastic signals to find interesting dependencies in large datasets.\nAdditionally, simulations show that the CIM shows favorable performance to\nestimators of mutual information when discovering Markov network structure.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 11:26:04 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 20:48:59 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 01:10:44 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 16:27:41 GMT"}, {"version": "v5", "created": "Sat, 6 Oct 2018 06:09:04 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Karra", "Kiran", ""], ["Mili", "Lamine", ""]]}, {"id": "1703.06692", "submitter": "Peter Karkus", "authors": "Peter Karkus, David Hsu, Wee Sun Lee", "title": "QMDP-Net: Deep Learning for Planning under Partial Observability", "comments": "NIPS 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the QMDP-net, a neural network architecture for\nplanning under partial observability. The QMDP-net combines the strengths of\nmodel-free learning and model-based planning. It is a recurrent policy network,\nbut it represents a policy for a parameterized set of tasks by connecting a\nmodel with a planning algorithm that solves the model, thus embedding the\nsolution structure of planning in a network learning architecture. The QMDP-net\nis fully differentiable and allows for end-to-end training. We train a QMDP-net\non different tasks so that it can generalize to new ones in the parameterized\ntask set and \"transfer\" to other similar tasks beyond the set. In preliminary\nexperiments, QMDP-net showed strong performance on several robotic tasks in\nsimulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it\nsometimes outperforms the QMDP algorithm in the experiments, as a result of\nend-to-end learning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 11:44:00 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 12:59:39 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 03:31:43 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Karkus", "Peter", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1703.06700", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Independence clustering (without a matrix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independence clustering problem is considered in the following\nformulation: given a set $S$ of random variables, it is required to find the\nfinest partitioning $\\{U_1,\\dots,U_k\\}$ of $S$ into clusters such that the\nclusters $U_1,\\dots,U_k$ are mutually independent. Since mutual independence is\nthe target, pairwise similarity measurements are of no use, and thus\ntraditional clustering algorithms are inapplicable. The distribution of the\nrandom variables in $S$ is, in general, unknown, but a sample is available.\nThus, the problem is cast in terms of time series. Two forms of sampling are\nconsidered: i.i.d.\\ and stationary time series, with the main emphasis being on\nthe latter, more general, case. A consistent, computationally tractable\nalgorithm for each of the settings is proposed, and a number of open directions\nfor further research are outlined.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 12:09:53 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1703.06748", "submitter": "Yen-Chen Lin", "authors": "Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu\n  Liu, Min Sun", "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents", "comments": "To Appear at IJCAI 2017. Project website:\n  http://yenchenlin.me/adversarial_attack_RL/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two tactics to attack agents trained by deep reinforcement\nlearning algorithms using adversarial examples, namely the strategically-timed\nattack and the enchanting attack. In the strategically-timed attack, the\nadversary aims at minimizing the agent's reward by only attacking the agent at\na small subset of time steps in an episode. Limiting the attack activity to\nthis subset helps prevent detection of the attack by the agent. We propose a\nnovel method to determine when an adversarial example should be crafted and\napplied. In the enchanting attack, the adversary aims at luring the agent to a\ndesignated target state. This is achieved by combining a generative model and a\nplanning algorithm: while the generative model predicts the future states, the\nplanning algorithm generates a preferred sequence of actions for luring the\nagent. A sequence of adversarial examples is then crafted to lure the agent to\ntake the preferred sequence of actions. We apply the two tactics to the agents\ntrained by the state-of-the-art deep reinforcement learning algorithm including\nDQN and A3C. In 5 Atari games, our strategically timed attack reduces as much\nreward as the uniform attack (i.e., attacking at every time step) does by\nattacking the agent 4 times less often. Our enchanting attack lures the agent\ntoward designated target states with a more than 70% success rate. Videos are\navailable at http://yenchenlin.me/adversarial_attack_RL/\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 04:39:34 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 08:12:44 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 01:26:42 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 01:24:00 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lin", "Yen-Chen", ""], ["Hong", "Zhang-Wei", ""], ["Liao", "Yuan-Hong", ""], ["Shih", "Meng-Li", ""], ["Liu", "Ming-Yu", ""], ["Sun", "Min", ""]]}, {"id": "1703.06749", "submitter": "Nick Pawlowski", "authors": "Nick Pawlowski, Miguel Jaques, Ben Glocker", "title": "Efficient variational Bayesian neural network ensembles for outlier\n  detection", "comments": "Presented at Workshop track - ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we perform outlier detection using ensembles of neural networks\nobtained by variational approximation of the posterior in a Bayesian neural\nnetwork setting. The variational parameters are obtained by sampling from the\ntrue posterior by gradient descent. We show our outlier detection results are\ncomparable to those obtained using other efficient ensembling methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 14:02:11 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 13:03:20 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Pawlowski", "Nick", ""], ["Jaques", "Miguel", ""], ["Glocker", "Ben", ""]]}, {"id": "1703.06777", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Gavin C. Cawley", "title": "On the Use of Default Parameter Settings in the Empirical Evaluation of\n  Classification Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that, for a range of state-of-the-art machine learning\nalgorithms, the differences in generalisation performance obtained using\ndefault parameter settings and using parameters tuned via cross-validation can\nbe similar in magnitude to the differences in performance observed between\nstate-of-the-art and uncompetitive learning systems. This means that fair and\nrigorous evaluation of new learning algorithms requires performance comparison\nagainst benchmark methods with best-practice model selection procedures, rather\nthan using default parameter settings. We investigate the sensitivity of three\nkey machine learning algorithms (support vector machine, random forest and\nrotation forest) to their default parameter settings, and provide guidance on\ndetermining sensible default parameter values for implementations of these\nalgorithms. We also conduct an experimental comparison of these three\nalgorithms on 121 classification problems and find that, perhaps surprisingly,\nrotation forest is significantly more accurate on average than both random\nforest and a support vector machine.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 14:42:27 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Bagnall", "Anthony", ""], ["Cawley", "Gavin C.", ""]]}, {"id": "1703.06807", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, James Cheng, Kelvin Kai Wing Ng, Yuichi\n  Yoshida", "title": "Guaranteed Sufficient Decrease for Variance Reduced Stochastic Gradient\n  Descent", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sufficient decrease technique for variance\nreduced stochastic gradient descent methods such as SAG, SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nvariance reduction algorithms such as SVRG-SD and SAGA-SD as a byproduct. We\nintroduce a coefficient to scale current iterate and satisfy the sufficient\ndecrease property, which takes the decisions to shrink, expand or move in the\nopposite direction, and then give two specific update rules of the coefficient\nfor Lasso and ridge regression. Moreover, we analyze the convergence properties\nof our algorithms for strongly convex problems, which show that both of our\nalgorithms attain linear convergence rates. We also provide the convergence\nguarantees of our algorithms for non-strongly convex problems. Our experimental\nresults further verify that our algorithms achieve significantly better\nperformance than their counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 15:43:10 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 15:20:30 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""], ["Ng", "Kelvin Kai Wing", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1703.06856", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva", "title": "Counterfactual Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning can impact people with legal or ethical consequences when it\nis used to automate decisions in areas such as insurance, lending, hiring, and\npredictive policing. In many of these scenarios, previous decisions have been\nmade that are unfairly biased against certain subpopulations, for example those\nof a particular race, gender, or sexual orientation. Since this past data may\nbe biased, machine learning predictors must account for this to avoid\nperpetuating or creating discriminatory practices. In this paper, we develop a\nframework for modeling fairness using tools from causal inference. Our\ndefinition of counterfactual fairness captures the intuition that a decision is\nfair towards an individual if it is the same in (a) the actual world and (b) a\ncounterfactual world where the individual belonged to a different demographic\ngroup. We demonstrate our framework on a real-world problem of fair prediction\nof success in law school.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 17:18:57 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 20:06:54 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 11:23:13 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Kusner", "Matt J.", ""], ["Loftus", "Joshua R.", ""], ["Russell", "Chris", ""], ["Silva", "Ricardo", ""]]}, {"id": "1703.06857", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal and Radha Poovendran", "title": "On the Limitation of Convolutional Neural Networks in Recognizing\n  Negative Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance on a variety of computer vision tasks, particularly visual\nclassification problems, where new algorithms reported to achieve or even\nsurpass the human performance. In this paper, we examine whether CNNs are\ncapable of learning the semantics of training data. To this end, we evaluate\nCNNs on negative images, since they share the same structure and semantics as\nregular images and humans can classify them correctly. Our experimental results\nindicate that when training on regular images and testing on negative images,\nthe model accuracy is significantly lower than when it is tested on regular\nimages. This leads us to the conjecture that current training methods do not\neffectively train models to generalize the concepts. We then introduce the\nnotion of semantic adversarial examples - transformed inputs that semantically\nrepresent the same objects, but the model does not classify them correctly -\nand present negative images as one class of such inputs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 17:21:19 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:53:28 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Jaiswal", "Mayoore", ""], ["Poovendran", "Radha", ""]]}, {"id": "1703.06891", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Julian McAuley", "title": "Dance Dance Convolution", "comments": "Published as a conference paper at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players\nperform steps on a dance platform in synchronization with music as directed by\non-screen step charts. While many step charts are available in standardized\npacks, players may grow tired of existing charts, or wish to dance to a song\nfor which no chart exists. We introduce the task of learning to choreograph.\nGiven a raw audio track, the goal is to produce a new step chart. This task\ndecomposes naturally into two subtasks: deciding when to place steps and\ndeciding which steps to select. For the step placement task, we combine\nrecurrent and convolutional neural networks to ingest spectrograms of low-level\naudio features to predict steps, conditioned on chart difficulty. For step\nselection, we present a conditional LSTM generative model that substantially\noutperforms n-gram and fixed-window approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 18:00:13 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 07:44:55 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 00:45:51 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["McAuley", "Julian", ""]]}, {"id": "1703.06912", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Andreas Wieser", "title": "Application of backpropagation neural networks to both stages of\n  fingerprinting based WIPS", "comments": "11 pages, 11 figures, published in proceedings UPINLBS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme to employ backpropagation neural networks (BPNNs) for\nboth stages of fingerprinting-based indoor positioning using WLAN/WiFi signal\nstrengths (FWIPS): radio map construction during the offline stage, and\nlocalization during the online stage. Given a training radio map (TRM), i.e., a\nset of coordinate vectors and associated WLAN/WiFi signal strengths of the\navailable access points, a BPNN can be trained to output the expected signal\nstrengths for any input position within the region of interest (BPNN-RM). This\ncan be used to provide a continuous representation of the radio map and to\nfilter, densify or decimate a discrete radio map. Correspondingly, the TRM can\nalso be used to train another BPNN to output the expected position within the\nregion of interest for any input vector of recorded signal strengths and thus\ncarry out localization (BPNN-LA).Key aspects of the design of such artificial\nneural networks for a specific application are the selection of design\nparameters like the number of hidden layers and nodes within the network, and\nthe training procedure. Summarizing extensive numerical simulations, based on\nreal measurements in a testbed, we analyze the impact of these design choices\non the performance of the BPNN and compare the results in particular to those\nobtained using the $k$ nearest neighbors ($k$NN) and weighted $k$ nearest\nneighbors approaches to FWIPS.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 20:30:50 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1703.06934", "submitter": "William La Cava", "authors": "William La Cava and Jason H. Moore", "title": "Ensemble representation learning: an analysis of fitness and survival\n  for wrapper-based genetic programming methods", "comments": "Genetic and Evolutionary Computation Conference (GECCO) 2017, Berlin,\n  Germany", "journal-ref": null, "doi": "10.1145/3071178/3071215", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we proposed a general, ensemble-based feature engineering wrapper\n(FEW) that was paired with a number of machine learning methods to solve\nregression problems. Here, we adapt FEW for supervised classification and\nperform a thorough analysis of fitness and survival methods within this\nframework. Our tests demonstrate that two fitness metrics, one introduced as an\nadaptation of the silhouette score, outperform the more commonly used Fisher\ncriterion. We analyze survival methods and demonstrate that $\\epsilon$-lexicase\nsurvival works best across our test problems, followed by random survival which\noutperforms both tournament and deterministic crowding. We conduct a benchmark\ncomparison to several classification methods using a large set of problems and\nshow that FEW can improve the best classifier performance in several cases. We\nshow that FEW generates consistent, meaningful features for a biomedical\nproblem with different ML pairings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 19:26:00 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 12:46:40 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 19:01:17 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["La Cava", "William", ""], ["Moore", "Jason H.", ""]]}, {"id": "1703.06975", "submitter": "Florian Bordes", "authors": "Florian Bordes, Sina Honari, Pascal Vincent", "title": "Learning to Generate Samples from Noise through Infusion Training", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate a novel training procedure to learn a generative\nmodel as the transition operator of a Markov chain, such that, when applied\nrepeatedly on an unstructured random noise sample, it will denoise it into a\nsample that matches the target distribution from the training set. The novel\ntraining procedure to learn this progressive denoising operation involves\nsampling from a slightly different chain than the model chain used for\ngeneration in the absence of a denoising target. In the training chain we\ninfuse information from the training target example that we would like the\nchains to reach with a high probability. The thus learned transition operator\nis able to produce quality and varied samples in a small number of steps.\nExperiments show competitive results compared to the samples generated with a\nbasic Generative Adversarial Net\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 21:29:18 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Bordes", "Florian", ""], ["Honari", "Sina", ""], ["Vincent", "Pascal", ""]]}, {"id": "1703.06990", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel and Nil Geisweiller and Chris Poulin", "title": "Metalearning for Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general formulation of optimization problems in which various candidate\nsolutions may use different feature-sets is presented, encompassing supervised\nclassification, automated program learning and other cases. A novel\ncharacterization of the concept of a \"good quality feature\" for such an\noptimization problem is provided; and a proposal regarding the integration of\nquality based feature selection into metalearning is suggested, wherein the\nquality of a feature for a problem is estimated using knowledge about related\nfeatures in the context of related problems. Results are presented regarding\nextensive testing of this \"feature metalearning\" approach on supervised text\nclassification problems; it is demonstrated that, in this context, feature\nmetalearning can provide significant and sometimes dramatic speedup over\nstandard feature selection heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 22:32:36 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Goertzel", "Ben", ""], ["Geisweiller", "Nil", ""], ["Poulin", "Chris", ""]]}, {"id": "1703.07026", "submitter": "Xin Huang", "authors": "Xin Huang and Yuxin Peng", "title": "Cross-modal Deep Metric Learning with Multi-task Regularization", "comments": "Revision: Added reference [7] 6 pages, 1 figure, to appear in the\n  proceedings of the IEEE International Conference on Multimedia and Expo\n  (ICME), Jul 10, 2017 - Jul 14, 2017, Hong Kong, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based cross-modal retrieval has become a research hotspot, by which users\ncan search results across various modalities like image and text. However,\nexisting methods mainly focus on the pairwise correlation and reconstruction\nerror of labeled data. They ignore the semantically similar and dissimilar\nconstraints between different modalities, and cannot take advantage of\nunlabeled data. This paper proposes Cross-modal Deep Metric Learning with\nMulti-task Regularization (CDMLMR), which integrates quadruplet ranking loss\nand semi-supervised contrastive loss for modeling cross-modal semantic\nsimilarity in a unified multi-task learning architecture. The quadruplet\nranking loss can model the semantically similar and dissimilar constraints to\npreserve cross-modal relative similarity ranking information. The\nsemi-supervised contrastive loss is able to maximize the semantic similarity on\nboth labeled and unlabeled data. Compared to the existing methods, CDMLMR\nexploits not only the similarity ranking information but also unlabeled\ncross-modal data, and thus boosts cross-modal retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 02:04:30 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 05:02:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""]]}, {"id": "1703.07027", "submitter": "Zhiting Hu", "authors": "Prasoon Goyal, Zhiting Hu, Xiaodan Liang, Chenyu Wang, Eric Xing", "title": "Nonparametric Variational Auto-encoders for Hierarchical Representation\n  Learning", "comments": "Accepted in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently developed variational autoencoders (VAEs) have proved to be an\neffective confluence of the rich representational power of neural networks with\nBayesian methods. However, most work on VAEs use a rather simple prior over the\nlatent variables such as standard normal distribution, thereby restricting its\napplications to relatively simple phenomena. In this work, we propose\nhierarchical nonparametric variational autoencoders, which combines\ntree-structured Bayesian nonparametric priors with VAEs, to enable infinite\nflexibility of the latent representation space. Both the neural parameters and\nBayesian priors are learned jointly using tailored variational inference. The\nresulting model induces a hierarchical structure of latent semantic concepts\nunderlying the data corpus, and infers accurate representations of data\ninstances. We apply our model in video representation learning. Our method is\nable to discover highly interpretable activity hierarchies, and obtain improved\nclustering accuracy and generalization capacity based on the learned rich\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 02:08:05 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 18:52:57 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Goyal", "Prasoon", ""], ["Hu", "Zhiting", ""], ["Liang", "Xiaodan", ""], ["Wang", "Chenyu", ""], ["Xing", "Eric", ""]]}, {"id": "1703.07047", "submitter": "Krzysztof J. Geras", "authors": "Krzysztof J. Geras and Stacey Wolfson and Yiqiu Shen and Nan Wu and S.\n  Gene Kim and Eric Kim and Laura Heacock and Ujas Parikh and Linda Moy and\n  Kyunghyun Cho", "title": "High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 04:11:13 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 06:39:33 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 01:21:51 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Geras", "Krzysztof J.", ""], ["Wolfson", "Stacey", ""], ["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Kim", "S. Gene", ""], ["Kim", "Eric", ""], ["Heacock", "Laura", ""], ["Parikh", "Ujas", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1703.07056", "submitter": "Atsushi Shibagaki", "authors": "Atsushi Shibagaki, Ichiro Takeuchi", "title": "Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based\n  on Optimality Violations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study primal-dual type stochastic optimization algorithms with non-uniform\nsampling. Our main theoretical contribution in this paper is to present a\nconvergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with\narbitrary sampling. Based on this theoretical framework, we propose Optimality\nViolation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on\nOptimality Violation. We also propose two efficient heuristic variants of\novsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments,\nwe demonstrate that the proposed method and its variants are faster than other\nstate-of-the-art primal-dual type stochastic optimization methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 05:08:33 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Shibagaki", "Atsushi", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1703.07131", "submitter": "Mandar Kulkarni Mr.", "authors": "Mandar Kulkarni, Kalpesh Patil, Shirish Karande", "title": "Knowledge distillation using unlabeled mismatched images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches for Knowledge Distillation (KD) either directly use\ntraining data or sample from the training data distribution. In this paper, we\ndemonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for\nimage classification networks. For illustration, we consider scenarios where\nthis is a complete absence of training data, or mismatched stimulus has to be\nused for augmenting a small amount of training data. We demonstrate that\nstimulus complexity is a key factor for distillation's good performance. Our\nexamples include use of various datasets for stimulating MNIST and CIFAR\nteachers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:34:59 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Kulkarni", "Mandar", ""], ["Patil", "Kalpesh", ""], ["Karande", "Shirish", ""]]}, {"id": "1703.07168", "submitter": "Jean Daunizeau", "authors": "Jean Daunizeau", "title": "On parameters transformations for emulating sparse priors using\n  variational-Laplace inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called sparse estimators arise in the context of model fitting, when one a\npriori assumes that only a few (unknown) model parameters deviate from zero.\nSparsity constraints can be useful when the estimation problem is\nunder-determined, i.e. when number of model parameters is much higher than the\nnumber of data points. Typically, such constraints are enforced by minimizing\nthe L1 norm, which yields the so-called LASSO estimator. In this work, we\npropose a simple parameter transform that emulates sparse priors without\nsacrificing the simplicity and robustness of L2-norm regularization schemes. We\nshow how L1 regularization can be obtained with a \"sparsify\" remapping of\nparameters under normal Bayesian priors, and we demonstrate the ensuing\nvariational Laplace approach using Monte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:40:15 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Daunizeau", "Jean", ""]]}, {"id": "1703.07169", "submitter": "Patrick Flaherty", "authors": "Hachem Saddiki, Andrew C. Trapp, Patrick Flaherty", "title": "A Deterministic Global Optimization Method for Variational Inference", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods for latent variable statistical models have\ngained popularity because they are relatively fast, can handle large data sets,\nand have deterministic convergence guarantees. However, in practice it is\nunclear whether the fixed point identified by the variational inference\nalgorithm is a local or a global optimum. Here, we propose a method for\nconstructing iterative optimization algorithms for variational inference\nproblems that are guaranteed to converge to the $\\epsilon$-global variational\nlower bound on the log-likelihood. We derive inference algorithms for two\nvariational approximations to a standard Bayesian Gaussian mixture model\n(BGMM). We present a minimal data set for empirically testing convergence and\nshow that a variational inference algorithm frequently converges to a local\noptimum while our algorithm always converges to the globally optimal\nvariational lower bound. We characterize the loss incurred by choosing a\nnon-optimal variational approximation distribution suggesting that selection of\nthe approximating variational distribution deserves as much attention as the\nselection of the original statistical model for a given data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 12:33:19 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Saddiki", "Hachem", ""], ["Trapp", "Andrew C.", ""], ["Flaherty", "Patrick", ""]]}, {"id": "1703.07198", "submitter": "George Mathews", "authors": "George M. Mathews and John Vial", "title": "Overcoming model simplifications when quantifying predictive uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR physics.comp-ph physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally accepted that all models are wrong -- the difficulty is\ndetermining which are useful. Here, a useful model is considered as one that is\ncapable of combining data and expert knowledge, through an inversion or\ncalibration process, to adequately characterize the uncertainty in predictions\nof interest. This paper derives conditions that specify which simplified models\nare useful and how they should be calibrated. To start, the notion of an\noptimal simplification is defined. This relates the model simplifications to\nthe nature of the data and predictions, and determines when a standard\nprobabilistic calibration scheme is capable of accurately characterizing\nuncertainty. Furthermore, two additional conditions are defined for suboptimal\nmodels that determine when the simplifications can be safely ignored. The first\nallows a suboptimally simplified model to be used in a way that replicates the\nperformance of an optimal model. This is achieved through the judicial\nselection of a prior term for the calibration process that explicitly includes\nthe nature of the data, predictions and modelling simplifications. The second\nconsiders the dependency structure between the predictions and the available\ndata to gain insights into when the simplifications can be overcome by using\nthe right calibration data. Furthermore, the derived conditions are related to\nthe commonly used calibration schemes based on Tikhonov and subspace\nregularization. To allow concrete insights to be obtained, the analysis is\nperformed under a linear expansion of the model equations and where the\npredictive uncertainty is characterized via second order moments only.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 13:02:35 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Mathews", "George M.", ""], ["Vial", "John", ""]]}, {"id": "1703.07255", "submitter": "Hao Wang", "authors": "Hao Wang, Xiaodan Liang, Hao Zhang, Dit-Yan Yeung, Eric P. Xing", "title": "ZM-Net: Real-time Zero-shot Image Manipulation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in image processing and computer vision (e.g. colorization,\nstyle transfer) can be posed as 'manipulating' an input image into a\ncorresponding output image given a user-specified guiding signal. A holy-grail\nsolution towards generic image manipulation should be able to efficiently alter\nan input image with any personalized signals (even signals unseen during\ntraining), such as diverse paintings and arbitrary descriptive attributes.\nHowever, existing methods are either inefficient to simultaneously process\nmultiple signals (let alone generalize to unseen signals), or unable to handle\nsignals from other modalities. In this paper, we make the first attempt to\naddress the zero-shot image manipulation task. We cast this problem as\nmanipulating an input image according to a parametric model whose key\nparameters can be conditionally generated from any guiding signal (even unseen\nones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a\nfully-differentiable architecture that jointly optimizes an\nimage-transformation network (TNet) and a parameter network (PNet). The PNet\nlearns to generate key transformation parameters for the TNet given any guiding\nsignal while the TNet performs fast zero-shot image manipulation according to\nboth signal-dependent parameters from the PNet and signal-invariant parameters\nfrom the TNet itself. Extensive experiments show that our ZM-Net can perform\nhigh-quality image manipulation conditioned on different forms of guiding\nsignals (e.g. style images and attributes) in real-time (tens of milliseconds\nper image) even for unseen signals. Moreover, a large-scale style dataset with\nover 20,000 style images is also constructed to promote further research.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:01:59 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:08:40 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wang", "Hao", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Hao", ""], ["Yeung", "Dit-Yan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.07285", "submitter": "Mathurin Massias", "authors": "Mathurin Massias and Alexandre Gramfort and Joseph Salmon", "title": "From safe screening rules to working sets for faster Lasso-type solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex sparsity-promoting regularizations are ubiquitous in modern\nstatistical learning. By construction, they yield solutions with few non-zero\ncoefficients, which correspond to saturated constraints in the dual\noptimization formulation. Working set (WS) strategies are generic optimization\ntechniques that consist in solving simpler problems that only consider a subset\nof constraints, whose indices form the WS. Working set methods therefore\ninvolve two nested iterations: the outer loop corresponds to the definition of\nthe WS and the inner loop calls a solver for the subproblems. For the Lasso\nestimator a WS is a set of features, while for a Group Lasso it refers to a set\nof groups. In practice, WS are generally small in this context so the\nassociated feature Gram matrix can fit in memory. Here we show that the\nGauss-Southwell rule (a greedy strategy for block coordinate descent\ntechniques) leads to fast solvers in this case. Combined with a working set\nstrategy based on an aggressive use of so-called Gap Safe screening rules, we\npropose a solver achieving state-of-the-art performance on sparse learning\nproblems. Results are presented on Lasso and multi-task Lasso estimators.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:42:38 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 10:30:25 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Massias", "Mathurin", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1703.07305", "submitter": "Andrej Aderhold", "authors": "Marco Grzegorczyk, Andrej Aderhold, and Dirk Husmeier", "title": "Targeting Bayes factors with direct-path non-equilibrium thermodynamic\n  integration", "comments": "Accepted for publication in Computational Statistics, 38 pages, 17\n  figures", "journal-ref": "Computational Statistics, 1-45", "doi": "10.1007/s00180-017-0721-7", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermodynamic integration (TI) for computing marginal likelihoods is based on\nan inverse annealing path from the prior to the posterior distribution. In many\ncases, the resulting estimator suffers from high variability, which\nparticularly stems from the prior regime. When comparing complex models with\ndifferences in a comparatively small number of parameters, intrinsic errors\nfrom sampling fluctuations may outweigh the differences in the log marginal\nlikelihood estimates. In the present article, we propose a thermodynamic\nintegration scheme that directly targets the log Bayes factor. The method is\nbased on a modified annealing path between the posterior distributions of the\ntwo models compared, which systematically avoids the high variance prior\nregime. We combine this scheme with the concept of non-equilibrium TI to\nminimise discretisation errors from numerical integration. Results obtained on\nBayesian regression models applied to standard benchmark data, and a complex\nhierarchical model applied to biopathway inference, demonstrate a significant\nreduction in estimator variance over state-of-the-art TI methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:39:28 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Grzegorczyk", "Marco", ""], ["Aderhold", "Andrej", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1703.07345", "submitter": "Haichuan Yang", "authors": "Haichuan Yang, Shupeng Gui, Chuyang Ke, Daniel Stefankovic, Ryohei\n  Fujimaki, and Ji Liu", "title": "On The Projection Operator to A Three-view Cardinality Constrained Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cardinality constraint is an intrinsic way to restrict the solution\nstructure in many domains, for example, sparse learning, feature selection, and\ncompressed sensing. To solve a cardinality constrained problem, the key\nchallenge is to solve the projection onto the cardinality constraint set, which\nis NP-hard in general when there exist multiple overlapped cardinality\nconstraints. In this paper, we consider the scenario where the overlapped\ncardinality constraints satisfy a Three-view Cardinality Structure (TVCS),\nwhich reflects the natural restriction in many applications, such as\nidentification of gene regulatory networks and task-worker assignment problem.\nWe cast the projection into a linear programming, and show that for TVCS, the\nvertex solution of this linear programming is the solution for the original\nprojection problem. We further prove that such solution can be found with the\ncomplexity proportional to the number of variables and constraints. We finally\nuse synthetic experiments and two interesting applications in bioinformatics\nand crowdsourcing to validate the proposed TVCS model and method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 17:58:03 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 17:05:57 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Yang", "Haichuan", ""], ["Gui", "Shupeng", ""], ["Ke", "Chuyang", ""], ["Stefankovic", "Daniel", ""], ["Fujimaki", "Ryohei", ""], ["Liu", "Ji", ""]]}, {"id": "1703.07355", "submitter": "Srijan Kumar", "authors": "Srijan Kumar, Justin Cheng, Jure Leskovec, V.S. Subrahmanian", "title": "An Army of Me: Sockpuppets in Online Discussion Communities", "comments": "26th International World Wide Web conference 2017 (WWW 2017)", "journal-ref": null, "doi": "10.1145/3038912.3052677", "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In online discussion communities, users can interact and share information\nand opinions on a wide variety of topics. However, some users may create\nmultiple identities, or sockpuppets, and engage in undesired behavior by\ndeceiving others or manipulating discussions. In this work, we study\nsockpuppetry across nine discussion communities, and show that sockpuppets\ndiffer from ordinary users in terms of their posting behavior, linguistic\ntraits, as well as social network structure. Sockpuppets tend to start fewer\ndiscussions, write shorter posts, use more personal pronouns such as \"I\", and\nhave more clustered ego-networks. Further, pairs of sockpuppets controlled by\nthe same individual are more likely to interact on the same discussion at the\nsame time than pairs of ordinary users. Our analysis suggests a taxonomy of\ndeceptive behavior in discussion communities. Pairs of sockpuppets can vary in\ntheir deceptiveness, i.e., whether they pretend to be different users, or their\nsupportiveness, i.e., if they support arguments of other sockpuppets controlled\nby the same user. We apply these findings to a series of prediction tasks,\nnotably, to identify whether a pair of accounts belongs to the same underlying\nuser or not. Altogether, this work presents a data-driven view of deception in\nonline discussion communities and paves the way towards the automatic detection\nof sockpuppets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:00:02 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Kumar", "Srijan", ""], ["Cheng", "Justin", ""], ["Leskovec", "Jure", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1703.07370", "submitter": "George Tucker", "authors": "George Tucker, Andriy Mnih, Chris J. Maddison, Dieterich Lawson,\n  Jascha Sohl-Dickstein", "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent\n  variable models", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in models with discrete latent variables is challenging due to high\nvariance gradient estimators. Generally, approaches have relied on control\nvariates to reduce the variance of the REINFORCE estimator. Recent work (Jang\net al. 2016, Maddison et al. 2016) has taken a different approach, introducing\na continuous relaxation of discrete variables to produce low-variance, but\nbiased, gradient estimates. In this work, we combine the two approaches through\na novel control variate that produces low-variance, \\emph{unbiased} gradient\nestimates. Then, we introduce a modification to the continuous relaxation and\nshow that the tightness of the relaxation can be adapted online, removing it as\na hyperparameter. We show state-of-the-art variance reduction on several\nbenchmark generative modeling tasks, generally leading to faster convergence to\na better final log-likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:05:31 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 11:04:12 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 20:54:49 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 17:50:34 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Tucker", "George", ""], ["Mnih", "Andriy", ""], ["Maddison", "Chris J.", ""], ["Lawson", "Dieterich", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1703.07473", "submitter": "Niko S\\\"underhauf", "authors": "Feras Dayoub, Niko S\\\"underhauf, Peter Corke", "title": "Episode-Based Active Learning with Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different strategies for active learning with Bayesian deep\nneural networks. We focus our analysis on scenarios where new, unlabeled data\nis obtained episodically, such as commonly encountered in mobile robotics\napplications. An evaluation of different strategies for acquisition, updating,\nand final training on the CIFAR-10 dataset shows that incremental network\nupdates with final training on the accumulated acquisition set are essential\nfor best performance, while limiting the amount of required human labeling\nlabor.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 23:56:51 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""], ["Corke", "Peter", ""]]}, {"id": "1703.07506", "submitter": "Marc Goessling", "authors": "Marc Goessling", "title": "LogitBoost autoregressive networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2017.03.010", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate binary distributions can be decomposed into products of\nunivariate conditional distributions. Recently popular approaches have modeled\nthese conditionals through neural networks with sophisticated weight-sharing\nstructures. It is shown that state-of-the-art performance on several standard\nbenchmark datasets can actually be achieved by training separate probability\nestimators for each dimension. In that case, model training can be trivially\nparallelized over data dimensions. On the other hand, complexity control has to\nbe performed for each learned conditional distribution. Three possible methods\nare considered and experimentally compared. The estimator that is employed for\neach conditional is LogitBoost. Similarities and differences between the\nproposed approach and autoregressive models based on neural networks are\ndiscussed in detail.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 03:26:32 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Goessling", "Marc", ""]]}, {"id": "1703.07596", "submitter": "Ho Chung Leon Law", "authors": "Ho Chung Leon Law, Christopher Yau and Dino Sejdinovic", "title": "Testing and Learning on Distributions with Symmetric Noise Invariance", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD),\nthe resulting distance between distributions, are useful tools for fully\nnonparametric two-sample testing and learning on distributions. However, it is\nrarely that all possible differences between samples are of interest --\ndiscovered differences can be due to different types of measurement noise, data\ncollection artefacts or other irrelevant sources of variability. We propose\ndistances between distributions which encode invariance to additive symmetric\nnoise, aimed at testing whether the assumed true underlying processes differ.\nMoreover, we construct invariant features of distributions, leading to learning\nalgorithms robust to the impairment of the input distributions with symmetric\nadditive noise.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 10:40:40 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 12:22:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Law", "Ho Chung Leon", ""], ["Yau", "Christopher", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1703.07607", "submitter": "Rafael de Souza", "authors": "R. S. de Souza, M. L. L. Dantas, M. V. Costa-Duarte, E. D. Feigelson,\n  M. Killedar, P.-Y. Lablanche, R. Vilalta, A. Krone-Martins, R. Beck, F.\n  Gieseke", "title": "A probabilistic approach to emission-line galaxy classification", "comments": "Accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stx2156", "report-no": null, "categories": "astro-ph.GA astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We invoke a Gaussian mixture model (GMM) to jointly analyse two traditional\nemission-line classification schemes of galaxy ionization sources: the\nBaldwin-Phillips-Terlevich (BPT) and $\\rm W_{H\\alpha}$ vs. [NII]/H$\\alpha$\n(WHAN) diagrams, using spectroscopic data from the Sloan Digital Sky Survey\nData Release 7 and SEAGal/STARLIGHT datasets. We apply a GMM to empirically\ndefine classes of galaxies in a three-dimensional space spanned by the $\\log$\n[OIII]/H$\\beta$, $\\log$ [NII]/H$\\alpha$, and $\\log$ EW(H${\\alpha}$), optical\nparameters. The best-fit GMM based on several statistical criteria suggests a\nsolution around four Gaussian components (GCs), which are capable to explain up\nto 97 per cent of the data variance. Using elements of information theory, we\ncompare each GC to their respective astronomical counterpart. GC1 and GC4 are\nassociated with star-forming galaxies, suggesting the need to define a new\nstarburst subgroup. GC2 is associated with BPT's Active Galaxy Nuclei (AGN)\nclass and WHAN's weak AGN class. GC3 is associated with BPT's composite class\nand WHAN's strong AGN class. Conversely, there is no statistical evidence --\nbased on four GCs -- for the existence of a Seyfert/LINER dichotomy in our\nsample. Notwithstanding, the inclusion of an additional GC5 unravels it. The\nGC5 appears associated to the LINER and Passive galaxies on the BPT and WHAN\ndiagrams respectively. Subtleties aside, we demonstrate the potential of our\nmethodology to recover/unravel different objects inside the wilderness of\nastronomical datasets, without lacking the ability to convey physically\ninterpretable results. The probabilistic classifications from the GMM analysis\nare publicly available within the COINtoolbox\n(https://cointoolbox.github.io/GMM\\_Catalogue/).\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 11:48:11 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 16:56:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["de Souza", "R. S.", ""], ["Dantas", "M. L. L.", ""], ["Costa-Duarte", "M. V.", ""], ["Feigelson", "E. D.", ""], ["Killedar", "M.", ""], ["Lablanche", "P. -Y.", ""], ["Vilalta", "R.", ""], ["Krone-Martins", "A.", ""], ["Beck", "R.", ""], ["Gieseke", "F.", ""]]}, {"id": "1703.07608", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy, Daniel Russo, Zheng Wen", "title": "Deep Exploration via Randomized Value Functions", "comments": "Accepted for publication in Journal of Machine Learning Research 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of randomized value functions to guide deep exploration in\nreinforcement learning. This offers an elegant means for synthesizing\nstatistically and computationally efficient exploration with common practical\napproaches to value function learning. We present several reinforcement\nlearning algorithms that leverage randomized value functions and demonstrate\ntheir efficacy through computational studies. We also prove a regret bound that\nestablishes statistical efficiency with a tabular representation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 11:53:53 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 17:13:06 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 09:17:37 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 23:48:32 GMT"}, {"version": "v5", "created": "Mon, 23 Sep 2019 18:29:02 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""], ["Russo", "Daniel", ""], ["Wen", "Zheng", ""]]}, {"id": "1703.07625", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, St\\'ephane Thiery and Eric Nyiri", "title": "Clustering for Different Scales of Measurement - the Gap-Ratio Weighted\n  K-means Algorithm", "comments": "13 pages, 6 figures, 2 tables. This paper is under the review process\n  for AIAP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for clustering data that are spread out over\nlarge regions and which dimensions are on different scales of measurement. Such\nan algorithm was developed to implement a robotics application consisting in\nsorting and storing objects in an unsupervised way. The toy dataset used to\nvalidate such application consists of Lego bricks of different shapes and\ncolors. The uncontrolled lighting conditions together with the use of RGB color\nfeatures, respectively involve data with a large spread and different levels of\nmeasurement between data dimensions. To overcome the combination of these two\ncharacteristics in the data, we have developed a new weighted K-means\nalgorithm, called gap-ratio K-means, which consists in weighting each dimension\nof the feature space before running the K-means algorithm. The weight\nassociated with a feature is proportional to the ratio of the biggest gap\nbetween two consecutive data points, and the average of all the other gaps.\nThis method is compared with two other variants of K-means on the Lego bricks\nclustering problem as well as two other common classification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 12:50:15 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Thiery", "St\u00e9phane", ""], ["Nyiri", "Eric", ""]]}, {"id": "1703.07698", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang", "title": "Characterization of Deterministic and Probabilistic Sampling Patterns\n  for Finite Completability of Low Tensor-Train Rank Tensor", "comments": "arXiv admin note: text overlap with arXiv:1612.01597", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.AG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the fundamental conditions for low-rank tensor\ncompletion given the separation or tensor-train (TT) rank, i.e., ranks of\nunfoldings. We exploit the algebraic structure of the TT decomposition to\nobtain the deterministic necessary and sufficient conditions on the locations\nof the samples to ensure finite completability. Specifically, we propose an\nalgebraic geometric analysis on the TT manifold that can incorporate the whole\nrank vector simultaneously in contrast to the existing approach based on the\nGrassmannian manifold that can only incorporate one rank component. Our\nproposed technique characterizes the algebraic independence of a set of\npolynomials defined based on the sampling pattern and the TT decomposition,\nwhich is instrumental to obtaining the deterministic condition on the sampling\npattern for finite completability. In addition, based on the proposed analysis,\nassuming that the entries of the tensor are sampled independently with\nprobability $p$, we derive a lower bound on the sampling probability $p$, or\nequivalently, the number of sampled entries that ensures finite completability\nwith high probability. Moreover, we also provide the deterministic and\nprobabilistic conditions for unique completability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 15:11:55 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1703.07710", "submitter": "Christoph Dann", "authors": "Christoph Dann, Tor Lattimore, Emma Brunskill", "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement\n  Learning", "comments": "appears in Neural Information Processing Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical performance bounds for reinforcement learning (RL) algorithms can\nbe critical for high-stakes applications like healthcare. This paper introduces\na new framework for theoretically measuring the performance of such algorithms\ncalled Uniform-PAC, which is a strengthening of the classical Probably\nApproximately Correct (PAC) framework. In contrast to the PAC framework, the\nuniform version may be used to derive high probability regret guarantees and so\nforms a bridge between the two setups that has been missing in the literature.\nWe demonstrate the benefits of the new framework for finite-state episodic MDPs\nwith a new algorithm that is Uniform-PAC and simultaneously achieves optimal\nregret and PAC guarantees except for a factor of the horizon.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 15:34:23 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 21:04:38 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 13:25:46 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Dann", "Christoph", ""], ["Lattimore", "Tor", ""], ["Brunskill", "Emma", ""]]}, {"id": "1703.07754", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Bhuvana Ramabhadran, George Saon, Michael Picheny,\n  David Nahamoo", "title": "Direct Acoustics-to-Word Models for English Conversational Speech\n  Recognition", "comments": "Submitted to Interspeech-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on end-to-end automatic speech recognition (ASR) has shown that\nthe connectionist temporal classification (CTC) loss can be used to convert\nacoustics to phone or character sequences. Such systems are used with a\ndictionary and separately-trained Language Model (LM) to produce word\nsequences. However, they are not truly end-to-end in the sense of mapping\nacoustics directly to words without an intermediate phone representation. In\nthis paper, we present the first results employing direct acoustics-to-word CTC\nmodels on two well-known public benchmark tasks: Switchboard and CallHome.\nThese models do not require an LM or even a decoder at run-time and hence\nrecognize speech with minimal complexity. However, due to the large number of\nword output units, CTC word models require orders of magnitude more data to\ntrain reliably compared to traditional systems. We present some techniques to\nmitigate this issue. Our CTC word model achieves a word error rate of\n13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or\ndecoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also\npresent rescoring results on CTC word model lattices to quantify the\nperformance benefits of a LM, and contrast the performance of word and phone\nCTC models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 17:17:16 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Ramabhadran", "Bhuvana", ""], ["Saon", "George", ""], ["Picheny", "Michael", ""], ["Nahamoo", "David", ""]]}, {"id": "1703.07758", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Hongyang Zhang", "title": "Sample and Computationally Efficient Learning Algorithms under S-Concave\n  Distributions", "comments": "Appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new results for noise-tolerant and sample-efficient learning\nalgorithms under $s$-concave distributions. The new class of $s$-concave\ndistributions is a broad and natural generalization of log-concavity, and\nincludes many important additional distributions, e.g., the Pareto distribution\nand $t$-distribution. This class has been studied in the context of efficient\nsampling, integration, and optimization, but much remains unknown about the\ngeometry of this class of distributions and their applications in the context\nof learning. The challenge is that unlike the commonly used distributions in\nlearning (uniform or more generally log-concave distributions), this broader\nclass is not closed under the marginalization operator and many such\ndistributions are fat-tailed. In this work, we introduce new convex geometry\ntools to study the properties of $s$-concave distributions and use these\nproperties to provide bounds on quantities of interest to learning including\nthe probability of disagreement between two halfspaces, disagreement outside a\nband, and the disagreement coefficient. We use these results to significantly\ngeneralize prior results for margin-based active learning, disagreement-based\nactive learning, and passive learning of intersections of halfspaces. Our\nanalysis of geometric properties of $s$-concave distributions might be of\nindependent interest to optimization more broadly.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 17:27:57 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 21:10:19 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1703.07771", "submitter": "Hrayr Harutyunyan", "authors": "Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, Greg Ver Steeg,\n  and Aram Galstyan", "title": "Multitask learning and benchmarking with clinical time series data", "comments": "This version of the paper adds details about the generation of the\n  benchmark tasks and describes improved neural baselines", "journal-ref": "Scientific Data 6 (2019) 96", "doi": "10.1038/s41597-019-0103-9", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health care is one of the most exciting frontiers in data mining and machine\nlearning. Successful adoption of electronic health records (EHRs) created an\nexplosion in digital clinical data available for analysis, but progress in\nmachine learning for healthcare research has been difficult to measure because\nof the absence of publicly available benchmark data sets. To address this\nproblem, we propose four clinical prediction benchmarks using data derived from\nthe publicly available Medical Information Mart for Intensive Care (MIMIC-III)\ndatabase. These tasks cover a range of clinical problems including modeling\nrisk of mortality, forecasting length of stay, detecting physiologic decline,\nand phenotype classification. We propose strong linear and neural baselines for\nall four tasks and evaluate the effect of deep supervision, multitask training\nand data-specific architectural modifications on the performance of neural\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 17:53:27 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 21:56:38 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 19:21:40 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Harutyunyan", "Hrayr", ""], ["Khachatrian", "Hrant", ""], ["Kale", "David C.", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1703.07830", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "Randomized Kernel Methods for Least-Squares Support Vector Machines", "comments": "16 pages, 6 figures", "journal-ref": "Int. J. Mod. Phys. C, 28, 1750015 (2017)", "doi": "10.1142/S0129183117500152", "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least-squares support vector machine is a frequently used kernel method\nfor non-linear regression and classification tasks. Here we discuss several\napproximation algorithms for the least-squares support vector machine\nclassifier. The proposed methods are based on randomized block kernel matrices,\nand we show that they provide good accuracy and reliable scaling for\nmulti-class classification problems with relatively large data sets. Also, we\npresent several numerical experiments that illustrate the practical\napplicability of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 19:33:54 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1703.07886", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou", "title": "Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling", "comments": "Accepted for publication at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning and component analysis are part of one of the most\nwell-studied and active research fields, at the intersection of signal and\nimage processing, computer vision, and statistical machine learning. In\ndictionary learning, the current methods of choice are arguably K-SVD and its\nvariants, which learn a dictionary (i.e., a decomposition) for sparse coding\nvia Singular Value Decomposition. In robust component analysis, leading methods\nderive from Principal Component Pursuit (PCP), which recovers a low-rank matrix\nfrom sparse corruptions of unknown magnitude and support. However, K-SVD is\nsensitive to the presence of noise and outliers in the training set.\nAdditionally, PCP does not provide a dictionary that respects the structure of\nthe data (e.g., images), and requires expensive SVD computations when solved by\nconvex relaxation. In this paper, we introduce a new robust decomposition of\nimages by combining ideas from sparse dictionary learning and PCP. We propose a\nnovel Kronecker-decomposable component analysis which is robust to gross\ncorruption, can be used for low-rank modeling, and leverages separability to\nsolve significantly smaller problems. We design an efficient learning algorithm\nby drawing links with a restricted form of tensor factorization. The\neffectiveness of the proposed approach is demonstrated on real-world\napplications, namely background subtraction and image denoising, by performing\na thorough comparison with the current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 23:35:51 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:50:13 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Bahri", "Mehdi", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1703.07904", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Cross-Validation with Confidence", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is one of the most popular model selection methods in\nstatistics and machine learning. Despite its wide applicability, traditional\ncross validation methods tend to select overfitting models, due to the\nignorance of the uncertainty in the testing sample. We develop a new,\nstatistically principled inference tool based on cross-validation that takes\ninto account the uncertainty in the testing sample. This new method outputs a\nset of highly competitive candidate models containing the best one with\nguaranteed probability. As a consequence, our method can achieve consistent\nvariable selection in a classical linear regression setting, for which existing\ncross-validation methods require unconventional split ratios. When used for\nregularizing tuning parameter selection, the method can provide a further\ntrade-off between prediction accuracy and model interpretability. We\ndemonstrate the performance of the proposed method in several simulated and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 01:30:17 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 14:09:10 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1703.07909", "submitter": "Tegjyot Singh Sethi", "authors": "Tegjyot Singh Sethi, Mehmed Kantardzic", "title": "Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial\n  Domains", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.02.007", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern day web applications aim to create impact at the civilization\nlevel, they have become vulnerable to adversarial activity, where the next\ncyber-attack can take any shape and can originate from anywhere. The increasing\nscale and sophistication of attacks, has prompted the need for a data driven\nsolution, with machine learning forming the core of many cybersecurity systems.\nMachine learning was not designed with security in mind, and the essential\nassumption of stationarity, requiring that the training and testing data follow\nsimilar distributions, is violated in an adversarial domain. In this paper, an\nadversary's view point of a classification based system, is presented. Based on\na formal adversarial model, the Seed-Explore-Exploit framework is presented,\nfor simulating the generation of data driven and reverse engineering attacks on\nclassifiers. Experimental evaluation, on 10 real world datasets and using the\nGoogle Cloud Prediction Platform, demonstrates the innate vulnerability of\nclassifiers and the ease with which evasion can be carried out, without any\nexplicit information about the classifier type, the training data or the\napplication domain. The proposed framework, algorithms and empirical\nevaluation, serve as a white hat analysis of the vulnerabilities, and aim to\nfoster the development of secure machine learning frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 02:40:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sethi", "Tegjyot Singh", ""], ["Kantardzic", "Mehmed", ""]]}, {"id": "1703.07915", "submitter": "Dhagash Mehta", "authors": "Andrew J. Ballard, Ritankar Das, Stefano Martiniani, Dhagash Mehta,\n  Levent Sagun, Jacob D. Stevenson, David J. Wales", "title": "Perspective: Energy Landscapes for Machine Learning", "comments": "41 pages, 25 figures. Accepted for publication in Physical Chemistry\n  Chemical Physics, 2017", "journal-ref": null, "doi": "10.1039/C7CP01108C", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV cs.LG hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are being increasingly used as flexible\nnon-linear fitting and prediction tools in the physical sciences. Fitting\nfunctions that exhibit multiple solutions as local minima can be analysed in\nterms of the corresponding machine learning landscape. Methods to explore and\nvisualise molecular potential energy landscapes can be applied to these machine\nlearning landscapes to gain new insight into the solution space involved in\ntraining and the nature of the corresponding predictions. In particular, we can\ndefine quantities analogous to molecular structure, thermodynamics, and\nkinetics, and relate these emergent properties to the structure of the\nunderlying landscape. This Perspective aims to describe these analogies with\nexamples from recent applications, and suggest avenues for new\ninterdisciplinary research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 03:17:14 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ballard", "Andrew J.", ""], ["Das", "Ritankar", ""], ["Martiniani", "Stefano", ""], ["Mehta", "Dhagash", ""], ["Sagun", "Levent", ""], ["Stevenson", "Jacob D.", ""], ["Wales", "David J.", ""]]}, {"id": "1703.07928", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Arpit Jain, Ser Nam Lim", "title": "Self corrective Perturbations for Semantic Segmentation and\n  Classification", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been a subject of great importance over\nthe past decade and great strides have been made in their utility for producing\nstate of the art performance in many computer vision problems. However, the\nbehavior of deep networks is yet to be fully understood and is still an active\narea of research. In this work, we present an intriguing behavior: pre-trained\nCNNs can be made to improve their predictions by structurally perturbing the\ninput. We observe that these perturbations - referred as Guided Perturbations -\nenable a trained network to improve its prediction performance without any\nlearning or change in network weights. We perform various ablative experiments\nto understand how these perturbations affect the local context and feature\nrepresentations. Furthermore, we demonstrate that this idea can improve\nperformance of several existing approaches on semantic segmentation and scene\nlabeling tasks on the PASCAL VOC dataset and supervised classification tasks on\nMNIST and CIFAR10 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 04:25:48 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 15:42:06 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Jain", "Arpit", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1703.07940", "submitter": "Edward Barker", "authors": "Edward Barker and Charl Ras", "title": "Unsupervised Basis Function Adaptation for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using reinforcement learning (RL) algorithms it is common, given a large\nstate space, to introduce some form of approximation architecture for the value\nfunction (VF). The exact form of this architecture can have a significant\neffect on an agent's performance, however, and determining a suitable\napproximation architecture can often be a highly complex task. Consequently\nthere is currently interest among researchers in the potential for allowing RL\nalgorithms to adaptively generate (i.e. to learn) approximation architectures.\nOne relatively unexplored method of adapting approximation architectures\ninvolves using feedback regarding the frequency with which an agent has visited\ncertain states to guide which areas of the state space to approximate with\ngreater detail. In this article we will: (a) informally discuss the potential\nadvantages offered by such methods; (b) introduce a new algorithm based on such\nmethods which adapts a state aggregation approximation architecture on-line and\nis designed for use in conjunction with SARSA; (c) provide theoretical results,\nin a policy evaluation setting, regarding this particular algorithm's\ncomplexity, convergence properties and potential to reduce VF error; and\nfinally (d) test experimentally the extent to which this algorithm can improve\nperformance given a number of different test problems. Taken together our\nresults suggest that our algorithm (and potentially such methods more\ngenerally) can provide a versatile and computationally lightweight means of\nsignificantly boosting RL performance given suitable conditions which are\ncommonly encountered in practice.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 05:23:34 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:26:30 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 23:14:40 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Barker", "Edward", ""], ["Ras", "Charl", ""]]}, {"id": "1703.07948", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, James Cheng, and Jiacheng Zhuo", "title": "Fast Stochastic Variance Reduced Gradient Method with Momentum\n  Acceleration for Machine Learning", "comments": "Corrected a few typos in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, research on accelerated stochastic gradient descent methods (e.g.,\nSVRG) has made exciting progress (e.g., linear convergence for strongly convex\nproblems). However, the best-known methods (e.g., Katyusha) requires at least\ntwo auxiliary variables and two momentum parameters. In this paper, we propose\na fast stochastic variance reduction gradient (FSVRG) method, in which we\ndesign a novel update rule with the Nesterov's momentum and incorporate the\ntechnique of growing epoch size. FSVRG has only one auxiliary variable and one\nmomentum weight, and thus it is much simpler and has much lower per-iteration\ncomplexity. We prove that FSVRG achieves linear convergence for strongly convex\nproblems and the optimal $\\mathcal{O}(1/T^2)$ convergence rate for non-strongly\nconvex problems, where $T$ is the number of outer-iterations. We also extend\nFSVRG to directly solve the problems with non-smooth component functions, such\nas SVM. Finally, we empirically study the performance of FSVRG for solving\nvarious machine learning problems such as logistic regression, ridge\nregression, Lasso and SVM. Our results show that FSVRG outperforms the\nstate-of-the-art stochastic methods, including Katyusha.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 07:13:28 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 17:16:49 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Cheng", "James", ""], ["Zhuo", "Jiacheng", ""]]}, {"id": "1703.07950", "submitter": "Shaked Shammah", "authors": "Shai Shalev-Shwartz and Ohad Shamir and Shaked Shammah", "title": "Failures of Gradient-Based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning has become the go-to solution for a broad\nrange of applications, often outperforming state-of-the-art. However, it is\nimportant, for both theoreticians and practitioners, to gain a deeper\nunderstanding of the difficulties and limitations associated with common\napproaches and algorithms. We describe four types of simple problems, for which\nthe gradient-based algorithms commonly used in deep learning either fail or\nsuffer from significant difficulties. We illustrate the failures through\npractical experiments, and provide theoretical insights explaining their\nsource, and how they might be remedied.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 07:16:37 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 05:23:26 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""], ["Shammah", "Shaked", ""]]}, {"id": "1703.08031", "submitter": "Justin Bewsher", "authors": "Justin D. Bewsher, Alessandra Tosi, Michael A. Osborne, Stephen J.\n  Roberts", "title": "Distribution of Gaussian Process Arc Lengths", "comments": "10 pages, 4 figures, Accepted to The 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first treatment of the arc length of the Gaussian Process (GP)\nwith more than a single output dimension. GPs are commonly used for tasks such\nas trajectory modelling, where path length is a crucial quantity of interest.\nPreviously, only paths in one dimension have been considered, with no\ntheoretical consideration of higher dimensional problems. We fill the gap in\nthe existing literature by deriving the moments of the arc length for a\nstationary GP with multiple output dimensions. A new method is used to derive\nthe mean of a one-dimensional GP over a finite interval, by considering the\ndistribution of the arc length integrand. This technique is used to derive an\napproximate distribution over the arc length of a vector valued GP in\n$\\mathbb{R}^n$ by moment matching the distribution. Numerical simulations\nconfirm our theoretical derivations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:17:00 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Bewsher", "Justin D.", ""], ["Tosi", "Alessandra", ""], ["Osborne", "Michael A.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1703.08052", "submitter": "Maja Rudolph", "authors": "Maja Rudolph, David Blei", "title": "Dynamic Bernoulli Embeddings for Language Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful approach for unsupervised analysis of\nlanguage. Recently, Rudolph et al. (2016) developed exponential family\nembeddings, which cast word embeddings in a probabilistic framework. Here, we\ndevelop dynamic embeddings, building on exponential family embeddings to\ncapture how the meanings of words change over time. We use dynamic embeddings\nto analyze three large collections of historical texts: the U.S. Senate\nspeeches from 1858 to 2009, the history of computer science ACM abstracts from\n1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We\nfind dynamic embeddings provide better fits than classical embeddings and\ncapture interesting patterns about how language changes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 13:00:14 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Rudolph", "Maja", ""], ["Blei", "David", ""]]}, {"id": "1703.08065", "submitter": "Badong Chen", "authors": "Badong Chen, Lei Xing, Haiquan Zhao, Bin Xu, Jose C. Principe", "title": "Robustness of Maximum Correntropy Estimation Against Large Outliers", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum correntropy criterion (MCC) has recently been successfully\napplied in robust regression, classification and adaptive filtering, where the\ncorrentropy is maximized instead of minimizing the well-known mean square error\n(MSE) to improve the robustness with respect to outliers (or impulsive noises).\nConsiderable efforts have been devoted to develop various robust adaptive\nalgorithms under MCC, but so far little insight has been gained as to how the\noptimal solution will be affected by outliers. In this work, we study this\nproblem in the context of parameter estimation for a simple linear\nerrors-in-variables (EIV) model where all variables are scalar. Under certain\nconditions, we derive an upper bound on the absolute value of the estimation\nerror and show that the optimal solution under MCC can be very close to the\ntrue value of the unknown parameter even with outliers (whose values can be\narbitrarily large) in both input and output variables. Illustrative examples\nare presented to verify and clarify the theory.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 13:41:47 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 12:31:17 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Chen", "Badong", ""], ["Xing", "Lei", ""], ["Zhao", "Haiquan", ""], ["Xu", "Bin", ""], ["Principe", "Jose C.", ""]]}, {"id": "1703.08085", "submitter": "Christina Lee Yu", "authors": "Devavrat Shah and Christina Lee Yu", "title": "Reducing Crowdsourcing to Graphon Estimation, Statistically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the correct answers to binary tasks based on multiple noisy answers\nin an unsupervised manner has emerged as the canonical question for micro-task\ncrowdsourcing or more generally aggregating opinions. In graphon estimation,\none is interested in estimating edge intensities or probabilities between nodes\nusing a single snapshot of a graph realization. In the recent literature, there\nhas been exciting development within both of these topics. In the context of\ncrowdsourcing, the key intellectual challenge is to understand whether a given\ntask can be more accurately denoised by aggregating answers collected from\nother different tasks. In the context of graphon estimation, precise\ninformation limits and estimation algorithms remain of interest. In this paper,\nwe utilize a statistical reduction from crowdsourcing to graphon estimation to\nadvance the state-of-art for both of these challenges. We use concepts from\ngraphon estimation to design an algorithm that achieves better performance than\nthe {\\em majority voting} scheme for a setup that goes beyond the {\\em rank\none} models considered in the literature. We use known explicit lower bounds\nfor crowdsourcing to provide refined lower bounds for graphon estimation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 14:29:29 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 20:57:30 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 15:29:25 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 16:10:46 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Shah", "Devavrat", ""], ["Yu", "Christina Lee", ""]]}, {"id": "1703.08110", "submitter": "Mario Lucic", "authors": "Mario Lucic and Matthew Faulkner and Andreas Krause and Dan Feldman", "title": "Training Gaussian Mixture Models at Scale via Coresets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we train a statistical mixture model on a massive data set? In this\nwork we show how to construct coresets for mixtures of Gaussians. A coreset is\na weighted subset of the data, which guarantees that models fitting the coreset\nalso provide a good fit for the original data set. We show that, perhaps\nsurprisingly, Gaussian mixtures admit coresets of size polynomial in dimension\nand the number of mixture components, while being independent of the data set\nsize. Hence, one can harness computationally intensive algorithms to compute a\ngood approximation on a significantly smaller data set. More importantly, such\ncoresets can be efficiently constructed both in distributed and streaming\nsettings and do not impose restrictions on the data generating process. Our\nresults rely on a novel reduction of statistical estimation to problems in\ncomputational geometry and new combinatorial complexity results for mixtures of\nGaussians. Empirical evaluation on several real-world datasets suggests that\nour coreset-based approach enables significant reduction in training-time with\nnegligible approximation error.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 15:35:33 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 21:06:15 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Lucic", "Mario", ""], ["Faulkner", "Matthew", ""], ["Krause", "Andreas", ""], ["Feldman", "Dan", ""]]}, {"id": "1703.08251", "submitter": "David Ledbetter", "authors": "Long Ho, David Ledbetter, Melissa Aczon, Randall Wetzel", "title": "The Dependence of Machine Learning on Electronic Medical Record Quality", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in applying machine learning methods to Electronic\nMedical Records (EMR). Across different institutions, however, EMR quality can\nvary widely. This work investigated the impact of this disparity on the\nperformance of three advanced machine learning algorithms: logistic regression,\nmultilayer perceptron, and recurrent neural network. The EMR disparity was\nemulated using different permutations of the EMR collected at Children's\nHospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and\nCardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using\npatients from the PICU to predict in-ICU mortality for patients in a held out\nset of PICU and CTICU patients. The disparate patient populations between the\nPICU and CTICU provide an estimate of generalization errors across different\nICUs. We quantified and evaluated the generalization of these algorithms on\nvarying EMR size, input types, and fidelity of data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 23:27:12 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ho", "Long", ""], ["Ledbetter", "David", ""], ["Aczon", "Melissa", ""], ["Wetzel", "Randall", ""]]}, {"id": "1703.08267", "submitter": "Songtao Lu", "authors": "Songtao Lu, Mingyi Hong, and Zhengdao Wang", "title": "A Nonconvex Splitting Method for Symmetric Nonnegative Matrix\n  Factorization: Convergence Analysis and Optimality", "comments": "IEEE Transactions on Signal Processing (to appear)", "journal-ref": null, "doi": "10.1109/TSP.2017.2679687", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric nonnegative matrix factorization (SymNMF) has important\napplications in data analytics problems such as document clustering, community\ndetection and image segmentation. In this paper, we propose a novel nonconvex\nvariable splitting method for solving SymNMF. The proposed algorithm is\nguaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the\nnonconvex SymNMF problem. Furthermore, it achieves a global sublinear\nconvergence rate. We also show that the algorithm can be efficiently\nimplemented in parallel. Further, sufficient conditions are provided which\nguarantee the global and local optimality of the obtained solutions. Extensive\nnumerical results performed on both synthetic and real data sets suggest that\nthe proposed algorithm converges quickly to a local minimum solution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 02:31:23 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Lu", "Songtao", ""], ["Hong", "Mingyi", ""], ["Wang", "Zhengdao", ""]]}, {"id": "1703.08383", "submitter": "Joseph Lemley", "authors": "Joseph Lemley, Shabab Bazrafkan, Peter Corcoran", "title": "Smart Augmentation - Learning an Optimal Data Augmentation Strategy", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2017.2696121", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurring problem faced when training neural networks is that there is\ntypically not enough data to maximize the generalization capability of deep\nneural networks(DNN). There are many techniques to address this, including data\naugmentation, dropout, and transfer learning. In this paper, we introduce an\nadditional method which we call Smart Augmentation and we show how to use it to\nincrease the accuracy and reduce overfitting on a target network. Smart\nAugmentation works by creating a network that learns how to generate augmented\ndata during the training process of a target network in a way that reduces that\nnetworks loss. This allows us to learn augmentations that minimize the error of\nthat network.\n  Smart Augmentation has shown the potential to increase accuracy by\ndemonstrably significant measures on all datasets tested. In addition, it has\nshown potential to achieve similar or improved performance levels with\nsignificantly smaller network sizes in a number of tested cases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 12:07:34 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lemley", "Joseph", ""], ["Bazrafkan", "Shabab", ""], ["Corcoran", "Peter", ""]]}, {"id": "1703.08403", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain and David Schultz", "title": "Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor\n  Classification in Dynamic Time Warping Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor method together with the dynamic time warping (DTW)\ndistance is one of the most popular approaches in time series classification.\nThis method suffers from high storage and computation requirements for large\ntraining sets. As a solution to both drawbacks, this article extends learning\nvector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ\nscheme uses asymmetric weighted averaging as update rule. Empirical results\nexhibited superior performance of asymmetric generalized LVQ (GLVQ) over other\nstate-of-the-art prototype generation methods for nearest neighbor\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 13:29:52 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Jain", "Brijnesh", ""], ["Schultz", "David", ""]]}, {"id": "1703.08520", "submitter": "Kaspar M\\\"artens", "authors": "Kaspar M\\\"artens, Michalis K Titsias, Christopher Yau", "title": "Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models", "comments": null, "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha,Okinawa, Japan. PMLR: Volume\n  89", "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for factorial hidden Markov models is challenging due to\nthe exponentially sized latent variable space. Standard Monte Carlo samplers\ncan have difficulties effectively exploring the posterior landscape and are\noften restricted to exploration around localised regions that depend on\ninitialisation. We introduce a general purpose ensemble Markov Chain Monte\nCarlo (MCMC) technique to improve on existing poorly mixing samplers. This is\nachieved by combining parallel tempering and an auxiliary variable scheme to\nexchange information between the chains in an efficient way. The latter\nexploits a genetic algorithm within an augmented Gibbs sampler. We compare our\ntechnique with various existing samplers in a simulation study as well as in a\ncancer genomics application, demonstrating the improvements obtained by our\naugmented ensemble approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 17:17:45 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 20:46:32 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["M\u00e4rtens", "Kaspar", ""], ["Titsias", "Michalis K", ""], ["Yau", "Christopher", ""]]}, {"id": "1703.08544", "submitter": "Joshua Michalenko", "authors": "Joshua J. Michalenko, Andrew S. Lan, Richard G. Baraniuk", "title": "Data-Mining Textual Responses to Uncover Misconception Patterns", "comments": "7 Pages, Submitted to EDM 2017, Workshop version accepted to L@S\n  2017. Article title and acronym changed to more clearly indicate the\n  scientific goal of the paper of improving the quality of educational\n  instruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important, yet largely unstudied, problem in student data analysis is to\ndetect misconceptions from students' responses to open-response questions.\nMisconception detection enables instructors to deliver more targeted feedback\non the misconceptions exhibited by many students in their class, thus improving\nthe quality of instruction. In this paper, we propose a new natural language\nprocessing-based framework to detect the common misconceptions among students'\ntextual responses to short-answer questions. We propose a probabilistic model\nfor students' textual responses involving misconceptions and experimentally\nvalidate it on a real-world student-response dataset. Experimental results show\nthat our proposed framework excels at classifying whether a response exhibits\none or more misconceptions. More importantly, it can also automatically detect\nthe common misconceptions exhibited across responses from multiple students to\nmultiple questions; this property is especially important at large scale, since\ninstructors will no longer need to manually specify all possible misconceptions\nthat students might exhibit.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 14:49:58 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 02:50:33 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Michalenko", "Joshua J.", ""], ["Lan", "Andrew S.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1703.08581", "submitter": "Ron J Weiss", "authors": "Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, Zhifeng Chen", "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech", "comments": "5 pages, 1 figure. Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a recurrent encoder-decoder deep neural network architecture that\ndirectly translates speech in one language into text in another. The model does\nnot explicitly transcribe the speech into text in the source language, nor does\nit require supervision from the ground truth source language transcription\nduring training. We apply a slightly modified sequence-to-sequence with\nattention architecture that has previously been used for speech recognition and\nshow that it can be repurposed for this more complex task, illustrating the\npower of attention-based models. A single model trained end-to-end obtains\nstate-of-the-art performance on the Fisher Callhome Spanish-English speech\ntranslation task, outperforming a cascade of independently trained\nsequence-to-sequence speech recognition and machine translation models by 1.8\nBLEU points on the Fisher test set. In addition, we find that making use of the\ntraining data in both languages by multi-task training sequence-to-sequence\nspeech translation and recognition models with a shared encoder network can\nimprove performance by a further 1.4 BLEU points.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 19:45:24 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 13:54:12 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Weiss", "Ron J.", ""], ["Chorowski", "Jan", ""], ["Jaitly", "Navdeep", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""]]}, {"id": "1703.08619", "submitter": "Simon Bussy", "authors": "Mokhtar Z. Alaya, Simon Bussy, St\\'ephane Ga\\\"iffas, Agathe Guilloux", "title": "Binarsity: a penalization for one-hot encoded features in linear\n  supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of large-scale linear supervised learning\nin settings where a large number of continuous features are available. We\npropose to combine the well-known trick of one-hot encoding of continuous\nfeatures with a new penalization called \\emph{binarsity}. In each group of\nbinary features coming from the one-hot encoding of a single raw continuous\nfeature, this penalization uses total-variation regularization together with an\nextra linear constraint. This induces two interesting properties on the model\nweights of the one-hot encoded features: they are piecewise constant, and are\neventually block sparse. Non-asymptotic oracle inequalities for generalized\nlinear models are proposed. Moreover, under a sparse additive model assumption,\nwe prove that our procedure matches the state-of-the-art in this setting.\nNumerical experiments illustrate the good performances of our approach on\nseveral datasets. It is also noteworthy that our method has a numerical\ncomplexity comparable to standard $\\ell_1$ penalization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 22:54:17 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 10:58:03 GMT"}, {"version": "v3", "created": "Sat, 25 Nov 2017 15:46:07 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2019 14:58:44 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Alaya", "Mokhtar Z.", ""], ["Bussy", "Simon", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Guilloux", "Agathe", ""]]}, {"id": "1703.08667", "submitter": "Ronan Fruit", "authors": "Ronan Fruit, Alessandro Lazaric", "title": "Exploration--Exploitation in MDPs with Options", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a large body of empirical results show that temporally-extended actions\nand options may significantly affect the learning performance of an agent, the\ntheoretical understanding of how and when options can be beneficial in online\nreinforcement learning is relatively limited. In this paper, we derive an upper\nand lower bound on the regret of a variant of UCRL using options. While we\nfirst analyze the algorithm in the general case of semi-Markov decision\nprocesses (SMDPs), we show how these results can be translated to the specific\ncase of MDPs with options and we illustrate simple scenarios in which the\nregret of learning with options can be \\textit{provably} much smaller than the\nregret suffered when learning with primitive actions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 09:30:31 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 09:55:01 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Fruit", "Ronan", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "1703.08705", "submitter": "Franck Dernoncourt", "authors": "Sebastian Gehrmann, Franck Dernoncourt, Yeran Li, Eric T. Carlson, Joy\n  T. Wu, Jonathan Welt, John Foote Jr., Edward T. Moseley, David W. Grant,\n  Patrick D. Tyler, Leo Anthony Celi", "title": "Comparing Rule-Based and Deep Learning Models for Patient Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We investigate whether deep learning techniques for natural\nlanguage processing (NLP) can be used efficiently for patient phenotyping.\nPatient phenotyping is a classification task for determining whether a patient\nhas a medical condition, and is a crucial part of secondary analysis of\nhealthcare data. We assess the performance of deep learning algorithms and\ncompare them with classical NLP approaches.\n  Materials and Methods: We compare convolutional neural networks (CNNs),\nn-gram models, and approaches based on cTAKES that extract pre-defined medical\nconcepts from clinical notes and use them to predict patient phenotypes. The\nperformance is tested on 10 different phenotyping tasks using 1,610 discharge\nsummaries extracted from the MIMIC-III database.\n  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The\naverage F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our\nmodel having an F1-score up to 37 points higher than alternative approaches. We\nadditionally assess the interpretability of our model by presenting a method\nthat extracts the most salient phrases for a particular prediction.\n  Conclusion: We show that NLP methods based on deep learning improve the\nperformance of patient phenotyping. Our CNN-based algorithm automatically\nlearns the phrases associated with each patient phenotype. As such, it reduces\nthe annotation complexity for clinical domain experts, who are normally\nrequired to develop task-specific annotation rules and identify relevant\nphrases. Our method performs well in terms of both performance and\ninterpretability, which indicates that deep learning is an effective approach\nto patient phenotyping based on clinicians' notes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 15:37:09 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Dernoncourt", "Franck", ""], ["Li", "Yeran", ""], ["Carlson", "Eric T.", ""], ["Wu", "Joy T.", ""], ["Welt", "Jonathan", ""], ["Foote", "John", "Jr."], ["Moseley", "Edward T.", ""], ["Grant", "David W.", ""], ["Tyler", "Patrick D.", ""], ["Celi", "Leo Anthony", ""]]}, {"id": "1703.08710", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Genevieve Boucher and Craig A. Glastonbury and\n  Henry Z. Lo and Yoshua Bengio", "title": "Count-ception: Counting by Fully Convolutional Redundant Counting", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting objects in digital images is a process that should be replaced by\nmachines. This tedious task is time consuming and prone to errors due to\nfatigue of human annotators. The goal is to have a system that takes as input\nan image and returns a count of the objects inside and justification for the\nprediction in the form of object localization. We repose a problem, originally\nposed by Lempitsky and Zisserman, to instead predict a count map which contains\nredundant counts based on the receptive field of a smaller regression network.\nThe regression network predicts a count of the objects that exist inside this\nframe. By processing the image in a fully convolutional way each pixel is going\nto be accounted for some number of times, the number of windows which include\nit, which is the size of each window, (i.e., 32x32 = 1024). To recover the true\ncount we take the average over the redundant predictions. Our contribution is\nredundant counting instead of predicting a density map in order to average over\nerrors. We also propose a novel deep neural network architecture adapted from\nthe Inception family of networks called the Count-ception network. Together our\napproach results in a 20% relative improvement (2.9 to 2.3 MAE) over the state\nof the art method by Xie, Noble, and Zisserman in 2016.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 16:49:03 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 17:36:30 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Boucher", "Genevieve", ""], ["Glastonbury", "Craig A.", ""], ["Lo", "Henry Z.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1703.08729", "submitter": "Andrea Montanari", "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari, Roberto I. Oliveira", "title": "Solving SDPs for synchronization and MaxCut problems via the\n  Grothendieck inequality", "comments": "38 pages; 9 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of statistical estimation problems can be addressed by semidefinite\nprograms (SDP). While SDPs are solvable in polynomial time using interior point\nmethods, in practice generic SDP solvers do not scale well to high-dimensional\nproblems. In order to cope with this problem, Burer and Monteiro proposed a\nnon-convex rank-constrained formulation, which has good performance in practice\nbut is still poorly understood theoretically.\n  In this paper we study the rank-constrained version of SDPs arising in MaxCut\nand in synchronization problems. We establish a Grothendieck-type inequality\nthat proves that all the local maxima and dangerous saddle points are within a\nsmall multiplicative gap from the global maximum. We use this structural\ninformation to prove that SDPs can be solved within a known accuracy, by\napplying the Riemannian trust-region method to this non-convex problem, while\nconstraining the rank to be of order one. For the MaxCut problem, our\ninequality implies that any local maximizer of the rank-constrained SDP\nprovides a $ (1 - 1/(k-1)) \\times 0.878$ approximation of the MaxCut, when the\nrank is fixed to $k$.\n  We then apply our results to data matrices generated according to the\nGaussian ${\\mathbb Z}_2$ synchronization problem, and the two-groups stochastic\nblock model with large bounded degree. We prove that the error achieved by\nlocal maximizers undergoes a phase transition at the same threshold as for\ninformation-theoretically optimal methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 18:45:55 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 23:47:43 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""], ["Oliveira", "Roberto I.", ""]]}, {"id": "1703.08737", "submitter": "Guillem Collell", "authors": "Guillem Collell, Teddy Zhang, Marie-Francine Moens", "title": "Learning to Predict: A Fast Re-constructive Method to Generate\n  Multimodal Embeddings", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/03", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating visual and linguistic information into a single multimodal\nrepresentation is an unsolved problem with wide-reaching applications to both\nnatural language processing and computer vision. In this paper, we present a\nsimple method to build multimodal representations by learning a\nlanguage-to-vision mapping and using its output to build multimodal embeddings.\nIn this sense, our method provides a cognitively plausible way of building\nrepresentations, consistent with the inherently re-constructive and associative\nnature of human memory. Using seven benchmark concept similarity tests we show\nthat the mapped vectors not only implicitly encode multimodal information, but\nalso outperform strong unimodal baselines and state-of-the-art multimodal\nmethods, thus exhibiting more \"human-like\" judgments---particularly in\nzero-shot settings.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 20:06:10 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Collell", "Guillem", ""], ["Zhang", "Teddy", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1703.08772", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Xudong Shi and Yu Sun and Li Cheng", "title": "Multivariate Regression with Gross Errors on Manifold-valued Data", "comments": "14 pages, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the topic of multivariate regression on manifold-valued output,\nthat is, for a multivariate observation, its output response lies on a\nmanifold. Moreover, we propose a new regression model to deal with the presence\nof grossly corrupted manifold-valued responses, a bottleneck issue commonly\nencountered in practical scenarios. Our model first takes a correction step on\nthe grossly corrupted responses via geodesic curves on the manifold, and then\nperforms multivariate linear regression on the corrected data. This results in\na nonconvex and nonsmooth optimization problem on manifolds. To this end, we\npropose a dedicated approach named PALMR, by utilizing and extending the\nproximal alternating linearized minimization techniques. Theoretically, we\ninvestigate its convergence property, where it is shown to converge to a\ncritical point under mild conditions. Empirically, we test our model on both\nsynthetic and real diffusion tensor imaging data, and show that our model\noutperforms other multivariate regression models when manifold-valued responses\ncontain gross errors, and is effective in identifying gross errors.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 05:53:39 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:27:35 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Shi", "Xudong", ""], ["Sun", "Yu", ""], ["Cheng", "Li", ""]]}, {"id": "1703.08816", "submitter": "Konstantinos Zygalakis", "authors": "Andrea L. Bertozzi and Xiyang Luo and Andrew M. Stuart and\n  Konstantinos C. Zygalakis", "title": "Uncertainty quantification in graph-based classification of high\n  dimensional data", "comments": "33 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of high dimensional data finds wide-ranging applications. In\nmany of these applications equipping the resulting classification with a\nmeasure of uncertainty may be as important as the classification itself. In\nthis paper we introduce, develop algorithms for, and investigate the properties\nof, a variety of Bayesian models for the task of binary classification; via the\nposterior distribution on the classification labels, these methods\nautomatically give measures of uncertainty. The methods are all based around\nthe graph formulation of semi-supervised learning.\n  We provide a unified framework which brings together a variety of methods\nwhich have been introduced in different communities within the mathematical\nsciences. We study probit classification in the graph-based setting, generalize\nthe level-set method for Bayesian inverse problems to the classification\nsetting, and generalize the Ginzburg-Landau optimization-based classifier to a\nBayesian setting; we also show that the probit and level set approaches are\nnatural relaxations of the harmonic function approach introduced in [Zhu et al\n2003].\n  We introduce efficient numerical methods, suited to large data-sets, for both\nMCMC-based sampling as well as gradient-based MAP estimation. Through numerical\nexperiments we study classification accuracy and uncertainty quantification for\nour models; these experiments showcase a suite of datasets commonly used to\nevaluate graph-based semi-supervised learning algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 13:29:25 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 19:16:13 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bertozzi", "Andrea L.", ""], ["Luo", "Xiyang", ""], ["Stuart", "Andrew M.", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "1703.08831", "submitter": "Saber Salehkaleybar", "authors": "Saber Salehkaleybar, S. Jamaloddin Golestani", "title": "Token-based Function Computation with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed function computation, each node has an initial value and the\ngoal is to compute a function of these values in a distributed manner. In this\npaper, we propose a novel token-based approach to compute a wide class of\ntarget functions to which we refer as \"Token-based function Computation with\nMemory\" (TCM) algorithm. In this approach, node values are attached to tokens\nand travel across the network. Each pair of travelling tokens would coalesce\nwhen they meet, forming a token with a new value as a function of the original\ntoken values. In contrast to the Coalescing Random Walk (CRW) algorithm, where\ntoken movement is governed by random walk, meeting of tokens in our scheme is\naccelerated by adopting a novel chasing mechanism. We proved that, compared to\nthe CRW algorithm, the TCM algorithm results in a reduction of time complexity\nby a factor of at least $\\sqrt{n/\\log(n)}$ in Erd\\\"os-Renyi and complete\ngraphs, and by a factor of $\\log(n)/\\log(\\log(n))$ in torus networks.\nSimulation results show that there is at least a constant factor improvement in\nthe message complexity of TCM algorithm in all considered topologies.\nRobustness of the CRW and TCM algorithms in the presence of node failure is\nanalyzed. We show that their robustness can be improved by running multiple\ninstances of the algorithms in parallel.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:01:28 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Salehkaleybar", "Saber", ""], ["Golestani", "S. Jamaloddin", ""]]}, {"id": "1703.08937", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing strategies for finite-armed stochastic bandits mostly depend on a\nparameter of scale that must be known in advance. Sometimes this is in the form\nof a bound on the payoffs, or the knowledge of a variance or subgaussian\nparameter. The notable exceptions are the analysis of Gaussian bandits with\nunknown mean and variance by Cowan and Katehakis [2015] and of uniform\ndistributions with unknown support [Cowan and Katehakis, 2015]. The results\nderived in these specialised cases are generalised here to the non-parametric\nsetup, where the learner knows only a bound on the kurtosis of the noise, which\nis a scale free measure of the extremity of outliers.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 05:41:03 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1703.08972", "submitter": "Marc Abeille", "authors": "Marc Abeille and Alessandro Lazaric", "title": "Thompson Sampling for Linear-Quadratic Control Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the exploration-exploitation tradeoff in linear quadratic (LQ)\ncontrol problems, where the state dynamics is linear and the cost function is\nquadratic in states and controls. We analyze the regret of Thompson sampling\n(TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist\nsetting, i.e., when the parameters characterizing the LQ dynamics are fixed.\nDespite the empirical and theoretical success in a wide range of problems from\nmulti-armed bandit to linear bandit, we show that when studying the frequentist\nregret TS in control problems, we need to trade-off the frequency of sampling\noptimistic parameters and the frequency of switches in the control policy. This\nresults in an overall regret of $O(T^{2/3})$, which is significantly worse than\nthe regret $O(\\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty\nalgorithm in LQ control problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 08:45:57 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Abeille", "Marc", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "1703.08991", "submitter": "Philipp Probst", "authors": "Philipp Probst, Quay Au, Giuseppe Casalicchio, Clemens Stachl and\n  Bernd Bischl", "title": "Multilabel Classification with R Package mlr", "comments": "18 pages, 2 figures, to be published in R Journal; reference\n  corrected", "journal-ref": "The R Journal 9/1 (2017) 352-369", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We implemented several multilabel classification algorithms in the machine\nlearning package mlr. The implemented methods are binary relevance, classifier\nchains, nested stacking, dependent binary relevance and stacking, which can be\nused with any base learner that is accessible in mlr. Moreover, there is access\nto the multilabel classification versions of randomForestSRC and rFerns. All\nthese methods can be easily compared by different implemented multilabel\nperformance measures and resampling methods in the standardized mlr framework.\nIn a benchmark experiment with several multilabel datasets, the performance of\nthe different methods is evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 10:03:27 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 08:04:21 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Probst", "Philipp", ""], ["Au", "Quay", ""], ["Casalicchio", "Giuseppe", ""], ["Stachl", "Clemens", ""], ["Bischl", "Bernd", ""]]}, {"id": "1703.09112", "submitter": "Li-Fang Cheng", "authors": "Li-Fang Cheng, Gregory Darnell, Bianca Dumitrascu, Corey Chivers,\n  Michael E Draugelis, Kai Li, Barbara E Engelhardt", "title": "Sparse Multi-Output Gaussian Processes for Medical Time Series\n  Prediction", "comments": "Add new results and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scenario of real-time monitoring of hospital patients, high-quality\ninference of patients' health status using all information available from\nclinical covariates and lab tests is essential to enable successful medical\ninterventions and improve patient outcomes. Developing a computational\nframework that can learn from observational large-scale electronic health\nrecords (EHRs) and make accurate real-time predictions is a critical step. In\nthis work, we develop and explore a Bayesian nonparametric model based on\nGaussian process (GP) regression for hospital patient monitoring. We propose\nMedGP, a statistical framework that incorporates 24 clinical and lab covariates\nand supports a rich reference data set from which relationships between\nobserved covariates may be inferred and exploited for high-quality inference of\npatient state over time. To do this, we develop a highly structured sparse GP\nkernel to enable tractable computation over tens of thousands of time points\nwhile estimating correlations among clinical covariates, patients, and\nperiodicity in patient observations. MedGP has a number of benefits over\ncurrent methods, including (i) not requiring an alignment of the time series\ndata, (ii) quantifying confidence regions in the predictions, (iii) exploiting\na vast and rich database of patients, and (iv) inferring interpretable\nrelationships among clinical covariates. We evaluate and compare results from\nMedGP on the task of online prediction for three patient subgroups from two\nmedical data sets across 8,043 patients. We found MedGP improves online\nprediction over baseline methods for nearly all covariates across different\ndisease subgroups and studies. The publicly available code is at\nhttps://github.com/bee-hive/MedGP.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:38:15 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 18:17:03 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Cheng", "Li-Fang", ""], ["Darnell", "Gregory", ""], ["Dumitrascu", "Bianca", ""], ["Chivers", "Corey", ""], ["Draugelis", "Michael E", ""], ["Li", "Kai", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1703.09165", "submitter": "Xuehang Zheng", "authors": "Xuehang Zheng, Saiprasad Ravishankar, Yong Long, and Jeffrey A.\n  Fessler", "title": "PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for\n  Low-Dose 3D CT Image Reconstruction", "comments": "Accepted to IEEE Transaction on Medical Imaging", "journal-ref": "IEEE Transaction on Medical Imaging 37(6):1498-510 Jun 2018", "doi": "10.1109/TMI.2018.2832007", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of computed tomography (CT) image reconstruction methods that\nsignificantly reduce patient radiation exposure while maintaining high image\nquality is an important area of research in low-dose CT (LDCT) imaging. We\npropose a new penalized weighted least squares (PWLS) reconstruction method\nthat exploits regularization based on an efficient Union of Learned TRAnsforms\n(PWLS-ULTRA). The union of square transforms is pre-learned from numerous image\npatches extracted from a dataset of CT images or volumes. The proposed\nPWLS-based cost function is optimized by alternating between a CT image\nreconstruction step, and a sparse coding and clustering step. The CT image\nreconstruction step is accelerated by a relaxed linearized augmented Lagrangian\nmethod with ordered-subsets that reduces the number of forward and back\nprojections. Simulations with 2-D and 3-D axial CT scans of the extended\ncardiac-torso phantom and 3D helical chest and abdomen scans show that for both\nnormal-dose and low-dose levels, the proposed method significantly improves the\nquality of reconstructed images compared to PWLS reconstruction with a\nnonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization\nbased on a union of learned transforms leads to better image reconstructions\nthan using a single learned square transform. We also incorporate patch-based\nweights in PWLS-ULTRA that enhance image quality and help improve image\nresolution uniformity. The proposed approach achieves comparable or better\nimage quality compared to learned overcomplete synthesis dictionaries, but\nimportantly, is much faster (computationally more efficient).\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:16:35 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 04:18:10 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 08:27:13 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Zheng", "Xuehang", ""], ["Ravishankar", "Saiprasad", ""], ["Long", "Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1703.09194", "submitter": "Geoffrey Roeder", "authors": "Geoffrey Roeder, Yuhuai Wu, David Duvenaud", "title": "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for\n  Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and general variant of the standard reparameterized\ngradient estimator for the variational evidence lower bound. Specifically, we\nremove a part of the total derivative with respect to the variational\nparameters that corresponds to the score function. Removing this term produces\nan unbiased gradient estimator whose variance approaches zero as the\napproximate posterior approaches the exact posterior. We analyze the behavior\nof this gradient estimator theoretically and empirically, and generalize it to\nmore complex variational distributions such as mixtures and importance-weighted\nposteriors.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:25:02 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 21:22:19 GMT"}, {"version": "v3", "created": "Sun, 28 May 2017 17:47:33 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Roeder", "Geoffrey", ""], ["Wu", "Yuhuai", ""], ["Duvenaud", "David", ""]]}, {"id": "1703.09202", "submitter": "Aran Nayebi", "authors": "Aran Nayebi, Surya Ganguli", "title": "Biologically inspired protection of deep networks from adversarial\n  attacks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by biophysical principles underlying nonlinear dendritic computation\nin neural circuits, we develop a scheme to train deep neural networks to make\nthem robust to adversarial attacks. Our scheme generates highly nonlinear,\nsaturated neural networks that achieve state of the art performance on gradient\nbased adversarial examples on MNIST, despite never being exposed to\nadversarially chosen examples during training. Moreover, these networks exhibit\nunprecedented robustness to targeted, iterative schemes for generating\nadversarial examples, including second-order methods. We further identify\nprinciples governing how these networks achieve their robustness, drawing on\nmethods from information geometry. We find these networks progressively create\nhighly flat and compressed internal representations that are sensitive to very\nfew input dimensions, while still solving the task. Moreover, they employ\nhighly kurtotic weight distributions, also found in the brain, and we\ndemonstrate how such kurtosis can protect even linear classifiers from\nadversarial attack.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:45:07 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Nayebi", "Aran", ""], ["Ganguli", "Surya", ""]]}, {"id": "1703.09207", "submitter": "Richard Berk", "authors": "Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth", "title": "Fairness in Criminal Justice Risk Assessments: The State of the Art", "comments": "Under a Revise and Resubmit", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Discussions of fairness in criminal justice risk assessments\ntypically lack conceptual precision. Rhetoric too often substitutes for careful\nanalysis. In this paper, we seek to clarify the tradeoffs between different\nkinds of fairness and between fairness and accuracy.\n  Methods: We draw on the existing literatures in criminology, computer science\nand statistics to provide an integrated examination of fairness and accuracy in\ncriminal justice risk assessments. We also provide an empirical illustration\nusing data from arraignments.\n  Results: We show that there are at least six kinds of fairness, some of which\nare incompatible with one another and with accuracy.\n  Conclusions: Except in trivial cases, it is impossible to maximize accuracy\nand fairness at the same time, and impossible simultaneously to satisfy all\nkinds of fairness. In practice, a major complication is different base rates\nacross different legally protected groups. There is a need to consider\nchallenging tradeoffs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 17:50:53 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 03:43:12 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Berk", "Richard", ""], ["Heidari", "Hoda", ""], ["Jabbari", "Shahin", ""], ["Kearns", "Michael", ""], ["Roth", "Aaron", ""]]}, {"id": "1703.09244", "submitter": "Benedetta Tondi", "authors": "Mauro Barni and Benedetta Tondi", "title": "Adversarial Source Identification Game with Corrupted Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the source identification game with training data in\nwhich part of the training data is corrupted by an attacker. In the addressed\nscenario, the defender aims at deciding whether a test sequence has been drawn\naccording to a discrete memoryless source $X \\sim P_X$, whose statistics are\nknown to him through the observation of a training sequence generated by $X$.\nIn order to undermine the correct decision under the alternative hypothesis\nthat the test sequence has not been drawn from $X$, the attacker can modify a\nsequence produced by a source $Y \\sim P_Y$ up to a certain distortion, and\ncorrupt the training sequence either by adding some fake samples or by\nreplacing some samples with fake ones. We derive the unique rationalizable\nequilibrium of the two versions of the game in the asymptotic regime and by\nassuming that the defender bases its decision by relying only on the first\norder statistics of the test and the training sequences. By mimicking Stein's\nlemma, we derive the best achievable performance for the defender when the\nfirst type error probability is required to tend to zero exponentially fast\nwith an arbitrarily small, yet positive, error exponent. We then use such a\nresult to analyze the ultimate distinguishability of any two sources as a\nfunction of the allowed distortion and the fraction of corrupted samples\ninjected into the training sequence.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 18:07:32 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Barni", "Mauro", ""], ["Tondi", "Benedetta", ""]]}, {"id": "1703.09310", "submitter": "Brett Israelsen", "authors": "Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green,\n  Winston Bennett Jr", "title": "Adaptive Simulation-based Training of AI Decision-makers using Bayesian\n  Optimization", "comments": "submitted to JAIS for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies how an AI-controlled dog-fighting agent with tunable\ndecision-making parameters can learn to optimize performance against an\nintelligent adversary, as measured by a stochastic objective function evaluated\non simulated combat engagements. Gaussian process Bayesian optimization (GPBO)\ntechniques are developed to automatically learn global Gaussian Process (GP)\nsurrogate models, which provide statistical performance predictions in both\nexplored and unexplored areas of the parameter space. This allows a learning\nengine to sample full-combat simulations at parameter values that are most\nlikely to optimize performance and also provide highly informative data points\nfor improving future predictions. However, standard GPBO methods do not provide\na reliable surrogate model for the highly volatile objective functions found in\naerial combat, and thus do not reliably identify global maxima. These issues\nare addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point\nSampling (HRMS) techniques. Simulation studies show that HRMS improves the\naccuracy of GP surrogate models, allowing AI decision-makers to more accurately\npredict performance and efficiently tune parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 21:05:15 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 22:54:26 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Israelsen", "Brett W.", ""], ["Ahmed", "Nisar", ""], ["Center", "Kenneth", ""], ["Green", "Roderick", ""], ["Bennett", "Winston", "Jr"]]}, {"id": "1703.09390", "submitter": "Sean McGregor", "authors": "Sean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer,\n  Thomas G. Dietterich", "title": "Factoring Exogenous State for Model-Free Monte Carlo", "comments": "9 pages, 5 figures. Corrected equation 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy analysts wish to visualize a range of policies for large\nsimulator-defined Markov Decision Processes (MDPs). One visualization approach\nis to invoke the simulator to generate on-policy trajectories and then\nvisualize those trajectories. When the simulator is expensive, this is not\npractical, and some method is required for generating trajectories for new\npolicies without invoking the simulator. The method of Model-Free Monte Carlo\n(MFMC) can do this by stitching together state transitions for a new policy\nbased on previously-sampled trajectories from other policies. This \"off-policy\nMonte Carlo simulation\" method works well when the state space has low\ndimension but fails as the dimension grows. This paper describes a method for\nfactoring out some of the state and action variables so that MFMC can work in\nhigh-dimensional MDPs. The new method, MFMCi, is evaluated on a very\nchallenging wildfire management MDP.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 03:32:55 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 20:34:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["McGregor", "Sean", ""], ["Houtman", "Rachel", ""], ["Montgomery", "Claire", ""], ["Metoyer", "Ronald", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1703.09391", "submitter": "Sean McGregor", "authors": "Sean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer,\n  Thomas G. Dietterich", "title": "Fast Optimization of Wildfire Suppression Policies with SMAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managers of US National Forests must decide what policy to apply for dealing\nwith lightning-caused wildfires. Conflicts among stakeholders (e.g., timber\ncompanies, home owners, and wildlife biologists) have often led to spirited\npolitical debates and even violent eco-terrorism. One way to transform these\nconflicts into multi-stakeholder negotiations is to provide a high-fidelity\nsimulation environment in which stakeholders can explore the space of\nalternative policies and understand the tradeoffs therein. Such an environment\nneeds to support fast optimization of MDP policies so that users can adjust\nreward functions and analyze the resulting optimal policies. This paper\nassesses the suitability of SMAC---a black-box empirical function optimization\nalgorithm---for rapid optimization of MDP policies. The paper describes five\nreward function components and four stakeholder constituencies. It then\nintroduces a parameterized class of policies that can be easily understood by\nthe stakeholders. SMAC is applied to find the optimal policy in this class for\nthe reward functions of each of the stakeholder constituencies. The results\nconfirm that SMAC is able to rapidly find good policies that make sense from\nthe domain perspective. Because the full-fidelity forest fire simulator is far\ntoo expensive to support interactive optimization, SMAC is applied to a\nsurrogate model constructed from a modest number of runs of the full-fidelity\nsimulator. To check the quality of the SMAC-optimized policies, the policies\nare evaluated on the full-fidelity simulator. The results confirm that the\nsurrogate values estimates are valid. This is the first successful optimization\nof wildfire management policies using a full-fidelity simulation. The same\nmethodology should be applicable to other contentious natural resource\nmanagement problems where high-fidelity simulation is extremely expensive.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 03:33:02 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["McGregor", "Sean", ""], ["Houtman", "Rachel", ""], ["Montgomery", "Claire", ""], ["Metoyer", "Ronald", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1703.09397", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda and Shun Kataoka", "title": "Solving Non-parametric Inverse Problem in Continuous Markov Random Field\n  using Loopy Belief Propagation", "comments": null, "journal-ref": null, "doi": "10.7566/JPSJ.86.084806", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the inverse problem, or the statistical machine\nlearning problem, in Markov random fields with a non-parametric pair-wise\nenergy function with continuous variables. The inverse problem is formulated by\nmaximum likelihood estimation. The exact treatment of maximum likelihood\nestimation is intractable because of two problems: (1) it includes the\nevaluation of the partition function and (2) it is formulated in the form of\nfunctional optimization. We avoid Problem (1) by using Bethe approximation.\nBethe approximation is an approximation technique equivalent to the loopy\nbelief propagation. Problem (2) can be solved by using orthonormal function\nexpansion. Orthonormal function expansion can reduce a functional optimization\nproblem to a function optimization problem. Our method can provide an analytic\nform of the solution of the inverse problem within the framework of Bethe\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 04:42:11 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Yasuda", "Muneki", ""], ["Kataoka", "Shun", ""]]}, {"id": "1703.09477", "submitter": "Guillaume Garrigos", "authors": "Guillaume Garrigos, Lorenzo Rosasco, Silvia Villa", "title": "Convergence of the Forward-Backward Algorithm: Beyond the Worst Case\n  with the Help of Geometry", "comments": "After peer-review, the paper has been significantly modified: i)\n  Section 3.3 has been completely rewritten, and contains a new sum rule\n  (Theorem 3.15) ii) The end of Section 4.2 and Section 5.2 have been rewritten\n  to include mirror-stratifiable problems iii) The Annex contains new proofs\n  for small-but-not-trivial claims made throughout the paper iv) Theorems,\n  Examples etc have been renumbered", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We provide a comprehensive study of the convergence of the forward-backward\nalgorithm under suitable geometric conditions, such as conditioning or\n{\\L}ojasiewicz properties. These geometrical notions are usually local by\nnature, and may fail to describe the fine geometry of objective functions\nrelevant in inverse problems and signal processing, that have a nice behaviour\non manifolds, or sets open with respect to a weak topology. Motivated by this\nobservation, we revisit those geometric notions over arbitrary sets. In turn,\nthis allows us to present several new results as well as collect in a unified\nview a variety of results scattered in the literature. Our contributions\ninclude the analysis of infinite dimensional convex minimization problems,\nshowing the first {\\L}ojasiewicz inequality for a quadratic function associated\nto a compact operator, and the derivation of new linear rates for problems\narising from inverse problems with low-complexity priors. Our approach allows\nto establish unexpected connections between geometry and a priori conditions in\ninverse problems, such as source conditions, or restricted isometry properties.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:40:30 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 10:40:02 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 14:57:43 GMT"}, {"version": "v4", "created": "Fri, 13 Nov 2020 15:44:39 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Garrigos", "Guillaume", ""], ["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""]]}, {"id": "1703.09480", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall, Aaron Bostrom, James Large and Jason Lines", "title": "Simulated Data Experiments for Time Series Classification Part 1:\n  Accuracy Comparison with Default Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are now a broad range of time series classification (TSC) algorithms\ndesigned to exploit different representations of the data. These have been\nevaluated on a range of problems hosted at the UCR-UEA TSC Archive\n(www.timeseriesclassification.com), and there have been extensive comparative\nstudies. However, our understanding of why one algorithm outperforms another is\nstill anecdotal at best. This series of experiments is meant to help provide\ninsights into what sort of discriminatory features in the data lead one set of\nalgorithms that exploit a particular representation to be better than other\nalgorithms. We categorise five different feature spaces exploited by TSC\nalgorithms then design data simulators to generate randomised data from each\nrepresentation. We describe what results we expected from each class of\nalgorithm and data representation, then observe whether these prior beliefs are\nsupported by the experimental evidence. We provide an open source\nimplementation of all the simulators to allow for the controlled testing of\nhypotheses relating to classifier performance on different data\nrepresentations. We identify many surprising results that confounded our\nexpectations, and use these results to highlight how an over simplified view of\nclassifier structure can often lead to erroneous prior beliefs. We believe\nensembling can often overcome prior bias, and our results support the belief by\nshowing that the ensemble approach adopted by the Hierarchical Collective of\nTransform based Ensembles (HIVE-COTE) is significantly better than the\nalternatives when the data representation is unknown, and is significantly\nbetter than, or not significantly significantly better than, or not\nsignificantly worse than, the best other approach on three out of five of the\nindividual simulators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:48:52 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Bagnall", "Anthony", ""], ["Bostrom", "Aaron", ""], ["Large", "James", ""], ["Lines", "Jason", ""]]}, {"id": "1703.09528", "submitter": "Anh Tong", "authors": "Anh Tong and Jaesik Choi", "title": "Discovering Latent Covariance Structures for Multiple Time Series", "comments": "ICML2019, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing multivariate time series data is important to predict future events\nand changes of complex systems in finance, manufacturing, and administrative\ndecisions. The expressiveness power of Gaussian Process (GP) regression methods\nhas been significantly improved by compositional covariance structures. In this\npaper, we present a new GP model which naturally handles multiple time series\nby placing an Indian Buffet Process (IBP) prior on the presence of shared\nkernels. Our selective covariance structure decomposition allows exploiting\nshared parameters over a set of multiple, selected time series. We also\ninvestigate the well-definedness of the models when infinite latent components\nare introduced. We present a pragmatic search algorithm which explores a larger\nstructure space efficiently. Experiments conducted on five real-world data sets\ndemonstrate that our new model outperforms existing methods in term of\nstructure discoveries and predictive performances.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:10:45 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 07:16:12 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:36:06 GMT"}, {"version": "v4", "created": "Wed, 22 May 2019 06:39:41 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Tong", "Anh", ""], ["Choi", "Jaesik", ""]]}, {"id": "1703.09580", "submitter": "Maren Mahsereci", "authors": "Maren Mahsereci, Lukas Balles, Christoph Lassner, Philipp Hennig", "title": "Early Stopping without a Validation Set", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early stopping is a widely used technique to prevent poor generalization\nperformance when training an over-expressive model by means of gradient-based\noptimization. To find a good point to halt the optimizer, a common practice is\nto split the dataset into a training and a smaller validation set to obtain an\nongoing estimate of the generalization performance. We propose a novel early\nstopping criterion based on fast-to-compute local statistics of the computed\ngradients and entirely removes the need for a held-out validation set. Our\nexperiments show that this is a viable approach in the setting of least-squares\nand logistic regression, as well as neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 15:39:09 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 12:41:22 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Mahsereci", "Maren", ""], ["Balles", "Lukas", ""], ["Lassner", "Christoph", ""], ["Hennig", "Philipp", ""]]}, {"id": "1703.09631", "submitter": "Greg Ongie", "authors": "Greg Ongie, Rebecca Willett, Robert D. Nowak, Laura Balzano", "title": "Algebraic Variety Models for High-Rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of low-rank matrix completion to the case where\nthe data belongs to an algebraic variety, i.e. each data point is a solution to\na system of polynomial equations. In this case the original matrix is possibly\nhigh-rank, but it becomes low-rank after mapping each column to a higher\ndimensional space of monomial features. Many well-studied extensions of linear\nmodels, including affine subspaces and their union, can be described by a\nvariety model. In addition, varieties can be used to model a richer class of\nnonlinear quadratic and higher degree curves and surfaces. We study the\nsampling requirements for matrix completion under a variety model with a focus\non a union of affine subspaces. We also propose an efficient matrix completion\nalgorithm that minimizes a convex or non-convex surrogate of the rank of the\nmatrix of monomial features. Our algorithm uses the well-known \"kernel trick\"\nto avoid working directly with the high-dimensional monomial matrix. We show\nthe proposed algorithm is able to recover synthetically generated data up to\nthe predicted sampling complexity bounds. The proposed algorithm also\noutperforms standard low rank matrix completion and subspace clustering\ntechniques in experiments with real data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 15:28:28 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Ongie", "Greg", ""], ["Willett", "Rebecca", ""], ["Nowak", "Robert D.", ""], ["Balzano", "Laura", ""]]}, {"id": "1703.09646", "submitter": "Rundong Du", "authors": "Rundong Du, Barry Drake, Haesun Park", "title": "Hybrid Clustering based on Content and Connection Structure using Joint\n  Nonnegative Matrix Factorization", "comments": "9 pages, Submitted to a conference, Feb. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid method for latent information discovery on the data sets\ncontaining both text content and connection structure based on constrained low\nrank approximation. The new method jointly optimizes the Nonnegative Matrix\nFactorization (NMF) objective function for text clustering and the Symmetric\nNMF (SymNMF) objective function for graph clustering. We propose an effective\nalgorithm for the joint NMF objective function, based on a block coordinate\ndescent (BCD) framework. The proposed hybrid method discovers content\nassociations via latent connections found using SymNMF. The method can also be\napplied with a natural conversion of the problem when a hypergraph formulation\nis used or the content is associated with hypergraph edges.\n  Experimental results show that by simultaneously utilizing both content and\nconnection structure, our hybrid method produces higher quality clustering\nresults compared to the other NMF clustering methods that uses content alone\n(standard NMF) or connection structure alone (SymNMF). We also present some\ninteresting applications to several types of real world data such as citation\nrecommendations of papers. The hybrid method proposed in this paper can also be\napplied to general data expressed with both feature space vectors and pairwise\nsimilarities and can be extended to the case with multiple feature spaces or\nmultiple similarity measures.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 15:56:53 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Du", "Rundong", ""], ["Drake", "Barry", ""], ["Park", "Haesun", ""]]}, {"id": "1703.09700", "submitter": "Antti Kangasr\\\"a\\\"asi\\\"o", "authors": "Antti Kangasr\\\"a\\\"asi\\\"o, Samuel Kaski", "title": "Inverse Reinforcement Learning from Summary Data", "comments": "To appear in ECMLPKDD'2018", "journal-ref": null, "doi": "10.1007/s10994-018-5730-4", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning (IRL) aims to explain observed strategic\nbehavior by fitting reinforcement learning models to behavioral data. However,\ntraditional IRL methods are only applicable when the observations are in the\nform of state-action paths. This assumption may not hold in many real-world\nmodeling settings, where only partial or summarized observations are available.\nIn general, we may assume that there is a summarizing function $\\sigma$, which\nacts as a filter between us and the true state-action paths that constitute the\ndemonstration. Some initial approaches to extending IRL to such situations have\nbeen presented, but with very specific assumptions about the structure of\n$\\sigma$, such as that only certain state observations are missing. This paper\ninstead focuses on the most general case of the problem, where no assumptions\nare made about the summarizing function, except that it can be evaluated. We\ndemonstrate that inference is still possible. The paper presents exact and\napproximate inference algorithms that allow full posterior inference, which is\nparticularly important for assessing parameter uncertainty in this challenging\ninference situation. Empirical scalability is demonstrated to reasonably sized\nproblems, and practical applicability is demonstrated by estimating the\nposterior for a cognitive science RL model based on an observed user's task\ncompletion time only.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 16:13:23 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 14:55:11 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 06:46:22 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Kaski", "Samuel", ""]]}, {"id": "1703.09766", "submitter": "Kai Fan", "authors": "Kai Fan", "title": "Unifying the Stochastic Spectral Descent for Restricted Boltzmann\n  Machines with Bernoulli or Gaussian Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent based algorithms are typically used as the\ngeneral optimization tools for most deep learning models. A Restricted\nBoltzmann Machine (RBM) is a probabilistic generative model that can be stacked\nto construct deep architectures. For RBM with Bernoulli inputs, non-Euclidean\nalgorithm such as stochastic spectral descent (SSD) has been specifically\ndesigned to speed up the convergence with improved use of the gradient\nestimation by sampling methods. However, the existing algorithm and\ncorresponding theoretical justification depend on the assumption that the\npossible configurations of inputs are finite, like binary variables. The\npurpose of this paper is to generalize SSD for Gaussian RBM being capable of\nmod- eling continuous data, regardless of the previous assumption. We propose\nthe gradient descent methods in non-Euclidean space of parameters, via de-\nriving the upper bounds of logarithmic partition function for RBMs based on\nSchatten-infinity norm. We empirically show that the advantage and improvement\nof SSD over stochastic gradient descent (SGD).\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 19:42:16 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Fan", "Kai", ""]]}, {"id": "1703.09772", "submitter": "Dorian Cazau", "authors": "D. Cazau, G. Revillon, W. Yuancheng, O. Adam", "title": "Particle Filtering for PLCA model with Application to Music\n  Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Music Transcription (AMT) consists in automatically estimating the\nnotes in an audio recording, through three attributes: onset time, duration and\npitch. Probabilistic Latent Component Analysis (PLCA) has become very popular\nfor this task. PLCA is a spectrogram factorization method, able to model a\nmagnitude spectrogram as a linear combination of spectral vectors from a\ndictionary. Such methods use the Expectation-Maximization (EM) algorithm to\nestimate the parameters of the acoustic model. This algorithm presents\nwell-known inherent defaults (local convergence, initialization dependency),\nmaking EM-based systems limited in their applications to AMT, particularly in\nregards to the mathematical form and number of priors. To overcome such limits,\nwe propose in this paper to employ a different estimation framework based on\nParticle Filtering (PF), which consists in sampling the posterior distribution\nover larger parameter ranges. This framework proves to be more robust in\nparameter estimation, more flexible and unifying in the integration of prior\nknowledge in the system. Note-level transcription accuracies of 61.8 $\\%$ and\n59.5 $\\%$ were achieved on evaluation sound datasets of two different\ninstrument repertoires, including the classical piano (from MAPS dataset) and\nthe marovany zither, and direct comparisons to previous PLCA-based approaches\nare provided. Steps for further development are also outlined.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 19:56:47 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Cazau", "D.", ""], ["Revillon", "G.", ""], ["Yuancheng", "W.", ""], ["Adam", "O.", ""]]}, {"id": "1703.09775", "submitter": "Dorian Cazau", "authors": "D. Cazau, G. Revillon, O. Adam", "title": "Deep scattering transform applied to note onset detection and instrument\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Music Transcription (AMT) is one of the oldest and most\nwell-studied problems in the field of music information retrieval. Within this\nchallenging research field, onset detection and instrument recognition take\nimportant places in transcription systems, as they respectively help to\ndetermine exact onset times of notes and to recognize the corresponding\ninstrument sources. The aim of this study is to explore the usefulness of\nmultiscale scattering operators for these two tasks on plucked string\ninstrument and piano music. After resuming the theoretical background and\nillustrating the key features of this sound representation method, we evaluate\nits performances comparatively to other classical sound representations. Using\nboth MIDI-driven datasets with real instrument samples and real musical pieces,\nscattering is proved to outperform other sound representations for these AMT\nsubtasks, putting forward its richer sound representation and invariance\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 19:57:30 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Cazau", "D.", ""], ["Revillon", "G.", ""], ["Adam", "O.", ""]]}, {"id": "1703.09813", "submitter": "Jean Feng", "authors": "Jean Feng and Noah Simon", "title": "Gradient-based Regularization Parameter Selection for Problems with\n  Non-smooth Penalty Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional and/or non-parametric regression problems, regularization\n(or penalization) is used to control model complexity and induce desired\nstructure. Each penalty has a weight parameter that indicates how strongly the\nstructure corresponding to that penalty should be enforced. Typically the\nparameters are chosen to minimize the error on a separate validation set using\na simple grid search or a gradient-free optimization method. It is more\nefficient to tune parameters if the gradient can be determined, but this is\noften difficult for problems with non-smooth penalty functions. Here we show\nthat for many penalized regression problems, the validation loss is actually\nsmooth almost-everywhere with respect to the penalty parameters. We can\ntherefore apply a modified gradient descent algorithm to tune parameters.\nThrough simulation studies on example regression problems, we find that\nincreasing the number of penalty parameters and tuning them using our method\ncan decrease the generalization error.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 21:41:13 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Feng", "Jean", ""], ["Simon", "Noah", ""]]}, {"id": "1703.09842", "submitter": "Lillian Ratliff", "authors": "Lillian J. Ratliff and Eric Mazumdar", "title": "Inverse Risk-Sensitive Reinforcement Learning", "comments": "v3 (comments regarding updates): We significantly extended the theory\n  (Theorem 2, 3, 5 and Proposition 3). We also correct some minor typos\n  throughout the document; v2 (comments regarding updates): We corrected some\n  notational typos and made clarifications in the proof. We also added\n  clarifying remarks regarding reference points and acceptance levels which\n  were previously conflated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of inverse reinforcement learning in Markov decision\nprocesses where the agent is risk-sensitive. In particular, we model\nrisk-sensitivity in a reinforcement learning framework by making use of models\nof human decision-making having their origins in behavioral psychology,\nbehavioral economics, and neuroscience. We propose a gradient-based inverse\nreinforcement learning algorithm that minimizes a loss function defined on the\nobserved behavior. We demonstrate the performance of the proposed technique on\ntwo examples, the first of which is the canonical Grid World example and the\nsecond of which is a Markov decision process modeling passengers' decisions\nregarding ride-sharing. In the latter, we use pricing and travel time data from\na ride-sharing company to construct the transition probabilities and rewards of\nthe Markov decision process.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 00:10:17 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 01:14:47 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 20:51:05 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ratliff", "Lillian J.", ""], ["Mazumdar", "Eric", ""]]}, {"id": "1703.09930", "submitter": "JInglai Li", "authors": "Hongqiao Wang, Jinglai Li", "title": "Adaptive Gaussian process approximation for Bayesian inference with\n  expensive likelihood functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference problems with computationally intensive\nlikelihood functions. We propose a Gaussian process (GP) based method to\napproximate the joint distribution of the unknown parameters and the data. In\nparticular, we write the joint density approximately as a product of an\napproximate posterior density and an exponentiated GP surrogate. We then\nprovide an adaptive algorithm to construct such an approximation, where an\nactive learning method is used to choose the design points. With numerical\nexamples, we illustrate that the proposed method has competitive performance\nagainst existing approaches for Bayesian computation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 08:32:21 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 06:09:34 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 02:09:04 GMT"}, {"version": "v4", "created": "Wed, 14 Mar 2018 09:16:53 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wang", "Hongqiao", ""], ["Li", "Jinglai", ""]]}, {"id": "1703.09947", "submitter": "Kai Zheng", "authors": "Jiaqi Zhang, Kai Zheng, Wenlong Mou, Liwei Wang", "title": "Efficient Private ERM for Smooth Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider efficient differentially private empirical risk\nminimization from the viewpoint of optimization algorithms. For strongly convex\nand smooth objectives, we prove that gradient descent with output perturbation\nnot only achieves nearly optimal utility, but also significantly improves the\nrunning time of previous state-of-the-art private optimization algorithms, for\nboth $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP. For non-convex but smooth\nobjectives, we propose an RRPSGD (Random Round Private Stochastic Gradient\nDescent) algorithm, which provably converges to a stationary point with privacy\nguarantee. Besides the expected utility bounds, we also provide guarantees in\nhigh probability form. Experiments demonstrate that our algorithm consistently\noutperforms existing method in both utility and running time.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 09:31:47 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 12:57:48 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhang", "Jiaqi", ""], ["Zheng", "Kai", ""], ["Mou", "Wenlong", ""], ["Wang", "Liwei", ""]]}, {"id": "1703.09956", "submitter": "Indranil Pan", "authors": "Indranil Pan and Dirk Bester", "title": "Marginal likelihood based model comparison in Fuzzy Bayesian Learning", "comments": "6 pages, 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL)\nparadigm where expert opinions can be encoded in the form of fuzzy rule bases\nand the hyper-parameters of the fuzzy sets can be learned from data using a\nBayesian approach. The present paper extends this work for selecting the most\nappropriate rule base among a set of competing alternatives, which best\nexplains the data, by calculating the model evidence or marginal likelihood. We\nexplain why this is an attractive alternative over simply minimizing a mean\nsquared error metric of prediction and show the validity of the proposition\nusing synthetic examples and a real world case study in the financial services\nsector.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 10:17:57 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Pan", "Indranil", ""], ["Bester", "Dirk", ""]]}, {"id": "1703.09975", "submitter": "David Hofmeyr", "authors": "David Hofmeyr", "title": "Improving Spectral Clustering using the Asymptotic Value of the\n  Normalised Cut", "comments": "An updated version has been accepted to Journal of Computational and\n  Graphical Statistics", "journal-ref": null, "doi": "10.1080/10618600.2019.1593180", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a popular and versatile clustering method based on a\nrelaxation of the normalised graph cut objective. Despite its popularity,\nhowever, there is no single agreed upon method for tuning the important scaling\nparameter, nor for determining automatically the number of clusters to extract.\nPopular heuristics exist, but corresponding theoretical results are scarce. In\nthis paper we investigate the asymptotic value of the normalised cut for an\nincreasing sample assumed to arise from an underlying probability distribution,\nand based on this result provide recommendations for improving spectral\nclustering methodology. A corresponding algorithm is proposed with strong\nempirical performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 11:28:55 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 12:30:50 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hofmeyr", "David", ""]]}, {"id": "1703.10010", "submitter": "Christopher Dance", "authors": "Christopher R. Dance and Tomi Silander", "title": "Optimal Policies for Observing Time Series and Related Restless Bandit\n  Problems", "comments": "In submission to JMLR. Authors' website: http://www.xrce.xerox.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trade-off between the cost of acquiring and processing data, and\nuncertainty due to a lack of data is fundamental in machine learning. A basic\ninstance of this trade-off is the problem of deciding when to make noisy and\ncostly observations of a discrete-time Gaussian random walk, so as to minimise\nthe posterior variance plus observation costs. We present the first proof that\na simple policy, which observes when the posterior variance exceeds a\nthreshold, is optimal for this problem. The proof generalises to a wide range\nof cost functions other than the posterior variance.\n  This result implies that optimal policies for linear-quadratic-Gaussian\ncontrol with costly observations have a threshold structure. It also implies\nthat the restless bandit problem of observing multiple such time series, has a\nwell-defined Whittle index. We discuss computation of that index, give\nclosed-form formulae for it, and compare the performance of the associated\nindex policy with heuristic policies.\n  The proof is based on a new verification theorem that demonstrates threshold\nstructure for Markov decision processes, and on the relation between binary\nsequences known as mechanical words and the dynamics of discontinuous nonlinear\nmaps, which frequently arise in physics, control and biology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 12:46:59 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Dance", "Christopher R.", ""], ["Silander", "Tomi", ""]]}, {"id": "1703.10034", "submitter": "Maren Mahsereci", "authors": "Maren Mahsereci, Philipp Hennig", "title": "Probabilistic Line Searches for Stochastic Optimization", "comments": "Extended version of the NIPS '15 conference paper, includes detailed\n  pseudo-code, 59 pages, 35 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deterministic optimization, line searches are a standard tool ensuring\nstability and efficiency. Where only stochastic gradients are available, no\ndirect equivalent has so far been formulated, because uncertain gradients do\nnot allow for a strict sequence of decisions collapsing the search space. We\nconstruct a probabilistic line search by combining the structure of existing\ndeterministic methods with notions from Bayesian optimization. Our method\nretains a Gaussian process surrogate of the univariate optimization objective,\nand uses a probabilistic belief over the Wolfe conditions to monitor the\ndescent. The algorithm has very low computational cost, and no user-controlled\nparameters. Experiments show that it effectively removes the need to define a\nlearning rate for stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 13:43:52 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 16:18:08 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Mahsereci", "Maren", ""], ["Hennig", "Philipp", ""]]}, {"id": "1703.10121", "submitter": "Patrick O. Glauner", "authors": "Patrick Glauner, Manxing Du, Victor Paraschiv, Andrey Boytsov, Isabel\n  Lopez Andrade, Jorge Meira, Petko Valtchev, Radu State", "title": "The Top 10 Topics in Machine Learning Revisited: A Quantitative\n  Meta-Study", "comments": null, "journal-ref": "Proceedings of the 25th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN 2017)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which topics of machine learning are most commonly addressed in research?\nThis question was initially answered in 2007 by doing a qualitative survey\namong distinguished researchers. In our study, we revisit this question from a\nquantitative perspective. Concretely, we collect 54K abstracts of papers\npublished between 2007 and 2016 in leading machine learning journals and\nconferences. We then use machine learning in order to determine the top 10\ntopics in machine learning. We not only include models, but provide a holistic\nview across optimization, data, features, etc. This quantitative approach\nallows reducing the bias of surveys. It reveals new and up-to-date insights\ninto what the 10 most prolific topics in machine learning research are. This\nallows researchers to identify popular topics as well as new and rising topics\nfor their research.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:29:04 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Glauner", "Patrick", ""], ["Du", "Manxing", ""], ["Paraschiv", "Victor", ""], ["Boytsov", "Andrey", ""], ["Andrade", "Isabel Lopez", ""], ["Meira", "Jorge", ""], ["Valtchev", "Petko", ""], ["State", "Radu", ""]]}, {"id": "1703.10146", "submitter": "Emmanuel Abbe A", "authors": "Emmanuel Abbe", "title": "Community detection and stochastic block models: recent developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a random graph model with planted\nclusters. It is widely employed as a canonical model to study clustering and\ncommunity detection, and provides generally a fertile ground to study the\nstatistical and computational tradeoffs that arise in network and data\nsciences.\n  This note surveys the recent developments that establish the fundamental\nlimits for community detection in the SBM, both with respect to\ninformation-theoretic and computational thresholds, and for various recovery\nrequirements such as exact, partial and weak recovery (a.k.a., detection). The\nmain results discussed are the phase transitions for exact recovery at the\nChernoff-Hellinger threshold, the phase transition for weak recovery at the\nKesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial\nrecovery, the learning of the SBM parameters and the gap between\ninformation-theoretic and computational thresholds.\n  The note also covers some of the algorithms developed in the quest of\nachieving the limits, in particular two-round algorithms via graph-splitting,\nsemi-definite programming, linearized belief propagation, classical and\nnonbacktracking spectral methods. A few open problems are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 17:21:44 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Abbe", "Emmanuel", ""]]}, {"id": "1703.10230", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Numerical Gaussian Processes for Time-dependent and Non-linear Partial\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA math.AP math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of numerical Gaussian processes, which we define as\nGaussian processes with covariance functions resulting from temporal\ndiscretization of time-dependent partial differential equations. Numerical\nGaussian processes, by construction, are designed to deal with cases where: (1)\nall we observe are noisy data on black-box initial conditions, and (2) we are\ninterested in quantifying the uncertainty associated with such noisy data in\nour solutions to time-dependent partial differential equations. Our method\ncircumvents the need for spatial discretization of the differential operators\nby proper placement of Gaussian process priors. This is an attempt to construct\nstructured and data-efficient learning machines, which are explicitly informed\nby the underlying physics that possibly generated the observed data. The\neffectiveness of the proposed approach is demonstrated through several\nbenchmark problems involving linear and nonlinear time-dependent operators. In\nall examples, we are able to recover accurate approximations of the latent\nsolutions, and consistently propagate uncertainty, even in cases involving very\nlong time integration.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 20:17:30 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1703.10342", "submitter": "Katharina Eggensperger", "authors": "Katharina Eggensperger and Marius Lindauer and Holger H. Hoos and\n  Frank Hutter and Kevin Leyton-Brown", "title": "Efficient Benchmarking of Algorithm Configuration Procedures via\n  Model-Based Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of algorithm (hyper-)parameters is crucial for achieving\npeak performance across a wide range of domains, ranging from deep neural\nnetworks to solvers for hard combinatorial problems. The resulting algorithm\nconfiguration (AC) problem has attracted much attention from the machine\nlearning community. However, the proper evaluation of new AC procedures is\nhindered by two key hurdles. First, AC benchmarks are hard to set up. Second\nand even more significantly, they are computationally expensive: a single run\nof an AC procedure involves many costly runs of the target algorithm whose\nperformance is to be optimized in a given AC benchmark scenario. One common\nworkaround is to optimize cheap-to-evaluate artificial benchmark functions\n(e.g., Branin) instead of actual algorithms; however, these have different\nproperties than realistic AC problems. Here, we propose an alternative\nbenchmarking approach that is similarly cheap to evaluate but much closer to\nthe original AC problem: replacing expensive benchmarks by surrogate benchmarks\nconstructed from AC benchmarks. These surrogate benchmarks approximate the\nresponse surface corresponding to true target algorithm performance using a\nregression model, and the original and surrogate benchmark share the same\n(hyper-)parameter space. In our experiments, we construct and evaluate\nsurrogate benchmarks for hyperparameter optimization as well as for AC problems\nthat involve performance optimization of solvers for hard combinatorial\nproblems, drawing training data from the runs of existing AC procedures. We\nshow that our surrogate benchmarks capture overall important characteristics of\nthe AC scenarios, such as high- and low-performing regions, from which they\nwere derived, while being much easier to use and orders of magnitude cheaper to\nevaluate.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 07:50:15 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Eggensperger", "Katharina", ""], ["Lindauer", "Marius", ""], ["Hoos", "Holger H.", ""], ["Hutter", "Frank", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1703.10355", "submitter": "Senjian An Dr.", "authors": "Senjian An, Farid Boussaid, Mohammed Bennamoun, and Jiankun Hu", "title": "From Deep to Shallow: Transformations of Deep Rectifier Networks", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce transformations of deep rectifier networks,\nenabling the conversion of deep rectifier networks into shallow rectifier\nnetworks. We subsequently prove that any rectifier net of any depth can be\nrepresented by a maximum of a number of functions that can be realized by a\nshallow network with a single hidden layer. The transformations of both deep\nrectifier nets and deep residual nets are conducted to demonstrate the\nadvantages of the residual nets over the conventional neural nets and the\nadvantages of the deep neural nets over the shallow neural nets. In summary,\nfor two rectifier nets with different depths but with same total number of\nhidden units, the corresponding single hidden layer representation of the\ndeeper net is much more complex than the corresponding single hidden\nrepresentation of the shallower net. Similarly, for a residual net and a\nconventional rectifier net with the same structure except for the skip\nconnections in the residual net, the corresponding single hidden layer\nrepresentation of the residual net is much more complex than the corresponding\nsingle hidden layer representation of the conventional net.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:37:14 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["An", "Senjian", ""], ["Boussaid", "Farid", ""], ["Bennamoun", "Mohammed", ""], ["Hu", "Jiankun", ""]]}, {"id": "1703.10423", "submitter": "Olli-Pekka Koistinen", "authors": "Olli-Pekka Koistinen, Emile Maras, Aki Vehtari, Hannes J\\'onsson", "title": "Minimum energy path calculations with Gaussian process regression", "comments": null, "journal-ref": "Nanosystems: Physics, Chemisty, Mathematics, 2016, 7 (6), p.\n  925-935", "doi": "10.17586/2220-8054-2016-7-6-925-935", "report-no": null, "categories": "physics.chem-ph physics.atm-clus physics.comp-ph stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calculation of minimum energy paths for transitions such as atomic and/or\nspin re-arrangements is an important task in many contexts and can often be\nused to determine the mechanism and rate of transitions. An important challenge\nis to reduce the computational effort in such calculations, especially when ab\ninitio or electron density functional calculations are used to evaluate the\nenergy since they can require large computational effort. Gaussian process\nregression is used here to reduce significantly the number of energy\nevaluations needed to find minimum energy paths of atomic rearrangements. By\nusing results of previous calculations to construct an approximate energy\nsurface and then converge to the minimum energy path on that surface in each\nGaussian process iteration, the number of energy evaluations is reduced\nsignificantly as compared with regular nudged elastic band calculations. For a\ntest problem involving rearrangements of a heptamer island on a crystal\nsurface, the number of energy evaluations is reduced to less than a fifth. The\nscaling of the computational effort with the number of degrees of freedom as\nwell as various possible further improvements to this approach are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 11:49:27 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Koistinen", "Olli-Pekka", ""], ["Maras", "Emile", ""], ["Vehtari", "Aki", ""], ["J\u00f3nsson", "Hannes", ""]]}, {"id": "1703.10444", "submitter": "Jiashi Feng", "authors": "Jiashi Feng", "title": "On Fundamental Limits of Robust Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of robust PAC learning from distributed and\nstreaming data, which may contain malicious errors and outliers, and analyze\ntheir fundamental complexity questions. In particular, we establish lower\nbounds on the communication complexity for distributed robust learning\nperformed on multiple machines, and on the space complexity for robust learning\nfrom streaming data on a single machine. These results demonstrate that gaining\nrobustness of learning algorithms is usually at the expense of increased\ncomplexities. As far as we know, this work gives the first complexity results\nfor distributed and online robust PAC learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 12:46:46 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Feng", "Jiashi", ""]]}, {"id": "1703.10513", "submitter": "Zhenghan Zhu", "authors": "Zhenghan Zhu and Steven Kay", "title": "On Bayesian Exponentially Embedded Family for Model Order Selection", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2781642", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a Bayesian model order selection rule by using the\nexponentially embedded family method, termed Bayesian EEF. Unlike many other\nBayesian model selection methods, the Bayesian EEF can use vague proper priors\nand improper noninformative priors to be objective in the elicitation of\nparameter priors. Moreover, the penalty term of the rule is shown to be the sum\nof half of the parameter dimension and the estimated mutual information between\nparameter and observed data. This helps to reveal the EEF mechanism in\nselecting model orders and may provide new insights into the open problems of\nchoosing an optimal penalty term for model order selection and choosing a good\nprior from information theoretic viewpoints. The important example of linear\nmodel order selection is given to illustrate the algorithms and arguments.\nLastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule\nderived by frequentist strategies. This shows another interesting relationship\nbetween the frequentist and Bayesian philosophies for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 15:12:02 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 03:55:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zhu", "Zhenghan", ""], ["Kay", "Steven", ""]]}, {"id": "1703.10534", "submitter": "Zhaoqiang Liu", "authors": "Zhaoqiang Liu, Vincent Y. F. Tan", "title": "The Informativeness of $k$-Means for Learning Mixture Models", "comments": "Accepted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of mixture models can be viewed as a clustering problem. Indeed,\ngiven data samples independently generated from a mixture of distributions, we\noften would like to find the correct target clustering of the samples according\nto which component distribution they were generated from. For a clustering\nproblem, practitioners often choose to use the simple k-means algorithm.\nk-means attempts to find an optimal clustering which minimizes the\nsum-of-squared distance between each point and its cluster center. In this\npaper, we provide sufficient conditions for the closeness of any optimal\nclustering and the correct target clustering assuming that the data samples are\ngenerated from a mixture of log-concave distributions. Moreover, we show that\nunder similar or even weaker conditions on the mixture model, any optimal\nclustering for the samples with reduced dimensionality is also close to the\ncorrect target clustering. These results provide intuition for the\ninformativeness of k-means (with and without dimensionality reduction) as an\nalgorithm for learning mixture models. We verify the correctness of our\ntheorems using numerical experiments and demonstrate using datasets with\nreduced dimensionality significant speed ups for the time required to perform\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 15:41:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 06:54:35 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 13:48:52 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liu", "Zhaoqiang", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1703.10545", "submitter": "Srijan Kumar", "authors": "Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos\n  Faloutsos, V.S. Subrahamanian", "title": "FairJudge: Trustworthy User Prediction in Rating Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating platforms enable large-scale collection of user opinion about items\n(products, other users, etc.). However, many untrustworthy users give\nfraudulent ratings for excessive monetary gains. In the paper, we present\nFairJudge, a system to identify such fraudulent users. We propose three\nmetrics: (i) the fairness of a user that quantifies how trustworthy the user is\nin rating the products, (ii) the reliability of a rating that measures how\nreliable the rating is, and (iii) the goodness of a product that measures the\nquality of the product. Intuitively, a user is fair if it provides reliable\nratings that are close to the goodness of the product. We formulate a mutually\nrecursive definition of these metrics, and further address cold start problems\nand incorporate behavioral properties of users and products in the formulation.\nWe propose an iterative algorithm, FairJudge, to predict the values of the\nthree metrics. We prove that FairJudge is guaranteed to converge in a bounded\nnumber of iterations, with linear time complexity. By conducting five different\nexperiments on five rating platforms, we show that FairJudge significantly\noutperforms nine existing algorithms in predicting fair and unfair users. We\nreported the 100 most unfair users in the Flipkart network to their review\nfraud investigators, and 80 users were correctly identified (80% accuracy). The\nFairJudge algorithm is already being deployed at Flipkart.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 16:02:25 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Kumar", "Srijan", ""], ["Hooi", "Bryan", ""], ["Makhija", "Disha", ""], ["Kumar", "Mohit", ""], ["Faloutsos", "Christos", ""], ["Subrahamanian", "V. S.", ""]]}, {"id": "1703.10603", "submitter": "Joseph Gomes", "authors": "Joseph Gomes, Bharath Ramsundar, Evan N. Feinberg, Vijay S. Pande", "title": "Atomic Convolutional Networks for Predicting Protein-Ligand Binding\n  Affinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical scoring functions based on either molecular force fields or\ncheminformatics descriptors are widely used, in conjunction with molecular\ndocking, during the early stages of drug discovery to predict potency and\nbinding affinity of a drug-like molecule to a given target. These models\nrequire expert-level knowledge of physical chemistry and biology to be encoded\nas hand-tuned parameters or features rather than allowing the underlying model\nto select features in a data-driven procedure. Here, we develop a general\n3-dimensional spatial convolution operation for learning atomic-level chemical\ninteractions directly from atomic coordinates and demonstrate its application\nto structure-based bioactivity prediction. The atomic convolutional neural\nnetwork is trained to predict the experimentally determined binding affinity of\na protein-ligand complex by direct calculation of the energy associated with\nthe complex, protein, and ligand given the crystal structure of the binding\npose. Non-covalent interactions present in the complex that are absent in the\nprotein-ligand sub-structures are identified and the model learns the\ninteraction strength associated with these features. We test our model by\npredicting the binding free energy of a subset of protein-ligand complexes\nfound in the PDBBind dataset and compare with state-of-the-art cheminformatics\nand machine learning-based approaches. We find that all methods achieve\nexperimental accuracy and that atomic convolutional networks either outperform\nor perform competitively with the cheminformatics based methods. Unlike all\nprevious protein-ligand prediction systems, atomic convolutional networks are\nend-to-end and fully-differentiable. They represent a new data-driven,\nphysics-based deep learning model paradigm that offers a strong foundation for\nfuture improvements in structure-based bioactivity prediction.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:58:31 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Gomes", "Joseph", ""], ["Ramsundar", "Bharath", ""], ["Feinberg", "Evan N.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1703.10622", "submitter": "Siyuan Ma", "authors": "Siyuan Ma, Mikhail Belkin", "title": "Diving into the shallows: a computational perspective on large-scale\n  shallow learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first identify a basic limitation in gradient descent-based\noptimization methods when used in conjunctions with smooth kernels. An analysis\nbased on the spectral properties of the kernel demonstrates that only a\nvanishingly small portion of the function space is reachable after a polynomial\nnumber of gradient descent iterations. This lack of approximating power\ndrastically limits gradient descent for a fixed computational budget leading to\nserious over-regularization/underfitting. The issue is purely algorithmic,\npersisting even in the limit of infinite data.\n  To address this shortcoming in practice, we introduce EigenPro iteration,\nbased on a preconditioning scheme using a small number of approximately\ncomputed eigenvectors. It can also be viewed as learning a new kernel optimized\nfor gradient descent. It turns out that injecting this small (computationally\ninexpensive and SGD-compatible) amount of approximate second-order information\nleads to major improvements in convergence. For large data, this translates\ninto significant performance boost over the standard kernel methods. In\nparticular, we are able to consistently match or improve the state-of-the-art\nresults recently reported in the literature with a small fraction of their\ncomputational budget.\n  Finally, we feel that these results show a need for a broader computational\nperspective on modern large-scale learning to complement more traditional\nstatistical and convergence analyses. In particular, many phenomena of\nlarge-scale high-dimensional inference are best understood in terms of\noptimization on infinite dimensional Hilbert spaces, where standard algorithms\ncan sometimes have properties at odds with finite-dimensional intuition. A\nsystematic analysis concentrating on the approximation power of such algorithms\nwithin a budget of computation may lead to progress both in theory and\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 18:09:43 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 23:45:25 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ma", "Siyuan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1703.10651", "submitter": "Peter Schulam", "authors": "Peter Schulam and Suchi Saria", "title": "Reliable Decision Support using Counterfactual Models", "comments": "Published in the proceedings of Neural Information Processing Systems\n  (NIPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-makers are faced with the challenge of estimating what is likely to\nhappen when they take an action. For instance, if I choose not to treat this\npatient, are they likely to die? Practitioners commonly use supervised learning\nalgorithms to fit predictive models that help decision-makers reason about\nlikely future outcomes, but we show that this approach is unreliable, and\nsometimes even dangerous. The key issue is that supervised learning algorithms\nare highly sensitive to the policy used to choose actions in the training data,\nwhich causes the model to capture relationships that do not generalize. We\npropose using a different learning objective that predicts counterfactuals\ninstead of predicting outcomes under an existing action policy as in supervised\nlearning. To support decision-making in temporal settings, we introduce the\nCounterfactual Gaussian Process (CGP) to predict the counterfactual future\nprogression of continuous-time trajectories under sequences of future actions.\nWe demonstrate the benefits of the CGP on two important decision-support tasks:\nrisk prediction and \"what if?\" reasoning for individualized treatment planning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 19:51:03 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 18:05:23 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 14:28:13 GMT"}, {"version": "v4", "created": "Thu, 1 Feb 2018 13:40:16 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Schulam", "Peter", ""], ["Saria", "Suchi", ""]]}, {"id": "1703.10663", "submitter": "Vince Grolmusz", "authors": "Balazs Szalkai and Vince Grolmusz", "title": "Near Perfect Protein Multi-Label Classification with Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs) have gained a well-deserved popularity\namong machine learning tools upon their recent successful applications in\nimage- and sound processing and classification problems. ANNs have also been\napplied for predicting the family or function of a protein, knowing its residue\nsequence. Here we present two new ANNs with multi-label classification ability,\nshowing impressive accuracy when classifying protein sequences into 698 UniProt\nfamilies (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%).\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 20:25:01 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Szalkai", "Balazs", ""], ["Grolmusz", "Vince", ""]]}, {"id": "1703.10717", "submitter": "David Berthelot", "authors": "David Berthelot, Thomas Schumm, Luke Metz", "title": "BEGAN: Boundary Equilibrium Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new equilibrium enforcing method paired with a loss derived from\nthe Wasserstein distance for training auto-encoder based Generative Adversarial\nNetworks. This method balances the generator and discriminator during training.\nAdditionally, it provides a new approximate convergence measure, fast and\nstable training and high visual quality. We also derive a way of controlling\nthe trade-off between image diversity and visual quality. We focus on the image\ngeneration task, setting a new milestone in visual quality, even at higher\nresolutions. This is achieved while using a relatively simple model\narchitecture and a standard training procedure.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 00:13:33 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 22:42:24 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 23:54:04 GMT"}, {"version": "v4", "created": "Wed, 31 May 2017 19:05:58 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Berthelot", "David", ""], ["Schumm", "Thomas", ""], ["Metz", "Luke", ""]]}, {"id": "1703.10722", "submitter": "Oleksii Kuchaiev", "authors": "Oleksii Kuchaiev, Boris Ginsburg", "title": "Factorization tricks for LSTM networks", "comments": "accepted to ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 00:50:37 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 17:17:55 GMT"}, {"version": "v3", "created": "Sat, 24 Feb 2018 22:04:52 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kuchaiev", "Oleksii", ""], ["Ginsburg", "Boris", ""]]}, {"id": "1703.10740", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang", "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion", "comments": "arXiv admin note: text overlap with arXiv:1703.07698", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of low canonical polyadic (CP) rank tensor\ncompletion. A completion is a tensor whose entries agree with the observed\nentries and its rank matches the given CP rank. We analyze the manifold\nstructure corresponding to the tensors with the given rank and define a set of\npolynomials based on the sampling pattern and CP decomposition. Then, we show\nthat finite completability of the sampled tensor is equivalent to having a\ncertain number of algebraically independent polynomials among the defined\npolynomials. Our proposed approach results in characterizing the maximum number\nof algebraically independent polynomials in terms of a simple geometric\nstructure of the sampling pattern, and therefore we obtain the deterministic\nnecessary and sufficient condition on the sampling pattern for finite\ncompletability of the sampled tensor. Moreover, assuming that the entries of\nthe tensor are sampled independently with probability $p$ and using the\nmentioned deterministic analysis, we propose a combinatorial method to derive a\nlower bound on the sampling probability $p$, or equivalently, the number of\nsampled entries that guarantees finite completability with high probability. We\nalso show that the existing result for the matrix completion problem can be\nused to obtain a loose lower bound on the sampling probability $p$. In\naddition, we obtain deterministic and probabilistic conditions for unique\ncompletability. It is seen that the number of samples required for finite or\nunique completability obtained by the proposed analysis on the CP manifold is\norders-of-magnitude lower than that is obtained by the existing analysis on the\nGrassmannian manifold.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 03:21:32 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1703.10761", "submitter": "Houman Owhadi", "authors": "Houman Owhadi and Clint Scovel", "title": "Universal Scalable Robust Solvers from Computational Information Games\n  and fast eigenspace adapted Multiresolution Analysis", "comments": "142 pages. 14 Figures. Presented at AFOSR (Aug 2016), DARPA (Sep\n  2016), IPAM (Apr 3, 2017), Hausdorff (April 13, 2017) and ICERM (June 5,\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the discovery of robust scalable numerical solvers for arbitrary\nbounded linear operators can be automated as a Game Theory problem by\nreformulating the process of computing with partial information and limited\nresources as that of playing underlying hierarchies of adversarial information\ngames. When the solution space is a Banach space $B$ endowed with a quadratic\nnorm $\\|\\cdot\\|$, the optimal measure (mixed strategy) for such games (e.g. the\nadversarial recovery of $u\\in B$, given partial measurements $[\\phi_i, u]$ with\n$\\phi_i\\in B^*$, using relative error in $\\|\\cdot\\|$-norm as a loss) is a\ncentered Gaussian field $\\xi$ solely determined by the norm $\\|\\cdot\\|$, whose\nconditioning (on measurements) produces optimal bets. When measurements are\nhierarchical, the process of conditioning this Gaussian field produces a\nhierarchy of elementary bets (gamblets). These gamblets generalize the notion\nof Wavelets and Wannier functions in the sense that they are adapted to the\nnorm $\\|\\cdot\\|$ and induce a multi-resolution decomposition of $B$ that is\nadapted to the eigensubspaces of the operator defining the norm $\\|\\cdot\\|$.\nWhen the operator is localized, we show that the resulting gamblets are\nlocalized both in space and frequency and introduce the Fast Gamblet Transform\n(FGT) with rigorous accuracy and (near-linear) complexity estimates. As the FFT\ncan be used to solve and diagonalize arbitrary PDEs with constant coefficients,\nthe FGT can be used to decompose a wide range of continuous linear operators\n(including arbitrary continuous linear bijections from $H^s_0$ to $H^{-s}$ or\nto $L^2$) into a sequence of independent linear systems with uniformly bounded\ncondition numbers and leads to $\\mathcal{O}(N \\operatorname{polylog} N)$\nsolvers and eigenspace adapted Multiresolution Analysis (resulting in near\nlinear complexity approximation of all eigensubspaces).\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 05:26:05 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 20:21:26 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Owhadi", "Houman", ""], ["Scovel", "Clint", ""]]}, {"id": "1703.10827", "submitter": "Amal Rannen Triki", "authors": "Amal Rannen Triki, Matthew B. Blaschko, Yoon Mo Jung, Seungri Song,\n  Hyun Ju Han, Seung Il Kim and Chulmin Joo", "title": "Intraoperative margin assessment of human breast tissue in optical\n  coherence tomography images using deep neural networks", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: In this work, we perform margin assessment of human breast tissue\nfrom optical coherence tomography (OCT) images using deep neural networks\n(DNNs). This work simulates an intraoperative setting for breast cancer\nlumpectomy. Methods: To train the DNNs, we use both the state-of-the-art\nmethods (Weight Decay and DropOut) and a newly introduced regularization method\nbased on function norms. Commonly used methods can fail when only a small\ndatabase is available. The use of a function norm introduces a direct control\nover the complexity of the function with the aim of diminishing the risk of\noverfitting. Results: As neither the code nor the data of previous results are\npublicly available, the obtained results are compared with reported results in\nthe literature for a conservative comparison. Moreover, our method is applied\nto locally collected data on several data configurations. The reported results\nare the average over the different trials. Conclusion: The experimental results\nshow that the use of DNNs yields significantly better results than other\ntechniques when evaluated in terms of sensitivity, specificity, F1 score,\nG-mean and Matthews correlation coefficient. Function norm regularization\nyielded higher and more robust results than competing methods. Significance: We\nhave demonstrated a system that shows high promise for (partially) automated\nmargin assessment of human breast tissue, Equal error rate (EER) is reduced\nfrom approximately 12\\% (the lowest reported in the literature) to 5\\%\\,--\\,a\n58\\% reduction. The method is computationally feasible for intraoperative\napplication (less than 2 seconds per image).\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 10:05:00 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Triki", "Amal Rannen", ""], ["Blaschko", "Matthew B.", ""], ["Jung", "Yoon Mo", ""], ["Song", "Seungri", ""], ["Han", "Hyun Ju", ""], ["Kim", "Seung Il", ""], ["Joo", "Chulmin", ""]]}, {"id": "1703.10887", "submitter": "Dorian Cazau", "authors": "Cazau Dorian, Riwal Lefort, Julien Bonnel, Jean-Luc Zarader and\n  Olivier Adam", "title": "Bi-class classification of humpback whale sound units against complex\n  background noise with Deep Convolution Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1702.02741 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting sound units of humpback whales in complex\ntime-varying background noises is a current challenge for scientists. In this\npaper, we explore the applicability of Convolution Neural Network (CNN) method\nfor this task. In the evaluation stage, we present 6 bi-class classification\nexperimentations of whale sound detection against different background noise\ntypes (e.g., rain, wind). In comparison to classical FFT-based representation\nlike spectrograms, we showed that the use of image-based pretrained CNN\nfeatures brought higher performance to classify whale sounds and background\nnoise.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 13:11:06 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Dorian", "Cazau", ""], ["Lefort", "Riwal", ""], ["Bonnel", "Julien", ""], ["Zarader", "Jean-Luc", ""], ["Adam", "Olivier", ""]]}, {"id": "1703.10893", "submitter": "Jen-Cheng Hou", "authors": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang,\n  Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional\n  Neural Networks", "comments": "To appear in IEEE Transactions on Emerging Topics in Computational\n  Intelligence. Some audio samples can be reached in this link:\n  https://sites.google.com/view/avse2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus only on addressing audio information. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. We also propose a multi-task learning\nframework for reconstructing audio and visual signals at the output layer.\nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual\nencoder-decoder network, in which audio and visual data are first processed\nusing individual CNNs, and then fused into a joint network to generate enhanced\nspeech (the primary task) and reconstructed images (the secondary task) at the\noutput layer. The model is trained in an end-to-end manner, and parameters are\njointly learned through back-propagation. We evaluate enhanced speech using\nfive instrumental criteria. Results show that the AVDCNN model yields a notably\nsuperior performance compared with an audio-only CNN-based SE model and two\nconventional SE approaches, confirming the effectiveness of integrating visual\ninformation into the SE process. In addition, the AVDCNN model also outperforms\nan existing audio-visual SE model, confirming its capability of effectively\ncombining audio and visual information in SE.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:59:24 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:42:54 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 23:37:42 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 21:19:19 GMT"}, {"version": "v5", "created": "Mon, 18 Dec 2017 21:58:03 GMT"}, {"version": "v6", "created": "Wed, 24 Jan 2018 17:54:39 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Hou", "Jen-Cheng", ""], ["Wang", "Syu-Siang", ""], ["Lai", "Ying-Hui", ""], ["Tsao", "Yu", ""], ["Chang", "Hsiu-Wen", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1703.10935", "submitter": "Alberto Abadie", "authors": "Alberto Abadie and Maximilian Kasy", "title": "The Risk of Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applied settings in empirical economics involve simultaneous estimation\nof a large number of parameters. In particular, applied economists are often\ninterested in estimating the effects of many-valued treatments (like teacher\neffects or location effects), treatment effects for many groups, and prediction\nmodels with many regressors. In these settings, machine learning methods that\ncombine regularized estimation and data-driven choices of regularization\nparameters are useful to avoid over-fitting. In this article, we analyze the\nperformance of a class of machine learning estimators that includes ridge,\nlasso and pretest in contexts that require simultaneous estimation of many\nparameters. Our analysis aims to provide guidance to applied researchers on (i)\nthe choice between regularized estimators in practice and (ii) data-driven\nselection of regularization parameters. To address (i), we characterize the\nrisk (mean squared error) of regularized estimators and derive their relative\nperformance as a function of simple features of the data generating process. To\naddress (ii), we show that data-driven choices of regularization parameters,\nbased on Stein's unbiased risk estimate or on cross-validation, yield\nestimators with risk uniformly close to the risk attained under the optimal\n(unfeasible) choice of regularization parameters. We use data from recent\nexamples in the empirical economics literature to illustrate the practical\napplicability of our results.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:13:33 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Abadie", "Alberto", ""], ["Kasy", "Maximilian", ""]]}, {"id": "1703.10936", "submitter": "Evan Ray", "authors": "Evan L. Ray and Nicholas G. Reich", "title": "Prediction of infectious disease epidemics via weighted density\n  ensembles", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005910", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable predictions of infectious disease dynamics can be\nvaluable to public health organizations that plan interventions to decrease or\nprevent disease transmission. A great variety of models have been developed for\nthis task, using different model structures, covariates, and targets for\nprediction. Experience has shown that the performance of these models varies;\nsome tend to do better or worse in different seasons or at different points\nwithin a season. Ensemble methods combine multiple models to obtain a single\nprediction that leverages the strengths of each model. We considered a range of\nensemble methods that each form a predictive density for a target of interest\nas a weighted sum of the predictive densities from component models. In the\nsimplest case, equal weight is assigned to each component model; in the most\ncomplex case, the weights vary with the region, prediction target, week of the\nseason when the predictions are made, a measure of component model uncertainty,\nand recent observations of disease incidence. We applied these methods to\npredict measures of influenza season timing and severity in the United States,\nboth at the national and regional levels, using three component models. We\ntrained the models on retrospective predictions from 14 seasons (1997/1998 -\n2010/2011) and evaluated each model's prospective, out-of-sample performance in\nthe five subsequent influenza seasons. In this test phase, the ensemble methods\nshowed overall performance that was similar to the best of the component\nmodels, but offered more consistent performance across seasons than the\ncomponent models. Ensemble methods offer the potential to deliver more reliable\npredictions to public health decision makers.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:13:54 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ray", "Evan L.", ""], ["Reich", "Nicholas G.", ""]]}, {"id": "1703.10951", "submitter": "Kedi Wu", "authors": "Kedi Wu, Guo-Wei Wei", "title": "Comparison of multi-task convolutional neural network (MT-CNN) and a few\n  other methods for toxicity prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxicity analysis and prediction are of paramount importance to human health\nand environmental protection. Existing computational methods are built from a\nwide variety of descriptors and regressors, which makes their performance\nanalysis difficult. For example, deep neural network (DNN), a successful\napproach in many occasions, acts like a black box and offers little conceptual\nelegance or physical understanding. The present work constructs a common set of\nmicroscopic descriptors based on established physical models for charges,\nsurface areas and free energies to assess the performance of multi-task\nconvolutional neural network (MT-CNN) architectures and a few other approaches,\nincluding random forest (RF) and gradient boosting decision tree (GBDT), on an\nequal footing. Comparison is also given to convolutional neural network (CNN)\nand non-convolutional deep neural network (DNN) algorithms. Four benchmark\ntoxicity data sets (i.e., endpoints) are used to evaluate various approaches.\nExtensive numerical studies indicate that the present MT-CNN architecture is\nable to outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:40:24 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Wu", "Kedi", ""], ["Wei", "Guo-Wei", ""]]}, {"id": "1703.10993", "submitter": "Courtney Paquette", "authors": "Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal,\n  Zaid Harchaoui", "title": "Catalyst Acceleration for Gradient-Based Non-Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generic scheme to solve nonconvex optimization problems using\ngradient-based algorithms originally designed for minimizing convex functions.\nEven though these methods may originally require convexity to operate, the\nproposed approach allows one to use them on weakly convex objectives, which\ncovers a large class of non-convex functions typically appearing in machine\nlearning and signal processing. In general, the scheme is guaranteed to produce\na stationary point with a worst-case efficiency typical of first-order methods,\nand when the objective turns out to be convex, it automatically accelerates in\nthe sense of Nesterov and achieves near-optimal convergence rate in function\nvalues. These properties are achieved without assuming any knowledge about the\nconvexity of the objective, by automatically adapting to the unknown weak\nconvexity constant. We conclude the paper by showing promising experimental\nresults obtained by applying our approach to incremental algorithms such as\nSVRG and SAGA for sparse matrix factorization and for learning neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 17:27:12 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 19:12:32 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 19:59:54 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Paquette", "Courtney", ""], ["Lin", "Hongzhou", ""], ["Drusvyatskiy", "Dmitriy", ""], ["Mairal", "Julien", ""], ["Harchaoui", "Zaid", ""]]}]