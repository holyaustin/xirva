[{"id": "1405.0042", "submitter": "Silvia Villa", "authors": "Lorenzo Rosasco, Silvia Villa", "title": "Learning with incremental iterative regularization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a statistical learning setting, we propose and study an iterative\nregularization algorithm for least squares defined by an incremental gradient\nmethod. In particular, we show that, if all other parameters are fixed a\npriori, the number of passes over the data (epochs) acts as a regularization\nparameter, and prove strong universal consistency, i.e. almost sure convergence\nof the risk, as well as sharp finite sample bounds for the iterates. Our\nresults are a step towards understanding the effect of multiple epochs in\nstochastic gradient techniques in machine learning and rely on integrating\nstatistical and optimization results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 21:48:34 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 13:12:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""]]}, {"id": "1405.0099", "submitter": "Max Sklar", "authors": "Max Sklar", "title": "Fast MLE Computation for the Dirichlet Multinomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of categorical data, we want to find the parameters of a\nDirichlet distribution which maximizes the likelihood of that data. Newton's\nmethod is typically used for this purpose but current implementations require\nreading through the entire dataset on each iteration. In this paper, we propose\na modification which requires only a single pass through the dataset and\nsubstantially decreases running time. Furthermore we analyze both theoretically\nand empirically the performance of the proposed algorithm, and provide an open\nsource implementation.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 05:27:51 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Sklar", "Max", ""]]}, {"id": "1405.0110", "submitter": "Tom LaGatta", "authors": "Tom LaGatta and P. Richard Hahn", "title": "A Structural Approach to Coordinate-Free Statistics", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of learning in general topological vector spaces. By\nexploiting known (or parametrized) covariance structures, our Main Theorem\ndemonstrates that any continuous linear map corresponds to a certain\nisomorphism of embedded Hilbert spaces. By inverting this isomorphism and\nextending continuously, we construct a version of the Ordinary Least Squares\nestimator in absolute generality. Our Gauss-Markov theorem demonstrates that\nOLS is a \"best linear unbiased estimator\", extending the classical result. We\nconstruct a stochastic version of the OLS estimator, which is a continuous\ndisintegration exactly for the class of \"uncorrelated implies independent\"\n(UII) measures. As a consequence, Gaussian measures always exhibit continuous\ndisintegrations through continuous linear maps, extending a theorem of the\nfirst author. Applying this framework to some problems in machine learning, we\nprove a useful representation theorem for covariance tensors, and show that OLS\ndefines a good kriging predictor for vector-valued arrays on general index\nspaces. We also construct a support-vector machine classifier in this setting.\nWe hope that our article shines light on some deeper connections between\nprobability theory, statistics and machine learning, and may serve as a point\nof intersection for these three communities.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 07:40:48 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 05:39:00 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["LaGatta", "Tom", ""], ["Hahn", "P. Richard", ""]]}, {"id": "1405.0133", "submitter": "Binbin Lin", "authors": "Binbin Lin, Ji Yang, Xiaofei He and Jieping Ye", "title": "Geodesic Distance Function Learning via Heat Flow on Vector Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a distance function or metric on a given data manifold is of great\nimportance in machine learning and pattern recognition. Many of the previous\nworks first embed the manifold to Euclidean space and then learn the distance\nfunction. However, such a scheme might not faithfully preserve the distance\nfunction if the original manifold is not Euclidean. Note that the distance\nfunction on a manifold can always be well-defined. In this paper, we propose to\nlearn the distance function directly on the manifold without embedding. We\nfirst provide a theoretical characterization of the distance function by its\ngradient field. Based on our theoretical analysis, we propose to first learn\nthe gradient field of the distance function and then learn the distance\nfunction itself. Specifically, we set the gradient field of a local distance\nfunction as an initial vector field. Then we transport it to the whole manifold\nvia heat flow on vector fields. Finally, the geodesic distance function can be\nobtained by requiring its gradient field to be close to the normalized vector\nfield. Experimental results on both synthetic and real data demonstrate the\neffectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 11:10:36 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 05:07:21 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Lin", "Binbin", ""], ["Yang", "Ji", ""], ["He", "Xiaofei", ""], ["Ye", "Jieping", ""]]}, {"id": "1405.0352", "submitter": "Stefan Wager", "authors": "Stefan Wager", "title": "Asymptotic Theory for Random Forests", "comments": "This manuscript is superseded by \"Estimation and Inference of\n  Heterogeneous Treatment Effects using Random Forests\" by Wager and Athey\n  (arXiv:1510.04342). The new paper extends the asymptotic theory developed\n  here, and applies it to causal inference in the potential outcomes framework\n  with unconfoundedness. The present version is maintained online for archival\n  purposes only", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests have proven to be reliable predictive algorithms in many\napplication areas. Not much is known, however, about the statistical properties\nof random forests. Several authors have established conditions under which\ntheir predictions are consistent, but these results do not provide practical\nestimates of random forest errors. In this paper, we analyze a random forest\nmodel based on subsampling, and show that random forest predictions are\nasymptotically normal provided that the subsample size s scales as s(n)/n =\no(log(n)^{-d}), where n is the number of training examples and d is the number\nof features. Moreover, we show that the asymptotic variance can consistently be\nestimated using an infinitesimal jackknife for bagged ensembles recently\nproposed by Efron (2014). In other words, our results let us both characterize\nand estimate the error-distribution of random forest predictions, thus taking a\nstep towards making random forests tools for statistical inference instead of\njust black-box predictive algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 07:16:55 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 00:28:36 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Wager", "Stefan", ""]]}, {"id": "1405.0530", "submitter": "Yuting Chen", "authors": "Jing Qian, Jonathan Root, Venkatesh Saligrama, Yuting Chen", "title": "A Rank-SVM Approach to Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel non-parametric adaptive anomaly detection algorithm for\nhigh dimensional data based on rank-SVM. Data points are first ranked based on\nscores derived from nearest neighbor graphs on n-point nominal data. We then\ntrain a rank-SVM using this ranked data. A test-point is declared as an anomaly\nat alpha-false alarm level if the predicted score is in the alpha-percentile.\nThe resulting anomaly detector is shown to be asymptotically optimal and\nadaptive in that for any false alarm rate alpha, its decision region converges\nto the alpha-percentile level set of the unknown underlying density. In\naddition we illustrate through a number of synthetic and real-data experiments\nboth the statistical performance and computational efficiency of our anomaly\ndetector.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 22:36:24 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Qian", "Jing", ""], ["Root", "Jonathan", ""], ["Saligrama", "Venkatesh", ""], ["Chen", "Yuting", ""]]}, {"id": "1405.0558", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Alex Smola, Ryan J. Tibshirani", "title": "The Falling Factorial Basis and Its Statistical Applications", "comments": "Full version for the ICML paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel spline-like basis, which we name the \"falling factorial\nbasis\", bearing many similarities to the classic truncated power basis. The\nadvantage of the falling factorial basis is that it enables rapid, linear-time\ncomputations in basis matrix multiplication and basis matrix inversion. The\nfalling factorial functions are not actually splines, but are close enough to\nsplines that they provably retain some of the favorable properties of the\nlatter functions. We examine their application in two problems: trend filtering\nover arbitrary input points, and a higher-order variant of the two-sample\nKolmogorov-Smirnov test.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 07:35:29 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 21:46:29 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Smola", "Alex", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1405.0586", "submitter": "Ambuj Tewari", "authors": "Ambuj Tewari and Sougata Chaudhuri", "title": "On Lipschitz Continuity and Smoothness of Loss Functions in Learning to\n  Rank", "comments": "This paper has been withdrawn as it was superseded by an ICML 2015\n  paper \"Generalization error bounds for learning to rank: Does the length of\n  document lists matter?\" available as arXiv:1603.01860", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary classification and regression problems, it is well understood that\nLipschitz continuity and smoothness of the loss function play key roles in\ngoverning generalization error bounds for empirical risk minimization\nalgorithms. In this paper, we show how these two properties affect\ngeneralization error bounds in the learning to rank problem. The learning to\nrank problem involves vector valued predictions and therefore the choice of the\nnorm with respect to which Lipschitz continuity and smoothness are defined\nbecomes crucial. Choosing the $\\ell_\\infty$ norm in our definition of Lipschitz\ncontinuity allows us to improve existing bounds. Furthermore, under smoothness\nassumptions, our choice enables us to prove rates that interpolate between\n$1/\\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular\nlearning to rank method, gives state-of-the-art performance guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 13:36:59 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 14:53:40 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 18:06:14 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Tewari", "Ambuj", ""], ["Chaudhuri", "Sougata", ""]]}, {"id": "1405.0591", "submitter": "Ambuj  Tewari", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Perceptron-like Algorithms and Generalization Bounds for Learning to\n  Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is a supervised learning problem where the output space is\nthe space of rankings but the supervision space is the space of relevance\nscores. We make theoretical contributions to the learning to rank problem both\nin the online and batch settings. First, we propose a perceptron-like algorithm\nfor learning a ranking function in an online setting. Our algorithm is an\nextension of the classic perceptron algorithm for the classification problem.\nSecond, in the setting of batch learning, we introduce a sufficient condition\nfor convex ranking surrogates to ensure a generalization bound that is\nindependent of number of objects per query. Our bound holds when linear ranking\nfunctions are used: a common practice in many learning to rank algorithms. En\nroute to developing the online algorithm and generalization bound, we propose a\nnovel family of listwise large margin ranking surrogates. Our novel surrogate\nfamily is obtained by modifying a well-known pairwise large margin ranking\nsurrogate and is distinct from the listwise large margin surrogates developed\nusing the structured prediction framework. Using the proposed family, we\nprovide a guaranteed upper bound on the cumulative NDCG (or MAP) induced loss\nunder the perceptron-like algorithm. We also show that the novel surrogates\nsatisfy the generalization bound condition.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 14:38:47 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1405.0602", "submitter": "Ian Fellows", "authors": "Ian E Fellows", "title": "Why (and When and How) Contrastive Divergence Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Contrastive divergence (CD) is a promising method of inference in high\ndimensional distributions with intractable normalizing constants, however, the\ntheoretical foundations justifying its use are somewhat shaky. This document\nproposes a framework for understanding CD inference, how/when it works, and\nprovides multiple justifications for the CD moment conditions, including\nframing them as a variational approximation. Algorithms for performing\ninference are discussed and are applied to social network data using an\nexponential-family random graph models (ERGM). The framework also provides\nguidance about how to construct MCMC kernels providing good CD inference, which\nturn out to be quite different from those used typically to provide fast global\nmixing.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 15:52:00 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Fellows", "Ian E", ""]]}, {"id": "1405.0616", "submitter": "James Brofos", "authors": "James Brofos, Ajay Kannan, Rui Shu", "title": "Automated Attribution and Intertextual Analysis", "comments": "10 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we employ quantitative methods from the realm of statistics and\nmachine learning to develop novel methodologies for author attribution and\ntextual analysis. In particular, we develop techniques and software suitable\nfor applications to Classical study, and we illustrate the efficacy of our\napproach in several interesting open questions in the field. We apply our\nnumerical analysis techniques to questions of authorship attribution in the\ncase of the Greek tragedian Euripides, to instances of intertextuality and\ninfluence in the poetry of the Roman statesman Seneca the Younger, and to cases\nof \"interpolated\" text with respect to the histories of Livy.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 19:02:44 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Brofos", "James", ""], ["Kannan", "Ajay", ""], ["Shu", "Rui", ""]]}, {"id": "1405.0833", "submitter": "Alexander Zimin", "authors": "Alexander Zimin and Rasmus Ibsen-Jensen and Krishnendu Chatterjee", "title": "Generalized Risk-Aversion in Stochastic Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the regret in stochastic multi-armed\nbandit, when the measure of goodness of an arm is not the mean return, but some\ngeneral function of the mean and the variance.We characterize the conditions\nunder which learning is possible and present examples for which no natural\nalgorithm can achieve sublinear regret.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 09:29:17 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Zimin", "Alexander", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Chatterjee", "Krishnendu", ""]]}, {"id": "1405.0869", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "Robust Subspace Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Rare data in a large-scale database are called outliers that reveal\nsignificant information in the real world. The subspace-based outlier detection\nis regarded as a feasible approach in very high dimensional space. However, the\noutliers found in subspaces are only part of the true outliers in high\ndimensional space, indeed. The outliers hidden in normal-clustered points are\nsometimes neglected in the projected dimensional subspace. In this paper, we\npropose a robust subspace method for detecting such inner outliers in a given\ndataset, which uses two dimensional-projections: detecting outliers in\nsubspaces with local density ratio in the first projected dimensions; finding\noutliers by comparing neighbor's positions in the second projected dimensions.\nEach point's weight is calculated by summing up all related values got in the\ntwo steps projected dimensions, and then the points scoring the largest weight\nvalues are taken as outliers. By taking a series of experiments with the number\nof dimensions from 10 to 10000, the results show that our proposed method\nachieves high precision in the case of extremely high dimensional space, and\nworks well in low dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:01:24 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.1004", "submitter": "Gabriel Peyre", "authors": "Samuel Vaiter (CEREMADE), Gabriel Peyr\\'e (CEREMADE), Jalal M. Fadili\n  (GREYC)", "title": "Model Consistency of Partly Smooth Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies least-square regression penalized with partly smooth\nconvex regularizers. This class of functions is very large and versatile\nallowing to promote solutions conforming to some notion of low-complexity.\nIndeed, they force solutions of variational problems to belong to a\nlow-dimensional manifold (the so-called model) which is stable under small\nperturbations of the function. This property is crucial to make the underlying\nlow-complexity model robust to small noise. We show that a generalized\n\"irrepresentable condition\" implies stable model selection under small noise\nperturbations in the observations and the design matrix, when the\nregularization parameter is tuned proportionally to the noise level. This\ncondition is shown to be almost a necessary condition. We then show that this\ncondition implies model consistency of the regularized estimator. That is, with\na probability tending to one as the number of measurements increases, the\nregularized estimator belongs to the correct low-dimensional model manifold.\nThis work unifies and generalizes several previous ones, where model\nconsistency is known to hold for sparse, group sparse, total variation and\nlow-rank regularizations.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 19:26:51 GMT"}, {"version": "v2", "created": "Sun, 8 Jun 2014 08:13:32 GMT"}, {"version": "v3", "created": "Sun, 29 Jun 2014 19:45:20 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Vaiter", "Samuel", "", "CEREMADE"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"], ["Fadili", "Jalal M.", "", "GREYC"]]}, {"id": "1405.1027", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "K-NS: Section-Based Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1405.0869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Finding rare information hidden in a huge amount of data from the Internet is\na necessary but complex issue. Many researchers have studied this issue and\nhave found effective methods to detect anomaly data in low dimensional space.\nHowever, as the dimension increases, most of these existing methods perform\npoorly in detecting outliers because of \"high dimensional curse\". Even though\nsome approaches aim to solve this problem in high dimensional space, they can\nonly detect some anomaly data appearing in low dimensional space and cannot\ndetect all of anomaly data which appear differently in high dimensional space.\nTo cope with this problem, we propose a new k-nearest section-based method\n(k-NS) in a section-based space. Our proposed approach not only detects\noutliers in low dimensional space with section-density ratio but also detects\noutliers in high dimensional space with the ratio of k-nearest section against\naverage value. After taking a series of experiments with the dimension from 10\nto 10000, the experiment results show that our proposed method achieves 100%\nprecision and 100% recall result in the case of extremely high dimensional\nspace, and better improvement in low dimensional space compared to our\npreviously proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:06:06 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.1119", "submitter": "Yishi Zhang", "authors": "Yishi Zhang, Chao Yang, Anrong Yang, Chan Xiong, Xingchi Zhou, Zigang\n  Zhang", "title": "Feature selection for classification with class-separability strategy\n  and data envelopment analysis", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel feature selection method is presented, which is based\non Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). To\nbetter capture the relationship between features and the class, class labels\nare separated into individual variables and relevance and redundancy are\nexplicitly handled on each class label. Super-efficiency DEA is employed to\nevaluate and rank features via their conditional dependence scores on all class\nlabels, and the feature with maximum super-efficiency score is then added in\nthe conditioning set for conditional dependence estimation in the next\niteration, in such a way as to iteratively select features and get the final\nselected features. Eventually, experiments are conducted to evaluate the\neffectiveness of proposed method comparing with four state-of-the-art methods\nfrom the viewpoint of classification accuracy. Empirical results verify the\nfeasibility and the superiority of proposed feature selection method.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 01:17:26 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 12:00:07 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zhang", "Yishi", ""], ["Yang", "Chao", ""], ["Yang", "Anrong", ""], ["Xiong", "Chan", ""], ["Zhou", "Xingchi", ""], ["Zhang", "Zigang", ""]]}, {"id": "1405.1297", "submitter": "Dong Huang", "authors": "Dong Huang and Jian-Huang Lai and Chang-Dong Wang", "title": "Combining Multiple Clusterings via Crowd Agreement Estimation and\n  Multi-Granularity Link Analysis", "comments": "The MATLAB source code of this work is available at:\n  https://www.researchgate.net/publication/281970316", "journal-ref": "Neurocomputing, 2015, vol.170, pp.240-250", "doi": "10.1016/j.neucom.2014.05.094", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering ensemble technique aims to combine multiple clusterings into a\nprobably better and more robust clustering and has been receiving an increasing\nattention in recent years. There are mainly two aspects of limitations in the\nexisting clustering ensemble approaches. Firstly, many approaches lack the\nability to weight the base clusterings without access to the original data and\ncan be affected significantly by the low-quality, or even ill clusterings.\nSecondly, they generally focus on the instance level or cluster level in the\nensemble system and fail to integrate multi-granularity cues into a unified\nmodel. To address these two limitations, this paper proposes to solve the\nclustering ensemble problem via crowd agreement estimation and\nmulti-granularity link analysis. We present the normalized crowd agreement\nindex (NCAI) to evaluate the quality of base clusterings in an unsupervised\nmanner and thus weight the base clusterings in accordance with their clustering\nvalidity. To explore the relationship between clusters, the source aware\nconnected triple (SACT) similarity is introduced with regard to their common\nneighbors and the source reliability. Based on NCAI and multi-granularity\ninformation collected among base clusterings, clusters, and data instances, we\nfurther propose two novel consensus functions, termed weighted evidence\naccumulation clustering (WEAC) and graph partitioning with multi-granularity\nlink analysis (GP-MGLA) respectively. The experiments are conducted on eight\nreal-world datasets. The experimental results demonstrate the effectiveness and\nrobustness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 15:05:02 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 16:10:19 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Huang", "Dong", ""], ["Lai", "Jian-Huang", ""], ["Wang", "Chang-Dong", ""]]}, {"id": "1405.1380", "submitter": "Yingbo Zhou", "authors": "Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju", "title": "Is Joint Training Better for Deep Auto-Encoders?", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, when generative models of data are developed via deep\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\nmodel, the lower layer of the architecture models the data distribution\nconditional upon the hidden variables, while the higher layers model the hidden\ndistribution prior. But due to the greedy scheme of the layerwise training\ntechnique, the parameters of lower layers are fixed when training higher\nlayers. This makes it extremely challenging for the model to learn the hidden\ndistribution prior, which in turn leads to a suboptimal model for the data\ndistribution. We therefore investigate joint training of deep autoencoders,\nwhere the architecture is viewed as one stack of two or more single-layer\nautoencoders. A single global reconstruction objective is jointly optimized,\nsuch that the objective for the single autoencoders at each layer acts as a\nlocal, layer-level regularizer. We empirically evaluate the performance of this\njoint training scheme and observe that it not only learns a better data model,\nbut also learns better higher layer representations, which highlights its\npotential for unsupervised feature learning. In addition, we find that the\nusage of regularizations in the joint training scheme is crucial in achieving\ngood performance. In the supervised setting, joint training also shows superior\nperformance when training deeper models. The joint training framework can thus\nprovide a platform for investigating more efficient usage of different types of\nregularizers, especially in light of the growing volumes of available unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 17:41:33 GMT"}, {"version": "v2", "created": "Sat, 14 Jun 2014 15:48:53 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 18:02:06 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 23:52:59 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhou", "Yingbo", ""], ["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1405.1429", "submitter": "Justin Cheng", "authors": "Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec", "title": "How Community Feedback Shapes User Behavior", "comments": "ICWSM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media systems rely on user feedback and rating mechanisms for\npersonalization, ranking, and content filtering. However, when users evaluate\ncontent contributed by fellow users (e.g., by liking a post or voting on a\ncomment), these evaluations create complex social feedback effects. This paper\ninvestigates how ratings on a piece of content affect its author's future\nbehavior. By studying four large comment-based news communities, we find that\nnegative feedback leads to significant behavioral changes that are detrimental\nto the community. Not only do authors of negatively-evaluated content\ncontribute more, but also their future posts are of lower quality, and are\nperceived by the community as such. Moreover, these authors are more likely to\nsubsequently evaluate their fellow users negatively, percolating these effects\nthrough the community. In contrast, positive feedback does not carry similar\neffects, and neither encourages rewarded authors to write more, nor improves\nthe quality of their posts. Interestingly, the authors that receive no feedback\nare most likely to leave a community. Furthermore, a structural analysis of the\nvoter network reveals that evaluations polarize the community the most when\npositive and negative votes are equally split.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:00:40 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Cheng", "Justin", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1405.1436", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Russell Greiner, Brendan Frey", "title": "Training Restricted Boltzmann Machine by Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to maximum likelihood learning of discrete graphical models\nand RBM in particular is introduced. Our method, Perturb and Descend (PD) is\ninspired by two ideas (I) perturb and MAP method for sampling (II) learning by\nContrastive Divergence minimization. In contrast to perturb and MAP, PD\nleverages training data to learn the models that do not allow efficient MAP\nestimation. During the learning, to produce a sample from the current model, we\nstart from a training data and descend in the energy landscape of the\n\"perturbed model\", for a fixed number of steps, or until a local optima is\nreached. For RBM, this involves linear calculations and thresholding which can\nbe very fast. Furthermore we show that the amount of perturbation is closely\nrelated to the temperature parameter and it can regularize the model by\nproducing robust features resulting in sparse hidden layer activation.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:02:46 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Greiner", "Russell", ""], ["Frey", "Brendan", ""]]}, {"id": "1405.1444", "submitter": "Robert McGibbon", "authors": "Robert T. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, Gert Kiss,\n  and Vijay S. Pande", "title": "Understanding Protein Dynamics with L1-Regularized Reversible Hidden\n  Markov Models", "comments": null, "journal-ref": "Proceedings of the 31st International Conference on Machine\n  Learning, Beijing, China, 2014", "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning framework for modeling protein dynamics. Our\napproach uses L1-regularized, reversible hidden Markov models to understand\nlarge protein datasets generated via molecular dynamics simulations. Our model\nis motivated by three design principles: (1) the requirement of massive\nscalability; (2) the need to adhere to relevant physical law; and (3) the\nnecessity of providing accessible interpretations, critical for both cellular\nbiology and rational drug design. We present an EM algorithm for learning and\nintroduce a model selection criteria based on the physical notion of\nconvergence in relaxation timescales. We contrast our model with standard\nmethods in biophysics and demonstrate improved robustness. We implement our\nalgorithm on GPUs and apply the method to two large protein simulation datasets\ngenerated respectively on the NCSA Bluewaters supercomputer and the\nFolding@Home distributed computing network. Our analysis identifies the\nconformational dynamics of the ubiquitin protein critical to cellular\nsignaling, and elucidates the stepwise activation mechanism of the c-Src kinase\nprotein.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:16:41 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["McGibbon", "Robert T.", ""], ["Ramsundar", "Bharath", ""], ["Sultan", "Mohammad M.", ""], ["Kiss", "Gert", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1405.1533", "submitter": "Pierre Gaillard", "authors": "Pierre Gaillard (GREGH), Paul Baudin (INRIA Rocquencourt)", "title": "A consistent deterministic regression tree for non-parametric prediction\n  of time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online prediction of bounded stationary ergodic processes. To do so,\nwe consider the setting of prediction of individual sequences and build a\ndeterministic regression tree that performs asymptotically as well as the best\nL-Lipschitz constant predictors. Then, we show why the obtained regret bound\nentails the asymptotical optimality with respect to the class of bounded\nstationary ergodic processes.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 08:33:41 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 20:12:02 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Gaillard", "Pierre", "", "GREGH"], ["Baudin", "Paul", "", "INRIA Rocquencourt"]]}, {"id": "1405.1580", "submitter": "Tim van Erven", "authors": "Tim van Erven", "title": "PAC-Bayes Mini-tutorial: A Continuous Union Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When I first encountered PAC-Bayesian concentration inequalities they seemed\nto me to be rather disconnected from good old-fashioned results like\nHoeffding's and Bernstein's inequalities. But, at least for one flavour of the\nPAC-Bayesian bounds, there is actually a very close relation, and the main\ninnovation is a continuous version of the union bound, along with some\ningenious applications. Here's the gist of what's going on, presented from a\nmachine learning perspective.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 12:02:40 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["van Erven", "Tim", ""]]}, {"id": "1405.1773", "submitter": "Ming Yuan", "authors": "Ming Yuan and Cun-Hui Zhang", "title": "On Tensor Completion via Nuclear Norm Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.NA math.OC math.PR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Many problems can be formulated as recovering a low-rank tensor. Although an\nincreasingly common task, tensor recovery remains a challenging problem because\nof the delicacy associated with the decomposition of higher order tensors. To\novercome these difficulties, existing approaches often proceed by unfolding\ntensors into matrices and then apply techniques for matrix completion. We show\nhere that such matricization fails to exploit the tensor structure and may lead\nto suboptimal procedure. More specifically, we investigate a convex\noptimization approach to tensor completion by directly minimizing a tensor\nnuclear norm and prove that this leads to an improved sample size requirement.\nTo establish our results, we develop a series of algebraic and probabilistic\ntechniques such as characterization of subdifferetial for tensor nuclear norm\nand concentration inequalities for tensor martingales, which may be of\nindependent interests and could be useful in other tensor related problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 22:58:32 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Yuan", "Ming", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1405.2278", "submitter": "Robert Lyon", "authors": "R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers", "title": "Hellinger Distance Trees for Imbalanced Streams", "comments": "6 Pages, 2 figures, to be published in Proceedings 22nd International\n  Conference on Pattern Recognition (ICPR) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers trained on data sets possessing an imbalanced class distribution\nare known to exhibit poor generalisation performance. This is known as the\nimbalanced learning problem. The problem becomes particularly acute when we\nconsider incremental classifiers operating on imbalanced data streams,\nespecially when the learning objective is rare class identification. As\naccuracy may provide a misleading impression of performance on imbalanced data,\nexisting stream classifiers based on accuracy can suffer poor minority class\nperformance on imbalanced streams, with the result being low minority class\nrecall rates. In this paper we address this deficiency by proposing the use of\nthe Hellinger distance measure, as a very fast decision tree split criterion.\nWe demonstrate that by using Hellinger a statistically significant improvement\nin recall rates on imbalanced data streams can be achieved, with an acceptable\nincrease in the false positive rate.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 16:14:47 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Lyon", "R. J.", ""], ["Brooke", "J. M.", ""], ["Knowles", "J. D.", ""], ["Stappers", "B. W.", ""]]}, {"id": "1405.2294", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi", "title": "Nonparametric Detection of Anomalous Data Streams", "comments": "Submitted to IEEE Transactions on Signal Processing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric anomalous hypothesis testing problem is investigated, in\nwhich there are totally n sequences with s anomalous sequences to be detected.\nEach typical sequence contains m independent and identically distributed\n(i.i.d.) samples drawn from a distribution p, whereas each anomalous sequence\ncontains m i.i.d. samples drawn from a distribution q that is distinct from p.\nThe distributions p and q are assumed to be unknown in advance.\nDistribution-free tests are constructed using maximum mean discrepancy as the\nmetric, which is based on mean embeddings of distributions into a reproducing\nkernel Hilbert space. The probability of error is bounded as a function of the\nsample size m, the number s of anomalous sequences and the number n of\nsequences. It is then shown that with s known, the constructed test is\nexponentially consistent if m is greater than a constant factor of log n, for\nany p and q, whereas with s unknown, m should has an order strictly greater\nthan log n. Furthermore, it is shown that no test can be consistent for\narbitrary p and q if m is less than a constant factor of log n, thus the\norder-level optimality of the proposed test is established. Numerical results\nare provided to demonstrate that our tests outperform (or perform as well as)\nthe tests based on other competitive approaches under various cases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 15:52:47 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 02:06:49 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Zou", "Shaofeng", ""], ["Liang", "Yingbin", ""], ["Poor", "H. Vincent", ""], ["Shi", "Xinghua", ""]]}, {"id": "1405.2377", "submitter": "James Brofos", "authors": "James Brofos", "title": "A Hybrid Monte Carlo Architecture for Parameter Optimization", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent research has been conducted in the area of Bayesian learning,\nparticularly with regard to the optimization of hyper-parameters via Gaussian\nprocess regression. The methodologies rely chiefly on the method of maximizing\nthe expected improvement of a score function with respect to adjustments in the\nhyper-parameters. In this work, we present a novel algorithm that exploits\nnotions of confidence intervals and uncertainties to enable the discovery of\nthe best optimal within a targeted region of the parameter space. We\ndemonstrate the efficacy of our algorithm with respect to machine learning\nproblems and show cases where our algorithm is competitive with the method of\nmaximizing expected improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 02:03:22 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Brofos", "James", ""]]}, {"id": "1405.2432", "submitter": "Jia Yuan Yu", "authors": "Long Tran-Thanh and Jia Yuan Yu", "title": "Functional Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functional bandit problem, where the objective is to find an\narm that optimises a known functional of the unknown arm-reward distributions.\nThese problems arise in many settings such as maximum entropy methods in\nnatural language processing, and risk-averse decision-making, but current\nbest-arm identification techniques fail in these domains. We propose a new\napproach, that combines functional estimation and arm elimination, to tackle\nthis problem. This method achieves provably efficient performance guarantees.\nIn addition, we illustrate this method on a number of important functionals in\nrisk management and information theory, and refine our generic theoretical\nresults in those cases.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 13:34:22 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Tran-Thanh", "Long", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1405.2566", "submitter": "Edoardo Airoldi", "authors": "Elham Azizi, James E. Galagan, Edoardo M. Airoldi", "title": "Learning modular structures from network data and node variables", "comments": "22 pages, 6 figures, 3 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard technique for understanding underlying dependency structures among\na set of variables posits a shared conditional probability distribution for the\nvariables measured on individuals within a group. This approach is often\nreferred to as module networks, where individuals are represented by nodes in a\nnetwork, groups are termed modules, and the focus is on estimating the network\nstructure among modules. However, estimation solely from node-specific\nvariables can lead to spurious dependencies, and unverifiable structural\nassumptions are often used for regularization. Here, we propose an extended\nmodel that leverages direct observations about the network in addition to\nnode-specific variables. By integrating complementary data types, we avoid the\nneed for structural assumptions. We illustrate theoretical and practical\nsignificance of the model and develop a reversible-jump MCMC learning procedure\nfor learning modules and model parameters. We demonstrate the method accuracy\nin predicting modular structures from synthetic data and capability to learn\ninfluence structures in twitter data and regulatory modules in the\nMycobacterium tuberculosis gene regulatory network.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 18:48:26 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Azizi", "Elham", ""], ["Galagan", "James E.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1405.2600", "submitter": "Yuyi Wang", "authors": "Yuyi Wang and Jan Ramon and Zheng-Chu Guo", "title": "Learning from networked examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are based on the assumption that training\nexamples are drawn independently. However, this assumption does not hold\nanymore when learning from a networked sample because two or more training\nexamples may share some common objects, and hence share the features of these\nshared objects. We show that the classic approach of ignoring this problem\npotentially can have a harmful effect on the accuracy of statistics, and then\nconsider alternatives. One of these is to only use independent examples,\ndiscarding other information. However, this is clearly suboptimal. We analyze\nsample error bounds in this networked setting, providing significantly improved\nresults. An important component of our approach is formed by efficient sample\nweighting schemes, which leads to novel concentration inequalities.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 23:11:52 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:24:18 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 00:23:06 GMT"}, {"version": "v4", "created": "Sat, 3 Jun 2017 12:03:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Yuyi", ""], ["Ramon", "Jan", ""], ["Guo", "Zheng-Chu", ""]]}, {"id": "1405.2606", "submitter": "Joshua Joseph", "authors": "Joshua Joseph, Javier Velez, Nicholas Roy", "title": "Structural Return Maximization for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from\na designer-provided class of policies given a fixed set of training data.\nChoosing the policy which maximizes an estimate of return often leads to\nover-fitting when only limited data is available, due to the size of the policy\nclass in relation to the amount of data available. In this work, we focus on\nlearning policy classes that are appropriately sized to the amount of data\navailable. We accomplish this by using the principle of Structural Risk\nMinimization, from Statistical Learning Theory, which uses Rademacher\ncomplexity to identify a policy class that maximizes a bound on the return of\nthe best policy in the chosen policy class, given the available data. Unlike\nsimilar batch RL approaches, our bound on return requires only extremely weak\nassumptions on the true system.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 00:26:12 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Joseph", "Joshua", ""], ["Velez", "Javier", ""], ["Roy", "Nicholas", ""]]}, {"id": "1405.2639", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Sharp Finite-Time Iterated-Logarithm Martingale Concentration", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give concentration bounds for martingales that are uniform over finite\ntimes and extend classical Hoeffding and Bernstein inequalities. We also\ndemonstrate our concentration bounds to be optimal with a matching\nanti-concentration inequality, proved using the same method. Together these\nconstitute a finite-time version of the law of the iterated logarithm, and shed\nlight on the relationship between it and the central limit theorem.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 06:32:49 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 21:19:07 GMT"}, {"version": "v3", "created": "Wed, 7 Jan 2015 20:27:27 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 21:15:23 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1405.2664", "submitter": "Ji Zhao", "authors": "Ji Zhao, Deyu Meng", "title": "FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test", "comments": null, "journal-ref": "Neural Computation, 2015 June, Vol. 27, No. 6, Pages 1345-1372", "doi": "10.1162/NECO_a_00732", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum mean discrepancy (MMD) is a recently proposed test statistic for\ntwo-sample test. Its quadratic time complexity, however, greatly hampers its\navailability to large-scale applications. To accelerate the MMD calculation, in\nthis study we propose an efficient method called FastMMD. The core idea of\nFastMMD is to equivalently transform the MMD with shift-invariant kernels into\nthe amplitude expectation of a linear combination of sinusoid components based\non Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking\nadvantage of sampling of Fourier transform, FastMMD decreases the time\ncomplexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$\nare the size and dimension of the sample set, respectively. Here $L$ is the\nnumber of basis functions for approximating kernels which determines the\napproximation accuracy. For kernels that are spherically invariant, the\ncomputation can be further accelerated to $O(L N \\log d)$ by using the Fastfood\ntechnique (Le et al., 2013). The uniform convergence of our method has also\nbeen theoretically proved in both unbiased and biased estimates. We have\nfurther provided a geometric explanation for our method, namely ensemble of\ncircular discrepancy, which facilitates us to understand the insight of MMD,\nand is hopeful to help arouse more extensive metrics for assessing two-sample\ntest. Experimental results substantiate that FastMMD is with similar accuracy\nas exact MMD, while with faster computation speed and lower variance than the\nexisting MMD approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 08:20:21 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 12:06:14 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Zhao", "Ji", ""], ["Meng", "Deyu", ""]]}, {"id": "1405.2690", "submitter": "Prashanth L.A.", "authors": "Prashanth L.A.", "title": "Policy Gradients for CVaR-Constrained MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a risk-constrained version of the stochastic shortest path (SSP)\nproblem, where the risk measure considered is Conditional Value-at-Risk (CVaR).\nWe propose two algorithms that obtain a locally risk-optimal policy by\nemploying four tools: stochastic approximation, mini batches, policy gradients\nand importance sampling. Both the algorithms incorporate a CVaR estimation\nprocedure, along the lines of Bardou et al. [2009], which in turn is based on\nRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio\nprinciple for estimating the gradient of the sum of one cost function\n(objective of the SSP) and the gradient of the CVaR of the sum of another cost\nfunction (in the constraint of SSP). The algorithms differ in the manner in\nwhich they approximate the CVaR estimates/necessary gradients - the first\nalgorithm uses stochastic approximation, while the second employ mini-batches\nin the spirit of Monte Carlo methods. We establish asymptotic convergence of\nboth the algorithms. Further, since estimating CVaR is related to rare-event\nsimulation, we incorporate an importance sampling based variance reduction\nscheme into our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 09:59:59 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["A.", "Prashanth L.", ""]]}, {"id": "1405.2798", "submitter": "Jun Wang", "authors": "Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros\n  Kalousis", "title": "Two-Stage Metric Learning", "comments": "Accepted for publication in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel two-stage metric learning algorithm. We\nfirst map each learning instance to a probability distribution by computing its\nsimilarities to a set of fixed anchor points. Then, we define the distance in\nthe input data space as the Fisher information distance on the associated\nstatistical manifold. This induces in the input data space a new family of\ndistance metric with unique properties. Unlike kernelized metric learning, we\ndo not require the similarity measure to be positive semi-definite. Moreover,\nit can also be interpreted as a local metric learning algorithm with well\ndefined distance approximation. We evaluate its performance on a number of\ndatasets. It outperforms significantly other metric learning methods and SVM.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 15:18:15 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Wang", "Jun", ""], ["Sun", "Ke", ""], ["Sha", "Fei", ""], ["Marchand-Maillet", "Stephane", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1405.2878", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA)", "title": "Approximate Policy Iteration Schemes: A Comparison", "comments": "ICML (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the infinite-horizon discounted optimal control problem\nformalized by Markov Decision Processes. We focus on several approximate\nvariations of the Policy Iteration algorithm: Approximate Policy Iteration,\nConservative Policy Iteration (CPI), a natural adaptation of the Policy Search\nby Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$),\nand the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all\nalgorithms, we describe performance bounds, and make a comparison by paying a\nparticular attention to the concentrability constants involved, the number of\niterations and the memory required. Our analysis highlights the following\npoints: 1) The performance guarantee of CPI can be arbitrarily better than that\nof API/API($\\alpha$), but this comes at the cost of a relative---exponential in\n$\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$\nenjoys the best of both worlds: its performance guarantee is similar to that of\nCPI, but within a number of iterations similar to that of API. 3) Contrary to\nAPI that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$\nis proportional to their number of iterations, which may be problematic when\nthe discount factor $\\gamma$ is close to 1 or the approximation error\n$\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make\nan overall trade-off between memory and performance. Simulations with these\nschemes confirm our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 19:11:03 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1405.2881", "submitter": "Erwan Scornet", "authors": "Erwan Scornet (LSTA), G\\'erard Biau (LSTA, LPMA), Jean-Philippe Vert\n  (CBIO)", "title": "Consistency of random forests", "comments": null, "journal-ref": "Annals of Statistics, Institute of Mathematical Statistics (IMS),\n  2015, 43 (4), pp.1716-1741", "doi": "10.1214/15-AOS1321", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a learning algorithm proposed by Breiman [Mach. Learn. 45\n(2001) 5--32] that combines several randomized decision trees and aggregates\ntheir predictions by averaging. Despite its wide usage and outstanding\npractical performance, little is known about the mathematical properties of the\nprocedure. This disparity between theory and practice originates in the\ndifficulty to simultaneously analyze both the randomization process and the\nhighly data-dependent tree structure. In the present paper, we take a step\nforward in forest exploration by proving a consistency result for Breiman's\n[Mach. Learn. 45 (2001) 5--32] original algorithm in the context of additive\nregression models. Our analysis also sheds an interesting light on how random\nforests can nicely adapt to sparsity. 1. Introduction. Random forests are an\nensemble learning method for classification and regression that constructs a\nnumber of randomized decision trees during the training phase and predicts by\naveraging the results. Since its publication in the seminal paper of Breiman\n(2001), the procedure has become a major data analysis tool, that performs well\nin practice in comparison with many standard methods. What has greatly\ncontributed to the popularity of forests is the fact that they can be applied\nto a wide range of prediction problems and have few parameters to tune. Aside\nfrom being simple to use, the method is generally recognized for its accuracy\nand its ability to deal with small sample sizes, high-dimensional feature\nspaces and complex data structures. The random forest methodology has been\nsuccessfully involved in many practical problems, including air quality\nprediction (winning code of the EMC data science global hackathon in 2012, see\nhttp://www.kaggle.com/c/dsg-hackathon), chemoinformatics [Svetnik et al.\n(2003)], ecology [Prasad, Iverson and Liaw (2006), Cutler et al. (2007)], 3D\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 19:15:32 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 07:51:30 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 15:07:46 GMT"}, {"version": "v4", "created": "Sat, 8 Aug 2015 17:20:57 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Scornet", "Erwan", "", "LSTA"], ["Biau", "G\u00e9rard", "", "LSTA, LPMA"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1405.2936", "submitter": "Manuel Gomez Rodriguez", "authors": "Hadi Daneshmand and Manuel Gomez-Rodriguez and Le Song and Bernhard\n  Schoelkopf", "title": "Estimating Diffusion Network Structures: Recovery Conditions, Sample\n  Complexity & Soft-thresholding Algorithm", "comments": "To appear in the 31st International Conference on Machine Learning\n  (ICML), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information spreads across social and technological networks, but often the\nnetwork structures are hidden from us and we only observe the traces left by\nthe diffusion processes, called cascades. Can we recover the hidden network\nstructures from these observed cascades? What kind of cascades and how many\ncascades do we need? Are there some network structures which are more difficult\nthan others to recover? Can we design efficient inference algorithms with\nprovable guarantees?\n  Despite the increasing availability of cascade data and methods for inferring\nnetworks from these data, a thorough theoretical understanding of the above\nquestions remains largely unexplored in the literature. In this paper, we\ninvestigate the network structure inference problem for a general family of\ncontinuous-time diffusion models using an $l_1$-regularized likelihood\nmaximization framework. We show that, as long as the cascade sampling process\nsatisfies a natural incoherence condition, our framework can recover the\ncorrect network structure with high probability if we observe $O(d^3 \\log N)$\ncascades, where $d$ is the maximum number of parents of a node and $N$ is the\ntotal number of nodes. Moreover, we develop a simple and efficient\nsoft-thresholding inference algorithm, which we use to illustrate the\nconsequences of our theoretical results, and show that our framework\noutperforms other alternatives in practice.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 20:01:04 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Daneshmand", "Hadi", ""], ["Gomez-Rodriguez", "Manuel", ""], ["Song", "Le", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1405.2951", "submitter": "Cengiz Pehlevan", "authors": "Tao Hu, Zaid J. Towfic, Cengiz Pehlevan, Alex Genkin, Dmitri B.\n  Chklovskii", "title": "A Neuron as a Signal Processing Device", "comments": "2013 Asilomar Conference on Signals, Systems and Computers, see\n  http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6810296", "journal-ref": null, "doi": "10.1109/ACSSC.2013.6810296", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuron is a basic physiological and computational unit of the brain. While\nmuch is known about the physiological properties of a neuron, its computational\nrole is poorly understood. Here we propose to view a neuron as a signal\nprocessing device that represents the incoming streaming data matrix as a\nsparse vector of synaptic weights scaled by an outgoing sparse activity vector.\nFormally, a neuron minimizes a cost function comprising a cumulative squared\nrepresentation error and regularization terms. We derive an online algorithm\nthat minimizes such cost function by alternating between the minimization with\nrespect to activity and with respect to synaptic weights. The steps of this\nalgorithm reproduce well-known physiological properties of a neuron, such as\nweighted summation and leaky integration of synaptic inputs, as well as an\nOja-like, but parameter-free, synaptic learning rule. Our theoretical framework\nmakes several predictions, some of which can be verified by the existing data,\nothers require further experiments. Such framework should allow modeling the\nfunction of neuronal circuits without necessarily measuring all the microscopic\nbiophysical parameters, as well as facilitate the design of neuromorphic\nelectronics.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 20:43:33 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Hu", "Tao", ""], ["Towfic", "Zaid J.", ""], ["Pehlevan", "Cengiz", ""], ["Genkin", "Alex", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1405.3034", "submitter": "Onkar Dalal", "authors": "Onkar Dalal and Bala Rajaratnam", "title": "G-AMA: Sparse Gaussian graphical model estimation via alternating\n  minimization", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been recently proposed for estimating sparse Gaussian\ngraphical models using $\\ell_{1}$ regularization on the inverse covariance\nmatrix. Despite recent advances, contemporary applications require methods that\nare even faster in order to handle ill-conditioned high dimensional modern day\ndatasets. In this paper, we propose a new method, G-AMA, to solve the sparse\ninverse covariance estimation problem using Alternating Minimization Algorithm\n(AMA), that effectively works as a proximal gradient algorithm on the dual\nproblem. Our approach has several novel advantages over existing methods.\nFirst, we demonstrate that G-AMA is faster than the previous best algorithms by\nmany orders of magnitude and is thus an ideal approach for modern high\nthroughput applications. Second, global linear convergence of G-AMA is\ndemonstrated rigorously, underscoring its good theoretical properties. Third,\nthe dual algorithm operates on the covariance matrix, and thus easily\nfacilitates incorporating additional constraints on pairwise/marginal\nrelationships between feature pairs based on domain specific knowledge. Over\nand above estimating a sparse inverse covariance matrix, we also illustrate how\nto (1) incorporate constraints on the (bivariate) correlations and, (2)\nincorporate equality (equisparsity) or linear constraints between individual\ninverse covariance elements. Fourth, we also show that G-AMA is better adept at\nhandling extremely ill-conditioned problems, as is often the case with real\ndata. The methodology is demonstrated on both simulated and real datasets to\nillustrate its superior performance over recently proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 04:43:50 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 06:05:06 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Dalal", "Onkar", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1405.3080", "submitter": "Tong Zhang", "authors": "Peilin Zhao, Tong Zhang", "title": "Accelerating Minibatch Stochastic Gradient Descent using Stratified\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a popular optimization method which has\nbeen applied to many important machine learning tasks such as Support Vector\nMachines and Deep Neural Networks. In order to parallelize SGD, minibatch\ntraining is often employed. The standard approach is to uniformly sample a\nminibatch at each step, which often leads to high variance. In this paper we\npropose a stratified sampling strategy, which divides the whole dataset into\nclusters with low within-cluster variance; we then take examples from these\nclusters using a stratified sampling technique. It is shown that the\nconvergence rate can be significantly improved by the algorithm. Encouraging\nexperimental results confirm the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 09:45:49 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Zhao", "Peilin", ""], ["Zhang", "Tong", ""]]}, {"id": "1405.3133", "submitter": "Vincent Lyzinski", "authors": "Vince Lyzinski, Donniell Fishkind, Marcelo Fiori, Joshua T.\n  Vogelstein, Carey E. Priebe, and Guillermo Sapiro", "title": "Graph Matching: Relax at Your Own Risk", "comments": "14 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching---aligning a pair of graphs to minimize their edge\ndisagreements---has received wide-spread attention from both theoretical and\napplied communities over the past several decades, including combinatorics,\ncomputer vision, and connectomics. Its attention can be partially attributed to\nits computational difficulty. Although many heuristics have previously been\nproposed in the literature to approximately solve graph matching, very few have\nany theoretical support for their performance. A common technique is to relax\nthe discrete problem to a continuous problem, therefore enabling practitioners\nto bring gradient-descent-type algorithms to bear. We prove that an indefinite\nrelaxation (when solved exactly) almost always discovers the optimal\npermutation, while a common convex relaxation almost always fails to discover\nthe optimal permutation. These theoretical results suggest that initializing\nthe indefinite algorithm with the convex optimum might yield improved practical\nperformance. Indeed, experimental results illuminate and corroborate these\ntheoretical findings, demonstrating that excellent results are achieved in both\nbenchmark and real data problems by amalgamating the two approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 12:53:59 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 19:31:11 GMT"}, {"version": "v3", "created": "Sat, 10 Jan 2015 04:33:02 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Lyzinski", "Vince", ""], ["Fishkind", "Donniell", ""], ["Fiori", "Marcelo", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1405.3162", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang", "title": "Circulant Binary Embedding", "comments": "ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embedding of high-dimensional data requires long codes to preserve the\ndiscriminative power of the input space. Traditional binary coding methods\noften suffer from very high computation and storage costs in such a scenario.\nTo address this problem, we propose Circulant Binary Embedding (CBE) which\ngenerates binary codes by projecting the data with a circulant matrix. The\ncirculant structure enables the use of Fast Fourier Transformation to speed up\nthe computation. Compared to methods that use unstructured matrices, the\nproposed method improves the time complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d\\log{d})$, and the space complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a novel\ntime-frequency alternating optimization to learn data-dependent circulant\nprojections, which alternatively minimizes the objective in original and\nFourier domains. We show by extensive experiments that the proposed approach\ngives much better performance than the state-of-the-art approaches for fixed\ntime, and provides much faster computation with no performance degradation for\nfixed number of bits.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 14:17:11 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Yu", "Felix X.", ""], ["Kumar", "Sanjiv", ""], ["Gong", "Yunchao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1405.3222", "submitter": "Taylor Arnold", "authors": "Taylor Arnold and Ryan Tibshirani", "title": "Efficient Implementations of the Generalized Lasso Dual Path Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficient implementations of the generalized lasso dual path\nalgorithm of Tibshirani and Taylor (2011). We first describe a generic approach\nthat covers any penalty matrix D and any (full column rank) matrix X of\npredictor variables. We then describe fast implementations for the special\ncases of trend filtering problems, fused lasso problems, and sparse fused lasso\nproblems, both with X=I and a general matrix X. These specialized\nimplementations offer a considerable improvement over the generic\nimplementation, both in terms of numerical stability and efficiency of the\nsolution path computation. These algorithms are all available for use in the\ngenlasso R package, which can be found in the CRAN repository.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:42:45 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:44:50 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Arnold", "Taylor", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1405.3224", "submitter": "Aurelien Garivier", "authors": "Emilie Kaufmann (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (IMT)", "title": "On the Complexity of A/B Testing", "comments": null, "journal-ref": "Conference on Learning Theory, Jun 2014, Barcelona, Spain. JMLR:\n  Workshop and Conference Proceedings, 35, pp.461-481", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing refers to the task of determining the best option among two\nalternatives that yield random outcomes. We provide distribution-dependent\nlower bounds for the performance of A/B testing that improve over the results\ncurrently available both in the fixed-confidence (or delta-PAC) and\nfixed-budget settings. When the distribution of the outcomes are Gaussian, we\nprove that the complexity of the fixed-confidence and fixed-budget settings are\nequivalent, and that uniform sampling of both alternatives is optimal only in\nthe case of equal variances. In the common variance case, we also provide a\nstopping rule that terminates faster than existing fixed-confidence algorithms.\nIn the case of Bernoulli distributions, we show that the complexity of\nfixed-budget setting is smaller than that of fixed-confidence setting and that\nuniform sampling of both alternatives -though not optimal- is advisable in\npractice when combined with an appropriate stopping criterion.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:47:17 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 08:55:57 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Kaufmann", "Emilie", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1405.3263", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis and Rabeeh Karimi Mahabadi and Quoc Tran-Dinh\n  and Volkan Cevher", "title": "Scalable sparse covariance estimation via self-concordance", "comments": "7 pages, 1 figure, Accepted at AAAI-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of convex minimization problems, composed of a\nself-concordant function, such as the $\\log\\det$ metric, a convex data fidelity\nterm $h(\\cdot)$ and, a regularizing -- possibly non-smooth -- function\n$g(\\cdot)$. This type of problems have recently attracted a great deal of\ninterest, mainly due to their omnipresence in top-notch applications. Under\nthis \\emph{locally} Lipschitz continuous gradient setting, we analyze the\nconvergence behavior of proximal Newton schemes with the added twist of a\nprobable presence of inexact evaluations. We prove attractive convergence rate\nguarantees and enhance state-of-the-art optimization schemes to accommodate\nsuch developments. Experimental results on sparse covariance estimation show\nthe merits of our algorithm, both in terms of recovery efficiency and\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 19:21:09 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Mahabadi", "Rabeeh Karimi", ""], ["Tran-Dinh", "Quoc", ""], ["Cevher", "Volkan", ""]]}, {"id": "1405.3295", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter and Christoph Waldhauser", "title": "Effects of Sampling Methods on Prediction Quality. The Case of\n  Classifying Land Cover Using Decision Trees", "comments": null, "journal-ref": "Proceedings of COMPSTAT 2014: 585-592. 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clever sampling methods can be used to improve the handling of big data and\nincrease its usefulness. The subject of this study is remote sensing,\nspecifically airborne laser scanning point clouds representing different\nclasses of ground cover. The aim is to derive a supervised learning model for\nthe classification using CARTs. In order to measure the effect of different\nsampling methods on the classification accuracy, various experiments with\nvarying types of sampling methods, sample sizes, and accuracy metrics have been\ndesigned. Numerical results for a subset of a large surveying project covering\nthe lower Rhine area in Germany are shown. General conclusions regarding\nsampling design are drawn and presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:07:09 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hochreiter", "Ronald", ""], ["Waldhauser", "Christoph", ""]]}, {"id": "1405.3316", "submitter": "Yonatan Gur", "authors": "Omar Besbes, Yonatan Gur, Assaf Zeevi", "title": "Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with\n  Non-stationary Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit (MAB) problem a gambler needs to choose at each round\nof play one of K arms, each characterized by an unknown reward distribution.\nReward realizations are only observed when an arm is selected, and the\ngambler's objective is to maximize his cumulative expected earnings over some\ngiven horizon of play T. To do this, the gambler needs to acquire information\nabout arms (exploration) while simultaneously optimizing immediate rewards\n(exploitation); the price paid due to this trade off is often referred to as\nthe regret, and the main question is how small can this price be as a function\nof the horizon length T. This problem has been studied extensively when the\nreward distributions do not change over time; an assumption that supports a\nsharp characterization of the regret, yet is often violated in practical\nsettings. In this paper, we focus on a MAB formulation which allows for a broad\nrange of temporal uncertainties in the rewards, while still maintaining\nmathematical tractability. We fully characterize the (regret) complexity of\nthis class of MAB problems by establishing a direct link between the extent of\nallowable reward \"variation\" and the minimal achievable regret. Our analysis\ndraws some connections between two rather disparate strands of literature: the\nadversarial and the stochastic MAB frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:15:06 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 16:42:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Besbes", "Omar", ""], ["Gur", "Yonatan", ""], ["Zeevi", "Assaf", ""]]}, {"id": "1405.3319", "submitter": "Longhai Li", "authors": "Longhai Li and Weixin Yao", "title": "Fully Bayesian Logistic Regression with Hyper-Lasso Priors for\n  High-dimensional Feature Selection", "comments": "33 pages. arXiv admin note: substantial text overlap with\n  arXiv:1308.4690", "journal-ref": "Journal of Statistical Computation and Simulation, 2018, 88:14,\n  2827-2851", "doi": "10.1080/00949655.2018.1490418", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional feature selection arises in many areas of modern science.\nFor example, in genomic research we want to find the genes that can be used to\nseparate tissues of different classes (e.g. cancer and normal) from tens of\nthousands of genes that are active (expressed) in certain tissue cells. To this\nend, we wish to fit regression and classification models with a large number of\nfeatures (also called variables, predictors). In the past decade, penalized\nlikelihood methods for fitting regression models based on hyper-LASSO\npenalization have received increasing attention in the literature. However,\nfully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in\nlack of development in the literature. In this paper we introduce an MCMC\n(fully Bayesian) method for learning severely multi-modal posteriors of\nlogistic regression models based on hyper-LASSO priors (non-convex penalties).\nOur MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling\nframework; we call our method Bayesian logistic regression with hyper-LASSO\n(BLRHL) priors. We have used simulation studies and real data analysis to\ndemonstrate the superior performance of hyper-LASSO priors, and to investigate\nthe issues of choosing heaviness and scale of hyper-LASSO priors.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:31:09 GMT"}, {"version": "v2", "created": "Mon, 19 May 2014 19:03:36 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 16:50:44 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 03:10:10 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Li", "Longhai", ""], ["Yao", "Weixin", ""]]}, {"id": "1405.3379", "submitter": "Andreas Christmann", "authors": "Andreas Christmann and Ding-Xuan Zhou", "title": "Learning rates for the risk of kernel based quantile regression\n  estimators in additive models", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models play an important role in semiparametric statistics. This\npaper gives learning rates for regularized kernel based methods for additive\nmodels. These learning rates compare favourably in particular in high\ndimensions to recent results on optimal learning rates for purely nonparametric\nregularized kernel based quantile regression using the Gaussian radial basis\nfunction kernel, provided the assumption of an additive model is valid.\nAdditionally, a concrete example is presented to show that a Gaussian function\ndepending only on one variable lies in a reproducing kernel Hilbert space\ngenerated by an additive Gaussian kernel, but does not belong to the\nreproducing kernel Hilbert space generated by the multivariate Gaussian kernel\nof the same variance.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 06:50:08 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Christmann", "Andreas", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1405.3536", "submitter": "Preux Philippe", "authors": "Olivier Nicol (INRIA Lille - Nord Europe, LIFL), J\\'er\\'emie Mary\n  (INRIA Lille - Nord Europe, LIFL), Philippe Preux (INRIA Lille - Nord Europe,\n  LIFL)", "title": "Improving offline evaluation of contextual bandit algorithms via\n  bootstrapping techniques", "comments": null, "journal-ref": "International Conference on Machine Learning 32 (2014)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many recommendation applications such as news recommendation, the items\nthat can be rec- ommended come and go at a very fast pace. This is a challenge\nfor recommender systems (RS) to face this setting. Online learning algorithms\nseem to be the most straight forward solution. The contextual bandit framework\nwas introduced for that very purpose. In general the evaluation of a RS is a\ncritical issue. Live evaluation is of- ten avoided due to the potential loss of\nrevenue, hence the need for offline evaluation methods. Two options are\navailable. Model based meth- ods are biased by nature and are thus difficult to\ntrust when used alone. Data driven methods are therefore what we consider here.\nEvaluat- ing online learning algorithms with past data is not simple but some\nmethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-\ntory mainly due to their mechanism of data re- jection that only allow the\nexploitation of a small fraction of the data. We precisely address this issue\nin this paper. After highlighting the limita- tions of the previous methods, we\npresent a new method, based on bootstrapping techniques. This new method comes\nwith two important improve- ments: it is much more accurate and it provides a\nmeasure of quality of its estimation. The latter is a highly desirable property\nin order to minimize the risks entailed by putting online a RS for the first\ntime. We provide both theoretical and ex- perimental proofs of its superiority\ncompared to state-of-the-art methods, as well as an analysis of the convergence\nof the measure of quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 15:29:02 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Nicol", "Olivier", "", "INRIA Lille - Nord Europe, LIFL"], ["Mary", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe, LIFL"], ["Preux", "Philippe", "", "INRIA Lille - Nord Europe,\n  LIFL"]]}, {"id": "1405.3559", "submitter": "Andrea Mignatti", "authors": "Giorgio Corani and Andrea Mignatti", "title": "Credal Model Averaging for classification: representing prior ignorance\n  and expert opinions", "comments": "15 pages 6 figures Preprint submitted to the International Journal of\n  Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging (BMA) is the state of the art approach for\novercoming model uncertainty. Yet, especially on small data sets, the results\nyielded by BMA might be sensitive to the prior over the models. Credal Model\nAveraging (CMA) addresses this problem by substituting the single prior over\nthe models by a set of priors (credal set). Such approach solves the problem of\nhow to choose the prior over the models and automates sensitivity analysis. We\ndiscuss various CMA algorithms for building an ensemble of logistic regressors\ncharacterized by different sets of covariates. We show how CMA can be\nappropriately tuned to the case in which one is prior-ignorant and to the case\nin which instead domain knowledge is available. CMA detects prior-dependent\ninstances, namely instances in which a different class is more probable\ndepending on the prior over the models. On such instances CMA suspends the\njudgment, returning multiple classes. We thoroughly compare different BMA and\nCMA variants on a real case study, predicting presence of Alpine marmot burrows\nin an Alpine valley. We find that BMA is almost a random guesser on the\ninstances recognized as prior-dependent by CMA.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 16:06:39 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Corani", "Giorgio", ""], ["Mignatti", "Andrea", ""]]}, {"id": "1405.3726", "submitter": "Xi Qiu", "authors": "Xi Qiu and Christopher Stewart", "title": "Topic words analysis based on LDA model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 02:15:01 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Qiu", "Xi", ""], ["Stewart", "Christopher", ""]]}, {"id": "1405.3738", "submitter": "Nicolas Chapados", "authors": "Nicolas Chapados", "title": "Effective Bayesian Modeling of Groups of Related Count Time Series", "comments": "10 pages, 7 figures. Appears in Proceedings of the 31st International\n  Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series of counts arise in a variety of forecasting applications, for\nwhich traditional models are generally inappropriate. This paper introduces a\nhierarchical Bayesian formulation applicable to count time series that can\neasily account for explanatory variables and share statistical strength across\ngroups of related time series. We derive an efficient approximate inference\ntechnique, and illustrate its performance on a number of datasets from supply\nchain planning.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 04:32:29 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Chapados", "Nicolas", ""]]}, {"id": "1405.3952", "submitter": "Yichao Lu", "authors": "Yichao Lu and Dean P. Foster", "title": "Fast Ridge Regression with Randomized Principal Component Analysis and\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new two stage algorithm LING for large scale regression\nproblems. LING has the same risk as the well known Ridge Regression under the\nfixed design setting and can be computed much faster. Our experiments have\nshown that LING performs well in terms of both prediction accuracy and\ncomputational efficiency compared with other large scale regression algorithms\nlike Gradient Descent, Stochastic Gradient Descent and Principal Component\nRegression on both simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 19:14:44 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Lu", "Yichao", ""], ["Foster", "Dean P.", ""]]}, {"id": "1405.4047", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Methods and Models for Interpretable Linear Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integer programming framework to build accurate and\ninterpretable discrete linear classification models. Unlike existing\napproaches, our framework is designed to provide practitioners with the control\nand flexibility they need to tailor accurate and interpretable models for a\ndomain of choice. To this end, our framework can produce models that are fully\noptimized for accuracy, by minimizing the 0--1 classification loss, and that\naddress multiple aspects of interpretability, by incorporating a range of\ndiscrete constraints and penalty functions. We use our framework to produce\nmodels that are difficult to create with existing methods, such as scoring\nsystems and M-of-N rule tables. In addition, we propose specially designed\noptimization methods to improve the scalability of our framework through\ndecomposition and data reduction. We show that discrete linear classifiers can\nattain the training accuracy of any other linear classifier, and provide an\nOccam's Razor type argument as to why the use of small discrete coefficients\ncan provide better generalization. We demonstrate the performance and\nflexibility of our framework through numerical experiments and a case study in\nwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 01:30:11 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 23:33:31 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1405.4141", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "Alexander G. de. G Matthews and Zoubin Ghahramani", "title": "Classification using log Gaussian Cox processes", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McCullagh and Yang (2006) suggest a family of classification algorithms based\non Cox processes. We further investigate the log Gaussian variant which has a\nnumber of appealing properties. Conditioned on the covariates, the distribution\nover labels is given by a type of conditional Markov random field. In the\nsupervised case, computation of the predictive probability of a single test\npoint scales linearly with the number of training points and the multiclass\ngeneralization is straightforward. We show new links between the supervised\nmethod and classical nonparametric methods. We give a detailed analysis of the\npairwise graph representable Markov random field, which we use to extend the\nmodel to semi-supervised learning problems, and propose an inference method\nbased on graph min-cuts. We give the first experimental analysis on supervised\nand semi-supervised datasets and show good empirical performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 12:10:12 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 13:02:49 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Matthews", "Alexander G. de. G", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1405.4251", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Noah Simon, and Daniela Witten", "title": "Selection Bias Correction and Effect Size Estimation under Dependence", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale studies in which it is of interest to test a very\nlarge number of hypotheses, and then to estimate the effect sizes corresponding\nto the rejected hypotheses. For instance, this setting arises in the analysis\nof gene expression or DNA sequencing data. However, naive estimates of the\neffect sizes suffer from selection bias, i.e., some of the largest naive\nestimates are large due to chance alone. Many authors have proposed methods to\nreduce the effects of selection bias under the assumption that the naive\nestimates of the effect sizes are independent. Unfortunately, when the effect\nsize estimates are dependent, these existing techniques can have very poor\nperformance, and in practice there will often be dependence. We propose an\nestimator that adjusts for selection bias under a recently-proposed frequentist\nframework, without the independence assumption. We study some properties of the\nproposed estimator, and illustrate that it outperforms past proposals in a\nsimulation study and on two gene expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 17:32:34 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 19:31:52 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Tan", "Kean Ming", ""], ["Simon", "Noah", ""], ["Witten", "Daniela", ""]]}, {"id": "1405.4324", "submitter": "Akshay Gadde", "authors": "Akshay Gadde, Aamir Anis and Antonio Ortega", "title": "Active Semi-Supervised Learning Using Sampling Theory for Graph Signals", "comments": "10 pages, 6 figures, To appear in KDD'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of offline, pool-based active semi-supervised\nlearning on graphs. This problem is important when the labeled data is scarce\nand expensive whereas unlabeled data is easily available. The data points are\nrepresented by the vertices of an undirected graph with the similarity between\nthem captured by the edge weights. Given a target number of nodes to label, the\ngoal is to choose those nodes that are most informative and then predict the\nunknown labels. We propose a novel framework for this problem based on our\nrecent results on sampling theory for graph signals. A graph signal is a\nreal-valued function defined on each node of the graph. A notion of frequency\nfor such signals can be defined using the spectrum of the graph Laplacian\nmatrix. The sampling theory for graph signals aims to extend the traditional\nNyquist-Shannon sampling theory by allowing us to identify the class of graph\nsignals that can be reconstructed from their values on a subset of vertices.\nThis approach allows us to define a criterion for active learning based on\nsampling set selection which aims at maximizing the frequency of the signals\nthat can be reconstructed from their samples on the set. Experiments show the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 22:31:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Gadde", "Akshay", ""], ["Anis", "Aamir", ""], ["Ortega", "Antonio", ""]]}, {"id": "1405.4394", "submitter": "Tapio Pahikkala", "authors": "Michiel Stock, Thomas Fober, Eyke H\\\"ullermeier, Serghei Glinca,\n  Gerhard Klebe, Tapio Pahikkala, Antti Airola, Bernard De Baets, Willem\n  Waegeman", "title": "Identification of functionally related enzymes by learning-to-rank\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enzyme sequences and structures are routinely used in the biological sciences\nas queries to search for functionally related enzymes in online databases. To\nthis end, one usually departs from some notion of similarity, comparing two\nenzymes by looking for correspondences in their sequences, structures or\nsurfaces. For a given query, the search operation results in a ranking of the\nenzymes in the database, from very similar to dissimilar enzymes, while\ninformation about the biological function of annotated database enzymes is\nignored.\n  In this work we show that rankings of that kind can be substantially improved\nby applying kernel-based learning algorithms. This approach enables the\ndetection of statistical dependencies between similarities of the active cleft\nand the biological function of annotated enzymes. This is in contrast to\nsearch-based approaches, which do not take annotated training data into\naccount. Similarity measures based on the active cleft are known to outperform\nsequence-based or structure-based measures under certain conditions. We\nconsider the Enzyme Commission (EC) classification hierarchy for obtaining\nannotated enzymes during the training phase. The results of a set of sizeable\nexperiments indicate a consistent and significant improvement for a set of\nsimilarity measures that exploit information about small cavities in the\nsurface of enzymes.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 13:51:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Stock", "Michiel", ""], ["Fober", "Thomas", ""], ["H\u00fcllermeier", "Eyke", ""], ["Glinca", "Serghei", ""], ["Klebe", "Gerhard", ""], ["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1405.4599", "submitter": "Dalei Wu", "authors": "Dalei Wu and Haiqing Wu", "title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for\n  Multivariate Gaussian Mixture Models with an Application to Noisy Speech\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The trimming scheme with a prefixed cutoff portion is known as a method of\nimproving the robustness of statistical models such as multivariate Gaussian\nmixture models (MG- MMs) in small scale tests by alleviating the impacts of\noutliers. However, when this method is applied to real- world data, such as\nnoisy speech processing, it is hard to know the optimal cut-off portion to\nremove the outliers and sometimes removes useful data samples as well. In this\npaper, we propose a new method based on measuring the dispersion degree (DD) of\nthe training data to avoid this problem, so as to realise automatic robust\nestimation for MGMMs. The DD model is studied by using two different measures.\nFor each one, we theoretically prove that the DD of the data samples in a\ncontext of MGMMs approximately obeys a specific (chi or chi-square)\ndistribution. The proposed method is evaluated on a real-world application with\na moderately-sized speaker recognition task. Experiments show that the proposed\nmethod can significantly improve the robustness of the conventional training\nmethod of GMMs for speaker recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 04:36:38 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Wu", "Dalei", ""], ["Wu", "Haiqing", ""]]}, {"id": "1405.4807", "submitter": "Yuxin Chen", "authors": "Qixing Huang, Yuxin Chen, and Leonidas Guibas", "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation", "comments": "accepted to International Conference on Machine Learning (ICML 2014)", "journal-ref": "International Conference on Machine Learning (ICML), vol. 32, pp.\n  64-72, June 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum a posteriori (MAP) inference over discrete Markov random fields is a\nfundamental task spanning a wide spectrum of real-world applications, which is\nknown to be NP-hard for general graphs. In this paper, we propose a novel\nsemidefinite relaxation formulation (referred to as SDR) to estimate the MAP\nassignment. Algorithmically, we develop an accelerated variant of the\nalternating direction method of multipliers (referred to as SDPAD-LR) that can\neffectively exploit the special structure of the new relaxation. Encouragingly,\nthe proposed procedure allows solving SDR for large-scale problems, e.g.,\nproblems on a grid graph comprising hundreds of thousands of variables with\nmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable\nof attaining comparable accuracy while exhibiting remarkably improved\nscalability, in contrast to the commonly held belief that semidefinite\nrelaxation can only been applied on small-scale MRF problems. We have evaluated\nthe performance of SDR on various benchmark datasets including OPENGM2 and PIC\nin terms of both the quality of the solutions and computation time.\nExperimental results demonstrate that for a broad class of problems, SDPAD-LR\noutperforms state-of-the-art algorithms in producing better MAP assignment in\nan efficient manner.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 16:58:24 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Huang", "Qixing", ""], ["Chen", "Yuxin", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1405.4897", "submitter": "Yun Wang", "authors": "Zhen James Xiang, Yun Wang and Peter J. Ramadge", "title": "Screening Tests for Lasso Problems", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2568185", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a survey of dictionary screening for the lasso problem. The\nlasso problem seeks a sparse linear combination of the columns of a dictionary\nto best match a given target vector. This sparse representation has proven\nuseful in a variety of subsequent processing and decision tasks. For a given\ntarget vector, dictionary screening quickly identifies a subset of dictionary\ncolumns that will receive zero weight in a solution of the corresponding lasso\nproblem. These columns can be removed from the dictionary prior to solving the\nlasso problem without impacting the optimality of the solution obtained. This\nhas two potential advantages: it reduces the size of the dictionary, allowing\nthe lasso problem to be solved with less resources, and it may speed up\nobtaining a solution. Using a geometrically intuitive framework, we provide\nbasic insights for understanding useful lasso screening tests and their\nlimitations. We also provide illustrative numerical studies on several\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 21:07:08 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 22:04:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Xiang", "Zhen James", ""], ["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1405.4951", "submitter": "Pili Hu", "authors": "Pili Hu, Sherman S.M. Chow and Wing Cheong Lau", "title": "Secure Friend Discovery via Privacy-Preserving and Decentralized\n  Community Detection", "comments": "ICML 2014 Workshop on Learning, Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of secure friend discovery on a social network has long been\nproposed and studied. The requirement is that a pair of nodes can make\nbefriending decisions with minimum information exposed to the other party. In\nthis paper, we propose to use community detection to tackle the problem of\nsecure friend discovery. We formulate the first privacy-preserving and\ndecentralized community detection problem as a multi-objective optimization. We\ndesign the first protocol to solve this problem, which transforms community\ndetection to a series of Private Set Intersection (PSI) instances using\nTruncated Random Walk (TRW). Preliminary theoretical results show that our\nprotocol can uncover communities with overwhelming probability and preserve\nprivacy. We also discuss future works, potential extensions and variations.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 04:46:37 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Hu", "Pili", ""], ["Chow", "Sherman S. M.", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1405.4969", "submitter": "Raja Giryes", "authors": "Raja Giryes and Michael Elad and Alfred M. Bruckstein", "title": "Sparsity Based Methods for Overparameterized Variational Problems", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two complementary approaches have been extensively used in signal and image\nprocessing leading to novel results, the sparse representation methodology and\nthe variational strategy. Recently, a new sparsity based model has been\nproposed, the cosparse analysis framework, which may potentially help in\nbridging sparse approximation based methods to the traditional total-variation\nminimization. Based on this, we introduce a sparsity based framework for\nsolving overparameterized variational problems. The latter has been used to\nimprove the estimation of optical flow and also for general denoising of\nsignals and images. However, the recovery of the space varying parameters\ninvolved was not adequately addressed by traditional variational methods. We\nfirst demonstrate the efficiency of the new framework for one dimensional\nsignals in recovering a piecewise linear and polynomial function. Then, we\nillustrate how the new technique can be used for denoising and segmentation of\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 06:56:04 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 20:59:26 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 17:43:54 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 14:24:49 GMT"}, {"version": "v5", "created": "Fri, 14 Aug 2015 17:19:06 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Giryes", "Raja", ""], ["Elad", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1405.4980", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck", "title": "Convex Optimization: Algorithms and Complexity", "comments": "A previous version of the manuscript was titled \"Theory of Convex\n  Optimization for Machine Learning\"", "journal-ref": "In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp\n  231-357, 2015", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph presents the main complexity theorems in convex optimization\nand their corresponding algorithms. Starting from the fundamental theory of\nblack-box optimization, the material progresses towards recent advances in\nstructural optimization and stochastic optimization. Our presentation of\nblack-box optimization, strongly influenced by Nesterov's seminal book and\nNemirovski's lecture notes, includes the analysis of cutting plane methods, as\nwell as (accelerated) gradient descent schemes. We also pay special attention\nto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror\ndescent, and dual averaging) and discuss their relevance in machine learning.\nWe provide a gentle introduction to structural optimization with FISTA (to\noptimize a sum of a smooth and a simple non-smooth term), saddle-point mirror\nprox (Nemirovski's alternative to Nesterov's smoothing), and a concise\ndescription of interior point methods. In stochastic optimization we discuss\nstochastic gradient descent, mini-batches, random coordinate descent, and\nsublinear algorithms. We also briefly touch upon convex relaxation of\ncombinatorial problems and the use of randomness to round solutions, as well as\nrandom walks based methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:50:56 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 18:52:04 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1405.5096", "submitter": "Richard Combes", "authors": "Richard Combes and Alexandre Proutiere", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "comments": "ICML 2014 (technical report). arXiv admin note: text overlap with\n  arXiv:1307.7309", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandits where the expected reward is a\nunimodal function over partially ordered arms. This important class of problems\nhas been recently investigated in (Cope 2009, Yu 2011). The set of arms is\neither discrete, in which case arms correspond to the vertices of a finite\ngraph whose structure represents similarity in rewards, or continuous, in which\ncase arms belong to a bounded interval. For discrete unimodal bandits, we\nderive asymptotic lower bounds for the regret achieved under any algorithm, and\npropose OSUB, an algorithm whose regret matches this lower bound. Our algorithm\noptimally exploits the unimodal structure of the problem, and surprisingly, its\nasymptotic regret does not depend on the number of arms. We also provide a\nregret upper bound for OSUB in non-stationary environments where the expected\nrewards smoothly evolve over time. The analytical results are supported by\nnumerical experiments showing that OSUB performs significantly better than the\nstate-of-the-art algorithms. For continuous sets of arms, we provide a brief\ndiscussion. We show that combining an appropriate discretization of the set of\narms with the UCB algorithm yields an order-optimal regret, and in practice,\noutperforms recently proposed algorithms designed to exploit the unimodal\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 14:15:54 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1405.5156", "submitter": "Liping Liu", "authors": "Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich", "title": "Gaussian Approximation of Collective Graphical Models", "comments": "Accepted by ICML 2014. 10 page version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Collective Graphical Model (CGM) models a population of independent and\nidentically distributed individuals when only collective statistics (i.e.,\ncounts of individuals) are observed. Exact inference in CGMs is intractable,\nand previous work has explored Markov Chain Monte Carlo (MCMC) and MAP\napproximations for learning and inference. This paper studies Gaussian\napproximations to the CGM. As the population grows large, we show that the CGM\ndistribution converges to a multivariate Gaussian distribution (GCGM) that\nmaintains the conditional independence properties of the original CGM. If the\nobservations are exact marginals of the CGM or marginals that are corrupted by\nGaussian noise, inference in the GCGM approximation can be computed efficiently\nin closed form. If the observations follow a different noise model (e.g.,\nPoisson), then expectation propagation provides efficient and accurate\napproximate inference. The accuracy and speed of GCGM inference is compared to\nthe MCMC and MAP methods on a simulated bird migration problem. The GCGM\nmatches or exceeds the accuracy of the MAP method while being significantly\nfaster.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:12:56 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Liu", "Li-Ping", ""], ["Sheldon", "Daniel", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1405.5170", "submitter": "Martin Drohmann", "authors": "Martin Drohmann and Kevin Carlberg", "title": "The ROMES method for statistical modeling of reduced-order-model error", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, Vol. 3, No. 1, p.\n  116-145 (2015)", "doi": "10.1137/140969841", "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a technique for statistically modeling errors introduced\nby reduced-order models. The method employs Gaussian-process regression to\nconstruct a mapping from a small number of computationally inexpensive `error\nindicators' to a distribution over the true error. The variance of this\ndistribution can be interpreted as the (epistemic) uncertainty introduced by\nthe reduced-order model. To model normed errors, the method employs existing\nrigorous error bounds and residual norms as indicators; numerical experiments\nshow that the method leads to a near-optimal expected effectivity in contrast\nto typical error bounds. To model errors in general outputs, the method uses\ndual-weighted residuals---which are amenable to uncertainty control---as\nindicators. Experiments illustrate that correcting the reduced-order-model\noutput with this surrogate can improve prediction accuracy by an order of\nmagnitude; this contrasts with existing `multifidelity correction' approaches,\nwhich often fail for reduced-order models and suffer from the curse of\ndimensionality. The proposed error surrogates also lead to a notion of\n`probabilistic rigor', i.e., the surrogate bounds the error with specified\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:52:01 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 19:36:38 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 22:40:58 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Drohmann", "Martin", ""], ["Carlberg", "Kevin", ""]]}, {"id": "1405.5239", "submitter": "Rui Song", "authors": "Ailin Fan, Wenbin Lu and Rui Song", "title": "Sequential Advantage Selection for Optimal Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for optimal treatment regime in a clinical trial or an\nobservational study is getting more attention. Most existing variable selection\ntechniques focused on selecting variables that are important for prediction,\ntherefore some variables that are poor in prediction but are critical for\ndecision-making may be ignored. A qualitative interaction of a variable with\ntreatment arises when treatment effect changes direction as the value of this\nvariable varies. The qualitative interaction indicates the importance of this\nvariable for decision-making. Gunter et al. (2011) proposed S-score which\ncharacterizes the magnitude of qualitative interaction of each variable with\ntreatment individually. In this article, we developed a sequential advantage\nselection method based on the modified S-score. Our method selects\nqualitatively interacted variables sequentially, and hence excludes marginally\nimportant but jointly unimportant variables {or vice versa}. The optimal\ntreatment regime based on variables selected via joint model is more\ncomprehensive and reliable. With the proposed stopping criteria, our method can\nhandle a large amount of covariates even if sample size is small. Simulation\nresults show our method performs well in practical settings. We further applied\nour method to data from a clinical trial for depression.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 20:36:40 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Fan", "Ailin", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "1405.5311", "submitter": "Atanu Ghosh KUMAR", "authors": "Atanu Kumar Ghosh, Arnab Chakraborty", "title": "Compressive Sampling Using EM Algorithm", "comments": "9 pages, 4 figures. This paper has been published as a technical\n  report in Applied Statistics Unit in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": "Technical Report No: ASU/2014/4", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches of sampling signals follow the celebrated theorem of\nNyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and\nTao, is a new paradigm that goes against the conventional methods in data\nacquisition and provides a way of recovering signals using fewer samples than\nthe traditional methods use. Here we suggest an alternative way of\nreconstructing the original signals in compressive sampling using EM algorithm.\nWe first propose a naive approach which has certain computational difficulties\nand subsequently modify it to a new approach which performs better than the\nconventional methods of compressive sampling. The comparison of the different\napproaches and the performance of the new approach has been studied using\nsimulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 06:53:16 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Ghosh", "Atanu Kumar", ""], ["Chakraborty", "Arnab", ""]]}, {"id": "1405.5505", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur\n  Gretton, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Shrinkage Estimators", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel\nmean, is central to kernel methods in that it is used by many classical\nalgorithms such as kernel principal component analysis, and it also forms the\ncore inference step of modern kernel methods that rely on embedding probability\ndistributions in RKHSs. Given a finite sample, an empirical average has been\nused commonly as a standard estimator of the true kernel mean. Despite a\nwidespread use of this estimator, we show that it can be improved thanks to the\nwell-known Stein phenomenon. We propose a new family of estimators called\nkernel mean shrinkage estimators (KMSEs), which benefit from both theoretical\njustifications and good empirical performance. The results demonstrate that the\nproposed estimators outperform the standard one, especially in a \"large d,\nsmall n\" paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 18:17:37 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 13:01:18 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 09:28:14 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Muandet", "Krikamol", ""], ["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1405.5576", "submitter": "Sam Davanloo", "authors": "Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique Del Castillo", "title": "On the Theoretical Guarantees for Parameter Estimation of Gaussian\n  Random Field Models: A Sparse Precision Matrix Approach", "comments": "Two new sections 4.2.1, 5.1 and four new figures 5-8 are added. The\n  title, abstract, and concluding remarks are revised", "journal-ref": "Journal of Machine Learning Research. 21 (2020) 1-41", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum\nlikelihood (ML) estimation requires solving a nonconvex optimization problem.\nThe problem is aggravated for anisotropic GRFs where the number of covariance\nfunction parameters increases with the dimension. Even evaluation of the\nlikelihood function requires $O(n^3)$ floating point operations, where $n$\ndenotes the number of data locations. In this paper, we propose a new two-stage\nprocedure to estimate the parameters of second-order stationary GRFs. First, a\nconvex likelihood problem regularized with a weighted $\\ell_1$-norm, utilizing\nthe available distance information between observation locations, is solved to\nfit a sparse precision (inverse covariance) matrix to the observed data.\nSecond, the parameters of the covariance function are estimated by solving a\nleast squares problem. Theoretical error bounds for the solutions of stage I\nand II problems are provided, and their tightness are investigated.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 23:54:14 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 19:47:11 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 04:09:39 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 03:52:45 GMT"}, {"version": "v5", "created": "Thu, 6 Feb 2020 22:52:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tajbakhsh", "Sam Davanloo", ""], ["Aybat", "Necdet Serhat", ""], ["Del Castillo", "Enrique", ""]]}, {"id": "1405.5869", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 19:42:57 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1405.5873", "submitter": "Anastasios Kyrillidis", "authors": "Michail Vlachos and Nikolaos Freris and Anastasios Kyrillidis", "title": "Compressive Mining: Fast and Optimal Data Mining in the Compressed\n  Domain", "comments": "25 pages, 20 figures, accepted in VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data typically contain repeated and periodic patterns. This\nsuggests that they can be effectively represented and compressed using only a\nfew coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).\nHowever, distance estimation when the data are represented using different sets\nof coefficients is still a largely unexplored area. This work studies the\noptimization problems related to obtaining the \\emph{tightest} lower/upper\nbound on Euclidean distances when each data object is potentially compressed\nusing a different set of orthonormal coefficients. Our technique leads to\ntighter distance estimates, which translates into more accurate search,\nlearning and mining operations \\textit{directly} in the compressed domain.\n  We formulate the problem of estimating lower/upper distance bounds as an\noptimization problem. We establish the properties of optimal solutions, and\nleverage the theoretical analysis to develop a fast algorithm to obtain an\n\\emph{exact} solution to the problem. The suggested solution provides the\ntightest estimation of the $L_2$-norm or the correlation. We show that typical\ndata-analysis operations, such as k-NN search or k-Means clustering, can\noperate more accurately using the proposed compression and distance\nreconstruction technique. We compare it with many other prevalent compression\nand reconstruction techniques, including random projections and PCA-based\ntechniques. We highlight a surprising result, namely that when the data are\nhighly sparse in some basis, our technique may even outperform PCA-based\ncompression.\n  The contributions of this work are generic as our methodology is applicable\nto any sequential or high-dimensional data as well as to any orthogonal data\ntransformation used for the underlying data compression scheme.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 15:01:07 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Vlachos", "Michail", ""], ["Freris", "Nikolaos", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1405.5960", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "LASS: a simple assignment model with Laplacian smoothing", "comments": "20 pages, 4 figures. A shorter version appears in AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning soft assignments of $N$ items to $K$\ncategories given two sources of information: an item-category similarity\nmatrix, which encourages items to be assigned to categories they are similar to\n(and to not be assigned to categories they are dissimilar to), and an item-item\nsimilarity matrix, which encourages similar items to have similar assignments.\nWe propose a simple quadratic programming model that captures this intuition.\nWe give necessary conditions for its solution to be unique, define an\nout-of-sample mapping, and derive a simple, effective training algorithm based\non the alternating direction method of multipliers. The model predicts\nreasonable assignments from even a few similarity values, and can be seen as a\ngeneralization of semisupervised learning. It is particularly useful when items\nnaturally belong to multiple categories, as for example when annotating\ndocuments with keywords or pictures with tags, with partially tagged items, or\nwhen the categories have complex interrelations (e.g. hierarchical) that are\nunknown.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 04:28:29 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1405.6012", "submitter": "Qi Xie", "authors": "Qi Xie, Deyu Meng, Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng\n  and Zongben Xu", "title": "On the Optimal Solution of Weighted Nuclear Norm Minimization", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the nuclear norm minimization (NNM) problem has been\nattracting much attention in computer vision and machine learning. The NNM\nproblem is capitalized on its convexity and it can be solved efficiently. The\nstandard nuclear norm regularizes all singular values equally, which is however\nnot flexible enough to fit real scenarios. Weighted nuclear norm minimization\n(WNNM) is a natural extension and generalization of NNM. By assigning properly\ndifferent weights to different singular values, WNNM can lead to\nstate-of-the-art results in applications such as image denoising. Nevertheless,\nso far the global optimal solution of WNNM problem is not completely solved yet\ndue to its non-convexity in general cases. In this article, we study the\ntheoretical properties of WNNM and prove that WNNM can be equivalently\ntransformed into a quadratic programming problem with linear constraints. This\nimplies that WNNM is equivalent to a convex problem and its global optimum can\nbe readily achieved by off-the-shelf convex optimization solvers. We further\nshow that when the weights are non-descending, the globally optimal solution of\nWNNM can be obtained in closed-form.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 10:15:04 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Xie", "Qi", ""], ["Meng", "Deyu", ""], ["Gu", "Shuhang", ""], ["Zhang", "Lei", ""], ["Zuo", "Wangmeng", ""], ["Feng", "Xiangchu", ""], ["Xu", "Zongben", ""]]}, {"id": "1405.6070", "submitter": "Carey Priebe", "authors": "Shakira Suwan, Dominic S. Lee, Runze Tang, Daniel L. Sussman, Minh\n  Tang, Carey E. Priebe", "title": "Empirical Bayes Estimation for the Stochastic Blockmodel", "comments": "to appear at Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for the stochastic blockmodel is currently of burgeoning interest\nin the statistical community, as well as in various application domains as\ndiverse as social networks, citation networks, brain connectivity networks\n(connectomics), etc. Recent theoretical developments have shown that spectral\nembedding of graphs yields tractable distributional results; in particular, a\nrandom dot product latent position graph formulation of the stochastic\nblockmodel informs a mixture of normal distributions for the adjacency spectral\nembedding. We employ this new theory to provide an empirical Bayes methodology\nfor estimation of block memberships of vertices in a random graph drawn from\nthe stochastic blockmodel, and demonstrate its practical utility. The posterior\ninference is conducted using a Metropolis-within-Gibbs algorithm. The theory\nand methods are illustrated through Monte Carlo simulation studies, both within\nthe stochastic blockmodel and beyond, and experimental results on a Wikipedia\ndata set are presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 14:08:08 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 13:52:04 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 10:06:00 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Suwan", "Shakira", ""], ["Lee", "Dominic S.", ""], ["Tang", "Runze", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1405.6159", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "A Bi-clustering Framework for Consensus Problems", "comments": null, "journal-ref": null, "doi": "10.1137/140967325", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider grouping as a general characterization for problems such as\nclustering, community detection in networks, and multiple parametric model\nestimation. We are interested in merging solutions from different grouping\nalgorithms, distilling all their good qualities into a consensus solution. In\nthis paper, we propose a bi-clustering framework and perspective for reaching\nconsensus in such grouping problems. In particular, this is the first time that\nthe task of finding/fitting multiple parametric models to a dataset is formally\nposed as a consensus problem. We highlight the equivalence of these tasks and\nestablish the connection with the computational Gestalt program, that seeks to\nprovide a psychologically-inspired detection theory for visual events. We also\npresent a simple but powerful bi-clustering algorithm, specially tuned to the\nnature of the problem we address, though general enough to handle many\ndifferent instances inscribed within our characterization. The presentation is\naccompanied with diverse and extensive experimental results in clustering,\ncommunity detection, and multiple parametric model estimation in image\nprocessing applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 21:58:10 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 17:44:55 GMT"}, {"version": "v3", "created": "Wed, 20 Aug 2014 22:12:15 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1405.6210", "submitter": "Jacob Bien", "authors": "Jacob Bien, Florentina Bunea, Luo Xiao", "title": "Convex Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sparse estimator of the covariance matrix for\nhigh-dimensional models in which the variables have a known ordering. Our\nestimator, which is the solution to a convex optimization problem, is\nequivalently expressed as an estimator which tapers the sample covariance\nmatrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result of\nthis adaptivity, the convex banding estimator enjoys theoretical optimality\nproperties not attained by previous banding or tapered estimators. In\nparticular, our convex banding estimator is minimax rate adaptive in Frobenius\nand operator norms, up to log factors, over commonly-studied classes of\ncovariance matrices, and over more general classes. Furthermore, it correctly\nrecovers the bandwidth when the true covariance is exactly banded. Our convex\nformulation admits a simple and efficient algorithm. Empirical studies\ndemonstrate its practical effectiveness and illustrate that our exactly-banded\nestimator works well even when the true covariance matrix is only close to a\nbanded matrix, confirming our theoretical results. Our method compares\nfavorably with all existing methods, in terms of accuracy and speed. We\nillustrate the practical merits of the convex banding estimator by showing that\nit can be used to improve the performance of discriminant analysis for\nclassifying sound recordings.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 20:00:52 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Bien", "Jacob", ""], ["Bunea", "Florentina", ""], ["Xiao", "Luo", ""]]}, {"id": "1405.6231", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui and Hau-tieng Wu", "title": "Connection graph Laplacian methods can be made robust to noise", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.SP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several data analytic techniques based on connection graph\nlaplacian (CGL) ideas have appeared in the literature. At this point, the\nproperties of these methods are starting to be understood in the setting where\nthe data is observed without noise. We study the impact of additive noise on\nthese methods, and show that they are remarkably robust. As a by-product of our\nanalysis, we propose modifications of the standard algorithms that increase\ntheir robustness to noise. We illustrate our results in numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 21:04:30 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1405.6353", "submitter": "Nima Noorshams", "authors": "Nima Noorshams and Aravind Iyengar", "title": "A Novel Stochastic Decoding of LDPC Codes with Quantitative Guarantees", "comments": "This paper has been submitted to IEEE Transactions on Information\n  Theory on May 24th 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-density parity-check codes, a class of capacity-approaching linear codes,\nare particularly recognized for their efficient decoding scheme. The decoding\nscheme, known as the sum-product, is an iterative algorithm consisting of\npassing messages between variable and check nodes of the factor graph. The\nsum-product algorithm is fully parallelizable, owing to the fact that all\nmessages can be update concurrently. However, since it requires extensive\nnumber of highly interconnected wires, the fully-parallel implementation of the\nsum-product on chips is exceedingly challenging. Stochastic decoding\nalgorithms, which exchange binary messages, are of great interest for\nmitigating this challenge and have been the focus of extensive research over\nthe past decade. They significantly reduce the required wiring and\ncomputational complexity of the message-passing algorithm. Even though\nstochastic decoders have been shown extremely effective in practice, the\ntheoretical aspect and understanding of such algorithms remains limited at\nlarge. Our main objective in this paper is to address this issue. We first\npropose a novel algorithm referred to as the Markov based stochastic decoding.\nThen, we provide concrete quantitative guarantees on its performance for\ntree-structured as well as general factor graphs. More specifically, we provide\nupper-bounds on the first and second moments of the error, illustrating that\nthe proposed algorithm is an asymptotically consistent estimate of the\nsum-product algorithm. We also validate our theoretical predictions with\nexperimental results, showing we achieve comparable performance to other\npractical stochastic decoders.\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 00:45:14 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Noorshams", "Nima", ""], ["Iyengar", "Aravind", ""]]}, {"id": "1405.6444", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Weiran Wang and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "The role of dimensionality reduction in linear classification", "comments": "15 pages, 6 figures. A shorter version appears in AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) is often used as a preprocessing step in\nclassification, but usually one first fixes the DR mapping, possibly using\nlabel information, and then learns a classifier (a filter approach). Best\nperformance would be obtained by optimizing the classification error jointly\nover DR mapping and classifier (a wrapper approach), but this is a difficult\nnonconvex problem, particularly with nonlinear DR. Using the method of\nauxiliary coordinates, we give a simple, efficient algorithm to train a\ncombination of nonlinear DR and a classifier, and apply it to a RBF mapping\nwith a linear SVM. This alternates steps where we train the RBF mapping and a\nlinear SVM as usual regression and classification, respectively, with a\nclosed-form step that coordinates both. The resulting nonlinear low-dimensional\nclassifier achieves classification errors competitive with the state-of-the-art\nbut is fast at training and testing, and allows the user to trade off runtime\nfor classification accuracy easily. We then study the role of nonlinear DR in\nlinear classification, and the interplay between the DR mapping, the number of\nlatent dimensions and the number of classes. When trained jointly, the DR\nmapping takes an extreme role in eliminating variation: it tends to collapse\nclasses in latent space, erasing all manifold structure, and lay out class\ncentroids so they are linearly separable with maximum margin.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 01:15:44 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1405.6447", "submitter": "Xiaotong Suo", "authors": "Xiaotong Suo, Robert Tibshirani", "title": "An Ordered Lasso and Sparse Time-Lagged Regression", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1080/00401706.2015.1079245", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression scenarios where it is natural to impose an order\nconstraint on the coefficients. We propose an order-constrained version of\nL1-regularized regression for this problem, and show how to solve it\nefficiently using the well-known Pool Adjacent Violators Algorithm as its\nproximal operator. The main application of this idea is time-lagged regression,\nwhere we predict an outcome at time t from features at the previous K time\npoints. In this setting it is natural to assume that the coefficients decay as\nwe move farther away from t, and hence the order constraint is reasonable.\nPotential applications include financial time series and prediction of dynamic\npatient out- comes based on clinical measurements. We illustrate this idea on\nreal and simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 01:37:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 06:26:45 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Suo", "Xiaotong", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1405.6472", "submitter": "Julien Mairal", "authors": "Yuansi Chen (EECS, INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann)", "title": "Fast and Robust Archetypal Analysis for Representation Learning", "comments": null, "journal-ref": "CVPR 2014 - IEEE Conference on Computer Vision \\& Pattern\n  Recognition (2014)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a pioneer unsupervised learning technique called archetypal\nanalysis, which is related to successful data analysis methods such as sparse\ncoding and non-negative matrix factorization. Since it was proposed, archetypal\nanalysis did not gain a lot of popularity even though it produces more\ninterpretable models than other alternatives. Because no efficient\nimplementation has ever been made publicly available, its application to\nimportant scientific problems may have been severely limited. Our goal is to\nbring back into favour archetypal analysis. We propose a fast optimization\nscheme using an active-set strategy, and provide an efficient open-source\nimplementation interfaced with Matlab, R, and Python. Then, we demonstrate the\nusefulness of archetypal analysis for computer vision tasks, such as codebook\nlearning, signal classification, and large image collection visualization.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 06:25:18 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Chen", "Yuansi", "", "EECS, INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"]]}, {"id": "1405.6642", "submitter": "Guang Cheng", "authors": "Wei Sun (Yahoo Labs), Xingye Qiao (Binghamton) and Guang Cheng\n  (Purdue)", "title": "Stabilized Nearest Neighbor Classifier and Its Statistical Properties", "comments": "48 Pages, 11 Figures. To Appear in JASA--T&M", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stability of statistical analysis is an important indicator for\nreproducibility, which is one main principle of scientific method. It entails\nthat similar statistical conclusions can be reached based on independent\nsamples from the same underlying population. In this paper, we introduce a\ngeneral measure of classification instability (CIS) to quantify the sampling\nvariability of the prediction made by a classification method. Interestingly,\nthe asymptotic CIS of any weighted nearest neighbor classifier turns out to be\nproportional to the Euclidean norm of its weight vector. Based on this concise\nform, we propose a stabilized nearest neighbor (SNN) classifier, which\ndistinguishes itself from other nearest neighbor classifiers, by taking the\nstability into consideration. In theory, we prove that SNN attains the minimax\noptimal convergence rate in risk, and a sharp convergence rate in CIS. The\nlatter rate result is established for general plug-in classifiers under a\nlow-noise condition. Extensive simulated and real examples demonstrate that SNN\nachieves a considerable improvement in CIS over existing nearest neighbor\nclassifiers, with comparable classification accuracy. We implement the\nalgorithm in a publicly available R package snn.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 17:07:10 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 18:56:05 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Sun", "Wei", "", "Yahoo Labs"], ["Qiao", "Xingye", "", "Binghamton"], ["Cheng", "Guang", "", "Purdue"]]}, {"id": "1405.6804", "submitter": "Zhuowen Tu", "authors": "Zhuowen Tu and Piotr Dollar and Yingnian Wu", "title": "Layered Logic Classifiers: Exploring the `And' and `Or' Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective and efficient classifier for pattern analysis is a key\nproblem in machine learning and computer vision. Many the solutions to the\nproblem require to perform logic operations such as `and', `or', and `not'.\nClassification and regression tree (CART) include these operations explicitly.\nOther methods such as neural networks, SVM, and boosting learn/compute a\nweighted sum on features (weak classifiers), which weakly perform the 'and' and\n'or' operations. However, it is hard for these classifiers to deal with the\n'xor' pattern directly. In this paper, we propose layered logic classifiers for\npatterns of complicated distributions by combining the `and', `or', and `not'\noperations. The proposed algorithm is very general and easy to implement. We\ntest the classifiers on several typical datasets from the Irvine repository and\ntwo challenging vision applications, object segmentation and pedestrian\ndetection. We observe significant improvements on all the datasets over the\nwidely used decision stump based AdaBoost algorithm. The resulting classifiers\nhave much less training complexity than decision tree based AdaBoost, and can\nbe applied in a wide range of domains.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:29:01 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 00:51:08 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Tu", "Zhuowen", ""], ["Dollar", "Piotr", ""], ["Wu", "Yingnian", ""]]}, {"id": "1405.6886", "submitter": "Rasmus Troelsg{\\aa}rd", "authors": "Rasmus Troelsg{\\aa}rd, Bj{\\o}rn Sand Jensen, Lars Kai Hansen", "title": "A Topic Model Approach to Multi-Modal Similarity", "comments": "topic modelling workshop at NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating similarities between objects defined by many heterogeneous data\nmodalities is an important challenge in many multimedia applications. We use a\nmulti-modal topic model as a basis for defining such a similarity between\nobjects. We propose to compare the resulting similarities from different model\nrealizations using the non-parametric Mantel test. The approach is evaluated on\na music dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 12:34:24 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Troelsg\u00e5rd", "Rasmus", ""], ["Jensen", "Bj\u00f8rn Sand", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1405.6914", "submitter": "Ivan Ivek", "authors": "Ivan Ivek", "title": "Supervised Dictionary Learning by a Variational Bayesian Group Sparse\n  Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) with group sparsity constraints is\nformulated as a probabilistic graphical model and, assuming some observed data\nhave been generated by the model, a feasible variational Bayesian algorithm is\nderived for learning model parameters. When used in a supervised learning\nscenario, NMF is most often utilized as an unsupervised feature extractor\nfollowed by classification in the obtained feature subspace. Having mapped the\nclass labels to a more general concept of groups which underlie sparsity of the\ncoefficients, what the proposed group sparse NMF model allows is incorporating\nclass label information to find low dimensional label-driven dictionaries which\nnot only aim to represent the data faithfully, but are also suitable for class\ndiscrimination. Experiments performed in face recognition and facial expression\nrecognition domains point to advantages of classification in such label-driven\nfeature subspaces over classification in feature subspaces obtained in an\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 14:02:45 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Ivek", "Ivan", ""]]}, {"id": "1405.6922", "submitter": "Omid Aghazadeh", "authors": "Omid Aghazadeh and Stefan Carlsson", "title": "Large Scale, Large Margin Classification using Indefinite Similarity\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of the popular kernelized support vector machines, they\nhave two major limitations: they are restricted to Positive Semi-Definite (PSD)\nkernels, and their training complexity scales at least quadratically with the\nsize of the data. Many natural measures of similarity between pairs of samples\nare not PSD e.g. invariant kernels, and those that are implicitly or explicitly\ndefined by latent variable models. In this paper, we investigate scalable\napproaches for using indefinite similarity measures in large margin frameworks.\nIn particular we show that a normalization of similarity to a subset of the\ndata points constitutes a representation suitable for linear classifiers. The\nresult is a classifier which is competitive to kernelized SVM in terms of\naccuracy, despite having better training and test time complexities.\nExperimental results demonstrate that on CIFAR-10 dataset, the model equipped\nwith similarity measures invariant to rigid and non-rigid deformations, can be\nmade more than 5 times sparser while being more accurate than kernelized SVM\nusing RBF kernels.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 14:18:26 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Aghazadeh", "Omid", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1405.6974", "submitter": "Max Kuhn", "authors": "Max Kuhn", "title": "Futility Analysis in the Cross-Validation of Machine Learning Models", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning models have important structural tuning parameters that\ncannot be directly estimated from the data. The common tactic for setting these\nparameters is to use resampling methods, such as cross--validation or the\nbootstrap, to evaluate a candidate set of values and choose the best based on\nsome pre--defined criterion. Unfortunately, this process can be time consuming.\nHowever, the model tuning process can be streamlined by adaptively resampling\ncandidate values so that settings that are clearly sub-optimal can be\ndiscarded. The notion of futility analysis is introduced in this context. An\nexample is shown that illustrates how adaptive resampling can be used to reduce\ntraining time. Simulation studies are used to understand how the potential\nspeed--up is affected by parallel processing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 16:52:49 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Kuhn", "Max", ""]]}, {"id": "1405.7085", "submitter": "Raef Bassily", "authors": "Raef Bassily, Adam Smith, Abhradeep Thakurta", "title": "Differentially Private Empirical Risk Minimization: Efficient Algorithms\n  and Tight Error Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a systematic investigation of differentially\nprivate algorithms for convex empirical risk minimization. Various\ninstantiations of this problem have been studied before. We provide new\nalgorithms and matching lower bounds for private ERM assuming only that each\ndata point's contribution to the loss function is Lipschitz bounded and that\nthe domain of optimization is bounded. We provide a separate set of algorithms\nand matching lower bounds for the setting in which the loss functions are known\nto also be strongly convex.\n  Our algorithms run in polynomial time, and in some cases even match the\noptimal non-private running time (as measured by oracle complexity). We give\nseparate algorithms (and lower bounds) for $(\\epsilon,0)$- and\n$(\\epsilon,\\delta)$-differential privacy; perhaps surprisingly, the techniques\nused for designing optimal algorithms in the two cases are completely\ndifferent.\n  Our lower bounds apply even to very simple, smooth function families, such as\nlinear and quadratic functions. This implies that algorithms from previous work\ncan be used to obtain optimal error rates, under the additional assumption that\nthe contributions of each data point to the loss function is smooth. We show\nthat simple approaches to smoothing arbitrary loss functions (in order to apply\nprevious techniques) do not yield optimal error rates. In particular, optimal\nalgorithms were not previously known for problems such as training support\nvector machines and the high-dimensional median.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 22:58:26 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 23:49:13 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1405.7129", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi", "title": "Marginalization and Conditioning for LWF Chain Graphs", "comments": "48 pages, 9 figures, 2 tables", "journal-ref": "Annals of Statistics, 44 (4), 1792--1816, 2016", "doi": "10.1214/16-AOS1451", "report-no": null, "categories": "stat.OT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of marginalization over and\nconditioning on two disjoint subsets of the node set of chain graphs (CGs) with\nthe LWF Markov property. For this purpose, we define the class of chain mixed\ngraphs (CMGs) with three types of edges and, for this class, provide a\nseparation criterion under which the class of CMGs is stable under\nmarginalization and conditioning and contains the class of LWF CGs as its\nsubclass. We provide a method for generating such graphs after marginalization\nand conditioning for a given CMG or a given LWF CG. We then define and study\nthe class of anterial graphs, which is also stable under marginalization and\nconditioning and contains LWF CGs, but has a simpler structure than CMGs.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 06:29:35 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 15:15:11 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2016 22:06:58 GMT"}, {"version": "v4", "created": "Sun, 28 Aug 2016 20:00:42 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Sadeghi", "Kayvan", ""]]}, {"id": "1405.7292", "submitter": "Michael Smith", "authors": "Michael R. Smith and Andrew White and Christophe Giraud-Carrier and\n  Tony Martinez", "title": "An Easy to Use Repository for Comparing and Improving Machine Learning\n  Algorithm Usage", "comments": "7 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results from most machine learning experiments are used for a specific\npurpose and then discarded. This results in a significant loss of information\nand requires rerunning experiments to compare learning algorithms. This also\nrequires implementation of another algorithm for comparison, that may not\nalways be correctly implemented. By storing the results from previous\nexperiments, machine learning algorithms can be compared easily and the\nknowledge gained from them can be used to improve their performance. The\npurpose of this work is to provide easy access to previous experimental results\nfor learning and comparison. These stored results are comprehensive -- storing\nthe prediction for each test instance as well as the learning algorithm,\nhyperparameters, and training set that were used. Previous results are\nparticularly important for meta-learning, which, in a broad sense, is the\nprocess of learning from previous machine learning results such that the\nlearning process is improved. While other experiment databases do exist, one of\nour focuses is on easy access to the data. We provide meta-learning data sets\nthat are ready to be downloaded for meta-learning experiments. In addition,\nqueries to the underlying database can be made if specific information is\ndesired. We also differ from previous experiment databases in that our\ndatabases is designed at the instance level, where an instance is an example in\na data set. We store the predictions of a learning algorithm trained on a\nspecific training set for each instance in the test set. Data set level\ninformation can then be obtained by aggregating the results from the instances.\nThe instance level information can be used for many tasks such as determining\nthe diversity of a classifier or algorithmically determining the optimal subset\nof training instances for a learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 16:08:32 GMT"}, {"version": "v2", "created": "Thu, 5 Jun 2014 15:47:26 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Smith", "Michael R.", ""], ["White", "Andrew", ""], ["Giraud-Carrier", "Christophe", ""], ["Martinez", "Tony", ""]]}, {"id": "1405.7393", "submitter": "Kalpit Desai", "authors": "Kalpit V Desai, Roopesh Ranjan", "title": "Insights from the Wikipedia Contest (IEEE Contest for Data Mining 2011)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Wikimedia Foundation has recently observed that newly joining editors on\nWikipedia are increasingly failing to integrate into the Wikipedia editors'\ncommunity, i.e. the community is becoming increasingly harder to penetrate. To\nsustain healthy growth of the community, the Wikimedia Foundation aims to\nquantitatively understand the factors that determine the editing behavior, and\nexplain why most new editors become inactive soon after joining. As a step\ntowards this broader goal, the Wikimedia foundation sponsored the ICDM (IEEE\nInternational Conference for Data Mining) contest for the year 2011.\n  The objective for the participants was to develop models to predict the\nnumber of edits that an editor will make in future five months based on the\nediting history of the editor. Here we describe the approach we followed for\ndeveloping predictive models towards this goal, the results that we obtained\nand the modeling insights that we gained from this exercise. In addition,\ntowards the broader goal of Wikimedia Foundation, we also summarize the factors\nthat emerged during our model building exercise as powerful predictors of\nfuture editing activity.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 11:55:26 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Desai", "Kalpit V", ""], ["Ranjan", "Roopesh", ""]]}, {"id": "1405.7569", "submitter": "Ngoc-Cuong Nguyen Dr.", "authors": "Ngoc-Cuong Nguyen and Jaime Peraire", "title": "Functional Gaussian processes for regression with linear PDE models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new statistical approach to the problem of\nincorporating experimental observations into a mathematical model described by\nlinear partial differential equations (PDEs) to improve the prediction of the\nstate of a physical system. We augment the linear PDE with a functional that\naccounts for the uncertainty in the mathematical model and is modeled as a {\\em\nGaussian process}. This gives rise to a stochastic PDE which is characterized\nby the Gaussian functional. We develop a {\\em functional Gaussian process\nregression} method to determine the posterior mean and covariance of the\nGaussian functional, thereby solving the stochastic PDE to obtain the posterior\ndistribution for our prediction of the physical state. Our method has the\nfollowing features which distinguish itself from other regression methods.\nFirst, it incorporates both the mathematical model and the observations into\nthe regression procedure. Second, it can handle the observations given in the\nform of linear functionals of the field variable. Third, the method is\nnon-parametric in the sense that it provides a systematic way to optimally\ndetermine the prior covariance operator of the Gaussian functional based on the\nobservations. Fourth, it provides the posterior distribution quantifying the\nmagnitude of uncertainty in our prediction of the physical state. We present\nnumerical results to illustrate these features of the method and compare its\nperformance to that of the standard Gaussian process regression.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 14:40:11 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Nguyen", "Ngoc-Cuong", ""], ["Peraire", "Jaime", ""]]}, {"id": "1405.7752", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Michal Valko", "title": "Learning to Act Greedily: Polymatroid Semi-Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important optimization problems, such as the minimum spanning tree and\nminimum-cost flow, can be solved optimally by a greedy method. In this work, we\nstudy a learning variant of these problems, where the model of the problem is\nunknown and has to be learned by interacting repeatedly with the environment in\nthe bandit setting. We formalize our learning problem quite generally, as\nlearning how to maximize an unknown modular function on a known polymatroid. We\npropose a computationally efficient algorithm for solving our problem and bound\nits expected cumulative regret. Our gap-dependent upper bound is tight up to a\nconstant and our gap-free upper bound is tight up to polylogarithmic factors.\nFinally, we evaluate our method on three problems and demonstrate that it is\npractical.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 00:35:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 21:26:40 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 10:13:34 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Valko", "Michal", ""]]}, {"id": "1405.7764", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula and Cynthia Rudin", "title": "Generalization Bounds for Learning with Linear, Polygonal, Quadratic and\n  Conic Side Knowledge", "comments": "37 pages, 3 figures, a shorter version appeared in ISAIM 2014 (new\n  additions include a reference change and a new figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a supervised learning setting where side knowledge\nis provided about the labels of unlabeled examples. The side knowledge has the\neffect of reducing the hypothesis space, leading to tighter generalization\nbounds, and thus possibly better generalization. We consider several types of\nside knowledge, the first leading to linear and polygonal constraints on the\nhypothesis space, the second leading to quadratic constraints, and the last\nleading to conic constraints. We show how different types of domain knowledge\ncan lead directly to these kinds of side knowledge. We prove bounds on\ncomplexity measures of the hypothesis space for quadratic and conic side\nknowledge, and show that these bounds are tight in a specific sense for the\nquadratic case.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 02:05:37 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 00:58:03 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 16:45:06 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}]