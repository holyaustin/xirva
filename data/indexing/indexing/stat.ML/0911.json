[{"id": "0911.0054", "submitter": "Ambuj Tewari", "authors": "Sham M. Kakade, Ohad Shamir, Karthik Sridharan, Ambuj Tewari", "title": "Learning Exponential Families in High-Dimensions: Strong Convexity and\n  Sparsity", "comments": "Errata added. Incorrect claim about cumulants of the Bernoulli\n  distribution fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The versatility of exponential families, along with their attendant convexity\nproperties, make them a popular and effective statistical model. A central\nissue is learning these models in high-dimensions, such as when there is some\nsparsity pattern of the optimal parameter. This work characterizes a certain\nstrong convexity property of general exponential families, which allow their\ngeneralization ability to be quantified. In particular, we show how this\nproperty can be used to analyze generic exponential families under L_1\nregularization.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2009 02:56:18 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 22:45:35 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kakade", "Sham M.", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "0911.0280", "submitter": "Jonas Peters", "authors": "Jonas Peters, Dominik Janzing, Bernhard Sch\\\"olkopf", "title": "Causal Inference on Discrete Data using Additive Noise Models", "comments": null, "journal-ref": "IEEE TPAMI vol. 33 no. 12 (2011) 2436-2450", "doi": "10.1109/TPAMI.2011.71", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the causal structure of a set of random variables from a finite\nsample of the joint distribution is an important problem in science. Recently,\nmethods using additive noise models have been suggested to approach the case of\ncontinuous variables. In many situations, however, the variables of interest\nare discrete or even have only finitely many states. In this work we extend the\nnotion of additive noise models to these cases. We prove that whenever the\njoint distribution $\\prob^{(X,Y)}$ admits such a model in one direction, e.g.\n$Y=f(X)+N, N \\independent X$, it does not admit the reversed model\n$X=g(Y)+\\tilde N, \\tilde N \\independent Y$ as long as the model is chosen in a\ngeneric way. Based on these deliberations we propose an efficient new algorithm\nthat is able to distinguish between cause and effect for a finite sample of\ndiscrete variables. In an extensive experimental study we show that this\nalgorithm works both on synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2009 12:00:33 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "0911.0491", "submitter": "Alex Smola J", "authors": "John Langford (1), Alexander Smola (1 and 2), Martin Zinkevich (2)\n  ((1) Yahoo Labs, (2) Australian National University, (3) Yahoo Labs)", "title": "Slow Learners are Fast", "comments": "Extended version of conference paper - NIPS 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms have impressive convergence properties when it\ncomes to risk minimization and convex games on very large problems. However,\nthey are inherently sequential in their design which prevents them from taking\nadvantage of modern multi-core architectures. In this paper we prove that\nonline learning with delayed updates converges well, thereby facilitating\nparallel online learning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 05:25:51 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Langford", "John", "", "Yahoo Labs"], ["Smola", "Alexander", "", "1 and 2"], ["Zinkevich", "Martin", "", "Australian National University"]]}, {"id": "0911.2888", "submitter": "Lotfi Chaari", "authors": "L. Cha\\^ari, J.-C. Pesquet, J.-Y. Tourneret, Ph. Ciuciu and A.\n  Benazza-Benyahia", "title": "A Hierarchical Bayesian Model for Frame Representation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2010.2055562", "report-no": null, "categories": "stat.ME math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many signal processing problems, it may be fruitful to represent the\nsignal under study in a frame. If a probabilistic approach is adopted, it\nbecomes then necessary to estimate the hyper-parameters characterizing the\nprobability distribution of the frame coefficients. This problem is difficult\nsince in general the frame synthesis operator is not bijective. Consequently,\nthe frame coefficients are not directly observable. This paper introduces a\nhierarchical Bayesian model for frame representation. The posterior\ndistribution of the frame coefficients and model hyper-parameters is derived.\nHybrid Markov Chain Monte Carlo algorithms are subsequently proposed to sample\nfrom this posterior distribution. The generated samples are then exploited to\nestimate the hyper-parameters and the frame coefficients of the target signal.\nValidation experiments show that the proposed algorithms provide an accurate\nestimation of the frame coefficients and hyper-parameters. Application to\npractical problems of image denoising show the impact of the resulting Bayesian\nestimation on the recovered signal quality.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2009 17:07:28 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Cha\u00e2ri", "L.", ""], ["Pesquet", "J. -C.", ""], ["Tourneret", "J. -Y.", ""], ["Ciuciu", "Ph.", ""], ["Benazza-Benyahia", "A.", ""]]}, {"id": "0911.2919", "submitter": "Philippe Rigollet", "authors": "Philippe Rigollet", "title": "Kullback-Leibler aggregation and misspecified generalized linear models", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS961 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 639-665", "doi": "10.1214/11-AOS961", "report-no": "IMS-AOS-AOS961", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression setup with deterministic design, we study the pure\naggregation problem and introduce a natural extension from the Gaussian\ndistribution to distributions in the exponential family. While this extension\nbears strong connections with generalized linear models, it does not require\nidentifiability of the parameter or even that the model on the systematic\ncomponent is true. It is shown that this problem can be solved by constrained\nand/or penalized likelihood maximization and we derive sharp oracle\ninequalities that hold both in expectation and with high probability. Finally\nall the bounds are proved to be optimal in a minimax sense.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2009 15:27:49 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2010 15:45:22 GMT"}, {"version": "v3", "created": "Tue, 30 Nov 2010 21:48:16 GMT"}, {"version": "v4", "created": "Sun, 8 Jan 2012 19:23:14 GMT"}, {"version": "v5", "created": "Tue, 5 Jun 2012 05:40:43 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Rigollet", "Philippe", ""]]}, {"id": "0911.3633", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein and J. Hyam Rubinstein", "title": "A Geometric Approach to Sample Compression", "comments": "37 pages, 18 figures, submitted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO math.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sample Compression Conjecture of Littlestone & Warmuth has remained\nunsolved for over two decades. This paper presents a systematic geometric\ninvestigation of the compression of finite maximum concept classes. Simple\narrangements of hyperplanes in Hyperbolic space, and Piecewise-Linear\nhyperplane arrangements, are shown to represent maximum classes, generalizing\nthe corresponding Euclidean result. A main result is that PL arrangements can\nbe swept by a moving hyperplane to unlabeled d-compress any finite maximum\nclass, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary\nis that some d-maximal classes cannot be embedded into any maximum class of VC\ndimension d+k, for any constant k. The construction of the PL sweeping involves\nPachner moves on the one-inclusion graph, corresponding to moves of a\nhyperplane across the intersection of d other hyperplanes. This extends the\nwell known Pachner moves for triangulations to cubical complexes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2009 19:22:09 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Rubinstein", "J. Hyam", ""]]}, {"id": "0911.3944", "submitter": "Patrick J. Wolfe", "authors": "Christopher M. White, Sanjeev P. Khudanpur, and Patrick J. Wolfe", "title": "Likelihood-based semi-supervised model selection with applications to\n  speech processing", "comments": "11 pages, 2 figures; submitted for publication", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  1016-1026, 2010", "doi": "10.1109/JSTSP.2010.2076050", "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional supervised pattern recognition tasks, model selection is\ntypically accomplished by minimizing the classification error rate on a set of\nso-called development data, subject to ground-truth labeling by human experts\nor some other means. In the context of speech processing systems and other\nlarge-scale practical applications, however, such labeled development data are\ntypically costly and difficult to obtain. This article proposes an alternative\nsemi-supervised framework for likelihood-based model selection that leverages\nunlabeled data by using trained classifiers representing each model to\nautomatically generate putative labels. The errors that result from this\nautomatic labeling are shown to be amenable to results from robust statistics,\nwhich in turn provide for minimax-optimal censored likelihood ratio tests that\nrecover the nonparametric sign test as a limiting case. This approach is then\nvalidated experimentally using a state-of-the-art automatic speech recognition\nsystem to select between candidate word pronunciations using unlabeled speech\ndata that only potentially contain instances of the words under test. Results\nprovide supporting evidence for the utility of this approach, and suggest that\nit may also find use in other applications of machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 01:30:36 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["White", "Christopher M.", ""], ["Khudanpur", "Sanjeev P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0911.4046", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama", "title": "Super-Linear Convergence of Dual Augmented-Lagrangian Algorithm for\n  Sparsity Regularized Estimation", "comments": "51 pages, 9 figures", "journal-ref": "Journal of Machine Learning Research, 12(May):1537-1586, 2011", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence behaviour of a recently proposed algorithm for\nregularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is\nbased on a new interpretation of DAL as a proximal minimization algorithm. We\ntheoretically show under some conditions that DAL converges super-linearly in a\nnon-asymptotic and global sense. Due to a special modelling of sparse\nestimation problems in the context of machine learning, the assumptions we make\nare milder and more natural than those made in conventional analysis of\naugmented Lagrangian algorithms. In addition, the new interpretation enables us\nto generalize DAL to wide varieties of sparse estimation problems. We\nexperimentally confirm our analysis in a large scale $\\ell_1$-regularized\nlogistic regression problem and extensively compare the efficiency of DAL\nalgorithm to previously proposed algorithms on both synthetic and benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 13:44:28 GMT"}, {"version": "v2", "created": "Wed, 12 May 2010 12:33:07 GMT"}, {"version": "v3", "created": "Sun, 2 Jan 2011 07:04:21 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "0911.4397", "submitter": "Wolfgang Konen K", "authors": "Wolfgang Konen, Patrick Koch", "title": "How slow is slow? SFA detects signals that are slower than the driving\n  force", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is a method for extracting slowly varying driving\nforces from quickly varying nonstationary time series. We show here that it is\npossible for SFA to detect a component which is even slower than the driving\nforce itself (e.g. the envelope of a modulated sine wave). It is shown that it\ndepends on circumstances like the embedding dimension, the time series\npredictability, or the base frequency, whether the driving force itself or a\nslower subcomponent is detected. We observe a phase transition from one regime\nto the other and it is the purpose of this work to quantify the influence of\nvarious parameters on this phase transition. We conclude that what is percieved\nas slow by SFA varies and that a more or less fast switching from one regime to\nthe other occurs, perhaps showing some similarity to human perception.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2009 14:00:50 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Konen", "Wolfgang", ""], ["Koch", "Patrick", ""]]}, {"id": "0911.4511", "submitter": "Gowtham Bellala", "authors": "Gowtham Bellala, Suresh Bhavnani and Clayton Scott", "title": "Group-based Query Learning for rapid diagnosis in time-critical\n  situations", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In query learning, the goal is to identify an unknown object while minimizing\nthe number of \"yes or no\" questions (queries) posed about that object. We\nconsider three extensions of this fundamental problem that are motivated by\npractical considerations in real-world, time-critical identification tasks such\nas emergency response. First, we consider the problem where the objects are\npartitioned into groups, and the goal is to identify only the group to which\nthe object belongs. Second, we address the situation where the queries are\npartitioned into groups, and an algorithm may suggest a group of queries to a\nhuman user, who then selects the actual query. Third, we consider the problem\nof query learning in the presence of persistent query noise, and relate it to\ngroup identification. To address these problems we show that a standard\nalgorithm for query learning, known as the splitting algorithm or generalized\nbinary search, may be viewed as a generalization of Shannon-Fano coding. We\nthen extend this result to the group-based settings, leading to new algorithms.\nThe performance of our algorithms is demonstrated on simulated data and on a\ndatabase used by first responders for toxic chemical identification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2009 19:53:54 GMT"}], "update_date": "2009-11-25", "authors_parsed": [["Bellala", "Gowtham", ""], ["Bhavnani", "Suresh", ""], ["Scott", "Clayton", ""]]}, {"id": "0911.4899", "submitter": "Zhiyi Chi", "authors": "Zhiyi Chi", "title": "On $\\ell_1$-regularized estimation for nonlinear models that have sparse\n  underlying linear structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work (arXiv:0910.2517), for nonlinear models with sparse\nunderlying linear structures, we studied the error bounds of\n$\\ell_0$-regularized estimation. In this note, we show that\n$\\ell_1$-regularized estimation in some important cases can achieve the same\norder of error bounds as those in the aforementioned work.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2009 16:06:00 GMT"}], "update_date": "2009-11-26", "authors_parsed": [["Chi", "Zhiyi", ""]]}, {"id": "0911.5107", "submitter": "Mauricio A. \\'Alvarez", "authors": "Mauricio A. \\'Alvarez and Neil D. Lawrence", "title": "Sparse Convolved Multiple Output Gaussian Processes", "comments": "Extended version of the conference paper Sparse Convolved Gaussian\n  Processes for Multi-output Regression (NIPS 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an increasing interest in methods that deal with\nmultiple outputs. This has been motivated partly by frameworks like multitask\nlearning, multisensor networks or structured output data. From a Gaussian\nprocesses perspective, the problem reduces to specifying an appropriate\ncovariance function that, whilst being positive semi-definite, captures the\ndependencies between all the data points and across all the outputs. One\napproach to account for non-trivial correlations between outputs employs\nconvolution processes. Under a latent function interpretation of the\nconvolution transform we establish dependencies between output variables. The\nmain drawbacks of this approach are the associated computational and storage\ndemands. In this paper we address these issues. We present different sparse\napproximations for dependent output Gaussian processes constructed through the\nconvolution formalism. We exploit the conditional independencies present\nnaturally in the model. This leads to a form of the covariance similar in\nspirit to the so called PITC and FITC approximations for a single output. We\nshow experimental results with synthetic and real data, in particular, we show\nresults in pollution prediction, school exams score prediction and gene\nexpression data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2009 16:01:20 GMT"}], "update_date": "2009-11-30", "authors_parsed": [["\u00c1lvarez", "Mauricio A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "0911.5242", "submitter": "Coryn Bailer-Jones", "authors": "C.A.L. Bailer-Jones (Max Planck Institute for Astronomy, Heidelberg)", "title": "The ILIUM forward modelling algorithm for multivariate parameter\n  estimation and its application to derive stellar parameters from Gaia\n  spectrophotometry", "comments": "MNRAS, in press. This revision corrects a few minor errors and typos.\n  A better formatted version for A4 paper is available at\n  http://www.mpia.de/home/calj/ilium.pdf", "journal-ref": null, "doi": "10.1111/j.1365-2966.2009.16125.x", "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce an algorithm for estimating parameters from multidimensional data\nbased on forward modelling. In contrast to many machine learning approaches it\navoids fitting an inverse model and the problems associated with this. The\nalgorithm makes explicit use of the sensitivities of the data to the\nparameters, with the goal of better treating parameters which only have a weak\nimpact on the data. The forward modelling approach provides uncertainty (full\ncovariance) estimates in the predicted parameters as well as a goodness-of-fit\nfor observations. I demonstrate the algorithm, ILIUM, with the estimation of\nstellar astrophysical parameters (APs) from simulations of the low resolution\nspectrophotometry to be obtained by Gaia. The AP accuracy is competitive with\nthat obtained by a support vector machine. For example, for zero extinction\nstars covering a wide range of metallicity, surface gravity and temperature,\nILIUM can estimate Teff to an accuracy of 0.3% at G=15 and to 4% for (lower\nsignal-to-noise ratio) spectra at G=20. [Fe/H] and logg can be estimated to\naccuracies of 0.1-0.4dex for stars with G<=18.5. If extinction varies a priori\nover a wide range (Av=0-10mag), then Teff and Av can be estimated quite\naccurately (3-4% and 0.1-0.2mag respectively at G=15), but there is a strong\nand ubiquitous degeneracy in these parameters which limits our ability to\nestimate either accurately at faint magnitudes. Using the forward model we can\nmap these degeneracies (in advance), and thus provide a complete probability\ndistribution over solutions. (Abridged)\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2009 11:28:04 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2010 07:18:42 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Bailer-Jones", "C. A. L.", "", "Max Planck Institute for Astronomy, Heidelberg"]]}, {"id": "0911.5367", "submitter": "Marco Cuturi", "authors": "Marco Cuturi", "title": "Positive Definite Kernels in Machine Learning", "comments": "draft. corrected a typo in figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey is an introduction to positive definite kernels and the set of\nmethods they have inspired in the machine learning literature, namely kernel\nmethods. We first discuss some properties of positive definite kernels as well\nas reproducing kernel Hibert spaces, the natural extension of the set of\nfunctions $\\{k(x,\\cdot),x\\in\\mathcal{X}\\}$ associated with a kernel $k$ defined\non a space $\\mathcal{X}$. We discuss at length the construction of kernel\nfunctions that take advantage of well-known statistical models. We provide an\noverview of numerous data-analysis methods which take advantage of reproducing\nkernel Hilbert spaces and discuss the idea of combining several kernels to\nimprove the performance on certain tasks. We also provide a short cookbook of\ndifferent kernels which are particularly useful for certain data-types such as\nimages, graphs or speech segments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2009 01:42:04 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2009 19:51:36 GMT"}], "update_date": "2009-12-04", "authors_parsed": [["Cuturi", "Marco", ""]]}, {"id": "0911.5439", "submitter": "Ali Shojaie", "authors": "Ali Shojaie and George Michailidis", "title": "Penalized Likelihood Methods for Estimation of Sparse High Dimensional\n  Directed Acyclic Graphs", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) are commonly used to represent causal\nrelationships among random variables in graphical models. Applications of these\nmodels arise in the study of physical, as well as biological systems, where\ndirected edges between nodes represent the influence of components of the\nsystem on each other. The general problem of estimating DAGs from observed data\nis computationally NP-hard, Moreover two directed graphs may be observationally\nequivalent. When the nodes exhibit a natural ordering, the problem of\nestimating directed graphs reduces to the problem of estimating the structure\nof the network. In this paper, we propose a penalized likelihood approach that\ndirectly estimates the adjacency matrix of DAGs. Both lasso and adaptive lasso\npenalties are considered and an efficient algorithm is proposed for estimation\nof high dimensional DAGs. We study variable selection consistency of the two\npenalties when the number of variables grows to infinity with the sample size.\nWe show that although lasso can only consistently estimate the true network\nunder stringent assumptions, adaptive lasso achieves this task under mild\nregularity conditions. The performance of the proposed methods is compared to\nalternative methods in simulated, as well as real, data examples.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2009 23:47:30 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Shojaie", "Ali", ""], ["Michailidis", "George", ""]]}, {"id": "0911.5460", "submitter": "Yiyuan She", "authors": "Yiyuan She", "title": "An Iterative Algorithm for Fitting Nonconvex Penalized Generalized\n  Linear Models with Grouped Predictors", "comments": "Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data pose challenges in statistical learning and modeling.\nSometimes the predictors can be naturally grouped where pursuing the\nbetween-group sparsity is desired. Collinearity may occur in real-world\nhigh-dimensional applications where the popular $l_1$ technique suffers from\nboth selection inconsistency and prediction inaccuracy. Moreover, the problems\nof interest often go beyond Gaussian models. To meet these challenges,\nnonconvex penalized generalized linear models with grouped predictors are\ninvestigated and a simple-to-implement algorithm is proposed for computation. A\nrigorous theoretical result guarantees its convergence and provides tight\npreliminary scaling. This framework allows for grouped predictors and nonconvex\npenalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties.\nPenalty design and parameter tuning for nonconvex penalties are examined.\nApplications of super-resolution spectrum estimation in signal processing and\ncancer classification with joint gene selection in bioinformatics show the\nperformance improvement by nonconvex penalized estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2009 06:27:48 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2009 08:07:21 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2010 04:39:15 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2010 22:06:34 GMT"}, {"version": "v5", "created": "Thu, 10 Nov 2011 20:19:34 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["She", "Yiyuan", ""]]}, {"id": "0911.5482", "submitter": "Ya'acov Ritov", "authors": "Natalia Bochkina and Ya'acov Ritov", "title": "Sparse Empirical Bayes Analysis (SEBA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a joint processing of $n$ independent sparse regression problems.\nEach is based on a sample $(y_{i1},x_{i1})...,(y_{im},x_{im})$ of $m$ \\iid\nobservations from $y_{i1}=x_{i1}\\t\\beta_i+\\eps_{i1}$, $y_{i1}\\in \\R$, $x_{i\n1}\\in\\R^p$, $i=1,...,n$, and $\\eps_{i1}\\dist N(0,\\sig^2)$, say. $p$ is large\nenough so that the empirical risk minimizer is not consistent. We consider\nthree possible extensions of the lasso estimator to deal with this problem, the\nlassoes, the group lasso and the RING lasso, each utilizing a different\nassumption how these problems are related. For each estimator we give a\nBayesian interpretation, and we present both persistency analysis and\nnon-asymptotic error bounds based on restricted eigenvalue - type assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 20:06:04 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2010 05:21:40 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Bochkina", "Natalia", ""], ["Ritov", "Ya'acov", ""]]}]