[{"id": "0809.0490", "submitter": "Alexander Gorban", "authors": "A. N. Gorban, A. Y. Zinovyev", "title": "Principal Graphs and Manifolds", "comments": "36 pages, 6 figures, minor corrections", "journal-ref": "Handbook of Research on Machine Learning Applications and Trends:\n  Algorithms, Methods and Techniques, Ch. 2, Information Science Reference,\n  2009. 28-59", "doi": "10.4018/978-1-60566-766-9", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In many physical, statistical, biological and other investigations it is\ndesirable to approximate a system of points by objects of lower dimension\nand/or complexity. For this purpose, Karl Pearson invented principal component\nanalysis in 1901 and found 'lines and planes of closest fit to system of\npoints'. The famous k-means algorithm solves the approximation problem too, but\nby finite sets instead of lines and planes. This chapter gives a brief\npractical introduction into the methods of construction of general principal\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\nset. As a basis, the unifying framework of mean squared distance approximation\nof finite datasets is selected. Principal graphs and manifolds are constructed\nas generalisations of principal components and k-means principal points. For\nthis purpose, the family of expectation/maximisation algorithms with nearest\ngeneralisations is presented. Construction of principal graphs with controlled\ncomplexity is based on the graph grammar approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 18:04:53 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 13:23:08 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Gorban", "A. N.", ""], ["Zinovyev", "A. Y.", ""]]}, {"id": "0809.0492", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "From Data to the p-Adic or Ultrametric Model", "comments": "15 pages, 6 figures. To appear in: Proceedings of Third International\n  Conference on p-Adic Mathematical Physics: From Planck Scale Physics to\n  Complex Systems to Biology, Steklov Mathematics Institute, Russian Academy of\n  Sciences", "journal-ref": "p-Adic Numbers, Ultrametric Analysis and Applications, 1, 58-68,\n  2009", "doi": "10.1134/S2070046609010063", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model anomaly and change in data by embedding the data in an ultrametric\nspace. Taking our initial data as cross-tabulation counts (or other input data\nformats), Correspondence Analysis allows us to endow the information space with\na Euclidean metric. We then model anomaly or change by an induced ultrametric.\nThe induced ultrametric that we are particularly interested in takes a\nsequential - e.g. temporal - ordering of the data into account. We apply this\nwork to the flow of narrative expressed in the film script of the Casablanca\nmovie; and to the evolution between 1988 and 2004 of the Colombian social\nconflict and violence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 18:10:52 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "0809.1260", "submitter": "Benjamin Recht", "authors": "Benjamin Recht, Weiyu Xu, Babak Hassibi", "title": "Necessary and Sufficient Conditions for Success of the Nuclear Norm\n  Heuristic for Rank Minimization", "comments": "21 pages, 3 figures. A short version of this paper will appear at the\n  47th IEEE Conference on Decision and Control", "journal-ref": null, "doi": "10.1109/CDC.2008.4739332", "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the rank of a matrix subject to constraints is a challenging\nproblem that arises in many applications in control theory, machine learning,\nand discrete geometry. This class of optimization problems, known as rank\nminimization, is NP-HARD, and for most practical problems there are no\nefficient algorithms that yield exact solutions. A popular heuristic algorithm\nreplaces the rank function with the nuclear norm--equal to the sum of the\nsingular values--of the decision variable. In this paper, we provide a\nnecessary and sufficient condition that quantifies when this heuristic\nsuccessfully finds the minimum rank solution of a linear constraint set. We\nadditionally provide a probability distribution over instances of the affine\nrank minimization problem such that instances sampled from this distribution\nsatisfy our conditions for success with overwhelming probability provided the\nnumber of constraints is appropriately large. Finally, we give empirical\nevidence that these probabilistic bounds provide accurate predictions of the\nheuristic's performance in non-asymptotic scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2008 23:54:15 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Recht", "Benjamin", ""], ["Xu", "Weiyu", ""], ["Hassibi", "Babak", ""]]}, {"id": "0809.1270", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Predictive Hypothesis Identification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistics focusses on hypothesis testing and on estimating (properties\nof) the true sampling distribution, in machine learning the performance of\nlearning algorithms on future data is the primary issue. In this paper we\nbridge the gap with a general principle (PHI) that identifies hypotheses with\nbest predictive performance. This includes predictive point and interval\nestimation, simple and composite hypothesis testing, (mixture) model selection,\nand others as special cases. For concrete instantiations we will recover\nwell-known methods, variations thereof, and new ones. PHI nicely justifies,\nreconciles, and blends (a reparametrization invariant variation of) MAP, ML,\nMDL, and moment estimation. One particular feature of PHI is that it can\ngenuinely deal with nested hypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2008 04:18:17 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0809.1493", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For supervised and unsupervised learning, positive definite kernels allow to\nuse large and potentially infinite dimensional feature spaces with a\ncomputational cost that only depends on the number of observations. This is\nusually done through the penalization of predictor functions by Euclidean or\nHilbertian norms. In this paper, we explore penalizing by sparsity-inducing\nnorms such as the l1-norm or the block l1-norm. We assume that the kernel\ndecomposes into a large sum of individual basis kernels which can be embedded\nin a directed acyclic graph; we show that it is then possible to perform kernel\nselection through a hierarchical multiple kernel learning framework, in\npolynomial time in the number of selected kernels. This framework is naturally\napplied to non linear variable selection; our extensive simulations on\nsynthetic datasets and datasets from the UCI repository show that efficiently\nexploring the large feature space through sparsity-inducing norms leads to\nstate-of-the-art predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2008 06:48:10 GMT"}], "update_date": "2008-09-10", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0809.3373", "submitter": "Coryn Bailer-Jones", "authors": "C.A.L. Bailer-Jones (1), K.W. Smith (1), C. Tiede (1), R. Sordo (2),\n  A. Vallenari (2) ((1) MPIA, Heidelberg, (2) INAF, Padua)", "title": "Finding rare objects and building pure samples: Probabilistic quasar\n  classification from low resolution Gaia spectra", "comments": "MNRAS accepted", "journal-ref": "MNRAS 391, 1838 (2008)", "doi": "10.1111/j.1365-2966.2008.13983.x", "report-no": null, "categories": "astro-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and demonstrate a probabilistic method for classifying rare\nobjects in surveys with the particular goal of building very pure samples. It\nworks by modifying the output probabilities from a classifier so as to\naccommodate our expectation (priors) concerning the relative frequencies of\ndifferent classes of objects. We demonstrate our method using the Discrete\nSource Classifier, a supervised classifier currently based on Support Vector\nMachines, which we are developing in preparation for the Gaia data analysis.\nDSC classifies objects using their very low resolution optical spectra. We look\nin detail at the problem of quasar classification, because identification of a\npure quasar sample is necessary to define the Gaia astrometric reference frame.\nBy varying a posterior probability threshold in DSC we can trade off sample\ncompleteness and contamination. We show, using our simulated data, that it is\npossible to achieve a pure sample of quasars (upper limit on contamination of 1\nin 40,000) with a completeness of 65% at magnitudes of G=18.5, and 50% at\nG=20.0, even when quasars have a frequency of only 1 in every 2000 objects. The\nstar sample completeness is simultaneously 99% with a contamination of 0.7%.\nIncluding parallax and proper motion in the classifier barely changes the\nresults. We further show that not accounting for class priors in the target\npopulation leads to serious misclassifications and poor predictions for sample\ncompleteness and contamination. (Truncated)\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2008 13:30:01 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Bailer-Jones", "C. A. L.", "", "MPIA, Heidelberg"], ["Smith", "K. W.", "", "MPIA, Heidelberg"], ["Tiede", "C.", "", "MPIA, Heidelberg"], ["Sordo", "R.", "", "INAF, Padua"], ["Vallenari", "A.", "", "INAF, Padua"]]}, {"id": "0809.3803", "submitter": "Emilia Nascimento", "authors": "Emilia Matos do Nascimento, Basilio de Braganca Pereira, Samanta\n  Teixeira Basto, Joaquim Ribeiro Filho", "title": "Survival tree and meld to predict long term survival in liver\n  transplantation waiting list", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many authors have described MELD as a predictor of short-term\nmortality in the liver transplantation waiting list. However MELD score\naccuracy to predict long term mortality has not been statistically evaluated.\nObjective: The aim of this study is to analyze the MELD score as well as other\nvariables as a predictor of long-term mortality using a new model: the Survival\nTree analysis. Study Design and Setting: The variables obtained at the time of\nliver transplantation list enrollment and considered in this study are: sex,\nage, blood type, body mass index, etiology of liver disease, hepatocellular\ncarcinoma, waiting time for transplant and MELD. Mortality on the waiting list\nis the outcome. Exclusion, transplantation or still in the transplantation list\nat the end of the study are censored data. Results: The graphical\nrepresentation of the survival trees showed that the most statistically\nsignificant cut off is related to MELD score at point 16. Conclusion: The\nresults are compatible with the cut off point of MELD indicated in the clinical\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2008 14:39:36 GMT"}], "update_date": "2008-09-24", "authors_parsed": [["Nascimento", "Emilia Matos do", ""], ["Pereira", "Basilio de Braganca", ""], ["Basto", "Samanta Teixeira", ""], ["Filho", "Joaquim Ribeiro", ""]]}, {"id": "0809.3902", "submitter": "Stefano M. Iacus", "authors": "Alessandro De Gregorio, Stefano Maria Iacus", "title": "Clustering of discretely observed diffusion processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.PR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new dissimilarity measure to identify groups of assets\ndynamics is proposed. The underlying generating process is assumed to be a\ndiffusion process solution of stochastic differential equations and observed at\ndiscrete time. The mesh of observations is not required to shrink to zero. As\ndistance between two observed paths, the quadratic distance of the\ncorresponding estimated Markov operators is considered. Analysis of both\nsynthetic data and real financial data from NYSE/NASDAQ stocks, give evidence\nthat this distance seems capable to catch differences in both the drift and\ndiffusion coefficients contrary to other commonly used metrics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2008 12:31:17 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["De Gregorio", "Alessandro", ""], ["Iacus", "Stefano Maria", ""]]}, {"id": "0809.4178", "submitter": "Michael GB Blum", "authors": "M. G. B. Blum, O. Francois", "title": "Non-linear regression models for Approximate Bayesian Computation", "comments": "4 figures; version 3 minor changes; to appear in Statistics and\n  Computing", "journal-ref": "Statistics and Computing, 20: 63-73 (2010)", "doi": "10.1007/s11222-009-9116-0", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian inference on the basis of summary statistics is\nwell-suited to complex problems for which the likelihood is either\nmathematically or computationally intractable. However the methods that use\nrejection suffer from the curse of dimensionality when the number of summary\nstatistics is increased. Here we propose a machine-learning approach to the\nestimation of the posterior density by introducing two innovations. The new\nmethod fits a nonlinear conditional heteroscedastic regression of the parameter\non the summary statistics, and then adaptively improves estimation using\nimportance sampling. The new algorithm is compared to the state-of-the-art\napproximate Bayesian methods, and achieves considerable reduction of the\ncomputational burden in two examples of inference in statistical genetics and\nin a queueing model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2008 13:09:50 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2009 09:51:42 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Blum", "M. G. B.", ""], ["Francois", "O.", ""]]}, {"id": "0809.4866", "submitter": "Kevin Carter", "authors": "Kevin M. Carter, Raviv Raich, and Alfred O. Hero III", "title": "An Information Geometric Framework for Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report concerns the problem of dimensionality reduction through\ninformation geometric methods on statistical manifolds. While there has been\nconsiderable work recently presented regarding dimensionality reduction for the\npurposes of learning tasks such as classification, clustering, and\nvisualization, these methods have focused primarily on Riemannian manifolds in\nEuclidean space. While sufficient for many applications, there are many\nhigh-dimensional signals which have no straightforward and meaningful Euclidean\nrepresentation. In these cases, signals may be more appropriately represented\nas a realization of some distribution lying on a statistical manifold, or a\nmanifold of probability density functions (PDFs). We present a framework for\ndimensionality reduction that uses information geometry for both statistical\nmanifold reconstruction as well as dimensionality reduction in the data domain.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2008 15:15:10 GMT"}], "update_date": "2008-09-30", "authors_parsed": [["Carter", "Kevin M.", ""], ["Raich", "Raviv", ""], ["Hero", "Alfred O.", "III"]]}]