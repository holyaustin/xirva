[{"id": "0912.0171", "submitter": "Ngoc Duong", "authors": "Ngoc Duong (INRIA - Irisa), Emmanuel Vincent (INRIA - Irisa), Remi\n  Gribonval (INRIA - Irisa)", "title": "Under-determined reverberant audio source separation using a full-rank\n  spatial covariance model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the modeling of reverberant recording environments in\nthe context of under-determined convolutive blind source separation. We model\nthe contribution of each source to all mixture channels in the time-frequency\ndomain as a zero-mean Gaussian random variable whose covariance encodes the\nspatial characteristics of the source. We then consider four specific\ncovariance models, including a full-rank unconstrained model. We derive a\nfamily of iterative expectationmaximization (EM) algorithms to estimate the\nparameters of each model and propose suitable procedures to initialize the\nparameters and to align the order of the estimated sources across all frequency\nbins based on their estimated directions of arrival (DOA). Experimental results\nover reverberant synthetic mixtures and live recordings of speech data show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2009 14:54:36 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2009 13:14:20 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Duong", "Ngoc", "", "INRIA - Irisa"], ["Vincent", "Emmanuel", "", "INRIA - Irisa"], ["Gribonval", "Remi", "", "INRIA - Irisa"]]}, {"id": "0912.0284", "submitter": "Thomas Schick", "authors": "Laurent Bartholdi (1), Thomas Schick (1), Nat Smale (2), Steve Smale\n  (3), Anthony W. Baker (4) ((1) Georg-August-Universit\\\"at G\\\"ottingen, (2)\n  University of Utah, (3) City University of Hong Kong, (4) The Boing Company)", "title": "Hodge Theory on Metric Spaces", "comments": "appendix by Anthony W. Baker, 48 pages, AMS-LaTeX. v2: final version,\n  to appear in Foundations of Computational Mathematics. Minor changes and\n  additions", "journal-ref": "Foundations of Computational Mathematics 12:1 (2012) 1-48", "doi": "10.1007/s10208-011-9107-3", "report-no": null, "categories": "math.KT cs.CG math.GT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hodge theory is a beautiful synthesis of geometry, topology, and analysis,\nwhich has been developed in the setting of Riemannian manifolds. On the other\nhand, spaces of images, which are important in the mathematical foundations of\nvision and pattern recognition, do not fit this framework. This motivates us to\ndevelop a version of Hodge theory on metric spaces with a probability measure.\nWe believe that this constitutes a step towards understanding the geometry of\nvision.\n  The appendix by Anthony Baker provides a separable, compact metric space with\ninfinite dimensional \\alpha-scale homology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2009 22:22:47 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2011 19:08:34 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Bartholdi", "Laurent", ""], ["Schick", "Thomas", ""], ["Smale", "Nat", ""], ["Smale", "Steve", ""], ["Baker", "Anthony W.", ""]]}, {"id": "0912.0874", "submitter": "Robert Hable", "authors": "Robert Hable and Andreas Christmann (University of Bayreuth)", "title": "Qualitative Robustness of Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines have attracted much attention in theoretical and in\napplied statistics. Main topics of recent interest are consistency, learning\nrates and robustness. In this article, it is shown that support vector machines\nare qualitatively robust. Since support vector machines can be represented by a\nfunctional on the set of all probability measures, qualitative robustness is\nproven by showing that this functional is continuous with respect to the\ntopology generated by weak convergence of probability measures. Combined with\nthe existence and uniqueness of support vector machines, our results show that\nsupport vector machines are the solutions of a well-posed mathematical problem\nin Hadamard's sense.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2009 15:07:30 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2011 14:37:26 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Hable", "Robert", "", "University of Bayreuth"], ["Christmann", "Andreas", "", "University of Bayreuth"]]}, {"id": "0912.1064", "submitter": "Wolfgang Konen K", "authors": "Wolfgang Konen", "title": "On the numeric stability of the SFA implementation sfa-tk", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is a method for extracting slowly varying\nfeatures from a quickly varying multidimensional signal. An open source\nMatlab-implementation sfa-tk makes SFA easily useable. We show here that under\ncertain circumstances, namely when the covariance matrix of the nonlinearly\nexpanded data does not have full rank, this implementation runs into numerical\ninstabilities. We propse a modified algorithm based on singular value\ndecomposition (SVD) which is free of those instabilities even in the case where\nthe rank of the matrix is only less than 10% of its size. Furthermore we show\nthat an alternative way of handling the numerical problems is to inject a small\namount of noise into the multidimensional input signal which can restore a\nrank-deficient covariance matrix to full rank, however at the price of\nmodifying the original data and the need for noise parameter tuning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 00:14:28 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Konen", "Wolfgang", ""]]}, {"id": "0912.1072", "submitter": "Cameron Freer", "authors": "Cameron E. Freer, Daniel M. Roy", "title": "Computable de Finetti measures", "comments": "32 pages. Final journal version; expanded somewhat, with minor\n  corrections. To appear in Annals of Pure and Applied Logic. Extended abstract\n  appeared in Proceedings of CiE '09, LNCS 5635, pp. 218-231", "journal-ref": "Annals of Pure and Applied Logic 163 (2012) pp. 530-546", "doi": "10.1016/j.apal.2011.06.011", "report-no": null, "categories": "math.LO cs.LO cs.PL math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a computable version of de Finetti's theorem on exchangeable\nsequences of real random variables. As a consequence, exchangeable stochastic\nprocesses expressed in probabilistic functional programming languages can be\nautomatically rewritten as procedures that do not modify non-local state. Along\nthe way, we prove that a distribution on the unit interval is computable if and\nonly if its moments are uniformly computable.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 02:50:35 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2011 09:28:50 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Freer", "Cameron E.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "0912.1128", "submitter": "Timon Schroeter", "authors": "David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe,\n  Katja Hansen, Klaus-Robert Mueller", "title": "How to Explain Individual Classification Decisions", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After building a classifier with modern tools of machine learning we\ntypically have a black box at hand that is able to predict well for unseen\ndata. Thus, we get an answer to the question what is the most likely label of a\ngiven unseen data point. However, most methods will provide no answer why the\nmodel predicted the particular label for a single instance and what features\nwere most influential for that particular instance. The only method that is\ncurrently able to provide such explanations are decision trees. This paper\nproposes a procedure which (based on a set of assumptions) allows to explain\nthe decisions of any classification method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 19:29:04 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Baehrens", "David", ""], ["Schroeter", "Timon", ""], ["Harmeling", "Stefan", ""], ["Kawanabe", "Motoaki", ""], ["Hansen", "Katja", ""], ["Mueller", "Klaus-Robert", ""]]}, {"id": "0912.1618", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "St\\'ephane Ga\\\"iffas, Guillaume Lecu\\'e", "title": "Hyper-sparse optimal aggregation", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of \"hyper-sparse aggregation\". Namely,\ngiven a dictionary $F = \\{f_1, ..., f_M \\}$ of functions, we look for an\noptimal aggregation algorithm that writes $\\tilde f = \\sum_{j=1}^M \\theta_j\nf_j$ with as many zero coefficients $\\theta_j$ as possible. This problem is of\nparticular interest when $F$ contains many irrelevant functions that should not\nappear in $\\tilde{f}$. We provide an exact oracle inequality for $\\tilde f$,\nwhere only two coefficients are non-zero, that entails $\\tilde f$ to be an\noptimal aggregation algorithm. Since selectors are suboptimal aggregation\nprocedures, this proves that 2 is the minimal number of elements of $F$\nrequired for the construction of an optimal aggregation procedures in every\nsituations. A simulated example of this algorithm is proposed on a dictionary\nobtained using LARS, for the problem of selection of the regularization\nparameter of the LASSO. We also give an example of use of aggregation to\nachieve minimax adaptation over anisotropic Besov spaces, which was not\npreviously known in minimax theory (in regression on a random design).\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2009 21:59:52 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Ga\u00efffas", "St\u00e9phane", ""], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "0912.2412", "submitter": "Stefan Haufe", "authors": "Stefan Haufe, Ryota Tomioka, Guido Nolte, Klaus-Robert Mueller and\n  Motoaki Kawanabe", "title": "Modeling sparse connectivity between underlying brain sources for\n  EEG/MEG", "comments": "9 pages, 6 figures", "journal-ref": "IEEE Trans. Biomed. Eng. 57(8) (2010) 1954 - 1963;", "doi": "10.1109/TBME.2010.2046325", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to assess functional brain connectivity in\nEEG/MEG signals. Our method, called Sparsely-Connected Sources Analysis (SCSA),\ncan overcome the problem of volume conduction by modeling neural data\ninnovatively with the following ingredients: (a) the EEG is assumed to be a\nlinear mixture of correlated sources following a multivariate autoregressive\n(MVAR) model, (b) the demixing is estimated jointly with the source MVAR\nparameters, (c) overfitting is avoided by using the Group Lasso penalty. This\napproach allows to extract the appropriate level cross-talk between the\nextracted sources and in this manner we obtain a sparse data-driven model of\nfunctional connectivity. We demonstrate the usefulness of SCSA with simulated\ndata, and compare to a number of existing algorithms with excellent results.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2009 11:03:23 GMT"}], "update_date": "2010-08-05", "authors_parsed": [["Haufe", "Stefan", ""], ["Tomioka", "Ryota", ""], ["Nolte", "Guido", ""], ["Mueller", "Klaus-Robert", ""], ["Kawanabe", "Motoaki", ""]]}, {"id": "0912.2492", "submitter": "Hannes Nickisch", "authors": "Hannes Nickisch, Pushmeet Kohli and Carsten Rother", "title": "Learning an Interactive Segmentation System", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful applications of computer vision to image or video\nmanipulation are interactive by nature. However, parameters of such systems are\noften trained neglecting the user. Traditionally, interactive systems have been\ntreated in the same manner as their fully automatic counterparts. Their\nperformance is evaluated by computing the accuracy of their solutions under\nsome fixed set of user interactions. This paper proposes a new evaluation and\nlearning method which brings the user in the loop. It is based on the use of an\nactive robot user - a simulated model of a human user. We show how this\napproach can be used to evaluate and learn parameters of state-of-the-art\ninteractive segmentation systems. We also show how simulated user models can be\nintegrated into the popular max-margin method for parameter learning and\npropose an algorithm to solve the resulting optimisation problem.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2009 12:27:37 GMT"}], "update_date": "2009-12-15", "authors_parsed": [["Nickisch", "Hannes", ""], ["Kohli", "Pushmeet", ""], ["Rother", "Carsten", ""]]}, {"id": "0912.2695", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng and Rui Song", "title": "Nonparametric Independence Screening in Sparse Ultra-High Dimensional\n  Additive Models", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variable screening procedure via correlation learning was proposed Fan and\nLv (2008) to reduce dimensionality in sparse ultra-high dimensional models.\nEven when the true model is linear, the marginal regression can be highly\nnonlinear. To address this issue, we further extend the correlation learning to\nmarginal nonparametric learning. Our nonparametric independence screening is\ncalled NIS, a specific member of the sure independence screening. Several\nclosely related variable screening procedures are proposed. Under the\nnonparametric additive models, it is shown that under some mild technical\nconditions, the proposed independence screening methods enjoy a sure screening\nproperty. The extent to which the dimensionality can be reduced by independence\nscreening is also explicitly quantified. As a methodological extension, an\niterative nonparametric independence screening (INIS) is also proposed to\nenhance the finite sample performance for fitting sparse additive models. The\nsimulation results and a real data analysis demonstrate that the proposed\nprocedure works well with moderate sample size and large dimension and performs\nbetter than competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2009 17:35:50 GMT"}, {"version": "v2", "created": "Tue, 18 Jan 2011 18:17:27 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Song", "Rui", ""]]}, {"id": "0912.2800", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Taiji Suzuki, Masashi Sugiyama", "title": "Condition Number Analysis of Kernel-based Density Ratio Estimation", "comments": "37 pages, 1 figure, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ratio of two probability densities can be used for solving various\nmachine learning tasks such as covariate shift adaptation (importance\nsampling), outlier detection (likelihood-ratio test), and feature selection\n(mutual information). Recently, several methods of directly estimating the\ndensity ratio have been developed, e.g., kernel mean matching, maximum\nlikelihood density ratio estimation, and least-squares density ratio fitting.\nIn this paper, we consider a kernelized variant of the least-squares method and\ninvestigate its theoretical properties from the viewpoint of the condition\nnumber using smoothed analysis techniques--the condition number of the Hessian\nmatrix determines the convergence rate of optimization and the numerical\nstability. We show that the kernel least-squares method has a smaller condition\nnumber than a version of kernel mean matching and other M-estimators, implying\nthat the kernel least-squares method has preferable numerical properties. We\nfurther give an alternative formulation of the kernel least-squares estimator\nwhich is shown to possess an even smaller condition number. We show that\nnumerical studies meet our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2009 05:45:35 GMT"}], "update_date": "2009-12-16", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "0912.3211", "submitter": "Ilkka Huopaniemi", "authors": "Ilkka Huopaniemi, Tommi Suvitaival, Janne Nikkil\\\"a, Matej\n  Ore\\v{s}i\\v{c}, Samuel Kaski", "title": "Multi-Way, Multi-View Learning", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend multi-way, multivariate ANOVA-type analysis to cases where one\ncovariate is the view, with features of each view coming from different,\nhigh-dimensional domains. The different views are assumed to be connected by\nhaving paired samples; this is a common setup in recent bioinformatics\nexperiments, of which we analyze metabolite profiles in different conditions\n(disease vs. control and treatment vs. untreated) in different tissues (views).\nWe introduce a multi-way latent variable model for this new task, by extending\nthe generative model of Bayesian canonical correlation analysis (CCA) both to\ntake multi-way covariate information into account as population priors, and by\nreducing the dimensionality by an integrated factor analysis that assumes the\nmetabolites to come in correlated groups.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2009 17:51:39 GMT"}], "update_date": "2009-12-17", "authors_parsed": [["Huopaniemi", "Ilkka", ""], ["Suvitaival", "Tommi", ""], ["Nikkil\u00e4", "Janne", ""], ["Ore\u0161i\u010d", "Matej", ""], ["Kaski", "Samuel", ""]]}, {"id": "0912.3268", "submitter": "Mauricio A. \\'Alvarez", "authors": "Mauricio A. \\'Alvarez, David Luengo, Michalis K. Titsias, Neil D.\n  Lawrence", "title": "Variational Inducing Kernels for Sparse Convolved Multiple Output\n  Gaussian Processes", "comments": "Technical report, 22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in multioutput kernel methods is increasing, whether under the guise\nof multitask learning, multisensor networks or structured output data. From the\nGaussian process perspective a multioutput Mercer kernel is a covariance\nfunction over correlated output functions. One way of constructing such kernels\nis based on convolution processes (CP). A key problem for this approach is\nefficient inference. Alvarez and Lawrence (2009) recently presented a sparse\napproximation for CPs that enabled efficient inference. In this paper, we\nextend this work in two directions: we introduce the concept of variational\ninducing functions to handle potential non-smooth functions involved in the\nkernel CP construction and we consider an alternative approach to approximate\ninference based on variational methods, extending the work by Titsias (2009) to\nthe multiple output case. We demonstrate our approaches on prediction of school\nmarks, compiler performance and financial time series.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2009 21:26:00 GMT"}], "update_date": "2009-12-18", "authors_parsed": [["\u00c1lvarez", "Mauricio A.", ""], ["Luengo", "David", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "0912.3301", "submitter": "Mark Reid", "authors": "Mark D. Reid, Robert C. Williamson", "title": "Composite Binary Losses", "comments": "38 pages, 4 figures. Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study losses for binary classification and class probability estimation\nand extend the understanding of them from margin losses to general composite\nlosses which are the composition of a proper loss with a link function. We\ncharacterise when margin losses can be proper composite losses, explicitly show\nhow to determine a symmetric loss in full from half of one of its partial\nlosses, introduce an intrinsic parametrisation of composite binary losses and\ngive a complete characterisation of the relationship between proper losses and\n``classification calibrated'' losses. We also consider the question of the\n``best'' surrogate binary loss. We introduce a precise notion of ``best'' and\nshow there exist situations where two convex surrogate losses are\nincommensurable. We provide a complete explicit characterisation of the\nconvexity of composite binary losses in terms of the link function and the\nweight function associated with the proper loss which make up the composite\nloss. This characterisation suggests new ways of ``surrogate tuning''. Finally,\nin an appendix we present some new algorithm-independent results on the\nrelationship between properness, convexity and robustness to misclassification\nnoise for binary losses and show that all convex proper losses are non-robust\nto misclassification noise.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2009 01:34:31 GMT"}], "update_date": "2009-12-18", "authors_parsed": [["Reid", "Mark D.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "0912.3408", "submitter": "Markus Maier", "authors": "Markus Maier, Matthias Hein, Ulrike von Luxburg", "title": "Optimal construction of k-nearest neighbor graphs for identifying noisy\n  clusters", "comments": "31 pages, 2 figures", "journal-ref": "Theoretical Computer Science, 410(19), 1749-1764, April 2009", "doi": "10.1016/j.tcs.2009.01.009", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study clustering algorithms based on neighborhood graphs on a random\nsample of data points. The question we ask is how such a graph should be\nconstructed in order to obtain optimal clustering results. Which type of\nneighborhood graph should one choose, mutual k-nearest neighbor or symmetric\nk-nearest neighbor? What is the optimal parameter k? In our setting, clusters\nare defined as connected components of the t-level set of the underlying\nprobability distribution. Clusters are said to be identified in the\nneighborhood graph if connected components in the graph correspond to the true\nunderlying clusters. Using techniques from random geometric graph theory, we\nprove bounds on the probability that clusters are identified successfully, both\nin a noise-free and in a noisy setting. Those bounds lead to several\nconclusions. First, k has to be chosen surprisingly high (rather of the order n\nthan of the order log n) to maximize the probability of cluster identification.\nSecondly, the major difference between the mutual and the symmetric k-nearest\nneighbor graph occurs when one attempts to detect the most significant cluster\nonly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2009 15:12:40 GMT"}], "update_date": "2009-12-18", "authors_parsed": [["Maier", "Markus", ""], ["Hein", "Matthias", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "0912.3604", "submitter": "Gilles Stoltz", "authors": "Shie Mannor (EE-Technion), Gilles Stoltz (DMA, GREGH)", "title": "A Geometric Proof of Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide yet another proof of the existence of calibrated forecasters; it\nhas two merits. First, it is valid for an arbitrary finite number of outcomes.\nSecond, it is short and simple and it follows from a direct application of\nBlackwell's approachability theorem to carefully chosen vector-valued payoff\nfunction and convex target set. Our proof captures the essence of existing\nproofs based on approachability (e.g., the proof by Foster, 1999 in case of\nbinary outcomes) and highlights the intrinsic connection between\napproachability and calibration.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2009 08:06:06 GMT"}, {"version": "v2", "created": "Sun, 3 Oct 2010 18:23:08 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Mannor", "Shie", "", "EE-Technion"], ["Stoltz", "Gilles", "", "DMA, GREGH"]]}, {"id": "0912.3648", "submitter": "Sayan Mukherjee", "authors": "Sim\\'on Lunag\\'omez, Sayan Mukherjee, Robert L. Wolpert, Edoardo M.\n  Airoldi", "title": "Geometric Representations of Random Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parametrization of hypergraphs based on the geometry of points in\n$\\mathbf{R}^d$ is developed. Informative prior distributions on hypergraphs are\ninduced through this parametrization by priors on point configurations via\nspatial processes. This prior specification is used to infer conditional\nindependence models or Markov structure of multivariate distributions.\nSpecifically, we can recover both the junction tree factorization as well as\nthe hyper Markov law. This approach offers greater control on the distribution\nof graph features than Erd\\\"os-R\\'enyi random graphs, supports inference of\nfactorizations that cannot be retrieved by a graph alone, and leads to new\nMetropolis\\slash Hastings Markov chain Monte Carlo algorithms with both local\nand global moves in graph space. We illustrate the utility of this\nparametrization and prior specification using simulations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2009 11:30:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 06:38:50 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 18:53:00 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Lunag\u00f3mez", "Sim\u00f3n", ""], ["Mukherjee", "Sayan", ""], ["Wolpert", "Robert L.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "0912.4045", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou", "title": "Restricted Eigenvalue Conditions on Subgaussian Random Matrices", "comments": "24 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is natural to ask: what kinds of matrices satisfy the Restricted\nEigenvalue (RE) condition? In this paper, we associate the RE condition\n(Bickel-Ritov-Tsybakov 09) with the complexity of a subset of the sphere in\n$\\R^p$, where $p$ is the dimensionality of the data, and show that a class of\nrandom matrices with independent rows, but not necessarily independent columns,\nsatisfy the RE condition, when the sample size is above a certain lower bound.\nHere we explicitly introduce an additional covariance structure to the class of\nrandom matrices that we have known by now that satisfy the Restricted Isometry\nProperty as defined in Candes and Tao 05 (and hence the RE condition), in order\nto compose a broader class of random matrices for which the RE condition holds.\nIn this case, tools from geometric functional analysis in characterizing the\nintrinsic low-dimensional structures associated with the RE condition has been\ncrucial in analyzing the sample complexity and understanding its statistical\nimplications for high dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2009 12:08:11 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2009 01:07:52 GMT"}], "update_date": "2009-12-27", "authors_parsed": [["Zhou", "Shuheng", ""]]}, {"id": "0912.5410", "submitter": "Edoardo Airoldi", "authors": "Anna Goldenberg, Alice X Zheng, Stephen E Fienberg, Edoardo M Airoldi", "title": "A survey of statistical network models", "comments": "96 pages, 14 figures, 333 references", "journal-ref": "Foundations and Trends in Machine Learning, 2(2):1-117, 2009", "doi": null, "report-no": null, "categories": "stat.ME cs.LG physics.soc-ph q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in science and have become a focal point for\ndiscussion in everyday life. Formal statistical models for the analysis of\nnetwork data have emerged as a major topic of interest in diverse areas of\nstudy, and most of these involve a form of graphical representation.\nProbability models on graphs date back to 1959. Along with empirical studies in\nsocial psychology and sociology from the 1960s, these early works generated an\nactive network community and a substantial literature in the 1970s. This effort\nmoved into the statistical literature in the late 1970s and 1980s, and the past\ndecade has seen a burgeoning network literature in statistical physics and\ncomputer science. The growth of the World Wide Web and the emergence of online\nnetworking communities such as Facebook, MySpace, and LinkedIn, and a host of\nmore specialized professional network communities has intensified interest in\nthe study of networks and network data. Our goal in this review is to provide\nthe reader with an entry point to this burgeoning literature. We begin with an\noverview of the historical development of statistical network modeling and then\nwe introduce a number of examples that have been studied in the network\nliterature. Our subsequent discussion focuses on a number of prominent static\nand dynamic network models and their interconnections. We emphasize formal\nmodel descriptions, and pay special attention to the interpretation of\nparameters and their estimation. We end with a description of some open\nproblems and challenges for machine learning and statistics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 17:53:13 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Goldenberg", "Anna", ""], ["Zheng", "Alice X", ""], ["Fienberg", "Stephen E", ""], ["Airoldi", "Edoardo M", ""]]}, {"id": "0912.5507", "submitter": "Jun Zhu", "authors": "Jun Zhu, Amr Ahmed, Eric P. Xing", "title": "MedLDA: A General Framework of Maximum Margin Supervised Topic Models", "comments": "27 Pages", "journal-ref": "Journal of Machine Learning Research, 13(Aug): 2237--2278, 2012", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models utilize document's side information for discovering\npredictive low dimensional representations of documents. Existing models apply\nthe likelihood-based estimation. In this paper, we present a general framework\nof max-margin supervised topic models for both continuous and categorical\nresponse variables. Our approach, the maximum entropy discrimination latent\nDirichlet allocation (MedLDA), utilizes the max-margin principle to train\nsupervised topic models and estimate predictive topic representations that are\narguably more suitable for prediction tasks. The general principle of MedLDA\ncan be applied to perform joint max-margin learning and maximum likelihood\nestimation for arbitrary topic models, directed or undirected, and supervised\nor unsupervised, when the supervised side information is available. We develop\nefficient variational methods for posterior inference and parameter estimation,\nand demonstrate qualitatively and quantitatively the advantages of MedLDA over\nlikelihood-based topic models on movie review and 20 Newsgroups data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 18:32:21 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Zhu", "Jun", ""], ["Ahmed", "Amr", ""], ["Xing", "Eric P.", ""]]}]