[{"id": "1603.00106", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein,\n  and Naren Ramakrishnan", "title": "Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach", "comments": "this paper has been submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 00:45:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 20:45:50 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Cohn", "Emily", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1603.00284", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Stephen Becker", "title": "Dual Smoothing and Level Set Techniques for Variational Matrix\n  Decomposition", "comments": "38 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1406.1089", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the robust principal component analysis (RPCA) problem, and\nreview a range of old and new convex formulations for the problem and its\nvariants. We then review dual smoothing and level set techniques in convex\noptimization, present several novel theoretical results, and apply the\ntechniques on the RPCA problem. In the final sections, we show a range of\nnumerical experiments for simulated and real-world problems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 14:33:12 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Becker", "Stephen", ""]]}, {"id": "1603.00285", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Peter B\\\"uhlmann, Bernhard Sch\\\"olkopf, and Jonas\n  Peters", "title": "Kernel-based Tests for Joint Independence", "comments": "67 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of testing whether $d$ random variables, which may\nor may not be continuous, are jointly (or mutually) independent. Our method\nbuilds on ideas of the two variable Hilbert-Schmidt independence criterion\n(HSIC) but allows for an arbitrary number of variables. We embed the\n$d$-dimensional joint distribution and the product of the marginals into a\nreproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt\nindependence criterion (dHSIC) as the squared distance between the embeddings.\nIn the population case, the value of dHSIC is zero if and only if the $d$\nvariables are jointly independent, as long as the kernel is characteristic.\nBased on an empirical estimate of dHSIC, we define three different\nnon-parametric hypothesis tests: a permutation test, a bootstrap test and a\ntest based on a Gamma approximation. We prove that the permutation test\nachieves the significance level and that the bootstrap test achieves pointwise\nasymptotic significance level as well as pointwise asymptotic consistency\n(i.e., it is able to detect any type of fixed dependence in the large sample\nlimit). The Gamma approximation does not come with these guarantees; however,\nit is computationally very fast and for small $d$, it performs well in\npractice. Finally, we apply the test to a problem in causal discovery.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 14:34:53 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 12:06:39 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 10:20:59 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Pfister", "Niklas", ""], ["B\u00fchlmann", "Peter", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Peters", "Jonas", ""]]}, {"id": "1603.00389", "submitter": "Matthias Poloczek", "authors": "Matthias Poloczek, Jialei Wang, and Peter I. Frazier", "title": "Multi-Information Source Optimization", "comments": "Added: benchmark logistic regression on MNIST/USPS, comparison to\n  MTBO/entropy search, estimation of hyper-parameters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian optimization of an expensive-to-evaluate black-box\nobjective function, where we also have access to cheaper approximations of the\nobjective. In general, such approximations arise in applications such as\nreinforcement learning, engineering, and the natural sciences, and are subject\nto an inherent, unknown bias. This model discrepancy is caused by an inadequate\ninternal model that deviates from reality and can vary over the domain, making\nthe utilization of these approximations a non-trivial task.\n  We present a novel algorithm that provides a rigorous mathematical treatment\nof the uncertainties arising from model discrepancies and noisy observations.\nIts optimization decisions rely on a value of information analysis that extends\nthe Knowledge Gradient factor to the setting of multiple information sources\nthat vary in cost: each sampling decision maximizes the predicted benefit per\nunit cost.\n  We conduct an experimental evaluation that demonstrates that the method\nconsistently outperforms other state-of-the-art techniques: it finds designs of\nconsiderably higher objective value and additionally inflicts less cost in the\nexploration process.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:28:04 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 00:22:21 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Poloczek", "Matthias", ""], ["Wang", "Jialei", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1603.00391", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski, Misha Denil and Yoshua Bengio", "title": "Noisy Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Common nonlinear activation functions used in neural networks can cause\ntraining difficulties due to the saturation behavior of the activation\nfunction, which may hide dependencies that are not visible to vanilla-SGD\n(using first order gradients only). Gating mechanisms that use softly\nsaturating activation functions to emulate the discrete switching of digital\nlogic circuits are good examples of this. We propose to exploit the injection\nof appropriate noise so that the gradients may flow easily, even if the\nnoiseless application of the activation function would yield zero gradient.\nLarge noise will dominate the noise-free gradient and allow stochastic gradient\ndescent toexplore more. By adding noise only to the problematic parts of the\nactivation function, we allow the optimization procedure to explore the\nboundary between the degenerate (saturating) and the well-behaved parts of the\nactivation function. We also establish connections to simulated annealing, when\nthe amount of noise is annealed down, making it easier to optimize hard\nobjective functions. We find experimentally that replacing such saturating\nactivation functions by noisy variants helps training in many contexts,\nyielding state-of-the-art or competitive results on different datasets and\ntask, especially when training seems to be the most difficult, e.g., when\ncurriculum learning is necessary to obtain good results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:30:15 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 20:51:57 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2016 21:41:47 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.00531", "submitter": "Kui Yu", "authors": "Kui Yu, Wei Ding, Xindong Wu", "title": "LOFS: Library of Online Streaming Feature Selection", "comments": null, "journal-ref": "Knowledge-based Systems, 113(2016):1-3", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging research direction, online streaming feature selection deals\nwith sequentially added dimensions in a feature space while the number of data\ninstances is fixed. Online streaming feature selection provides a new,\ncomplementary algorithmic methodology to enrich online feature selection,\nespecially targets to high dimensionality in big data analytics. This paper\nintroduces the first comprehensive open-source library for use in MATLAB that\nimplements the state-of-the-art algorithms of online streaming feature\nselection. The library is designed to facilitate the development of new\nalgorithms in this exciting research direction and make comparisons between the\nnew methods and existing ones available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 00:21:00 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Yu", "Kui", ""], ["Ding", "Wei", ""], ["Wu", "Xindong", ""]]}, {"id": "1603.00564", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright,\n  Michael I. Jordan", "title": "Asymptotic behavior of $\\ell_p$-based Laplacian regularization in\n  semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted graph with $N$ vertices, consider a real-valued regression\nproblem in a semi-supervised setting, where one observes $n$ labeled vertices,\nand the task is to label the remaining ones. We present a theoretical study of\n$\\ell_p$-based Laplacian regularization under a $d$-dimensional geometric\nrandom graph model. We provide a variational characterization of the\nperformance of this regularized learner as $N$ grows to infinity while $n$\nstays constant, the associated optimality conditions lead to a partial\ndifferential equation that must be satisfied by the associated function\nestimate $\\hat{f}$. From this formulation we derive several predictions on the\nlimiting behavior the $d$-dimensional function $\\hat{f}$, including (a) a phase\ntransition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoff\nbetween smoothness and sensitivity to the underlying unlabeled data\ndistribution $P$. Thus, over the range $p \\leq d$, the function estimate\n$\\hat{f}$ is degenerate and \"spiky,\" whereas for $p\\geq d+1$, the function\nestimate $\\hat{f}$ is smooth. We show that the effect of the underlying density\nvanishes monotonically with $p$, such that in the limit $p = \\infty$,\ncorresponding to the so-called Absolutely Minimal Lipschitz Extension, the\nestimate $\\hat{f}$ is independent of the distribution $P$. Under the assumption\nof semi-supervised smoothness, ignoring $P$ can lead to poor statistical\nperformance, in particular, we construct a specific example for $d=1$ to\ndemonstrate that $p=2$ has lower risk than $p=\\infty$ due to the former penalty\nadapting to $P$ and the latter ignoring it. We also provide simulations that\nverify the accuracy of our predictions for finite sample sizes. Together, these\nproperties show that $p = d+1$ is an optimal choice, yielding a function\nestimate $\\hat{f}$ that is both smooth and non-degenerate, while remaining\nmaximally sensitive to $P$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 03:31:28 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Cheng", "Xiang", ""], ["Ramdas", "Aaditya", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1603.00570", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Without-Replacement Sampling for Stochastic Gradient Methods:\n  Convergence Results and Application to Distributed Optimization", "comments": "Fixed a few minor typos, and slightly tightened Corollary 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient methods for machine learning and optimization problems\nare usually analyzed assuming data points are sampled \\emph{with} replacement.\nIn practice, however, sampling \\emph{without} replacement is very common,\neasier to implement in many cases, and often performs better. In this paper, we\nprovide competitive convergence guarantees for without-replacement sampling,\nunder various scenarios, for three types of algorithms: Any algorithm with\nonline regret guarantees, stochastic gradient descent, and SVRG. A useful\napplication of our SVRG analysis is a nearly-optimal algorithm for regularized\nleast squares in a distributed setting, in terms of both communication\ncomplexity and runtime complexity, when the data is randomly partitioned and\nthe condition number can be as large as the data size per machine (up to\nlogarithmic factors). Our proof techniques combine ideas from stochastic\noptimization, adversarial online learning, and transductive learning theory,\nand can potentially be applied to other stochastic optimization and learning\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 04:02:57 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 00:29:34 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 03:58:41 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1603.00788", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n  Blei", "title": "Automatic Differentiation Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is iterative. A scientist posits a simple model, fits\nit to her data, refines it according to her analysis, and repeats. However,\nfitting complex models to large data is a bottleneck in this process. Deriving\nalgorithms for new models can be both mathematically and computationally\nchallenging, which makes it difficult to efficiently cycle through the steps.\nTo this end, we develop automatic differentiation variational inference (ADVI).\nUsing our method, the scientist only provides a probabilistic model and a\ndataset, nothing else. ADVI automatically derives an efficient variational\ninference algorithm, freeing the scientist to refine and explore many models.\nADVI supports a broad class of models-no conjugacy assumptions are required. We\nstudy ADVI across ten different models and apply it to a dataset with millions\nof observations. ADVI is integrated into Stan, a probabilistic programming\nsystem; it is available for immediate use.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:43:15 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Gelman", "Andrew", ""], ["Blei", "David M.", ""]]}, {"id": "1603.00810", "submitter": "Marta R. Costa-Juss\\`a", "authors": "Marta R. Costa-Juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-based Neural Machine Translation", "comments": "Accepted for publication at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:01:57 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 14:02:48 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 10:28:36 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Costa-Juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1603.00856", "submitter": "Steven Kearnes", "authors": "Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick\n  Riley", "title": "Molecular Graph Convolutions: Moving Beyond Fingerprints", "comments": "See \"Version information\" section", "journal-ref": "J Comput Aided Mol Des (2016)", "doi": "10.1007/s10822-016-9938-8", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular \"fingerprints\" encoding structural information are the workhorse of\ncheminformatics and machine learning in drug discovery applications. However,\nfingerprint representations necessarily emphasize particular aspects of the\nmolecular structure while ignoring others, rather than allowing the model to\nmake data-driven decisions. We describe molecular \"graph convolutions\", a\nmachine learning architecture for learning from undirected graphs, specifically\nsmall molecules. Graph convolutions use a simple encoding of the molecular\ngraph---atoms, bonds, distances, etc.---which allows the model to take greater\nadvantage of information in the graph structure. Although graph convolutions do\nnot outperform all fingerprint-based methods, they (along with other\ngraph-based methods) represent a new paradigm in ligand-based virtual screening\nwith exciting opportunities for future improvement.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 20:34:08 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 22:26:57 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 17:17:05 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Kearnes", "Steven", ""], ["McCloskey", "Kevin", ""], ["Berndl", "Marc", ""], ["Pande", "Vijay", ""], ["Riley", "Patrick", ""]]}, {"id": "1603.00929", "submitter": "Paul Rubenstein", "authors": "Paul K. Rubenstein, Kacper P. Chwialkowski and Arthur Gretton", "title": "A Kernel Test for Three-Variable Interactions with Random Processes", "comments": "15 pages including 5 pages of supplementary material, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a wild bootstrap method to the Lancaster three-variable interaction\nmeasure in order to detect factorisation of the joint distribution on three\nvariables forming a stationary random process, for which the existing\npermutation bootstrap method fails. As in the i.i.d. case, the Lancaster test\nis found to outperform existing tests in cases for which two independent\nvariables individually have a weak influence on a third, but that when\nconsidered jointly the influence is strong. The main contributions of this\npaper are twofold: first, we prove that the Lancaster statistic satisfies the\nconditions required to estimate the quantiles of the null distribution using\nthe wild bootstrap; second, the manner in which this is proved is novel,\nsimpler than existing methods, and can further be applied to other statistics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 23:38:11 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 23:34:57 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Chwialkowski", "Kacper P.", ""], ["Gretton", "Arthur", ""]]}, {"id": "1603.00952", "submitter": "Nicola Bulso", "authors": "Nicola Bulso, Matteo Marsili, Yasser Roudi", "title": "Sparse model selection in the highly under-sampled regime", "comments": "54 pages, 26 figures", "journal-ref": "J. Stat. Mech. (2016) 093404", "doi": "10.1088/1742-5468/2016/09/093404", "report-no": null, "categories": "stat.ML cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for recovering the structure of a sparse undirected\ngraphical model when very few samples are available. The method decides about\nthe presence or absence of bonds between pairs of variable by considering one\npair at a time and using a closed form formula, analytically derived by\ncalculating the posterior probability for every possible model explaining a two\nbody system using Jeffreys prior. The approach does not rely on the\noptimisation of any cost functions and consequently is much faster than\nexisting algorithms. Despite this time and computational advantage, numerical\nresults show that for several sparse topologies the algorithm is comparable to\nthe best existing algorithms, and is more accurate in the presence of hidden\nvariables. We apply this approach to the analysis of US stock market data and\nto neural data, in order to show its efficiency in recovering robust\nstatistical dependencies in real data with non stationary correlations in time\nand space.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 02:56:16 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 18:12:09 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Bulso", "Nicola", ""], ["Marsili", "Matteo", ""], ["Roudi", "Yasser", ""]]}, {"id": "1603.00954", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Training Input-Output Recurrent Neural Networks through Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training input-output recurrent neural networks\n(RNN) for sequence labeling tasks. We propose a novel spectral approach for\nlearning the network parameters. It is based on decomposition of the\ncross-moment tensor between the output and a non-linear transformation of the\ninput, based on score functions. We guarantee consistent learning with\npolynomial sample and computational complexity under transparent conditions\nsuch as non-degeneracy of model parameters, polynomial activations for the\nneurons, and a Markovian evolution of the input sequence. We also extend our\nresults to Bidirectional RNN which uses both previous and future information to\noutput the label at each time point, and is employed in many NLP tasks such as\nPOS tagging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:14:47 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 03:46:12 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 07:03:46 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 19:52:21 GMT"}, {"version": "v5", "created": "Mon, 31 Oct 2016 18:30:51 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1603.01029", "submitter": "Hiroaki Shiino", "authors": "Hiroaki Shiino, Hiroaki Sasaki, Gang Niu and Masashi Sugiyama", "title": "Whitening-Free Least-Squares Non-Gaussian Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian component analysis (NGCA) is an unsupervised linear dimension\nreduction method that extracts low-dimensional non-Gaussian \"signals\" from\nhigh-dimensional data contaminated with Gaussian noise. NGCA can be regarded as\na generalization of projection pursuit (PP) and independent component analysis\n(ICA) to multi-dimensional and dependent non-Gaussian components. Indeed,\nseminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCA\napproach called least-squares NGCA (LSNGCA) has been developed, which gives a\nsolution analytically through least-squares estimation of log-density gradients\nand eigendecomposition. However, since pre-whitening of data is involved in\nLSNGCA, it performs unreliably when the data covariance matrix is\nill-conditioned, which is often the case in high-dimensional data analysis. In\nthis paper, we propose a whitening-free LSNGCA method and experimentally\ndemonstrate its superiority.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 08:56:50 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 17:50:43 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Shiino", "Hiroaki", ""], ["Sasaki", "Hiroaki", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1603.01140", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz, Michalis K. Titsias, David M. Blei", "title": "Overdispersed Black-Box Variational Inference", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce overdispersed black-box variational inference, a method to\nreduce the variance of the Monte Carlo estimator of the gradient in black-box\nvariational inference. Instead of taking samples from the variational\ndistribution, we use importance sampling to take samples from an overdispersed\ndistribution in the same exponential family as the variational approximation.\nOur approach is general since it can be readily applied to any exponential\nfamily distribution, which is the typical choice for the variational\napproximation. We run experiments on two non-conjugate probabilistic models to\nshow that our method effectively reduces the variance, and the overhead\nintroduced by the computation of the proposal parameters and the importance\nweights is negligible. We find that our overdispersed importance sampling\nscheme provides lower variance than black-box variational inference, even when\nthe latter uses twice the number of samples. This results in faster convergence\nof the black-box inference procedure.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 15:48:47 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Titsias", "Michalis K.", ""], ["Blei", "David M.", ""]]}, {"id": "1603.01354", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "comments": "10 pages, 3 figures. To appear on ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 05:55:02 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 06:19:37 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 05:16:17 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 21:46:13 GMT"}, {"version": "v5", "created": "Sun, 29 May 2016 00:42:15 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1603.01359", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Learning deep representation of multityped objects and tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep multitask architecture to integrate multityped\nrepresentations of multimodal objects. This multitype exposition is less\nabstract than the multimodal characterization, but more machine-friendly, and\nthus is more precise to model. For example, an image can be described by\nmultiple visual views, which can be in the forms of bag-of-words (counts) or\ncolor/texture histograms (real-valued). At the same time, the image may have\nseveral social tags, which are best described using a sparse binary vector. Our\ndeep model takes as input multiple type-specific features, narrows the\ncross-modality semantic gaps, learns cross-type correlation, and produces a\nhigh-level homogeneous representation. At the same time, the model supports\nheterogeneously typed tasks. We demonstrate the capacity of the model on two\napplications: social image retrieval and multiple concept prediction. The deep\narchitecture produces more compact representation, naturally integrates\nmultiviews and multimodalities, exploits better side information, and most\nimportantly, performs competitively against baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 06:34:24 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1603.01374", "submitter": "Sarathkrishna Swaminathan", "authors": "John Moeller, Sarathkrishna Swaminathan, Suresh Venkatasubramanian", "title": "A Unified View of Localized Kernel Learning", "comments": "14 pages, 2 figures, 4 tables. Reformatted version of the accepted\n  SDM 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to\nlearn not only a classifier/regressor but also the best kernel for the training\ntask, usually from a combination of existing kernel functions. Most MKL methods\nseek the combined kernel that performs best over every training example,\nsacrificing performance in some areas to seek a global optimum. Localized\nkernel learning (LKL) overcomes this limitation by allowing the training\nalgorithm to match a component kernel to the examples that can exploit it best.\nSeveral approaches to the localized kernel learning problem have been explored\nin the last several years. We unify many of these approaches under one simple\nsystem and design a new algorithm with improved performance. We also develop\nenhanced versions of existing algorithms, with an eye on scalability and\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 08:29:03 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Moeller", "John", ""], ["Swaminathan", "Sarathkrishna", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1603.01376", "submitter": "Florian Ziel", "authors": "Florian Ziel and Bidong Liu", "title": "Lasso estimation for GEFCom2014 probabilistic electric load forecasting", "comments": "appears in the special issue Probabilistic Energy Forecasting of the\n  International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 32.3 (2016) 1029-1037", "doi": "10.1016/j.ijforecast.2016.01.001", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology for probabilistic load forecasting that is based on\nlasso (least absolute shrinkage and selection operator) estimation. The model\nconsidered can be regarded as a bivariate time-varying threshold\nautoregressive(AR) process for the hourly electric load and temperature. The\njoint modeling approach incorporates the temperature effects directly, and\nreflects daily, weekly, and annual seasonal patterns and public holiday\neffects. We provide two empirical studies, one based on the probabilistic load\nforecasting track of the Global Energy Forecasting Competition 2014\n(GEFCom2014-L), and the other based on another recent probabilistic load\nforecasting competition that follows a setup similar to that of GEFCom2014-L.\nIn both empirical case studies, the proposed methodology outperforms two\nmultiple linear regression based benchmarks from among the top eight entries to\nGEFCom2014-L.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 08:39:22 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Ziel", "Florian", ""], ["Liu", "Bidong", ""]]}, {"id": "1603.01431", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju", "title": "Normalization Propagation: A Parametric Technique for Removing Internal\n  Covariate Shift in Deep Networks", "comments": "11 pages, ICML 2016, appendix added to the last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- Internal Covariate\nShift-- the current solution has certain drawbacks. Specifically, BN depends on\nbatch statistics for layerwise input normalization during training which makes\nthe estimates of mean and standard deviation of input (distribution) to hidden\nlayers inaccurate for validation due to shifting parameter values (especially\nduring initial training epochs). Also, BN cannot be used with batch-size 1\nduring training. We address these drawbacks by proposing a non-adaptive\nnormalization technique for removing internal covariate shift, that we call\nNormalization Propagation. Our approach does not depend on batch statistics,\nbut rather uses a data-independent parametric estimate of mean and\nstandard-deviation in every layer thus being computationally faster compared\nwith BN. We exploit the observation that the pre-activation before Rectified\nLinear Units follow Gaussian distribution in deep networks, and that once the\nfirst and second order statistics of any given dataset are normalized, we can\nforward propagate this normalization without the need for recalculating the\napproximate statistics for hidden layers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 12:01:58 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 16:41:25 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 23:01:55 GMT"}, {"version": "v4", "created": "Mon, 30 May 2016 02:08:06 GMT"}, {"version": "v5", "created": "Sun, 3 Jul 2016 20:17:44 GMT"}, {"version": "v6", "created": "Tue, 12 Jul 2016 13:57:19 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Kota", "Bhargava U.", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1603.01566", "submitter": "Konstantin Usevich", "authors": "Pierre Comon, Yang Qi, Konstantin Usevich", "title": "Identifiability of an X-rank decomposition of polynomial maps", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a polynomial decomposition model that arises in\nproblems of system identification, signal processing and machine learning. We\nshow that this decomposition is a special case of the X-rank decomposition ---\na powerful novel concept in algebraic geometry that generalizes the tensor CP\ndecomposition. We prove new results on generic/maximal rank and on\nidentifiability of a particular polynomial decomposition model. In the paper,\nwe try to make results and basic tools accessible for general audience\n(assuming no knowledge of algebraic geometry or its prerequisites).\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 18:50:40 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 20:29:09 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 20:20:21 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Comon", "Pierre", ""], ["Qi", "Yang", ""], ["Usevich", "Konstantin", ""]]}, {"id": "1603.01597", "submitter": "Jeroen De Gussem", "authors": "Mike Kestemont, Jeroen De Gussem", "title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation\n  Learning", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a\n  Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts\n  and Computing Text Similarities (August 6, 2017) jdmdh:3835", "doi": "10.46298/jdmdh.1398", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two sequence tagging tasks for medieval Latin:\npart-of-speech tagging and lemmatization. These are both basic, yet\nfoundational preprocessing steps in applications such as text re-use detection.\nNevertheless, they are generally complicated by the considerable orthographic\nvariation which is typical of medieval Latin. In Digital Classics, these tasks\nare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.\nFor example, a lexicon is used to generate all the potential lemma-tag pairs\nfor a token, and next, a context-aware PoS-tagger is used to select the most\nappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,\nerror percolation is a major downside of such approaches. In this paper we\nexplore the possibility to elegantly solve these tasks using a single,\nintegrated approach. For this, we make use of a layered neural network\narchitecture from the field of deep representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:13:56 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 08:18:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kestemont", "Mike", ""], ["De Gussem", "Jeroen", ""]]}, {"id": "1603.01681", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh, Anastasios Kyrillidis and Volkan Cevher", "title": "A single-phase, proximal path-following framework", "comments": "26 pages, 2 figures, 4 tables (This is the first revision. The\n  original one was uploaded on arxiv on March 5, 2016", "journal-ref": null, "doi": null, "report-no": "90C06, 90C25, 90-08", "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new proximal, path-following framework for a class of\nconstrained convex problems. We consider settings where the nonlinear---and\npossibly non-smooth---objective part is endowed with a proximity operator, and\nthe constraint set is equipped with a self-concordant barrier. Our approach\nrelies on the following two main ideas. First, we re-parameterize the\noptimality condition as an auxiliary problem, such that a good initial point is\navailable; by doing so, a family of alternative paths towards the optimum is\ngenerated. Second, we combine the proximal operator with path-following ideas\nto design a single-phase, proximal, path-following algorithm. Our method has\nseveral advantages. First, it allows handling non-smooth objectives via\nproximal operators; this avoids lifting the problem dimension in order to\naccommodate non-smooth components in optimization. Second, it consists of only\na \\emph{single phase}: While the overall convergence rate of classical\npath-following schemes for self-concordant objectives does not suffer from the\ninitialization phase, proximal path-following schemes undergo slow convergence,\nin order to obtain a good starting point \\cite{TranDinh2013e}. In this work, we\nshow how to overcome this limitation in the proximal setting and prove that our\nscheme has the same $\\mathcal{O}(\\sqrt{\\nu}\\log(1/\\varepsilon))$ worst-case\niteration-complexity with standard approaches \\cite{Nesterov2004,Nesterov1994}\nwithout requiring an initial phase, where $\\nu$ is the barrier parameter and\n$\\varepsilon$ is a desired accuracy. Finally, our framework allows errors in\nthe calculation of proximal-Newton directions, without sacrificing the\nworst-case iteration complexity. We demonstrate the merits of our algorithm via\nthree numerical examples, where proximal operators play a key role.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 05:18:06 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 05:43:13 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Kyrillidis", "Anastasios", ""], ["Cevher", "Volkan", ""]]}, {"id": "1603.01700", "submitter": "Martin Spindler", "authors": "Victor Chernozhukov and Chris Hansen and Martin Spindler", "title": "High-Dimensional Metrics in R", "comments": "34 pages; vignette for the R package hdm, available at\n  http://cran.r-project.org/web/packages/hdm/ and\n  http://r-forge.r-project.org/R/?group_id=2084 (development version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The package High-dimensional Metrics (\\Rpackage{hdm}) is an evolving\ncollection of statistical methods for estimation and quantification of\nuncertainty in high-dimensional approximately sparse models. It focuses on\nproviding confidence intervals and significance testing for (possibly many)\nlow-dimensional subcomponents of the high-dimensional parameter vector.\nEfficient estimators and uniformly valid confidence intervals for regression\ncoefficients on target variables (e.g., treatment or policy variable) in a\nhigh-dimensional approximately sparse regression model, for average treatment\neffect (ATE) and average treatment effect for the treated (ATET), as well for\nextensions of these parameters to the endogenous setting are provided. Theory\ngrounded, data-driven methods for selecting the penalization parameter in Lasso\nregressions under heteroscedastic and non-Gaussian errors are implemented.\nMoreover, joint/ simultaneous confidence intervals for regression coefficients\nof a high-dimensional sparse regression are implemented, including a joint\nsignificance test for Lasso regression. Data sets which have been used in the\nliterature and might be useful for classroom demonstration and for testing new\nestimators are included. \\R and the package \\Rpackage{hdm} are open-source\nsoftware projects and can be freely downloaded from CRAN:\n\\texttt{http://cran.r-project.org}.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 08:57:26 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 12:20:09 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Chris", ""], ["Spindler", "Martin", ""]]}, {"id": "1603.01801", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Variational methods for Conditional Multimodal Deep Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of conditional modality learning,\nwhereby one is interested in generating one modality given the other. While it\nis straightforward to learn a joint distribution over multiple modalities using\na deep multimodal architecture, we observe that such models aren't very\neffective at conditional generation. Hence, we address the problem by learning\nconditional distributions between the modalities. We use variational methods\nfor maximizing the corresponding conditional log-likelihood. The resultant deep\nmodel, which we refer to as conditional multimodal autoencoder (CMMA), forces\nthe latent representation obtained from a single modality alone to be `close'\nto the joint representation obtained from multiple modalities. We use the\nproposed model to generate faces from attributes. We show that the faces\ngenerated from attributes using the proposed model, are qualitatively and\nquantitatively more representative of the attributes from which they were\ngenerated, than those obtained by other deep generative models. We also propose\na secondary task, whereby the existing faces are modified by modifying the\ncorresponding attributes. We observe that the modifications in face introduced\nby the proposed model are representative of the corresponding modifications in\nattributes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 07:33:03 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 10:57:41 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1603.01857", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok", "title": "Classical Statistics and Statistical Learning in Imaging Neuroscience", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging research has predominantly drawn conclusions based on classical\nstatistics, including null-hypothesis testing, t-tests, and ANOVA. Throughout\nrecent years, statistical learning methods enjoy increasing popularity,\nincluding cross-validation, pattern classification, and sparsity-inducing\nregression. These two methodological families used for neuroimaging data\nanalysis can be viewed as two extremes of a continuum. Yet, they originated\nfrom different historical contexts, build on different theories, rest on\ndifferent assumptions, evaluate different outcome metrics, and permit different\nconclusions. This paper portrays commonalities and differences between\nclassical statistics and statistical learning with their relation to\nneuroimaging research. The conceptual implications are illustrated in three\ncommon analysis scenarios. It is thus tried to resolve possible confusion\nbetween classical hypothesis testing and data-guided model estimation by\ndiscussing their ramifications for the neuroimaging access to neurobiology.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 18:56:08 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 12:09:01 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Bzdok", "Danilo", ""]]}, {"id": "1603.01882", "submitter": "Robert Zinkov", "authors": "Robert Zinkov, Chung-chieh Shan", "title": "Composing inference algorithms as program transformations", "comments": "10 pages, 5 figures. To appear in Proceedings of the 33rd Conference\n  on Uncertainty in Artificial Intelligence (UAI2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference procedures are usually coded painstakingly from\nscratch, for each target model and each inference algorithm. We reduce this\neffort by generating inference procedures from models automatically. We make\nthis code generation modular by decomposing inference algorithms into reusable\nprogram-to-program transformations. These transformations perform exact\ninference as well as generate probabilistic programs that compute expectations,\ndensities, and MCMC samples. The resulting inference procedures are about as\naccurate and fast as other probabilistic programming systems on real-world\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 21:30:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:01:42 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zinkov", "Robert", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "1603.01901", "submitter": "Behrouz Behmardi", "authors": "Behrouz Behmardi, Forrest Briggs, Xiaoli Z. Fern, and Raviv Raich", "title": "Confidence-Constrained Maximum Entropy Framework for Learning from\n  Multi-Instance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance data, in which each object (bag) contains a collection of\ninstances, are widespread in machine learning, computer vision, bioinformatics,\nsignal processing, and social sciences. We present a maximum entropy (ME)\nframework for learning from multi-instance data. In this approach each bag is\nrepresented as a distribution using the principle of ME. We introduce the\nconcept of confidence-constrained ME (CME) to simultaneously learn the\nstructure of distribution space and infer each distribution. The shared\nstructure underlying each density is used to learn from instances inside each\nbag. The proposed CME is free of tuning parameters. We devise a fast\noptimization algorithm capable of handling large scale multi-instance data. In\nthe experimental section, we evaluate the performance of the proposed approach\nin terms of exact rank recovery in the space of distributions and compare it\nwith the regularized ME approach. Moreover, we compare the performance of CME\nwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show a\ncomparable performance in terms of accuracy with reduced computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 00:30:10 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Behmardi", "Behrouz", ""], ["Briggs", "Forrest", ""], ["Fern", "Xiaoli Z.", ""], ["Raich", "Raviv", ""]]}, {"id": "1603.01912", "submitter": "David Carlson", "authors": "David Carlson, Patrick Stinson, Ari Pakman, Liam Paninski", "title": "Partition Functions from Rao-Blackwellized Tempered Sampling", "comments": "15 pages, 8 figures; Appearing at International Conference on Machine\n  Learning 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition functions of probability distributions are important quantities for\nmodel evaluation and comparisons. We present a new method to compute partition\nfunctions of complex and multimodal distributions. Such distributions are often\nsampled using simulated tempering, which augments the target space with an\nauxiliary inverse temperature variable. Our method exploits the multinomial\nprobability law of the inverse temperatures, and provides estimates of the\npartition function in terms of a simple quotient of Rao-Blackwellized marginal\ninverse temperature probability estimates, which are updated while sampling. We\nshow that the method has interesting connections with several alternative\npopular methods, and offers some significant advantages. In particular, we\nempirically find that the new method provides more accurate estimates than\nAnnealed Importance Sampling when calculating partition functions of large\nRestricted Boltzmann Machines (RBM); moreover, the method is sufficiently\naccurate to track training and validation log-likelihoods during learning of\nRBMs, at minimal computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 01:42:04 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 19:50:04 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 20:57:57 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Carlson", "David", ""], ["Stinson", "Patrick", ""], ["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1603.01913", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Gholamreza Haffari and Jacob Eisenstein", "title": "A Latent Variable Recurrent Neural Network for Discourse Relation\n  Language Models", "comments": "NAACL 2016 camera ready, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel latent variable recurrent neural network\narchitecture for jointly modeling sequences of words and (possibly latent)\ndiscourse relations between adjacent sentences. A recurrent neural network\ngenerates individual words, thus reaping the benefits of\ndiscriminatively-trained vector representations. The discourse relations are\nrepresented with a latent variable, which can be predicted or marginalized,\ndepending on the task. The resulting model can therefore employ a training\nobjective that includes not only discourse relation classification, but also\nword prediction. As a result, it outperforms state-of-the-art alternatives for\ntwo tasks: implicit discourse relation classification in the Penn Discourse\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\nby marginalizing over latent discourse relations at test time, we obtain a\ndiscourse informed language model, which improves over a strong LSTM baseline.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 01:54:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:58:10 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Ji", "Yangfeng", ""], ["Haffari", "Gholamreza", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.02010", "submitter": "Borja Balle", "authors": "Borja Balle, Maziar Gomrokchi, Doina Precup", "title": "Differentially Private Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first differentially private algorithms for reinforcement\nlearning, which apply to the task of evaluating a fixed policy. We establish\ntwo approaches for achieving differential privacy, provide a theoretical\nanalysis of the privacy and utility of the two algorithms, and show promising\nresults on simple empirical examples.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 11:23:57 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Balle", "Borja", ""], ["Gomrokchi", "Maziar", ""], ["Precup", "Doina", ""]]}, {"id": "1603.02074", "submitter": "Mohammed Rayyan Sheriff", "authors": "Mohammed Rayyan Sheriff and Debasish Chatterjee", "title": "Optimal dictionary for least squares representation", "comments": "24 pages", "journal-ref": "Journal of Machine Learning Research, Vol. 18, Paper No. 107, pp.\n  1-28, 2017", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries are collections of vectors used for representations of random\nvectors in Euclidean spaces. Recent research on optimal dictionaries is focused\non constructing dictionaries that offer sparse representations, i.e.,\n$\\ell_0$-optimal representations. Here we consider the problem of finding\noptimal dictionaries with which representations of samples of a random vector\nare optimal in an $\\ell_2$-sense: optimality of representation is defined as\nattaining the minimal average $\\ell_2$-norm of the coefficients used to\nrepresent the random vector. With the help of recent results on rank-$1$\ndecompositions of symmetric positive semidefinite matrices, we provide an\nexplicit description of $\\ell_2$-optimal dictionaries as well as their\nalgorithmic constructions in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 14:13:24 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 14:44:44 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 15:49:49 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sheriff", "Mohammed Rayyan", ""], ["Chatterjee", "Debasish", ""]]}, {"id": "1603.02160", "submitter": "Seth Flaxman", "authors": "Seth Flaxman, Dino Sejdinovic, John P. Cunningham, and Sarah Filippi", "title": "Bayesian Learning of Kernel Embeddings", "comments": "Conference paper appearing in UAI 2016, including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are one of the mainstays of machine learning, but the problem\nof kernel learning remains challenging, with only a few heuristics and very\nlittle theory. This is of particular importance in methods based on estimation\nof kernel mean embeddings of probability measures. For characteristic kernels,\nwhich include most commonly used ones, the kernel mean embedding uniquely\ndetermines its probability measure, so it can be used to design a powerful\nstatistical testing framework, which includes nonparametric two-sample and\nindependence tests. In practice, however, the performance of these tests can be\nvery sensitive to the choice of kernel and its lengthscale parameters. To\naddress this central issue, we propose a new probabilistic model for kernel\nmean embeddings, the Bayesian Kernel Embedding model, combining a Gaussian\nprocess prior over the Reproducing Kernel Hilbert Space containing the mean\nembedding with a conjugate likelihood function, thus yielding a closed form\nposterior over the mean embedding. The posterior mean of our model is closely\nrelated to recently proposed shrinkage estimators for kernel mean embeddings,\nwhile the posterior uncertainty is a new, interesting feature with various\npossible applications. Critically for the purposes of kernel learning, our\nmodel gives a simple, closed form marginal pseudolikelihood of the observed\ndata given the kernel hyperparameters. This marginal pseudolikelihood can\neither be optimized to inform the hyperparameter choice or fully Bayesian\ninference can be used.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 17:16:46 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 10:08:37 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""], ["Cunningham", "John P.", ""], ["Filippi", "Sarah", ""]]}, {"id": "1603.02185", "submitter": "Jialei Wang", "authors": "Jialei Wang, Mladen Kolar, Nathan Srebro", "title": "Distributed Multi-Task Learning with Shared Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:11:54 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Wang", "Jialei", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1603.02412", "submitter": "Tomoya Murata", "authors": "Tomoya Murata and Taiji Suzuki", "title": "Stochastic dual averaging methods using variance reduction techniques\n  for regularized empirical risk minimization problems", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a composite convex minimization problem associated with\nregularized empirical risk minimization, which often arises in machine\nlearning. We propose two new stochastic gradient methods that are based on\nstochastic dual averaging method with variance reduction. Our methods generate\na sparser solution than the existing methods because we do not need to take the\naverage of the history of the solutions. This is favorable in terms of both\ninterpretability and generalization. Moreover, our methods have theoretical\nsupport for both a strongly and a non-strongly convex regularizer and achieve\nthe best known convergence rates among existing nonaccelerated stochastic\ngradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 08:26:28 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Murata", "Tomoya", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1603.02434", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda", "title": "Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines", "comments": null, "journal-ref": "Proceedings of 22nd International Conference on Pattern\n  Recognition (ICPR2014), pp.3600-3605, 2014", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neural\nnetwork models that can describe multi-modal nonnegative data. NNBMs form\nrectified Gaussian distributions that appear in biological neural network\nmodels, positive matrix factorization, nonnegative matrix factorization, and so\non. In this paper, an effective inference method for NNBMs is proposed that\nuses the mean-field method, referred to as the Thouless--Anderson--Palmer\nequation, and the diagonal consistency method, which was recently proposed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 09:31:53 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Yasuda", "Muneki", ""]]}, {"id": "1603.02443", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer", "title": "Note on the equivalence of hierarchical variational models and auxiliary\n  deep generative models", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note compares two recently published machine learning methods for\nconstructing flexible, but tractable families of variational hidden-variable\nposteriors. The first method, called \"hierarchical variational models\" enriches\nthe inference model with an extra variable, while the other, called \"auxiliary\ndeep generative models\", enriches the generative model instead. We conclude\nthat the two methods are mathematically equivalent.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 09:46:30 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 10:54:36 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Br\u00fcmmer", "Niko", ""]]}, {"id": "1603.02494", "submitter": "Tapesh Santra", "authors": "Tapesh Santra", "title": "A Bayesian non-parametric method for clustering high-dimensional binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real life problems, objects are described by large number of binary\nfeatures. For instance, documents are characterized by presence or absence of\ncertain keywords; cancer patients are characterized by presence or absence of\ncertain mutations etc. In such cases, grouping together similar\nobjects/profiles based on such high dimensional binary features is desirable,\nbut challenging. Here, I present a Bayesian non parametric algorithm for\nclustering high dimensional binary data. It uses a Dirichlet Process (DP)\nmixture model and simulated annealing to not only cluster binary data, but also\nfind optimal number of clusters in the data. The performance of the algorithm\nwas evaluated and compared with other algorithms using simulated datasets. It\noutperformed all other clustering methods that were tested in the simulation\nstudies. It was also used to cluster real datasets arising from document\nanalysis, handwritten image analysis and cancer research. It successfully\ndivided a set of documents based on their topics, hand written images based on\ndifferent styles of writing digits and identified tissue and mutation\nspecificity of chemotherapy treatments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 12:02:59 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Santra", "Tapesh", ""]]}, {"id": "1603.02501", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Clayton Scott and Ambuj Tewari", "title": "Mixture Proportion Estimation via Kernel Embedding of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture proportion estimation (MPE) is the problem of estimating the weight\nof a component distribution in a mixture, given samples from the mixture and\ncomponent. This problem constitutes a key part in many \"weakly supervised\nlearning\" problems like learning with positive and unlabelled samples, learning\nwith label noise, anomaly detection and crowdsourcing. While there have been\nseveral methods proposed to solve this problem, to the best of our knowledge no\nefficient algorithm with a proven convergence rate towards the true proportion\nexists for this problem. We fill this gap by constructing a provably correct\nalgorithm for MPE, and derive convergence rates under certain assumptions on\nthe distribution. Our method is based on embedding distributions onto an RKHS,\nand implementing it only requires solving a simple convex quadratic programming\nproblem a few times. We run our algorithm on several standard classification\ndatasets, and demonstrate that it performs comparably to or better than other\nalgorithms on most datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 12:43:29 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 16:41:44 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Scott", "Clayton", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1603.02532", "submitter": "Antti Honkela", "authors": "Otte Hein\\\"avaara, Janne Lepp\\\"a-aho, Jukka Corander and Antti Honkela", "title": "On the inconsistency of $\\ell_1$-penalised sparse precision matrix\n  estimation", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various $\\ell_1$-penalised estimation methods such as graphical lasso and\nCLIME are widely used for sparse precision matrix estimation. Many of these\nmethods have been shown to be consistent under various quantitative assumptions\nabout the underlying true covariance matrix. Intuitively, these conditions are\nrelated to situations where the penalty term will dominate the optimisation. In\nthis paper, we explore the consistency of $\\ell_1$-based methods for a class of\nsparse latent variable -like models, which are strongly motivated by several\ntypes of applications. We show that all $\\ell_1$-based methods fail\ndramatically for models with nearly linear dependencies between the variables.\nWe also study the consistency on models derived from real gene expression data\nand note that the assumptions needed for consistency never hold even for modest\nsized gene networks and $\\ell_1$-based methods also become unreliable in\npractice for larger networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 14:24:11 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Hein\u00e4vaara", "Otte", ""], ["Lepp\u00e4-aho", "Janne", ""], ["Corander", "Jukka", ""], ["Honkela", "Antti", ""]]}, {"id": "1603.02638", "submitter": "Le Riche Rodolphe", "authors": "Hossein Mohammadi, Rodolphe Le Riche, Eric Touboul", "title": "Small ensembles of kriging models for optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Efficient Global Optimization (EGO) algorithm uses a conditional\nGaus-sian Process (GP) to approximate an objective function known at a finite\nnumber of observation points and sequentially adds new points which maximize\nthe Expected Improvement criterion according to the GP. The important factor\nthat controls the efficiency of EGO is the GP covariance function (or kernel)\nwhich should be chosen according to the objective function. Traditionally, a\npa-rameterized family of covariance functions is considered whose parameters\nare learned through statistical procedures such as maximum likelihood or\ncross-validation. However, it may be questioned whether statistical procedures\nfor learning covariance functions are the most efficient for optimization as\nthey target a global agreement between the GP and the observations which is not\nthe ultimate goal of optimization. Furthermore, statistical learning procedures\nare computationally expensive. The main alternative to the statistical learning\nof the GP is self-adaptation, where the algorithm tunes the kernel parameters\nbased on their contribution to objective function improvement. After\nquestioning the possibility of self-adaptation for kriging based optimizers,\nthis paper proposes a novel approach for tuning the length-scale of the GP in\nEGO: At each iteration, a small ensemble of kriging models structured by their\nlength-scales is created. All of the models contribute to an iterate in an\nEGO-like fashion. Then, the set of models is densified around the model whose\nlength-scale yielded the best iterate and further points are produced.\nNumerical experiments are provided which motivate the use of many\nlength-scales. The tested implementation does not perform better than the\nclassical EGO algorithm in a sequential context but show the potential of the\napproach for parallel implementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:42:14 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Riche", "Rodolphe Le", ""], ["Touboul", "Eric", ""]]}, {"id": "1603.02644", "submitter": "Christophe Dupuy", "authors": "Christophe Dupuy (SIERRA), Francis Bach (LIENS, SIERRA)", "title": "Online but Accurate Inference for Latent Variable Models with Local\n  Gibbs Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, Journal of Machine Learning\n  Research, 2017, 18, pp.1 - 45", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameter inference in large-scale latent variable models. We first\npropose an unified treatment of online inference for latent variable models\nfrom a non-canonical exponential family, and draw explicit links between\nseveral previously proposed frequentist or Bayesian methods. We then propose a\nnovel inference method for the frequentist estimation of parameters, that\nadapts MCMC methods to online inference of latent variable models with the\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\nprovide an extensive set of experiments and comparisons with existing work,\nwhere our new approach outperforms all previously proposed methods. In\nparticular, using Gibbs sampling for latent variable inference is superior to\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\ninference through variational methods perform poorly, sometimes leading to\nworse fits with latent variables of higher dimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:57:47 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 20:00:25 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 12:54:53 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 07:19:15 GMT"}, {"version": "v5", "created": "Wed, 31 Jan 2018 08:38:26 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Dupuy", "Christophe", "", "SIERRA"], ["Bach", "Francis", "", "LIENS, SIERRA"]]}, {"id": "1603.02736", "submitter": "Umamahesh Srinivas", "authors": "Umamahesh Srinivas", "title": "Discriminative models for robust image classification", "comments": "Doctoral dissertation, Department of Electrical Engineering, The\n  Pennsylvania State University, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of real-world tasks involve the classification of images into\npre-determined categories. Designing image classification algorithms that\nexhibit robustness to acquisition noise and image distortions, particularly\nwhen the available training data are insufficient to learn accurate models, is\na significant challenge. This dissertation explores the development of\ndiscriminative models for robust image classification that exploit underlying\nsignal structure, via probabilistic graphical models and sparse signal\nrepresentations.\n  Probabilistic graphical models are widely used in many applications to\napproximate high-dimensional data in a reduced complexity set-up. Learning\ngraphical structures to approximate probability distributions is an area of\nactive research. Recent work has focused on learning graphs in a discriminative\nmanner with the goal of minimizing classification error. In the first part of\nthe dissertation, we develop a discriminative learning framework that exploits\nthe complementary yet correlated information offered by multiple\nrepresentations (or projections) of a given signal/image. Specifically, we\npropose a discriminative tree-based scheme for feature fusion by explicitly\nlearning the conditional correlations among such multiple projections in an\niterative manner. Experiments reveal the robustness of the resulting graphical\nmodel classifier to training insufficiency.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 23:15:18 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Srinivas", "Umamahesh", ""]]}, {"id": "1603.02740", "submitter": "Johan Ugander", "authors": "Stephen Ragain, Johan Ugander", "title": "Pairwise Choice Markov Chains", "comments": "Advances in Neural Information Processing Systems (NIPS) 29, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets capturing human choices grow in richness and scale --\nparticularly in online domains -- there is an increasing need for choice models\nthat escape traditional choice-theoretic axioms such as regularity, stochastic\ntransitivity, and Luce's choice axiom. In this work we introduce the Pairwise\nChoice Markov Chain (PCMC) model of discrete choice, an inferentially tractable\nmodel that does not assume any of the above axioms while still satisfying the\nfoundational axiom of uniform expansion, a considerably weaker assumption than\nLuce's choice axiom. We show that the PCMC model significantly outperforms the\nMultinomial Logit (MNL) model in prediction tasks on both synthetic and\nempirical datasets known to exhibit violations of Luce's axiom. Our analysis\nalso synthesizes several recent observations connecting the Multinomial Logit\nmodel and Markov chains; the PCMC model retains the Multinomial Logit model as\na special case.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 23:47:03 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 06:54:59 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 18:38:48 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 05:00:26 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ragain", "Stephen", ""], ["Ugander", "Johan", ""]]}, {"id": "1603.02743", "submitter": "Carsten Dormann", "authors": "Severin Hauenstein, Carsten F. Dormann and Simon N Wood", "title": "Computing AIC for black-box models using Generalised Degrees of Freedom:\n  a comparison with cross-validation", "comments": "accompanying R-code on github", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA\n93:120-131), represent the sensitivity of model fits to perturbations of the\ndata. As such they can be computed for any statistical model, making it\npossible, in principle, to derive the number of parameters in machine-learning\napproaches. Defined originally for normally distributed data only, we here\ninvestigate the potential of this approach for Bernoulli-data. GDF-values for\nmodels of simulated and real data are compared to model complexity-estimates\nfrom cross-validation. Similarly, we computed GDF-based AICc for randomForest,\nneural networks and boosted regression trees and demonstrated its similarity to\ncross-validation. GDF-estimates for binary data were unstable and\ninconsistently sensitive to the number of data points perturbed simultaneously,\nwhile at the same time being extremely computer-intensive in their calculation.\nRepeated 10-fold cross-validation was more robust, based on fewer assumptions\nand faster to compute. Our findings suggest that the GDF-approach does not\nreadily transfer to Bernoulli data and a wider range of regression approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 00:01:18 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Hauenstein", "Severin", ""], ["Dormann", "Carsten F.", ""], ["Wood", "Simon N", ""]]}, {"id": "1603.02752", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Kevin Jamieson, Benjamin Recht", "title": "Best-of-K Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Best-of-K Bandit game: At each time the player chooses\na subset S among all N-choose-K possible options and observes reward max(X(i) :\ni in S) where X is a random vector drawn from a joint distribution. The\nobjective is to identify the subset that achieves the highest expected reward\nwith high probability using as few queries as possible. We present\ndistribution-dependent lower bounds based on a particular construction which\nforce a learner to consider all N-choose-K subsets, and match naive extensions\nof known upper bounds in the bandit setting obtained by treating each subset as\na separate arm. Nevertheless, we present evidence that exhaustive search may be\navoided for certain, favorable distributions because the influence of\nhigh-order order correlations may be dominated by lower order statistics.\nFinally, we present an algorithm and analysis for independent arms, which\nmitigates the surprising non-trivial information occlusion that occurs due to\nonly observing the max in the subset. This may inform strategies for more\ngeneral dependent measures, and we complement these result with independent-arm\nlower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 00:55:58 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 20:06:50 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Simchowitz", "Max", ""], ["Jamieson", "Kevin", ""], ["Recht", "Benjamin", ""]]}, {"id": "1603.02763", "submitter": "Marina Meila", "authors": "James McQueen and Marina Meila and Jacob VanderPlas and Zhongyue Zhang", "title": "megaman: Manifold Learning with Millions of points", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold Learning is a class of algorithms seeking a low-dimensional\nnon-linear representation of high-dimensional data. Thus manifold learning\nalgorithms are, at least in theory, most applicable to high-dimensional data\nand sample sizes to enable accurate estimation of the manifold. Despite this,\nmost existing manifold learning implementations are not particularly scalable.\nHere we present a Python package that implements a variety of manifold learning\nalgorithms in a modular and scalable fashion, using fast approximate neighbors\nsearches and fast sparse eigendecompositions. The package incorporates\ntheoretical advances in manifold learning, such as the unbiased Laplacian\nestimator and the estimation of the embedding distortion by the Riemannian\nmetric method. In benchmarks, even on a single-core desktop computer, our code\nembeds millions of data points in minutes, and takes just 200 minutes to embed\nthe main sample of galaxy spectra from the Sloan Digital Sky Survey ---\nconsisting of 0.6 million samples in 3750-dimensions --- a task which has not\npreviously been possible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 02:05:11 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["McQueen", "James", ""], ["Meila", "Marina", ""], ["VanderPlas", "Jacob", ""], ["Zhang", "Zhongyue", ""]]}, {"id": "1603.02782", "submitter": "Megasthenis Asteris", "authors": "Megasthenis Asteris and Anastasios Kyrillidis and Dimitris\n  Papailiopoulos and Alexandros G. Dimakis", "title": "Bipartite Correlation Clustering -- Maximizing Agreements", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bipartite Correlation Clustering (BCC) we are given a complete bipartite\ngraph $G$ with `+' and `-' edges, and we seek a vertex clustering that\nmaximizes the number of agreements: the number of all `+' edges within clusters\nplus all `-' edges cut across clusters. BCC is known to be NP-hard.\n  We present a novel approximation algorithm for $k$-BCC, a variant of BCC with\nan upper bound $k$ on the number of clusters. Our algorithm outputs a\n$k$-clustering that provably achieves a number of agreements within a\nmultiplicative ${(1-\\delta)}$-factor from the optimal, for any desired accuracy\n$\\delta$. It relies on solving a combinatorially constrained bilinear\nmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in\n$k$ and $\\delta^{-1}$, but linear in the size of the input.\n  Further, we show that, in the (unconstrained) BCC setting, an\n${(1-\\delta)}$-approximation can be achieved by $O(\\delta^{-1})$ clusters\nregardless of the size of the graph. In turn, our $k$-BCC algorithm implies an\nEfficient PTAS for the BCC objective of maximizing agreements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 05:18:53 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Papailiopoulos", "Dimitris", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1603.02806", "submitter": "Gustau Camps-Valls", "authors": "Emma Izquierdo-Verdiguier, Valero Laparra, Robert Jenssen, Luis\n  G\\'omez-Chova, Gustau Camps-Valls", "title": "Optimized Kernel Entropy Components", "comments": "IEEE Transactions on Neural Networks and Learning Systems, 2016", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2530403.", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses two main issues of the standard Kernel Entropy Component\nAnalysis (KECA) algorithm: the optimization of the kernel decomposition and the\noptimization of the Gaussian kernel parameter. KECA roughly reduces to a\nsorting of the importance of kernel eigenvectors by entropy instead of by\nvariance as in Kernel Principal Components Analysis. In this work, we propose\nan extension of the KECA method, named Optimized KECA (OKECA), that directly\nextracts the optimal features retaining most of the data entropy by means of\ncompacting the information in very few features (often in just one or two). The\nproposed method produces features which have higher expressive power. In\nparticular, it is based on the Independent Component Analysis (ICA) framework,\nand introduces an extra rotation to the eigen-decomposition, which is optimized\nvia gradient ascent search. This maximum entropy preservation suggests that\nOKECA features are more efficient than KECA features for density estimation. In\naddition, a critical issue in both methods is the selection of the kernel\nparameter since it critically affects the resulting performance. Here we\nanalyze the most common kernel length-scale selection criteria. Results of both\nmethods are illustrated in different synthetic and real problems. Results show\nthat 1) OKECA returns projections with more expressive power than KECA, 2) the\nmost successful rule for estimating the kernel parameter is based on maximum\nlikelihood, and 3) OKECA is more robust to the selection of the length-scale\nparameter in kernel density estimation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 08:09:59 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Izquierdo-Verdiguier", "Emma", ""], ["Laparra", "Valero", ""], ["Jenssen", "Robert", ""], ["G\u00f3mez-Chova", "Luis", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1603.02977", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi and Danilo P. Mandic", "title": "Frequency estimation in three-phase power systems with harmonic\n  contamination: A multistage quaternion Kalman filtering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for accurate frequency information, a novel algorithm\nfor estimating the fundamental frequency and its rate of change in three-phase\npower systems is developed. This is achieved through two stages of Kalman\nfiltering. In the first stage a quaternion extended Kalman filter, which\nprovides a unified framework for joint modeling of voltage measurements from\nall the phases, is used to estimate the instantaneous phase increment of the\nthree-phase voltages. The phase increment estimates are then used as\nobservations of the extended Kalman filter in the second stage that accounts\nfor the dynamic behavior of the system frequency and simultaneously estimates\nthe fundamental frequency and its rate of change. The framework is then\nextended to account for the presence of harmonics. Finally, the concept is\nvalidated through simulation on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 08:07:26 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Talebi", "Sayed Pouria", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1603.03030", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathanael Perraudin, Benjamin Ricaud, David Shuman, Pierre\n  Vandergheynst", "title": "Global and Local Uncertainty Principles for Signals on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty principles such as Heisenberg's provide limits on the\ntime-frequency concentration of a signal, and constitute an important\ntheoretical tool for designing and evaluating linear signal transforms.\nGeneralizations of such principles to the graph setting can inform dictionary\ndesign for graph signals, lead to algorithms for reconstructing missing\ninformation from graph signals via sparse representations, and yield new graph\nanalysis tools. While previous work has focused on generalizing notions of\nspreads of a graph signal in the vertex and graph spectral domains, our\napproach is to generalize the methods of Lieb in order to develop uncertainty\nprinciples that provide limits on the concentration of the analysis\ncoefficients of any graph signal under a dictionary transform whose atoms are\njointly localized in the vertex and graph spectral domains. One challenge we\nhighlight is that due to the inhomogeneity of the underlying graph data domain,\nthe local structure in a single small region of the graph can drastically\naffect the uncertainty bounds for signals concentrated in different regions of\nthe graph, limiting the information provided by global uncertainty principles.\nAccordingly, we suggest a new way to incorporate a notion of locality, and\ndevelop local uncertainty principles that bound the concentration of the\nanalysis coefficients of each atom of a localized graph spectral filter frame\nin terms of quantities that depend on the local structure of the graph around\nthe center vertex of the given atom. Finally, we demonstrate how our proposed\nlocal uncertainty measures can improve the random sampling of graph signals.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 00:59:15 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Ricaud", "Benjamin", ""], ["Shuman", "David", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1603.03089", "submitter": "Eleftherios Kofidis", "authors": "Eleftherios Kofidis", "title": "Blind Source Separation: Fundamentals and Recent Advances (A Tutorial\n  Overview Presented at SBrT-2001)", "comments": "Tutorial overview of BSS (also presented at SBrT-2001), providing a\n  complete account of the area in early 2000's", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation (BSS), i.e., the decoupling of unknown signals that\nhave been mixed in an unknown way, has been a topic of great interest in the\nsignal processing community for the last decade, covering a wide range of\napplications in such diverse fields as digital communications, pattern\nrecognition, biomedical engineering, and financial data analysis, among others.\nThis course aims at an introduction to the BSS problem via an exposition of\nwell-known and established as well as some more recent approaches to its\nsolution. A unified way is followed in presenting the various results so as to\nmore easily bring out their similarities/differences and emphasize their\nrelative advantages/disadvantages. Only a representative sample of the existing\nknowledge on BSS will be included in this course. The interested readers are\nencouraged to consult the list of bibliographical references for more details\non this exciting and always active research topic.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 22:30:58 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Kofidis", "Eleftherios", ""]]}, {"id": "1603.03130", "submitter": "Gang Niu", "authors": "Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and\n  Masashi Sugiyama", "title": "Theoretical Comparisons of Positive-Unlabeled Learning against\n  Positive-Negative Learning", "comments": "NIPS 2016 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In PU learning, a binary classifier is trained from positive (P) and\nunlabeled (U) data without negative (N) data. Although N data is missing, it\nsometimes outperforms PN learning (i.e., ordinary supervised learning).\nHitherto, neither theoretical nor experimental analysis has been given to\nexplain this phenomenon. In this paper, we theoretically compare PU (and NU)\nlearning against PN learning based on the upper bounds on estimation errors. We\nfind simple conditions when PU and NU learning are likely to outperform PN\nlearning, and we prove that, in terms of the upper bounds, either PU or NU\nlearning (depending on the class-prior probability and the sizes of P and N\ndata) given infinite U data will improve on PN learning. Our theoretical\nfindings well agree with the experimental results on artificial and benchmark\ndata even when the experimental setup does not match the theoretical\nassumptions exactly.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 02:53:52 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 04:35:47 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 13:37:46 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Niu", "Gang", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Sakai", "Tomoya", ""], ["Ma", "Yao", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1603.03236", "submitter": "Sebastian Weichwald", "authors": "James Townsend, Niklas Koep, Sebastian Weichwald", "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic\n  Differentiation", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(137):1-5, 2016 (\n  https://jmlr.org/papers/v17/16-177.html )", "doi": null, "report-no": null, "categories": "cs.MS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization on manifolds is a class of methods for optimization of an\nobjective function, subject to constraints which are smooth, in the sense that\nthe set of points which satisfy the constraints admits the structure of a\ndifferentiable manifold. While many optimization problems are of the described\nform, technicalities of differential geometry and the laborious calculation of\nderivatives pose a significant barrier for experimenting with these methods.\n  We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox\nfor optimization on manifolds, implemented in Python, that---similarly to the\nManopt Matlab toolbox---implements several manifold geometries and optimization\nalgorithms. Moreover, we lower the barriers to users further by using automated\ndifferentiation for calculating derivative information, saving users time and\nsaving them from potential calculation and implementation errors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 12:23:12 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 12:46:31 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 10:04:13 GMT"}, {"version": "v4", "created": "Thu, 8 Sep 2016 09:23:08 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Townsend", "James", ""], ["Koep", "Niklas", ""], ["Weichwald", "Sebastian", ""]]}, {"id": "1603.03629", "submitter": "David Inouye", "authors": "David I. Inouye, Pradeep Ravikumar, Inderjit S. Dhillon", "title": "Square Root Graphical Models: Multivariate Generalizations of Univariate\n  Exponential Families that Permit Positive Dependencies", "comments": null, "journal-ref": "ICML 2016", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Square Root Graphical Models (SQR), a novel class of parametric\ngraphical models that provides multivariate generalizations of univariate\nexponential family distributions. Previous multivariate graphical models [Yang\net al. 2015] did not allow positive dependencies for the exponential and\nPoisson generalizations. However, in many real-world datasets, variables\nclearly have positive dependencies. For example, the airport delay time in New\nYork---modeled as an exponential distribution---is positively related to the\ndelay time in Boston. With this motivation, we give an example of our model\nclass derived from the univariate exponential distribution that allows for\nalmost arbitrary positive and negative dependencies with only a mild condition\non the parameter matrix---a condition akin to the positive definiteness of the\nGaussian covariance matrix. Our Poisson generalization allows for both positive\nand negative dependencies without any constraints on the parameter values. We\nalso develop parameter estimation methods using node-wise regressions with\n$\\ell_1$ regularization and likelihood approximation methods using sampling.\nFinally, we demonstrate our exponential generalization on a synthetic dataset\nand a real-world dataset of airport delay times.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 14:02:20 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 19:04:59 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Inouye", "David I.", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1603.03678", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Stephen Kelley, Alfred Hero", "title": "Nonstationary Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with provided sets of pairwise\nsimilarity and dissimilarity constraints. The learned transformations lead to\nimproved retrieval, classification, and clustering algorithms due to the better\nadapted distance or similarity measures. Here, we introduce the problem of\nlearning these transformations when the underlying constraint generation\nprocess is nonstationary. This nonstationarity can be due to changes in either\nthe ground-truth clustering used to generate constraints or changes to the\nfeature subspaces in which the class structure is apparent. We propose and\nevaluate COMID-SADL, an adaptive, online approach for learning and tracking\noptimal metrics as they change over time that is highly robust to a variety of\nnonstationary behaviors in the changing metric. We demonstrate COMID-SADL on\nboth real and synthetic data sets and show significant performance improvements\nrelative to previously proposed batch and online distance metric learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 16:16:45 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 09:27:10 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Kelley", "Stephen", ""], ["Hero", "Alfred", ""]]}, {"id": "1603.03724", "submitter": "Niharika Gauraha Niharika Gauraha", "authors": "Niharika Gauraha, Swapan K. Parui", "title": "Efficient Clustering of Correlated Variables and Variable Selection in\n  High-Dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable\nselection in high dimensional sparse regression models with strongly correlated\nvariables. To handle correlated variables, the concept of clustering or\ngrouping variables and then pursuing model fitting is widely accepted. When the\ndimension is very high, finding an appropriate group structure is as difficult\nas the original problem. The ACL is a three-stage procedure where, at the first\nstage, we use the Lasso(or its adaptive or thresholded version) to do initial\nselection, then we also include those variables which are not selected by the\nLasso but are strongly correlated with the variables selected by the Lasso. At\nthe second stage we cluster the variables based on the reduced set of\npredictors and in the third stage we perform sparse estimation such as Lasso on\ncluster representatives or the group Lasso based on the structures generated by\nclustering procedure. We show that our procedure is consistent and efficient in\nfinding true underlying population group structure(under assumption of\nirrepresentable and beta-min conditions). We also study the group selection\nconsistency of our method and we support the theory using simulated and\npseudo-real dataset examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 19:06:33 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Gauraha", "Niharika", ""], ["Parui", "Swapan K.", ""]]}, {"id": "1603.03788", "submitter": "Andrey Kormilitzin", "authors": "Ilya Chevyrev, Andrey Kormilitzin", "title": "A Primer on the Signature Method in Machine Learning", "comments": "45 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes, we wish to provide an introduction to the signature method,\nfocusing on its basic theoretical properties and recent numerical applications.\n  The notes are split into two parts. The first part focuses on the definition\nand fundamental properties of the signature of a path, or the path signature.\nWe have aimed for a minimalistic approach, assuming only familiarity with\nclassical real analysis and integration theory, and supplementing theory with\nstraightforward examples. We have chosen to focus in detail on the principle\nproperties of the signature which we believe are fundamental to understanding\nits role in applications. We also present an informal discussion on some of its\ndeeper properties and briefly mention the role of the signature in rough paths\ntheory, which we hope could serve as a light introduction to rough paths for\nthe interested reader.\n  The second part of these notes discusses practical applications of the path\nsignature to the area of machine learning. The signature approach represents a\nnon-parametric way for extraction of characteristic features from data. The\ndata are converted into a multi-dimensional path by means of various embedding\nalgorithms and then processed for computation of individual terms of the\nsignature which summarise certain information contained in the data. The\nsignature thus transforms raw data into a set of features which are used in\nmachine learning tasks. We will review current progress in applications of\nsignatures to machine learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:24:42 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chevyrev", "Ilya", ""], ["Kormilitzin", "Andrey", ""]]}, {"id": "1603.03799", "submitter": "Gustavo C. Amaral Dr.", "authors": "Mario Souto, Joaquim D. Garcia and Gustavo C. Amaral", "title": "$\\ell_1$ Adaptive Trend Filter via Fast Coordinate Descent", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the unknown underlying trend of a given noisy signal is extremely\nuseful for a wide range of applications. The number of potential trends might\nbe exponential, which can be computationally exhaustive even for short signals.\nAnother challenge, is the presence of abrupt changes and outliers at unknown\ntimes which impart resourceful information regarding the signal's\ncharacteristics. In this paper, we present the $\\ell_1$ Adaptive Trend Filter,\nwhich can consistently identify the components in the underlying trend and\nmultiple level-shifts, even in the presence of outliers. Additionally, an\nenhanced coordinate descent algorithm which exploit the filter design is\npresented. Some implementation details are discussed and a version in the Julia\nlanguage is presented along with two distinct applications to illustrate the\nfilter's potential.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:58:02 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:03:24 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Souto", "Mario", ""], ["Garcia", "Joaquim D.", ""], ["Amaral", "Gustavo C.", ""]]}, {"id": "1603.03805", "submitter": "Huishuai Zhang", "authors": "Huishuai Zhang, Yuejie Chi, Yingbin Liang", "title": "Median-Truncated Nonconvex Approach for Phase Retrieval with Outliers", "comments": "journal version under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the phase retrieval problem, which aims to recover a\nsignal from the magnitudes of its linear measurements. We develop statistically\nand computationally efficient algorithms for the situation when the\nmeasurements are corrupted by sparse outliers that can take arbitrary values.\nWe propose a novel approach to robustify the gradient descent algorithm by\nusing the sample median as a guide for pruning spurious samples in\ninitialization and local search. Adopting the Poisson loss and the reshaped\nquadratic loss respectively, we obtain two algorithms termed median-TWF and\nmedian-RWF, both of which provably recover the signal from a near-optimal\nnumber of measurements when the measurement vectors are composed of i.i.d.\nGaussian entries, up to a logarithmic factor, even when a constant fraction of\nthe measurements are adversarially corrupted. We further show that both\nalgorithms are stable in the presence of additional dense bounded noise. Our\nanalysis is accomplished by developing non-trivial concentration results of\nmedian-related quantities, which may be of independent interest. We provide\nnumerical experiments to demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 22:10:04 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 02:48:21 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Zhang", "Huishuai", ""], ["Chi", "Yuejie", ""], ["Liang", "Yingbin", ""]]}, {"id": "1603.03827", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "comments": "Accepted as a conference paper at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:02:51 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1603.03972", "submitter": "Keith Levin", "authors": "Keith Levin and Vince Lyzinski", "title": "Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2645517", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning and dimensionality reduction techniques are ubiquitous in\nscience and engineering, but can be computationally expensive procedures when\napplied to large data sets or when similarities are expensive to compute. To\ndate, little work has been done to investigate the tradeoff between\ncomputational resources and the quality of learned representations. We present\nboth theoretical and experimental explorations of this question. In particular,\nwe consider Laplacian eigenmaps embeddings based on a kernel matrix, and\nexplore how the embeddings behave when this kernel matrix is corrupted by\nocclusion and noise. Our main theoretical result shows that under modest noise\nand occlusion assumptions, we can (with high probability) recover a good\napproximation to the Laplacian eigenmaps embedding based on the uncorrupted\nkernel matrix. Our results also show how regularization can aid this\napproximation. Experimentally, we explore the effects of noise and occlusion on\nLaplacian eigenmaps embeddings of two real-world data sets, one from speech\nprocessing and one from neuroscience, as well as a synthetic data set.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 23:02:20 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 14:16:56 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Levin", "Keith", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1603.03977", "submitter": "Shuang Song", "authors": "Shuang Song, Yizhen Wang, Kamalika Chaudhuri", "title": "Pufferfish Privacy Mechanisms for Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern databases include personal and sensitive correlated data, such as\nprivate information on users connected together in a social network, and\nmeasurements of physical activity of single subjects across time. However,\ndifferential privacy, the current gold standard in data privacy, does not\nadequately address privacy issues in this kind of data.\n  This work looks at a recent generalization of differential privacy, called\nPufferfish, that can be used to address privacy in correlated data. The main\nchallenge in applying Pufferfish is a lack of suitable mechanisms. We provide\nthe first mechanism -- the Wasserstein Mechanism -- which applies to any\ngeneral Pufferfish framework. Since this mechanism may be computationally\ninefficient, we provide an additional mechanism that applies to some practical\ncases such as physical activity measurements across time, and is\ncomputationally efficient. Our experimental evaluations indicate that this\nmechanism provides privacy and utility for synthetic as well as real data in\ntwo separate domains.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 00:47:15 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 06:01:57 GMT"}, {"version": "v3", "created": "Sun, 12 Mar 2017 22:47:02 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Song", "Shuang", ""], ["Wang", "Yizhen", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1603.03980", "submitter": "Ravi Ganti", "authors": "Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak", "title": "On Learning High Dimensional Structured Single Index Models", "comments": "7 pages, 3 tables, 1 Figure, substantial text overlap with\n  arXiv:1506.08910; Accepted for publication at AAAI 2017; added new\n  experimental results comparing our method to a single layer neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nmachine learning, where the response variable is modeled as a monotonic\nfunction of a linear combination of features. Estimation in this context\nrequires learning both the feature weights and the nonlinear function that\nrelates features to observations. While methods have been described to learn\nSIMs in the low dimensional regime, a method that can efficiently learn SIMs in\nhigh dimensions, and under general structural assumptions, has not been\nforthcoming. In this paper, we propose computationally efficient algorithms for\nSIM inference in high dimensions with structural constraints. Our general\napproach specializes to sparsity, group sparsity, and low-rank assumptions\namong others. Experiments show that the proposed method enjoys superior\npredictive performance when compared to generalized linear models, and achieves\nresults comparable to or better than single layer feedforward neural networks\nwith significantly less computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 01:53:40 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:55:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Rao", "Nikhil", ""], ["Ganti", "Ravi", ""], ["Balzano", "Laura", ""], ["Willett", "Rebecca", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04017", "submitter": "Gautier Marti", "authors": "Gautier Marti, S\\'ebastien Andler, Frank Nielsen, Philippe Donnat", "title": "Clustering Financial Time Series: How Long is Enough?", "comments": "Accepted at IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have used from 30 days to several years of daily returns as\nsource data for clustering financial time series based on their correlations.\nThis paper sets up a statistical framework to study the validity of such\npractices. We first show that clustering correlated random variables from their\nobserved values is statistically consistent. Then, we also give a first\nempirical answer to the much debated question: How long should the time series\nbe? If too short, the clusters found can be spurious; if too long, dynamics can\nbe smoothed out.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 10:47:00 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 20:03:41 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Marti", "Gautier", ""], ["Andler", "S\u00e9bastien", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1603.04064", "submitter": "Andrea Montanari", "authors": "Andrea Montanari", "title": "A Grothendieck-type inequality for local maxima", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of problems in optimization, machine learning, signal\nprocessing can be effectively addressed by suitable semidefinite programming\n(SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyond\ninstances with a few hundreds variables (in the underlying combinatorial\nproblem). On the other hand, it has been observed empirically that an effective\nstrategy amounts to introducing a (non-convex) rank constraint, and solving the\nresulting smooth optimization problem by ascent methods. This non-convex\nproblem has --generically-- a large number of local maxima, and the reason for\nthis success is therefore unclear.\n  This paper provides rigorous support for this approach. For the problem of\nmaximizing a linear functional over the elliptope, we prove that all local\nmaxima are within a small gap from the SDP optimum. In several problems of\ninterest, arbitrarily small relative error can be achieved by taking the rank\nconstraint $k$ to be of order one, independently of the problem size.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 18:44:24 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Montanari", "Andrea", ""]]}, {"id": "1603.04118", "submitter": "Ravi Ganti", "authors": "Aniruddha Bhargava, Ravi Ganti, Robert Nowak", "title": "Active Algorithms For Preference Learning Problems with Multiple\n  Populations", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we model the problem of learning preferences of a population as\nan active learning problem. We propose an algorithm can adaptively choose pairs\nof items to show to users coming from a heterogeneous population, and use the\nobtained reward to decide which pair of items to show next. We provide\ncomputationally efficient algorithms with provable sample complexity guarantees\nfor this problem in both the noiseless and noisy cases. In the process of\nestablishing sample complexity guarantees for our algorithms, we establish new\nresults using a Nystr{\\\"o}m-like method which can be of independent interest.\nWe supplement our theoretical results with experimental comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:08:24 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:48:58 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Bhargava", "Aniruddha", ""], ["Ganti", "Ravi", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04119", "submitter": "Alekh Agarwal", "authors": "David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert\n  E. Schapire", "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional observations and complex real-world dynamics present major\nchallenges in reinforcement learning for both function approximation and\nexploration. We address both of these challenges with two complementary\ntechniques: First, we develop a gradient-boosting style, non-parametric\nfunction approximator for learning on $Q$-function residuals. And second, we\npropose an exploration strategy inspired by the principles of state abstraction\nand information acquisition under uncertainty. We demonstrate the empirical\neffectiveness of these techniques, first, as a preliminary check, on two\nstandard tasks (Blackjack and $n$-Chain), and then on two much larger and more\nrealistic tasks with high-dimensional observation spaces. Specifically, we\nintroduce two benchmarks built within the game Minecraft where the observations\nare pixel arrays of the agent's visual field. A combination of our two\nalgorithmic techniques performs competitively on the standard\nreinforcement-learning tasks while consistently and substantially outperforming\nbaselines on the two tasks with high-dimensional observation spaces. The new\nfunction approximator, exploration strategy, and evaluation benchmarks are each\nof independent interest in the pursuit of reinforcement-learning methods that\nscale to real-world domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:16:25 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Abel", "David", ""], ["Agarwal", "Alekh", ""], ["Diaz", "Fernando", ""], ["Krishnamurthy", "Akshay", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1603.04136", "submitter": "Kun Yuan", "authors": "Kun Yuan, Bicheng Ying, and Ali H. Sayed", "title": "On the Influence of Momentum Acceleration on Online Learning", "comments": "66 pages, 9 figures, to appear in Journal of Machine Learning\n  Research, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article examines in some detail the convergence rate and\nmean-square-error performance of momentum stochastic gradient methods in the\nconstant step-size and slow adaptation regime. The results establish that\nmomentum methods are equivalent to the standard stochastic gradient method with\na re-scaled (larger) step-size value. The size of the re-scaling is determined\nby the value of the momentum parameter. The equivalence result is established\nfor all time instants and not only in steady-state. The analysis is carried out\nfor general strongly convex and smooth risk functions, and is not limited to\nquadratic risks. One notable conclusion is that the well-known bene ts of\nmomentum constructions for deterministic optimization problems do not\nnecessarily carry over to the adaptive online setting when small constant\nstep-sizes are used to enable continuous adaptation and learn- ing in the\npresence of persistent gradient noise. From simulations, the equivalence\nbetween momentum and standard stochastic gradient methods is also observed for\nnon-differentiable and non-convex problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 05:05:54 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 06:27:47 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 23:18:27 GMT"}, {"version": "v4", "created": "Wed, 12 Oct 2016 05:19:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Yuan", "Kun", ""], ["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1603.04153", "submitter": "Changho Suh", "authors": "Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh", "title": "Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is\n  Optimal", "comments": "23 pages, 3 figures, submitted to the Journals of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the top-$K$ rank aggregation problem. Suppose a collection of\nitems is compared in pairs repeatedly, and we aim to recover a consistent\nordering that focuses on the top-$K$ ranked items based on partially revealed\npreference information. We investigate the Bradley-Terry-Luce model in which\none ranks items according to their perceived utilities modeled as noisy\nobservations of their underlying true utilities. Our main contributions are\ntwo-fold. First, in a general comparison model where item pairs to compare are\ngiven a priori, we attain an upper and lower bound on the sample size for\nreliable recovery of the top-$K$ ranked items. Second, more importantly,\nextending the result to a random comparison model where item pairs to compare\nare chosen independently with some probability, we show that in slightly\nrestricted regimes, the gap between the derived bounds reduces to a constant\nfactor, hence reveals that a spectral method can achieve the minimax optimality\non the (order-wise) sample size required for top-$K$ ranking. That is to say,\nwe demonstrate a spectral method alone to be sufficient to achieve the\noptimality and advantageous in terms of computational complexity, as it does\nnot require an additional stage of maximum likelihood estimation that a\nstate-of-the-art scheme employs to achieve the optimality. We corroborate our\nmain results by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 07:01:28 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Jang", "Minje", ""], ["Kim", "Sunghyun", ""], ["Suh", "Changho", ""], ["Oh", "Sewoong", ""]]}, {"id": "1603.04190", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski, Wouter M. Koolen, Alan Malek", "title": "Online Isotonic Regression", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online version of the isotonic regression problem. Given a\nset of linearly ordered points (e.g., on the real line), the learner must\npredict labels sequentially at adversarially chosen positions and is evaluated\nby her total squared loss compared against the best isotonic (non-decreasing)\nfunction in hindsight. We survey several standard online learning algorithms\nand show that none of them achieve the optimal regret exponent; in fact, most\nof them (including Online Gradient Descent, Follow the Leader and Exponential\nWeights) incur linear regret. We then prove that the Exponential Weights\nalgorithm played over a covering net of isotonic functions has a regret bounded\nby $O\\big(T^{1/3} \\log^{2/3}(T)\\big)$ and present a matching $\\Omega(T^{1/3})$\nlower bound on regret. We provide a computationally efficient version of this\nalgorithm. We also analyze the noise-free case, in which the revealed labels\nare isotonic, and show that the bound can be improved to $O(\\log T)$ or even to\n$O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the\nanalysis beyond squared loss and give bounds for entropic loss and absolute\nloss.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:26:23 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 15:52:29 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""], ["Koolen", "Wouter M.", ""], ["Malek", "Alan", ""]]}, {"id": "1603.04245", "submitter": "Andre Wibisono", "authors": "Andre Wibisono, Ashia C. Wilson, Michael I. Jordan", "title": "A Variational Perspective on Accelerated Methods in Optimization", "comments": "38 pages. Subsumes an earlier working draft arXiv:1509.03616", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated gradient methods play a central role in optimization, achieving\noptimal rates in many settings. While many generalizations and extensions of\nNesterov's original acceleration method have been proposed, it is not yet clear\nwhat is the natural scope of the acceleration concept. In this paper, we study\naccelerated methods from a continuous-time perspective. We show that there is a\nLagrangian functional that we call the \\emph{Bregman Lagrangian} which\ngenerates a large class of accelerated methods in continuous time, including\n(but not limited to) accelerated gradient descent, its non-Euclidean extension,\nand accelerated higher-order gradient methods. We show that the continuous-time\nlimit of all of these methods correspond to traveling the same curve in\nspacetime at different speeds. From this perspective, Nesterov's technique and\nmany of its generalizations can be viewed as a systematic way to go from the\ncontinuous-time curves generated by the Bregman Lagrangian to a family of\ndiscrete-time accelerated algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:00:18 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Wibisono", "Andre", ""], ["Wilson", "Ashia C.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1603.04319", "submitter": "Jalal Etesami", "authors": "Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal", "title": "Learning Network of Multivariate Hawkes Processes: A Time Series\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the influence structure of multiple time series data is of great\ninterest to many disciplines. This paper studies the problem of recovering the\ncausal structure in network of multivariate linear Hawkes processes. In such\nprocesses, the occurrence of an event in one process affects the probability of\noccurrence of new events in some other processes. Thus, a natural notion of\ncausality exists between such processes captured by the support of the\nexcitation matrix. We show that the resulting causal influence network is\nequivalent to the Directed Information graph (DIG) of the processes, which\nencodes the causal factorization of the joint distribution of the processes.\nFurthermore, we present an algorithm for learning the support of excitation\nmatrix (or equivalently the DIG). The performance of the algorithm is evaluated\non synthesized multivariate Hawkes networks as well as a stock market and\nMemeTracker real-world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 16:08:26 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""], ["Singhal", "Kushagra", ""]]}, {"id": "1603.04381", "submitter": "C\\'edric Malherbe", "authors": "C\\'edric Malherbe and Nicolas Vayatis", "title": "A ranking approach to global optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing an unknown function over a compact and\nconvex set using as few observations as possible. We observe that the\noptimization of the function essentially relies on learning the induced\nbipartite ranking rule of f. Based on this idea, we relate global optimization\nto bipartite ranking which allows to address problems with high dimensional\ninput space, as well as cases of functions with weak regularity properties. The\npaper introduces novel meta-algorithms for global optimization which rely on\nthe choice of any bipartite ranking method. Theoretical properties are provided\nas well as convergence guarantees and equivalences between various optimization\nmethods are obtained as a by-product. Eventually, numerical evidence is given\nto show that the main algorithm of the paper which adapts empirically to the\nunderlying ranking structure essentially outperforms existing state-of-the-art\nglobal optimization algorithms in typical benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 18:40:54 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 12:54:15 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Malherbe", "C\u00e9dric", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1603.04419", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli", "title": "Modeling and Estimation of Discrete-Time Reciprocal Processes via\n  Probabilistic Graphical Models", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocal processes are acausal generalizations of Markov processes\nintroduced by Bernstein in 1932. In the literature, a significant amount of\nattention has been focused on developing dynamical models for reciprocal\nprocesses. In this paper, we provide a probabilistic graphical model for\nreciprocal processes. This leads to a principled solution of the smoothing\nproblem via message passing algorithms. For the finite state space case,\nconvergence analysis is revisited via the Hilbert metric.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:52:04 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 18:59:18 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 19:57:11 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Carli", "Francesca Paola", ""]]}, {"id": "1603.04549", "submitter": "Vijay Kamble", "authors": "Ramesh Johari, Vijay Kamble and Yash Kanoria", "title": "Matching while Learning", "comments": "This paper has been accepted for publication in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem faced by a service platform that needs to match\nlimited supply with demand but also to learn the attributes of new users in\norder to match them better in the future. We introduce a benchmark model with\nheterogeneous \"workers\" (demand) and a limited supply of \"jobs\" that arrive\nover time. Job types are known to the platform, but worker types are unknown\nand must be learned by observing match outcomes. Workers depart after\nperforming a certain number of jobs. The expected payoff from a match depends\non the pair of types and the goal is to maximize the steady-state rate of\naccumulation of payoff. Though we use terminology inspired by labor markets,\nour framework applies more broadly to platforms where a limited supply of\nheterogeneous products is matched to users over time.\n  Our main contribution is a complete characterization of the structure of the\noptimal policy in the limit that each worker performs many jobs. The platform\nfaces a trade-off for each worker between myopically maximizing payoffs\n(exploitation) and learning the type of the worker (exploration). This creates\na multitude of multi-armed bandit problems, one for each worker, coupled\ntogether by the constraint on availability of jobs of different types (capacity\nconstraints). We find that the platform should estimate a shadow price for each\njob type, and use the payoffs adjusted by these prices, first, to determine its\nlearning goals and then, for each worker, (i) to balance learning with payoffs\nduring the \"exploration phase,\" and (ii) to myopically match after it has\nachieved its learning goals during the \"exploitation phase.\"\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:29:31 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:11:06 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 00:39:01 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 21:36:16 GMT"}, {"version": "v5", "created": "Sat, 7 Dec 2019 18:16:30 GMT"}, {"version": "v6", "created": "Thu, 23 Apr 2020 19:49:49 GMT"}, {"version": "v7", "created": "Wed, 5 Aug 2020 22:17:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Johari", "Ramesh", ""], ["Kamble", "Vijay", ""], ["Kanoria", "Yash", ""]]}, {"id": "1603.04572", "submitter": "Hongbo Dong", "authors": "Hongbo Dong", "title": "On the exact recovery of sparse signals via conic relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we compare two recently proposed semidefinite relaxations for\nthe sparse linear regression problem by Pilanci, Wainwright and El Ghaoui\n(Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth\n(Relaxation vs. Regularization A conic optimization perspective of statistical\nvariable selection, 2015). We focus on the cardinality constrained formulation,\nand prove that the relaxation proposed by Dong, etc. is theoretically no weaker\nthan the one proposed by Pilanci, etc. Therefore any sufficient condition of\nexact recovery derived by Pilanci can be readily applied to the other\nrelaxation, including their results on high probability recovery for Gaussian\nensemble. Finally we provide empirical evidence that the relaxation by Dong,\netc. requires much fewer observations to guarantee the recovery of true\nsupport.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 07:06:33 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Dong", "Hongbo", ""]]}, {"id": "1603.04628", "submitter": "David Stephenson", "authors": "David Stephenson and James R Kermode and Duncan A Lockerby", "title": "Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cond-mat.mes-hall stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid continuum-atomistic scheme which combines molecular\ndynamics (MD) simulations with on-the-fly machine learning techniques for the\naccurate and efficient prediction of multiscale fluidic systems. By using a\nGaussian process as a surrogate model for the computationally expensive MD\nsimulations, we use Bayesian inference to predict the system behaviour at the\natomistic scale, purely by consideration of the macroscopic inputs and outputs.\nWhenever the uncertainty of this prediction is greater than a predetermined\nacceptable threshold, a new MD simulation is performed to continually augment\nthe database, which is never required to be complete. This provides a\nsubstantial enhancement to the current generation of hybrid methods, which\noften require many similar atomistic simulations to be performed, discarding\ninformation after it is used once.\n  We apply our hybrid scheme to nano-confined unsteady flow through a\nhigh-aspect-ratio converging-diverging channel, and make comparisons between\nthe new scheme and full MD simulations for a range of uncertainty thresholds\nand initial databases. For low thresholds, our hybrid solution is highly\naccurate\\,---\\,within the thermal noise of a full MD simulation. As the\nuncertainty threshold is raised, the accuracy of our scheme decreases and the\ncomputational speed-up increases (relative to a full MD simulation), enabling\nthe compromise between precision and efficiency to be tuned. The speed-up of\nour hybrid solution ranges from an order of magnitude, with no initial\ndatabase, to cases where an extensive initial database ensures no new MD\nsimulations are required.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 10:45:15 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Stephenson", "David", ""], ["Kermode", "James R", ""], ["Lockerby", "Duncan A", ""]]}, {"id": "1603.04733", "submitter": "Christos Louizos", "authors": "Christos Louizos and Max Welling", "title": "Structured and Efficient Variational Deep Learning with Matrix Gaussian\n  Posteriors", "comments": "Updated results with the original folds in the regression\n  experiments. Appearing in the International Conference on Machine Learning\n  (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variational Bayesian neural network where the parameters are\ngoverned via a probability distribution on random matrices. Specifically, we\nemploy a matrix variate Gaussian \\cite{gupta1999matrix} parameter posterior\ndistribution where we explicitly model the covariance among the input and\noutput dimensions of each layer. Furthermore, with approximate covariance\nmatrices we can achieve a more efficient way to represent those correlations\nthat is also cheaper than fully factorized parameter posteriors. We further\nshow that with the \"local reprarametrization trick\"\n\\cite{kingma2015variational} on this posterior distribution we arrive at a\nGaussian Process \\cite{rasmussen2006gaussian} interpretation of the hidden\nunits in each layer and we, similarly with \\cite{gal2015dropout}, provide\nconnections with deep Gaussian processes. We continue in taking advantage of\nthis duality and incorporate \"pseudo-data\" \\cite{snelson2005sparse} in our\nmodel, which in turn allows for more efficient sampling while maintaining the\nproperties of the original model. The validity of the proposed approach is\nverified through extensive experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 16:01:14 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 10:21:07 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 11:29:16 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 07:18:12 GMT"}, {"version": "v5", "created": "Thu, 23 Jun 2016 19:03:47 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Louizos", "Christos", ""], ["Welling", "Max", ""]]}, {"id": "1603.04833", "submitter": "Anirban Santara", "authors": "Debapriya Maji, Anirban Santara, Pabitra Mitra and Debdoot Sheet", "title": "Ensemble of Deep Convolutional Neural Networks for Learning to Detect\n  Retinal Vessels in Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision impairment due to pathological damage of the retina can largely be\nprevented through periodic screening using fundus color imaging. However the\nchallenge with large scale screening is the inability to exhaustively detect\nfine blood vessels crucial to disease diagnosis. In this work we present a\ncomputational imaging framework using deep and ensemble learning for reliable\ndetection of blood vessels in fundus color images. An ensemble of deep\nconvolutional neural networks is trained to segment vessel and non-vessel areas\nof a color fundus image. During inference, the responses of the individual\nConvNets of the ensemble are averaged to form the final segmentation. In\nexperimental evaluation with the DRIVE database, we achieve the objective of\nvessel detection with maximum average accuracy of 94.7\\% and area under ROC\ncurve of 0.9283.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:40:34 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Maji", "Debapriya", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1603.04882", "submitter": "Qiang Wu", "authors": "Qiang Wu", "title": "Bias Correction for Regularized Regression and its Application in\n  Learning with Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to reduce the bias of ridge regression and\nregularization kernel network. When applied to a single data set the new\nalgorithms have comparable learning performance with the original ones. When\napplied to incremental learning with block wise streaming data the new\nalgorithms are more efficient due to bias reduction. Both theoretical\ncharacterizations and simulation studies are used to verify the effectiveness\nof these new algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 20:46:46 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Wu", "Qiang", ""]]}, {"id": "1603.04904", "submitter": "Roderich Gross", "authors": "Wei Li, Melvin Gauci and Roderich Gross", "title": "Turing learning: a metric-free approach to inferring behavior and its\n  application to swarms", "comments": "camera-ready version", "journal-ref": "Swarm Intelligence, 10(3):211-243, 2016", "doi": "10.1007/s11721-016-0126-1", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Turing Learning, a novel system identification method for\ninferring the behavior of natural or artificial systems. Turing Learning\nsimultaneously optimizes two populations of computer programs, one representing\nmodels of the behavior of the system under investigation, and the other\nrepresenting classifiers. By observing the behavior of the system as well as\nthe behaviors produced by the models, two sets of data samples are obtained.\nThe classifiers are rewarded for discriminating between these two sets, that\nis, for correctly categorizing data samples as either genuine or counterfeit.\nConversely, the models are rewarded for 'tricking' the classifiers into\ncategorizing their data samples as genuine. Unlike other methods for system\nidentification, Turing Learning does not require predefined metrics to quantify\nthe difference between the system and its models. We present two case studies\nwith swarms of simulated robots and prove that the underlying behaviors cannot\nbe inferred by a metric-based system identification method. By contrast, Turing\nLearning infers the behaviors with high accuracy. It also produces a useful\nby-product - the classifiers - that can be used to detect abnormal behavior in\nthe swarm. Moreover, we show that Turing Learning also successfully infers the\nbehavior of physical robot swarms. The results show that collective behaviors\ncan be directly inferred from motion trajectories of individuals in the swarm,\nwhich may have significant implications for the study of animal collectives.\nFurthermore, Turing Learning could prove useful whenever a behavior is not\neasily characterizable using metrics, making it suitable for a wide range of\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 22:20:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 07:37:00 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Li", "Wei", ""], ["Gauci", "Melvin", ""], ["Gross", "Roderich", ""]]}, {"id": "1603.04918", "submitter": "Shahzad Bhatti", "authors": "Shahzad Bhatti, Carolyn Beck, Angelia Nedic", "title": "Data Clustering and Graph Partitioning via Simulated Mixing", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering approaches have led to well-accepted algorithms for\nfinding accurate clusters in a given dataset. However, their application to\nlarge-scale datasets has been hindered by computational complexity of\neigenvalue decompositions. Several algorithms have been proposed in the recent\npast to accelerate spectral clustering, however they compromise on the accuracy\nof the spectral clustering to achieve faster speed. In this paper, we propose a\nnovel spectral clustering algorithm based on a mixing process on a graph.\nUnlike the existing spectral clustering algorithms, our algorithm does not\nrequire computing eigenvectors. Specifically, it finds the equivalent of a\nlinear combination of eigenvectors of the normalized similarity matrix weighted\nwith corresponding eigenvalues. This linear combination is then used to\npartition the dataset into meaningful clusters. Simulations on real datasets\nshow that partitioning datasets based on such linear combinations of\neigenvectors achieves better accuracy than standard spectral clustering methods\nas the number of clusters increase. Our algorithm can easily be implemented in\na distributed setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 23:06:19 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Bhatti", "Shahzad", ""], ["Beck", "Carolyn", ""], ["Nedic", "Angelia", ""]]}, {"id": "1603.04981", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, Patrick Loiseau, Jean Walrand", "title": "An Approximate Dynamic Programming Approach to Adversarial Online\n  Learning", "comments": "There was an error in the statement of Proposition 4.2 in the\n  previous version that is fixed in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approximate dynamic programming (ADP) approach to compute\napproximations of the optimal strategies and of the minimal losses that can be\nguaranteed in discounted repeated games with vector-valued losses. Such games\nprominently arise in the analysis of regret in repeated decision-making in\nadversarial environments, also known as adversarial online learning. At the\ncore of our approach is a characterization of the lower Pareto frontier of the\nset of expected losses that a player can guarantee in these games as the unique\nfixed point of a set-valued dynamic programming operator. When applied to the\nproblem of regret minimization with discounted losses, our approach yields\nalgorithms that achieve markedly improved performance bounds compared to\noff-the-shelf online learning algorithms like Hedge. These results thus suggest\nthe significant potential of ADP-based approaches in adversarial online\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 07:04:24 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:08:00 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 23:42:51 GMT"}, {"version": "v4", "created": "Sun, 7 Jan 2018 23:51:48 GMT"}, {"version": "v5", "created": "Sun, 30 Sep 2018 23:51:16 GMT"}, {"version": "v6", "created": "Mon, 26 Oct 2020 16:55:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kamble", "Vijay", ""], ["Loiseau", "Patrick", ""], ["Walrand", "Jean", ""]]}, {"id": "1603.05060", "submitter": "Mauricio A. \\'Alvarez", "authors": "Edgar A. Valencia and Mauricio A. \\'Alvarez", "title": "Short-term time series prediction using Hilbert space embeddings of\n  autoregressive processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear autoregressive models serve as basic representations of discrete time\nstochastic processes. Different attempts have been made to provide non-linear\nversions of the basic autoregressive process, including different versions\nbased on kernel methods. Motivated by the powerful framework of Hilbert space\nembeddings of distributions, in this paper we apply this methodology for the\nkernel embedding of an autoregressive process of order $p$. By doing so, we\nprovide a non-linear version of an autoregressive process, that shows increased\nperformance over the linear model in highly complex time series. We use the\nmethod proposed for one-step ahead forecasting of different time-series, and\ncompare its performance against other non-linear methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 12:24:24 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Valencia", "Edgar A.", ""], ["\u00c1lvarez", "Mauricio A.", ""]]}, {"id": "1603.05106", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor,\n  Daan Wierstra", "title": "One-Shot Generalization in Deep Generative Models", "comments": "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd\n  International Conference on Machine Learning, JMLR: W&CP volume 48, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 14:10:00 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:57:19 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Danihelka", "Ivo", ""], ["Gregor", "Karol", ""], ["Wierstra", "Daan", ""]]}, {"id": "1603.05152", "submitter": "Kleanthis Malialis", "authors": "Kleanthis Malialis and Jun Wang and Gary Brooks and George Frangou", "title": "Feature Selection as a Multiagent Coordination Problem", "comments": "AAMAS-16 Workshop on Adaptive and Learning Agents (ALA-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets with hundreds to tens of thousands features is the new norm. Feature\nselection constitutes a central problem in machine learning, where the aim is\nto derive a representative set of features from which to construct a\nclassification (or prediction) model for a specific task. Our experimental\nstudy involves microarray gene expression datasets, these are high-dimensional\nand noisy datasets that contain genetic data typically used for distinguishing\nbetween benign or malicious tissues or classifying different types of cancer.\nIn this paper, we formulate feature selection as a multiagent coordination\nproblem and propose a novel feature selection method using multiagent\nreinforcement learning. The central idea of the proposed approach is to\n\"assign\" a reinforcement learning agent to each feature where each agent learns\nto control a single feature, we refer to this approach as MARL. Applying this\nto microarray datasets creates an enormous multiagent coordination problem\nbetween thousands of learning agents. To address the scalability challenge we\napply a form of reward shaping called CLEAN rewards. We compare in total nine\nfeature selection methods, including state-of-the-art methods, and show that\nthe proposed method using CLEAN rewards can significantly scale-up, thus\noutperforming the rest of learning-based methods. We further show that a hybrid\nvariant of MARL achieves the best overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:49:37 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Malialis", "Kleanthis", ""], ["Wang", "Jun", ""], ["Brooks", "Gary", ""], ["Frangou", "George", ""]]}, {"id": "1603.05296", "submitter": "Brendan Ames", "authors": "Aleksis Pirinen and Brendan Ames", "title": "Exact Clustering of Weighted Graphs via Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a model problem for clustering, we consider the densest k-disjoint-clique\nproblem of partitioning a weighted complete graph into k disjoint subgraphs\nsuch that the sum of the densities of these subgraphs is maximized. We\nestablish that such subgraphs can be recovered from the solution of a\nparticular semidefinite relaxation with high probability if the input graph is\nsampled from a distribution of clusterable graphs. Specifically, the\nsemidefinite relaxation is exact if the graph consists of k large disjoint\nsubgraphs, corresponding to clusters, with weight concentrated within these\nsubgraphs, plus a moderate number of outliers. Further, we establish that if\nnoise is weakly obscuring these clusters, i.e, the between-cluster edges are\nassigned very small weights, then we can recover significantly smaller\nclusters. For example, we show that in approximately sparse graphs, where the\nbetween-cluster weights tend to zero as the size n of the graph tends to\ninfinity, we can recover clusters of size polylogarithmic in n. Empirical\nevidence from numerical simulations is also provided to support these\ntheoretical phase transitions to perfect recovery of the cluster structure.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 22:04:28 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 17:52:25 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 02:42:18 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 21:02:29 GMT"}, {"version": "v5", "created": "Fri, 15 Feb 2019 17:40:15 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Pirinen", "Aleksis", ""], ["Ames", "Brendan", ""]]}, {"id": "1603.05305", "submitter": "Junchi Li", "authors": "Chris Junchi Li, Mengdi Wang, Han Liu, Tong Zhang", "title": "Near-Optimal Stochastic Approximation for Online Principal Component\n  Estimation", "comments": "Finalized version (bib and typos updated). To appear in Mathematical\n  Programming", "journal-ref": null, "doi": "10.1007/s10107-017-1182-z", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) has been a prominent tool for\nhigh-dimensional data analysis. Online algorithms that estimate the principal\ncomponent by processing streaming data are of tremendous practical and\ntheoretical interests. Despite its rich applications, theoretical convergence\nanalysis remains largely open. In this paper, we cast online PCA into a\nstochastic nonconvex optimization problem, and we analyze the online PCA\nalgorithm as a stochastic approximation iteration. The stochastic approximation\niteration processes data points incrementally and maintains a running estimate\nof the principal component. We prove for the first time a nearly optimal\nfinite-sample error bound for the online PCA algorithm. Under the subgaussian\nassumption, we show that the finite-sample error bound closely matches the\nminimax information lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 22:35:58 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:01:31 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 05:43:55 GMT"}, {"version": "v4", "created": "Thu, 5 Oct 2017 19:29:51 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Li", "Chris Junchi", ""], ["Wang", "Mengdi", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1603.05359", "submitter": "Shi Zong", "authors": "Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and\n  Branislav Kveton", "title": "Cascading Bandits for Large-Scale Recommendation Problems", "comments": "Accepted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recommender systems recommend a list of items. The user examines the\nlist, from the first item to the last, and often chooses the first attractive\nitem and does not examine the rest. This type of user behavior can be modeled\nby the cascade model. In this work, we study cascading bandits, an online\nlearning variant of the cascade model where the goal is to recommend $K$ most\nattractive items from a large set of $L$ candidate items. We propose two\nalgorithms for solving this problem, which are based on the idea of linear\ngeneralization. The key idea in our solutions is that we learn a predictor of\nthe attraction probabilities of items from their features, as opposing to\nlearning the attraction probability of each item independently as in the\nexisting work. This results in practical learning algorithms whose regret does\nnot depend on the number of items $L$. We bound the regret of one algorithm and\ncomprehensively evaluate the other on a range of recommendation problems. The\nalgorithm performs well and outperforms all baselines.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 05:37:12 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 17:07:26 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Zong", "Shi", ""], ["Ni", "Hao", ""], ["Sung", "Kenny", ""], ["Ke", "Nan Rosemary", ""], ["Wen", "Zheng", ""], ["Kveton", "Branislav", ""]]}, {"id": "1603.05412", "submitter": "Mattia  Zorzi", "authors": "Diego Romeres and Mattia Zorzi and Raffaello Camoriano and Alessandro\n  Chiuso", "title": "Online semi-parametric learning for inverse dynamics modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a semi-parametric algorithm for online learning of a\nrobot inverse dynamics model. It combines the strength of the parametric and\nnon-parametric modeling. The former exploits the rigid body dynamics equa-\ntion, while the latter exploits a suitable kernel function. We provide an\nextensive comparison with other methods from the literature using real data\nfrom the iCub humanoid robot. In doing so we also compare two different\ntechniques, namely cross validation and marginal likelihood optimization, for\nestimating the hyperparameters of the kernel function.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 10:14:27 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 08:24:39 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Romeres", "Diego", ""], ["Zorzi", "Mattia", ""], ["Camoriano", "Raffaello", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1603.05486", "submitter": "Andreas Svensson", "authors": "Andreas Svensson and Thomas B. Sch\\\"on", "title": "A flexible state space model for learning nonlinear dynamical systems", "comments": null, "journal-ref": "Automatica 80(2017), page 189-199", "doi": null, "report-no": null, "categories": "stat.CO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonlinear state-space model with the state transition and\nobservation functions expressed as basis function expansions. The coefficients\nin the basis function expansions are learned from data. Using a connection to\nGaussian processes we also develop priors on the coefficients, for tuning the\nmodel flexibility and to prevent overfitting to data, akin to a Gaussian\nprocess state-space model. The priors can alternatively be seen as a\nregularization, and helps the model in generalizing the data without\nsacrificing the richness offered by the basis function expansion. To learn the\ncoefficients and other unknown parameters efficiently, we tailor an algorithm\nusing state-of-the-art sequential Monte Carlo methods, which comes with\ntheoretical guarantees on the learning. Our approach indicates promising\nresults when evaluated on a classical benchmark as well as real data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 13:51:17 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 09:41:05 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1603.05594", "submitter": "Enmei Tu", "authors": "Enmei Tu, Nikola Kasabov and Jie Yang", "title": "Mapping Temporal Variables into the NeuCube for Improved Pattern\n  Recognition, Predictive Modelling and Understanding of Stream Data", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for an optimized mapping of temporal\nvariables, describing a temporal stream data, into the recently proposed\nNeuCube spiking neural network architecture. This optimized mapping extends the\nuse of the NeuCube, which was initially designed for spatiotemporal brain data,\nto work on arbitrary stream data and to achieve a better accuracy of temporal\npattern recognition, a better and earlier event prediction and a better\nunderstanding of complex temporal stream data through visualization of the\nNeuCube connectivity. The effect of the new mapping is demonstrated on three\nbench mark problems. The first one is early prediction of patient sleep stage\nevent from temporal physiological data. The second one is pattern recognition\nof dynamic temporal patterns of traffic in the Bay Area of California and the\nlast one is the Challenge 2012 contest data set. In all cases the use of the\nproposed mapping leads to an improved accuracy of pattern recognition and event\nprediction and a better understanding of the data when compared to traditional\nmachine learning techniques or spiking neural network reservoirs with arbitrary\nmapping of the variables.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 17:58:48 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Tu", "Enmei", ""], ["Kasabov", "Nikola", ""], ["Yang", "Jie", ""]]}, {"id": "1603.05642", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Optimal Black-Box Reductions Between Optimization Objectives", "comments": "new applications of our optimal reductions are obtained in this\n  version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diverse world of machine learning applications has given rise to a\nplethora of algorithms and optimization methods, finely tuned to the specific\nregression or classification task at hand. We reduce the complexity of\nalgorithm design for machine learning by reductions: we develop reductions that\ntake a method developed for one setting and apply it to the entire spectrum of\nsmoothness and strong-convexity in applications.\n  Furthermore, unlike existing results, our new reductions are OPTIMAL and more\nPRACTICAL. We show how these new reductions give rise to new and faster running\ntimes on training linear classifiers for various families of loss functions,\nand conclude with experiments showing their successes also in practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:51:59 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 05:11:42 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 17:03:15 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05643", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Variance Reduction for Faster Non-Convex Optimization", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem in non-convex optimization of efficiently\nreaching a stationary point. In contrast to the convex case, in the long\nhistory of this basic problem, the only known theoretical results on\nfirst-order non-convex optimization remain to be full gradient descent that\nconverges in $O(1/\\varepsilon)$ iterations for smooth objectives, and\nstochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations\nfor objectives that are sum of smooth functions.\n  We provide the first improvement in this line of research. Our result is\nbased on the variance reduction trick recently introduced to convex\noptimization, as well as a brand new analysis of variance reduction that is\nsuitable for non-convex optimization. For objectives that are sum of smooth\nfunctions, our first-order minibatch stochastic method converges with an\n$O(1/\\varepsilon)$ rate, and is faster than full gradient descent by\n$\\Omega(n^{1/3})$.\n  We demonstrate the effectiveness of our methods on empirical risk\nminimizations with non-convex loss functions and training neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:55:12 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 02:34:00 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05691", "submitter": "Gregor Urban", "authors": "Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan,\n  Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose and Matt\n  Richardson", "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, they do. This paper provides the first empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to\ntrain accurate models on CIFAR-10 unless the student models contain multiple\nlayers of convolution. Although the student models do not have to be as deep as\nthe teacher model they mimic, the students need multiple convolutional layers\nto learn functions of comparable accuracy as the deep convolutional teacher.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 21:10:38 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 02:40:37 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 08:24:34 GMT"}, {"version": "v4", "created": "Sat, 4 Mar 2017 00:24:45 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Urban", "Gregor", ""], ["Geras", "Krzysztof J.", ""], ["Kahou", "Samira Ebrahimi", ""], ["Aslan", "Ozlem", ""], ["Wang", "Shengjie", ""], ["Caruana", "Rich", ""], ["Mohamed", "Abdelrahman", ""], ["Philipose", "Matthai", ""], ["Richardson", "Matt", ""]]}, {"id": "1603.05729", "submitter": "Bai Jiang", "authors": "Bai Jiang, Tung-Yu Wu, Yifan Jin, Wing H. Wong", "title": "Convergence of Contrastive Divergence Algorithm in Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Contrastive Divergence (CD) algorithm has achieved notable success in\ntraining energy-based models including Restricted Boltzmann Machines and played\na key role in the emergence of deep learning. The idea of this algorithm is to\napproximate the intractable term in the exact gradient of the log-likelihood\nfunction by using short Markov chain Monte Carlo (MCMC) runs. The approximate\ngradient is computationally-cheap but biased. Whether and why the CD algorithm\nprovides an asymptotically consistent estimate are still open questions. This\npaper studies the asymptotic properties of the CD algorithm in canonical\nexponential families, which are special cases of the energy-based model.\nSuppose the CD algorithm runs $m$ MCMC transition steps at each iteration $t$\nand iteratively generates a sequence of parameter estimates $\\{\\theta_t\\}_{t\n\\ge 0}$ given an i.i.d. data sample $\\{X_i\\}_{i=1}^n \\sim p_{\\theta_\\star}$.\nUnder conditions which are commonly obeyed by the CD algorithm in practice, we\nprove the existence of some bounded $m$ such that any limit point of the time\naverage $\\left. \\sum_{s=0}^{t-1} \\theta_s \\right/ t$ as $t \\to \\infty$ is a\nconsistent estimate for the true parameter $\\theta_\\star$. Our proof is based\non the fact that $\\{\\theta_t\\}_{t \\ge 0}$ is a homogenous Markov chain\nconditional on the data sample $\\{X_i\\}_{i=1}^n$. This chain meets the\nFoster-Lyapunov drift criterion and converges to a random walk around the\nMaximum Likelihood Estimate. The range of the random walk shrinks to zero at\nrate $\\mathcal{O}(1/\\sqrt[3]{n})$ as the sample size $n \\to \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 23:48:15 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 07:23:25 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 22:25:20 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Jiang", "Bai", ""], ["Wu", "Tung-Yu", ""], ["Jin", "Yifan", ""], ["Wong", "Wing H.", ""]]}, {"id": "1603.05770", "submitter": "Wei Xiao", "authors": "Wei Xiao", "title": "A Probabilistic Machine Learning Approach to Detect Industrial Plant\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault detection in industrial plants is a hot research area as more and more\nsensor data are being collected throughout the industrial process. Automatic\ndata-driven approaches are widely needed and seen as a promising area of\ninvestment. This paper proposes an effective machine learning algorithm to\npredict industrial plant faults based on classification methods such as\npenalized logistic regression, random forest and gradient boosted tree. A\nfault's start time and end time are predicted sequentially in two steps by\nformulating the original prediction problems as classification problems. The\nalgorithms described in this paper won first place in the Prognostics and\nHealth Management Society 2015 Data Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 05:31:12 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Xiao", "Wei", ""]]}, {"id": "1603.05800", "submitter": "Zhiyun Lu", "authors": "Zhiyun Lu, Dong Guo, Alireza Bagheri Garakani, Kuan Liu, Avner May,\n  Aurelien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael\n  Picheny, Fei Sha", "title": "A Comparison between Deep Neural Nets and Kernel Acoustic Models for\n  Speech Recognition", "comments": "arXiv admin note: text overlap with arXiv:1411.4000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale kernel methods for acoustic modeling and compare to DNNs\non performance metrics related to both acoustic modeling and recognition.\nMeasuring perplexity and frame-level classification accuracy, kernel-based\nacoustic models are as effective as their DNN counterparts. However, on\ntoken-error-rates DNN models can be significantly better. We have discovered\nthat this might be attributed to DNN's unique strength in reducing both the\nperplexity and the entropy of the predicted posterior probabilities. Motivated\nby our findings, we propose a new technique, entropy regularized perplexity,\nfor model selection. This technique can noticeably improve the recognition\nperformance of both types of models, and reduces the gap between them. While\neffective on Broadcast News, this technique could be also applicable to other\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 09:16:01 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Lu", "Zhiyun", ""], ["Guo", "Dong", ""], ["Garakani", "Alireza Bagheri", ""], ["Liu", "Kuan", ""], ["May", "Avner", ""], ["Bellet", "Aurelien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1603.05876", "submitter": "Saverio Salzo", "authors": "Saverio Salzo and Johan A.K. Suykens", "title": "Generalized support vector regression: duality and tensor-kernel\n  representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the variational problem associated to support vector\nregression in Banach function spaces. Using the Fenchel-Rockafellar duality\ntheory, we give explicit formulation of the dual problem as well as of the\nrelated optimality conditions. Moreover, we provide a new computational\nframework for solving the problem which relies on a tensor-kernel\nrepresentation. This analysis overcomes the typical difficulties connected to\nlearning in Banach spaces. We finally present a large class of tensor-kernels\nto which our theory fully applies: power series tensor kernels. This type of\nkernels describe Banach spaces of analytic functions and include\ngeneralizations of the exponential and polynomial kernels as well as, in the\ncomplex case, generalizations of the Szeg\\\"o and Bergman kernels.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 13:53:29 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 10:19:10 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Salzo", "Saverio", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1603.05953", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "comments": "Version 6. Appeared in Symposium on Theory of Computing (STOC 2017)\n  and Journal of Machine Learning Research (JMLR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nesterov's momentum trick is famously known for accelerating gradient\ndescent, and has been proven useful in building fast iterative algorithms.\nHowever, in the stochastic setting, counterexamples exist and prevent\nNesterov's momentum from providing similar acceleration, even if the underlying\nproblem is convex and finite-sum.\n  We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient\nmethod to fix this issue. In convex finite-sum stochastic optimization,\n$\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an\noptimal parallel linear speedup in the mini-batch setting.\n  The main ingredient is $\\textit{Katyusha momentum}$, a novel \"negative\nmomentum\" on top of Nesterov's momentum. It can be incorporated into a\nvariance-reduction based algorithm and speed it up, both in terms of\n$\\textit{sequential and parallel}$ performance. Since variance reduction has\nbeen successfully applied to a growing list of practical problems, our paper\nsuggests that in each of such cases, one could potentially try to give Katyusha\na hug.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 18:46:05 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 18:13:51 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 02:12:23 GMT"}, {"version": "v4", "created": "Sun, 21 Aug 2016 21:49:29 GMT"}, {"version": "v5", "created": "Tue, 2 May 2017 06:05:45 GMT"}, {"version": "v6", "created": "Mon, 24 Sep 2018 04:49:31 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1603.06002", "submitter": "Patrick Eschenfeldt", "authors": "Patrick Eschenfeldt and David Gamarnik", "title": "A Message Passing Algorithm for the Problem of Path Packing in Graphs", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of packing node-disjoint directed paths in a directed\ngraph. We consider a variant of this problem where each path starts within a\nfixed subset of root nodes, subject to a given bound on the length of paths.\nThis problem is motivated by the so-called kidney exchange problem, but has\npotential other applications and is interesting in its own right.\n  We propose a new algorithm for this problem based on the message\npassing/belief propagation technique. A priori this problem does not have an\nassociated graphical model, so in order to apply a belief propagation algorithm\nwe provide a novel representation of the problem as a graphical model. Standard\nbelief propagation on this model has poor scaling behavior, so we provide an\nefficient implementation that significantly decreases the complexity. We\nprovide numerical results comparing the performance of our algorithm on both\nartificially created graphs and real world networks to several alternative\nalgorithms, including algorithms based on integer programming (IP) techniques.\nThese comparisons show that our algorithm scales better to large instances than\nIP-based algorithms and often finds better solutions than a simple algorithm\nthat greedily selects the longest path from each root node. In some cases it\nalso finds better solutions than the ones found by IP-based algorithms even\nwhen the latter are allowed to run significantly longer than our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 21:26:59 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Eschenfeldt", "Patrick", ""], ["Gamarnik", "David", ""]]}, {"id": "1603.06035", "submitter": "Shihua Zhang", "authors": "Wenwen Min, Juan Liu, Shihua Zhang", "title": "L0-norm Sparse Graph-regularized SVD for Biclustering", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning the \"blocking\" structure is a central challenge for high dimensional\ndata (e.g., gene expression data). Recently, a sparse singular value\ndecomposition (SVD) has been used as a biclustering tool to achieve this goal.\nHowever, this model ignores the structural information between variables (e.g.,\ngene interaction graph). Although typical graph-regularized norm can\nincorporate such prior graph information to get accurate discovery and better\ninterpretability, it fails to consider the opposite effect of variables with\ndifferent signs. Motivated by the development of sparse coding and\ngraph-regularized norm, we propose a novel sparse graph-regularized SVD as a\npowerful biclustering tool for analyzing high-dimensional data. The key of this\nmethod is to impose two penalties including a novel graph-regularized norm\n($|\\pmb{u}|\\pmb{L}|\\pmb{u}|$) and $L_0$-norm ($\\|\\pmb{u}\\|_0$) on singular\nvectors to induce structural sparsity and enhance interpretability. We design\nan efficient Alternating Iterative Sparse Projection (AISP) algorithm to solve\nit. Finally, we apply our method and related ones to simulated and real data to\nshow its efficiency in capturing natural blocking structures.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 02:53:48 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Min", "Wenwen", ""], ["Liu", "Juan", ""], ["Zhang", "Shihua", ""]]}, {"id": "1603.06038", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov and Ivan Oseledets", "title": "Tensor Methods and Recommender Systems", "comments": "Submitted to WIREs Data Mining and Knowledge Discovery. 41 page, 3\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial progress in development of new and efficient tensor\nfactorization techniques has led to an extensive research of their\napplicability in recommender systems field. Tensor-based recommender models\npush the boundaries of traditional collaborative filtering techniques by taking\ninto account a multifaceted nature of real environments, which allows to\nproduce more accurate, situational (e.g. context-aware, criteria-driven)\nrecommendations. Despite the promising results, tensor-based methods are poorly\ncovered in existing recommender systems surveys. This survey aims to complement\nprevious works and provide a comprehensive overview on the subject. To the best\nof our knowledge, this is the first attempt to consolidate studies from various\napplication domains in an easily readable, digestible format, which helps to\nget a notion of the current state of the field. We also provide a high level\ndiscussion of the future perspectives and directions for further improvement of\ntensor-based recommendation systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:38:47 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 14:44:44 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1603.06125", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "The Computational Power of Dynamic Bayesian Networks", "comments": null, "journal-ref": "Proceedings of the 4th International Workshop on Artificial\n  Intelligence and Cognition co-located with the Joint Multi-Conference on\n  Human-Level Artificial Intelligence (HLAI 2016) 158-166", "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the computational power of constant size, dynamic\nBayesian networks. Although discrete dynamic Bayesian networks are no more\npowerful than hidden Markov models, dynamic Bayesian networks with continuous\nrandom variables and discrete children of continuous parents are capable of\nperforming Turing-complete computation. With modified versions of existing\nalgorithms for belief propagation, such a simulation can be carried out in real\ntime. This result suggests that dynamic Bayesian networks may be more powerful\nthan previously considered. Relationships to causal models and recurrent neural\nnetworks are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:30:02 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}, {"id": "1603.06159", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Fast Incremental Method for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a fast incremental aggregated gradient method for optimizing\nnonconvex problems of the form $\\min_x \\sum_i f_i(x)$. Specifically, we analyze\nthe SAGA algorithm within an Incremental First-order Oracle framework, and show\nthat it converges to a stationary point provably faster than both gradient\ndescent and stochastic gradient descent. We also discuss a Polyak's special\nclass of nonconvex problems for which SAGA converges at a linear rate to the\nglobal optimum. Finally, we analyze the practically valuable regularized and\nminibatch variants of SAGA. To our knowledge, this paper presents the first\nanalysis of fast convergence for an incremental aggregated gradient method for\nnonconvex problems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:28:44 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1603.06160", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Stochastic Variance Reduction for Nonconvex Optimization", "comments": "Minor feedback changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonconvex finite-sum problems and analyze stochastic variance\nreduced gradient (SVRG) methods for them. SVRG and related methods have\nrecently surged into prominence for convex optimization given their edge over\nstochastic gradient descent (SGD); but their theoretical analysis almost\nexclusively assumes convexity. In contrast, we prove non-asymptotic rates of\nconvergence (to stationary points) of SVRG for nonconvex optimization, and show\nthat it is provably faster than SGD and gradient descent. We also analyze a\nsubclass of nonconvex problems on which SVRG attains linear convergence to the\nglobal optimum. We extend our analysis to mini-batch variants of SVRG, showing\n(theoretical) linear speedup due to mini-batching in parallel settings.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:37:38 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 23:08:20 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Hefny", "Ahmed", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1603.06170", "submitter": "Zhijian Ou", "authors": "Haotian Xu, Zhijian Ou", "title": "Joint Stochastic Approximation learning of Helmholtz Machines", "comments": "Fixing typos. Published at ICLR-2016 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though with progress, model learning and performing posterior inference still\nremains a common challenge for using deep generative models, especially for\nhandling discrete hidden variables. This paper is mainly concerned with\nalgorithms for learning Helmholz machines, which is characterized by pairing\nthe generative model with an auxiliary inference model. A common drawback of\nprevious learning algorithms is that they indirectly optimize some bounds of\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\nnew class of algorithms, based on stochastic approximation (SA) theory of the\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\nJSA's performance is consistently superior to that of competing algorithms like\nRWS, for learning a range of difficult models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 00:55:06 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 13:16:25 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Xu", "Haotian", ""], ["Ou", "Zhijian", ""]]}, {"id": "1603.06186", "submitter": "Horace Pan", "authors": "Risi Kondor, Horace Pan", "title": "The Multiscale Laplacian Graph Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world graphs, such as the graphs of molecules, exhibit structure at\nmultiple different scales, but most existing kernels between graphs are either\npurely local or purely global in character. In contrast, by building a\nhierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG\nkernels) that we define in this paper can account for structure at a range of\ndifferent scales. At the heart of the MLG construction is another new graph\nkernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has\nthe property that it can lift a base kernel defined on the vertices of two\ngraphs to a kernel between the graphs. The MLG kernel applies such FLG kernels\nto subgraphs recursively. To make the MLG kernel computationally feasible, we\nalso introduce a randomized projection procedure, similar to the Nystr\\\"om\nmethod, but for RKHS operators.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 06:33:43 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 07:29:29 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Kondor", "Risi", ""], ["Pan", "Horace", ""]]}, {"id": "1603.06202", "submitter": "Siddartha Ghoshal", "authors": "Sid Ghoshal, Stephen Roberts", "title": "Extracting Predictive Information from Heterogeneous Data Streams using\n  Gaussian Processes", "comments": "15 pages, 5 figures, accepted for publication in Algorithmic Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are notoriously complex environments, presenting vast\namounts of noisy, yet potentially informative data. We consider the problem of\nforecasting financial time series from a wide range of information sources\nusing online Gaussian Processes with Automatic Relevance Determination (ARD)\nkernels. We measure the performance gain, quantified in terms of Normalised\nRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson\ncorrelation, from fusing each of four separate data domains: time series\ntechnicals, sentiment analysis, options market data and broker recommendations.\nWe show evidence that ARD kernels produce meaningful feature rankings that help\nretain salient inputs and reduce input dimensionality, providing a framework\nfor sifting through financial complexity. We measure the performance gain from\nfusing each domain's heterogeneous data streams into a single probabilistic\nmodel. In particular our findings highlight the critical value of options data\nin mapping out the curvature of price space and inspire an intuitive, novel\ndirection for research in financial prediction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 11:11:54 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 17:06:47 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Ghoshal", "Sid", ""], ["Roberts", "Stephen", ""]]}, {"id": "1603.06277", "submitter": "Matthew Johnson", "authors": "Matthew J. Johnson and David Duvenaud and Alexander B. Wiltschko and\n  Sandeep R. Datta and Ryan P. Adams", "title": "Composing graphical models with neural networks for structured\n  representations and fast inference", "comments": "v5 fixes tex compilation bugs and also a math bug in the statement\n  and proof of Prop. 4.1 (and D.3). v4 adds two paragraphs to the related work\n  section and fixes typos in the appendices. v3 fixes some typos in the\n  appendices. v2 is a rewrite from v1 to be more readable and to include\n  detailed appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general modeling and inference framework that composes\nprobabilistic graphical models with deep learning methods and combines their\nrespective strengths. Our model family augments graphical structure in latent\nvariables with neural network observation models. For inference, we extend\nvariational autoencoders to use graphical model approximating distributions\nwith recognition networks that output conjugate potentials. All components of\nthese models are learned simultaneously with a single objective, giving a\nscalable algorithm that leverages stochastic variational inference, natural\ngradients, graphical model message passing, and the reparameterization trick.\nWe illustrate this framework with several example models and an application to\nmouse behavioral phenotyping.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 22:01:02 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 22:13:54 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 17:26:33 GMT"}, {"version": "v4", "created": "Tue, 25 Apr 2017 18:33:36 GMT"}, {"version": "v5", "created": "Fri, 7 Jul 2017 16:00:42 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Johnson", "Matthew J.", ""], ["Duvenaud", "David", ""], ["Wiltschko", "Alexander B.", ""], ["Datta", "Sandeep R.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1603.06288", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff\n  Schneider, Barnabas Poczos", "title": "Multi-fidelity Gaussian Process Bandit Optimisation", "comments": "Preliminary version appeared at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific and engineering applications, we are tasked with the\nmaximisation of an expensive to evaluate black box function $f$. Traditional\nsettings for this problem assume just the availability of this single function.\nHowever, in many cases, cheap approximations to $f$ may be obtainable. For\nexample, the expensive real world behaviour of a robot can be approximated by a\ncheap computer simulation. We can use these approximations to eliminate low\nfunction value regions cheaply and use the expensive evaluations of $f$ in a\nsmall but promising region and speedily identify the optimum. We formalise this\ntask as a \\emph{multi-fidelity} bandit problem where the target function and\nits approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a\nnovel method based on upper confidence bound techniques. In our theoretical\nanalysis we demonstrate that it exhibits precisely the above behaviour, and\nachieves better regret than strategies which ignore multi-fidelity information.\nEmpirically, MF-GP-UCB outperforms such naive strategies and other\nmulti-fidelity methods on several synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 22:58:43 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 00:29:30 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 01:25:57 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 18:05:28 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Dasarathy", "Gautam", ""], ["Oliva", "Junier B.", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1603.06313", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis, Bubacarr Bah, Rouzbeh Hasheminezhad, Quoc\n  Tran-Dinh, Luca Baldassarre, Volkan Cevher", "title": "Convex block-sparse linear regression with expanders -- provably", "comments": "12 pages, 6 figures, to appear at AISTATS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrices are favorable objects in machine learning and optimization.\nWhen such matrices are used, in place of dense ones, the overall complexity\nrequirements in optimization can be significantly reduced in practice, both in\nterms of space and run-time. Prompted by this observation, we study a convex\noptimization scheme for block-sparse recovery from linear measurements. To\nobtain linear sketches, we use expander matrices, i.e., sparse matrices\ncontaining only few non-zeros per column. Hitherto, to the best of our\nknowledge, such algorithmic solutions have been only studied from a non-convex\nperspective. Our aim here is to theoretically characterize the performance of\nconvex approaches under such setting.\n  Our key novelty is the expression of the recovery error in terms of the\nmodel-based norm, while assuring that solution lives in the model. To achieve\nthis, we show that sparse model-based matrices satisfy a group version of the\nnull-space property. Our experimental findings on synthetic and real\napplications support our claims for faster recovery in the convex setting -- as\nopposed to using dense sensing matrices, while showing a competitive recovery\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 02:34:26 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 03:41:42 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Bah", "Bubacarr", ""], ["Hasheminezhad", "Rouzbeh", ""], ["Tran-Dinh", "Quoc", ""], ["Baldassarre", "Luca", ""], ["Cevher", "Volkan", ""]]}, {"id": "1603.06318", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing", "title": "Harnessing Deep Neural Networks with Logic Rules", "comments": "Fix typos in appendix. ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 03:33:20 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 05:28:21 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 23:30:48 GMT"}, {"version": "v4", "created": "Tue, 15 Nov 2016 21:41:21 GMT"}, {"version": "v5", "created": "Tue, 26 Mar 2019 05:16:10 GMT"}, {"version": "v6", "created": "Sat, 8 Aug 2020 07:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hu", "Zhiting", ""], ["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""], ["Xing", "Eric", ""]]}, {"id": "1603.06340", "submitter": "Stefan Wager", "authors": "Stefan Wager, William Fithian, and Percy Liang", "title": "Data Augmentation via Levy Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a document is about travel, we may expect that short snippets of the\ndocument should also be about travel. We introduce a general framework for\nincorporating these types of invariances into a discriminative classifier. The\nframework imagines data as being drawn from a slice of a Levy process. If we\nslice the Levy process at an earlier point in time, we obtain additional\npseudo-examples, which can be used to train the classifier. We show that this\nscheme has two desirable properties: it preserves the Bayes decision boundary,\nand it is equivalent to fitting a generative model in the limit where we rewind\ntime back to 0. Our construction captures popular schemes such as Gaussian\nfeature noising and dropout training, as well as admitting new generalizations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 07:13:51 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Wager", "Stefan", ""], ["Fithian", "William", ""], ["Liang", "Percy", ""]]}, {"id": "1603.06541", "submitter": "Ping Li", "authors": "Ping Li", "title": "A Comparison Study of Nonlinear Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF\n(folded RBF), acos, and acos-$\\chi^2$, on a wide range of publicly available\ndatasets. The proposed fRBF kernel performs very similarly to the RBF kernel.\nBoth RBF and fRBF kernels require an important tuning parameter ($\\gamma$).\nInterestingly, for a significant portion of the datasets, the min-max kernel\noutperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\\chi^2$\nkernel also perform well in general and in some datasets achieve the best\naccuracies.\n  One crucial issue with the use of nonlinear kernels is the excessive\ncomputational and memory cost. These days, one increasingly popular strategy is\nto linearize the kernels through various randomization algorithms. In our\nstudy, the randomization method for the min-max kernel demonstrates excellent\nperformance compared to the randomization methods for other types of nonlinear\nkernels, measured in terms of the number of nonzero terms in the transformed\ndataset.\n  Our study provides evidence for supporting the use of the min-max kernel and\nthe corresponding randomized linearization method (i.e., the so-called \"0-bit\nCWS\"). Furthermore, the results motivate at least two directions for future\nresearch: (i) To develop new (and linearizable) nonlinear kernels for better\naccuracies; and (ii) To develop better linearization algorithms for improving\nthe current linearization methods for the RBF kernel, the acos kernel, and the\nacos-$\\chi^2$ kernel. One attempt is to combine the min-max kernel with the\nacos kernel or the acos-$\\chi^2$ kernel. The advantages of these two new and\ntuning-free nonlinear kernels are demonstrated vias our extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:11:50 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1603.06560", "submitter": "Lisha Li", "authors": "Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet\n  Talwalkar", "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization", "comments": "Changes: - Updated to JMLR version", "journal-ref": "Journal of Machine Learning Research 18 (2018) 1-52", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of machine learning algorithms depends critically on identifying\na good set of hyperparameters. While recent approaches use Bayesian\noptimization to adaptively select configurations, we focus on speeding up\nrandom search through adaptive resource allocation and early-stopping. We\nformulate hyperparameter optimization as a pure-exploration non-stochastic\ninfinite-armed bandit problem where a predefined resource like iterations, data\nsamples, or features is allocated to randomly sampled configurations. We\nintroduce a novel algorithm, Hyperband, for this framework and analyze its\ntheoretical properties, providing several desirable guarantees. Furthermore, we\ncompare Hyperband with popular Bayesian optimization methods on a suite of\nhyperparameter optimization problems. We observe that Hyperband can provide\nover an order-of-magnitude speedup over our competitor set on a variety of\ndeep-learning and kernel-based learning problems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:51:04 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:04:57 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 19:39:54 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 23:01:43 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Lisha", ""], ["Jamieson", "Kevin", ""], ["DeSalvo", "Giulia", ""], ["Rostamizadeh", "Afshin", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1603.06624", "submitter": "R Devon Hjelm", "authors": "R. Devon Hjelm and Sergey M. Plis and Vince C. Calhoun", "title": "Variational Autoencoders for Feature Detection of Magnetic Resonance\n  Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA), as an approach to the blind\nsource-separation (BSS) problem, has become the de-facto standard in many\nmedical imaging settings. Despite successes and a large ongoing research\neffort, the limitation of ICA to square linear transformations have not been\novercome, so that general INFOMAX is still far from being realized. As an\nalternative, we present feature analysis in medical imaging as a problem solved\nby Helmholtz machines, which include dimensionality reduction and\nreconstruction of the raw data under the same objective, and which recently\nhave overcome major difficulties in inference and learning with deep and\nnonlinear configurations. We demonstrate one approach to training Helmholtz\nmachines, variational auto-encoders (VAE), as a viable approach toward feature\nextraction with magnetic resonance imaging (MRI) data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 21:31:36 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Hjelm", "R. Devon", ""], ["Plis", "Sergey M.", ""], ["Calhoun", "Vince C.", ""]]}, {"id": "1603.06743", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Koh Takeuchi, Tomoharu Iwata, John Shawe-Taylor, Samuel\n  Kaski", "title": "Localized Lasso for High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the localized Lasso, which is suited for learning models that\nare both interpretable and have a high predictive power in problems with high\ndimensionality $d$ and small sample size $n$. More specifically, we consider a\nfunction defined by local sparse models, one at each data point. We introduce\nsample-wise network regularization to borrow strength across the models, and\nsample-wise exclusive group sparsity (a.k.a., $\\ell_{1,2}$ norm) to introduce\ndiversity into the choice of feature sets in the local models. The local models\nare interpretable in terms of similarity of their sparsity patterns. The cost\nfunction is convex, and thus has a globally optimal solution. Moreover, we\npropose a simple yet efficient iterative least-squares based optimization\nprocedure for the localized Lasso, which does not need a tuning parameter, and\nis guaranteed to converge to a globally optimal solution. The solution is\nempirically shown to outperform alternatives for both simulated and genomic\npersonalized medicine data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 11:41:28 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 13:43:21 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 02:15:06 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Yamada", "Makoto", ""], ["Takeuchi", "Koh", ""], ["Iwata", "Tomoharu", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.06785", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Emilia Rejmund, Krzysztof Marasek", "title": "Multi-domain machine translation enhancements by parallel data\n  extraction from comparable corpora", "comments": "parallel corpus, Polish, English, machine learning, comparable\n  corpora, NLP. in Gruszczy\\'nska, Ewa; Le\\'nko-Szyma\\'nska, Agnieszka, red.\n  (2016). Polskoj\\k{e}zyczne korpusy r\\'ownoleg{\\l}e. Polish-language Parallel\n  Corpora. Warszawa: Instytut Lingwistyki Stosowanej. ISBN: 978-83-935320-4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel texts are a relatively rare language resource, however, they\nconstitute a very useful research material with a wide range of applications.\nThis study presents and analyses new methodologies we developed for obtaining\nsuch data from previously built comparable corpora. The methodologies are\nautomatic and unsupervised which makes them good for large scale research. The\ntask is highly practical as non-parallel multilingual data occur much more\nfrequently than parallel corpora and accessing them is easy, although parallel\nsentences are a considerably more useful resource. In this study, we propose a\nmethod of automatic web crawling in order to build topic-aligned comparable\ncorpora, e.g. based on the Wikipedia or Euronews.com. We also developed new\nmethods of obtaining parallel sentences from comparable data and proposed\nmethods of filtration of corpora capable of selecting inconsistent or only\npartially equivalent translations. Our methods are easily scalable to other\nlanguages. Evaluation of the quality of the created corpora was performed by\nanalysing the impact of their use on statistical machine translation systems.\nExperiments were presented on the basis of the Polish-English language pair for\ntexts from different domains, i.e. lectures, phrasebooks, film dialogues,\nEuropean Parliament proceedings and texts contained medicines leaflets. We also\ntested a second method of creating parallel corpora based on data from\ncomparable corpora which allows for automatically expanding the existing corpus\nof sentences about a given domain on the basis of analogies found between them.\nIt does not require, therefore, having past parallel resources in order to\ntrain a classifier.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:34:28 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Rejmund", "Emilia", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1603.06846", "submitter": "Ruiyu Yang", "authors": "Ruiyu Yang, Yuxiang Jiang, Scott Mathews, Elizabeth A. Housworth,\n  Matthew W. Hahn, Predrag Radivojac", "title": "A new class of metrics for learning on real-valued and structured data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of metrics on sets, vectors, and functions that can be\nused in various stages of data mining, including exploratory data analysis,\nlearning, and result interpretation. These new distance functions unify and\ngeneralize some of the popular metrics, such as the Jaccard and bag distances\non sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance\non integrable functions. We prove that the new metrics are complete and show\nuseful relationships with $f$-divergences for probability distributions. To\nfurther extend our approach to structured objects such as concept hierarchies\nand ontologies, we introduce information-theoretic metrics on directed acyclic\ngraphs drawn according to a fixed probability distribution. We conduct\nempirical investigation to demonstrate intuitive interpretation of the new\nmetrics and their effectiveness on real-valued, high-dimensional, and\nstructured data. Extensive comparative evaluation demonstrates that the new\nmetrics outperformed multiple similarity and dissimilarity functions\ntraditionally used in data mining, including the Minkowski family, the\nfractional $L^p$ family, two $f$-divergences, cosine distance, and two\ncorrelation coefficients. Finally, we argue that the new class of metrics is\nparticularly appropriate for rapid processing of high-dimensional and\nstructured data in distance-based learning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:01:57 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 20:53:53 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 00:20:25 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Ruiyu", ""], ["Jiang", "Yuxiang", ""], ["Mathews", "Scott", ""], ["Housworth", "Elizabeth A.", ""], ["Hahn", "Matthew W.", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1603.06859", "submitter": "Fabricio de Franca Olivetti", "authors": "Andr\\'e L. V. Coelho and Fabr\\'icio O. de Fran\\c{c}a", "title": "Enhanced perceptrons using contrastive biclusters", "comments": "article under review by Neural Computing and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons are neuronal devices capable of fully discriminating linearly\nseparable classes. Although straightforward to implement and train, their\napplicability is usually hindered by non-trivial requirements imposed by\nreal-world classification problems. Therefore, several approaches, such as\nkernel perceptrons, have been conceived to counteract such difficulties. In\nthis paper, we investigate an enhanced perceptron model based on the notion of\ncontrastive biclusters. From this perspective, a good discriminative bicluster\ncomprises a subset of data instances belonging to one class that show high\ncoherence across a subset of features and high differentiation from nearest\ninstances of the other class under the same features (referred to as its\ncontrastive bicluster). Upon each local subspace associated with a pair of\ncontrastive biclusters a perceptron is trained and the model with highest area\nunder the receiver operating characteristic curve (AUC) value is selected as\nthe final classifier. Experiments conducted on a range of data sets, including\nthose related to a difficult biosignal classification problem, show that the\nproposed variant can be indeed very useful, prevailing in most of the cases\nupon standard and kernel perceptrons in terms of accuracy and AUC measures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:32:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Coelho", "Andr\u00e9 L. V.", ""], ["de Fran\u00e7a", "Fabr\u00edcio O.", ""]]}, {"id": "1603.06861", "submitter": "Anastasios Kyrillidis", "authors": "Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, Sujay\n  Sanghavi", "title": "Trading-off variance and complexity in stochastic gradient descent", "comments": "14 pages, 13 figures, first edition on 9th of October 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent is the method of choice for large-scale machine\nlearning problems, by virtue of its light complexity per iteration. However, it\nlags behind its non-stochastic counterparts with respect to the convergence\nrate, due to high variance introduced by the stochastic updates. The popular\nStochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,\nintroducing a new update rule which requires infrequent passes over the entire\ninput dataset to compute the full-gradient.\n  In this work, we propose CheapSVRG, a stochastic variance-reduction\noptimization scheme. Our algorithm is similar to SVRG but instead of the full\ngradient, it uses a surrogate which can be efficiently computed on a small\nsubset of the input data. It achieves a linear convergence rate ---up to some\nerror level, depending on the nature of the optimization problem---and features\na trade-off between the computational complexity and the convergence rate.\nEmpirical evaluation shows that CheapSVRG performs at least competitively\ncompared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:34:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Shah", "Vatsal", ""], ["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1603.06881", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of\n  Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for aggregating pairwise comparison data in order to\nestimate outcome probabilities for future comparisons among a collection of n\nitems. Working within a flexible framework that imposes only a form of strong\nstochastic transitivity (SST), we introduce an adaptivity index defined by the\nindifference sets of the pairwise comparison probabilities. In addition to\nmeasuring the usual worst-case risk of an estimator, this adaptivity index also\ncaptures the extent to which the estimator adapts to instance-specific\ndifficulty relative to an oracle estimator. We prove three main results that\ninvolve this adaptivity index and different algorithms. First, we propose a\nthree-step estimator termed Count-Randomize-Least squares (CRL), and show that\nit has adaptivity index upper bounded as $\\sqrt{n}$ up to logarithmic factors.\nWe then show that that conditional on the hardness of planted clique, no\ncomputationally efficient estimator can achieve an adaptivity index smaller\nthan $\\sqrt{n}$. Second, we show that a regularized least squares estimator can\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a\n$\\sqrt{n}$-gap between optimal and computationally achievable adaptivity.\nFinally, we prove that the standard least squares estimator, which is known to\nbe optimally adaptive in several closely related problems, fails to adapt in\nthe context of estimating pairwise probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 17:28:08 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1603.06895", "submitter": "Deanne Proctor", "authors": "D. D. Proctor", "title": "A Selection of Giant Radio Sources from NVSS", "comments": "20 pages of text, 6 figures, 22 pages tables, total 55 pages. The\n  stub for Table 6 is followed by the complete machine readable file. To be\n  published in The Astrophysical Journal Supplement. Revision 1: Corrected\n  typos, references updated/corrected, addition to acknowledgments. Five\n  candidates identified as SNR (Thanks to D. A. Green)", "journal-ref": null, "doi": "10.3847/0067-0049/224/2/18", "report-no": "LLNL-JRNL-686322", "categories": "astro-ph.GA cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results of the application of pattern recognition techniques to the problem\nof identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are\npresented and issues affecting the process are explored. Decision-tree pattern\nrecognition software was applied to training set source pairs developed from\nknown NVSS large angular size radio galaxies. The full training set consisted\nof 51,195 source pairs, 48 of which were known GRS for which each lobe was\nprimarily represented by a single catalog component. The source pairs had a\nmaximum separation of 20 arc minutes and a minimum component area of 1.87\nsquare arc minutes at the 1.4 mJy level. The importance of comparing resulting\nprobability distributions of the training and application sets for cases of\nunknown class ratio is demonstrated. The probability of correctly ranking a\nrandomly selected (GRS, non-GRS) pair from the best of the tested classifiers\nwas determined to be 97.8 +/- 1.5%. The best classifiers were applied to the\nover 870,000 candidate pairs from the entire catalog. Images of higher ranked\nsources were visually screened and a table of over sixteen hundred candidates,\nincluding morphological annotation, is presented. These systems include doubles\nand triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped\nsystems, and core-jets and resolved cores. While some resolved lobe systems are\nrecovered with this technique, generally it is expected that such systems would\nrequire a different approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:18:16 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 01:29:20 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Proctor", "D. D.", ""]]}, {"id": "1603.06898", "submitter": "Diana Cai", "authors": "Tamara Broderick and Diana Cai", "title": "Edge-exchangeable graphs and sparsity", "comments": "This paper appeared in the NIPS 2015 Workshop on Networks in the\n  Social and Information Sciences,\n  http://stanford.edu/~jugander/NetworksNIPS2015/. An earlier version appeared\n  in the NIPS 2015 Workshop Bayesian Nonparametrics: The Next Generation,\n  https://sites.google.com/site/nipsbnp2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A known failing of many popular random graph models is that the Aldous-Hoover\nTheorem guarantees these graphs are dense with probability one; that is, the\nnumber of edges grows quadratically with the number of nodes. This behavior is\nconsidered unrealistic in observed graphs. We define a notion of edge\nexchangeability for random graphs in contrast to the established notion of\ninfinite exchangeability for random graphs --- which has traditionally relied\non exchangeability of nodes (rather than edges) in a graph. We show that,\nunlike node exchangeability, edge exchangeability encompasses models that are\nknown to provide a projective sequence of random graphs that circumvent the\nAldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the\nnumber of edges with the number of nodes. We show how edge-exchangeability of\ngraphs relates naturally to existing notions of exchangeability from clustering\n(a.k.a. partitions) and other familiar combinatorial structures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:28:09 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Broderick", "Tamara", ""], ["Cai", "Diana", ""]]}, {"id": "1603.06915", "submitter": "Diana Cai", "authors": "Diana Cai and Tamara Broderick", "title": "Completely random measures for modeling power laws in sparse graphs", "comments": "This paper appeared in the NIPS 2015 Workshop on Networks in the\n  Social and Information Sciences,\n  http://stanford.edu/~jugander/NetworksNIPS2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data appear in a number of applications, such as online social\nnetworks and biological networks, and there is growing interest in both\ndeveloping models for networks as well as studying the properties of such data.\nSince individual network datasets continue to grow in size, it is necessary to\ndevelop models that accurately represent the real-life scaling properties of\nnetworks. One behavior of interest is having a power law in the degree\ndistribution. However, other types of power laws that have been observed\nempirically and considered for applications such as clustering and feature\nallocation models have not been studied as frequently in models for graph data.\nIn this paper, we enumerate desirable asymptotic behavior that may be of\ninterest for modeling graph data, including sparsity and several types of power\nlaws. We outline a general framework for graph generative models using\ncompletely random measures; by contrast to the pioneering work of Caron and Fox\n(2015), we consider instantiating more of the existing atoms of the random\nmeasure as the dataset size increases rather than adding new atoms to the\nmeasure. We see that these two models can be complementary; they respectively\nyield interpretations as (1) time passing among existing members of a network\nand (2) new individuals joining a network. We detail a particular instance of\nthis framework and show simulated results that suggest this model exhibits some\ndesirable asymptotic power-law behavior.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 19:14:55 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Cai", "Diana", ""], ["Broderick", "Tamara", ""]]}, {"id": "1603.06923", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang and Alexander Rakhlin", "title": "Inference via Message Passing on Partially Labeled Stochastic Block\n  Models", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the community detection and recovery problem in partially-labeled\nstochastic block models (SBM). We develop a fast linearized message-passing\nalgorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$\nintra and inter block connectivity) when $\\delta$ proportion of node labels are\nrevealed. The signal-to-noise ratio ${\\sf SNR}(n,k,p,q,\\delta)$ is shown to\ncharacterize the fundamental limitations of inference via local algorithms. On\nthe one hand, when ${\\sf SNR}>1$, the linearized message-passing algorithm\nprovides the statistical inference guarantee with mis-classification rate at\nmost $\\exp(-({\\sf SNR}-1)/2)$, thus interpolating smoothly between strong and\nweak consistency. This exponential dependence improves upon the known error\nrate $({\\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the other\nhand, when ${\\sf SNR}<1$ (for $k=2$) and ${\\sf SNR}<1/4$ (for general growing\n$k$), we prove that local algorithms suffer an error rate at least $\\frac{1}{2}\n- \\sqrt{\\delta \\cdot {\\sf SNR}}$, which is only slightly better than random\nguess for small $\\delta$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 19:30:14 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1603.07094", "submitter": "Kai Morino", "authors": "Motohide Higaki, Kai Morino, Hiroshi Murata, Ryo Asaoka, and Kenji\n  Yamanishi", "title": "Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating\n  Clustering-based Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the issue of predicting the glaucomatous visual field\nloss from patient disease datasets. Our goal is to accurately predict the\nprogress of the disease in individual patients. As very few measurements are\navailable for each patient, it is difficult to produce good predictors for\nindividuals. A recently proposed clustering-based method enhances the power of\nprediction using patient data with similar spatiotemporal patterns. Each\npatient is categorized into a cluster of patients, and a predictive model is\nconstructed using all of the data in the class. Predictions are highly\ndependent on the quality of clustering, but it is difficult to identify the\nbest clustering method. Thus, we propose a method for aggregating cluster-based\npredictors to obtain better prediction accuracy than from a single\ncluster-based prediction. Further, the method shows very high performances by\nhierarchically aggregating experts generated from several cluster-based\nmethods. We use real datasets to demonstrate that our method performs\nsignificantly better than conventional clustering-based and patient-wise\nregression methods, because the hierarchical aggregating strategy has a\nmechanism whereby good predictors in a small community can thrive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 09:06:19 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Higaki", "Motohide", ""], ["Morino", "Kai", ""], ["Murata", "Hiroshi", ""], ["Asaoka", "Ryo", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "1603.07285", "submitter": "Francesco Visin", "authors": "Vincent Dumoulin, Francesco Visin", "title": "A guide to convolution arithmetic for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 17:52:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:54:25 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Visin", "Francesco", ""]]}, {"id": "1603.07292", "submitter": "Shayak Sen", "authors": "Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and\n  Deepak Vijaykeerthy", "title": "Debugging Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional programs (such as operating systems or word processors)\nwhich have large amounts of code, machine learning tasks use programs with\nrelatively small amounts of code (written in machine learning libraries), but\nvoluminous amounts of data. Just like developers of traditional programs debug\nerrors in their code, developers of machine learning tasks debug and fix errors\nin their data. However, algorithms and tools for debugging and fixing errors in\ndata are less common, when compared to their counterparts for detecting and\nfixing errors in code. In this paper, we consider classification tasks where\nerrors in training data lead to misclassifications in test points, and propose\nan automated method to find the root causes of such misclassifications. Our\nroot cause analysis is based on Pearl's theory of causation, and uses Pearl's\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\nencodes the computation of PS as a probabilistic program, and uses recent work\non probabilistic programs and transformations on probabilistic programs (along\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\nPsi is able to identify root causes of data errors in interesting data sets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:30:37 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Chakarov", "Aleksandar", ""], ["Nori", "Aditya", ""], ["Rajamani", "Sriram", ""], ["Sen", "Shayak", ""], ["Vijaykeerthy", "Deepak", ""]]}, {"id": "1603.07294", "submitter": "James Foulds", "authors": "James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri", "title": "On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis", "comments": "Updated to match the accepted UAI version. Generalized the ARE result\n  and included a more detailed proof. Improved some figures, etc", "journal-ref": "Proceedings of the 32nd Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference has great promise for the privacy-preserving analysis of\nsensitive data, as posterior sampling automatically preserves differential\nprivacy, an algorithmic notion of data privacy, under certain conditions\n(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample\n(OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in\nthe sense of asymptotic relative efficiency (ARE). We show that a simple\nalternative based on the Laplace mechanism, the workhorse of differential\nprivacy, is as asymptotically efficient as non-private posterior inference,\nunder general assumptions. This technique also has practical advantages\nincluding efficient use of the privacy budget for MCMC. We demonstrate the\npracticality of our approach on a time-series analysis of sensitive military\nrecords from the Afghanistan and Iraq wars disclosed by the Wikileaks\norganization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:31:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 00:00:10 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Foulds", "James", ""], ["Geumlek", "Joseph", ""], ["Welling", "Max", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1603.07341", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, Yurii Vlasov", "title": "Acceleration of Deep Neural Network Training with Resistive Cross-Point\n  Devices", "comments": "19 pages, 5 figures, 2 tables", "journal-ref": "Front. Neurosci 10, 333 (2016)", "doi": "10.3389/fnins.2016.00333", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNN) have demonstrated significant\nbusiness impact in large scale analysis and classification tasks such as speech\nrecognition, visual object detection, pattern extraction, etc. Training of\nlarge DNNs, however, is universally considered as time consuming and\ncomputationally intensive task that demands datacenter-scale computational\nresources recruited for many days. Here we propose a concept of resistive\nprocessing unit (RPU) devices that can potentially accelerate DNN training by\norders of magnitude while using much less power. The proposed RPU device can\nstore and update the weight values locally thus minimizing data movement during\ntraining and allowing to fully exploit the locality and the parallelism of the\ntraining algorithm. We identify the RPU device and system specifications for\nimplementation of an accelerator chip for DNN training in a realistic\nCMOS-compatible technology. For large DNNs with about 1 billion weights this\nmassively parallel RPU architecture can achieve acceleration factors of 30,000X\ncompared to state-of-the-art microprocessors while providing power efficiency\nof 84,000 GigaOps/s/W. Problems that currently require days of training on a\ndatacenter-size cluster with thousands of machines can be addressed within\nhours on a single RPU accelerator. A system consisted of a cluster of RPU\naccelerators will be able to tackle Big Data problems with trillions of\nparameters that is impossible to address today like, for example, natural\nspeech recognition and translation between all world languages, real-time\nanalytics on large streams of business and scientific data, integration and\nanalysis of multimodal sensory data flows from massive number of IoT (Internet\nof Things) sensors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:13:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Vlasov", "Yurii", ""]]}, {"id": "1603.07394", "submitter": "Diego Klabjan", "authors": "Papis Wongchaisuwat, Diego Klabjan, John O. McGinnis", "title": "Predicting litigation likelihood and time to litigation for patents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patent lawsuits are costly and time-consuming. An ability to forecast a\npatent litigation and time to litigation allows companies to better allocate\nbudget and time in managing their patent portfolios. We develop predictive\nmodels for estimating the likelihood of litigation for patents and the expected\ntime to litigation based on both textual and non-textual features. Our work\nfocuses on improving the state-of-the-art by relying on a different set of\nfeatures and employing more sophisticated algorithms with more realistic data.\nThe rate of patent litigations is very low, which consequently makes the\nproblem difficult. The initial model for predicting the likelihood is further\nmodified to capture a time-to-litigation perspective.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 23:42:02 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Wongchaisuwat", "Papis", ""], ["Klabjan", "Diego", ""], ["McGinnis", "John O.", ""]]}, {"id": "1603.07593", "submitter": "Diego Klabjan", "authors": "Nikhil Byanna, Diego Klabjan", "title": "Evaluating the Performance of Offensive Linemen in the NFL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does one objectively measure the performance of an individual offensive\nlineman in the NFL? The existing literature proposes various measures that rely\non subjective assessments of game film, but has yet to develop an objective\nmethodology to evaluate performance. Using a variety of statistics related to\nan offensive lineman's performance, we develop a framework to objectively\nanalyze the overall performance of an individual offensive lineman and\ndetermine specific linemen who are overvalued or undervalued relative to their\nsalary. We identify eight players across the 2013-2014 and 2014-2015 NFL\nseasons that are considered to be overvalued or undervalued and corroborate the\nresults with existing metrics that are based on subjective evaluation. To the\nbest of our knowledge, the techniques set forth in this work have not been\nutilized in previous works to evaluate the performance of NFL players at any\nposition, including offensive linemen.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:36:51 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 19:22:00 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Byanna", "Nikhil", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07602", "submitter": "Diego Klabjan", "authors": "Alexander Lavin, Diego Klabjan", "title": "Clustering Time-Series Energy Data from Smart Meters", "comments": null, "journal-ref": "Energy Efficiency, July 2015, Volume 8, Issue 4, pp 681-689", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigations have been performed into using clustering methods in data\nmining time-series data from smart meters. The problem is to identify patterns\nand trends in energy usage profiles of commercial and industrial customers over\n24-hour periods, and group similar profiles. We tested our method on energy\nusage data provided by several U.S. power utilities. The results show accurate\ngrouping of accounts similar in their energy usage patterns, and potential for\nthe method to be utilized in energy efficiency programs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:44:52 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Lavin", "Alexander", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07610", "submitter": "Diego Klabjan", "authors": "Anders Drachen, Joseph Riley, Shawna Baskin, Diego Klabjan", "title": "Going Out of Business: Auction House Behavior in the Massively\n  Multi-Player Online Game", "comments": null, "journal-ref": "Entertainment Computing, Vol. 5, No. 4, 2014, p. 219-232", "doi": null, "report-no": null, "categories": "cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in-game economies of massively multi-player online games (MMOGs) are\ncomplex systems that have to be carefully designed and managed. This paper\npresents the results of an analysis of auction house data from the MMOG Glitch,\nacross a 14 month time period, the entire lifetime of the game. The data\ncomprise almost 3 million data points, over 20,000 unique players and more than\n650 products. Furthermore, an interactive visualization, based on Sankey flow\ndiagrams, is presented which shows the proportion of the different clusters\nacross each time bin, as well as the flow of players between clusters. The\ndiagram allows evaluation of migration of players between clusters as a\nfunction of time, as well as churn analysis. The presented work provides a\ntemplate analysis and visualization model for progression-based or\ntemporal-based analysis of player behavior broadly applicable to games.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:00:35 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Drachen", "Anders", ""], ["Riley", "Joseph", ""], ["Baskin", "Shawna", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07624", "submitter": "Diego Klabjan", "authors": "Eun Hee Ko, Diego Klabjan", "title": "Semantic Properties of Customer Sentiment in Tweets", "comments": "The 28th IEEE International Conference on Advanced Information\n  Networking and Applications. Victoria, Canada, 2014", "journal-ref": null, "doi": "10.1109/WAINA.2014.151", "report-no": null, "categories": "cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of people are using online social networking services\n(SNSs), and a significant amount of information related to experiences in\nconsumption is shared in this new media form. Text mining is an emerging\ntechnique for mining useful information from the web. We aim at discovering in\nparticular tweets semantic patterns in consumers' discussions on social media.\nSpecifically, the purposes of this study are twofold: 1) finding similarity and\ndissimilarity between two sets of textual documents that include consumers'\nsentiment polarities, two forms of positive vs. negative opinions and 2)\ndriving actual content from the textual data that has a semantic trend. The\nconsidered tweets include consumers opinions on US retail companies (e.g.,\nAmazon, Walmart). Cosine similarity and K-means clustering methods are used to\nachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic\nmodeling algorithm, is used for the latter purpose. This is the first study\nwhich discover semantic properties of textual data in consumption context\nbeyond sentiment analysis. In addition to major findings, we apply LDA (Latent\nDirichlet Allocations) to the same data and drew latent topics that represent\nconsumers' positive opinions and negative opinions on social media.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:22:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ko", "Eun Hee", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07692", "submitter": "Diego Klabjan", "authors": "Taeheon Jeong, Diego Klabjan, Justin Starren", "title": "Predictive Analytics Using Smartphone Sensors for Depressive Episodes", "comments": "HIAI 2016, Expanding the Boundaries of Health Informatics using AI,\n  Phoenix, AZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behaviors of patients with depression are usually difficult to predict\nbecause the patients demonstrate the symptoms of a depressive episode without a\nwarning at unexpected times. The goal of this research is to build algorithms\nthat detect signals of such unusual moments so that doctors can be proactive in\napproaching already diagnosed patients before they fall in depression. Each\npatient is equipped with a smartphone with the capability to track its sensors.\nWe first find the home location of a patient, which is then augmented with\nother sensor data to identify sleep patterns and select communication patterns.\nThe algorithms require two to three weeks of training data to build standard\npatterns, which are considered normal behaviors; and then, the methods identify\nany anomalies in day-to-day data readings of sensors. Four smartphone sensors,\nincluding the accelerometer, the gyroscope, the location probe and the\ncommunication log probe are used for anomaly detection in sleeping and\ncommunication patterns.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:14:43 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Jeong", "Taeheon", ""], ["Klabjan", "Diego", ""], ["Starren", "Justin", ""]]}, {"id": "1603.07738", "submitter": "Diego Klabjan", "authors": "Anders Drachen, Matthew Yancey, John Maguire, Derrek Chu, Iris Yuhui\n  Wang, Tobias Mahlmann, Matthias Schubert, and Diego Klabjan", "title": "Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of\n  The Ancients 2", "comments": null, "journal-ref": "6th IEEE Consumer Electronics Society Games, Entertainment, Media\n  Conference, Toronto, 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplayer Online Battle Arena (MOBA) games are among the most played\ndigital games in the world. In these games, teams of players fight against each\nother in arena environments, and the gameplay is focused on tactical combat.\nMastering MOBAs requires extensive practice, as is exemplified in the popular\nMOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three\ndata-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2)\nDistribution of team members and: 3) Time series clustering via a fuzzy\napproach. We present a method for obtaining accurate positional data from DotA\n2. We investigate how behavior varies across these measures as a function of\nthe skill level of teams, using four tiers from novice to professional players.\nResults indicate that spatio-temporal behavior of MOBA teams is related to team\nskill, with professional teams having smaller within-team distances and\nconducting more zone changes than amateur teams. The temporal distribution of\nthe within-team distances of professional and high-skilled teams also generally\nfollows patterns distinct from lower skill ranks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 20:09:25 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Drachen", "Anders", ""], ["Yancey", "Matthew", ""], ["Maguire", "John", ""], ["Chu", "Derrek", ""], ["Wang", "Iris Yuhui", ""], ["Mahlmann", "Tobias", ""], ["Schubert", "Matthias", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07749", "submitter": "Xi Luo", "authors": "Yi Zhao, Xi Luo", "title": "Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High\n  Dimensional Mediators", "comments": "26 pages and 7 figures. Presented at the 2016 ENAR meeting, March 8,\n  2016, see slides at\n  https://rluo.github.io/slides/MultipleMediator_ENAR_2016.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific studies, it becomes increasingly important to delineate\nthe causal pathways through a large number of mediators, such as genetic and\nbrain mediators. Structural equation modeling (SEM) is a popular technique to\nestimate the pathway effects, commonly expressed as products of coefficients.\nHowever, it becomes unstable to fit such models with high dimensional\nmediators, especially for a general setting where all the mediators are\ncausally dependent but the exact causal relationships between them are unknown.\nThis paper proposes a sparse mediation model using a regularized SEM approach,\nwhere sparsity here means that a small number of mediators have nonzero\nmediation effects between a treatment and an outcome. To address the model\nselection challenge, we innovate by introducing a new penalty called Pathway\nLasso. This penalty function is a convex relaxation of the non-convex product\nfunction, and it enables a computationally tractable optimization criterion to\nestimate and select many pathway effects simultaneously. We develop a fast\nADMM-type algorithm to compute the model parameters, and we show that the\niterative updates can be expressed in closed form. On both simulated data and a\nreal fMRI dataset, the proposed approach yields higher pathway selection\naccuracy and lower estimation bias than other competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 20:46:24 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1603.07758", "submitter": "Subhaneil Lahiri", "authors": "Subhaneil Lahiri, Jascha Sohl-Dickstein and Surya Ganguli", "title": "A universal tradeoff between power, precision and speed in physical\n  communication", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT physics.bio-ph q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the speed and precision of communication while minimizing power\ndissipation is a fundamental engineering design goal. Also, biological systems\nachieve remarkable speed, precision and power efficiency using poorly\nunderstood physical design principles. Powerful theories like information\ntheory and thermodynamics do not provide general limits on power, precision and\nspeed. Here we go beyond these classical theories to prove that the product of\nprecision and speed is universally bounded by power dissipation in any physical\ncommunication channel whose dynamics is faster than that of the signal.\nMoreover, our derivation involves a novel connection between friction and\ninformation geometry. These results may yield insight into both the engineering\ndesign of communication devices and the structure and function of biological\nsignaling systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 21:10:40 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Lahiri", "Subhaneil", ""], ["Sohl-Dickstein", "Jascha", ""], ["Ganguli", "Surya", ""]]}, {"id": "1603.07834", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher\n  Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam,\n  Soumik Sarkar", "title": "An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection", "comments": "A 10 pages, 8 figures International Conference on Machine\n  Leaning(ICML) Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 07:12:32 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lee", "Nigel", ""], ["Chawla", "Vikas", ""], ["Mullaney", "Mark", ""], ["Marett", "Christopher", ""], ["Singh", "Asheesh", ""], ["Singh", "Arti", ""], ["Tylka", "Greg", ""], ["Ganapathysubramaniam", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07850", "submitter": "Olivier Catoni", "authors": "Olivier Catoni and Thomas Mainguy", "title": "Markov substitute processes : a new model for linguistics and beyond", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Markov substitute processes, a new model at the crossroad of\nstatistics and formal grammars, and prove its main property : Markov substitute\nprocesses with a given support form an exponential family.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:53:29 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Catoni", "Olivier", ""], ["Mainguy", "Thomas", ""]]}, {"id": "1603.07871", "submitter": "Lo\\\"ic Schwaller", "authors": "Lo\\\"ic Schwaller, St\\'ephane Robin", "title": "Exact Bayesian inference for off-line change-point detection in\n  tree-structured graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of change-point detection in multivariate\ntime-series. The multivariate distribution of the observations is supposed to\nfollow a graphical model, whose graph and parameters are affected by abrupt\nchanges throughout time. We demonstrate that it is possible to perform exact\nBayesian inference whenever one considers a simple class of undirected graphs\ncalled spanning trees as possible structures. We are then able to integrate on\nthe graph and segmentation spaces at the same time by combining classical\ndynamic programming with algebraic results pertaining to spanning trees. In\nparticular, we show that quantities such as posterior distributions for\nchange-points or posterior edge probabilities over time can efficiently be\nobtained. We illustrate our results on both synthetic and experimental data\narising from biology and neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 10:43:47 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 20:52:21 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Schwaller", "Lo\u00efc", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1603.07879", "submitter": "Raja Kishor D Mr.", "authors": "D. Raja Kishor, N. B. Venkateswarlu", "title": "Hybridization of Expectation-Maximization and K-Means Algorithms for\n  Better Clustering Performance", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present work proposes hybridization of Expectation-Maximization (EM) and\nK-Means techniques as an attempt to speed-up the clustering process. Though\nboth K-Means and EM techniques look into different areas, K-means can be viewed\nas an approximate way to obtain maximum likelihood estimates for the means.\nAlong with the proposed algorithm for hybridization, the present work also\nexperiments with the Standard EM algorithm. Six different datasets are used for\nthe experiments of which three are synthetic datasets. Clustering fitness and\nSum of Squared Errors (SSE) are computed for measuring the clustering\nperformance. In all the experiments it is observed that the proposed algorithm\nfor hybridization of EM and K-Means techniques is consistently taking less\nexecution time with acceptable Clustering Fitness value and less SSE than the\nstandard EM algorithm. It is also observed that the proposed algorithm is\nproducing better clustering results than the Cluster package of Purdue\nUniversity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:09:22 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Kishor", "D. Raja", ""], ["Venkateswarlu", "N. B.", ""]]}, {"id": "1603.08029", "submitter": "Diogo Almeida", "authors": "Sasha Targ, Diogo Almeida, Kevin Lyman", "title": "Resnet in Resnet: Generalizing Residual Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:55:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Targ", "Sasha", ""], ["Almeida", "Diogo", ""], ["Lyman", "Kevin", ""]]}, {"id": "1603.08035", "submitter": "Horia Mania", "authors": "Horia Mania, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan,\n  Benjamin Recht", "title": "On kernel methods for covariates that are rankings", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation-valued features arise in a variety of applications, either in a\ndirect way when preferences are elicited over a collection of items, or an\nindirect way in which numerical ratings are converted to a ranking. To date,\nthere has been relatively limited study of regression, classification, and\ntesting problems based on permutation-valued features, as opposed to\npermutation-valued responses. This paper studies the use of reproducing kernel\nHilbert space methods for learning from permutation-valued features. These\nmethods embed the rankings into an implicitly defined function space, and allow\nfor efficient estimation of regression and test functions in this richer space.\nOur first contribution is to characterize both the feature spaces and spectral\nproperties associated with two kernels for rankings, the Kendall and Mallows\nkernels. Using tools from representation theory, we explain the limited\nexpressive power of the Kendall kernel by characterizing its degenerate\nspectrum, and in sharp contrast, we prove that Mallows' kernel is universal and\ncharacteristic. We also introduce families of polynomial kernels that\ninterpolate between the Kendall (degree one) and Mallows' (infinite degree)\nkernels. We show the practical effectiveness of our methods via applications to\nEurobarometer survey data as well as a Movielens ratings dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:09:54 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:58:31 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Mania", "Horia", ""], ["Ramdas", "Aaditya", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1603.08048", "submitter": "Michael Ruster", "authors": "Michael Ruster", "title": "\"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles\n  for Deletion Discussions", "comments": "Master's Thesis, Koblenz 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This thesis focuses on gaining linguistic insights into textual discussions\non a word level. It was of special interest to distinguish messages that\nconstructively contribute to a discussion from those that are detrimental to\nthem. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are\nindicators for either of the two discussion styles. These messages are nowadays\noften used in guidelines for successful communication. Although their effects\nhave been successfully evaluated multiple times, a large-scale analysis has\nnever been conducted.\n  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions\ntogether with the records of blocked users and developed a fully automated\ncreation of an annotated data set. In this data set, messages were labelled\neither constructive or disruptive. We applied binary classifiers to the data to\ndetermine characteristic words for both discussion styles. Thereby, we also\ninvestigated whether function words like pronouns and conjunctions play an\nimportant role in distinguishing the two.\n  We found that \"You\"-messages were a strong indicator for disruptive messages\nwhich matches their attributed effects on communication. However, we found\n\"I\"-messages to be indicative for disruptive messages as well which is contrary\nto their attributed effects. The importance of function words could neither be\nconfirmed nor refuted. Other characteristic words for either communication\nstyle were not found. Yet, the results suggest that a different model might\nrepresent disruptive and constructive messages in textual discussions better.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 22:36:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ruster", "Michael", ""]]}, {"id": "1603.08113", "submitter": "Yohann De Castro", "authors": "Yohann De Castro and Thibault Espinasse and Paul Rochet", "title": "Reconstructing undirected graphs from eigenspaces", "comments": "25 pages, some figures. Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at recovering an undirected weighted graph of $N$\nvertices from the knowledge of a perturbed version of the eigenspaces of its\nadjacency matrix $W$. For instance, this situation arises for stationary\nsignals on graphs or for Markov chains observed at random times. Our approach\nis based on minimizing a cost function given by the Frobenius norm of the\ncommutator $\\mathsf{A} \\mathsf{B}-\\mathsf{B} \\mathsf{A}$ between symmetric\nmatrices $\\mathsf{A}$ and $\\mathsf{B}$.\n  In the Erd\\H{o}s-R\\'enyi model with no self-loops, we show that\nidentifiability (i.e., the ability to reconstruct $W$ from the knowledge of its\neigenspaces) follows a sharp phase transition on the expected number of edges\nwith threshold function $N\\log N/2$.\n  Given an estimation of the eigenspaces based on a $n$-sample, we provide\nsupport selection procedures from theoretical and practical point of views. In\nparticular, when deleting an edge from the active support, our study unveils\nthat our test statistic is the order of $\\mathcal O(1/n)$ when we overestimate\nthe true support and lower bounded by a positive constant when the estimated\nsupport is smaller than the true support. This feature leads to a powerful\npractical support estimation procedure. Simulated and real life numerical\nexperiments assert our new methodology.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 14:56:35 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 08:43:21 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 11:31:17 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["De Castro", "Yohann", ""], ["Espinasse", "Thibault", ""], ["Rochet", "Paul", ""]]}, {"id": "1603.08150", "submitter": "John J Nay", "authors": "John J. Nay, Jonathan M. Gilligan", "title": "Data-Driven Dynamic Decision Models", "comments": "Published in the Proceedings of the 2015 Winter Simulation Conference", "journal-ref": "Proceedings of the 2015 Winter Simulation Conference, Pages\n  2752-2763, IEEE Press", "doi": "10.1109/WSC.2015.7408381", "report-no": null, "categories": "stat.ML cs.GT cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article outlines a method for automatically generating models of dynamic\ndecision-making that both have strong predictive power and are interpretable in\nhuman terms. This is useful for designing empirically grounded agent-based\nsimulations and for gaining direct insight into observed dynamic processes. We\nuse an efficient model representation and a genetic algorithm-based estimation\nprocess to generate simple approximations that explain most of the structure of\ncomplex stochastic processes. This method, implemented in C++ and R, scales\nwell to large data sets. We apply our methods to empirical data from human\nsubjects game experiments and international relations. We also demonstrate the\nmethod's ability to recover known data-generating processes by simulating data\nwith agent-based models and correctly deriving the underlying decision models\nfor multiple agent models and degrees of stochasticity.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:45:13 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nay", "John J.", ""], ["Gilligan", "Jonathan M.", ""]]}, {"id": "1603.08163", "submitter": "Farouk Nathoo", "authors": "Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance", "title": "Regularization Parameter Selection for a Bayesian Multi-Level Group\n  Lasso Regression Model with Application to Imaging Genomics", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the choice of tuning parameters for a Bayesian multi-level\ngroup lasso model developed for the joint analysis of neuroimaging and genetic\ndata. The regression model we consider relates multivariate phenotypes\nconsisting of brain summary measures (volumetric and cortical thickness values)\nto single nucleotide polymorphism (SNPs) data and imposes penalization at two\nnested levels, the first corresponding to genes and the second corresponding to\nSNPs. Associated with each level in the penalty is a tuning parameter which\ncorresponds to a hyperparameter in the hierarchical Bayesian formulation.\nFollowing previous work on Bayesian lassos we consider the estimation of tuning\nparameters through either hierarchical Bayes based on hyperpriors and Gibbs\nsampling or through empirical Bayes based on maximizing the marginal likelihood\nusing a Monte Carlo EM algorithm. For the specific model under consideration we\nfind that these approaches can lead to severe overshrinkage of the regression\nparameter estimates in the high-dimensional setting or when the genetic effects\nare weak. We demonstrate these problems through simulation examples and study\nan approximation to the marginal likelihood which sheds light on the cause of\nthis problem. We then suggest an alternative approach based on the widely\napplicable information criterion (WAIC), an asymptotic approximation to\nleave-one-out cross-validation that can be computed conveniently within an MCMC\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 02:34:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Nathoo", "Farouk S.", ""], ["Greenlaw", "Keelin", ""], ["Lesperance", "Mary", ""]]}, {"id": "1603.08232", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn and\n  Khue-Dung Dang", "title": "The block-Poisson estimator for optimally tuned exact subsampling MCMC", "comments": "The main paper is 28 pages. The supplementary material is 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many\nobservations by data subsampling has recently received considerable attention.\nA pseudo-marginal MCMC method is proposed that estimates the likelihood by data\nsubsampling using a block-Poisson estimator. The estimator is a product of\nPoisson estimators, allowing us to update a single block of subsample\nindicators in each MCMC iteration so that a desired correlation is achieved\nbetween the logs of successive likelihood estimates. This is important since\npseudo-marginal MCMC with positively correlated likelihood estimates can use\nsubstantially smaller subsamples without adversely affecting the sampling\nefficiency. The block-Poisson estimator is unbiased but not necessarily\npositive, so the algorithm runs the MCMC on the absolute value of the\nlikelihood estimator and uses an importance sampling correction to obtain\nconsistent estimates of the posterior mean of any function of the parameters.\nOur article derives guidelines to select the optimal tuning parameters for our\nmethod and shows that it compares very favourably to regular MCMC without\nsubsampling, and to two other recently proposed exact subsampling approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:25:34 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 02:38:59 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2017 03:57:16 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 07:05:31 GMT"}, {"version": "v5", "created": "Tue, 10 Apr 2018 07:06:36 GMT"}, {"version": "v6", "created": "Tue, 7 Apr 2020 03:42:46 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Quiroz", "Matias", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""], ["Dang", "Khue-Dung", ""]]}, {"id": "1603.08482", "submitter": "Sida Wang", "authors": "Sida I. Wang and Arun Tejasvi Chaganty and Percy Liang", "title": "Estimating Mixture Models via Mixtures of Polynomials", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture modeling is a general technique for making any simple model more\nexpressive through weighted combination. This generality and simplicity in part\nexplains the success of the Expectation Maximization (EM) algorithm, in which\nupdates are easy to derive for a wide class of mixture models. However, the\nlikelihood of a mixture model is non-convex, so EM has no known global\nconvergence guarantees. Recently, method of moments approaches offer global\nguarantees for some mixture models, but they do not extend easily to the range\nof mixture models that exist. In this work, we present Polymom, an unifying\nframework based on method of moments in which estimation procedures are easily\nderivable, just as in EM. Polymom is applicable when the moments of a single\nmixture component are polynomials of the parameters. Our key observation is\nthat the moments of the mixture model are a mixture of these polynomials, which\nallows us to cast estimation as a Generalized Moment Problem. We solve its\nrelaxations using semidefinite optimization, and then extract parameters using\nideas from computer algebra. This framework allows us to draw insights and\napply tools from convex optimization, computer algebra and the theory of\nmoments to study problems in statistical estimation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:55:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Wang", "Sida I.", ""], ["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1603.08564", "submitter": "Bodhisattwa Majumder", "authors": "Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, and\n  Swagatam Das", "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image\n  Segmentation", "comments": "Journal Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel Kernelized image segmentation scheme for noisy\nimages that utilizes the concept of Smallest Univalue Segment Assimilating\nNucleus (SUSAN) and incorporates spatial constraints by computing circular\ncolour map induced weights. Fuzzy damping coefficients are obtained for each\nnucleus or center pixel on the basis of the corresponding weighted SUSAN area\nvalues, the weights being equal to the inverse of the number of horizontal and\nvertical moves required to reach a neighborhood pixel from the center pixel.\nThese weights are used to vary the contributions of the different nuclei in the\nKernel based framework. The paper also presents an edge quality metric obtained\nby fuzzy decision based edge candidate selection and final computation of the\nblurriness of the edges after their selection. The inability of existing\nalgorithms to preserve edge information and structural details in their\nsegmented maps necessitates the computation of the edge quality factor (EQF)\nfor all the competing algorithms. Qualitative and quantitative analysis have\nbeen rendered with respect to state-of-the-art algorithms and for images ridden\nwith varying types of noises. Speckle noise ridden SAR images and Rician noise\nridden Magnetic Resonance Images have also been considered for evaluating the\neffectiveness of the proposed algorithm in extracting important segmentation\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:09:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Mukherjee", "Satrajit", ""], ["Majumder", "Bodhisattwa Prasad", ""], ["Piplai", "Aritran", ""], ["Das", "Swagatam", ""]]}, {"id": "1603.08578", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos", "title": "Analysis of k-Nearest Neighbor Distances with Application to Entropy\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating entropy and mutual information consistently is important for many\nmachine learning applications. The Kozachenko-Leonenko (KL) estimator\n(Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for the\nentropy of multivariate continuous random variables, as well as the basis of\nthe mutual information estimator of Kraskov et al. (2004), perhaps the most\nwidely used estimator of mutual information in this setting. Despite the\npractical importance of these estimators, major theoretical questions regarding\ntheir finite-sample behavior remain open. This paper proves finite-sample\nbounds on the bias and variance of the KL estimator, showing that it achieves\nthe minimax convergence rate for certain classes of smooth functions. In\nproving these bounds, we analyze finite-sample behavior of k-nearest neighbors\n(k-NN) distance statistics (on which the KL estimator is based). We derive\nconcentration inequalities for k-NN distances and a general expectation bound\nfor statistics of k-NN distances, which may be useful for other analyses of\nk-NN methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 22:14:18 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 17:25:24 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1603.08584", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P \\'oczos", "title": "Exponential Concentration of a Density Functional Estimator", "comments": "In 29th Annual Conference on Neural Information Processing Systems\n  (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a plug-in estimator for a large class of integral functionals of\none or more continuous probability densities. This class includes important\nfamilies of entropy, divergence, mutual information, and their conditional\nversions. For densities on the $d$-dimensional unit cube $[0,1]^d$ that lie in\na $\\beta$-H\\\"older smoothness class, we prove our estimator converges at the\nrate $O \\left( n^{-\\frac{\\beta}{\\beta + d}} \\right)$. Furthermore, we prove the\nestimator is exponentially concentrated about its mean, whereas most previous\nrelated results have proven only expected error bounds on estimators.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 23:01:31 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Singh", "Shashank", ""], ["\u00f3czos", "Barnab\u00e1s P", ""]]}, {"id": "1603.08589", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos", "title": "Generalized Exponential Concentration Inequality for R\\'enyi Divergence\n  Estimation", "comments": "In 31st International Conference on Machine Learning (ICML), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating divergences in a consistent way is of great importance in many\nmachine learning tasks. Although this is a fundamental problem in nonparametric\nstatistics, to the best of our knowledge there has been no finite sample\nexponential inequality convergence bound derived for any divergence estimators.\nThe main contribution of our work is to provide such a bound for an estimator\nof R\\'enyi-$\\alpha$ divergence for a smooth H\\\"older class of densities on the\n$d$-dimensional unit cube $[0, 1]^d$. We also illustrate our theoretical\nresults with a numerical experiment.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 23:17:35 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1603.08616", "submitter": "Lin Chen", "authors": "Lin Chen, Forrest W Crawford, Amin Karbasi", "title": "Submodular Variational Inference for Network Reconstruction", "comments": "Accepted for UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world and online social networks, individuals receive and transmit\ninformation in real time. Cascading information transmissions (e.g. phone\ncalls, text messages, social media posts) may be understood as a realization of\na diffusion process operating on the network, and its branching path can be\nrepresented by a directed tree. The process only traverses and thus reveals a\nlimited portion of the edges. The network reconstruction/inference problem is\nto infer the unrevealed connections. Most existing approaches derive a\nlikelihood and attempt to find the network topology maximizing the likelihood,\na problem that is highly intractable. In this paper, we focus on the network\nreconstruction problem for a broad class of real-world diffusion processes,\nexemplified by a network diffusion scheme called respondent-driven sampling\n(RDS). We prove that under realistic and general models of network diffusion,\nthe posterior distribution of an observed RDS realization is a Bayesian\nlog-submodular model.We then propose VINE (Variational Inference for Network\nrEconstruction), a novel, accurate, and computationally efficient variational\ninference algorithm, for the network reconstruction problem under this model.\nCrucially, we do not assume any particular probabilistic model for the\nunderlying network. VINE recovers any connected graph with high accuracy as\nshown by our experimental results on real-life networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 02:13:17 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 06:58:29 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Lin", ""], ["Crawford", "Forrest W", ""], ["Karbasi", "Amin", ""]]}, {"id": "1603.08661", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "Regret Analysis of the Anytime Optimally Confident UCB Algorithm", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce and analyse an anytime version of the Optimally Confident UCB\n(OCUCB) algorithm designed for minimising the cumulative regret in finite-armed\nstochastic bandits with subgaussian noise. The new algorithm is simple,\nintuitive (in hindsight) and comes with the strongest finite-time regret\nguarantees for a horizon-free algorithm so far. I also show a finite-time lower\nbound that nearly matches the upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 07:12:14 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 19:06:26 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1603.08704", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia", "title": "Interpretability of Multivariate Brain Maps in Brain Decoding:\n  Definition and Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding is a popular multivariate approach for hypothesis testing in\nneuroimaging. It is well known that the brain maps derived from weights of\nlinear classifiers are hard to interpret because of high correlations between\npredictors, low signal to noise ratios, and the high dimensionality of\nneuroimaging data. Therefore, improving the interpretability of brain decoding\napproaches is of primary interest in many neuroimaging studies. Despite\nextensive studies of this type, at present, there is no formal definition for\ninterpretability of multivariate brain maps. As a consequence, there is no\nquantitative measure for evaluating the interpretability of different brain\ndecoding methods. In this paper, first, we present a theoretical definition of\ninterpretability in brain decoding; we show that the interpretability of\nmultivariate brain maps can be decomposed into their reproducibility and\nrepresentativeness. Second, as an application of the proposed theoretical\ndefinition, we formalize a heuristic method for approximating the\ninterpretability of multivariate brain maps in a binary magnetoencephalography\n(MEG) decoding scenario. Third, we propose to combine the approximated\ninterpretability and the performance of the brain decoding model into a new\nmulti-objective criterion for model selection. Our results for the MEG data\nshow that optimizing the hyper-parameters of the regularized linear classifier\nbased on the proposed criterion results in more informative multivariate brain\nmaps. More importantly, the presented definition provides the theoretical\nbackground for quantitative evaluation of interpretability, and hence,\nfacilitates the development of more effective brain decoding algorithms in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:04:07 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Kia", "Seyed Mostafa", ""]]}, {"id": "1603.08708", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Arindam Banerjee, Joydeep Ghosh", "title": "Unified View of Matrix Completion under General Structural Constraints", "comments": "published in NIPS 2015. Advances in Neural Information Processing\n  Systems 28, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a unified analysis of matrix completion under\ngeneral low-dimensional structural constraints induced by {\\em any} norm\nregularization. We consider two estimators for the general problem of\nstructured matrix completion, and provide unified upper bounds on the sample\ncomplexity and the estimation error. Our analysis relies on results from\ngeneric chaining, and we establish two intermediate results of independent\ninterest: (a) in characterizing the size or complexity of low dimensional\nsubsets in high dimensional ambient space, a certain partial complexity measure\nencountered in the analysis of matrix completion problems is characterized in\nterms of a well understood complexity measure of Gaussian widths, and (b) it is\nshown that a form of restricted strong convexity holds for matrix completion\nproblems under general norm regularization. Further, we provide several\nnon-trivial examples of structures included in our framework, notably the\nrecently proposed spectral $k$-support norm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:12:50 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 23:20:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Banerjee", "Arindam", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1603.08785", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (RANDOPT), Anne Auger (RANDOPT), Raymond Ros (TAO),\n  Olaf Mersmann (TU), Tea Tu\\v{s}ar (IJS), Dimo Brockhoff (RANDOPT)", "title": "COCO: A Platform for Comparing Continuous Optimizers in a Black-Box\n  Setting", "comments": "Optimization Methods and Software, Taylor & Francis, In press,\n  pp.1-31", "journal-ref": null, "doi": "10.1080/10556788.2020.1808977", "report-no": null, "categories": "cs.AI cs.MS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce COCO, an open source platform for Comparing Continuous\nOptimizers in a black-box setting. COCO aims at automatizing the tedious and\nrepetitive task of benchmarking numerical optimization algorithms to the\ngreatest possible extent. The platform and the underlying methodology allow to\nbenchmark in the same framework deterministic and stochastic solvers for both\nsingle and multiobjective optimization. We present the rationales behind the\n(decade-long) development of the platform as a general proposition for\nguidelines towards better benchmarking. We detail underlying fundamental\nconcepts of COCO such as the definition of a problem as a function instance,\nthe underlying idea of instances, the use of target values, and runtime defined\nby the number of function calls as the central performance measure. Finally, we\ngive a quick overview of the basic code structure and the currently available\ntest suites.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:18:52 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 06:27:09 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 15:19:31 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 14:41:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Hansen", "Nikolaus", "", "RANDOPT"], ["Auger", "Anne", "", "RANDOPT"], ["Ros", "Raymond", "", "TAO"], ["Mersmann", "Olaf", "", "TU"], ["Tu\u0161ar", "Tea", "", "IJS"], ["Brockhoff", "Dimo", "", "RANDOPT"]]}, {"id": "1603.08813", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir, Jean-Luc Jannink", "title": "Locally Epistatic Models for Genome-wide Prediction and Association by\n  Importance Sampling", "comments": "*Corresponding Author: Deniz Akdemir (denjz.akdemir.work@gmail.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical genetics an important task involves building predictive models\nfor the genotype-phenotype relationships and thus attribute a proportion of the\ntotal phenotypic variance to the variation in genotypes. Numerous models have\nbeen proposed to incorporate additive genetic effects into models for\nprediction or association. However, there is a scarcity of models that can\nadequately account for gene by gene or other forms of genetical interactions.\nIn addition, there is an increased interest in using marker annotations in\ngenome-wide prediction and association. In this paper, we discuss an hybrid\nmodeling methodology which combines the parametric mixed modeling approach and\nthe non-parametric rule ensembles. This approach gives us a flexible class of\nmodels that can be used to capture additive, locally epistatic genetic effects,\ngene x background interactions and allows us to incorporate one or more\nannotations into the genomic selection or association models. We use benchmark\ndata sets covering a range of organisms and traits in addition to simulated\ndata sets to illustrate the strengths of this approach. The improvement of\nmodel accuracies and association results suggest that a part of the \"missing\nheritability\" in complex traits can be captured by modeling local epistasis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:30:46 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Akdemir", "Deniz", ""], ["Jannink", "Jean-Luc", ""]]}, {"id": "1603.08981", "submitter": "Shuang Li", "authors": "Shuang Li, Yao Xie, Mehrdad Farajtabar, Apurv Verma, and Le Song", "title": "Detecting weak changes in dynamic events over networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volume of networked streaming event data are becoming increasingly\navailable in a wide variety of applications, such as social network analysis,\nInternet traffic monitoring and healthcare analytics. Streaming event data are\ndiscrete observation occurred in continuous time, and the precise time interval\nbetween two events carries a great deal of information about the dynamics of\nthe underlying systems. How to promptly detect changes in these dynamic systems\nusing these streaming event data? In this paper, we propose a novel\nchange-point detection framework for multi-dimensional event data over\nnetworks. We cast the problem into sequential hypothesis test, and derive the\nlikelihood ratios for point processes, which are computed efficiently via an\nEM-like algorithm that is parameter-free and can be computed in a distributed\nfashion. We derive a highly accurate theoretical characterization of the\nfalse-alarm-rate, and show that it can achieve weak signal detection by\naggregating local statistics over time and networks. Finally, we demonstrate\nthe good performance of our algorithm on numerical examples and real-world\ndatasets from twitter and Memetracker.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:54:56 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 20:09:56 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Farajtabar", "Mehrdad", ""], ["Verma", "Apurv", ""], ["Song", "Le", ""]]}, {"id": "1603.08988", "submitter": "Yusuf Bugra Erol", "authors": "Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell", "title": "Towards Practical Bayesian Parameter and State Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 22:41:17 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Erol", "Yusuf Bugra", ""], ["Wu", "Yi", ""], ["Li", "Lei", ""], ["Russell", "Stuart", ""]]}, {"id": "1603.09000", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Online Rules for Control of False Discovery Rate and False Discovery\n  Exceedance", "comments": "44 pages, 9 figures, to appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a core problem in statistical inference and\narises in almost every scientific field. Given a set of null hypotheses\n$\\mathcal{H}(n) = (H_1,\\dotsc, H_n)$, Benjamini and Hochberg introduced the\nfalse discovery rate (FDR), which is the expected proportion of false positives\namong rejected null hypotheses, and proposed a testing procedure that controls\nFDR below a pre-assigned significance level. Nowadays FDR is the criterion of\nchoice for large scale multiple hypothesis testing. In this paper we consider\nthe problem of controlling FDR in an \"online manner\". Concretely, we consider\nan ordered --possibly infinite-- sequence of null hypotheses $\\mathcal{H} =\n(H_1,H_2,H_3,\\dots )$ where, at each step $i$, the statistician must decide\nwhether to reject hypothesis $H_i$ having access only to the previous\ndecisions. This model was introduced by Foster and Stine. We study a class of\n\"generalized alpha-investing\" procedures and prove that any rule in this class\ncontrols online FDR, provided $p$-values corresponding to true nulls are\nindependent from the other $p$-values. (Earlier work only established mFDR\ncontrol.) Next, we obtain conditions under which generalized alpha-investing\ncontrols FDR in the presence of general $p$-values dependencies. Finally, we\ndevelop a modified set of procedures that also allow to control the false\ndiscovery exceedance (the tail of the proportion of false discoveries).\nNumerical simulations and analytical results indicate that online procedures do\nnot incur a large loss in statistical power with respect to offline approaches,\nsuch as Benjamini-Hochberg.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 23:41:51 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 23:49:37 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 23:37:43 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1603.09029", "submitter": "Nguyen Viet Cuong", "authors": "Nguyen Viet Cuong, Huan Xu", "title": "Adaptive Maximization of Pointwise Submodular Functions With Budget\n  Constraint", "comments": "This paper was published at the 30th Conference on Neural Information\n  Processing Systems (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the worst-case adaptive optimization problem with budget constraint\nthat is useful for modeling various practical applications in artificial\nintelligence and machine learning. We investigate the near-optimality of greedy\nalgorithms for this problem with both modular and non-modular cost functions.\nIn both cases, we prove that two simple greedy algorithms are not near-optimal\nbut the best between them is near-optimal if the utility function satisfies\npointwise submodularity and pointwise cost-sensitive submodularity\nrespectively. This implies a combined algorithm that is near-optimal with\nrespect to the optimal algorithm that uses half of the budget. We discuss\napplications of our theoretical results and also report experiments comparing\nthe greedy algorithms on the active learning problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 03:27:41 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 21:08:53 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Cuong", "Nguyen Viet", ""], ["Xu", "Huan", ""]]}, {"id": "1603.09035", "submitter": "Ignacio Cano", "authors": "Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino and Giovanni\n  Matteo Fumarola", "title": "Towards Geo-Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency to end-users and regulatory requirements push large companies to\nbuild data centers all around the world. The resulting data is \"born\"\ngeographically distributed. On the other hand, many machine learning\napplications require a global view of such data in order to achieve the best\nresults. These types of applications form a new class of learning problems,\nwhich we call Geo-Distributed Machine Learning (GDML). Such applications need\nto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)\ngrowing privacy concerns that are pushing for stricter data sovereignty\nregulations. Current solutions to learning from geo-distributed data sources\nrevolve around the idea of first centralizing the data in one data center, and\nthen training locally. As machine learning algorithms are\ncommunication-intensive, the cost of centralizing the data is thought to be\noffset by the lower cost of intra-data center communication during training. In\nthis work, we show that the current centralized practice can be far from\noptimal, and propose a system for doing geo-distributed training. Furthermore,\nwe argue that the geo-distributed approach is structurally more amenable to\ndealing with regulatory constraints, as raw data never leaves the source data\ncenter. Our empirical evaluation on three real datasets confirms the general\nvalidity of our approach, and shows that GDML is not only possible but also\nadvisable in many scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 04:05:29 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Cano", "Ignacio", ""], ["Weimer", "Markus", ""], ["Mahajan", "Dhruv", ""], ["Curino", "Carlo", ""], ["Fumarola", "Giovanni Matteo", ""]]}, {"id": "1603.09045", "submitter": "Federico Ricci-Tersenghi", "authors": "Adel Javanmard, Andrea Montanari, Federico Ricci-Tersenghi", "title": "Performance of a community detection algorithm based on semidefinite\n  programming", "comments": "12 pages, 7 figures. Proceedings for the HD3-2015 conference (Kyoto,\n  Dec 14-17, 2015)", "journal-ref": "J. Phys.: Conf. Ser. 699 (2016) 012015", "doi": "10.1088/1742-6596/699/1/012015", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting communities in a graph is maybe one the most studied\ninference problems, given its simplicity and widespread diffusion among several\ndisciplines. A very common benchmark for this problem is the stochastic block\nmodel or planted partition problem, where a phase transition takes place in the\ndetection of the planted partition by changing the signal-to-noise ratio.\nOptimal algorithms for the detection exist which are based on spectral methods,\nbut we show these are extremely sensible to slight modification in the\ngenerative model. Recently Javanmard, Montanari and Ricci-Tersenghi\n(arXiv:1511.08769) have used statistical physics arguments, and numerical\nsimulations to show that finding communities in the stochastic block model via\nsemidefinite programming is quasi optimal. Further, the resulting semidefinite\nrelaxation can be solved efficiently, and is very robust with respect to\nchanges in the generative model. In this paper we study in detail several\npractical aspects of this new algorithm based on semidefinite programming for\nthe detection of the planted partition. The algorithm turns out to be very\nfast, allowing the solution of problems with $O(10^5)$ variables in few second\non a laptop computer.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 05:47:15 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""], ["Ricci-Tersenghi", "Federico", ""]]}, {"id": "1603.09048", "submitter": "Kuan-Hao Huang", "authors": "Kuan-Hao Huang and Hsuan-Tien Lin", "title": "Cost-Sensitive Label Embedding for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-017-5659-z", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label embedding (LE) is an important family of multi-label classification\nalgorithms that digest the label information jointly for better performance.\nDifferent real-world applications evaluate performance by different cost\nfunctions of interest. Current LE algorithms often aim to optimize one specific\ncost function, but they can suffer from bad performance with respect to other\ncost functions. In this paper, we resolve the performance issue by proposing a\nnovel cost-sensitive LE algorithm that takes the cost function of interest into\naccount. The proposed algorithm, cost-sensitive label embedding with\nmultidimensional scaling (CLEMS), approximates the cost information with the\ndistances of the embedded vectors by using the classic multidimensional scaling\napproach for manifold learning. CLEMS is able to deal with both symmetric and\nasymmetric cost functions, and effectively makes cost-sensitive decisions by\nnearest-neighbor decoding within the embedded vectors. We derive theoretical\nresults that justify how CLEMS achieves the desired cost-sensitivity.\nFurthermore, extensive experimental results demonstrate that CLEMS is\nsignificantly better than a wide spectrum of existing LE algorithms and\nstate-of-the-art cost-sensitive algorithms across different cost functions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:19:02 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 06:08:14 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 08:05:10 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 04:45:25 GMT"}, {"version": "v5", "created": "Sat, 5 Aug 2017 04:05:40 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Huang", "Kuan-Hao", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1603.09050", "submitter": "Nguyen Viet Cuong", "authors": "Nguyen Viet Cuong, Nan Ye, Wee Sun Lee", "title": "Robustness of Bayesian Pool-based Active Learning Against Prior\n  Misspecification", "comments": "This paper is published at AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of active learning (AL) algorithms against prior\nmisspecification: whether an algorithm achieves similar performance using a\nperturbed prior as compared to using the true prior. In both the average and\nworst cases of the maximum coverage setting, we prove that all\n$\\alpha$-approximate algorithms are robust (i.e., near $\\alpha$-approximate) if\nthe utility is Lipschitz continuous in the prior. We further show that\nrobustness may not be achieved if the utility is non-Lipschitz. This suggests\nwe should use a Lipschitz utility for AL if robustness is required. For the\nminimum cost setting, we can also obtain a robustness result for approximate AL\nalgorithms. Our results imply that many commonly used AL algorithms are robust\nagainst perturbed priors. We then propose the use of a mixture prior to\nalleviate the problem of prior misspecification. We analyze the robustness of\nthe uniform mixture prior and show experimentally that it performs reasonably\nwell in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:21:42 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Cuong", "Nguyen Viet", ""], ["Ye", "Nan", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1603.09128", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Ivan Titov and Gertjan van Noord", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "comments": "11 pages, to appear at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learning multi-sense word embeddings relying both\non monolingual and bilingual information. Our model consists of an encoder,\nwhich uses monolingual and bilingual context (i.e. a parallel sentence) to\nchoose a sense for a given word, and a decoder which predicts context words\nbased on the chosen sense. The two components are estimated jointly. We observe\nthat the word representations induced from bilingual data outperform the\nmonolingual counterparts across a range of evaluation tasks, even though\ncrosslingual information is not available at test time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 11:09:01 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["\u0160uster", "Simon", ""], ["Titov", "Ivan", ""], ["van Noord", "Gertjan", ""]]}, {"id": "1603.09170", "submitter": "Bin Wang", "authors": "Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura", "title": "Model Interpolation with Trans-dimensional Random Field Language Models\n  for Speech Recognition", "comments": "three pages, 2 experiment result tables, reporting the WERs on an\n  Englisth dateset and a Chinese dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant language models (LMs) such as n-gram and neural network (NN)\nmodels represent sentence probabilities in terms of conditionals. In contrast,\na new trans-dimensional random field (TRF) LM has been recently introduced to\nshow superior performances, where the whole sentence is modeled as a random\nfield. In this paper, we examine how the TRF models can be interpolated with\nthe NN models, and obtain 12.1\\% and 17.9\\% relative error rate reductions over\n6-gram LMs for English and Chinese speech recognition respectively through\nlog-linear combination.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:09:20 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 13:19:06 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 02:36:33 GMT"}, {"version": "v4", "created": "Wed, 17 Aug 2016 01:54:32 GMT"}, {"version": "v5", "created": "Sat, 20 Aug 2016 03:50:16 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""], ["He", "Yong", ""], ["Kawamura", "Akinori", ""]]}, {"id": "1603.09254", "submitter": "Yasushi Terazono", "authors": "Yasushi Terazono (1) ((1) Graduate School of Information Science and\n  Technology, The University of Tokyo)", "title": "A latent-observed dissimilarity measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitatively assessing relationships between latent variables and observed\nvariables is important for understanding and developing generative models and\nrepresentation learning. In this paper, we propose latent-observed\ndissimilarity (LOD) to evaluate the dissimilarity between the probabilistic\ncharacteristics of latent and observed variables. We also define four essential\ntypes of generative models with different independence/conditional independence\nconfigurations. Experiments using tractable real-world data show that LOD can\neffectively capture the differences between models and reflect the capability\nfor higher layer learning. They also show that the conditional independence of\nlatent variables given observed variables contributes to improving the\ntransmission of information and characteristics from lower layers to higher\nlayers.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 15:45:51 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Terazono", "Yasushi", ""]]}, {"id": "1603.09260", "submitter": "Vladimir Jojic", "authors": "Tianxiang Gao and Vladimir Jojic", "title": "Degrees of Freedom in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore degrees of freedom in deep sigmoidal neural\nnetworks. We show that the degrees of freedom in these models is related to the\nexpected optimism, which is the expected difference between test error and\ntraining error. We provide an efficient Monte-Carlo method to estimate the\ndegrees of freedom for multi-class classification methods. We show degrees of\nfreedom are lower than the parameter count in a simple XOR network. We extend\nthese results to neural nets trained on synthetic and real data, and\ninvestigate impact of network's architecture and different regularization\nchoices. The degrees of freedom in deep networks are dramatically smaller than\nthe number of parameters, in some real datasets several orders of magnitude.\nFurther, we observe that for fixed number of parameters, deeper networks have\nless degrees of freedom exhibiting a regularization-by-depth.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:16:57 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 14:45:35 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Gao", "Tianxiang", ""], ["Jojic", "Vladimir", ""]]}, {"id": "1603.09272", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Paul Blomstedt and Samuel Kaski", "title": "Bayesian inference in hierarchical models by combining independent\n  posteriors", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models are versatile tools for joint modeling of data sets\narising from different, but related, sources. Fully Bayesian inference may,\nhowever, become computationally prohibitive if the source-specific data models\nare complex, or if the number of sources is very large. To facilitate\ncomputation, we propose an approach, where inference is first made\nindependently for the parameters of each data set, whereupon the obtained\nposterior samples are used as observed data in a substitute hierarchical model,\nbased on a scaled likelihood function. Compared to direct inference in a full\nhierarchical model, the approach has the advantage of being able to speed up\nconvergence by breaking down the initial large inference problem into smaller\nindividual subproblems with better convergence properties. Moreover it enables\nparallel processing of the possibly complex inferences of the source-specific\nparameters, which may otherwise create a computational bottleneck if processed\njointly as part of a hierarchical model. The approach is illustrated with both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 09:33:22 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Blomstedt", "Paul", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.09279", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli", "title": "On the Geometry of Message Passing Algorithms for Gaussian Reciprocal\n  Processes", "comments": "15 pages; Typos corrected; This paper introduces belief propagation\n  for Gaussian reciprocal processes and extends the convergence analysis in\n  arXiv:1603.04419 to the Gaussian case", "journal-ref": "Proc. of the 55th IEEE Conference on Decision and Control (CDC\n  2016)", "doi": "10.1109/CDC.2016.7798965", "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocal processes are acausal generalizations of Markov processes\nintroduced by Bernstein in 1932. In the literature, a significant amount of\nattention has been focused on developing dynamical models for reciprocal\nprocesses. Recently, probabilistic graphical models for reciprocal processes\nhave been provided. This opens the way to the application of efficient\ninference algorithms in the machine learning literature to solve the smoothing\nproblem for reciprocal processes. Such algorithms are known to converge if the\nunderlying graph is a tree. This is not the case for a reciprocal process,\nwhose associated graphical model is a single loop network. The contribution of\nthis paper is twofold. First, we introduce belief propagation for Gaussian\nreciprocal processes. Second, we establish a link between convergence analysis\nof belief propagation for Gaussian reciprocal processes and stability theory\nfor differentially positive systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 17:15:02 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 17:39:38 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Carli", "Francesca Paola", ""]]}, {"id": "1603.09326", "submitter": "Guido Imbens", "authors": "Susan Athey, Raj Chetty, Guido Imbens, Hyunseung Kang", "title": "Estimating Treatment Effects using Multiple Surrogates: The Role of the\n  Surrogate Score and the Surrogate Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the long-term effects of treatments is of interest in many fields.\nA common challenge in estimating such treatment effects is that long-term\noutcomes are unobserved in the time frame needed to make policy decisions. One\napproach to overcome this missing data problem is to analyze treatments effects\non an intermediate outcome, often called a statistical surrogate, if it\nsatisfies the condition that treatment and outcome are independent conditional\non the statistical surrogate. The validity of the surrogacy condition is often\ncontroversial. Here we exploit that fact that in modern datasets, researchers\noften observe a large number, possibly hundreds or thousands, of intermediate\noutcomes, thought to lie on or close to the causal chain between the treatment\nand the long-term outcome of interest. Even if none of the individual proxies\nsatisfies the statistical surrogacy criterion by itself, using multiple proxies\ncan be useful in causal inference. We focus primarily on a setting with two\nsamples, an experimental sample containing data about the treatment indicator\nand the surrogates and an observational sample containing information about the\nsurrogates and the primary outcome. We state assumptions under which the\naverage treatment effect be identified and estimated with a high-dimensional\nvector of proxies that collectively satisfy the surrogacy assumption, and\nderive the bias from violations of the surrogacy assumption, and show that even\nif the primary outcome is also observed in the experimental sample, there is\nstill information to be gained from using surrogates.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 19:45:52 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 19:32:10 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 07:18:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Athey", "Susan", ""], ["Chetty", "Raj", ""], ["Imbens", "Guido", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1603.09441", "submitter": "Ian Dewancker", "authors": "Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra\n  Johnson and George Ke", "title": "A Stratified Analysis of Bayesian Optimization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical analysis serves as an important complement to theoretical analysis\nfor studying practical Bayesian optimization. Often empirical insights expose\nstrengths and weaknesses inaccessible to theoretical analysis. We define two\nmetrics for comparing the performance of Bayesian optimization methods and\npropose a ranking mechanism for summarizing performance within various genres\nor strata of test functions. These test functions serve to mimic the complexity\nof hyperparameter optimization problems, the most prominent application of\nBayesian optimization, but with a closed form which allows for rapid evaluation\nand more predictable behavior. This offers a flexible and efficient way to\ninvestigate functions with specific properties of interest, such as oscillatory\nbehavior or an optimum on the domain boundary.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 02:23:24 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dewancker", "Ian", ""], ["McCourt", "Michael", ""], ["Clark", "Scott", ""], ["Hayes", "Patrick", ""], ["Johnson", "Alexandra", ""], ["Ke", "George", ""]]}, {"id": "1603.09584", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI), Anne Sabourin (LTCI), St\\'ephan Cl\\'emen\\c{c}on\n  (LTCI)", "title": "Sparse Representation of Multivariate Extremes with Applications to\n  Anomaly Ranking", "comments": "arXiv admin note: text overlap with arXiv:1507.05899", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremes play a special role in Anomaly Detection. Beyond inference and\nsimulation purposes, probabilistic tools borrowed from Extreme Value Theory\n(EVT), such as the angular measure, can also be used to design novel\nstatistical learning methods for Anomaly Detection/ranking. This paper proposes\na new algorithm based on multivariate EVT to learn how to rank observations in\na high dimensional space with respect to their degree of 'abnormality'. The\nprocedure relies on an original dimension-reduction technique in the extreme\ndomain that possibly produces a sparse representation of multivariate extremes\nand allows to gain insight into the dependence structure thereof, escaping the\ncurse of dimensionality. The representation output by the unsupervised\nmethodology we propose here can be combined with any Anomaly Detection\ntechnique tailored to non-extreme data. As it performs linearly with the\ndimension and almost linearly in the data (in O(dn log n)), it fits to large\nscale problems. The approach in this paper is novel in that EVT has never been\nused in its multivariate version in the field of Anomaly Detection.\nIllustrative experimental results provide strong empirical evidence of the\nrelevance of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 13:25:16 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1603.09620", "submitter": "Hans Verstraete", "authors": "Laurens Bliek, Hans R. G. W. Verstraete, Michel Verhaegen, and Sander\n  Wahls", "title": "Online Optimization with Costly and Noisy Measurements using Random\n  Fourier Expansions", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes DONE, an online optimization algorithm that iteratively\nminimizes an unknown function based on costly and noisy measurements. The\nalgorithm maintains a surrogate of the unknown function in the form of a random\nFourier expansion (RFE). The surrogate is updated whenever a new measurement is\navailable, and then used to determine the next measurement point. The algorithm\nis comparable to Bayesian optimization algorithms, but its computational\ncomplexity per iteration does not depend on the number of measurements. We\nderive several theoretical results that provide insight on how the\nhyper-parameters of the algorithm should be chosen. The algorithm is compared\nto a Bayesian optimization algorithm for a benchmark problem and three\napplications, namely, optical coherence tomography, optical beam-forming\nnetwork tuning, and robot arm control. It is found that the DONE algorithm is\nsignificantly faster than Bayesian optimization in the discussed problems,\nwhile achieving a similar or better performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:00:06 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 14:07:04 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 11:13:00 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Bliek", "Laurens", ""], ["Verstraete", "Hans R. G. W.", ""], ["Verhaegen", "Michel", ""], ["Wahls", "Sander", ""]]}, {"id": "1603.09638", "submitter": "Berkay Celik", "authors": "Z. Berkay Celik, Patrick McDaniel, Rauf Izmailov, Nicolas Papernot,\n  Ryan Sheatsley, Raquel Alvarez, Ananthram Swami", "title": "Detection under Privileged Information", "comments": "A short version of this paper is accepted to ASIACCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For well over a quarter century, detection systems have been driven by models\nlearned from input features collected from real or simulated environments. An\nartifact (e.g., network event, potential malware sample, suspicious email) is\ndeemed malicious or non-malicious based on its similarity to the learned model\nat runtime. However, the training of the models has been historically limited\nto only those features available at runtime. In this paper, we consider an\nalternate learning approach that trains models using \"privileged\"\ninformation--features available at training time but not at runtime--to improve\nthe accuracy and resilience of detection systems. In particular, we adapt and\nextend recent advances in knowledge transfer, model influence, and distillation\nto enable the use of forensic or other data unavailable at runtime in a range\nof security domains. An empirical evaluation shows that privileged information\nincreases precision and recall over a system with no privileged information: we\nobserve up to 7.7% relative decrease in detection error for fast-flux bot\ndetection, 8.6% for malware traffic detection, 7.3% for malware classification,\nand 16.9% for face recognition. We explore the limitations and applications of\ndifferent privileged information techniques in detection systems. Such\ntechniques provide a new means for detection systems to learn from data that\nwould otherwise not be available at runtime.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:28:45 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 13:59:01 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 01:17:06 GMT"}, {"version": "v4", "created": "Sat, 31 Mar 2018 02:12:21 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Celik", "Z. Berkay", ""], ["McDaniel", "Patrick", ""], ["Izmailov", "Rauf", ""], ["Papernot", "Nicolas", ""], ["Sheatsley", "Ryan", ""], ["Alvarez", "Raquel", ""], ["Swami", "Ananthram", ""]]}, {"id": "1603.09643", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for Speech and Speaker Recognition", "comments": "APSIPA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:37:29 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 05:54:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 06:25:01 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 12:27:17 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}, {"id": "1603.09739", "submitter": "Prithwish Chakraborty", "authors": "Prithwish Chakraborty and Sathappan Muthiah and Ravi Tandon and Naren\n  Ramakrishnan", "title": "Hierarchical Quickest Change Detection via Surrogates", "comments": "Submitted to a journal. See demo at\n  https://prithwi.github.io/hqcd_supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) in time series data is a critical problem as it reveal\nchanges in the underlying generative processes driving the time series. Despite\nhaving received significant attention, one important unexplored aspect is how\nto efficiently utilize additional correlated information to improve the\ndetection and the understanding of changepoints. We propose hierarchical\nquickest change detection (HQCD), a framework that formalizes the process of\nincorporating additional correlated sources for early changepoint detection.\nThe core ideas behind HQCD are rooted in the theory of quickest detection and\nHQCD can be regarded as its novel generalization to a hierarchical setting. The\nsources are classified into targets and surrogates, and HQCD leverages this\nstructure to systematically assimilate observed data to update changepoint\nstatistics across layers. The decision on actual changepoints are provided by\nminimizing the delay while still maintaining reliability bounds. In addition,\nHQCD also uncovers interesting relations between changes at targets from\nchanges across surrogates. We validate HQCD for reliability and performance\nagainst several state-of-the-art methods for both synthetic dataset (known\nchangepoints) and several real-life examples (unknown changepoints). Our\nexperiments indicate that we gain significant robustness without loss of\ndetection delay through HQCD. Our real-life experiments also showcase the\nusefulness of the hierarchical setting by connecting the surrogate sources\n(such as Twitter chatter) to target sources (such as Employment related\nprotests that ultimately lead to major uprisings).\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:50:45 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Chakraborty", "Prithwish", ""], ["Muthiah", "Sathappan", ""], ["Tandon", "Ravi", ""], ["Ramakrishnan", "Naren", ""]]}]