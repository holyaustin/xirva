[{"id": "0802.0566", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LM-Orsay, INRIA Futurs)", "title": "V-fold cross-validation improved: V-fold penalization", "comments": "40 pages, plus a separate technical appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  We study the efficiency of V-fold cross-validation (VFCV) for model selection\nfrom the non-asymptotic viewpoint, and suggest an improvement on it, which we\ncall ``V-fold penalization''. Considering a particular (though simple)\nregression problem, we prove that VFCV with a bounded V is suboptimal for model\nselection, because it ``overpenalizes'' all the more that V is large. Hence,\nasymptotic optimality requires V to go to infinity. However, when the\nsignal-to-noise ratio is low, it appears that overpenalizing is necessary, so\nthat the optimal V is not always the larger one, despite of the variability\nissue. This is confirmed by some simulated data. In order to improve on the\nprediction performance of VFCV, we define a new model selection procedure,\ncalled ``V-fold penalization'' (penVF). It is a V-fold subsampling version of\nEfron's bootstrap penalties, so that it has the same computational cost as\nVFCV, while being more flexible. In a heteroscedastic regression framework,\nassuming the models to have a particular structure, we prove that penVF\nsatisfies a non-asymptotic oracle inequality with a leading constant that tends\nto 1 when the sample size goes to infinity. In particular, this implies\nadaptivity to the smoothness of the regression function, even with a highly\nheteroscedastic noise. Moreover, it is easy to overpenalize with penVF,\nindependently from the V parameter. A simulation study shows that this results\nin a significant improvement on VFCV in non-asymptotic situations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2008 08:56:27 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2008 11:34:45 GMT"}], "update_date": "2008-02-07", "authors_parsed": [["Arlot", "Sylvain", "", "LM-Orsay, INRIA Futurs"]]}, {"id": "0802.0964", "submitter": "Tim Hesterberg", "authors": "Tim Hesterberg, Nam Hee Choi, Lukas Meier, Chris Fraley", "title": "Least angle and $\\ell_1$ penalized regression: A review", "comments": "Published in at http://dx.doi.org/10.1214/08-SS035 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, 61-93", "doi": "10.1214/08-SS035", "report-no": "IMS-SS-SS_2008_35", "categories": "stat.ME stat.ML", "license": null, "abstract": "  Least Angle Regression is a promising technique for variable selection\napplications, offering a nice alternative to stepwise regression. It provides\nan explanation for the similar behavior of LASSO ($\\ell_1$-penalized\nregression) and forward stagewise regression, and provides a fast\nimplementation of both. The idea has caught on rapidly, and sparked a great\ndeal of research interest. In this paper, we give an overview of Least Angle\nRegression and the current state of related research.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2008 12:53:59 GMT"}, {"version": "v2", "created": "Wed, 21 May 2008 06:40:12 GMT"}], "update_date": "2008-05-21", "authors_parsed": [["Hesterberg", "Tim", ""], ["Choi", "Nam Hee", ""], ["Meier", "Lukas", ""], ["Fraley", "Chris", ""]]}, {"id": "0802.1244", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou", "title": "Learning Balanced Mixtures of Discrete Distributions with Small Sample", "comments": "24 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": null, "abstract": "  We study the problem of partitioning a small sample of $n$ individuals from a\nmixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according\nto their distributions. Each distribution is described by a vector of allele\nfrequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the\naverage $\\ell_2^2$ distance in frequencies across $K$ dimensions, which\nmeasures the statistical divergence between them. We study the case assuming\nthat bits are independently distributed across $K$ dimensions. This work\ndemonstrates that, for a balanced input instance for $k = 2$, a certain\ngraph-based optimization function returns the correct partition with high\nprobability, where a weighted graph $G$ is formed over $n$ individuals, whose\npairwise hamming distances between their corresponding bit vectors define the\nedge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln\nn/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where\nthe weight of a cut is the sum of the weights across all edges in the cut. This\nresult demonstrates a nice property in the high-dimensional feature space: one\ncan trade off the number of features that are required with the size of the\nsample to accomplish certain tasks like clustering.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2008 07:38:49 GMT"}], "update_date": "2008-02-21", "authors_parsed": [["Zhou", "Shuheng", ""]]}, {"id": "0802.1517", "submitter": "Han Liu", "authors": "Han Liu, Jian Zhang", "title": "On the $\\ell_1-\\ell_q$ Regularized Regression", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of grouped variable selection in\nhigh-dimensional regression using $\\ell_1-\\ell_q$ regularization ($1\\leq q \\leq\n\\infty$), which can be viewed as a natural generalization of the\n$\\ell_1-\\ell_2$ regularization (the group Lasso). The key condition is that the\ndimensionality $p_n$ can increase much faster than the sample size $n$, i.e.\n$p_n \\gg n$ (in our case $p_n$ is the number of groups), but the number of\nrelevant groups is small. The main conclusion is that many good properties from\n$\\ell_1-$regularization (Lasso) naturally carry on to the $\\ell_1-\\ell_q$ cases\n($1 \\leq q \\leq \\infty$), even if the number of variables within each group\nalso increases with the sample size. With fixed design, we show that the whole\nfamily of estimators are both estimation consistent and variable selection\nconsistent under different conditions. We also show the persistency result with\nrandom design under a much weaker condition. These results provide a unified\ntreatment for the whole family of estimators ranging from $q=1$ (Lasso) to\n$q=\\infty$ (iCAP), with $q=2$ (group Lasso)as a special case. When there is no\ngroup structure available, all the analysis reduces to the current results of\nthe Lasso estimator ($q=1$).\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2008 20:00:55 GMT"}], "update_date": "2008-02-12", "authors_parsed": [["Liu", "Han", ""], ["Zhang", "Jian", ""]]}, {"id": "0802.1669", "submitter": "Nicholas Chia", "authors": "Nicholas Chia and Junji Nakano", "title": "M-decomposability, elliptical unimodal densities, and applications to\n  clustering and kernel density estimation", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chia and Nakano (2009) introduced the concept of M-decomposability of\nprobability densities in one-dimension. In this paper, we generalize\nM-decomposability to any dimension. We prove that all elliptical unimodal\ndensities are M-undecomposable. We also derive an inequality to show that it is\nbetter to represent an M-decomposable density via a mixture of unimodal\ndensities. Finally, we demonstrate the application of M-decomposability to\nclustering and kernel density estimation, using real and simulated data. Our\nresults show that M-decomposability can be used as a non-parametric criterion\nto locate modes in probability densities.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2008 16:41:30 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2010 12:40:03 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Chia", "Nicholas", ""], ["Nakano", "Junji", ""]]}, {"id": "0802.2050", "submitter": "Kevin Carter", "authors": "Kevin M. Carter, Raviv Raich, William G. Finn, and Alfred O. Hero", "title": "FINE: Fisher Information Non-parametric Embedding", "comments": "30 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of clustering, classification, and visualization of\nhigh-dimensional data when no straightforward Euclidean representation exists.\nTypically, these tasks are performed by first reducing the high-dimensional\ndata to some lower dimensional Euclidean space, as many manifold learning\nmethods have been developed for this task. In many practical problems however,\nthe assumption of a Euclidean manifold cannot be justified. In these cases, a\nmore appropriate assumption would be that the data lies on a statistical\nmanifold, or a manifold of probability density functions (PDFs). In this paper\nwe propose using the properties of information geometry in order to define\nsimilarities between data sets using the Fisher information metric. We will\nshow this metric can be approximated using entirely non-parametric methods, as\nthe parameterization of the manifold is generally unknown. Furthermore, by\nusing multi-dimensional scaling methods, we are able to embed the corresponding\nPDFs into a low-dimensional Euclidean space. This not only allows for\nclassification of the data, but also visualization of the manifold. As a whole,\nwe refer to our framework as Fisher Information Non-parametric Embedding\n(FINE), and illustrate its uses on a variety of practical problems, including\nbio-medical applications and document classification.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2008 16:40:17 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Carter", "Kevin M.", ""], ["Raich", "Raviv", ""], ["Finn", "William G.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "0802.2758", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, John Lafferty and Larry Wasserman", "title": "Time Varying Undirected Graphs", "comments": "12 pages, 3 figures, to appear in COLT 2008", "journal-ref": "The 21st Annual Conference on Learning Theory (COLT 2008),\n  Helsinki, Finland", "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphs are often used to describe high dimensional distributions.\nUnder sparsity conditions, the graph can be estimated using $\\ell_1$\npenalization methods. However, current methods assume that the data are\nindependent and identically distributed. If the distribution, and hence the\ngraph, evolves over time then the data are not longer identically distributed.\nIn this paper, we show how to estimate the sequence of graphs for\nnon-identically distributed data, where the distribution evolves over time.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2008 20:54:29 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2008 23:15:40 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2008 06:56:27 GMT"}, {"version": "v4", "created": "Tue, 29 Apr 2008 00:56:14 GMT"}], "update_date": "2008-04-29", "authors_parsed": [["Zhou", "Shuheng", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "0802.2906", "submitter": "Raviv Raich", "authors": "Raviv Raich, Jose A. Costa, Steven B. Damelin, and Alfred O. Hero III", "title": "Classification Constrained Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a topic of recent interest. In this paper, we\npresent the classification constrained dimensionality reduction (CCDR)\nalgorithm to account for label information. The algorithm can account for\nmultiple classes as well as the semi-supervised setting. We present an\nout-of-sample expressions for both labeled and unlabeled data. For unlabeled\ndata, we introduce a method of embedding a new point as preprocessing to a\nclassifier. For labeled data, we introduce a method that improves the embedding\nduring the training phase using the out-of-sample extension. We investigate\nclassification performance using the CCDR algorithm on hyper-spectral satellite\nimagery data. We demonstrate the performance gain for both local and global\nclassifiers and demonstrate a 10% improvement of the $k$-nearest neighbors\nalgorithm performance. We present a connection between intrinsic dimension\nestimation and the optimal embedding dimension obtained using the CCDR\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2008 18:26:31 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2008 17:52:16 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Raich", "Raviv", ""], ["Costa", "Jose A.", ""], ["Damelin", "Steven B.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "0802.3125", "submitter": "Wei Pan", "authors": "Benhuai Xie, Wei Pan, Xiaotong Shen", "title": "Penalized model-based clustering with cluster-specific diagonal\n  covariance matrices and grouped variables", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS194 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2008, Vol. 2, 168-212", "doi": "10.1214/08-EJS194", "report-no": "IMS-EJS-EJS_2008_194", "categories": "stat.ML math.ST stat.TH", "license": null, "abstract": "  Clustering analysis is one of the most widely used statistical tools in many\nemerging areas such as microarray data analysis. For microarray and other\nhigh-dimensional data, the presence of many noise variables may mask underlying\nclustering structures. Hence removing noise variables via variable selection is\nnecessary. For simultaneous variable selection and parameter estimation,\nexisting penalized likelihood approaches in model-based clustering analysis all\nassume a common diagonal covariance matrix across clusters, which however may\nnot hold in practice. To analyze high-dimensional data, particularly those with\nrelatively low sample sizes, this article introduces a novel approach that\nshrinks the variances together with means, in a more general situation with\ncluster-specific (diagonal) covariance matrices. Furthermore, selection of\ngrouped variables via inclusion or exclusion of a group of variables altogether\nis permitted by a specific form of penalty, which facilitates incorporating\nsubject-matter knowledge, such as gene functions in clustering microarray\nsamples for disease subtype discovery. For implementation, EM algorithms are\nderived for parameter estimation, in which the M-steps clearly demonstrate the\neffects of shrinkage and thresholding. Numerical examples, including an\napplication to acute leukemia subtype discovery with microarray gene expression\ndata, are provided to demonstrate the utility and advantage of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2008 14:53:16 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2008 08:07:54 GMT"}], "update_date": "2008-03-26", "authors_parsed": [["Xie", "Benhuai", ""], ["Pan", "Wei", ""], ["Shen", "Xiaotong", ""]]}, {"id": "0802.3141", "submitter": "Joseph Rynkiewicz", "authors": "Joseph Rynkiewicz (CES, Samos)", "title": "Testing the number of parameters with multidimensional MLP", "comments": null, "journal-ref": "Dans ASMDA 2005 - ASMDA 2005, Brest : France (2005)", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  This work concerns testing the number of parameters in one hidden layer\nmultilayer perceptron (MLP). For this purpose we assume that we have\nidentifiable models, up to a finite group of transformations on the weights,\nthis is for example the case when the number of hidden units is know. In this\nframework, we show that we get a simple asymptotic distribution, if we use the\nlogarithm of the determinant of the empirical error covariance matrix as cost\nfunction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2008 15:54:05 GMT"}], "update_date": "2008-02-22", "authors_parsed": [["Rynkiewicz", "Joseph", "", "CES, Samos"]]}, {"id": "0802.3142", "submitter": "Joseph Rynkiewicz", "authors": "Joseph Rynkiewicz (CES, Samos)", "title": "Efficient Estimation of Multidimensional Regression Model with\n  Multilayer Perceptron", "comments": null, "journal-ref": "Dans ESANN 2005 - ESANN 2005, Bruges : Belgique (2005)", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": null, "abstract": "  This work concerns estimation of multidimensional nonlinear regression models\nusing multilayer perceptron (MLP). The main problem with such model is that we\nhave to know the covariance matrix of the noise to get optimal estimator.\nhowever we show that, if we choose as cost function the logarithm of the\ndeterminant of the empirical error covariance matrix, we get an asymptotically\noptimal estimator.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2008 15:54:35 GMT"}], "update_date": "2008-02-22", "authors_parsed": [["Rynkiewicz", "Joseph", "", "CES, Samos"]]}, {"id": "0802.3150", "submitter": "Joseph Rynkiewicz", "authors": "Joseph Rynkiewicz (CES, Samos)", "title": "Self Organizing Map algorithm and distortion measure", "comments": null, "journal-ref": "Neural Networks 19, 6-7 (2006) 671-678", "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": null, "abstract": "  We study the statistical meaning of the minimization of distortion measure\nand the relation between the equilibrium points of the SOM algorithm and the\nminima of distortion measure. If we assume that the observations and the map\nlie in an compact Euclidean space, we prove the strong consistency of the map\nwhich almost minimizes the empirical distortion. Moreover, after calculating\nthe derivatives of the theoretical distortion measure, we show that the points\nminimizing this measure and the equilibria of the Kohonen map do not match in\ngeneral. We illustrate, with a simple example, how this occurs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2008 16:08:09 GMT"}], "update_date": "2008-02-22", "authors_parsed": [["Rynkiewicz", "Joseph", "", "CES, Samos"]]}]